{
    "docs": [
        {
            "location": "/", 
            "text": "Joost van der Griendt's CI/CD Knowledge Docs!\n\n\nCI/CD\n stands for \nContinuous Integration\n \n \nContinuous Delivery\n.\n\n\nThis is a collection of knowledge that I keep reusing at different customers.\nSo I figured, lets just host it online for everyone to use.\n\n\nContinuous Integration\n\n\nA good definition can be found here: \nhttp://www.martinfowler.com/articles/continuousIntegration.html\n\n\nContinuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\"\n\n\nContinuous Delivery\n\n\nContinuous Delivery/deployment is the next step in getting yr software changes at the desired server in order to let your clients take a look at it.\nThis article provides a good example of it: \nhttp://www.martinfowler.com/articles/continuousIntegration.html\n\n\nTo do Continuous Integration you need multiple environments, one to run commit tests, one or more to run secondary tests. Since you are moving executables between these environments multiple times a day, you'll want to do this automatically. So it's important to have scripts that will allow you to deploy the application into any environment easily.", 
            "title": "Home"
        }, 
        {
            "location": "/#joost-van-der-griendts-cicd-knowledge-docs", 
            "text": "CI/CD  stands for  Continuous Integration     Continuous Delivery .  This is a collection of knowledge that I keep reusing at different customers.\nSo I figured, lets just host it online for everyone to use.", 
            "title": "Joost van der Griendt's CI/CD Knowledge Docs!"
        }, 
        {
            "location": "/#continuous-integration", 
            "text": "A good definition can be found here:  http://www.martinfowler.com/articles/continuousIntegration.html  Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\"", 
            "title": "Continuous Integration"
        }, 
        {
            "location": "/#continuous-delivery", 
            "text": "Continuous Delivery/deployment is the next step in getting yr software changes at the desired server in order to let your clients take a look at it.\nThis article provides a good example of it:  http://www.martinfowler.com/articles/continuousIntegration.html  To do Continuous Integration you need multiple environments, one to run commit tests, one or more to run secondary tests. Since you are moving executables between these environments multiple times a day, you'll want to do this automatically. So it's important to have scripts that will allow you to deploy the application into any environment easily.", 
            "title": "Continuous Delivery"
        }, 
        {
            "location": "/java/", 
            "text": "Java\n\n\nPatterns/Anti-patterns\n\n\nConstants\n\n\nUse a class that cannot be instantiated for the use of constants.\n\n\nUsing an interface is \nan anti-pattern\n because of what an interface implies.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n/**\n\n\n * It should also be final, else we can extend this and create a constructor allowing us to instantiate it anyway.\n\n\n */\n\n\npublic\n \nfinal\n \nclass\n \nConstants\n \n{\n\n    \nprivate\n \nConstants\n()\n \n{}\n \n// we should not instantiate this class\n\n\n    \npublic\n \nstatic\n \nfinal\n \nString\n \nHELLO\n \n=\n \nWORLD\n;\n\n    \npublic\n \nstatic\n \nfinal\n \nint\n \nAMOUNT_OF_CONSTANTS\n \n=\n \n2\n;\n\n\n}\nedge\n \n\n\n\n\n\n\nOther usefull things\n\n\n\n\nRandom integer", 
            "title": "General"
        }, 
        {
            "location": "/java/#java", 
            "text": "", 
            "title": "Java"
        }, 
        {
            "location": "/java/#patternsanti-patterns", 
            "text": "", 
            "title": "Patterns/Anti-patterns"
        }, 
        {
            "location": "/java/#constants", 
            "text": "Use a class that cannot be instantiated for the use of constants.  Using an interface is  an anti-pattern  because of what an interface implies.  1\n2\n3\n4\n5\n6\n7\n8\n9 /**   * It should also be final, else we can extend this and create a constructor allowing us to instantiate it anyway.   */  public   final   class   Constants   { \n     private   Constants ()   {}   // we should not instantiate this class \n\n     public   static   final   String   HELLO   =   WORLD ; \n     public   static   final   int   AMOUNT_OF_CONSTANTS   =   2 ;  } edge", 
            "title": "Constants"
        }, 
        {
            "location": "/java/#other-usefull-things", 
            "text": "Random integer", 
            "title": "Other usefull things"
        }, 
        {
            "location": "/java/spring/boot/", 
            "text": "", 
            "title": "Spring Boot"
        }, 
        {
            "location": "/java/java9/", 
            "text": "Java 9\n\n\n\n\nhttps://jaxenter.com/maven-on-java-9-things-you-need-to-know-140985.html\n\n\nhttps://blog.codefx.org/java/five-command-line-options-to-hack-the-java-9-module-system/", 
            "title": "Java 9"
        }, 
        {
            "location": "/java/java9/#java-9", 
            "text": "https://jaxenter.com/maven-on-java-9-things-you-need-to-know-140985.html  https://blog.codefx.org/java/five-command-line-options-to-hack-the-java-9-module-system/", 
            "title": "Java 9"
        }, 
        {
            "location": "/java/ecosystem/", 
            "text": "Java Ecosysten", 
            "title": "Java Ecosystem"
        }, 
        {
            "location": "/java/ecosystem/#java-ecosysten", 
            "text": "", 
            "title": "Java Ecosysten"
        }, 
        {
            "location": "/java/networking/", 
            "text": "Java Networking\n\n\nGeneral Remarks\n\n\n\n\nNetwork API works for IPv4 (32-bit adrressing) and IPv6 (128-bit addressing)\n\n\nJava only supports \nTCP/IP\n and \nUDP/IP\n\n\n\n\nJava proxy system params\n\n\n\n\nsocksProxyHost\n\n\nsocksProxyPort\n\n\nhttp.proxySet\n\n\nhttp.proxyHost\n\n\nhttp.proxyPort\n\n\nhttps.proxySet\n\n\nhttps.proxyHost\n\n\nhttps.proxyPort\n\n\nftpProxySet\n\n\nftpProxyHost\n\n\nftpProxyPort\n\n\ngopherProxySet \n\n\ngopherProxyHost\n\n\ngopherProxyPort \n\n\n\n\nSpecial IPv4 segments\n\n\nInternal\n\n\n\n\n10.\n.\n.* \n\n\n172.17.\n.\n - 172.31.\n.\n\n\n192.168.\n.\n\n\n\n\nLocal\n\n\n\n\n127.\n.\n.*\n\n\n\n\nBroadcast\n\n\n\n\n255.255.255.255\n    \n Packets sent to this address are received by all nodes on the local network, though they are not routed beyond the local network\n\n\n\n\nSpecial IPv6 segments\n\n\nLocal\n\n\n\n\n0:0:0:0:0:0:0:1 (or ::::::1 or ::1)", 
            "title": "Java Networking"
        }, 
        {
            "location": "/java/networking/#java-networking", 
            "text": "", 
            "title": "Java Networking"
        }, 
        {
            "location": "/java/networking/#general-remarks", 
            "text": "Network API works for IPv4 (32-bit adrressing) and IPv6 (128-bit addressing)  Java only supports  TCP/IP  and  UDP/IP", 
            "title": "General Remarks"
        }, 
        {
            "location": "/java/networking/#java-proxy-system-params", 
            "text": "socksProxyHost  socksProxyPort  http.proxySet  http.proxyHost  http.proxyPort  https.proxySet  https.proxyHost  https.proxyPort  ftpProxySet  ftpProxyHost  ftpProxyPort  gopherProxySet   gopherProxyHost  gopherProxyPort", 
            "title": "Java proxy system params"
        }, 
        {
            "location": "/java/networking/#special-ipv4-segments", 
            "text": "", 
            "title": "Special IPv4 segments"
        }, 
        {
            "location": "/java/networking/#internal", 
            "text": "10. . .*   172.17. .  - 172.31. .  192.168. .", 
            "title": "Internal"
        }, 
        {
            "location": "/java/networking/#local", 
            "text": "127. . .*", 
            "title": "Local"
        }, 
        {
            "location": "/java/networking/#broadcast", 
            "text": "255.255.255.255\n      Packets sent to this address are received by all nodes on the local network, though they are not routed beyond the local network", 
            "title": "Broadcast"
        }, 
        {
            "location": "/java/networking/#special-ipv6-segments", 
            "text": "", 
            "title": "Special IPv6 segments"
        }, 
        {
            "location": "/java/networking/#local_1", 
            "text": "0:0:0:0:0:0:0:1 (or ::::::1 or ::1)", 
            "title": "Local"
        }, 
        {
            "location": "/java/streams/", 
            "text": "Java Streams\n\n\nTry-with-Resources\n\n\n\n\ntry with resources can be used with any object that implements the\n  Closeable interface, which includes almost every object you need to\n  dispose. So far, JavaMail Transport objects are the only exceptions\n  I\u2019ve encountered. Those still need to be disposed of explicitly.\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\npublic\n \nclass\n \nMain\n \n{\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \n{\n\n        \ntry\n \n(\nOutputStream\n \nout\n \n=\n \nnew\n \nFileOutputStream\n(\n/tmp/data.txt\n))\n \n{\n\n            \n// work with the output stream...\n\n        \n}\n \ncatch\n \n(\nIOException\n \nex\n)\n \n{\n\n            \nSystem\n.\nerr\n.\nprintln\n(\nex\n.\ngetMessage\n());\n\n        \n}\n\n    \n}\n\n\n}", 
            "title": "Java Streams"
        }, 
        {
            "location": "/java/streams/#java-streams", 
            "text": "", 
            "title": "Java Streams"
        }, 
        {
            "location": "/java/streams/#try-with-resources", 
            "text": "try with resources can be used with any object that implements the\n  Closeable interface, which includes almost every object you need to\n  dispose. So far, JavaMail Transport objects are the only exceptions\n  I\u2019ve encountered. Those still need to be disposed of explicitly.   1\n2\n3\n4\n5\n6\n7\n8\n9 public   class   Main   { \n     public   static   void   main ( String []   args )   { \n         try   ( OutputStream   out   =   new   FileOutputStream ( /tmp/data.txt ))   { \n             // work with the output stream... \n         }   catch   ( IOException   ex )   { \n             System . err . println ( ex . getMessage ()); \n         } \n     }  }", 
            "title": "Try-with-Resources"
        }, 
        {
            "location": "/java/concurrency/", 
            "text": "Java Concurrency\n\n\nTerminology\n\n\nCorrectness\nCorrectness means that a class \nconforms to its specification\n. \nA good specification defines \ninvariants\n constraining an object\u2019s state \nand \npostconditions\n describing the effects of its operations. \n6\nThread Safe Class\na class is thread-safe when it continues to behave correctly when accessed\nfrom multiple threads\nNo set of operations performed sequentially or concurrently on instances of a thread-safe class \ncan cause an instance to be in an invalid state. \n6\nMutex\nEvery Java object can implicitly act as a lock for purposes of synchronization;\nthese built-in locks are called \nintrinsic locks\n or \nmonitor locks\n. \nThe lock is auto-matically acquired by the executing thread before entering a synchronized block\nand automatically released when control exits the synchronized block, whether\nby the normal control path or by throwing an exception out of the block.\nIntrinsic locks in Java act as \nmutexes\n (or \nmutual exclusion locks\n), which means\nthat at most one thread may own the lock. When thread A attempts to acquire a\nlock held by thread B, A must wait, or block, until B releases it. If B never releases\nthe lock, A waits forever. \n6\nReentrant locks\nWhen a thread requests a lock that is already held by another thread, the requesting thread blocks. \nBut because intrinsic locks are \nreentrant\n, if a thread tries to acquire a lock that it already holds, the request succeeds. \nReentrancy\n means that locks are acquired on a per-thread rather than per-invocation basis. \nReentrancy\n is implemented by associating with each lock an \nacquisition count\n and an owning \nthread\n. \nWhen the count is zero, the lock is considered unheld. \nWhen a thread acquires a previously unheld lock, the JVM records the owner and sets the acquisition count to one. \nIf that same thread acquires the lock again, the count is incremented, and when the owning thread exits the \nsynchronized block\n, \nthe count is decremented. When the count reaches zero, the lock is released. \n6\nLiveness\nIn concurrent computing, liveness refers to a set of properties of concurrent systems, \nthat require a system to make progress despite the fact that its concurrently executing components (\"processes\") \n    may have to \"take turns\" in critical sections, parts of the program that cannot be simultaneously run by multiple processes.\n1\n \nLiveness guarantees are important properties in operating systems and distributed systems.\n2\nA liveness property cannot be violated in a finite execution of a distributed system because the \"good\" event might only theoretically occur at some time after execution ends. \nEventual consistency is an example of a liveness property.\n3\n \nAll properties can be expressed as the intersection of safety and liveness properties.\n4\nVolatile fields\nWhen a field is declared \nvolatile\n, the \ncompiler\n and \nruntime\n are put on notice that this variable is shared \nand that operations on it should not be reordered with other memory operations. \nVolatile variables are not cached in registers or in caches where they are hidden from other processors, \nso a read of a volatile variable always returns the \nmost recent write\n by \nany\n thread. \n6\nYou can use volatile variables only when all the following criteria are met:\nWrites to the variable do not depend on its current value, or you can ensure that only a single thread ever updates the value;\nThe variable does not participate in invariants with other state variables;\nLocking is not required for any other reason while the variable is being accessed\nConfinement\nConfined objects must not escape their intended scope. \nAn object may be confined to a class instance (such as a private class member), a lexical scope (such\nas a local variable), or a thread (such as an object that is passed from method to\nmethod within a thread, but not supposed to be shared across threads). \nObjects don\u2019t escape on their own, of course\u2014they need help from the developer,\n who assists by publishing the object beyond its intended scope. \n6\nLatch\nSimply put, a CountDownLatch has a counter field, which you can decrement as we require. \nWe can then use it to block a calling thread until it\u2019s been counted down to zero.\nIf we were doing some parallel processing, we could instantiate the CountDownLatch with \nthe same value for the counter as a number of threads we want to work across. \nThen, we could just call countdown() after each thread finishes, \nguaranteeing that a dependent thread calling await() will block until the worker threads are finished.\n\n7\nSemaphore\nIn computer science, a semaphore is a variable or abstract data type used to control access to a common resource by multiple processes \nin a concurrent system such as a multiprogramming operating system.\nA trivial semaphore is a plain variable that is changed (for example, incremented or decremented, or toggled) \ndepending on programmer-defined conditions. The variable is then used as a condition to control access to some system resource.\nA useful way to think of a semaphore as used in the real-world systems is as a record of how many units \nof a particular resource are available, coupled with operations to adjust that record safely \n(i.e. to avoid race conditions) as units are required or become free, and, if necessary, \nwait until a unit of the resource becomes available. \n7\nJava Thread pools\nThere are several different types of Thread pools available.\nFixedThreadPool\n: A fixed-size thread pool creates threads as tasks are submitted, \n    up to the maximum pool size, and then attempts to keep the pool\n    size constant (adding new threads if a thread dies due to an unexpected Exception ).\nCachedThreadPool\n: A cached thread pool has more flexibility to reap idle threads when the current size of the pool \nexceeds the demand for processing, and to add new threads when demand increases, but places no bounds on the size of the pool.\nSingleThreadExecutor\n: A single-threaded executor creates a single worker thread to process tasks, \n    replacing it if it dies unexpectedly. \n    Tasks are guaranteed to be processed sequentially according to the order imposed by the task queue (FIFO, LIFO, priority order). 4\nScheduledThreadPool\n: A fixed-size thread pool that supports delayed and periodic task execution, similar to Timer.\n\n6\nInterrupt\nThread provides the \ninterrupt\n method for interrupting a thread and for querying whether a thread has been interrupted. \nEach thread has a boolean property that represents its interrupted status; interrupting a thread sets this status.\nInterruption is a \ncooperative\n mechanism. \nOne thread cannot force another to stop what it is doing and do something else; \nwhen thread A interrupts thread B, A is merely requesting that B stop what it is doing \nwhen it gets to a convenient stopping point\u2014if it feels like it.\nWhen your code calls a method that throws InterruptedException , then your\nmethod is a blocking method too, and must have a plan for responding to inter-\nruption. \nFor library code, there are basically two choices:\nPropagate the InterruptedException\n: This is often the most sensible policy if you can get away with it: \n    just propagate the InterruptedException to your caller. \n    This could involve not catching InterruptedException , or catching it and throwing it again after performing some brief activity-specific cleanup.\nRestore the interrupt\n: Sometimes you cannot throw InterruptedException , for instance when your code is part of a Runnable . \n    In these situations, you must catch InterruptedException and restore the interrupted status by calling interrupt on the current thread,\n     so that code higher up the call stack can see that an interrupt was issued.\n\n6\nPatterns\n\n\nQueue \n Deque\n\n\n\n\nQueue \n Deque\n\n\nA Deque is a double-ended queue that allows efficient insertion and removal from both the head and the tail. \nImplementations include ArrayDeque and LinkedBlockingDeque .\n\n\nJust as blocking queues lend themselves to the producer-consumer pattern,\ndeques lend themselves to a related pattern called work stealing. \n\n\nA producer-consumer design has one shared work queue for all consumers; \nin a work stealing design, every consumer has its own deque. \n\n\nIf a consumer exhausts the work in its own deque, it can steal work from the tail of someone else\u2019s deque. \n\n\nWork stealing can be more scalable than a traditional producer-consumer design \nbecause workers don\u2019t contend for a shared work queue; most of the time they access only their own deque, reducing contention. \n\n\nWhen a worker has to access another\u2019s queue, it does so from the tail rather than the head, further reducing contention.\n\n6\n\n\n\n\nMonitor pattern\n\n\nResources\n\n\n\n\nconcurrency-patterns-monitor-object\n\n\nWikipedia article on monitor pattern\n\n\ne-zest blog on monitor pattern java\n\n\n\n\nExamples\n\n\nConfinement\n\n\n\n\nPersonSet (below) illustrates how confinement and locking can work\n  together to make a class thread-safe even when its component state variables are not. \n  The state of PersonSet is managed by a HashSet , which is not thread-safe.\n\n\n\n\nBut because mySet is private and not allowed to escape, the HashSet is confined to the PersonSet. \n\n\nThe only code paths that can access mySet are addPerson and containsPerson , and each of these acquires the lock on the PersonSet. \n\n\nAll its state is guarded by its intrinsic lock, making PersonSet thread-safe. \n6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\npublic\n \nclass\n \nPersonSet\n \n{\n\n    \n@GuardedBy\n(\nthis\n)\n\n    \nprivate\n \nfinal\n \nSet\nPerson\n \nmySet\n \n=\n \nnew\n \nHashSet\nPerson\n();\n\n\n    \npublic\n \nsynchronized\n \nvoid\n \naddPerson\n(\nPerson\n \np\n)\n \n{\n\n        \nmySet\n.\nadd\n(\np\n);\n\n    \n}\n\n\n    \npublic\n \nsynchronized\n \nboolean\n \ncontainsPerson\n(\nPerson\n \np\n)\n \n{\n\n        \nreturn\n \nmySet\n.\ncontains\n(\np\n);\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nHTTP Call Counter\n\n\nUnsafe Counter\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\npublic\n \nclass\n \nUnsafeCounter\n \n{\n\n    \nprivate\n \nlong\n \ncount\n \n=\n \n0\n;\n\n\n    \npublic\n \nlong\n \ngetCount\n()\n \n{\n\n        \nreturn\n \ncount\n;\n\n    \n}\n\n\n    \npublic\n \nvoid\n \nservice\n()\n \n{\n\n        \n// do some work\n\n        \ntry\n \n{\n\n            \nint\n \npseudoRandom\n \n=\n \nnew\n \nRandom\n().\nnextInt\n(\n20\n);\n\n            \nThread\n.\nsleep\n(\npseudoRandom\n \n*\n \n100\n);\n\n            \n++\ncount\n;\n\n        \n}\n \ncatch\n \n(\nInterruptedException\n \ne\n)\n \n{\n\n            \ne\n.\nprintStackTrace\n();\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nSafe Counter\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\npublic\n \nclass\n \nSafeCounter\n \n{\n\n\n    \nprivate\n \nfinal\n \nAtomicLong\n \ncount\n \n=\n \nnew\n \nAtomicLong\n(\n0\n);\n\n\n    \npublic\n \nlong\n \ngetCount\n()\n \n{\n\n        \nreturn\n \ncount\n.\nget\n();\n\n    \n}\n\n\n    \npublic\n \nvoid\n \nservice\n()\n \n{\n\n        \ntry\n \n{\n\n            \nint\n \npseudoRandom\n \n=\n \nnew\n \nRandom\n().\nnextInt\n(\n20\n);\n\n            \nThread\n.\nsleep\n(\npseudoRandom\n \n*\n \n100\n);\n\n            \ncount\n.\nincrementAndGet\n();\n\n        \n}\n \ncatch\n \n(\nInterruptedException\n \ne\n)\n \n{\n\n            \ne\n.\nprintStackTrace\n();\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nCaller\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\npublic\n \nclass\n \nServer\n \n{\n\n    \npublic\n \nvoid\n \nstart\n(\nint\n \nport\n)\n \nthrows\n \nException\n \n{\n\n        \nHttpServer\n \nserver\n \n=\n \nHttpServer\n.\ncreate\n(\nnew\n \nInetSocketAddress\n(\nport\n),\n \n0\n);\n\n        \nUnsafeCounter\n \nunsafeCounter\n \n=\n \nnew\n \nUnsafeCounter\n();\n\n        \nSafeCounter\n \nsafeCounter\n \n=\n \nnew\n \nSafeCounter\n();\n\n        \nserver\n.\ncreateContext\n(\n/test\n,\n \nnew\n \nMyTestHandler\n(\nunsafeCounter\n,\n \nsafeCounter\n));\n\n        \nserver\n.\ncreateContext\n(\n/\n,\n \nnew\n \nMyHandler\n(\nunsafeCounter\n,\n \nsafeCounter\n));\n\n        \nExecutor\n \nexecutor\n \n=\n \nExecutors\n.\nnewFixedThreadPool\n(\n5\n);\n\n        \nserver\n.\nsetExecutor\n(\nexecutor\n);\n \n// creates a default executor\n\n        \nserver\n.\nstart\n();\n\n    \n}\n\n\n    \nstatic\n \nclass\n \nMyTestHandler\n \nimplements\n \nHttpHandler\n \n{\n\n        \nprivate\n \nUnsafeCounter\n \nunsafeCounter\n;\n\n        \nprivate\n \nSafeCounter\n \nsafeCounter\n;\n\n\n        \npublic\n \nMyTestHandler\n(\nUnsafeCounter\n \nunsafeCounter\n,\n \nSafeCounter\n \nsafeCounter\n)\n \n{\n\n            \nthis\n.\nunsafeCounter\n \n=\n \nunsafeCounter\n;\n\n            \nthis\n.\nsafeCounter\n \n=\n \nsafeCounter\n;\n\n        \n}\n\n\n        \n@Override\n\n        \npublic\n \nvoid\n \nhandle\n(\nHttpExchange\n \nt\n)\n \nthrows\n \nIOException\n \n{\n\n            \nsafeCounter\n.\nservice\n();\n\n            \nunsafeCounter\n.\nservice\n();\n\n            \nSystem\n.\nout\n.\nprintln\n(\nGot a request on /test, counts so far:\n+\n \nunsafeCounter\n.\ngetCount\n()\n \n+\n \n::\n \n+\n \nsafeCounter\n.\ngetCount\n());\n\n            \nString\n \nresponse\n \n=\n \nThis is the response\n;\n\n            \nt\n.\nsendResponseHeaders\n(\n200\n,\n \nresponse\n.\nlength\n());\n\n            \ntry\n \n(\nOutputStream\n \nos\n \n=\n \nt\n.\ngetResponseBody\n())\n \n{\n\n                \nos\n.\nwrite\n(\nresponse\n.\ngetBytes\n());\n\n            \n}\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nOutcome\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nStarting server on port \n8080\n\nServer started\nGot a request on /, counts so far:2::1\nGot a request on /, counts so far:6::2\nGot a request on /, counts so far:6::3\nGot a request on /, counts so far:6::4\nGot a request on /, counts so far:6::5\nGot a request on /, counts so far:6::6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLamport, L. (1977). \"Proving the Correctness of Multiprocess Programs\". IEEE Transactions on Software Engineering (2): 125\u2013143. doi:\n10.1109/TSE.1977.229904\n.\n\n\n\n\n\n\nLu\u00eds Rodrigues, Christian Cachin; Rachid Guerraoui (2010). Introduction to reliable and secure distributed programming (2. ed.). Berlin: Springer Berlin. pp. 22\u201324. \nISBN\n \n978-3-642-15259-7\n.\n\n\n\n\n\n\nBailis, P.; Ghodsi, A. (2013). \"Eventual Consistency Today: Limitations, Extensions, and Beyond\". Queue. 11 (3): 20. doi:\n10.1145/2460276.2462076\n.\n\n\n\n\n\n\nAlpern, B.; Schneider, F. B. (1987). \"Recognizing safety and liveness\". Distributed Computing. 2 (3): 117. doi:\n10.1007/BF01782772\n.\n\n\n\n\n\n\nLiveness article Wikipedia\n\n\n\n\n\n\nJava Concurrency in Practice / Brian Goetz, with Tim Peierls. . . [et al.] \nConcurrency in Practice\n\n\n\n\n\n\nBaeldung tutorial on CountDownLatch\n\n\n\n\n\n\nWikipedia article on Semaphore", 
            "title": "Java Concurrency"
        }, 
        {
            "location": "/java/concurrency/#java-concurrency", 
            "text": "", 
            "title": "Java Concurrency"
        }, 
        {
            "location": "/java/concurrency/#terminology", 
            "text": "Correctness Correctness means that a class  conforms to its specification . \nA good specification defines  invariants  constraining an object\u2019s state \nand  postconditions  describing the effects of its operations.  6 Thread Safe Class a class is thread-safe when it continues to behave correctly when accessed\nfrom multiple threads No set of operations performed sequentially or concurrently on instances of a thread-safe class \ncan cause an instance to be in an invalid state.  6 Mutex Every Java object can implicitly act as a lock for purposes of synchronization;\nthese built-in locks are called  intrinsic locks  or  monitor locks . \nThe lock is auto-matically acquired by the executing thread before entering a synchronized block\nand automatically released when control exits the synchronized block, whether\nby the normal control path or by throwing an exception out of the block. Intrinsic locks in Java act as  mutexes  (or  mutual exclusion locks ), which means\nthat at most one thread may own the lock. When thread A attempts to acquire a\nlock held by thread B, A must wait, or block, until B releases it. If B never releases\nthe lock, A waits forever.  6 Reentrant locks When a thread requests a lock that is already held by another thread, the requesting thread blocks. \nBut because intrinsic locks are  reentrant , if a thread tries to acquire a lock that it already holds, the request succeeds.  Reentrancy  means that locks are acquired on a per-thread rather than per-invocation basis.  Reentrancy  is implemented by associating with each lock an  acquisition count  and an owning  thread . \nWhen the count is zero, the lock is considered unheld.  When a thread acquires a previously unheld lock, the JVM records the owner and sets the acquisition count to one.  If that same thread acquires the lock again, the count is incremented, and when the owning thread exits the  synchronized block , \nthe count is decremented. When the count reaches zero, the lock is released.  6 Liveness In concurrent computing, liveness refers to a set of properties of concurrent systems, \nthat require a system to make progress despite the fact that its concurrently executing components (\"processes\") \n    may have to \"take turns\" in critical sections, parts of the program that cannot be simultaneously run by multiple processes. 1   Liveness guarantees are important properties in operating systems and distributed systems. 2 A liveness property cannot be violated in a finite execution of a distributed system because the \"good\" event might only theoretically occur at some time after execution ends. \nEventual consistency is an example of a liveness property. 3   All properties can be expressed as the intersection of safety and liveness properties. 4 Volatile fields When a field is declared  volatile , the  compiler  and  runtime  are put on notice that this variable is shared \nand that operations on it should not be reordered with other memory operations.  Volatile variables are not cached in registers or in caches where they are hidden from other processors, \nso a read of a volatile variable always returns the  most recent write  by  any  thread.  6 You can use volatile variables only when all the following criteria are met: Writes to the variable do not depend on its current value, or you can ensure that only a single thread ever updates the value; The variable does not participate in invariants with other state variables; Locking is not required for any other reason while the variable is being accessed Confinement Confined objects must not escape their intended scope. \nAn object may be confined to a class instance (such as a private class member), a lexical scope (such\nas a local variable), or a thread (such as an object that is passed from method to\nmethod within a thread, but not supposed to be shared across threads).  Objects don\u2019t escape on their own, of course\u2014they need help from the developer,\n who assists by publishing the object beyond its intended scope.  6 Latch Simply put, a CountDownLatch has a counter field, which you can decrement as we require. \nWe can then use it to block a calling thread until it\u2019s been counted down to zero. If we were doing some parallel processing, we could instantiate the CountDownLatch with \nthe same value for the counter as a number of threads we want to work across.  Then, we could just call countdown() after each thread finishes, \nguaranteeing that a dependent thread calling await() will block until the worker threads are finished. 7 Semaphore In computer science, a semaphore is a variable or abstract data type used to control access to a common resource by multiple processes \nin a concurrent system such as a multiprogramming operating system. A trivial semaphore is a plain variable that is changed (for example, incremented or decremented, or toggled) \ndepending on programmer-defined conditions. The variable is then used as a condition to control access to some system resource. A useful way to think of a semaphore as used in the real-world systems is as a record of how many units \nof a particular resource are available, coupled with operations to adjust that record safely \n(i.e. to avoid race conditions) as units are required or become free, and, if necessary, \nwait until a unit of the resource becomes available.  7 Java Thread pools There are several different types of Thread pools available. FixedThreadPool : A fixed-size thread pool creates threads as tasks are submitted, \n    up to the maximum pool size, and then attempts to keep the pool\n    size constant (adding new threads if a thread dies due to an unexpected Exception ). CachedThreadPool : A cached thread pool has more flexibility to reap idle threads when the current size of the pool \nexceeds the demand for processing, and to add new threads when demand increases, but places no bounds on the size of the pool. SingleThreadExecutor : A single-threaded executor creates a single worker thread to process tasks, \n    replacing it if it dies unexpectedly. \n    Tasks are guaranteed to be processed sequentially according to the order imposed by the task queue (FIFO, LIFO, priority order). 4 ScheduledThreadPool : A fixed-size thread pool that supports delayed and periodic task execution, similar to Timer. 6 Interrupt Thread provides the  interrupt  method for interrupting a thread and for querying whether a thread has been interrupted. \nEach thread has a boolean property that represents its interrupted status; interrupting a thread sets this status.\nInterruption is a  cooperative  mechanism.  One thread cannot force another to stop what it is doing and do something else; \nwhen thread A interrupts thread B, A is merely requesting that B stop what it is doing \nwhen it gets to a convenient stopping point\u2014if it feels like it. When your code calls a method that throws InterruptedException , then your\nmethod is a blocking method too, and must have a plan for responding to inter-\nruption.  For library code, there are basically two choices: Propagate the InterruptedException : This is often the most sensible policy if you can get away with it: \n    just propagate the InterruptedException to your caller. \n    This could involve not catching InterruptedException , or catching it and throwing it again after performing some brief activity-specific cleanup. Restore the interrupt : Sometimes you cannot throw InterruptedException , for instance when your code is part of a Runnable . \n    In these situations, you must catch InterruptedException and restore the interrupted status by calling interrupt on the current thread,\n     so that code higher up the call stack can see that an interrupt was issued. 6", 
            "title": "Terminology"
        }, 
        {
            "location": "/java/concurrency/#patterns", 
            "text": "", 
            "title": "Patterns"
        }, 
        {
            "location": "/java/concurrency/#queue-deque", 
            "text": "Queue   Deque  A Deque is a double-ended queue that allows efficient insertion and removal from both the head and the tail. \nImplementations include ArrayDeque and LinkedBlockingDeque .  Just as blocking queues lend themselves to the producer-consumer pattern,\ndeques lend themselves to a related pattern called work stealing.   A producer-consumer design has one shared work queue for all consumers; \nin a work stealing design, every consumer has its own deque.   If a consumer exhausts the work in its own deque, it can steal work from the tail of someone else\u2019s deque.   Work stealing can be more scalable than a traditional producer-consumer design \nbecause workers don\u2019t contend for a shared work queue; most of the time they access only their own deque, reducing contention.   When a worker has to access another\u2019s queue, it does so from the tail rather than the head, further reducing contention. 6", 
            "title": "Queue &amp; Deque"
        }, 
        {
            "location": "/java/concurrency/#monitor-pattern", 
            "text": "", 
            "title": "Monitor pattern"
        }, 
        {
            "location": "/java/concurrency/#resources", 
            "text": "concurrency-patterns-monitor-object  Wikipedia article on monitor pattern  e-zest blog on monitor pattern java", 
            "title": "Resources"
        }, 
        {
            "location": "/java/concurrency/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/java/concurrency/#confinement", 
            "text": "PersonSet (below) illustrates how confinement and locking can work\n  together to make a class thread-safe even when its component state variables are not. \n  The state of PersonSet is managed by a HashSet , which is not thread-safe.   But because mySet is private and not allowed to escape, the HashSet is confined to the PersonSet.   The only code paths that can access mySet are addPerson and containsPerson , and each of these acquires the lock on the PersonSet.   All its state is guarded by its intrinsic lock, making PersonSet thread-safe.  6   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 public   class   PersonSet   { \n     @GuardedBy ( this ) \n     private   final   Set Person   mySet   =   new   HashSet Person (); \n\n     public   synchronized   void   addPerson ( Person   p )   { \n         mySet . add ( p ); \n     } \n\n     public   synchronized   boolean   containsPerson ( Person   p )   { \n         return   mySet . contains ( p ); \n     }  }", 
            "title": "Confinement"
        }, 
        {
            "location": "/java/concurrency/#http-call-counter", 
            "text": "", 
            "title": "HTTP Call Counter"
        }, 
        {
            "location": "/java/concurrency/#unsafe-counter", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 public   class   UnsafeCounter   { \n     private   long   count   =   0 ; \n\n     public   long   getCount ()   { \n         return   count ; \n     } \n\n     public   void   service ()   { \n         // do some work \n         try   { \n             int   pseudoRandom   =   new   Random (). nextInt ( 20 ); \n             Thread . sleep ( pseudoRandom   *   100 ); \n             ++ count ; \n         }   catch   ( InterruptedException   e )   { \n             e . printStackTrace (); \n         } \n     }  }", 
            "title": "Unsafe Counter"
        }, 
        {
            "location": "/java/concurrency/#safe-counter", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 public   class   SafeCounter   { \n\n     private   final   AtomicLong   count   =   new   AtomicLong ( 0 ); \n\n     public   long   getCount ()   { \n         return   count . get (); \n     } \n\n     public   void   service ()   { \n         try   { \n             int   pseudoRandom   =   new   Random (). nextInt ( 20 ); \n             Thread . sleep ( pseudoRandom   *   100 ); \n             count . incrementAndGet (); \n         }   catch   ( InterruptedException   e )   { \n             e . printStackTrace (); \n         } \n     }  }", 
            "title": "Safe Counter"
        }, 
        {
            "location": "/java/concurrency/#caller", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34 public   class   Server   { \n     public   void   start ( int   port )   throws   Exception   { \n         HttpServer   server   =   HttpServer . create ( new   InetSocketAddress ( port ),   0 ); \n         UnsafeCounter   unsafeCounter   =   new   UnsafeCounter (); \n         SafeCounter   safeCounter   =   new   SafeCounter (); \n         server . createContext ( /test ,   new   MyTestHandler ( unsafeCounter ,   safeCounter )); \n         server . createContext ( / ,   new   MyHandler ( unsafeCounter ,   safeCounter )); \n         Executor   executor   =   Executors . newFixedThreadPool ( 5 ); \n         server . setExecutor ( executor );   // creates a default executor \n         server . start (); \n     } \n\n     static   class   MyTestHandler   implements   HttpHandler   { \n         private   UnsafeCounter   unsafeCounter ; \n         private   SafeCounter   safeCounter ; \n\n         public   MyTestHandler ( UnsafeCounter   unsafeCounter ,   SafeCounter   safeCounter )   { \n             this . unsafeCounter   =   unsafeCounter ; \n             this . safeCounter   =   safeCounter ; \n         } \n\n         @Override \n         public   void   handle ( HttpExchange   t )   throws   IOException   { \n             safeCounter . service (); \n             unsafeCounter . service (); \n             System . out . println ( Got a request on /test, counts so far: +   unsafeCounter . getCount ()   +   ::   +   safeCounter . getCount ()); \n             String   response   =   This is the response ; \n             t . sendResponseHeaders ( 200 ,   response . length ()); \n             try   ( OutputStream   os   =   t . getResponseBody ())   { \n                 os . write ( response . getBytes ()); \n             } \n         } \n     }  }", 
            "title": "Caller"
        }, 
        {
            "location": "/java/concurrency/#outcome", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8 Starting server on port  8080 \nServer started\nGot a request on /, counts so far:2::1\nGot a request on /, counts so far:6::2\nGot a request on /, counts so far:6::3\nGot a request on /, counts so far:6::4\nGot a request on /, counts so far:6::5\nGot a request on /, counts so far:6::6       Lamport, L. (1977). \"Proving the Correctness of Multiprocess Programs\". IEEE Transactions on Software Engineering (2): 125\u2013143. doi: 10.1109/TSE.1977.229904 .    Lu\u00eds Rodrigues, Christian Cachin; Rachid Guerraoui (2010). Introduction to reliable and secure distributed programming (2. ed.). Berlin: Springer Berlin. pp. 22\u201324.  ISBN   978-3-642-15259-7 .    Bailis, P.; Ghodsi, A. (2013). \"Eventual Consistency Today: Limitations, Extensions, and Beyond\". Queue. 11 (3): 20. doi: 10.1145/2460276.2462076 .    Alpern, B.; Schneider, F. B. (1987). \"Recognizing safety and liveness\". Distributed Computing. 2 (3): 117. doi: 10.1007/BF01782772 .    Liveness article Wikipedia    Java Concurrency in Practice / Brian Goetz, with Tim Peierls. . . [et al.]  Concurrency in Practice    Baeldung tutorial on CountDownLatch    Wikipedia article on Semaphore", 
            "title": "Outcome"
        }, 
        {
            "location": "/docker/multi-stage-builds/", 
            "text": "Docker Multi-Stage Builds", 
            "title": "Multi-Stage Builds"
        }, 
        {
            "location": "/docker/multi-stage-builds/#docker-multi-stage-builds", 
            "text": "", 
            "title": "Docker Multi-Stage Builds"
        }, 
        {
            "location": "/docker/graceful-shutdown/", 
            "text": "Docker \n Graceful shutdown\n\n\nThe case for graceful shutdown\n\n\n\n\nWe can speak about the graceful shutdown of our application, when all of the resources it used and all of the traffic and/or data processing what it handled are closed and released properly.\n  It means that no database connection remains open and no ongoing request fails because we stop our application. \n1\n\n\n\n\nI thank \nP\u00e9ter M\u00e1rton\n for the quote and giving a nice case for the graceful shutdown and docker.\nWhere his blog post goes into how to do this with/for Kubernetes, I will do this for Docker's Swarm (mode) orchestrator.\n\n\nSo, the case for graceful shutdown.\nAs the quote shows, what we mean with it, is that an application shuts down gracefully if it cleans up all its mess.\n\n\nAll things considered, I think most people would agree that cleaning up your mess - resources, connections or saying goodbye is preferred above just disappearing. \n\n\nShutting down nicely and leaving nothing behind will reduce the amount of potential (hard to debug) errors.\nIt also allows other applications or services to reliably know when you are there and when you're not there.\nNot every application will have such dependencies (to it), but in today's cluster environments with many moving parts you'll never know.\n\n\nSo I would recommend to always do a graceful shutdown if you're able.\nIn the light of Docker, that might be a bit different than you're use to.\n\n\nExec (form) vs Shell (form)\n\n\nThere are several ways to run a command in a \nDockerfile\n.\n\n\nThese are are:\n\n\n\n\nRUN\n: runs a command during the docker build phase\n\n\nCMD\n: runs a command when the container gets started\n\n\nENTRYPOINT\n: provides the location from where commands get run when the container starts\n\n\n\n\n\n\nNote\n\n\nYou need at least one ENTRYPOINT or CMD in a Dockerfile for it to be valid.\nThey can be used in collaboration but they can do similar things.\n\n\n\n\nAl these commands can be put in both a shell form and a exec form \n2\n. For more information on these commands you should check out \nJohn Zaccone's blog on Entrypoint vs CMD\n.\n\n\nIn summary, the shell form will run the command as a shell command and spawn a process via \n/bin/sh -c\n.\n\n\nWhereas the exec form will execute a child process that is still attached to PID1 \n4\n.\n\n\nThis means that if you run the examples below, you will notice that you cannot \nctrl+c\n out of the shell form, but you \ncan\n out of the exec form.\n\n\nExec form is the recommended form to use and is a requirement for graceful shutdown.\nBelow you'll find some further reading on the CMD and ENTRYPOINT commands \n5\n6\n.\n\n\nShell form example\n\n\n1\n2\nFROM\n alpine  \n\n\nENTRYPOINT\n ping www.google.com  # \nshell\n format  \n\n\n\n\n\n\n3\n\n\nExec form example\n\n\n1\n2\nFROM\n alpine  \n\n\nENTRYPOINT\n [\nping\n, \nwww.google.com\n]  # \nexec\n format  \n\n\n\n\n\n\n3\n\n\nPID1\n\n\nNow you run your commands nicely as PID1 and make sure it is your process that receives the \nSIGNALS\n.\n\n\nThen all is good for a while, but at one point you will run into the problems of either having \nzombie child processes\n or \nprocesses that aren't designed for running as PID1\n.\n\n\nThis means you will have to do something about this, and luckily there's already some people who have done this for you.\n\n\nThere's \ntini\n: a tiny initialization system designed for Docker.\n\n\nIf used with the \nexec\n form it will run as PID1 and will manage your process and its child processes for you.\n\n\nFor what it adds exactly and why it was introduced, you can rever to \nthis excellent explanation from its creator\n.\n\n\nIn fact, tini was so successful, that \ndocker included it in docker\n!\n\n\nWhile it works for \ndocker run\n and \ndocker compose\n it doesn't yet work for Docker Swarm stacks.\n\n\nExample docker run with init\n\n\n1\n2\n3\n4\n5\n6\ndocker run \n\\ \n\n    --rm \n\\\n\n    -ti \n\\\n\n\n    --init\n\\\n\n\n    --name dui-test\n\\\n\n     dui\n\n\n\n\n\n\nExample Dockerfile with tini\n\n\n1\n2\n3\n4\n5\n6\nFROM\n debian:stable-slim\n\n\nENV\n TINI_VERSION v0.16.1\n\n\nADD\n https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini\n\n\nRUN\n chmod +x /tini\n\nENTRYPOINT\n [\n/tini\n, \n-vv\n,\n-g\n, \n--\n, \n/usr/bin/dui\n]\n\nCOPY --from\n=\nbuild /usr/bin/dui-image/ /usr/bin/dui\n\n\n\n\nTini will be the entrypoint, executing our own user program (/usr/bin/dui).\n\n\nThis can also be achieved as follows:\n\n\n1\n2\nENTRYPOINT\n [\n/tini\n, \n-vv\n,\n-g\n, \n--\n]\n\n\nCMD\n[\n/usr/bin/dui\n]\n\n\n\n\n\n\n\n\n\n-vv\n: debug log level 2 (-v =1, -vv=2, -vvv=3)\n\n\n-g\n: kill the entire group of processes when signal is received\n\n\n--\n: end of tini and start of your command \n\n\n\n\nSignals\n\n\nNow that we can correctly respond to signals we need to take care of which signals to listen to.\n\n\n\n\nThere are essentially two commands: \ndocker stop\n and \ndocker kill\n that can be used to stop it. Behind the scenes, \ndocker stop\n stops a running container by sending it SIGINT signal, let the main process process it, and after a grace period uses SIGKILL to terminate the application. \n7\n\n\n\n\nYou can test it with the following image, which uses Java's shutdown hook to shutdown gracefully: this only happens when it is stopped.\nRun the command below and press \nctrl+c\n, and you will see that the application shuts down gracefully.\n\n\n1\ndocker run --rm -ti --name \ntest\n caladreas/buming\n\n\n\n\nMake sure you have two terminal windows open. In terminal one, run this command:\n\n\n1\n2\ndocker run -d --name \ntest\n caladreas/buming\ndocker logs -f \ntest\n\n\n\n\n\n\n\nIn terminal two, run this command:\n\n\n1\ndocker rm -f \ntest\n\n\n\n\n\n\n\nAnd now you will not see the graceful shutdown log, as the JVM was killed without being allowed to call shutdown hooks.\n\n\nDocker allows you to specify which signal it should send via \n--stop-signal\n in the run command or \nstop_signal:\n in a compose file.\n\n\n1\n2\n3\n4\n5\n6\n7\nversion\n:\n \n3.5\n\n\n\nservices\n:\n\n  \nyi\n:\n\n    \nimage\n:\n \ndui\n\n    \nbuild\n:\n \n.\n\n    \nstop_signal\n:\n \nSIGINT\n\n\n\n\n\n\n\nExamples\n\n\nHow to actually listen to the signals and determine which one to use will depend on your programming language.\n\n\nThere's three examples I have worked out, one for Go (lang) and two for Java: pojo and Spring Boot.\n\n\nGo\n\n\nDockerfile\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n# build stage\n\nFROM golang:latest AS build-env\nRUN go get -v github.com/docker/docker/client/...\nRUN go get -v github.com/docker/docker/api/...\nADD src/ \n$GOPATH\n/flow-proxy-service-lister\nWORKDIR \n$GOPATH\n/flow-proxy-service-lister\nRUN go build -o main -tags netgo main.go\n\n\n# final stage\n\nFROM alpine\nENTRYPOINT \n[\n/app/main\n]\n\nCOPY --from\n=\nbuild-env /go/flow-proxy-service-lister/main /app/\nRUN chmod +x /app/main\n\n\n\n\n\n\nGo code for graceful shutdown\n\n\nThe following is a way for Go to shutdown a http server when receiving a termination signal.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\nfunc\n \nmain\n()\n \n{\n\n    \nc\n \n:=\n \nmake\n(\nchan\n \nbool\n)\n \n// make channel for main \n--\n webserver communication\n\n    \ngo\n \nwebserver\n.\nStart\n(\n7777\n,\n \nwebserverData\n,\n \nc\n)\n \n// ignore the missing data\n\n\n    \nstop\n \n:=\n \nmake\n(\nchan\n \nos\n.\nSignal\n,\n \n1\n)\n \n// make a channel that listens to is signals\n\n    \nsignal\n.\nNotify\n(\nstop\n,\n \nsyscall\n.\nSIGINT\n,\n \nsyscall\n.\nSIGTERM\n)\n \n// we listen to some specific syscall signals\n\n\n    \nfor\n \ni\n \n:=\n \n1\n;\n \n;\n \ni\n++\n \n{\n \n// this is still infinite\n\n        \nt\n \n:=\n \ntime\n.\nNewTicker\n(\ntime\n.\nSecond\n \n*\n \n30\n)\n \n// set a timer for the polling\n\n        \nselect\n \n{\n\n        \ncase\n \n-\nstop\n:\n \n// this means we got a os signal on our channel\n\n            \nbreak\n \n// so we can stop\n\n        \ncase\n \n-\nt\n.\nC\n:\n\n            \n// our timer expired, refresh our data\n\n            \ncontinue\n \n// and continue with the loop\n\n        \n}\n\n        \nbreak\n\n    \n}\n\n    \nfmt\n.\nPrintln\n(\nShutting down webserver\n)\n \n// if we got here, we have to inform the webserver to close shop\n\n    \nc\n \n-\n \ntrue\n \n// we do this by sending a message on the channel\n\n    \nif\n \nb\n \n:=\n \n-\nc\n;\n \nb\n \n{\n \n// when we get true back, that means the webserver is doing with a graceful shutdown\n\n        \nfmt\n.\nPrintln\n(\nWebserver shut down\n)\n \n// webserver is done\n\n    \n}\n\n    \nfmt\n.\nPrintln\n(\nShut down app\n)\n \n// we can close shop ourselves now\n\n\n}\n\n\n\n\n\n\n\nJava plain (Docker Swarm)\n\n\nThis application is a Java 9 modular application, which can be found on github, \ngithub.com/joostvdg\n.\n\n\nDockerfile\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\nFROM\n openjdk:9-jdk AS build\n\n\n\nRUN\n mkdir -p /usr/src/mods/jars\n\nRUN\n mkdir -p /usr/src/mods/compiled\n\nCOPY . /usr/src\n\nWORKDIR\n /usr/src\n\n\n\nRUN\n javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src \n$(\nfind src -name \n*.java\n)\n\n\nRUN\n jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version \n1\n.0 -C /usr/src/mods/compiled/joostvdg.dui.logging .\n\nRUN\n jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version \n1\n.0 -C /usr/src/mods/compiled/joostvdg.dui.api .\n\nRUN\n jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version \n1\n.0 -C /usr/src/mods/compiled/joostvdg.dui.client .\n\nRUN\n jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version \n1\n.0  -e com.github.joostvdg.dui.server.cli.DockerApp\n\\\n\n    -C /usr/src/mods/compiled/joostvdg.dui.server .\n\n\nRUN\n rm -rf /usr/bin/dui-image\n\nRUN\n jlink --module-path /usr/src/mods/jars/:/\n${\nJAVA_HOME\n}\n/jmods \n\\\n\n    --add-modules joostvdg.dui.api \n\\\n\n    --add-modules joostvdg.dui.logging \n\\\n\n    --add-modules joostvdg.dui.server \n\\\n\n    --add-modules joostvdg.dui.client \n\\\n\n    --launcher \ndui\n=\njoostvdg.dui.server \n\\\n\n    --output /usr/bin/dui-image\n\n\nRUN\n ls -lath /usr/bin/dui-image\n\nRUN\n ls -lath /usr/bin/dui-image\n\nRUN\n /usr/bin/dui-image/bin/java --list-modules\n\n\nFROM\n debian:stable-slim\n\nLABEL \nauthors\n=\nJoost van der Griendt \njoostvdg@gmail.com\n\nLABEL \nversion\n=\n0.1.0\n\nLABEL \ndescription\n=\nDocker image for playing with java applications in a concurrent, parallel and distributed manor.\n\n\n# Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/\n\n\nENV\n TINI_VERSION v0.16.1\n\n\nADD\n https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini\n\n\nRUN\n chmod +x /tini\n\nENTRYPOINT\n [\n/tini\n, \n-vv\n,\n-g\n, \n--\n, \n/usr/bin/dui/bin/dui\n]\n\n\nENV\n DATE_CHANGED=\n20180120-1525\n\nCOPY --from\n=\nbuild /usr/bin/dui-image/ /usr/bin/dui\n\nRUN\n /usr/bin/dui/bin/java --list-modules\n\n\n\n\n\n\nHandling code\n\n\nThe code first initializes the server which and when started, creates the \nShutdown Hook\n.\n\n\nJava handles certain signals in specific ways, as can be found \nin this table\n for linux.\nFor more information, you can read the \ndocs from Oracle\n. \n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\npublic\n \nclass\n \nDockerApp\n \n{\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \n{\n\n        \nServiceLoader\nLogger\n \nloggers\n \n=\n \nServiceLoader\n.\nload\n(\nLogger\n.\nclass\n);\n\n                \nLogger\n \nlogger\n \n=\n \nloggers\n.\nfindFirst\n().\nisPresent\n()\n \n?\n \nloggers\n.\nfindFirst\n().\nget\n()\n \n:\n \nnull\n;\n\n                \nif\n \n(\nlogger\n \n==\n \nnull\n)\n \n{\n\n                    \nSystem\n.\nerr\n.\nprintln\n(\nDid not find any loggers, quiting\n);\n\n                    \nSystem\n.\nexit\n(\n1\n);\n\n                \n}\n\n                \nlogger\n.\nstart\n(\nLogLevel\n.\nINFO\n);\n\n\n                \nint\n \npseudoRandom\n \n=\n \nnew\n \nRandom\n().\nnextInt\n(\nProtocolConstants\n.\nPOTENTIAL_SERVER_NAMES\n.\nlength\n \n-\n1\n);\n\n                \nString\n \nserverName\n \n=\n \nProtocolConstants\n.\nPOTENTIAL_SERVER_NAMES\n[\npseudoRandom\n];\n\n                \nint\n \nlistenPort\n \n=\n \nProtocolConstants\n.\nEXTERNAL_COMMUNICATION_PORT_A\n;\n\n                \nString\n \nmulticastGroup\n \n=\n \nProtocolConstants\n.\nMULTICAST_GROUP\n;\n\n\n                \nDuiServer\n \ndistributedServer\n \n=\n \nDuiServerFactory\n.\nnewDistributedServer\n(\nlistenPort\n,\nmulticastGroup\n \n,\n \nserverName\n,\n \nlogger\n);\n\n\n                \ndistributedServer\n.\nlogMembership\n();\n\n\n                \nExecutorService\n \nexecutorService\n \n=\n \nExecutors\n.\nnewFixedThreadPool\n(\n1\n);\n\n                \nexecutorService\n.\nsubmit\n(\ndistributedServer\n::\nstartServer\n);\n\n\n                \nlong\n \nthreadId\n \n=\n \nThread\n.\ncurrentThread\n().\ngetId\n();\n\n\n                \nRuntime\n.\ngetRuntime\n().\naddShutdownHook\n(\nnew\n \nThread\n(()\n \n-\n \n{\n\n                    \nSystem\n.\nout\n.\nprintln\n(\nShutdown hook called!\n);\n\n                    \nlogger\n.\nlog\n(\nLogLevel\n.\nWARN\n,\n \nApp\n,\n \nShotdownHook\n,\n \nthreadId\n,\n \nShutting down at request of Docker\n);\n\n                    \ndistributedServer\n.\nstopServer\n();\n\n                    \ndistributedServer\n.\ncloseServer\n();\n\n                    \nexecutorService\n.\nshutdown\n();\n\n                    \ntry\n \n{\n\n                        \nThread\n.\nsleep\n(\n100\n);\n\n                        \nexecutorService\n.\nshutdownNow\n();\n\n                        \nlogger\n.\nstop\n();\n\n                    \n}\n \ncatch\n \n(\nInterruptedException\n \ne\n)\n \n{\n\n                        \ne\n.\nprintStackTrace\n();\n\n                    \n}\n\n                \n}));\n        \n    \n}\n\n\n}\n\n\n\n\n\n\n\nJava Plain (Kubernetes)\n\n\nSo far we've utilized the utilities from Docker itself in conjunction with it's native Docker Swarm orchestrator.\n\n\nUnfortunately, when it comes to popularity \nKubernetes beats Swarm hands down\n.\n\n\nSo this isn't complete if it doesn't also do graceful shutdown in Kubernetes. \n\n\nIn Dockerfile\n\n\nOur original file had to be changed, as Debian's Slim image doesn't actually contain the kill package.\nAnd we need a kill package, as we cannot instruct Kubernetes to issue a specific SIGNAL.\nInstead, we can issue a \nPreStop exec command\n, which we can utilise to execute a \nkillall\n java \n-INT\n.\n\n\nThe command will be specified in the Kubernetes deployment definition below.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\nFROM\n openjdk:9-jdk AS build\n\n\n\nRUN\n mkdir -p /usr/src/mods/jars\n\nRUN\n mkdir -p /usr/src/mods/compiled\n\nCOPY . /usr/src\n\nWORKDIR\n /usr/src\n\n\n\nRUN\n javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src \n$(\nfind src -name \n*.java\n)\n\n\nRUN\n jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version \n1\n.0 -C /usr/src/mods/compiled/joostvdg.dui.logging .\n\nRUN\n jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version \n1\n.0 -C /usr/src/mods/compiled/joostvdg.dui.api .\n\nRUN\n jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version \n1\n.0 -C /usr/src/mods/compiled/joostvdg.dui.client .\n\nRUN\n jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version \n1\n.0  -e com.github.joostvdg.dui.server.cli.DockerApp\n\\\n\n    -C /usr/src/mods/compiled/joostvdg.dui.server .\n\n\nRUN\n rm -rf /usr/bin/dui-image\n\nRUN\n jlink --module-path /usr/src/mods/jars/:/\n${\nJAVA_HOME\n}\n/jmods \n\\\n\n    --add-modules joostvdg.dui.api \n\\\n\n    --add-modules joostvdg.dui.logging \n\\\n\n    --add-modules joostvdg.dui.server \n\\\n\n    --add-modules joostvdg.dui.client \n\\\n\n    --launcher \ndui\n=\njoostvdg.dui.server \n\\\n\n    --output /usr/bin/dui-image\n\n\nRUN\n ls -lath /usr/bin/dui-image\n\nRUN\n ls -lath /usr/bin/dui-image\n\nRUN\n /usr/bin/dui-image/bin/java --list-modules\n\n\nFROM\n debian:stable-slim\n\nLABEL \nauthors\n=\nJoost van der Griendt \njoostvdg@gmail.com\n\nLABEL \nversion\n=\n0.1.0\n\nLABEL \ndescription\n=\nDocker image for playing with java applications in a concurrent, parallel and distributed manor.\n\n\n# Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/\n\n\nENV\n TINI_VERSION v0.16.1\n\n\nADD\n https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini\n\n\nRUN\n chmod +x /tini\n\nENTRYPOINT\n [\n/tini\n, \n-vv\n,\n-g\n, \n--\n, \n/usr/bin/dui/bin/dui\n]\n\n\nENV\n DATE_CHANGED=\n20180120-1525\n\n\nRUN\n apt-get update \n apt-get install --no-install-recommends -y \npsmisc\n=\n22\n.* \n rm -rf /var/lib/apt/lists/*\nCOPY --from\n=\nbuild /usr/bin/dui-image/ /usr/bin/dui\n\nRUN\n /usr/bin/dui/bin/java --list-modules\n\n\n\n\n\n\nKubernetes Deployment\n\n\nSo here we have the image's K8s \nDeployment\n descriptor.\n\n\nIncluding the Pod's \nlifecycle\n \npreStop\n with a exec style command. You should know by now \nwhy we prefer that\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\napiVersion\n:\n \nextensions/v1beta1\n\n\nkind\n:\n \nDeployment\n\n\nmetadata\n:\n\n  \nname\n:\n \ndui-deployment\n\n  \nnamespace\n:\n \ndefault\n\n  \nlabels\n:\n\n    \nk8s-app\n:\n \ndui\n\n\nspec\n:\n\n  \nreplicas\n:\n \n3\n\n  \ntemplate\n:\n\n    \nmetadata\n:\n\n      \nlabels\n:\n\n        \nk8s-app\n:\n \ndui\n\n    \nspec\n:\n\n      \ncontainers\n:\n\n        \n-\n \nname\n:\n \nmaster\n\n          \nimage\n:\n \ncaladreas/buming\n\n          \nports\n:\n\n            \n-\n \nname\n:\n \nhttp\n\n              \ncontainerPort\n:\n \n7777\n\n          \nlifecycle\n:\n\n            \npreStop\n:\n\n              \nexec\n:\n\n                \ncommand\n:\n \n[\nkillall\n,\n \njava\n \n,\n \n-INT\n]\n\n      \nterminationGracePeriodSeconds\n:\n \n60\n\n\n\n\n\n\n\nJava Spring Boot (1.x)\n\n\nThis example is for Spring Boot 1.x, in time we will have an example for 2.x.\n\n\nThis example is for the scenario of a Fat Jar with Tomcat as container \n8\n.\n\n\nExecute example\n\n\n1\ndocker-compose build\n\n\n\n\n\n\nExecute the following command:\n\n\n1\ndocker run --rm -ti --name \ntest\n spring-boot-graceful\n\n\n\n\n\n\nExit the application/container via \nctrl+c\n and you should see the application shutting down gracefully.\n\n\n1\n2\n3\n2018\n-01-30 \n13\n:35:46.327  INFO \n7\n --- \n[\n       Thread-3\n]\n ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@6e5e91e4: startup date \n[\nTue Jan \n30\n \n13\n:35:42 GMT \n2018\n]\n;\n root of context hierarchy\n\n2018\n-01-30 \n13\n:35:46.405  INFO \n7\n --- \n[\n       Thread-3\n]\n BootGracefulApplication\n$GracefulShutdown\n : Tomcat was shutdown gracefully within the allotted time.\n\n2018\n-01-30 \n13\n:35:46.408  INFO \n7\n --- \n[\n       Thread-3\n]\n o.s.j.e.a.AnnotationMBeanExporter        : Unregistering JMX-exposed beans on shutdown\n\n\n\n\n\n\nDockerfile\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nFROM\n maven:3-jdk-8 AS build\n\n\nENV\n MAVEN_OPTS=-Dmaven.repo.local=/usr/share/maven/repository\n\n\nENV\n WORKDIR=/usr/src/graceful\n\n\nRUN\n mkdir \n$WORKDIR\n\n\nWORKDIR\n $WORKDIR\n\nCOPY pom.xml \n$WORKDIR\n\n\nRUN\n mvn -B -e org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline\nCOPY . \n$WORKSPACE\n\n\nRUN\n mvn -B -e clean verify\n\n\nFROM\n anapsix/alpine-java:8_jdk_unlimited\n\nLABEL \nauthors\n=\nJoost van der Griendt \njoostvdg@gmail.com\n\n\nENV\n TINI_VERSION v0.16.1\n\n\nADD\n https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini\n\n\nRUN\n chmod +x /tini\n\nENTRYPOINT\n [\n/tini\n, \n-vv\n,\n-g\n, \n--\n]\n\n\nENV\n DATE_CHANGED=\n20180120-1525\n\nCOPY --from\n=\nbuild /usr/src/graceful/target/spring-boot-graceful.jar /app.jar\n\nCMD\n [\njava\n, \n-Xms256M\n,\n-Xmx480M\n, \n-Djava.security.egd=file:/dev/./urandom\n, \n-jar\n, \n/app.jar\n]\n\n\n\n\n\n\n\nDocker compose file\n\n\n1\n2\n3\n4\n5\n6\n7\nversion\n:\n \n3.5\n\n\n\nservices\n:\n\n  \nweb\n:\n\n    \nimage\n:\n \nspring-boot-graceful\n\n    \nbuild\n:\n \n.\n\n    \nstop_signal\n:\n \nSIGINT\n\n\n\n\n\n\n\nJava handling code\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\npackage\n \ncom.github.joostvdg.demo.springbootgraceful\n;\n\n\n\nimport\n \norg.springframework.boot.SpringApplication\n;\n\n\nimport\n \norg.springframework.boot.autoconfigure.SpringBootApplication\n;\n\n\n\nimport\n \norg.apache.catalina.connector.Connector\n;\n\n\nimport\n \norg.apache.tomcat.util.threads.ThreadPoolExecutor\n;\n\n\nimport\n \norg.slf4j.Logger\n;\n\n\nimport\n \norg.slf4j.LoggerFactory\n;\n\n\n\nimport\n \norg.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer\n;\n\n\nimport\n \norg.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer\n;\n\n\nimport\n \norg.springframework.boot.context.embedded.tomcat.TomcatConnectorCustomizer\n;\n\n\nimport\n \norg.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory\n;\n\n\nimport\n \norg.springframework.context.ApplicationListener\n;\n\n\nimport\n \norg.springframework.context.annotation.Bean\n;\n\n\nimport\n \norg.springframework.context.event.ContextClosedEvent\n;\n\n\n\nimport\n \njava.util.concurrent.Executor\n;\n\n\nimport\n \njava.util.concurrent.TimeUnit\n;\n\n\n\n@SpringBootApplication\n\n\npublic\n \nclass\n \nSpringBootGracefulApplication\n \n{\n\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \n{\n\n        \nSpringApplication\n.\nrun\n(\nSpringBootGracefulApplication\n.\nclass\n,\n \nargs\n);\n\n    \n}\n\n\n    \n@Bean\n\n    \npublic\n \nGracefulShutdown\n \ngracefulShutdown\n()\n \n{\n\n        \nreturn\n \nnew\n \nGracefulShutdown\n();\n\n    \n}\n\n\n    \n@Bean\n\n    \npublic\n \nEmbeddedServletContainerCustomizer\n \ntomcatCustomizer\n()\n \n{\n\n        \nreturn\n \nnew\n \nEmbeddedServletContainerCustomizer\n()\n \n{\n\n\n            \n@Override\n\n            \npublic\n \nvoid\n \ncustomize\n(\nConfigurableEmbeddedServletContainer\n \ncontainer\n)\n \n{\n\n                \nif\n \n(\ncontainer\n \ninstanceof\n \nTomcatEmbeddedServletContainerFactory\n)\n \n{\n\n                    \n((\nTomcatEmbeddedServletContainerFactory\n)\n \ncontainer\n)\n\n                            \n.\naddConnectorCustomizers\n(\ngracefulShutdown\n());\n\n                \n}\n\n\n            \n}\n\n        \n};\n\n    \n}\n\n\n    \nprivate\n \nstatic\n \nclass\n \nGracefulShutdown\n \nimplements\n \nTomcatConnectorCustomizer\n,\n\n            \nApplicationListener\nContextClosedEvent\n \n{\n\n\n        \nprivate\n \nstatic\n \nfinal\n \nLogger\n \nlog\n \n=\n \nLoggerFactory\n.\ngetLogger\n(\nGracefulShutdown\n.\nclass\n);\n\n\n        \nprivate\n \nvolatile\n \nConnector\n \nconnector\n;\n\n\n        \n@Override\n\n        \npublic\n \nvoid\n \ncustomize\n(\nConnector\n \nconnector\n)\n \n{\n\n            \nthis\n.\nconnector\n \n=\n \nconnector\n;\n\n        \n}\n\n\n        \n@Override\n\n        \npublic\n \nvoid\n \nonApplicationEvent\n(\nContextClosedEvent\n \nevent\n)\n \n{\n\n            \nthis\n.\nconnector\n.\npause\n();\n\n            \nExecutor\n \nexecutor\n \n=\n \nthis\n.\nconnector\n.\ngetProtocolHandler\n().\ngetExecutor\n();\n\n            \nif\n \n(\nexecutor\n \ninstanceof\n \nThreadPoolExecutor\n)\n \n{\n\n                \ntry\n \n{\n\n                    \nThreadPoolExecutor\n \nthreadPoolExecutor\n \n=\n \n(\nThreadPoolExecutor\n)\n \nexecutor\n;\n\n                    \nthreadPoolExecutor\n.\nshutdown\n();\n\n                    \nif\n \n(!\nthreadPoolExecutor\n.\nawaitTermination\n(\n30\n,\n \nTimeUnit\n.\nSECONDS\n))\n \n{\n\n                        \nlog\n.\nwarn\n(\nTomcat thread pool did not shut down gracefully within \n\n                                \n+\n \n30 seconds. Proceeding with forceful shutdown\n);\n\n                    \n}\n \nelse\n \n{\n\n                        \nlog\n.\ninfo\n(\nTomcat was shutdown gracefully within the allotted time.\n);\n\n                    \n}\n\n                \n}\n\n                \ncatch\n \n(\nInterruptedException\n \nex\n)\n \n{\n\n                    \nThread\n.\ncurrentThread\n().\ninterrupt\n();\n\n                \n}\n\n            \n}\n\n        \n}\n\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nExample with Docker Swarm\n\n\nFor now there's only an example with \ndocker swarm\n, in time there will also be a \nKubernetes\n example.\n\n\nNow that you can create Java applications packaged neatly in Docker images that support graceful shutdown, it would be nice to utilize.\n\n\nA good scenario would be a microservices architecture where services can come and go, but are registered in a \nservice registry such as Eureka\n.\n\n\nOr a membership based protocol where members interact with each other and perhaps shard data.\n\n\nIn these cases, of course the interactions are designed to be fault tolerant and discover faulty nodes on their own.\nBut wouldn't it be better that if you knew you're going to quit, you inform the rest?\n\n\nWe can reuse the \ncaladreas/buming\n image and make it a docker swarm stack and run the service on every node.\nThis way, we can easily see members coming and going and reduce the time to detect failure by notifying our peers of our impeding end.  \n\n\nDocker swarm cluster\n\n\nSetting up a docker swarm cluster is easy, but has some requirements:\n\n\n\n\nvirtual box 4.x+\n\n\ndocker-machine 1.12+\n\n\ndocker 17.06+\n\n\n\n\n\n\nWarn\n\n\nMake sure this is the first and only virtualbox docker-machine VM being created/running, so that the ip range starts with 192.168.99.100\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\ndocker-machine create --driver virtualbox dui-1\ndocker-machine create --driver virtualbox dui-2\ndocker-machine create --driver virtualbox dui-3\n\n\neval\n \n$(\ndocker-machine env dui-1\n)\n\n\nIP\n=\n192\n.168.99.100\ndocker swarm init --advertise-addr \n$IP\n\n\nTOKEN\n=\n$(\ndocker swarm join-token -q worker\n)\n\n\n\neval\n \n$(\ndocker-machine env dui-2\n)\n\ndocker swarm join --token \n${\nTOKEN\n}\n \n${\nIP\n}\n:2377\n\n\neval\n \n$(\ndocker-machine env dui-3\n)\n\ndocker swarm join --token \n${\nTOKEN\n}\n \n${\nIP\n}\n:2377\n\n\neval\n \n$(\ndocker-machine env dui-1\n)\n\ndocker node ls\n\n\n\n\n\n\nDocker swarm network and multicast\n\n\nUnfortunately, docker swarm's swarm mode network \noverlay\n does not support multicast \n9\n10\n.\n\n\nWhy is this a problem? Well, the application I use to test the graceful shutdown requires this, sorry.\n\n\nLuckily there is a very easy solution for this, its by using \nWeavenet\n's docker network plugin.\n\n\nDon't want to know about it or how you install it? Don't worry, just execute the script below.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n#!/usr/bin/env bash\n\n\necho\n \n=\n Prepare dui-2\n\n\neval\n \n$(\ndocker-machine env dui-2\n)\n\ndocker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions\ndocker plugin disable weaveworks/net-plugin:2.1.3\ndocker plugin \nset\n weaveworks/net-plugin:2.1.3 \nWEAVE_MULTICAST\n=\n1\n\ndocker plugin \nenable\n weaveworks/net-plugin:2.1.3\n\n\necho\n \n=\n Prepare dui-3\n\n\neval\n \n$(\ndocker-machine env dui-3\n)\n\ndocker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions\ndocker plugin disable weaveworks/net-plugin:2.1.3\ndocker plugin \nset\n weaveworks/net-plugin:2.1.3 \nWEAVE_MULTICAST\n=\n1\n\ndocker plugin \nenable\n weaveworks/net-plugin:2.1.3\n\n\necho\n \n=\n Prepare dui-1\n\n\neval\n \n$(\ndocker-machine env dui-1\n)\n\ndocker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions\ndocker plugin disable weaveworks/net-plugin:2.1.3\ndocker plugin \nset\n weaveworks/net-plugin:2.1.3 \nWEAVE_MULTICAST\n=\n1\n\ndocker plugin \nenable\n weaveworks/net-plugin:2.1.3\ndocker network create --driver\n=\nweaveworks/net-plugin:2.1.3 --opt works.weave.multicast\n=\ntrue\n --attachable dui\n\n\n\n\n\n\nDocker stack\n\n\nNow to create a service that runs on every node it is the easiest to create a \ndocker stack\n.\n\n\nCompose file (docker-stack.yml)\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nversion\n:\n \n3.5\n\n\n\nservices\n:\n\n  \ndui\n:\n\n    \nimage\n:\n \ncaladreas/buming\n\n    \nbuild\n:\n \n.\n\n    \nstop_signal\n:\n \nSIGINT\n\n    \nnetworks\n:\n\n      \n-\n \ndui\n\n    \ndeploy\n:\n\n      \nmode\n:\n \nglobal\n\n\nnetworks\n:\n\n  \ndui\n:\n\n    \nexternal\n:\n \ntrue\n\n\n\n\n\n\n\nCreate stack\n\n\n1\ndocker stack deploy --compose-file docker-stack.yml buming\n\n\n\n\n\n\nExecute example\n\n\nNow that we have a docker swarm cluster and a stack - which has a service running on every node - we can showcase the power of graceful shutdown in a cluster of dependent services.\n\n\nConfirm the service is running correctly on every node, first lets check our nodes.\n\n\n1\n2\neval\n \n$(\ndocker-machine env dui-1\n)\n\ndocker node ls\n\n\n\n\nWhich should look like this:\n\n\n1\n2\n3\n4\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS\nf21ilm4thxegn5xbentmss5ur *   dui-1               Ready               Active              Leader\ny7475bo5uplt2b58d050b4wfd     dui-2               Ready               Active              \n6ssxola6y1i6h9p8256pi7bfv     dui-3               Ready               Active                            \n\n\n\n\n\n\nThen check the service.\n\n\n1\ndocker service ps buming_dui\n\n\n\n\n\n\nWhich should look like this.\n\n\n1\n2\n3\n4\nID                  NAME                                   IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\n3mrpr0jg31x1        buming_dui.6ssxola6y1i6h9p8256pi7bfv   dui:latest          dui-3               Running             Running \n17\n seconds ago                       \npfubtiy4j7vo        buming_dui.f21ilm4thxegn5xbentmss5ur   dui:latest          dui-1               Running             Running \n17\n seconds ago                       \nf4gjnmhoe3y4        buming_dui.y7475bo5uplt2b58d050b4wfd   dui:latest          dui-2               Running             Running \n17\n seconds ago                       \n\n\n\n\n\n\nNow open a second terminal window.\nIn window one, follow the service logs:\n\n\n1\n2\neval\n \n$(\ndocker-machine env dui-1\n)\n\ndocker service logs -f buming_dui\n\n\n\n\n\n\nIn window two, go to a different node and stop the container.\n\n\n1\n2\n3\neval\n \n$(\ndocker-machine env dui-2\n)\n\ndocker ps\ndocker stop buming_dui.y7475bo5uplt2b58d050b4wfd.pnoui2x6elrz0tvkjz51njz94\n\n\n\n\n\n\nIn this case, you will see the other nodes receiving a leave notice and then the node stopping.\n\n\n1\n2\n3\n4\n5\n6\nbuming_dui.0.ryd8szexxku3@dui-3    \n|\n \n[\nServer-John D. Carmack\n]\n           \n[\nWARN\n]\n  \n[\n14\n:19:02.604011\n]\n   \n[\n16\n]\n    \n[\nMain\n]\n              Received membership leave notice from MessageOrigin\n{\nhost\n=\n83918f6ad817\n, \nip\n=\n10.0.0.7\n, \nname\n=\nKen Thompson\n}\n\nbuming_dui.0.so5m14sz8ksh@dui-1    \n|\n \n[\nServer-Alan Kay\n]\n                  \n[\nWARN\n]\n  \n[\n14\n:19:02.602082\n]\n   \n[\n16\n]\n    \n[\nMain\n]\n              Received membership leave notice from MessageOrigin\n{\nhost\n=\n83918f6ad817\n, \nip\n=\n10.0.0.7\n, \nname\n=\nKen Thompson\n}\n\nbuming_dui.0.pnoui2x6elrz@dui-2    \n|\n Shutdown hook called!\nbuming_dui.0.pnoui2x6elrz@dui-2    \n|\n \n[\nApp\n]\n                              \n[\nWARN\n]\n  \n[\n14\n:19:02.598759\n]\n   \n[\n1\n]\n \n[\nShotdownHook\n]\n      Shutting down at request of Docker\nbuming_dui.0.pnoui2x6elrz@dui-2    \n|\n \n[\nServer-Ken Thompson\n]\n              \n[\nINFO\n]\n  \n[\n14\n:19:02.598858\n]\n   \n[\n12\n]\n    \n[\nMain\n]\n               Stopping\nbuming_dui.0.pnoui2x6elrz@dui-2    \n|\n \n[\nServer-Ken Thompson\n]\n              \n[\nINFO\n]\n  \n[\n14\n:19:02.601008\n]\n   \n[\n12\n]\n    \n[\nMain\n]\n               Closing\n\n\n\n\n\n\nFurther reading\n\n\n\n\nWikipedia page on reboots\n\n\nMicrosoft about graceful shutdown\n\n\nGracefully stopping docker containers\n\n\nWhat to know about Java and shutdown hooks\n\n\nhttps://www.weave.works/blog/docker-container-networking-multicast-fast/\n\n\nhttps://www.weave.works/docs/net/latest/install/plugin/plugin-how-it-works/\n\n\nhttps://www.weave.works/docs/net/latest/install/plugin/plugin-v2/\n\n\nhttps://www.auzias.net/en/docker-network-multihost/\n\n\nhttps://forums.docker.com/t/cannot-get-zookeeper-to-work-running-in-docker-using-swarm-mode/27109\n\n\nhttps://github.com/docker/libnetwork/issues/740\n\n\n\n\nReferences\n\n\n\n\n\n\n\n\n\n\nP\u00e9ter M\u00e1rton(@slashdotpeter) Gracefule Shutdown NodeJS Kubernetes\n\n\n\n\n\n\nDocker docs on building\n\n\n\n\n\n\nJohn Zaccone on Entrypoint vs CMD\n\n\n\n\n\n\nLinux Exec command\n\n\n\n\n\n\nStackoverflow thread on CMD vs Entrypoint\n\n\n\n\n\n\nCodeship blog on CMD and Entrypoint details\n\n\n\n\n\n\nGrigorii Chudnov blog on Trapping Docker Signals\n\n\n\n\n\n\nAndy Wilkinson (from pivotal) explaining Spring Boot shutdown hook for Tomcat\n\n\n\n\n\n\nDocker Swarm issue with multicast\n\n\n\n\n\n\nDocker network library issue with multicast\n\n\n\n\n\n\nExcellent article on JVM details inside Containers", 
            "title": "Graceful shutdown"
        }, 
        {
            "location": "/docker/graceful-shutdown/#docker-graceful-shutdown", 
            "text": "", 
            "title": "Docker &amp; Graceful shutdown"
        }, 
        {
            "location": "/docker/graceful-shutdown/#the-case-for-graceful-shutdown", 
            "text": "We can speak about the graceful shutdown of our application, when all of the resources it used and all of the traffic and/or data processing what it handled are closed and released properly.\n  It means that no database connection remains open and no ongoing request fails because we stop our application.  1   I thank  P\u00e9ter M\u00e1rton  for the quote and giving a nice case for the graceful shutdown and docker.\nWhere his blog post goes into how to do this with/for Kubernetes, I will do this for Docker's Swarm (mode) orchestrator.  So, the case for graceful shutdown.\nAs the quote shows, what we mean with it, is that an application shuts down gracefully if it cleans up all its mess.  All things considered, I think most people would agree that cleaning up your mess - resources, connections or saying goodbye is preferred above just disappearing.   Shutting down nicely and leaving nothing behind will reduce the amount of potential (hard to debug) errors.\nIt also allows other applications or services to reliably know when you are there and when you're not there.\nNot every application will have such dependencies (to it), but in today's cluster environments with many moving parts you'll never know.  So I would recommend to always do a graceful shutdown if you're able.\nIn the light of Docker, that might be a bit different than you're use to.", 
            "title": "The case for graceful shutdown"
        }, 
        {
            "location": "/docker/graceful-shutdown/#exec-form-vs-shell-form", 
            "text": "There are several ways to run a command in a  Dockerfile .  These are are:   RUN : runs a command during the docker build phase  CMD : runs a command when the container gets started  ENTRYPOINT : provides the location from where commands get run when the container starts    Note  You need at least one ENTRYPOINT or CMD in a Dockerfile for it to be valid.\nThey can be used in collaboration but they can do similar things.   Al these commands can be put in both a shell form and a exec form  2 . For more information on these commands you should check out  John Zaccone's blog on Entrypoint vs CMD .  In summary, the shell form will run the command as a shell command and spawn a process via  /bin/sh -c .  Whereas the exec form will execute a child process that is still attached to PID1  4 .  This means that if you run the examples below, you will notice that you cannot  ctrl+c  out of the shell form, but you  can  out of the exec form.  Exec form is the recommended form to use and is a requirement for graceful shutdown.\nBelow you'll find some further reading on the CMD and ENTRYPOINT commands  5 6 .", 
            "title": "Exec (form) vs Shell (form)"
        }, 
        {
            "location": "/docker/graceful-shutdown/#shell-form-example", 
            "text": "1\n2 FROM  alpine    ENTRYPOINT  ping www.google.com  #  shell  format      3", 
            "title": "Shell form example"
        }, 
        {
            "location": "/docker/graceful-shutdown/#exec-form-example", 
            "text": "1\n2 FROM  alpine    ENTRYPOINT  [ ping ,  www.google.com ]  #  exec  format      3", 
            "title": "Exec form example"
        }, 
        {
            "location": "/docker/graceful-shutdown/#pid1", 
            "text": "Now you run your commands nicely as PID1 and make sure it is your process that receives the  SIGNALS .  Then all is good for a while, but at one point you will run into the problems of either having  zombie child processes  or  processes that aren't designed for running as PID1 .  This means you will have to do something about this, and luckily there's already some people who have done this for you.  There's  tini : a tiny initialization system designed for Docker.  If used with the  exec  form it will run as PID1 and will manage your process and its child processes for you.  For what it adds exactly and why it was introduced, you can rever to  this excellent explanation from its creator .  In fact, tini was so successful, that  docker included it in docker !  While it works for  docker run  and  docker compose  it doesn't yet work for Docker Swarm stacks.", 
            "title": "PID1"
        }, 
        {
            "location": "/docker/graceful-shutdown/#example-docker-run-with-init", 
            "text": "1\n2\n3\n4\n5\n6 docker run  \\  \n    --rm  \\ \n    -ti  \\      --init \\      --name dui-test \\ \n     dui", 
            "title": "Example docker run with init"
        }, 
        {
            "location": "/docker/graceful-shutdown/#example-dockerfile-with-tini", 
            "text": "1\n2\n3\n4\n5\n6 FROM  debian:stable-slim  ENV  TINI_VERSION v0.16.1  ADD  https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini  RUN  chmod +x /tini ENTRYPOINT  [ /tini ,  -vv , -g ,  -- ,  /usr/bin/dui ] \nCOPY --from = build /usr/bin/dui-image/ /usr/bin/dui  \nTini will be the entrypoint, executing our own user program (/usr/bin/dui).  This can also be achieved as follows:  1\n2 ENTRYPOINT  [ /tini ,  -vv , -g ,  -- ]  CMD [ /usr/bin/dui ]     -vv : debug log level 2 (-v =1, -vv=2, -vvv=3)  -g : kill the entire group of processes when signal is received  -- : end of tini and start of your command", 
            "title": "Example Dockerfile with tini"
        }, 
        {
            "location": "/docker/graceful-shutdown/#signals", 
            "text": "Now that we can correctly respond to signals we need to take care of which signals to listen to.   There are essentially two commands:  docker stop  and  docker kill  that can be used to stop it. Behind the scenes,  docker stop  stops a running container by sending it SIGINT signal, let the main process process it, and after a grace period uses SIGKILL to terminate the application.  7   You can test it with the following image, which uses Java's shutdown hook to shutdown gracefully: this only happens when it is stopped.\nRun the command below and press  ctrl+c , and you will see that the application shuts down gracefully.  1 docker run --rm -ti --name  test  caladreas/buming  \nMake sure you have two terminal windows open. In terminal one, run this command:  1\n2 docker run -d --name  test  caladreas/buming\ndocker logs -f  test    In terminal two, run this command:  1 docker rm -f  test    And now you will not see the graceful shutdown log, as the JVM was killed without being allowed to call shutdown hooks.  Docker allows you to specify which signal it should send via  --stop-signal  in the run command or  stop_signal:  in a compose file.  1\n2\n3\n4\n5\n6\n7 version :   3.5  services : \n   yi : \n     image :   dui \n     build :   . \n     stop_signal :   SIGINT", 
            "title": "Signals"
        }, 
        {
            "location": "/docker/graceful-shutdown/#examples", 
            "text": "How to actually listen to the signals and determine which one to use will depend on your programming language.  There's three examples I have worked out, one for Go (lang) and two for Java: pojo and Spring Boot.", 
            "title": "Examples"
        }, 
        {
            "location": "/docker/graceful-shutdown/#go", 
            "text": "", 
            "title": "Go"
        }, 
        {
            "location": "/docker/graceful-shutdown/#dockerfile", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 # build stage \nFROM golang:latest AS build-env\nRUN go get -v github.com/docker/docker/client/...\nRUN go get -v github.com/docker/docker/api/...\nADD src/  $GOPATH /flow-proxy-service-lister\nWORKDIR  $GOPATH /flow-proxy-service-lister\nRUN go build -o main -tags netgo main.go # final stage \nFROM alpine\nENTRYPOINT  [ /app/main ] \nCOPY --from = build-env /go/flow-proxy-service-lister/main /app/\nRUN chmod +x /app/main", 
            "title": "Dockerfile"
        }, 
        {
            "location": "/docker/graceful-shutdown/#go-code-for-graceful-shutdown", 
            "text": "The following is a way for Go to shutdown a http server when receiving a termination signal.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25 func   main ()   { \n     c   :=   make ( chan   bool )   // make channel for main  --  webserver communication \n     go   webserver . Start ( 7777 ,   webserverData ,   c )   // ignore the missing data \n\n     stop   :=   make ( chan   os . Signal ,   1 )   // make a channel that listens to is signals \n     signal . Notify ( stop ,   syscall . SIGINT ,   syscall . SIGTERM )   // we listen to some specific syscall signals \n\n     for   i   :=   1 ;   ;   i ++   {   // this is still infinite \n         t   :=   time . NewTicker ( time . Second   *   30 )   // set a timer for the polling \n         select   { \n         case   - stop :   // this means we got a os signal on our channel \n             break   // so we can stop \n         case   - t . C : \n             // our timer expired, refresh our data \n             continue   // and continue with the loop \n         } \n         break \n     } \n     fmt . Println ( Shutting down webserver )   // if we got here, we have to inform the webserver to close shop \n     c   -   true   // we do this by sending a message on the channel \n     if   b   :=   - c ;   b   {   // when we get true back, that means the webserver is doing with a graceful shutdown \n         fmt . Println ( Webserver shut down )   // webserver is done \n     } \n     fmt . Println ( Shut down app )   // we can close shop ourselves now  }", 
            "title": "Go code for graceful shutdown"
        }, 
        {
            "location": "/docker/graceful-shutdown/#java-plain-docker-swarm", 
            "text": "This application is a Java 9 modular application, which can be found on github,  github.com/joostvdg .", 
            "title": "Java plain (Docker Swarm)"
        }, 
        {
            "location": "/docker/graceful-shutdown/#dockerfile_1", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40 FROM  openjdk:9-jdk AS build  RUN  mkdir -p /usr/src/mods/jars RUN  mkdir -p /usr/src/mods/compiled\n\nCOPY . /usr/src WORKDIR  /usr/src  RUN  javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src  $( find src -name  *.java )  RUN  jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version  1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN  jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version  1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN  jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version  1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN  jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version  1 .0  -e com.github.joostvdg.dui.server.cli.DockerApp \\ \n    -C /usr/src/mods/compiled/joostvdg.dui.server . RUN  rm -rf /usr/bin/dui-image RUN  jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods  \\ \n    --add-modules joostvdg.dui.api  \\ \n    --add-modules joostvdg.dui.logging  \\ \n    --add-modules joostvdg.dui.server  \\ \n    --add-modules joostvdg.dui.client  \\ \n    --launcher  dui = joostvdg.dui.server  \\ \n    --output /usr/bin/dui-image RUN  ls -lath /usr/bin/dui-image RUN  ls -lath /usr/bin/dui-image RUN  /usr/bin/dui-image/bin/java --list-modules FROM  debian:stable-slim \nLABEL  authors = Joost van der Griendt  joostvdg@gmail.com \nLABEL  version = 0.1.0 \nLABEL  description = Docker image for playing with java applications in a concurrent, parallel and distributed manor.  # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/  ENV  TINI_VERSION v0.16.1  ADD  https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini  RUN  chmod +x /tini ENTRYPOINT  [ /tini ,  -vv , -g ,  -- ,  /usr/bin/dui/bin/dui ]  ENV  DATE_CHANGED= 20180120-1525 \nCOPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN  /usr/bin/dui/bin/java --list-modules", 
            "title": "Dockerfile"
        }, 
        {
            "location": "/docker/graceful-shutdown/#handling-code", 
            "text": "The code first initializes the server which and when started, creates the  Shutdown Hook .  Java handles certain signals in specific ways, as can be found  in this table  for linux.\nFor more information, you can read the  docs from Oracle .    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40 public   class   DockerApp   { \n     public   static   void   main ( String []   args )   { \n         ServiceLoader Logger   loggers   =   ServiceLoader . load ( Logger . class ); \n                 Logger   logger   =   loggers . findFirst (). isPresent ()   ?   loggers . findFirst (). get ()   :   null ; \n                 if   ( logger   ==   null )   { \n                     System . err . println ( Did not find any loggers, quiting ); \n                     System . exit ( 1 ); \n                 } \n                 logger . start ( LogLevel . INFO ); \n\n                 int   pseudoRandom   =   new   Random (). nextInt ( ProtocolConstants . POTENTIAL_SERVER_NAMES . length   - 1 ); \n                 String   serverName   =   ProtocolConstants . POTENTIAL_SERVER_NAMES [ pseudoRandom ]; \n                 int   listenPort   =   ProtocolConstants . EXTERNAL_COMMUNICATION_PORT_A ; \n                 String   multicastGroup   =   ProtocolConstants . MULTICAST_GROUP ; \n\n                 DuiServer   distributedServer   =   DuiServerFactory . newDistributedServer ( listenPort , multicastGroup   ,   serverName ,   logger ); \n\n                 distributedServer . logMembership (); \n\n                 ExecutorService   executorService   =   Executors . newFixedThreadPool ( 1 ); \n                 executorService . submit ( distributedServer :: startServer ); \n\n                 long   threadId   =   Thread . currentThread (). getId (); \n\n                 Runtime . getRuntime (). addShutdownHook ( new   Thread (()   -   { \n                     System . out . println ( Shutdown hook called! ); \n                     logger . log ( LogLevel . WARN ,   App ,   ShotdownHook ,   threadId ,   Shutting down at request of Docker ); \n                     distributedServer . stopServer (); \n                     distributedServer . closeServer (); \n                     executorService . shutdown (); \n                     try   { \n                         Thread . sleep ( 100 ); \n                         executorService . shutdownNow (); \n                         logger . stop (); \n                     }   catch   ( InterruptedException   e )   { \n                         e . printStackTrace (); \n                     } \n                 }));         \n     }  }", 
            "title": "Handling code"
        }, 
        {
            "location": "/docker/graceful-shutdown/#java-plain-kubernetes", 
            "text": "So far we've utilized the utilities from Docker itself in conjunction with it's native Docker Swarm orchestrator.  Unfortunately, when it comes to popularity  Kubernetes beats Swarm hands down .  So this isn't complete if it doesn't also do graceful shutdown in Kubernetes.", 
            "title": "Java Plain (Kubernetes)"
        }, 
        {
            "location": "/docker/graceful-shutdown/#in-dockerfile", 
            "text": "Our original file had to be changed, as Debian's Slim image doesn't actually contain the kill package.\nAnd we need a kill package, as we cannot instruct Kubernetes to issue a specific SIGNAL.\nInstead, we can issue a  PreStop exec command , which we can utilise to execute a  killall  java  -INT .  The command will be specified in the Kubernetes deployment definition below.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41 FROM  openjdk:9-jdk AS build  RUN  mkdir -p /usr/src/mods/jars RUN  mkdir -p /usr/src/mods/compiled\n\nCOPY . /usr/src WORKDIR  /usr/src  RUN  javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src  $( find src -name  *.java )  RUN  jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version  1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN  jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version  1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN  jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version  1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN  jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version  1 .0  -e com.github.joostvdg.dui.server.cli.DockerApp \\ \n    -C /usr/src/mods/compiled/joostvdg.dui.server . RUN  rm -rf /usr/bin/dui-image RUN  jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods  \\ \n    --add-modules joostvdg.dui.api  \\ \n    --add-modules joostvdg.dui.logging  \\ \n    --add-modules joostvdg.dui.server  \\ \n    --add-modules joostvdg.dui.client  \\ \n    --launcher  dui = joostvdg.dui.server  \\ \n    --output /usr/bin/dui-image RUN  ls -lath /usr/bin/dui-image RUN  ls -lath /usr/bin/dui-image RUN  /usr/bin/dui-image/bin/java --list-modules FROM  debian:stable-slim \nLABEL  authors = Joost van der Griendt  joostvdg@gmail.com \nLABEL  version = 0.1.0 \nLABEL  description = Docker image for playing with java applications in a concurrent, parallel and distributed manor.  # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/  ENV  TINI_VERSION v0.16.1  ADD  https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini  RUN  chmod +x /tini ENTRYPOINT  [ /tini ,  -vv , -g ,  -- ,  /usr/bin/dui/bin/dui ]  ENV  DATE_CHANGED= 20180120-1525  RUN  apt-get update   apt-get install --no-install-recommends -y  psmisc = 22 .*   rm -rf /var/lib/apt/lists/*\nCOPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN  /usr/bin/dui/bin/java --list-modules", 
            "title": "In Dockerfile"
        }, 
        {
            "location": "/docker/graceful-shutdown/#kubernetes-deployment", 
            "text": "So here we have the image's K8s  Deployment  descriptor.  Including the Pod's  lifecycle   preStop  with a exec style command. You should know by now  why we prefer that .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25 apiVersion :   extensions/v1beta1  kind :   Deployment  metadata : \n   name :   dui-deployment \n   namespace :   default \n   labels : \n     k8s-app :   dui  spec : \n   replicas :   3 \n   template : \n     metadata : \n       labels : \n         k8s-app :   dui \n     spec : \n       containers : \n         -   name :   master \n           image :   caladreas/buming \n           ports : \n             -   name :   http \n               containerPort :   7777 \n           lifecycle : \n             preStop : \n               exec : \n                 command :   [ killall ,   java   ,   -INT ] \n       terminationGracePeriodSeconds :   60", 
            "title": "Kubernetes Deployment"
        }, 
        {
            "location": "/docker/graceful-shutdown/#java-spring-boot-1x", 
            "text": "This example is for Spring Boot 1.x, in time we will have an example for 2.x.  This example is for the scenario of a Fat Jar with Tomcat as container  8 .", 
            "title": "Java Spring Boot (1.x)"
        }, 
        {
            "location": "/docker/graceful-shutdown/#execute-example", 
            "text": "1 docker-compose build   Execute the following command:  1 docker run --rm -ti --name  test  spring-boot-graceful   Exit the application/container via  ctrl+c  and you should see the application shutting down gracefully.  1\n2\n3 2018 -01-30  13 :35:46.327  INFO  7  ---  [        Thread-3 ]  ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@6e5e91e4: startup date  [ Tue Jan  30   13 :35:42 GMT  2018 ] ;  root of context hierarchy 2018 -01-30  13 :35:46.405  INFO  7  ---  [        Thread-3 ]  BootGracefulApplication $GracefulShutdown  : Tomcat was shutdown gracefully within the allotted time. 2018 -01-30  13 :35:46.408  INFO  7  ---  [        Thread-3 ]  o.s.j.e.a.AnnotationMBeanExporter        : Unregistering JMX-exposed beans on shutdown", 
            "title": "Execute example"
        }, 
        {
            "location": "/docker/graceful-shutdown/#dockerfile_2", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 FROM  maven:3-jdk-8 AS build  ENV  MAVEN_OPTS=-Dmaven.repo.local=/usr/share/maven/repository  ENV  WORKDIR=/usr/src/graceful  RUN  mkdir  $WORKDIR  WORKDIR  $WORKDIR \nCOPY pom.xml  $WORKDIR  RUN  mvn -B -e org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline\nCOPY .  $WORKSPACE  RUN  mvn -B -e clean verify FROM  anapsix/alpine-java:8_jdk_unlimited \nLABEL  authors = Joost van der Griendt  joostvdg@gmail.com  ENV  TINI_VERSION v0.16.1  ADD  https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini  RUN  chmod +x /tini ENTRYPOINT  [ /tini ,  -vv , -g ,  -- ]  ENV  DATE_CHANGED= 20180120-1525 \nCOPY --from = build /usr/src/graceful/target/spring-boot-graceful.jar /app.jar CMD  [ java ,  -Xms256M , -Xmx480M ,  -Djava.security.egd=file:/dev/./urandom ,  -jar ,  /app.jar ]", 
            "title": "Dockerfile"
        }, 
        {
            "location": "/docker/graceful-shutdown/#docker-compose-file", 
            "text": "1\n2\n3\n4\n5\n6\n7 version :   3.5  services : \n   web : \n     image :   spring-boot-graceful \n     build :   . \n     stop_signal :   SIGINT", 
            "title": "Docker compose file"
        }, 
        {
            "location": "/docker/graceful-shutdown/#java-handling-code", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83 package   com.github.joostvdg.demo.springbootgraceful ;  import   org.springframework.boot.SpringApplication ;  import   org.springframework.boot.autoconfigure.SpringBootApplication ;  import   org.apache.catalina.connector.Connector ;  import   org.apache.tomcat.util.threads.ThreadPoolExecutor ;  import   org.slf4j.Logger ;  import   org.slf4j.LoggerFactory ;  import   org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer ;  import   org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer ;  import   org.springframework.boot.context.embedded.tomcat.TomcatConnectorCustomizer ;  import   org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory ;  import   org.springframework.context.ApplicationListener ;  import   org.springframework.context.annotation.Bean ;  import   org.springframework.context.event.ContextClosedEvent ;  import   java.util.concurrent.Executor ;  import   java.util.concurrent.TimeUnit ;  @SpringBootApplication  public   class   SpringBootGracefulApplication   { \n\n     public   static   void   main ( String []   args )   { \n         SpringApplication . run ( SpringBootGracefulApplication . class ,   args ); \n     } \n\n     @Bean \n     public   GracefulShutdown   gracefulShutdown ()   { \n         return   new   GracefulShutdown (); \n     } \n\n     @Bean \n     public   EmbeddedServletContainerCustomizer   tomcatCustomizer ()   { \n         return   new   EmbeddedServletContainerCustomizer ()   { \n\n             @Override \n             public   void   customize ( ConfigurableEmbeddedServletContainer   container )   { \n                 if   ( container   instanceof   TomcatEmbeddedServletContainerFactory )   { \n                     (( TomcatEmbeddedServletContainerFactory )   container ) \n                             . addConnectorCustomizers ( gracefulShutdown ()); \n                 } \n\n             } \n         }; \n     } \n\n     private   static   class   GracefulShutdown   implements   TomcatConnectorCustomizer , \n             ApplicationListener ContextClosedEvent   { \n\n         private   static   final   Logger   log   =   LoggerFactory . getLogger ( GracefulShutdown . class ); \n\n         private   volatile   Connector   connector ; \n\n         @Override \n         public   void   customize ( Connector   connector )   { \n             this . connector   =   connector ; \n         } \n\n         @Override \n         public   void   onApplicationEvent ( ContextClosedEvent   event )   { \n             this . connector . pause (); \n             Executor   executor   =   this . connector . getProtocolHandler (). getExecutor (); \n             if   ( executor   instanceof   ThreadPoolExecutor )   { \n                 try   { \n                     ThreadPoolExecutor   threadPoolExecutor   =   ( ThreadPoolExecutor )   executor ; \n                     threadPoolExecutor . shutdown (); \n                     if   (! threadPoolExecutor . awaitTermination ( 30 ,   TimeUnit . SECONDS ))   { \n                         log . warn ( Tomcat thread pool did not shut down gracefully within  \n                                 +   30 seconds. Proceeding with forceful shutdown ); \n                     }   else   { \n                         log . info ( Tomcat was shutdown gracefully within the allotted time. ); \n                     } \n                 } \n                 catch   ( InterruptedException   ex )   { \n                     Thread . currentThread (). interrupt (); \n                 } \n             } \n         } \n\n     }  }", 
            "title": "Java handling code"
        }, 
        {
            "location": "/docker/graceful-shutdown/#example-with-docker-swarm", 
            "text": "For now there's only an example with  docker swarm , in time there will also be a  Kubernetes  example.  Now that you can create Java applications packaged neatly in Docker images that support graceful shutdown, it would be nice to utilize.  A good scenario would be a microservices architecture where services can come and go, but are registered in a  service registry such as Eureka .  Or a membership based protocol where members interact with each other and perhaps shard data.  In these cases, of course the interactions are designed to be fault tolerant and discover faulty nodes on their own.\nBut wouldn't it be better that if you knew you're going to quit, you inform the rest?  We can reuse the  caladreas/buming  image and make it a docker swarm stack and run the service on every node.\nThis way, we can easily see members coming and going and reduce the time to detect failure by notifying our peers of our impeding end.", 
            "title": "Example with Docker Swarm"
        }, 
        {
            "location": "/docker/graceful-shutdown/#docker-swarm-cluster", 
            "text": "Setting up a docker swarm cluster is easy, but has some requirements:   virtual box 4.x+  docker-machine 1.12+  docker 17.06+    Warn  Make sure this is the first and only virtualbox docker-machine VM being created/running, so that the ip range starts with 192.168.99.100    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 docker-machine create --driver virtualbox dui-1\ndocker-machine create --driver virtualbox dui-2\ndocker-machine create --driver virtualbox dui-3 eval   $( docker-machine env dui-1 )  IP = 192 .168.99.100\ndocker swarm init --advertise-addr  $IP  TOKEN = $( docker swarm join-token -q worker )  eval   $( docker-machine env dui-2 ) \ndocker swarm join --token  ${ TOKEN }   ${ IP } :2377 eval   $( docker-machine env dui-3 ) \ndocker swarm join --token  ${ TOKEN }   ${ IP } :2377 eval   $( docker-machine env dui-1 ) \ndocker node ls", 
            "title": "Docker swarm cluster"
        }, 
        {
            "location": "/docker/graceful-shutdown/#docker-swarm-network-and-multicast", 
            "text": "Unfortunately, docker swarm's swarm mode network  overlay  does not support multicast  9 10 .  Why is this a problem? Well, the application I use to test the graceful shutdown requires this, sorry.  Luckily there is a very easy solution for this, its by using  Weavenet 's docker network plugin.  Don't want to know about it or how you install it? Don't worry, just execute the script below.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 #!/usr/bin/env bash  echo   =  Prepare dui-2  eval   $( docker-machine env dui-2 ) \ndocker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions\ndocker plugin disable weaveworks/net-plugin:2.1.3\ndocker plugin  set  weaveworks/net-plugin:2.1.3  WEAVE_MULTICAST = 1 \ndocker plugin  enable  weaveworks/net-plugin:2.1.3 echo   =  Prepare dui-3  eval   $( docker-machine env dui-3 ) \ndocker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions\ndocker plugin disable weaveworks/net-plugin:2.1.3\ndocker plugin  set  weaveworks/net-plugin:2.1.3  WEAVE_MULTICAST = 1 \ndocker plugin  enable  weaveworks/net-plugin:2.1.3 echo   =  Prepare dui-1  eval   $( docker-machine env dui-1 ) \ndocker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions\ndocker plugin disable weaveworks/net-plugin:2.1.3\ndocker plugin  set  weaveworks/net-plugin:2.1.3  WEAVE_MULTICAST = 1 \ndocker plugin  enable  weaveworks/net-plugin:2.1.3\ndocker network create --driver = weaveworks/net-plugin:2.1.3 --opt works.weave.multicast = true  --attachable dui", 
            "title": "Docker swarm network and multicast"
        }, 
        {
            "location": "/docker/graceful-shutdown/#docker-stack", 
            "text": "Now to create a service that runs on every node it is the easiest to create a  docker stack .", 
            "title": "Docker stack"
        }, 
        {
            "location": "/docker/graceful-shutdown/#compose-file-docker-stackyml", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 version :   3.5  services : \n   dui : \n     image :   caladreas/buming \n     build :   . \n     stop_signal :   SIGINT \n     networks : \n       -   dui \n     deploy : \n       mode :   global  networks : \n   dui : \n     external :   true", 
            "title": "Compose file (docker-stack.yml)"
        }, 
        {
            "location": "/docker/graceful-shutdown/#create-stack", 
            "text": "1 docker stack deploy --compose-file docker-stack.yml buming", 
            "title": "Create stack"
        }, 
        {
            "location": "/docker/graceful-shutdown/#execute-example_1", 
            "text": "Now that we have a docker swarm cluster and a stack - which has a service running on every node - we can showcase the power of graceful shutdown in a cluster of dependent services.  Confirm the service is running correctly on every node, first lets check our nodes.  1\n2 eval   $( docker-machine env dui-1 ) \ndocker node ls  \nWhich should look like this:  1\n2\n3\n4 ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS\nf21ilm4thxegn5xbentmss5ur *   dui-1               Ready               Active              Leader\ny7475bo5uplt2b58d050b4wfd     dui-2               Ready               Active              \n6ssxola6y1i6h9p8256pi7bfv     dui-3               Ready               Active                               Then check the service.  1 docker service ps buming_dui   Which should look like this.  1\n2\n3\n4 ID                  NAME                                   IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\n3mrpr0jg31x1        buming_dui.6ssxola6y1i6h9p8256pi7bfv   dui:latest          dui-3               Running             Running  17  seconds ago                       \npfubtiy4j7vo        buming_dui.f21ilm4thxegn5xbentmss5ur   dui:latest          dui-1               Running             Running  17  seconds ago                       \nf4gjnmhoe3y4        buming_dui.y7475bo5uplt2b58d050b4wfd   dui:latest          dui-2               Running             Running  17  seconds ago                          Now open a second terminal window.\nIn window one, follow the service logs:  1\n2 eval   $( docker-machine env dui-1 ) \ndocker service logs -f buming_dui   In window two, go to a different node and stop the container.  1\n2\n3 eval   $( docker-machine env dui-2 ) \ndocker ps\ndocker stop buming_dui.y7475bo5uplt2b58d050b4wfd.pnoui2x6elrz0tvkjz51njz94   In this case, you will see the other nodes receiving a leave notice and then the node stopping.  1\n2\n3\n4\n5\n6 buming_dui.0.ryd8szexxku3@dui-3     |   [ Server-John D. Carmack ]             [ WARN ]    [ 14 :19:02.604011 ]     [ 16 ]      [ Main ]               Received membership leave notice from MessageOrigin { host = 83918f6ad817 ,  ip = 10.0.0.7 ,  name = Ken Thompson } \nbuming_dui.0.so5m14sz8ksh@dui-1     |   [ Server-Alan Kay ]                    [ WARN ]    [ 14 :19:02.602082 ]     [ 16 ]      [ Main ]               Received membership leave notice from MessageOrigin { host = 83918f6ad817 ,  ip = 10.0.0.7 ,  name = Ken Thompson } \nbuming_dui.0.pnoui2x6elrz@dui-2     |  Shutdown hook called!\nbuming_dui.0.pnoui2x6elrz@dui-2     |   [ App ]                                [ WARN ]    [ 14 :19:02.598759 ]     [ 1 ]   [ ShotdownHook ]       Shutting down at request of Docker\nbuming_dui.0.pnoui2x6elrz@dui-2     |   [ Server-Ken Thompson ]                [ INFO ]    [ 14 :19:02.598858 ]     [ 12 ]      [ Main ]                Stopping\nbuming_dui.0.pnoui2x6elrz@dui-2     |   [ Server-Ken Thompson ]                [ INFO ]    [ 14 :19:02.601008 ]     [ 12 ]      [ Main ]                Closing", 
            "title": "Execute example"
        }, 
        {
            "location": "/docker/graceful-shutdown/#further-reading", 
            "text": "Wikipedia page on reboots  Microsoft about graceful shutdown  Gracefully stopping docker containers  What to know about Java and shutdown hooks  https://www.weave.works/blog/docker-container-networking-multicast-fast/  https://www.weave.works/docs/net/latest/install/plugin/plugin-how-it-works/  https://www.weave.works/docs/net/latest/install/plugin/plugin-v2/  https://www.auzias.net/en/docker-network-multihost/  https://forums.docker.com/t/cannot-get-zookeeper-to-work-running-in-docker-using-swarm-mode/27109  https://github.com/docker/libnetwork/issues/740", 
            "title": "Further reading"
        }, 
        {
            "location": "/docker/graceful-shutdown/#references", 
            "text": "P\u00e9ter M\u00e1rton(@slashdotpeter) Gracefule Shutdown NodeJS Kubernetes    Docker docs on building    John Zaccone on Entrypoint vs CMD    Linux Exec command    Stackoverflow thread on CMD vs Entrypoint    Codeship blog on CMD and Entrypoint details    Grigorii Chudnov blog on Trapping Docker Signals    Andy Wilkinson (from pivotal) explaining Spring Boot shutdown hook for Tomcat    Docker Swarm issue with multicast    Docker network library issue with multicast    Excellent article on JVM details inside Containers", 
            "title": "References"
        }, 
        {
            "location": "/docker/swarm/", 
            "text": "Docker Swarm (mode)", 
            "title": "Swarm (mode)"
        }, 
        {
            "location": "/docker/swarm/#docker-swarm-mode", 
            "text": "", 
            "title": "Docker Swarm (mode)"
        }, 
        {
            "location": "/docker/kubernetes/", 
            "text": "Kubernetes\n\n\nKubernetes terminology\n\n\nKubernetes model\n\n\nResources\n\n\n\n\nhttps://github.com/weaveworks/scope\n\n\nhttps://github.com/hjacobs/kube-ops-view\n\n\nhttps://coreos.com/tectonic/docs/latest/tutorials/sandbox/install.html\n\n\nhttps://github.com/kubernetes/dashboard\n\n\nhttps://blog.alexellis.io/you-need-to-know-kubernetes-and-swarm/\n\n\nhttps://kubernetes.io/docs/reference/kubectl/cheatsheet/\n\n\nhttps://blog.heptio.com/core-kubernetes-jazz-improv-over-orchestration-a7903ea92ca", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/docker/kubernetes/#kubernetes", 
            "text": "", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/docker/kubernetes/#kubernetes-terminology", 
            "text": "", 
            "title": "Kubernetes terminology"
        }, 
        {
            "location": "/docker/kubernetes/#kubernetes-model", 
            "text": "", 
            "title": "Kubernetes model"
        }, 
        {
            "location": "/docker/kubernetes/#resources", 
            "text": "https://github.com/weaveworks/scope  https://github.com/hjacobs/kube-ops-view  https://coreos.com/tectonic/docs/latest/tutorials/sandbox/install.html  https://github.com/kubernetes/dashboard  https://blog.alexellis.io/you-need-to-know-kubernetes-and-swarm/  https://kubernetes.io/docs/reference/kubectl/cheatsheet/  https://blog.heptio.com/core-kubernetes-jazz-improv-over-orchestration-a7903ea92ca", 
            "title": "Resources"
        }, 
        {
            "location": "/swe/naming/", 
            "text": "On Naming\n\n\nResources\n\n\n\n\nhttps://www.slideshare.net/pirhilton/naming-guidelines-for-professional-programmers\n\n\nhttp://www.yourdictionary.com/diction4.html", 
            "title": "Naming"
        }, 
        {
            "location": "/swe/naming/#on-naming", 
            "text": "", 
            "title": "On Naming"
        }, 
        {
            "location": "/swe/naming/#resources", 
            "text": "https://www.slideshare.net/pirhilton/naming-guidelines-for-professional-programmers  http://www.yourdictionary.com/diction4.html", 
            "title": "Resources"
        }, 
        {
            "location": "/swe/ddd/", 
            "text": "Domain Driven Design", 
            "title": "Domain Driven Design"
        }, 
        {
            "location": "/swe/ddd/#domain-driven-design", 
            "text": "", 
            "title": "Domain Driven Design"
        }, 
        {
            "location": "/swe/microservices/", 
            "text": "Microservices", 
            "title": "Microservices"
        }, 
        {
            "location": "/swe/microservices/#microservices", 
            "text": "", 
            "title": "Microservices"
        }, 
        {
            "location": "/swe/algorithms/", 
            "text": "", 
            "title": "Algorithms"
        }, 
        {
            "location": "/swe/observability/", 
            "text": "Observability\n\n\nResources\n\n\n\n\nhttps://www.vividcortex.com/blog/monitoring-isnt-observability\n\n\nhttps://medium.com/@copyconstruct/monitoring-and-observability-8417d1952e1c\n\n\nhttps://codeascraft.com/2011/02/15/measure-anything-measure-everything/", 
            "title": "Observability"
        }, 
        {
            "location": "/swe/observability/#observability", 
            "text": "", 
            "title": "Observability"
        }, 
        {
            "location": "/swe/observability/#resources", 
            "text": "https://www.vividcortex.com/blog/monitoring-isnt-observability  https://medium.com/@copyconstruct/monitoring-and-observability-8417d1952e1c  https://codeascraft.com/2011/02/15/measure-anything-measure-everything/", 
            "title": "Resources"
        }, 
        {
            "location": "/swe/others/", 
            "text": "Other Software Engineering Concepts\n\n\nResource management\n\n\nWhen there are finite resources in your system, manage them explicitly.\n\n\nBe that memory, CPU, amount of connections to a database or incoming http connections.\n\n\nBack Pressure\n\n\n\n\nBack pressure\n\n\nWhen one component is struggling to keep-up, the system as a whole needs to respond in a sensible way. \n\n\nIt is unacceptable for the component under stress to fail catastrophically or to drop messages in an uncontrolled fashion. \n\n\nSince it can\u2019t cope and it can\u2019t fail it should communicate the fact that it is under stress to upstream components \nand so get them to reduce the load. This back-pressure is an important feedback mechanism that allows systems to \ngracefully respond to load rather than collapse under it. The back-pressure may cascade all the way up to the user, \nat which point responsiveness may degrade, but this mechanism will ensure that the system is resilient under load, \nand will provide information that may allow the system itself to apply other resources to help distribute the load, see Elasticity.\n\n1\n\n\n\n\nFurther reading:\n\n\n\n\nDZone article\n\n\nSpotify Engineering\n\n\n\n\nMemoization\n\n\n\n\nMemoization\n\n\nIn computing, memoization or memoisation is an optimization technique used primarily to speed up computer programs \nby storing the results of expensive function calls and returning the cached result when the same inputs occur again. \n\n\nMemoization has also been used in other contexts (and for purposes other than speed gains), \nsuch as in simple mutually recursive descent parsing.\n\n\n\n\nImportant Theories\n\n\n\n\nTheory of constraints\n\n\nLaw of demeter\n\n\nConway's law\n\n\nLittle's law\n\n\nCommoditization\n\n\nAmdahl's Law\n\n\n\n\n\n\n\n\n\n\n\n\nReactive Manifesto\n\n\n\n\n\n\nWikipedia article on Memoization", 
            "title": "Others"
        }, 
        {
            "location": "/swe/others/#other-software-engineering-concepts", 
            "text": "", 
            "title": "Other Software Engineering Concepts"
        }, 
        {
            "location": "/swe/others/#resource-management", 
            "text": "When there are finite resources in your system, manage them explicitly.  Be that memory, CPU, amount of connections to a database or incoming http connections.", 
            "title": "Resource management"
        }, 
        {
            "location": "/swe/others/#back-pressure", 
            "text": "Back pressure  When one component is struggling to keep-up, the system as a whole needs to respond in a sensible way.   It is unacceptable for the component under stress to fail catastrophically or to drop messages in an uncontrolled fashion.   Since it can\u2019t cope and it can\u2019t fail it should communicate the fact that it is under stress to upstream components \nand so get them to reduce the load. This back-pressure is an important feedback mechanism that allows systems to \ngracefully respond to load rather than collapse under it. The back-pressure may cascade all the way up to the user, \nat which point responsiveness may degrade, but this mechanism will ensure that the system is resilient under load, \nand will provide information that may allow the system itself to apply other resources to help distribute the load, see Elasticity. 1   Further reading:   DZone article  Spotify Engineering", 
            "title": "Back Pressure"
        }, 
        {
            "location": "/swe/others/#memoization", 
            "text": "Memoization  In computing, memoization or memoisation is an optimization technique used primarily to speed up computer programs \nby storing the results of expensive function calls and returning the cached result when the same inputs occur again.   Memoization has also been used in other contexts (and for purposes other than speed gains), \nsuch as in simple mutually recursive descent parsing.", 
            "title": "Memoization"
        }, 
        {
            "location": "/swe/others/#important-theories", 
            "text": "Theory of constraints  Law of demeter  Conway's law  Little's law  Commoditization  Amdahl's Law       Reactive Manifesto    Wikipedia article on Memoization", 
            "title": "Important Theories"
        }, 
        {
            "location": "/productivity/", 
            "text": "Developer Productivity\n\n\nThe Balancing act between centralized and decentralized\n\n\nHow do you measure productivity\n\n\nOn Multitasking\n\n\nLearning from Lean/Toyota\n\n\nHuman Psychology\n\n\nConway's Law\n\n\n\n\n\"organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations.\" - M. Conway \n1\n\n\n\n\nFurther reading\n\n\nArticles\n\n\n\n\nBlog on Twitter's Engineering Efficiency\n\n\nWhy Companies should have a Heroku platform for their developers\n\n\nMultitasking is bad for your health\n\n\nMicrosoft research on Developer's perception of productivity\n\n\nDeveloper Productivity Struggles\n\n\nYou cannot measure productivity\n\n\nThe Productivity Paradox\n\n\nThere is no Productivity Paradox: it lags behind investments\n\n\nEconomist: solving the paradox\n\n\nThe Myth Of Developer Productivity\n\n\nEffectiveness vs. Efficiency\n\n\nLean Manufactoring\n\n\nTheory of Constraints\n\n\nThoughtworks: demystifying Conway's Law\n\n\nJohn Allspaw: a mature role for automation\n\n\nResearch from DORA\n\n\n\n\nBooks\n\n\n\n\nThe Goal\n\n\nThe Phoenix Project\n\n\nContinuous Delivery\n\n\nThe Lean Startup\n\n\nThe Lean Enterprise\n\n\nDevOps Handbook\n\n\nThinking Fast and Slow\n\n\nSapiens\n\n\n\n\nReferences\n\n\n\n\n\n\n\n\n\n\nConway's law in wikipedia", 
            "title": "General"
        }, 
        {
            "location": "/productivity/#developer-productivity", 
            "text": "", 
            "title": "Developer Productivity"
        }, 
        {
            "location": "/productivity/#the-balancing-act-between-centralized-and-decentralized", 
            "text": "", 
            "title": "The Balancing act between centralized and decentralized"
        }, 
        {
            "location": "/productivity/#how-do-you-measure-productivity", 
            "text": "", 
            "title": "How do you measure productivity"
        }, 
        {
            "location": "/productivity/#on-multitasking", 
            "text": "", 
            "title": "On Multitasking"
        }, 
        {
            "location": "/productivity/#learning-from-leantoyota", 
            "text": "", 
            "title": "Learning from Lean/Toyota"
        }, 
        {
            "location": "/productivity/#human-psychology", 
            "text": "", 
            "title": "Human Psychology"
        }, 
        {
            "location": "/productivity/#conways-law", 
            "text": "\"organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations.\" - M. Conway  1", 
            "title": "Conway's Law"
        }, 
        {
            "location": "/productivity/#further-reading", 
            "text": "", 
            "title": "Further reading"
        }, 
        {
            "location": "/productivity/#articles", 
            "text": "Blog on Twitter's Engineering Efficiency  Why Companies should have a Heroku platform for their developers  Multitasking is bad for your health  Microsoft research on Developer's perception of productivity  Developer Productivity Struggles  You cannot measure productivity  The Productivity Paradox  There is no Productivity Paradox: it lags behind investments  Economist: solving the paradox  The Myth Of Developer Productivity  Effectiveness vs. Efficiency  Lean Manufactoring  Theory of Constraints  Thoughtworks: demystifying Conway's Law  John Allspaw: a mature role for automation  Research from DORA", 
            "title": "Articles"
        }, 
        {
            "location": "/productivity/#books", 
            "text": "The Goal  The Phoenix Project  Continuous Delivery  The Lean Startup  The Lean Enterprise  DevOps Handbook  Thinking Fast and Slow  Sapiens", 
            "title": "Books"
        }, 
        {
            "location": "/productivity/#references", 
            "text": "Conway's law in wikipedia", 
            "title": "References"
        }, 
        {
            "location": "/productivity/tools/", 
            "text": "Developer Productivity Tools", 
            "title": "Tools"
        }, 
        {
            "location": "/productivity/tools/#developer-productivity-tools", 
            "text": "", 
            "title": "Developer Productivity Tools"
        }, 
        {
            "location": "/productivity/intellij/", 
            "text": "Intelli J\n\n\nBeneficial OS changes\n\n\nLinux\n\n\n\n\nIncrease inotify watches\n\\", 
            "title": "Intelli J"
        }, 
        {
            "location": "/productivity/intellij/#intelli-j", 
            "text": "", 
            "title": "Intelli J"
        }, 
        {
            "location": "/productivity/intellij/#beneficial-os-changes", 
            "text": "", 
            "title": "Beneficial OS changes"
        }, 
        {
            "location": "/productivity/intellij/#linux", 
            "text": "Increase inotify watches \\", 
            "title": "Linux"
        }, 
        {
            "location": "/productivity/paradigms/", 
            "text": "Paradigms\n\n\nProduct centered\n\n\nResources", 
            "title": "Paradigms"
        }, 
        {
            "location": "/productivity/paradigms/#paradigms", 
            "text": "", 
            "title": "Paradigms"
        }, 
        {
            "location": "/productivity/paradigms/#product-centered", 
            "text": "", 
            "title": "Product centered"
        }, 
        {
            "location": "/productivity/paradigms/#resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/productivity/studies/", 
            "text": "Developer Productivity Studies", 
            "title": "Studies"
        }, 
        {
            "location": "/productivity/studies/#developer-productivity-studies", 
            "text": "", 
            "title": "Developer Productivity Studies"
        }, 
        {
            "location": "/other/mkdocs/", 
            "text": "MKDocs\n\n\nThis website is build using the following:\n\n\n\n\nMKDocs\n a python tool for building static websites from \nMarkDown\n files\n\n\nMK Material\n expansion/theme of MK Docs that makes it a responsive website with Google's Material theme\n\n\n\n\nAdd information to the docs\n\n\nMKDocs can be a bit daunting to use, especially when extended with \nMKDocs Material\n and \nPyMdown Extensions\n.\n\n\nThere are two parts to the site: 1) the markdown files, they're in \ndocs/\n and 2) the site listing (mkdocs.yml) and automation scripts, these can be found in \ndocs-scripts/\n.\n\n\nExtends current page\n\n\nTo extend a current page, simply write the MarkDown as you're used to.\n\n\nFor the specific extensions offered by PyMX and Material, checkout the following pages:\n\n\n\n\nMKDocs Material Getting Started Guide\n\n\nMKDocs Extensions\n\n\nPyMdown Extensions Usage Guide\n\n\n\n\nAdd a new page\n\n\nIn the \ndocs-scripts/mkdocs.yml\n you will find the site structure under the yml item of \npages\n.\n\n\n1\n2\n3\n4\n5\n6\npages:\n- Home: index.md\n- Other Root Page: some-page.md\n- Root with children:\n  - ChildOne: root2/child1.md\n  - ChildTwo: root2/child2.md\n\n\n\n\n\n\nThings to know\n\n\n\n\nAll .md files that are listed in the \npages\n will be translated to an HTML file and dubbed {OriginalFileName}.html\n\n\nNaming a file index.md will allow you to refer to it by path without the file name\n\n\nwe can refer to root2 simply by \nsite/root2\n and can omit the index.\n\n1\n2\n- Root: index.md\n- Root2: root2/index.html\n\n\n\n\n\n\n\n\n\n\n\nBuild the site locally\n\n\nAs it is a Python tool, you can easily build it with Python (2.7 is recommended).\n\n\nThe requirements are captured in a \npip\n install scripts: \ndocs-scripts/install.sh\n where the dependencies are in \nPip's requirements.txt\n.\n\n\nOnce that is done, you can do the following:\n\n\n1\nmkdocs build --clean\n\n\n\n\n\n\nWhich will generate the site into \ndocs-scripts/site\n where you can simply open the index.html with a browser - it is a static site.\n\n\nFor docker, you can use the \n*.sh\n scripts, or simply \nrun.sh\n to kick of the entire build.\n\n\nJenkins build\n\n\nDeclarative format\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\npipeline\n \n{\n\n    \nagent\n \nnone\n\n    \noptions\n \n{\n\n        \ntimeout(time:\n \n10,\n \nunit:\n \nMINUTES\n)\n\n        \ntimestamps()\n\n        \nbuildDiscarder(logRotator(numToKeepStr:\n \n5\n))\n\n    \n}\n\n    \nstages\n \n{\n\n        \nstage(\nPrepare\n){\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nsteps\n \n{\n\n                \ndeleteDir()\n\n            \n}\n\n        \n}\n\n        \nstage(\nCheckout\n)\n{\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nsteps\n \n{\n\n                \ncheckout\n \nscm\n\n                \nscript\n \n{\n\n                    \nenv.GIT_COMMIT_HASH\n \n=\n \nsh\n \nreturnStdout:\n \ntrue,\n \nscript:\n \ngit\n \nrev-parse\n \n--verify\n \nHEAD\n\n                \n}\n\n            \n}\n\n        \n}\n\n        \nstage(\nBuild\n \nDocs\n)\n \n{\n\n            \nagent\n \n{\n\n                \ndocker\n \n{\n\n                    \nimage\n \ncaladreas/mkdocs-docker-build-container\n\n                    \nlabel\n \ndocker\n\n                \n}\n\n            \n}\n\n            \nsteps\n \n{\n\n                \nsh\n \ncd\n \ndocs-scripts\n \n \nmkdocs\n \nbuild\n\n            \n}\n\n        \n}\n\n        \nstage(\nPrepare\n \nDocker\n \nImage\n)\n{\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nenvironment\n \n{\n\n                \nDOCKER_CRED\n \n=\n \ncredentials(\nldap\n)\n\n            \n}\n\n            \nsteps\n \n{\n\n                \nparallel\n \n(\n\n                        \nTestDockerfile:\n \n{\n\n                            \nscript\n \n{\n\n                                \ndef\n \nlintResult\n \n=\n \nsh\n \nreturnStdout:\n \ntrue,\n \nscript:\n \ncd\n \ndocs-scripts\n \n \ndocker\n \nrun\n \n--rm\n \n-i\n \nlukasmartinelli/hadolint\n \n \nDockerfile\n\n                                \nif\n \n(lintResult.trim()\n \n==\n \n)\n \n{\n\n                                    \nprintln\n \nLint\n \nfinished\n \nwith\n \nno\n \nerrors\n\n                                \n}\n \nelse\n \n{\n\n                                    \nprintln\n \nError\n \nfound\n \nin\n \nLint\n\n                                    \nprintln\n \n${lintResult}\n\n                                    \ncurrentBuild.result\n \n=\n \nUNSTABLE\n\n                                \n}\n\n                            \n}\n\n                        \n},\n \n//\n \nend\n \ntest\n \ndockerfile\n\n                        \nBuildImage:\n \n{\n\n                            \nsh\n \nchmod\n \n+x\n \ndocs-scripts/build.sh\n\n                            \nsh\n \ncd\n \ndocs-scripts\n \n \n./build.sh\n\n                        \n}\n,\n\n                        \nlogin:\n \n{\n\n                            \nsh\n \ndocker login -u ${DOCKER_CRED_USR} -p ${DOCKER_CRED_PSW} registry\n\n                        \n}\n\n                \n)\n\n            \n}\n\n            \npost\n \n{\n\n                \nsuccess\n \n{\n\n                    \nsh\n \nchmod\n \n+x\n \npush.sh\n\n                    \nsh\n \n./push.sh\n\n                \n}\n\n            \n}\n\n        \n}\n\n    \n}\n\n\n}", 
            "title": "MKDocs (Static Website Generator)"
        }, 
        {
            "location": "/other/mkdocs/#mkdocs", 
            "text": "This website is build using the following:   MKDocs  a python tool for building static websites from  MarkDown  files  MK Material  expansion/theme of MK Docs that makes it a responsive website with Google's Material theme", 
            "title": "MKDocs"
        }, 
        {
            "location": "/other/mkdocs/#add-information-to-the-docs", 
            "text": "MKDocs can be a bit daunting to use, especially when extended with  MKDocs Material  and  PyMdown Extensions .  There are two parts to the site: 1) the markdown files, they're in  docs/  and 2) the site listing (mkdocs.yml) and automation scripts, these can be found in  docs-scripts/ .", 
            "title": "Add information to the docs"
        }, 
        {
            "location": "/other/mkdocs/#extends-current-page", 
            "text": "To extend a current page, simply write the MarkDown as you're used to.  For the specific extensions offered by PyMX and Material, checkout the following pages:   MKDocs Material Getting Started Guide  MKDocs Extensions  PyMdown Extensions Usage Guide", 
            "title": "Extends current page"
        }, 
        {
            "location": "/other/mkdocs/#add-a-new-page", 
            "text": "In the  docs-scripts/mkdocs.yml  you will find the site structure under the yml item of  pages .  1\n2\n3\n4\n5\n6 pages:\n- Home: index.md\n- Other Root Page: some-page.md\n- Root with children:\n  - ChildOne: root2/child1.md\n  - ChildTwo: root2/child2.md", 
            "title": "Add a new page"
        }, 
        {
            "location": "/other/mkdocs/#things-to-know", 
            "text": "All .md files that are listed in the  pages  will be translated to an HTML file and dubbed {OriginalFileName}.html  Naming a file index.md will allow you to refer to it by path without the file name  we can refer to root2 simply by  site/root2  and can omit the index. 1\n2 - Root: index.md\n- Root2: root2/index.html", 
            "title": "Things to know"
        }, 
        {
            "location": "/other/mkdocs/#build-the-site-locally", 
            "text": "As it is a Python tool, you can easily build it with Python (2.7 is recommended).  The requirements are captured in a  pip  install scripts:  docs-scripts/install.sh  where the dependencies are in  Pip's requirements.txt .  Once that is done, you can do the following:  1 mkdocs build --clean   Which will generate the site into  docs-scripts/site  where you can simply open the index.html with a browser - it is a static site.  For docker, you can use the  *.sh  scripts, or simply  run.sh  to kick of the entire build.", 
            "title": "Build the site locally"
        }, 
        {
            "location": "/other/mkdocs/#jenkins-build", 
            "text": "", 
            "title": "Jenkins build"
        }, 
        {
            "location": "/other/mkdocs/#declarative-format", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71 pipeline   { \n     agent   none \n     options   { \n         timeout(time:   10,   unit:   MINUTES ) \n         timestamps() \n         buildDiscarder(logRotator(numToKeepStr:   5 )) \n     } \n     stages   { \n         stage( Prepare ){ \n             agent   {   label   docker   } \n             steps   { \n                 deleteDir() \n             } \n         } \n         stage( Checkout ) { \n             agent   {   label   docker   } \n             steps   { \n                 checkout   scm \n                 script   { \n                     env.GIT_COMMIT_HASH   =   sh   returnStdout:   true,   script:   git   rev-parse   --verify   HEAD \n                 } \n             } \n         } \n         stage( Build   Docs )   { \n             agent   { \n                 docker   { \n                     image   caladreas/mkdocs-docker-build-container \n                     label   docker \n                 } \n             } \n             steps   { \n                 sh   cd   docs-scripts     mkdocs   build \n             } \n         } \n         stage( Prepare   Docker   Image ) { \n             agent   {   label   docker   } \n             environment   { \n                 DOCKER_CRED   =   credentials( ldap ) \n             } \n             steps   { \n                 parallel   ( \n                         TestDockerfile:   { \n                             script   { \n                                 def   lintResult   =   sh   returnStdout:   true,   script:   cd   docs-scripts     docker   run   --rm   -i   lukasmartinelli/hadolint     Dockerfile \n                                 if   (lintResult.trim()   ==   )   { \n                                     println   Lint   finished   with   no   errors \n                                 }   else   { \n                                     println   Error   found   in   Lint \n                                     println   ${lintResult} \n                                     currentBuild.result   =   UNSTABLE \n                                 } \n                             } \n                         },   //   end   test   dockerfile \n                         BuildImage:   { \n                             sh   chmod   +x   docs-scripts/build.sh \n                             sh   cd   docs-scripts     ./build.sh \n                         } , \n                         login:   { \n                             sh   docker login -u ${DOCKER_CRED_USR} -p ${DOCKER_CRED_PSW} registry \n                         } \n                 ) \n             } \n             post   { \n                 success   { \n                     sh   chmod   +x   push.sh \n                     sh   ./push.sh \n                 } \n             } \n         } \n     }  }", 
            "title": "Declarative format"
        }, 
        {
            "location": "/jenkins/", 
            "text": "Jenkins\n\n\nCloudbees Study Guide\n\n\nBase configuration\n\n\nabc\n\n\nTuning\n\n\nPlease read the following articles from Cloudbees:\n\n\n\n\nPrepare-Jenkins-for-support\n\n\ntuning-jenkins-gc-responsiveness-and-stability\n\n\nAfter-moving-a-job-symlinks-for-folders-became-actual-folders\n\n\nHow-to-disable-the-weather-column-to-resolve-instance-slowness\n\n\nAccessing-graphs-on-a-Build-History-page-can-cause-Jenkins-to-become-unresponsive\n\n\nAutoBrowser-Feature-Can-Cause-Performance-Issues\n\n\nDisk-Space-Issue-after-upgrading-Branch-API-plugin\n\n\nJVM-Memory-settings-best-practice\n\n\n\n\nPipeline as code\n\n\n\n\nThe default interaction model with Jenkins, historically, has been very web UI driven, requiring users to manually create jobs, then manually fill in the details through a web browser. This requires additional effort to create and manage jobs to test and build multiple projects, it also keeps the configuration of a job to build/test/deploy separate from the actual code being built/tested/deployed. This prevents users from applying their existing CI/CD best practices to the job configurations themselves.\n\n\nWith the introduction of the Pipeline plugin, users now can implement a project\u2019s entire build/test/deploy pipeline in a Jenkinsfile and store that alongside their code, treating their pipeline as another piece of code checked into source control.\n\n\n\n\nWe will dive into several things that come into play when writing Jenkins pipelines.\n\n\n\n\nKind of Pipeline jobs\n\n\nInfo about Pipeline DSL (a groovy DSL)\n\n\nReuse pipeline DSL scripts\n\n\nThings to keep in mind\n\n\nDo's and Don't\n\n\n\n\nResources\n\n\n\n\nPipeline Steps\n\n\nPipeline Solution\n\n\nPipeline as Code\n\n\nDzone RefCard\n\n\n\n\nType of pipeline jobs\n\n\n\n\nPipeline (inline)\n\n\nPipeline (from SCM)\n\n\nMulti-Branch Pipeline\n\n\nGitHub Organization\n\n\nBitBucket Team/Project\n\n\n\n\n\n\nDanger\n\n\nWhen using the \nstash function\n keep in mind that the copying goes from where you are now to the master.\nWhen you unstash, it will copy the files from the master to where you are building.\n\n\nWhen your pipeline runs on a node and you stash and then unstash, it will copy the files from the node to the master and then back to the node.\nThis can have a severe penalty on the performance of your pipeline when you are copying over a network.\n\n\n\n\nAPI\n\n\nJenkins has an extensive \nAPI\n allowing you to retrieve a lot of information from the server.\n\n\nPlugin\n\n\nFor this way you of course have to know how to write a plugin.\nThere are some usefull resources to get started:\n* \nhttps://github.com/joostvdg/hello-world-jenkins-pipeline-plugin\n\n* \nhttps://wiki.jenkins-ci.org/display/JENKINS/Plugin+tutorial\n\n* \nhttps://jenkins.io/blog/2016/05/25/update-plugin-for-pipeline/\n\n\nDo's and Don't\n\n\nAside from the \nDo's and Don'ts\n from Cloudbees, there are some we want to share.\n\n\nThis changes the requirement for the component identifier property, as a job may only match a single group and a job listing in a group can only match a single. Thus the easiest way to make sure everything will stay unique (template names probably don\u2019t), is to make the component identifier property unique per file - let it use the name of the project.", 
            "title": "Jenkins"
        }, 
        {
            "location": "/jenkins/#jenkins", 
            "text": "Cloudbees Study Guide", 
            "title": "Jenkins"
        }, 
        {
            "location": "/jenkins/#base-configuration", 
            "text": "abc", 
            "title": "Base configuration"
        }, 
        {
            "location": "/jenkins/#tuning", 
            "text": "Please read the following articles from Cloudbees:   Prepare-Jenkins-for-support  tuning-jenkins-gc-responsiveness-and-stability  After-moving-a-job-symlinks-for-folders-became-actual-folders  How-to-disable-the-weather-column-to-resolve-instance-slowness  Accessing-graphs-on-a-Build-History-page-can-cause-Jenkins-to-become-unresponsive  AutoBrowser-Feature-Can-Cause-Performance-Issues  Disk-Space-Issue-after-upgrading-Branch-API-plugin  JVM-Memory-settings-best-practice", 
            "title": "Tuning"
        }, 
        {
            "location": "/jenkins/#pipeline-as-code", 
            "text": "The default interaction model with Jenkins, historically, has been very web UI driven, requiring users to manually create jobs, then manually fill in the details through a web browser. This requires additional effort to create and manage jobs to test and build multiple projects, it also keeps the configuration of a job to build/test/deploy separate from the actual code being built/tested/deployed. This prevents users from applying their existing CI/CD best practices to the job configurations themselves.  With the introduction of the Pipeline plugin, users now can implement a project\u2019s entire build/test/deploy pipeline in a Jenkinsfile and store that alongside their code, treating their pipeline as another piece of code checked into source control.   We will dive into several things that come into play when writing Jenkins pipelines.   Kind of Pipeline jobs  Info about Pipeline DSL (a groovy DSL)  Reuse pipeline DSL scripts  Things to keep in mind  Do's and Don't", 
            "title": "Pipeline as code"
        }, 
        {
            "location": "/jenkins/#resources", 
            "text": "Pipeline Steps  Pipeline Solution  Pipeline as Code  Dzone RefCard", 
            "title": "Resources"
        }, 
        {
            "location": "/jenkins/#type-of-pipeline-jobs", 
            "text": "Pipeline (inline)  Pipeline (from SCM)  Multi-Branch Pipeline  GitHub Organization  BitBucket Team/Project    Danger  When using the  stash function  keep in mind that the copying goes from where you are now to the master.\nWhen you unstash, it will copy the files from the master to where you are building.  When your pipeline runs on a node and you stash and then unstash, it will copy the files from the node to the master and then back to the node.\nThis can have a severe penalty on the performance of your pipeline when you are copying over a network.", 
            "title": "Type of pipeline jobs"
        }, 
        {
            "location": "/jenkins/#api", 
            "text": "Jenkins has an extensive  API  allowing you to retrieve a lot of information from the server.", 
            "title": "API"
        }, 
        {
            "location": "/jenkins/#plugin", 
            "text": "For this way you of course have to know how to write a plugin.\nThere are some usefull resources to get started:\n*  https://github.com/joostvdg/hello-world-jenkins-pipeline-plugin \n*  https://wiki.jenkins-ci.org/display/JENKINS/Plugin+tutorial \n*  https://jenkins.io/blog/2016/05/25/update-plugin-for-pipeline/", 
            "title": "Plugin"
        }, 
        {
            "location": "/jenkins/#dos-and-dont", 
            "text": "Aside from the  Do's and Don'ts  from Cloudbees, there are some we want to share.  This changes the requirement for the component identifier property, as a job may only match a single group and a job listing in a group can only match a single. Thus the easiest way to make sure everything will stay unique (template names probably don\u2019t), is to make the component identifier property unique per file - let it use the name of the project.", 
            "title": "Do's and Don't"
        }, 
        {
            "location": "/jenkins-jobs/jobdsl/", 
            "text": "Jenkins Job DSL\n\n\nJenkins is a wonderful system for managing builds, and people love using its UI to configure jobs. Unfortunately, as the number of jobs grows, maintaining them becomes tedious, and the paradigm of using a UI falls apart. Additionally, the common pattern in this situation is to copy jobs to create new ones, these \"children\" have a habit of diverging from their original \"template\" and consequently it becomes difficult to maintain consistency between these jobs.\n\n\nThe Jenkins job-dsl-plugin attempts to solve this problem by allowing jobs to be defined with the absolute minimum necessary in a programmatic form, with the help of templates that are synced with the generated jobs. The goal is for your project to be able to define all the jobs they want to be related to their project, declaring their intent for the jobs, leaving the common stuff up to a template that were defined earlier or hidden behind the DSL.\n\n\nPipeline with folder example\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\nimport\n \nhudson.model.*\n\n\nimport\n \njenkins.model.*\n\n\n\ndef\n \ndslExamplesFolder\n \n=\n \nDSL-Examples\n\n\ndef\n \ngitLabCredentialsId\n \n=\n \njoost-flusso-gitlab-ssh\n\n\ndef\n \ngitLabUrl\n \n=\n \ngit@gitlab.flusso.nl\n\n\ndef\n \ngitLabNamespace\n \n=\n \nkeep\n\n\ndef\n \ngitLabProject\n \n=\n \nkeep-api\n\n\n\n\nif\n(!\njenkins\n.\nmodel\n.\nJenkins\n.\ninstance\n.\ngetItem\n(\ndslExamplesFolder\n))\n \n{\n\n    \n//folder doesn\nt exist because item doesn\nt exist in runtime\n\n    \n//Therefore, create the folder.\n\n    \nfolder\n(\ndslExamplesFolder\n)\n \n{\n\n        \ndisplayName\n(\nDSL Examples\n)\n\n        \ndescription\n(\nFolder for job dsl examples\n)\n\n    \n}\n\n\n}\n\n\n\ncreateMultibranchPipelineJob\n(\ngitLabCredentialsId\n,\n \ngitLabUrl\n,\n \ndslExamplesFolder\n,\n \nkeep\n,\n \nkeep-api\n)\n\n\ncreateMultibranchPipelineJob\n(\ngitLabCredentialsId\n,\n \ngitLabUrl\n,\n \ndslExamplesFolder\n,\n \nkeep\n,\n \nkeep-backend-spring\n)\n\n\ncreateMultibranchPipelineJob\n(\ngitLabCredentialsId\n,\n \ngitLabUrl\n,\n \ndslExamplesFolder\n,\n \nkeep\n,\n \nkeep-frontend\n)\n\n\n\ndef\n \ncreateMultibranchPipelineJob\n(\ndef\n \ngitLabCredentialsId\n,\n \ndef\n \ngitLabUrl\n,\n \ndef\n \nfolder\n,\n \ndef\n \ngitNamespace\n,\n \ndef\n \nproject\n)\n \n{\n\n    \nmultibranchPipelineJob\n(\n${folder}/${project}-mb\n)\n \n{\n\n        \nbranchSources\n \n{\n\n            \ngit\n \n{\n\n                \nremote\n(\n${gitLabUrl}:${gitNamespace}/${project}.git\n)\n\n                \ncredentialsId\n(\ngitLabCredentialsId\n)\n\n            \n}\n\n        \n}\n\n        \norphanedItemStrategy\n \n{\n\n            \ndiscardOldItems\n \n{\n\n                \nnumToKeep\n(\n20\n)\n\n            \n}\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nFreestyle maven job\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\ndef\n \nproject\n \n=\n \nquidryan/aws-sdk-test\n\n\ndef\n \nbranchApi\n \n=\n \nnew\n \nURL\n(\nhttps://api.github.com/repos/${project}/branches\n)\n\n\ndef\n \nbranches\n \n=\n \nnew\n \ngroovy\n.\njson\n.\nJsonSlurper\n().\nparse\n(\nbranchApi\n.\nnewReader\n())\n\n\nbranches\n.\neach\n \n{\n\n    \ndef\n \nbranchName\n \n=\n \nit\n.\nname\n\n    \ndef\n \njobName\n \n=\n \n${project}-${branchName}\n.\nreplaceAll\n(\n/\n,\n-\n)\n\n    \njob\n(\njobName\n)\n \n{\n\n        \nscm\n \n{\n\n            \ngit\n(\ngit://github.com/${project}.git\n,\n \nbranchName\n)\n\n        \n}\n\n        \nsteps\n \n{\n\n            \nmaven\n(\ntest -Dproject.name=${project}/${branchName}\n)\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nResources\n\n\n\n\nTutorial\n\n\nLive Playground\n\n\nMain DSL Commands\n\n\nAPI Viewer\n\n\n\n\nOther References\n\n\n\n\nTalks and Blogs\n\n\nUser Power Movies\n\n\nDZone article\n\n\nTesting DSL Scripts", 
            "title": "JobDSL"
        }, 
        {
            "location": "/jenkins-jobs/jobdsl/#jenkins-job-dsl", 
            "text": "Jenkins is a wonderful system for managing builds, and people love using its UI to configure jobs. Unfortunately, as the number of jobs grows, maintaining them becomes tedious, and the paradigm of using a UI falls apart. Additionally, the common pattern in this situation is to copy jobs to create new ones, these \"children\" have a habit of diverging from their original \"template\" and consequently it becomes difficult to maintain consistency between these jobs.  The Jenkins job-dsl-plugin attempts to solve this problem by allowing jobs to be defined with the absolute minimum necessary in a programmatic form, with the help of templates that are synced with the generated jobs. The goal is for your project to be able to define all the jobs they want to be related to their project, declaring their intent for the jobs, leaving the common stuff up to a template that were defined earlier or hidden behind the DSL.", 
            "title": "Jenkins Job DSL"
        }, 
        {
            "location": "/jenkins-jobs/jobdsl/#pipeline-with-folder-example", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38 import   hudson.model.*  import   jenkins.model.*  def   dslExamplesFolder   =   DSL-Examples  def   gitLabCredentialsId   =   joost-flusso-gitlab-ssh  def   gitLabUrl   =   git@gitlab.flusso.nl  def   gitLabNamespace   =   keep  def   gitLabProject   =   keep-api  if (! jenkins . model . Jenkins . instance . getItem ( dslExamplesFolder ))   { \n     //folder doesn t exist because item doesn t exist in runtime \n     //Therefore, create the folder. \n     folder ( dslExamplesFolder )   { \n         displayName ( DSL Examples ) \n         description ( Folder for job dsl examples ) \n     }  }  createMultibranchPipelineJob ( gitLabCredentialsId ,   gitLabUrl ,   dslExamplesFolder ,   keep ,   keep-api )  createMultibranchPipelineJob ( gitLabCredentialsId ,   gitLabUrl ,   dslExamplesFolder ,   keep ,   keep-backend-spring )  createMultibranchPipelineJob ( gitLabCredentialsId ,   gitLabUrl ,   dslExamplesFolder ,   keep ,   keep-frontend )  def   createMultibranchPipelineJob ( def   gitLabCredentialsId ,   def   gitLabUrl ,   def   folder ,   def   gitNamespace ,   def   project )   { \n     multibranchPipelineJob ( ${folder}/${project}-mb )   { \n         branchSources   { \n             git   { \n                 remote ( ${gitLabUrl}:${gitNamespace}/${project}.git ) \n                 credentialsId ( gitLabCredentialsId ) \n             } \n         } \n         orphanedItemStrategy   { \n             discardOldItems   { \n                 numToKeep ( 20 ) \n             } \n         } \n     }  }", 
            "title": "Pipeline with folder example"
        }, 
        {
            "location": "/jenkins-jobs/jobdsl/#freestyle-maven-job", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 def   project   =   quidryan/aws-sdk-test  def   branchApi   =   new   URL ( https://api.github.com/repos/${project}/branches )  def   branches   =   new   groovy . json . JsonSlurper (). parse ( branchApi . newReader ())  branches . each   { \n     def   branchName   =   it . name \n     def   jobName   =   ${project}-${branchName} . replaceAll ( / , - ) \n     job ( jobName )   { \n         scm   { \n             git ( git://github.com/${project}.git ,   branchName ) \n         } \n         steps   { \n             maven ( test -Dproject.name=${project}/${branchName} ) \n         } \n     }  }", 
            "title": "Freestyle maven job"
        }, 
        {
            "location": "/jenkins-jobs/jobdsl/#resources", 
            "text": "Tutorial  Live Playground  Main DSL Commands  API Viewer", 
            "title": "Resources"
        }, 
        {
            "location": "/jenkins-jobs/jobdsl/#other-references", 
            "text": "Talks and Blogs  User Power Movies  DZone article  Testing DSL Scripts", 
            "title": "Other References"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/", 
            "text": "Jenkins Job Builder\n\n\nThe configuration setup of Jenkins Job Builder is composed of two main categories. Basic configuration and job configuration. Job configuration can be further split into several sub categories.\n\n\nBasic Configuration\n\n\nIn the basic configuration you will have to specify how the Jenkins Job Builder CLI can connect to the Jenkins instance you want to configure and how it should act.\n\n\nTo use such a configuration file, you add --conf \n to the CLI command.\n\n\nExample:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nlocalhost.ini\n[job_builder]\nignore_cache=True\nkeep_descriptions=False\ninclude_path=.:scripts:~/git/\nrecursive=False\nexclude=.*:manual:./development\nallow_duplicates=False\n\n[jenkins]\n#user=jenkins\n#password=\nurl=http://localhost:8080/\n\n\n\n\nFor more information see \nhttp://docs.openstack.org/infra/jenkins-job-builder/installation.html\n.\n\n\nJob Configuration\n\n\nThe configuration for configuring the jobs consists of several distinct parts which can all be in the same file or can be distributed in their own respected files.\n\n\nThese different parts can also be split into two different categories, those that are strictly linked within the configuration - via template matching - and those that are separate.\n\n\nSeparate:\n* Macro\u2019s\n* Global defaults\n* Job configuration defaults\n* External configuration files\n\n\nLinked:\n* Templates\n* Groups\n* Projects\n* Job definitions\n\n\n\n\nHere\u2019s a schematic representation on how they are linked.\nExampe in YAML config:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n-\n \njob-template\n:\n\n    \nname\n:\n \n{name}-{configComponentId}-ci\n\n    \ndescription\n:\n \nCI\n \nJob\n \nof\n \n{configComponentId}\n\n    \n:\n \n*config_job_defaults\n\n    \nbuilders\n:\n\n        \n-\n \nshell\n:\n \njenkins-jobs\n \ntest\n \n-r\n \nglobal/:definitions/\n \n-o\n \ncompiled/\n\n\n-\n \njob-template\n:\n\n    \nname\n:\n \n{name}-{configComponentId}-execute\n\n    \ndescription\n:\n \nExecutor\n \nJob\n \nof\n \n{configComponentId}\n\n    \n:\n \n*config_job_defaults\n\n    \nbuilders\n:\n\n        \n-\n \nshell\n:\n \njenkins-jobs\n \n--conf\n \nconfiguration/localhost.ini\n \nupdate\n \ndefinitions/\n\n\n-\n \njob-group\n:\n\n    \nname\n:\n \n{name}-config\n\n    \ngitlab-user\n:\n \njvandergriendt\n\n    \njobs\n:\n\n        \n-\n \n{name}-{configComponentId}-ci\n:\n\n        \n-\n \n{name}-{configComponentId}-execute\n:\n\n\n\n-\n \nproject\n:\n\n    \nname\n:\n \nRnD-Config\n\n    \njobs\n:\n\n        \n-\n \n{name}-config\n:\n\n            \nconfigComponentId\n:\n \nJenkinsJobDefinitions\n\n\n\n\n\nThe above will result in the following jobs:\nRnD-Config-JenkinsJobDefinitions-ci\nRnD-Config-JenkinsJobDefinitions-execute\n\n\nMacro\u2019s\n\n\nMacro\u2019s are what the name implies, a group of related commands which can be invoked by the group. In Jenkins Job Builder this means you can define specific configurations for a component type (e.g. builders, paramters, publishes etc).\n\n\nA component has a name and a macro name. In general the component name is plural and the macro name is singular. As can be seen in the examples below.\n\n\nHere\u2019s an example:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n# The \nadd\n macro takes a \nnumber\n parameter and will creates a\n\n\n# job which prints \nAdding \n followed by the \nnumber\n parameter:\n\n\n-\n \nbuilder\n:\n\n    \nname\n:\n \nadd\n\n    \nbuilders\n:\n\n     \n-\n \nshell\n:\n \necho\n \nAdding\n \n{number}\n\n\n\n# A specialized macro \naddtwo\n reusing the \nadd\n macro but with\n\n\n# a \nnumber\n parameter hardcoded to \ntwo\n:\n\n\n-\n \nbuilder\n:\n\n    \nname\n:\n \naddtwo\n\n    \nbuilders\n:\n\n     \n-\n \nadd\n:\n\n        \nnumber\n:\n \ntwo\n\n\n\n# Glue to have Jenkins Job Builder to expand this YAML example:\n\n\n-\n \njob\n:\n\n    \nname\n:\n \ntestingjob\n\n    \nbuilders\n:\n\n     \n# The specialized macro:\n\n     \n-\n \naddtwo\n\n     \n# Generic macro call with a parameter\n\n     \n-\n \nadd\n:\n\n        \nnumber\n:\n \nZERO\n\n     \n# Generic macro called without a parameter. Never do this!\n\n     \n# See below for the resulting wrong output :(\n\n     \n-\n \nadd\n\n\n\n\n\n\nTo expand the schematic representation, you will get the following.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n-\n \nbuilder\n:\n\n    \nname\n:\n \ntest\n\n    \nbuilders\n:\n\n     \n-\n \nshell\n:\n \njenkins-jobs\n \ntest\n \n-r\n \nglobal/:definitions/\n \n-o\n \ncompiled/\n\n\n\n-\n \nbuilder\n:\n\n    \nname\n:\n \nupdate\n\n    \nbuilders\n:\n\n     \n-\n \nshell\n:\n \njenkins-jobs\n \n--conf\n \nconfig.ini\n \nupdate\n \n-r\n \nglobal/:definitions/\n\n\n\n-\n \njob-template\n:\n\n    \nname\n:\n \n{name}-{configComponentId}-ci\n\n    \n:\n \n*config_job_defaults\n\n    \nbuilders\n:\n\n        \n-\n \ntest\n\n\n\n-\n \njob-template\n:\n\n    \nname\n:\n \n{name}-{configComponentId}-update\n\n    \n:\n \n*config_job_defaults\n\n    \nbuilders\n:\n\n        \n-\n \nupdate\n\n\n\n\n\n\n\nGlobal defaults\n\n\nGlobal defaults are defaults that should be global for the jobs you configure for a certain environment. It is the job counterpart of the basic configuration, usually containing variables for the specific environment. For example, url\u2019s, credential id\u2019s, JDK\u2019s etc.\n\n\nExample:\n\n1\n2\n3\n4\n5\n6\n7\n8\nglobal-defaults-localhost.yaml\n\n\n- defaults\n:\n\n    \nname\n:\n \nglobal\n\n    \nflusso-gitlab-url\n:\n \nhttps://gitlab.flusso.nl\n\n    \nnexus-npm-url\n:\n \nhttp://localhost:8081/nexus/content/repositories/npm-internal\n\n    \ndefault-jdk\n:\n \nJDK 1.8\n\n    \njenkinsJobsDefinitionJobName\n:\n \nRnD-Config-JenkinsJobDefinitions-ci\n\n    \ncredentialsId\n:\n \n4f0dfb96-a7b1-421c-a4ea-b6a154f91b08\n\n\n\n\n\n\nJob configuration defaults\n\n\nJob configuration defaults are nothing specific on their own. It refers to using a build in structure from YAML to create basic building blocks to be used by other configuration parts, usually the Templates.\n\n\nExample (definition):\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n-\n \nconfig_job_defaults\n:\n \nconfig_job_defaults\n\n    \nname\n:\n \nconfig_job_defaults\n\n    \nproject-type\n:\n \nfreestyle\n\n    \ndisabled\n:\n \nfalse\n\n    \nlogrotate\n:\n\n        \ndaysToKeep\n:\n \n7\n\n        \nnumToKeep\n:\n \n5\n\n        \nartifactDaysToKeep\n:\n \n-1\n\n        \nartifactNumToKeep\n:\n \n-1\n\n    \njdk\n:\n \n{default-jdk}\n\n\n\n\n\nExample (usage):\n\n1\n2\n3\n-\n \njob-template\n:\n\n    \nname\n:\n \n{name}-{configComponentId}-ci\n\n    \n:\n \n*config_job_defaults\n\n\n\n\n\n\nTemplates\n\n\nTemplates are used to define job templates. You define the entirety of the job using global defaults, configuration defaults and where useful refer to placeholders to be filled in by the other downstream configuration items.\n\n\nYou can configure almost every plugin that is available for Jenkins, these are divided in subdivisions which reflect the Jenkins\u2019 job definition sections.\n\n\nFor these subdivision and the available plugins see: \nhttp://docs.openstack.org/infra/jenkins-job-builder/definition.html#modules\n\n\nFor those plugins that are not supported, you can include the raw XML generated by the plugin. For how to do this, see: \nhttp://docs.openstack.org/infra/jenkins-job-builder/definition.html#raw-config\n\n\nExample:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n-\n \njob-template\n:\n\n    \nname\n:\n \n{name}-{configComponentId}-ci\n\n    \ndisplay-name\n:\n \n{name}-{configComponentId}-ci\n\n    \ndescription\n:\n \nCI\n \nJob\n \nof\n \n{configComponentId}\n\n    \n:\n \n*config_job_defaults\n\n    \nbuilders\n:\n\n        \n-\n \nshell\n:\n \njenkins-jobs\n \ntest\n \n-r\n \nglobal/:definitions/\n \n-o\n \ncompiled/\n\n    \npublishers\n:\n\n        \n-\n \narchive\n:\n\n            \nartifacts\n:\n \n{filesToArchive_1}\n\n            \nfingerprint\n:\n \ntrue\n\n        \n-\n \narchive\n:\n\n            \nartifacts\n:\n \n{filesToArchive_2}\n\n            \nfingerprint\n:\n \ntrue\n\n        \n-\n \nemail\n:\n\n            \nnotify-every-unstable-build\n:\n \ntrue\n\n            \nsend-to-individuals\n:\n \ntrue\n\n\n\n\n\n\nGroups\n\n\nGroups are used to group together related components that require the same set of jobs. Where you can also specify a similar set of properties, for example, a different JDK to be used.\n\n\nThe name property is mandatory and will be used to match Job definitions.\nThe jobs property is also mandatory and will be used to match Templates for which a Job will be generated per matching Job definition.\n\n\nExample\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n-\n \njob-group\n:\n\n    \nname\n:\n \n{name}-gulp\n\n    \ngitlab-user\n:\n \njvandergriendt\n\n    \nartifactId\n:\n \n{gulpComponentId}\n\n    \njobs\n:\n\n        \n-\n \n{name}-{gulpComponentId}-ci\n:\n\n        \n-\n \n{name}-{gulpComponentId}-version\n:\n\n        \n-\n \n{name}-{gulpComponentId}-sonar\n:\n\n        \n-\n \n{name}-{gulpComponentId}-publish\n:\n\n        \n-\n \n{name}-{gulpComponentId}-deploy-prep\n:\n\n        \n-\n \n{name}-{gulpComponentId}-deploy\n:\n\n        \n-\n \n{name}-{gulpComponentId}-acceptance\n:\n\n\n\n\n\n\nProjects\n\n\nProjects are used to list the actual Job definitions, which via grouping and Templates get generated, and can obviously be used to define jobs for a specific project.\n\n\nThe name property is mandatory and will be passed along with a Job definition and is generally used to tie job definitions to Groups.\n\n1\n2\n3\n4\n5\n6\n-\n \nproject\n:\n\n    \nname\n:\n \nRnD-Maven\n\n    \njobs\n:\n\n        \n-\n \n{name}-keep\n:\n\n            \ngulpComponentId\n:\n \nkeep-backend\n\n            \ndisplayName\n:\n \nKeep-Backend\n\n\n\n\n\n\nJob definitions\n\n\nJob definitions are what is all about. Although they are part of the Project configuration item I treat them separately.\n\n\nYou list the jobs under a Project and start with the name of the Group it belongs to.\nAfter that, you should define at least a name component to be able to differentiate the different jobs you want. As can be seen in the above examples with the gulpComponentId.\n\n\nExternal configuration files\nSometimes you run into the situation you want to use a multi-line configuration for a plugin, or a set of commands. Or, used at in different configurations or templates.\n\n\nThen you run into the situation that it is very difficult to manage in them neatly inside YAML configuration files. For this situation you are able to simply include a text file, via a native YAML construct. See: \nhttp://docs.openstack.org/infra/jenkins-job-builder/definition.html#module-jenkins_jobs.local_yaml\n\n\nFor example\n\n1\n2\n3\n4\n5\n6\n7\n-\n \njob\n:\n\n    \nname\n:\n \ntest-job-include-raw-1\n\n    \nbuilders\n:\n\n      \n-\n \nshell\n:\n\n          \n!include-raw\n \ninclude-raw001-hello-world.sh\n\n      \n-\n \nshell\n:\n\n          \n!include-raw\n \ninclude-raw001-vars.sh\n\n\n\n\n\n\f\n\n\nUsage\n\n\nThe information to how you use the tool is very well explained in the documentation. See \nhttp://docs.openstack.org/infra/jenkins-job-builder/installation.html#running\n\nAutomated maintenance\nIf all the jobs you can administer are done via Jenkins Job Builder, you can start to automate the maintenance of these jobs.\n\n\nSimply make jobs that poll/push on the code base where you have your Jenkins Job Builder configuration files.\n\n\nExample\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n-\n \nconfig_job_defaults\n:\n \nconfig_job_defaults\n\n    \nname\n:\n \nconfig_job_defaults\n\n    \nproject-type\n:\n \nfreestyle\n\n    \ndisabled\n:\n \nfalse\n\n    \nlogrotate\n:\n\n        \ndaysToKeep\n:\n \n7\n\n        \nnumToKeep\n:\n \n5\n\n        \nartifactDaysToKeep\n:\n \n-1\n\n        \nartifactNumToKeep\n:\n \n-1\n\n    \njdk\n:\n \n{default-jdk}\n\n    \ntriggers\n:\n\n        \n-\n \npollscm\n:\n \nH/15\n \n*\n \n*\n \n*\n \n*\n\n    \nscm\n:\n\n        \n-\n \ngit\n:\n\n            \nurl\n:\n \n{flusso-gitlab-url}/{gitlab-user}/{componentGitName}.git\n\n            \ncredentials-id\n:\n \n{credentialsId}\n\n    \npublishers\n:\n\n        \n-\n \nemail\n:\n\n            \nnotify-every-unstable-build\n:\n \ntrue\n\n            \nsend-to-individuals\n:\n \ntrue\n\n\n\n-\n \njob-template\n:\n\n    \nname\n:\n \n{name}-{configComponentId}-ci\n\n    \ndisplay-name\n:\n \n{name}-{configComponentId}-ci\n\n    \ndescription\n:\n \nCI\n \nJob\n \nof\n \n{configComponentId}\n\n    \n:\n \n*config_job_defaults\n\n    \nbuilders\n:\n\n        \n-\n \nshell\n:\n \njenkins-jobs\n \ntest\n \n-r\n \nglobal/:definitions/\n \n-o\n \ncompiled/\n\n    \npublishers\n:\n\n        \n-\n \narchive\n:\n\n            \nartifacts\n:\n \n{filesToArchive_1}\n\n            \nfingerprint\n:\n \ntrue\n\n        \n-\n \narchive\n:\n\n            \nartifacts\n:\n \n{filesToArchive_2}\n\n            \nfingerprint\n:\n \ntrue\n\n        \n-\n \nemail\n:\n\n            \nnotify-every-unstable-build\n:\n \ntrue\n\n            \nsend-to-individuals\n:\n \ntrue\n\n\n\n-\n \njob-template\n:\n\n    \nname\n:\n \n{name}-{configComponentId}-x\n\n    \ndisplay-name\n:\n \n{name}-{configComponentId}-execute\n\n    \ndescription\n:\n \nExecutor\n \nJob\n \nof\n \n{configComponentId},\n \nit\n \nwill\n \nexecute\n \nthe\n \nupdate\n \nand\n \ndelete\n \nold\n \ncommand\n\n    \n:\n \n*config_job_defaults\n\n    \nbuilders\n:\n\n        \n-\n \nshell\n:\n \njenkins-jobs\n \n--conf\n \nconfiguration/localhost.ini\n \nupdate\n \n--delete-old\n \n-r\n \nglobal/:definitions/\n\n\n\n-\n \njob-group\n:\n\n    \nname\n:\n \n{name}-config\n\n    \ngitlab-user\n:\n \njvandergriendt\n\n    \njobs\n:\n\n        \n-\n \n{name}-{configComponentId}-ci\n:\n\n        \n-\n \n{name}-{configComponentId}-x\n:\n\n\n\n-\n \nproject\n:\n\n    \nname\n:\n \nRnD-Config\n\n    \njobs\n:\n\n        \n-\n \n{name}-config\n:\n\n            \nconfigComponentId\n:\n \nJenkinsJobDefinitions\n\n            \ncomponentGitName\n:\n \njenkins-job-definitions\n\n            \nfilesToArchive_1\n:\n \nscripts/*.sh\n\n            \nfilesToArchive_2\n:\n \nmaven/settings.xml\n\n\n\n\n\n\nTips \n Trick\n\n\nAs the documentation is so extensive, it can sometimes be difficult to figure out what would be a good way to deal with some constructs.\nComponent identifier property\nOne important thing to keep in mind is that in order to create a whole set of jobs via the groups and templates it imperative to have a component* identifier property.\n\n\nThis way you can define hundreds of jobs in a project, dozens of groups and dozens of templates and generate thousands of unique individual jobs. Scale does not actually matter in this case, if you have more than one job in a project you will need this property. If the jobs that will be generated will not differ the execution will fail.\n\n\n\n\nBulk\n\n\nyou can combine multiple files or even entire folder structures together in a single call. For example, if you manage all the jobs of a company or a department and configure them in separate files.\n\n\n\n\nFor example\n\n1\njenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/", 
            "title": "JenkinsJobsBuilder"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#jenkins-job-builder", 
            "text": "The configuration setup of Jenkins Job Builder is composed of two main categories. Basic configuration and job configuration. Job configuration can be further split into several sub categories.", 
            "title": "Jenkins Job Builder"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#basic-configuration", 
            "text": "In the basic configuration you will have to specify how the Jenkins Job Builder CLI can connect to the Jenkins instance you want to configure and how it should act.  To use such a configuration file, you add --conf   to the CLI command.  Example:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 localhost.ini\n[job_builder]\nignore_cache=True\nkeep_descriptions=False\ninclude_path=.:scripts:~/git/\nrecursive=False\nexclude=.*:manual:./development\nallow_duplicates=False\n\n[jenkins]\n#user=jenkins\n#password=\nurl=http://localhost:8080/  \nFor more information see  http://docs.openstack.org/infra/jenkins-job-builder/installation.html .", 
            "title": "Basic Configuration"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#job-configuration", 
            "text": "The configuration for configuring the jobs consists of several distinct parts which can all be in the same file or can be distributed in their own respected files.  These different parts can also be split into two different categories, those that are strictly linked within the configuration - via template matching - and those that are separate.  Separate:\n* Macro\u2019s\n* Global defaults\n* Job configuration defaults\n* External configuration files  Linked:\n* Templates\n* Groups\n* Projects\n* Job definitions   Here\u2019s a schematic representation on how they are linked.\nExampe in YAML config:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 -   job-template : \n     name :   {name}-{configComponentId}-ci \n     description :   CI   Job   of   {configComponentId} \n     :   *config_job_defaults \n     builders : \n         -   shell :   jenkins-jobs   test   -r   global/:definitions/   -o   compiled/  -   job-template : \n     name :   {name}-{configComponentId}-execute \n     description :   Executor   Job   of   {configComponentId} \n     :   *config_job_defaults \n     builders : \n         -   shell :   jenkins-jobs   --conf   configuration/localhost.ini   update   definitions/  -   job-group : \n     name :   {name}-config \n     gitlab-user :   jvandergriendt \n     jobs : \n         -   {name}-{configComponentId}-ci : \n         -   {name}-{configComponentId}-execute :  -   project : \n     name :   RnD-Config \n     jobs : \n         -   {name}-config : \n             configComponentId :   JenkinsJobDefinitions   \nThe above will result in the following jobs:\nRnD-Config-JenkinsJobDefinitions-ci\nRnD-Config-JenkinsJobDefinitions-execute", 
            "title": "Job Configuration"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#macros", 
            "text": "Macro\u2019s are what the name implies, a group of related commands which can be invoked by the group. In Jenkins Job Builder this means you can define specific configurations for a component type (e.g. builders, paramters, publishes etc).  A component has a name and a macro name. In general the component name is plural and the macro name is singular. As can be seen in the examples below.  Here\u2019s an example:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27 # The  add  macro takes a  number  parameter and will creates a  # job which prints  Adding   followed by the  number  parameter:  -   builder : \n     name :   add \n     builders : \n      -   shell :   echo   Adding   {number}  # A specialized macro  addtwo  reusing the  add  macro but with  # a  number  parameter hardcoded to  two :  -   builder : \n     name :   addtwo \n     builders : \n      -   add : \n         number :   two  # Glue to have Jenkins Job Builder to expand this YAML example:  -   job : \n     name :   testingjob \n     builders : \n      # The specialized macro: \n      -   addtwo \n      # Generic macro call with a parameter \n      -   add : \n         number :   ZERO \n      # Generic macro called without a parameter. Never do this! \n      # See below for the resulting wrong output :( \n      -   add    To expand the schematic representation, you will get the following.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 -   builder : \n     name :   test \n     builders : \n      -   shell :   jenkins-jobs   test   -r   global/:definitions/   -o   compiled/  -   builder : \n     name :   update \n     builders : \n      -   shell :   jenkins-jobs   --conf   config.ini   update   -r   global/:definitions/  -   job-template : \n     name :   {name}-{configComponentId}-ci \n     :   *config_job_defaults \n     builders : \n         -   test  -   job-template : \n     name :   {name}-{configComponentId}-update \n     :   *config_job_defaults \n     builders : \n         -   update", 
            "title": "Macro\u2019s"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#global-defaults", 
            "text": "Global defaults are defaults that should be global for the jobs you configure for a certain environment. It is the job counterpart of the basic configuration, usually containing variables for the specific environment. For example, url\u2019s, credential id\u2019s, JDK\u2019s etc.  Example: 1\n2\n3\n4\n5\n6\n7\n8 global-defaults-localhost.yaml  - defaults : \n     name :   global \n     flusso-gitlab-url :   https://gitlab.flusso.nl \n     nexus-npm-url :   http://localhost:8081/nexus/content/repositories/npm-internal \n     default-jdk :   JDK 1.8 \n     jenkinsJobsDefinitionJobName :   RnD-Config-JenkinsJobDefinitions-ci \n     credentialsId :   4f0dfb96-a7b1-421c-a4ea-b6a154f91b08", 
            "title": "Global defaults"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#job-configuration-defaults", 
            "text": "Job configuration defaults are nothing specific on their own. It refers to using a build in structure from YAML to create basic building blocks to be used by other configuration parts, usually the Templates.  Example (definition):  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 -   config_job_defaults :   config_job_defaults \n     name :   config_job_defaults \n     project-type :   freestyle \n     disabled :   false \n     logrotate : \n         daysToKeep :   7 \n         numToKeep :   5 \n         artifactDaysToKeep :   -1 \n         artifactNumToKeep :   -1 \n     jdk :   {default-jdk}   \nExample (usage): 1\n2\n3 -   job-template : \n     name :   {name}-{configComponentId}-ci \n     :   *config_job_defaults", 
            "title": "Job configuration defaults"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#templates", 
            "text": "Templates are used to define job templates. You define the entirety of the job using global defaults, configuration defaults and where useful refer to placeholders to be filled in by the other downstream configuration items.  You can configure almost every plugin that is available for Jenkins, these are divided in subdivisions which reflect the Jenkins\u2019 job definition sections.  For these subdivision and the available plugins see:  http://docs.openstack.org/infra/jenkins-job-builder/definition.html#modules  For those plugins that are not supported, you can include the raw XML generated by the plugin. For how to do this, see:  http://docs.openstack.org/infra/jenkins-job-builder/definition.html#raw-config  Example:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 -   job-template : \n     name :   {name}-{configComponentId}-ci \n     display-name :   {name}-{configComponentId}-ci \n     description :   CI   Job   of   {configComponentId} \n     :   *config_job_defaults \n     builders : \n         -   shell :   jenkins-jobs   test   -r   global/:definitions/   -o   compiled/ \n     publishers : \n         -   archive : \n             artifacts :   {filesToArchive_1} \n             fingerprint :   true \n         -   archive : \n             artifacts :   {filesToArchive_2} \n             fingerprint :   true \n         -   email : \n             notify-every-unstable-build :   true \n             send-to-individuals :   true", 
            "title": "Templates"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#groups", 
            "text": "Groups are used to group together related components that require the same set of jobs. Where you can also specify a similar set of properties, for example, a different JDK to be used.  The name property is mandatory and will be used to match Job definitions.\nThe jobs property is also mandatory and will be used to match Templates for which a Job will be generated per matching Job definition.  Example  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 -   job-group : \n     name :   {name}-gulp \n     gitlab-user :   jvandergriendt \n     artifactId :   {gulpComponentId} \n     jobs : \n         -   {name}-{gulpComponentId}-ci : \n         -   {name}-{gulpComponentId}-version : \n         -   {name}-{gulpComponentId}-sonar : \n         -   {name}-{gulpComponentId}-publish : \n         -   {name}-{gulpComponentId}-deploy-prep : \n         -   {name}-{gulpComponentId}-deploy : \n         -   {name}-{gulpComponentId}-acceptance :", 
            "title": "Groups"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#projects", 
            "text": "Projects are used to list the actual Job definitions, which via grouping and Templates get generated, and can obviously be used to define jobs for a specific project.  The name property is mandatory and will be passed along with a Job definition and is generally used to tie job definitions to Groups. 1\n2\n3\n4\n5\n6 -   project : \n     name :   RnD-Maven \n     jobs : \n         -   {name}-keep : \n             gulpComponentId :   keep-backend \n             displayName :   Keep-Backend", 
            "title": "Projects"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#job-definitions", 
            "text": "Job definitions are what is all about. Although they are part of the Project configuration item I treat them separately.  You list the jobs under a Project and start with the name of the Group it belongs to.\nAfter that, you should define at least a name component to be able to differentiate the different jobs you want. As can be seen in the above examples with the gulpComponentId.  External configuration files\nSometimes you run into the situation you want to use a multi-line configuration for a plugin, or a set of commands. Or, used at in different configurations or templates.  Then you run into the situation that it is very difficult to manage in them neatly inside YAML configuration files. For this situation you are able to simply include a text file, via a native YAML construct. See:  http://docs.openstack.org/infra/jenkins-job-builder/definition.html#module-jenkins_jobs.local_yaml  For example 1\n2\n3\n4\n5\n6\n7 -   job : \n     name :   test-job-include-raw-1 \n     builders : \n       -   shell : \n           !include-raw   include-raw001-hello-world.sh \n       -   shell : \n           !include-raw   include-raw001-vars.sh", 
            "title": "Job definitions"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#usage", 
            "text": "The information to how you use the tool is very well explained in the documentation. See  http://docs.openstack.org/infra/jenkins-job-builder/installation.html#running \nAutomated maintenance\nIf all the jobs you can administer are done via Jenkins Job Builder, you can start to automate the maintenance of these jobs.  Simply make jobs that poll/push on the code base where you have your Jenkins Job Builder configuration files.  Example  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62 -   config_job_defaults :   config_job_defaults \n     name :   config_job_defaults \n     project-type :   freestyle \n     disabled :   false \n     logrotate : \n         daysToKeep :   7 \n         numToKeep :   5 \n         artifactDaysToKeep :   -1 \n         artifactNumToKeep :   -1 \n     jdk :   {default-jdk} \n     triggers : \n         -   pollscm :   H/15   *   *   *   * \n     scm : \n         -   git : \n             url :   {flusso-gitlab-url}/{gitlab-user}/{componentGitName}.git \n             credentials-id :   {credentialsId} \n     publishers : \n         -   email : \n             notify-every-unstable-build :   true \n             send-to-individuals :   true  -   job-template : \n     name :   {name}-{configComponentId}-ci \n     display-name :   {name}-{configComponentId}-ci \n     description :   CI   Job   of   {configComponentId} \n     :   *config_job_defaults \n     builders : \n         -   shell :   jenkins-jobs   test   -r   global/:definitions/   -o   compiled/ \n     publishers : \n         -   archive : \n             artifacts :   {filesToArchive_1} \n             fingerprint :   true \n         -   archive : \n             artifacts :   {filesToArchive_2} \n             fingerprint :   true \n         -   email : \n             notify-every-unstable-build :   true \n             send-to-individuals :   true  -   job-template : \n     name :   {name}-{configComponentId}-x \n     display-name :   {name}-{configComponentId}-execute \n     description :   Executor   Job   of   {configComponentId},   it   will   execute   the   update   and   delete   old   command \n     :   *config_job_defaults \n     builders : \n         -   shell :   jenkins-jobs   --conf   configuration/localhost.ini   update   --delete-old   -r   global/:definitions/  -   job-group : \n     name :   {name}-config \n     gitlab-user :   jvandergriendt \n     jobs : \n         -   {name}-{configComponentId}-ci : \n         -   {name}-{configComponentId}-x :  -   project : \n     name :   RnD-Config \n     jobs : \n         -   {name}-config : \n             configComponentId :   JenkinsJobDefinitions \n             componentGitName :   jenkins-job-definitions \n             filesToArchive_1 :   scripts/*.sh \n             filesToArchive_2 :   maven/settings.xml", 
            "title": "Usage"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#tips-trick", 
            "text": "As the documentation is so extensive, it can sometimes be difficult to figure out what would be a good way to deal with some constructs.\nComponent identifier property\nOne important thing to keep in mind is that in order to create a whole set of jobs via the groups and templates it imperative to have a component* identifier property.  This way you can define hundreds of jobs in a project, dozens of groups and dozens of templates and generate thousands of unique individual jobs. Scale does not actually matter in this case, if you have more than one job in a project you will need this property. If the jobs that will be generated will not differ the execution will fail.   Bulk  you can combine multiple files or even entire folder structures together in a single call. For example, if you manage all the jobs of a company or a department and configure them in separate files.   For example 1 jenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/", 
            "title": "Tips &amp; Trick"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/", 
            "text": "Core Concepts\n\n\nBelow are some core concepts to understand before building pipelines in Jenkins.\n\n\n\n\nPipeline as Code\n\n\nStep\n\n\nMaster vs Nodes\n\n\nCheckout\n\n\nWorkspace\n\n\nStage\n\n\nSandbox and Script Security\n\n\nJava vs. Groovy\n\n\nEnv (object)\n\n\nStash \n archive\n\n\nCredentials\n\n\nTools \n Build Environment\n\n\nPipeline Syntax Page\n\n\n\n\nTerminology\n\n\nThe terminology used in this page is based upon the terms used by Cloudbees as related to Jenkins.\n\n\nIf in doubt, please consult the \nJenkins Glossary\n. \n\n\nPipeline as Code\n\n\nStep\n\n\n\n\nA single task; fundamentally steps tell Jenkins what to do inside of a Pipeline or Project.\n\n\n\n\nConsider the following piece of pipeline code:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nnode\n \n{\n\n    \ntimestamps\n \n{\n\n        \nstage\n \n(\nMy\n \nFIrst\n \nStage\n)\n \n{\n\n\n            \nif\n \n(\nisUnix\n())\n \n{\n\n\n                \nsh\n \necho\n \nthis is Unix!\n\n\n            \n}\n \nelse\n \n{\n\n\n                \nbat\n \necho\n \nthis is windows\n\n\n            \n}\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nThe only execution that happens (almost) exclusively on the node (or build slave) are the \nisUnix()\n, \nsh\n and \nbat\n shell commands.\n\n\nThose specific tasks are the steps in pipeline code.\n\n\nMaster vs Nodes\n\n\nThere are many things to keep in mind about Pipelines in Jenkins. \nBy far the most important are those related to the distinction between Masters and Nodes.\n\n\nAside from the points below, the key thing to keep in mind: Nodes (build slaves) are designed to executes task, Masters are not.\n\n\n\n\n\n\nExcept for the steps themselves, \nall\n of the Pipeline logic, the Groovy conditionals, loops, etc \nexecute on the master\n. Whether simple or complex! Even \ninside a node block\n!\n\n\n\n\n\n\nSteps may use executors to do work where appropriate, but each step has a small on-master overhead too.\n\n\n\n\n\n\nPipeline code is written as Groovy but the execution model is radically transformed at compile-time to Continuation Passing Style (CPS).\n\n\n\n\n\n\nThis transformation provides valuable safety and durability guarantees for Pipelines, but it comes with trade-offs:\n\n\n\n\nSteps can invoke Java and execute fast and efficiently, but Groovy is much slower to run than normal.\n\n\nGroovy logic requires far more memory, because an object-based syntax/block tree is kept in memory.\n\n\n\n\n\n\n\n\nPipelines persist the program and its state frequently to be able to survive failure of the master.\n\n\n\n\n\n\nSource: \nSam van Oort\n, Cloudbees Engineer\n\n\nNode\n\n\n\n\nA machine which is part of the Jenkins environment and capable of executing Pipelines or Projects. Both the Master and Agents are considered to be Nodes.\n\n\n\n\nMaster\n\n\n\n\nThe central, coordinating process which stores configuration, loads plugins, and renders the various user interfaces for Jenkins.\n\n\n\n\nWhat to do?\n\n\nSo, if Pipeline code can cause big loads on Master, what should we do than?\n\n\n\n\nTry to limit the use of logic in your groovy code\n\n\nAvoid blocking or I/O calls unless explicitly done on a slave via a Step\n\n\nIf you need heavy processing, and there isn't a Step, create either a \n\n\nplugin\n \n\n\nShared Library\n\n\nOr use a CLI tool via a platform independent language, such as Java or Go\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nIf need to do any I/O, use a plugin or anything related to a workspace, you \nneed\n a node.\nIf you only need to interact with variables, for example for an input form, do this \noutside\n of a node block.\nSee \nPipeline Input\n for how that works.\n\n\n\n\nWorkspace\n\n\n\n\nA disposable directory on the file system of a Node where work can be done by a Pipeline or Project. Workspaces are typically left in place after a Build or Pipeline run completes unless specific Workspace cleanup policies have been put in place on the Jenkins Master.\n\n\n\n\nThe key part of the glossary entry there is \ndisposable directory\n. There are absolutely no guarantees about Workspaces in pipeline jobs.\n\n\nThat said, what you should take care of:\n\n\n\n\nalways clean your workspace before you start, you don't know the state of the folder you get\n\n\nalways clean your workspace after you finish, this way you're less likely to run into problems in subsequent builds\n\n\na workspace is a temporary folder on a single node's filesystem: so every time you use \nnode{}\n you have a new workspace\n\n\nafter your build is finish or leaving the node otherwise, your workspace should be considered gone: need something from? stash or archive it!\n\n\n\n\nCheckout\n\n\nThere are several ways to do a checkout in the Jenkins pipeline code.\n\n\nIn the groovy DSL you can use the \nCheckout\n dsl command, \nsvn\n shorthand or the \ngit\n shorthand.\n\n\n1\n2\n3\n4\n5\nnode\n \n{\n\n    \nstage\n(\nscm\n)\n \n{\n\n        \ngit\n \nhttps://github.com/joostvdg/jishi\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\n\n\nDanger\n\n\nIf you use a pipeline from SCM, multi-branch pipeline or a derived job type, beware!\nOnly the Jenkinsfile gets checked out. You still need to checkout the rest of your files yourself!\n\n\n\n\n\n\nTip\n\n\nHowever, when using pipeline from SCM, multi-branch pipeline or a derived job type.\nYou can use a shorthand: \ncheckout scm\n. This checks out the scm defined in your job (where the Jenkinsfile came from).\n    \n1\n2\n3\n4\n5\nnode\n \n{\n\n    \nstage\n(\nscm\n)\n \n{\n\n        \ncheckout\n \nscm\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\n\nStage\n\n\n\n\nStage is a step for defining a conceptually distinct subset of the entire Pipeline, for example: \"Build\", \"Test\", and \"Deploy\", which is used by many plugins to visualize or present Jenkins Pipeline status/progress.\n\n\n\n\nThe stage \"step\" has a primary function and a secondary function.\n\n\n\n\nIts primary function is to define the \nvisual boundaries\n between logically separable parts of the pipeline.\n\n\nFor example, you can define SCM, Build, QA, Deploy as stages to tell you where the build currently is or where it failed.\n\n\n\n\n\n\nThe secondary function is to provided a scope for variables.\n\n\nJust like most programming languages, code \nblocks\n are a more than just syntactic sugar, they also limit the scope of variables.\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nnode\n \n{\n\n    \nstage\n(\nSCM\n)\n \n{\n\n        \ndef\n \nmyVar\n \n=\n \nabc\n\n        \ncheckout\n \nscm\n\n    \n}\n\n    \nstage\n(\nBuild\n)\n \n{\n\n        \nsh\n \nmvn\n \nclean\n \ninstall\n\n\n        \necho\n \nmyVar\n \n#\n \nwill\n \nfail\n \nbecause\n \nthe\n \nvariable\n \ndoesn\nt\n \nexist\n \nhere\n\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nStages in classic view\n\n\n\n\nStages in Blue Ocean view\n\n\n\n\nSandbox and Script Security\n\n\nIn Jenkins some plugins - such as the pipeline plugin - allow you to write groovy code that gets executed on the master.\n\n\nThis means you could run code on the master that accesses the host machine with the same rights as Jenkins.\nAs is unsafe, Jenkins has some guards against this in the form the \nsandbox mode\n and the \nscript security\n.\n\n\nWhen you create a pipeline job, you get a inline code editor by default. \nIf you're an administrator you get the option to turn the \"sandbox\" mode of.\n\n\nIf you use a pipeline from SCM or any of the higher abstraction pipeline job types (Multibranch Pipeline, BitBucket Team) you are always running in sandbox mode.\n\n\nWhen you're in sandbox mode, your script will run past the script security. \nThis uses a whitelisting technique to block dangerous or undesired methods, but is does so in a very restrictive manner.\n\n\nIt could be you're doing something that is safe but still gets blocked.\nAn administrator can then go to the script approval page (under Jenkins Administration) and approve your script.\n\n\nFor more details, please consult \nScript Security plugin\n page.\n\n\nExample error\n\n\n1\n2\n3\n4\n5\n6\n7\n8\norg\n.\njenkinsci\n.\nplugins\n.\nscriptsecurity\n.\nsandbox\n.\nRejectedAccessException\n:\n \nunclassified\n \nstaticMethod\n \norg\n.\ntmatesoft\n.\nsvn\n.\ncore\n.\ninternal\n.\nio\n.\ndav\n.\nDAVRepositoryFactory\n \ncreate\n \norg\n.\ntmatesoft\n.\nsvn\n.\ncore\n.\nSVNURL\n\n    \nat\n \norg\n.\njenkinsci\n.\nplugins\n.\nscriptsecurity\n.\nsandbox\n.\ngroovy\n.\nSandboxInterceptor\n.\nonStaticCall\n(\nSandboxInterceptor\n.\njava\n:\n138\n)\n\n    \nat\n \norg\n.\nkohsuke\n.\ngroovy\n.\nsandbox\n.\nimpl\n.\nChecker$2\n.\ncall\n(\nChecker\n.\njava\n:\n180\n)\n\n    \nat\n \norg\n.\nkohsuke\n.\ngroovy\n.\nsandbox\n.\nimpl\n.\nChecker\n.\ncheckedStaticCall\n(\nChecker\n.\njava\n:\n177\n)\n\n    \nat\n \norg\n.\nkohsuke\n.\ngroovy\n.\nsandbox\n.\nimpl\n.\nChecker\n.\ncheckedCall\n(\nChecker\n.\njava\n:\n91\n)\n\n    \nat\n \ncom\n.\ncloudbees\n.\ngroovy\n.\ncps\n.\nsandbox\n.\nSandboxInvoker\n.\nmethodCall\n(\nSandboxInvoker\n.\njava\n:\n16\n)\n\n    \nat\n \nWorkflowScript\n.\nrun\n(\nWorkflowScript\n:\n12\n)\n\n    \nat\n \n___cps\n.\ntransform___\n(\nNative\n \nMethod\n)\n\n\n\n\n\n\n\n\n\nTip\n\n\nThere are three ways to deal with these errors.\n\n\n\n\ngo to manage jenkins \n script approval and approve the script\n\n\nuse a \nShared Library\n\n\nuse a CLI tool/script via a shell command to do what you need to do\n\n\n\n\n\n\nJava vs. Groovy\n\n\nThe pipeline code has to be written in groovy and therefor can also use java code.\nTwo big difference to note: \n\n\n\n\nthe usage of double quoted string (gstring, interpreted) and single quoted strings (literal)\n\n\ndef abc = 'xyz' # is a literal\n\n\necho '$abc' # prints $abc\n\n\necho \n$abc\n # prints xyz\n\n\n\n\n\n\nno use of \n;\n\n\n\n\nUnfortunately, due to the way the Pipeline code is processed, many of the groovy features don't work or don't work as expected.\n\n\nThings like the lambda's and for-each loops don't work well and are best avoided.\nIn these situations, it is best to keep to the standard syntax of Java.\n\n\nFor more information on how the groovy is being processed, it is best to read the \ntechnical-design\n.\n\n\nEnv (object)\n\n\nThe env object is an object that is available to use in any pipeline script.\n\n\nThe env object allows you to store objects and variables to be used anywhere during the script.\nSo things can be shared between nodes, the master and nodes and code blocks.\n\n\nWhy would you want to use it? As in general, global variables are a bad practice.\nBut if you need to have variables to be available through the execution on different machines (master, nodes) it is good to use this.\n\n\nAlso the env object contains context variables, such as BRANCH_NAME, JOB_NAME and so one.\nFor a complete overview, view the pipeline syntax page.\n\n\nDon't use the env object in functions, always feed them the parameters directly.\nOnly use it in the \"pipeline flow\" and use it for the parameters of the methods.  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\nnode\n \n{\n\n    \nstage\n(\nSCM\n)\n \n{\n\n        \ncheckout\n \nscm\n \n    \n}\n\n    \nstage\n(\nEcho\n){\n\n        \necho\n \nBranch=$env.BRANCH_NAME\n \n// will print Branch=master\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nStash \n archive\n\n\nIf you need to store files for keeping for later, there are two options available \nstash\n and \narchive\n.\n\n\nBoth should be avoided as they cause heavy I/O traffic, usually between the Node and Master.\n\n\nFor more specific information, please consult the Pipeline Syntax Page.\n\n\nStash\n\n\nStash allows you to copy files from the current workspace to a temp folder in the workspace in the master.\nIf you're currently on a different machine it will copy them one by one over the network, keep this in mind.\n\n\nThe files can only be retrieved during the pipeline execution and you can do so via the \nunstash\n command.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nnode\n(\nMachine1\n)\n \n{\n\n    \nstage\n(\nA\n)\n \n{\n\n        \n// generate some files\n\n        \nstash\n \nexcludes:\n \nsecret.txt\n,\n \nincludes:\n \n*.txt\n,\n \nname:\n \nabc\n\n    \n}\n\n\n}\n\n\nnode\n(\nMachine2\n)\n \n{\n\n    \nstage\n(\nB\n)\n \n{\n\n        \nunstash\n \nabc\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\n\n\nSaves a set of files for use later in the same build, generally on another node/workspace. Stashed files are not otherwise available and are generally discarded at the end of the build. Note that the stash and unstash steps are designed for use with small files. For large data transfers, use the External Workspace Manager plugin, or use an external repository manager such as Nexus or Artifactory.\n\n\n\n\nArchive \n archiveArtifacts\n\n\n\n\nArchives build output artifacts for later use. As of Jenkins 2.x, you may use the more configurable archiveArtifacts.\n\n\n\n\nWith archive you can store a file semi-permanently in your job. Semi as the files will be overridden by the latest build.\n\n\nThe files you archive will be stored in the Job folder on the master. \n\n\nOne usecase is to save a log file from a build tool.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nnode\n \n{\n\n    \nstage\n(\nA\n)\n \n{\n\n\n        \ntry\n \n{\n\n            \n// do some build\n\n        \n}\n \nfinally\n \n{\n\n            \n// This step should not normally be used in your script. Consult the inline help for details.\n\n            \narchive\n \nexcludes:\n \nuseless.log\n,\n \nincludes:\n \n*.log\n\n\n            \n// Use this instead, but only for permanent files, or external logfiles\n\n            \narchiveArtifacts\n \nallowEmptyArchive:\n \ntrue\n,\n \nartifacts:\n \n*.log\n,\n \nexcludes:\n \nuseless.log\n,\n \nfingerprint:\n \ntrue\n,\n \nonlyIfSuccessful:\n \ntrue\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nCredentials\n\n\nIn many pipelines you will have to deal with external systems, requiring credentials.\n\n\nJenkins has the \nCredentials API\n which you can also utilize in the pipeline.\n\n\nYou can use do this via the \nCredentials\n and \nCredentials Binding\n plugins, the first is the core plugin the second provides the integration for the pipeline.\n\n\nThe best way to generate the required code snippet, is to go to the pipeline syntax page, select \nwithCredentials\n and configure what you need.\n\n\n1\n2\n3\n4\n5\n6\n7\nnode\n \n{\n\n    \nstage\n(\nsomeRemoteCall\n)\n \n{\n\n        \nwithCredentials\n([\nusernameColonPassword\n(\ncredentialsId:\n \nsomeCredentialsId\n,\n \nvariable:\n \nUSRPASS\n)])\n \n{\n\n            \nsh\n \ncurl -u $env.USRPASS $URL\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nFor more examples, please consult Cloudbees' \nInjecting-Secrets-into-Jenkins-Build-Jobs\n blog post.\n\n\nTools \n Build Environment\n\n\nJenkins would not be Jenkins without the direct support for the build tools, such as JDK's, SDK's, Maven, Ant what have you not.\n\n\nSo, how do you use them in the pipeline?\n\n\nUnfortunately, this is a bit more cumbersome than it is in a freestyle (or \nlegacy\n) job.\n\n\nYou have to do two things:\n\n\n\n\nretrieve the tool's location via the \ntool\n DSL method\n\n\nset the environment variables to suit the tool\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nnode\n \n{\n\n    \nstage\n(\nMaven\n)\n \n{\n\n        \nString\n \njdk\n \n=\n \ntool\n \nname:\n \njdk_8\n,\n \ntype:\n \njdk\n\n        \nString\n \nmaven\n \n=\n \ntool\n \nname:\n \nmaven_3.5.0\n,\n \ntype:\n \nmaven\n\n        \nwithEnv\n([\nJAVA_HOME=$jdk\n,\n \nPATH+MAVEN=${jdk}/bin:${maven}/bin\n])\n \n{\n\n            \nsh\n \nmvn clean install\n \n        \n}\n\n\n        \n// or in one go\n\n        \nwithEnv\n([\nJAVA_HOME=${ tool \njdk_8\n }\n,\n \nPATH+MAVEN=${tool \nmaven_3.5.0\n}/bin:${env.JAVA_HOME}/bin\n])\n \n{\n\n            \nsh\n \nmvn clean install\n \n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nPipeline Syntax Page\n\n\nSoooo, do I always have to figure out how to write these code snippets?\n\n\nNo, don't worry. You don't have to.\n\n\nAt every pipeline job type there is a link called \"Pipeline Syntax\".\n\n\nThis gives you a page with a drop down menu, from where you can select all the available steps.\n\n\nOnce you select a step, you can use the UI to setup the step and then use the \ngenerate\n button to give you the correct syntax.", 
            "title": "Core Concepts"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#core-concepts", 
            "text": "Below are some core concepts to understand before building pipelines in Jenkins.   Pipeline as Code  Step  Master vs Nodes  Checkout  Workspace  Stage  Sandbox and Script Security  Java vs. Groovy  Env (object)  Stash   archive  Credentials  Tools   Build Environment  Pipeline Syntax Page", 
            "title": "Core Concepts"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#terminology", 
            "text": "The terminology used in this page is based upon the terms used by Cloudbees as related to Jenkins.  If in doubt, please consult the  Jenkins Glossary .", 
            "title": "Terminology"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#pipeline-as-code", 
            "text": "", 
            "title": "Pipeline as Code"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#step", 
            "text": "A single task; fundamentally steps tell Jenkins what to do inside of a Pipeline or Project.   Consider the following piece of pipeline code:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 node   { \n     timestamps   { \n         stage   ( My   FIrst   Stage )   {               if   ( isUnix ())   {                   sh   echo   this is Unix!               }   else   {                   bat   echo   this is windows               } \n         } \n     }  }    The only execution that happens (almost) exclusively on the node (or build slave) are the  isUnix() ,  sh  and  bat  shell commands.  Those specific tasks are the steps in pipeline code.", 
            "title": "Step"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#master-vs-nodes", 
            "text": "There are many things to keep in mind about Pipelines in Jenkins. \nBy far the most important are those related to the distinction between Masters and Nodes.  Aside from the points below, the key thing to keep in mind: Nodes (build slaves) are designed to executes task, Masters are not.    Except for the steps themselves,  all  of the Pipeline logic, the Groovy conditionals, loops, etc  execute on the master . Whether simple or complex! Even  inside a node block !    Steps may use executors to do work where appropriate, but each step has a small on-master overhead too.    Pipeline code is written as Groovy but the execution model is radically transformed at compile-time to Continuation Passing Style (CPS).    This transformation provides valuable safety and durability guarantees for Pipelines, but it comes with trade-offs:   Steps can invoke Java and execute fast and efficiently, but Groovy is much slower to run than normal.  Groovy logic requires far more memory, because an object-based syntax/block tree is kept in memory.     Pipelines persist the program and its state frequently to be able to survive failure of the master.    Source:  Sam van Oort , Cloudbees Engineer", 
            "title": "Master vs Nodes"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#node", 
            "text": "A machine which is part of the Jenkins environment and capable of executing Pipelines or Projects. Both the Master and Agents are considered to be Nodes.", 
            "title": "Node"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#master", 
            "text": "The central, coordinating process which stores configuration, loads plugins, and renders the various user interfaces for Jenkins.", 
            "title": "Master"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#what-to-do", 
            "text": "So, if Pipeline code can cause big loads on Master, what should we do than?   Try to limit the use of logic in your groovy code  Avoid blocking or I/O calls unless explicitly done on a slave via a Step  If you need heavy processing, and there isn't a Step, create either a   plugin    Shared Library  Or use a CLI tool via a platform independent language, such as Java or Go      Tip  If need to do any I/O, use a plugin or anything related to a workspace, you  need  a node.\nIf you only need to interact with variables, for example for an input form, do this  outside  of a node block.\nSee  Pipeline Input  for how that works.", 
            "title": "What to do?"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#workspace", 
            "text": "A disposable directory on the file system of a Node where work can be done by a Pipeline or Project. Workspaces are typically left in place after a Build or Pipeline run completes unless specific Workspace cleanup policies have been put in place on the Jenkins Master.   The key part of the glossary entry there is  disposable directory . There are absolutely no guarantees about Workspaces in pipeline jobs.  That said, what you should take care of:   always clean your workspace before you start, you don't know the state of the folder you get  always clean your workspace after you finish, this way you're less likely to run into problems in subsequent builds  a workspace is a temporary folder on a single node's filesystem: so every time you use  node{}  you have a new workspace  after your build is finish or leaving the node otherwise, your workspace should be considered gone: need something from? stash or archive it!", 
            "title": "Workspace"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#checkout", 
            "text": "There are several ways to do a checkout in the Jenkins pipeline code.  In the groovy DSL you can use the  Checkout  dsl command,  svn  shorthand or the  git  shorthand.  1\n2\n3\n4\n5 node   { \n     stage ( scm )   { \n         git   https://github.com/joostvdg/jishi \n     }  }     Danger  If you use a pipeline from SCM, multi-branch pipeline or a derived job type, beware!\nOnly the Jenkinsfile gets checked out. You still need to checkout the rest of your files yourself!    Tip  However, when using pipeline from SCM, multi-branch pipeline or a derived job type.\nYou can use a shorthand:  checkout scm . This checks out the scm defined in your job (where the Jenkinsfile came from).\n     1\n2\n3\n4\n5 node   { \n     stage ( scm )   { \n         checkout   scm \n     }  }", 
            "title": "Checkout"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#stage", 
            "text": "Stage is a step for defining a conceptually distinct subset of the entire Pipeline, for example: \"Build\", \"Test\", and \"Deploy\", which is used by many plugins to visualize or present Jenkins Pipeline status/progress.   The stage \"step\" has a primary function and a secondary function.   Its primary function is to define the  visual boundaries  between logically separable parts of the pipeline.  For example, you can define SCM, Build, QA, Deploy as stages to tell you where the build currently is or where it failed.    The secondary function is to provided a scope for variables.  Just like most programming languages, code  blocks  are a more than just syntactic sugar, they also limit the scope of variables.      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 node   { \n     stage ( SCM )   { \n         def   myVar   =   abc \n         checkout   scm \n     } \n     stage ( Build )   { \n         sh   mvn   clean   install           echo   myVar   #   will   fail   because   the   variable   doesn t   exist   here       }  }", 
            "title": "Stage"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#stages-in-classic-view", 
            "text": "", 
            "title": "Stages in classic view"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#stages-in-blue-ocean-view", 
            "text": "", 
            "title": "Stages in Blue Ocean view"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#sandbox-and-script-security", 
            "text": "In Jenkins some plugins - such as the pipeline plugin - allow you to write groovy code that gets executed on the master.  This means you could run code on the master that accesses the host machine with the same rights as Jenkins.\nAs is unsafe, Jenkins has some guards against this in the form the  sandbox mode  and the  script security .  When you create a pipeline job, you get a inline code editor by default. \nIf you're an administrator you get the option to turn the \"sandbox\" mode of.  If you use a pipeline from SCM or any of the higher abstraction pipeline job types (Multibranch Pipeline, BitBucket Team) you are always running in sandbox mode.  When you're in sandbox mode, your script will run past the script security. \nThis uses a whitelisting technique to block dangerous or undesired methods, but is does so in a very restrictive manner.  It could be you're doing something that is safe but still gets blocked.\nAn administrator can then go to the script approval page (under Jenkins Administration) and approve your script.  For more details, please consult  Script Security plugin  page.", 
            "title": "Sandbox and Script Security"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#example-error", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8 org . jenkinsci . plugins . scriptsecurity . sandbox . RejectedAccessException :   unclassified   staticMethod   org . tmatesoft . svn . core . internal . io . dav . DAVRepositoryFactory   create   org . tmatesoft . svn . core . SVNURL \n     at   org . jenkinsci . plugins . scriptsecurity . sandbox . groovy . SandboxInterceptor . onStaticCall ( SandboxInterceptor . java : 138 ) \n     at   org . kohsuke . groovy . sandbox . impl . Checker$2 . call ( Checker . java : 180 ) \n     at   org . kohsuke . groovy . sandbox . impl . Checker . checkedStaticCall ( Checker . java : 177 ) \n     at   org . kohsuke . groovy . sandbox . impl . Checker . checkedCall ( Checker . java : 91 ) \n     at   com . cloudbees . groovy . cps . sandbox . SandboxInvoker . methodCall ( SandboxInvoker . java : 16 ) \n     at   WorkflowScript . run ( WorkflowScript : 12 ) \n     at   ___cps . transform___ ( Native   Method )     Tip  There are three ways to deal with these errors.   go to manage jenkins   script approval and approve the script  use a  Shared Library  use a CLI tool/script via a shell command to do what you need to do", 
            "title": "Example error"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#java-vs-groovy", 
            "text": "The pipeline code has to be written in groovy and therefor can also use java code.\nTwo big difference to note:    the usage of double quoted string (gstring, interpreted) and single quoted strings (literal)  def abc = 'xyz' # is a literal  echo '$abc' # prints $abc  echo  $abc  # prints xyz    no use of  ;   Unfortunately, due to the way the Pipeline code is processed, many of the groovy features don't work or don't work as expected.  Things like the lambda's and for-each loops don't work well and are best avoided.\nIn these situations, it is best to keep to the standard syntax of Java.  For more information on how the groovy is being processed, it is best to read the  technical-design .", 
            "title": "Java vs. Groovy"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#env-object", 
            "text": "The env object is an object that is available to use in any pipeline script.  The env object allows you to store objects and variables to be used anywhere during the script.\nSo things can be shared between nodes, the master and nodes and code blocks.  Why would you want to use it? As in general, global variables are a bad practice.\nBut if you need to have variables to be available through the execution on different machines (master, nodes) it is good to use this.  Also the env object contains context variables, such as BRANCH_NAME, JOB_NAME and so one.\nFor a complete overview, view the pipeline syntax page.  Don't use the env object in functions, always feed them the parameters directly.\nOnly use it in the \"pipeline flow\" and use it for the parameters of the methods.    1\n2\n3\n4\n5\n6\n7\n8 node   { \n     stage ( SCM )   { \n         checkout   scm  \n     } \n     stage ( Echo ){ \n         echo   Branch=$env.BRANCH_NAME   // will print Branch=master \n     }  }", 
            "title": "Env (object)"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#stash-archive", 
            "text": "If you need to store files for keeping for later, there are two options available  stash  and  archive .  Both should be avoided as they cause heavy I/O traffic, usually between the Node and Master.  For more specific information, please consult the Pipeline Syntax Page.", 
            "title": "Stash &amp; archive"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#stash", 
            "text": "Stash allows you to copy files from the current workspace to a temp folder in the workspace in the master.\nIf you're currently on a different machine it will copy them one by one over the network, keep this in mind.  The files can only be retrieved during the pipeline execution and you can do so via the  unstash  command.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 node ( Machine1 )   { \n     stage ( A )   { \n         // generate some files \n         stash   excludes:   secret.txt ,   includes:   *.txt ,   name:   abc \n     }  }  node ( Machine2 )   { \n     stage ( B )   { \n         unstash   abc \n     }  }     Saves a set of files for use later in the same build, generally on another node/workspace. Stashed files are not otherwise available and are generally discarded at the end of the build. Note that the stash and unstash steps are designed for use with small files. For large data transfers, use the External Workspace Manager plugin, or use an external repository manager such as Nexus or Artifactory.", 
            "title": "Stash"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#archive-archiveartifacts", 
            "text": "Archives build output artifacts for later use. As of Jenkins 2.x, you may use the more configurable archiveArtifacts.   With archive you can store a file semi-permanently in your job. Semi as the files will be overridden by the latest build.  The files you archive will be stored in the Job folder on the master.   One usecase is to save a log file from a build tool.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 node   { \n     stage ( A )   { \n\n         try   { \n             // do some build \n         }   finally   { \n             // This step should not normally be used in your script. Consult the inline help for details. \n             archive   excludes:   useless.log ,   includes:   *.log \n\n             // Use this instead, but only for permanent files, or external logfiles \n             archiveArtifacts   allowEmptyArchive:   true ,   artifacts:   *.log ,   excludes:   useless.log ,   fingerprint:   true ,   onlyIfSuccessful:   true \n         } \n     }  }", 
            "title": "Archive &amp; archiveArtifacts"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#credentials", 
            "text": "In many pipelines you will have to deal with external systems, requiring credentials.  Jenkins has the  Credentials API  which you can also utilize in the pipeline.  You can use do this via the  Credentials  and  Credentials Binding  plugins, the first is the core plugin the second provides the integration for the pipeline.  The best way to generate the required code snippet, is to go to the pipeline syntax page, select  withCredentials  and configure what you need.  1\n2\n3\n4\n5\n6\n7 node   { \n     stage ( someRemoteCall )   { \n         withCredentials ([ usernameColonPassword ( credentialsId:   someCredentialsId ,   variable:   USRPASS )])   { \n             sh   curl -u $env.USRPASS $URL \n         } \n     }  }    For more examples, please consult Cloudbees'  Injecting-Secrets-into-Jenkins-Build-Jobs  blog post.", 
            "title": "Credentials"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#tools-build-environment", 
            "text": "Jenkins would not be Jenkins without the direct support for the build tools, such as JDK's, SDK's, Maven, Ant what have you not.  So, how do you use them in the pipeline?  Unfortunately, this is a bit more cumbersome than it is in a freestyle (or  legacy ) job.  You have to do two things:   retrieve the tool's location via the  tool  DSL method  set the environment variables to suit the tool    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 node   { \n     stage ( Maven )   { \n         String   jdk   =   tool   name:   jdk_8 ,   type:   jdk \n         String   maven   =   tool   name:   maven_3.5.0 ,   type:   maven \n         withEnv ([ JAVA_HOME=$jdk ,   PATH+MAVEN=${jdk}/bin:${maven}/bin ])   { \n             sh   mvn clean install  \n         } \n\n         // or in one go \n         withEnv ([ JAVA_HOME=${ tool  jdk_8  } ,   PATH+MAVEN=${tool  maven_3.5.0 }/bin:${env.JAVA_HOME}/bin ])   { \n             sh   mvn clean install  \n         } \n     }  }", 
            "title": "Tools &amp; Build Environment"
        }, 
        {
            "location": "/jenkins-pipeline/core-concepts/#pipeline-syntax-page", 
            "text": "Soooo, do I always have to figure out how to write these code snippets?  No, don't worry. You don't have to.  At every pipeline job type there is a link called \"Pipeline Syntax\".  This gives you a page with a drop down menu, from where you can select all the available steps.  Once you select a step, you can use the UI to setup the step and then use the  generate  button to give you the correct syntax.", 
            "title": "Pipeline Syntax Page"
        }, 
        {
            "location": "/jenkins-pipeline/declarative-pipeline/", 
            "text": "Declarative Pipeline\n\n\nDeclarative Pipeline is a relatively recent addition to Jenkins Pipeline [1] which presents a more simplified and opinionated syntax on top of the Pipeline sub-systems.\n\n\nAll valid Declarative Pipelines must be enclosed within a pipeline block, for example:\n\n\n1\n2\n3\npipeline\n \n{\n\n    \n/* insert Declarative Pipeline here */\n\n\n}\n\n\n\n\n\n\n\nHello World Example\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\npipeline\n \n{\n\n    \nagent\n \n{\n \ndocker\n \npython:3.5.1\n \n}\n\n    \nstages\n \n{\n\n        \nstage\n(\nbuild\n)\n \n{\n\n            \nsteps\n \n{\n\n                \nsh\n \npython --version\n\n            \n}\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nMKDocs Build Example\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\npipeline\n \n{\n\n    \nagent\n \nnone\n\n    \noptions\n \n{\n\n        \ntimeout\n(\ntime:\n \n10\n,\n \nunit:\n \nMINUTES\n)\n\n        \ntimestamps\n()\n\n        \nbuildDiscarder\n(\nlogRotator\n(\nnumToKeepStr:\n \n5\n))\n\n    \n}\n\n    \nstages\n \n{\n\n        \nstage\n(\nPrepare\n){\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nsteps\n \n{\n\n                \nparallel\n \n(\n\n                    \nClean:\n \n{\n\n                        \ndeleteDir\n()\n\n                    \n},\n\n                    \nNotifySlack:\n \n{\n\n                        \nslackSend\n \nchannel:\n \ncicd\n,\n \ncolor:\n \n#FFFF00\n,\n \nmessage:\n \nSTARTED: Job \n${env.JOB_NAME} [${env.BUILD_NUMBER}]\n (${env.BUILD_URL})\n\n                    \n}\n\n                \n)\n\n            \n}\n\n        \n}\n\n        \nstage\n(\nCheckout\n){\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nsteps\n \n{\n\n                \ngit\n \ncredentialsId:\n \n355df378-e726-4abd-90fa-e723c5c21ad5\n,\n \nurl:\n \ngit@gitlab.flusso.nl:CICD/ci-cd-docs.git\n\n                \nscript\n \n{\n\n                    \nenv\n.\nGIT_COMMIT_HASH\n \n=\n \nsh\n \nreturnStdout:\n \ntrue\n,\n \nscript:\n \ngit rev-parse --verify HEAD\n\n                \n}\n\n            \n}\n\n        \n}\n\n        \nstage\n(\nBuild Docs\n)\n \n{\n\n            \nagent\n \n{\n\n                \ndocker\n \n{\n\n                    \nimage\n \ncaladreas/mkdocs-docker-build-container\n\n                    \nlabel\n \ndocker\n\n                \n}\n\n            \n}\n\n            \nsteps\n \n{\n\n                \nsh\n \nmkdocs build\n\n            \n}\n\n        \n}\n\n        \nstage\n(\nPrepare Docker Image\n){\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nsteps\n \n{\n\n                \nparallel\n \n(\n\n                    \nTestDockerfile:\n \n{\n\n                        \nscript\n \n{\n\n                            \ndef\n \nlintResult\n \n=\n \nsh\n \nreturnStdout:\n \ntrue\n,\n \nscript:\n \ndocker run --rm -i lukasmartinelli/hadolint \n Dockerfile\n\n                            \nif\n \n(\nlintResult\n.\ntrim\n()\n \n==\n \n)\n \n{\n\n                                \nprintln\n \nLint finished with no errors\n\n                            \n}\n \nelse\n \n{\n\n                                \nprintln\n \nError found in Lint\n\n                                \nprintln\n \n${lintResult}\n\n                                \ncurrentBuild\n.\nresult\n \n=\n \nUNSTABLE\n\n                            \n}\n\n                        \n}\n\n                    \n},\n \n// end test dockerfile\n\n                    \nBuildImage:\n \n{\n\n                        \nsh\n \nchmod +x build.sh\n\n                        \nsh\n \n./build.sh\n\n                    \n}\n \n                \n)\n\n            \n}\n\n            \npost\n \n{\n\n                \nsuccess\n \n{\n\n                    \nsh\n \nchmod +x push.sh\n\n                    \nsh\n \n./push.sh\n\n                \n}\n\n            \n}\n\n        \n}\n\n        \nstage\n(\nUpdate Docker Container\n)\n \n{\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nsteps\n \n{\n\n                \nsh\n \nchmod +x container-update.sh\n\n                \nsh\n \n./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH}\n\n            \n}\n\n        \n}\n\n    \n}\n\n    \npost\n \n{\n\n        \nsuccess\n \n{\n\n            \nslackSend\n \nchannel:\n \ncicd\n,\n \ncolor:\n \n#00FF00\n,\n \nmessage:\n \nSUCCESSFUL: Job \n${env.JOB_NAME} [${env.BUILD_NUMBER}]\n (${env.BUILD_URL})\n\n        \n}\n\n        \nfailure\n \n{\n\n            \nslackSend\n \nchannel:\n \ncicd\n,\n \ncolor:\n \n#FF0000\n,\n \nmessage:\n \nFAILED: Job \n${env.JOB_NAME} [${env.BUILD_NUMBER}]\n (${env.BUILD_URL})\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nResources\n\n\n\n\nSyntax Reference\n\n\nGetting started\n\n\nNotifications", 
            "title": "Pipeline (declarative)"
        }, 
        {
            "location": "/jenkins-pipeline/declarative-pipeline/#declarative-pipeline", 
            "text": "Declarative Pipeline is a relatively recent addition to Jenkins Pipeline [1] which presents a more simplified and opinionated syntax on top of the Pipeline sub-systems.  All valid Declarative Pipelines must be enclosed within a pipeline block, for example:  1\n2\n3 pipeline   { \n     /* insert Declarative Pipeline here */  }", 
            "title": "Declarative Pipeline"
        }, 
        {
            "location": "/jenkins-pipeline/declarative-pipeline/#hello-world-example", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 pipeline   { \n     agent   {   docker   python:3.5.1   } \n     stages   { \n         stage ( build )   { \n             steps   { \n                 sh   python --version \n             } \n         } \n     }  }", 
            "title": "Hello World Example"
        }, 
        {
            "location": "/jenkins-pipeline/declarative-pipeline/#mkdocs-build-example", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87 pipeline   { \n     agent   none \n     options   { \n         timeout ( time:   10 ,   unit:   MINUTES ) \n         timestamps () \n         buildDiscarder ( logRotator ( numToKeepStr:   5 )) \n     } \n     stages   { \n         stage ( Prepare ){ \n             agent   {   label   docker   } \n             steps   { \n                 parallel   ( \n                     Clean:   { \n                         deleteDir () \n                     }, \n                     NotifySlack:   { \n                         slackSend   channel:   cicd ,   color:   #FFFF00 ,   message:   STARTED: Job  ${env.JOB_NAME} [${env.BUILD_NUMBER}]  (${env.BUILD_URL}) \n                     } \n                 ) \n             } \n         } \n         stage ( Checkout ){ \n             agent   {   label   docker   } \n             steps   { \n                 git   credentialsId:   355df378-e726-4abd-90fa-e723c5c21ad5 ,   url:   git@gitlab.flusso.nl:CICD/ci-cd-docs.git \n                 script   { \n                     env . GIT_COMMIT_HASH   =   sh   returnStdout:   true ,   script:   git rev-parse --verify HEAD \n                 } \n             } \n         } \n         stage ( Build Docs )   { \n             agent   { \n                 docker   { \n                     image   caladreas/mkdocs-docker-build-container \n                     label   docker \n                 } \n             } \n             steps   { \n                 sh   mkdocs build \n             } \n         } \n         stage ( Prepare Docker Image ){ \n             agent   {   label   docker   } \n             steps   { \n                 parallel   ( \n                     TestDockerfile:   { \n                         script   { \n                             def   lintResult   =   sh   returnStdout:   true ,   script:   docker run --rm -i lukasmartinelli/hadolint   Dockerfile \n                             if   ( lintResult . trim ()   ==   )   { \n                                 println   Lint finished with no errors \n                             }   else   { \n                                 println   Error found in Lint \n                                 println   ${lintResult} \n                                 currentBuild . result   =   UNSTABLE \n                             } \n                         } \n                     },   // end test dockerfile \n                     BuildImage:   { \n                         sh   chmod +x build.sh \n                         sh   ./build.sh \n                     }  \n                 ) \n             } \n             post   { \n                 success   { \n                     sh   chmod +x push.sh \n                     sh   ./push.sh \n                 } \n             } \n         } \n         stage ( Update Docker Container )   { \n             agent   {   label   docker   } \n             steps   { \n                 sh   chmod +x container-update.sh \n                 sh   ./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH} \n             } \n         } \n     } \n     post   { \n         success   { \n             slackSend   channel:   cicd ,   color:   #00FF00 ,   message:   SUCCESSFUL: Job  ${env.JOB_NAME} [${env.BUILD_NUMBER}]  (${env.BUILD_URL}) \n         } \n         failure   { \n             slackSend   channel:   cicd ,   color:   #FF0000 ,   message:   FAILED: Job  ${env.JOB_NAME} [${env.BUILD_NUMBER}]  (${env.BUILD_URL}) \n         } \n     }  }", 
            "title": "MKDocs Build Example"
        }, 
        {
            "location": "/jenkins-pipeline/declarative-pipeline/#resources", 
            "text": "Syntax Reference  Getting started  Notifications", 
            "title": "Resources"
        }, 
        {
            "location": "/jenkins-pipeline/groovy-pipeline/", 
            "text": "Jenkins Pipelines\n\n\n\n\nWarning\n\n\nThis style of pipeline definition is deprecated.\nWhen possible, please use the declarative version.\n\n\n\n\n\n\nJenkins Pipeline is a suite of plugins which supports implementing and integrating continuous delivery pipelines into Jenkins. Pipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines \"as code\" via the Pipeline DSL.\n\n\n\n\nThere are two ways to create pipelines in Jenkins.\nEither via the \nGroovy DSL\n or via the \nDeclarative pipeline\n.\n\n\nFor more information about the declarative pipeline, read the \nnext page\n.\n\n\nHello World Example\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nnode\n \n{\n\n    \ntimestamps\n \n{\n\n        \nstage\n \n(\nMy FIrst Stage\n)\n \n{\n\n            \nif\n \n(\nisUnix\n())\n \n{\n\n                \nsh\n \necho \nthis is Unix!\n\n            \n}\n \nelse\n \n{\n\n                \nbat\n \necho \nthis is windows\n\n            \n}\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nResources\n\n\n\n\nGetting started\n\n\nBest practices\n\n\nBest practices for scaling\n\n\nPossible Steps", 
            "title": "Groovy DSL Pipeline"
        }, 
        {
            "location": "/jenkins-pipeline/groovy-pipeline/#jenkins-pipelines", 
            "text": "Warning  This style of pipeline definition is deprecated.\nWhen possible, please use the declarative version.    Jenkins Pipeline is a suite of plugins which supports implementing and integrating continuous delivery pipelines into Jenkins. Pipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines \"as code\" via the Pipeline DSL.   There are two ways to create pipelines in Jenkins.\nEither via the  Groovy DSL  or via the  Declarative pipeline .  For more information about the declarative pipeline, read the  next page .", 
            "title": "Jenkins Pipelines"
        }, 
        {
            "location": "/jenkins-pipeline/groovy-pipeline/#hello-world-example", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 node   { \n     timestamps   { \n         stage   ( My FIrst Stage )   { \n             if   ( isUnix ())   { \n                 sh   echo  this is Unix! \n             }   else   { \n                 bat   echo  this is windows \n             } \n         } \n     }  }", 
            "title": "Hello World Example"
        }, 
        {
            "location": "/jenkins-pipeline/groovy-pipeline/#resources", 
            "text": "Getting started  Best practices  Best practices for scaling  Possible Steps", 
            "title": "Resources"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/", 
            "text": "Global Shared Library\n\n\nhttps://jenkins.io/doc/book/pipeline/shared-libraries/\n\n\nWhen you're making pipelines on Jenkins you will run into the situation that you will want to stay \nDRY\n.\nTo share pipeline code there are several ways.\n\n\n\n\nSCM:\n Have a pipeline dsl script in a SCM and load it from there\n\n\nPlugin:\n A Jenkins plugin that you can call via the pipeline dsl\n\n\nGlobal Workflow Library:\n There is a global library for pipeline dsl scripts in the Jekins master \nPreferred solution\n\n\n\n\nPlease read the \ndocumentation\n to get a basic idea.\n\n\n\n\nDanger\n\n\nWhen using a Global Library you will always have to import something from this library.\nThis doesn't make sense when you online use functions (via the vars folder).\nIn this case, you have to import nothing, which you do via: \"_\"\n\n\n\n\n1\n2\n@Library\n(\nFlussoGlobal\n)\n\n\nimport\n \nnl.flusso.Utilities\n\n\n\n\n\n\n\n1\n@Library\n(\nFlussoGlobal\n)\n \n_\n\n\n\n\n\n\n\nLibrary Directory structure\n\n\nThe directory structure of a shared library repository is as follows:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n(root)\n +- src                     # Groovy source files\n |   +- org\n |       +- foo\n |           +- Bar.groovy  # for org.foo.Bar class\n +- vars\n |   +- foo.groovy          # for global \nfoo\n variable/function\n |   +- foo.txt             # help for \nfoo\n variable/function\n +- resources               # resource files (external libraries only)\n |   +- org\n |       +- foo\n |           +- bar.json    # static helper data for org.foo.Bar\n\n\n\n\n\n\nThe \nsrc\n directory should look like standard Java source directory structure.\nThis directory is added to the classpath when executing Pipelines.\n\n\nThe \nvars\n directory hosts scripts that define global variables accessible from\nPipeline scripts.\nThe basename of each \n*.groovy\n file should be a Groovy (~ Java) identifier, conventionally \ncamelCased\n.\nThe matching \n*.txt\n, if present, can contain documentation, processed through the system\u2019s configured markup formatter\n(so may really be HTML, Markdown, etc., though the \ntxt\n extension is required).\n\n\nThe Groovy source files in these directories get the same \u201cCPS transformation\u201d as your Pipeline scripts.\n\n\nA \nresources\n directory allows the \nlibraryResource\n step to be used from an external library to load associated non-Groovy files.\nCurrently this feature is not supported for internal libraries.\n\n\nOther directories under the root are reserved for future enhancements.\n\n\nConfigure libraries in Jenkins\n\n\nThe a Jenkins Master you can configure the Global Pipeline Libraries.\n\n\nYou can find this in: Manage Jenkins -\n Configure System -\n Global Pipeline Libraries\n\n\nYou can configure multiple libraries, where the there is a preference for Git repositories.\nYou can select a default version (for example: the master branch), and either allow or disallow overrides to this.\n\n\nTo be able to use a different version, you would use the @\n in case of Git.\n\n\n1\n@Library\n(\nFlussoGlobal\n@my\n-\nfeature\n-\nbranch\n)\n\n\n\n\n\n\n\n\n\nHelloWorld Example\n\n\n\n\nCreate Git repository (see below for structure)\n\n\nConfigure this Git repository as an \"Global Pipeline Libraries\" entry\n\n\nName: FlussoGlobal\n\n\nDefault Version: master\n\n\nModern SCM: git\n\n\nProject repository: \n:CICD/jenkins-pipeline-library.git\n\n\nCreate the resources you want in the git repository\n\n\nUse the library in a pipeline\n\n\n\n\nUtil Class (class) Example\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n#!/usr/bin/groovy\n\n\n#\n/src/\nnl\n/flusso/\nUtilities\n.\ngroovy\n\n\npackage\n \nnl\n.\nflusso\n\n\n\nimport\n \njava.io.Serializable\n\n\n\nclass\n \nUtilities\n \nimplements\n \nSerializable\n \n{\n\n  \ndef\n \nsteps\n\n\n  \nUtilities\n(\nsteps\n)\n \n{\nthis\n.\nsteps\n \n=\n \nsteps\n}\n\n\n  \ndef\n \nsayHello\n(\nString\n \nname\n)\n \n{\n\n    \nsteps\n.\nsh\n \necho $name\n\n  \n}\n\n\n}\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n@Library\n(\nFlussoGlobal\n)\n\n\nimport\n \nnl.flusso.Utilities\n\n\n\ndef\n \nutils\n \n=\n \nnew\n \nUtilities\n(\nsteps\n)\n\n\n\nnode\n \n{\n\n    \nString\n \nname\n \n=\n \nJoost\n\n    \nutils\n.\nsayHello\n(\nname\n)\n\n\n}\n\n\n\n\n\n\n\nUtil method (var) Example\n\n\n1\n2\n3\n4\n5\n6\n#!/usr/bin/groovy\n\n\n#\n/vars/\nsayHello\n.\ngroovy\n\n\ndef\n \ncall\n(\nname\n)\n \n{\n\n    \n// you can call any valid step functions from your code, just like you can from Pipeline scripts\n\n    \necho\n \nHello world, ${name}\n\n\n}\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n@Library\n(\nFlussoGlobal\n)\n \n_\n\n\nnode\n \n{\n\n    \nString\n \nname\n \n=\n \nJoost\n\n    \nsayHello\n \nname\n\n\n}\n\n\n\n\n\n\n\nCombining libraries\n\n\nLets say you want to want to have a core library and multiple specific libraries that utilize these. There are several to do this, we will show two.\n\n\nImport both\n\n\nOne way is to explicitly import both libraries in the Jenkinsfile.\n\n\n1\n@Library\n([\ngithub.com/joostvdg/jenkins-pipeline-lib\n,\ngithub.com/joostvdg/jenkins-pipeline-go\n])\n \n_\n\n\n\n\n\n\n\nCon:\n\n\n\n\nyou have to import all the required libraries yourself\n\n\n\n\nPro:\n\n\n\n\nyou can specify the versions of each\n\n\n\n\nImplicit Import + Explicit Import\n\n\nYou can also configure the core (in this case jenkins-pipeline-lib) as \"loaded implicitly\".\nThis will make anything from this library available by default.\n\n\nBe careful with the naming of the vars though!\n\n\nThe resulting Jenkinsfile would then be.\n\n\n1\n@Library\n(\ngithub.com/joostvdg/jenkins-pipeline-go\n)\n \n_\n\n\n\n\n\n\n\nResources\n\n\n\n\nimplement-reusable-function-call", 
            "title": "Pipeline Libraries"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#global-shared-library", 
            "text": "https://jenkins.io/doc/book/pipeline/shared-libraries/  When you're making pipelines on Jenkins you will run into the situation that you will want to stay  DRY .\nTo share pipeline code there are several ways.   SCM:  Have a pipeline dsl script in a SCM and load it from there  Plugin:  A Jenkins plugin that you can call via the pipeline dsl  Global Workflow Library:  There is a global library for pipeline dsl scripts in the Jekins master  Preferred solution   Please read the  documentation  to get a basic idea.   Danger  When using a Global Library you will always have to import something from this library.\nThis doesn't make sense when you online use functions (via the vars folder).\nIn this case, you have to import nothing, which you do via: \"_\"   1\n2 @Library ( FlussoGlobal )  import   nl.flusso.Utilities    1 @Library ( FlussoGlobal )   _", 
            "title": "Global Shared Library"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#library-directory-structure", 
            "text": "The directory structure of a shared library repository is as follows:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 (root)\n +- src                     # Groovy source files\n |   +- org\n |       +- foo\n |           +- Bar.groovy  # for org.foo.Bar class\n +- vars\n |   +- foo.groovy          # for global  foo  variable/function\n |   +- foo.txt             # help for  foo  variable/function\n +- resources               # resource files (external libraries only)\n |   +- org\n |       +- foo\n |           +- bar.json    # static helper data for org.foo.Bar   The  src  directory should look like standard Java source directory structure.\nThis directory is added to the classpath when executing Pipelines.  The  vars  directory hosts scripts that define global variables accessible from\nPipeline scripts.\nThe basename of each  *.groovy  file should be a Groovy (~ Java) identifier, conventionally  camelCased .\nThe matching  *.txt , if present, can contain documentation, processed through the system\u2019s configured markup formatter\n(so may really be HTML, Markdown, etc., though the  txt  extension is required).  The Groovy source files in these directories get the same \u201cCPS transformation\u201d as your Pipeline scripts.  A  resources  directory allows the  libraryResource  step to be used from an external library to load associated non-Groovy files.\nCurrently this feature is not supported for internal libraries.  Other directories under the root are reserved for future enhancements.", 
            "title": "Library Directory structure"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#configure-libraries-in-jenkins", 
            "text": "The a Jenkins Master you can configure the Global Pipeline Libraries.  You can find this in: Manage Jenkins -  Configure System -  Global Pipeline Libraries  You can configure multiple libraries, where the there is a preference for Git repositories.\nYou can select a default version (for example: the master branch), and either allow or disallow overrides to this.  To be able to use a different version, you would use the @  in case of Git.  1 @Library ( FlussoGlobal @my - feature - branch )", 
            "title": "Configure libraries in Jenkins"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#helloworld-example", 
            "text": "Create Git repository (see below for structure)  Configure this Git repository as an \"Global Pipeline Libraries\" entry  Name: FlussoGlobal  Default Version: master  Modern SCM: git  Project repository:  :CICD/jenkins-pipeline-library.git  Create the resources you want in the git repository  Use the library in a pipeline", 
            "title": "HelloWorld Example"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#util-class-class-example", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 #!/usr/bin/groovy  # /src/ nl /flusso/ Utilities . groovy  package   nl . flusso  import   java.io.Serializable  class   Utilities   implements   Serializable   { \n   def   steps \n\n   Utilities ( steps )   { this . steps   =   steps } \n\n   def   sayHello ( String   name )   { \n     steps . sh   echo $name \n   }  }    1\n2\n3\n4\n5\n6\n7\n8\n9 @Library ( FlussoGlobal )  import   nl.flusso.Utilities  def   utils   =   new   Utilities ( steps )  node   { \n     String   name   =   Joost \n     utils . sayHello ( name )  }", 
            "title": "Util Class (class) Example"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#util-method-var-example", 
            "text": "1\n2\n3\n4\n5\n6 #!/usr/bin/groovy  # /vars/ sayHello . groovy  def   call ( name )   { \n     // you can call any valid step functions from your code, just like you can from Pipeline scripts \n     echo   Hello world, ${name}  }    1\n2\n3\n4\n5 @Library ( FlussoGlobal )   _  node   { \n     String   name   =   Joost \n     sayHello   name  }", 
            "title": "Util method (var) Example"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#combining-libraries", 
            "text": "Lets say you want to want to have a core library and multiple specific libraries that utilize these. There are several to do this, we will show two.", 
            "title": "Combining libraries"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#import-both", 
            "text": "One way is to explicitly import both libraries in the Jenkinsfile.  1 @Library ([ github.com/joostvdg/jenkins-pipeline-lib , github.com/joostvdg/jenkins-pipeline-go ])   _    Con:   you have to import all the required libraries yourself   Pro:   you can specify the versions of each", 
            "title": "Import both"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#implicit-import-explicit-import", 
            "text": "You can also configure the core (in this case jenkins-pipeline-lib) as \"loaded implicitly\".\nThis will make anything from this library available by default.  Be careful with the naming of the vars though!  The resulting Jenkinsfile would then be.  1 @Library ( github.com/joostvdg/jenkins-pipeline-go )   _", 
            "title": "Implicit Import + Explicit Import"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#resources", 
            "text": "implement-reusable-function-call", 
            "title": "Resources"
        }, 
        {
            "location": "/jenkins-pipeline/ide-integration-pipeline-dsl/", 
            "text": "IDE Integration for Jenkins Pipeline DSL\n\n\nSupported IDE's\n\n\nCurrently only Jetbrain's Intelli J's IDEA is \nsupported\n.\n\n\nThis via a Groovy DSL file (.gdsl).\n\n\nConfigure Intelli J IDEA\n\n\nGo to a Jenkins Pipeline job and open the Pipeline Syntax page.\n\n\nOn the page in the left hand menu, you will see a link to download a Jenkins Master specific Groovy DSL file.\nDownload this and save it into your project's workspace.\n\n\n\n\nIt will have to be part of your classpath, the easiest way to do this is to add the file as pipeline.gdsl in a/the src folder.\n\n\nFor more information, you can read \nSteffen Gerbert\n's blog.\n\n\nRemarks from Kohsuke Kawaguchi\n\n\nMore effort in this space will be taken by Cloudbees.\nBut the priority is low compared to other initiatives.\n\n\nIntegration of Pipeline Library\n\n\nIf you're using the Global Shared Libraries for sharing generic pipeline building blocks, it would be nice to have this awareness in your editor as well.\n\n\nOne of the ways to do this, is to checkout the source code of this library and make sure it is compiled.\nIn your editor (assuming Intelli J IDEA) you can then add the compiled classes as dependency (type: classes).\nThis way, at least every class defined in your library is usable as a normal dependency would be. \n\n\nFinal configuration Intelli J IDEA", 
            "title": "DSL IDE Integration"
        }, 
        {
            "location": "/jenkins-pipeline/ide-integration-pipeline-dsl/#ide-integration-for-jenkins-pipeline-dsl", 
            "text": "", 
            "title": "IDE Integration for Jenkins Pipeline DSL"
        }, 
        {
            "location": "/jenkins-pipeline/ide-integration-pipeline-dsl/#supported-ides", 
            "text": "Currently only Jetbrain's Intelli J's IDEA is  supported .  This via a Groovy DSL file (.gdsl).", 
            "title": "Supported IDE's"
        }, 
        {
            "location": "/jenkins-pipeline/ide-integration-pipeline-dsl/#configure-intelli-j-idea", 
            "text": "Go to a Jenkins Pipeline job and open the Pipeline Syntax page.  On the page in the left hand menu, you will see a link to download a Jenkins Master specific Groovy DSL file.\nDownload this and save it into your project's workspace.   It will have to be part of your classpath, the easiest way to do this is to add the file as pipeline.gdsl in a/the src folder.  For more information, you can read  Steffen Gerbert 's blog.", 
            "title": "Configure Intelli J IDEA"
        }, 
        {
            "location": "/jenkins-pipeline/ide-integration-pipeline-dsl/#remarks-from-kohsuke-kawaguchi", 
            "text": "More effort in this space will be taken by Cloudbees.\nBut the priority is low compared to other initiatives.", 
            "title": "Remarks from Kohsuke Kawaguchi"
        }, 
        {
            "location": "/jenkins-pipeline/ide-integration-pipeline-dsl/#integration-of-pipeline-library", 
            "text": "If you're using the Global Shared Libraries for sharing generic pipeline building blocks, it would be nice to have this awareness in your editor as well.  One of the ways to do this, is to checkout the source code of this library and make sure it is compiled.\nIn your editor (assuming Intelli J IDEA) you can then add the compiled classes as dependency (type: classes).\nThis way, at least every class defined in your library is usable as a normal dependency would be.", 
            "title": "Integration of Pipeline Library"
        }, 
        {
            "location": "/jenkins-pipeline/ide-integration-pipeline-dsl/#final-configuration-intelli-j-idea", 
            "text": "", 
            "title": "Final configuration Intelli J IDEA"
        }, 
        {
            "location": "/jenkins-pipeline/jenkins-parallel-pipeline/", 
            "text": "", 
            "title": "Parallel"
        }, 
        {
            "location": "/jenkins-pipeline/input/", 
            "text": "Jenkins Pipeline - Input\n\n\nThe Jenkins Pipeline has a plugin for dealing with external input.\nGenerally it is used to gather user input (values or approval), but it also has a REST API for this.\n\n\nGeneral Info\n\n\nThe \nPipeline Input Step\n allows you to\n\n\nThe plugin allows you to capture input in a variety of ways, but there are some gotcha's.\n\n\n\n\nIf you have a single parameter, it will be returned as a single value\n\n\nIf you have multiple parameters, it will be returned as a map\n\n\nThe choices for the Choice parameter should be a single line, where values are separated with /n\n\n\nDon't use input within a node {}, as this will block an executor slot\n\n\n..\n\n\n\n\nExamples\n\n\nSingle Parameter\n\n\n1\n2\n3\n4\n5\ndef\n \nhello\n \n=\n \ninput\n \nid:\n \nCustomId\n,\n \nmessage:\n \nWant to continue?\n,\n \nok:\n \nYes\n,\n \nparameters:\n \n[\nstring\n(\ndefaultValue:\n \nworld\n,\n \ndescription:\n \n,\n \nname:\n \nhello\n)]\n\n\n\nnode\n \n{\n\n    \nprintln\n \necho $hello\n\n\n}\n\n\n\n\n\n\n\nMultiple Parameters\n\n\n1\n2\n3\n4\n5\n6\n7\ndef\n \nuserInput\n \n=\n \ninput\n \nid:\n \nCustomId\n,\n \nmessage:\n \nWant to continue?\n,\n \nok:\n \nYes\n,\n \nparameters:\n \n[\nstring\n(\ndefaultValue:\n \nworld\n,\n \ndescription:\n \n,\n \nname:\n \nhello\n),\n \nstring\n(\ndefaultValue:\n \n,\n \ndescription:\n \n,\n \nname:\n \ntoken\n)]\n\n\n\nnode\n \n{\n\n    \ndef\n \nhello\n \n=\n \nuserInput\n[\nhello\n]\n\n    \ndef\n \ntoken\n \n=\n \nuserInput\n[\ntoken\n]\n\n    \nprintln\n \nhello=$hello, token=$token\n\n\n}\n\n\n\n\n\n\n\nTimeout on Input\n\n\n1\n2\n3\n4\n5\n6\ndef\n \nuserInput\n\n\n\ntimeout\n(\ntime:\n \n10\n,\n \nunit:\n \nSECONDS\n)\n \n{\n\n    \nprintln\n \nWaiting for input\n\n    \nuserInput\n \n=\n \ninput\n \nid:\n \nCustomId\n,\n \nmessage:\n \nWant to continue?\n,\n \nok:\n \nYes\n,\n \nparameters:\n \n[\nstring\n(\ndefaultValue:\n \nworld\n,\n \ndescription:\n \n,\n \nname:\n \nhello\n),\n \nstring\n(\ndefaultValue:\n \n,\n \ndescription:\n \n,\n \nname:\n \ntoken\n)]\n\n\n}\n\n\n\n\n\n\n\nREST API\n\n\nThere's a rest API for sending the input to a waiting input step.\nThe format of the url: \n{JenkinsURL}/\n{JenkinsURL}/\n{JobURL}/\n{Build#}/input/\n{Build#}/input/\n{InputID}/submit.\n\n\nThere are some things to keep in mind:\n\n\n\n\nIf Jenkins has CSRF protection enabled, you need a Crumb (see below) for the requests\n\n\nRequests are send via POST\n\n\nFor supplying values you need to have a JSON with the parameters with as \njson\n param\n\n\nYou need to supply the \nproceed\n value: the value of the \nok\n button, as \nproceed\n param\n\n\nYou will have to fill in the \ninput id\n, so it is best to configure a unique input id for the input steps you want to connect to from outside\n\n\n\n\nExamples\n\n\n1\n2\n3\n4\n5\n6\n{\nparameter\n:\n\n    \n[\n\n        \n{\nname\n:\n \nhello\n,\n \nvalue\n:\n \njoost\n},\n\n        \n{\nname\n:\n \ntoken\n,\n \nvalue\n:\n \nnot a token\n}\n\n    \n]\n\n\n}\n\n\n\n\n\n\n\n1\n2\n# single parameter\n\ncurl --user \n$USER\n:\n$PASS\n -X POST -H \nJenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33\n -d \njson\n=\n{\nparameter\n: {\nname\n: \nhello\n, \nvalue\n: \njoost\n}}\n -d \nproceed\n=\nYes\n \nhttps://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit\n\n\n\n\n\n\n1\n2\n# Multiple Parameters\n\ncurl --user \n$USER\n:\n$PASS\n -X POST -H \nJenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33\n -d \njson\n=\n{\nparameter\n: [{\nname\n: \nhello\n, \nvalue\n: \njoost\n},{\nname\n: \ntoken\n, \nvalue\n: \nnot a token\n}]}\n -d \nproceed\n=\nYes\n \nhttps://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit\n\n\n\n\n\n\nCrumb (secured Jenkins)\n\n\nIf Jenkins is secured against CSRF (via Global Security: Prevent Cross Site Request Forgery exploits), any API call requires a Crumb.\nYou can read more about it \nhere\n.\n\n\nTo get a valid crumb you have to send a crumb request as authenticated user.\n\n\n\n\nJSON: \nhttps://ci.flusso.nl/jenkins/crumbIssuer/api/json\n\n\nXML (parsed): \nhttps://ci.flusso.nl/jenkins/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,%22:%22,//crumb\n)", 
            "title": "Input"
        }, 
        {
            "location": "/jenkins-pipeline/input/#jenkins-pipeline-input", 
            "text": "The Jenkins Pipeline has a plugin for dealing with external input.\nGenerally it is used to gather user input (values or approval), but it also has a REST API for this.", 
            "title": "Jenkins Pipeline - Input"
        }, 
        {
            "location": "/jenkins-pipeline/input/#general-info", 
            "text": "The  Pipeline Input Step  allows you to  The plugin allows you to capture input in a variety of ways, but there are some gotcha's.   If you have a single parameter, it will be returned as a single value  If you have multiple parameters, it will be returned as a map  The choices for the Choice parameter should be a single line, where values are separated with /n  Don't use input within a node {}, as this will block an executor slot  ..", 
            "title": "General Info"
        }, 
        {
            "location": "/jenkins-pipeline/input/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/jenkins-pipeline/input/#single-parameter", 
            "text": "1\n2\n3\n4\n5 def   hello   =   input   id:   CustomId ,   message:   Want to continue? ,   ok:   Yes ,   parameters:   [ string ( defaultValue:   world ,   description:   ,   name:   hello )]  node   { \n     println   echo $hello  }", 
            "title": "Single Parameter"
        }, 
        {
            "location": "/jenkins-pipeline/input/#multiple-parameters", 
            "text": "1\n2\n3\n4\n5\n6\n7 def   userInput   =   input   id:   CustomId ,   message:   Want to continue? ,   ok:   Yes ,   parameters:   [ string ( defaultValue:   world ,   description:   ,   name:   hello ),   string ( defaultValue:   ,   description:   ,   name:   token )]  node   { \n     def   hello   =   userInput [ hello ] \n     def   token   =   userInput [ token ] \n     println   hello=$hello, token=$token  }", 
            "title": "Multiple Parameters"
        }, 
        {
            "location": "/jenkins-pipeline/input/#timeout-on-input", 
            "text": "1\n2\n3\n4\n5\n6 def   userInput  timeout ( time:   10 ,   unit:   SECONDS )   { \n     println   Waiting for input \n     userInput   =   input   id:   CustomId ,   message:   Want to continue? ,   ok:   Yes ,   parameters:   [ string ( defaultValue:   world ,   description:   ,   name:   hello ),   string ( defaultValue:   ,   description:   ,   name:   token )]  }", 
            "title": "Timeout on Input"
        }, 
        {
            "location": "/jenkins-pipeline/input/#rest-api", 
            "text": "There's a rest API for sending the input to a waiting input step.\nThe format of the url:  {JenkinsURL}/ {JenkinsURL}/ {JobURL}/ {Build#}/input/ {Build#}/input/ {InputID}/submit.  There are some things to keep in mind:   If Jenkins has CSRF protection enabled, you need a Crumb (see below) for the requests  Requests are send via POST  For supplying values you need to have a JSON with the parameters with as  json  param  You need to supply the  proceed  value: the value of the  ok  button, as  proceed  param  You will have to fill in the  input id , so it is best to configure a unique input id for the input steps you want to connect to from outside", 
            "title": "REST API"
        }, 
        {
            "location": "/jenkins-pipeline/input/#examples_1", 
            "text": "1\n2\n3\n4\n5\n6 { parameter : \n     [ \n         { name :   hello ,   value :   joost }, \n         { name :   token ,   value :   not a token } \n     ]  }    1\n2 # single parameter \ncurl --user  $USER : $PASS  -X POST -H  Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33  -d  json = { parameter : { name :  hello ,  value :  joost }}  -d  proceed = Yes   https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit    1\n2 # Multiple Parameters \ncurl --user  $USER : $PASS  -X POST -H  Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33  -d  json = { parameter : [{ name :  hello ,  value :  joost },{ name :  token ,  value :  not a token }]}  -d  proceed = Yes   https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit", 
            "title": "Examples"
        }, 
        {
            "location": "/jenkins-pipeline/input/#crumb-secured-jenkins", 
            "text": "If Jenkins is secured against CSRF (via Global Security: Prevent Cross Site Request Forgery exploits), any API call requires a Crumb.\nYou can read more about it  here .  To get a valid crumb you have to send a crumb request as authenticated user.   JSON:  https://ci.flusso.nl/jenkins/crumbIssuer/api/json  XML (parsed):  https://ci.flusso.nl/jenkins/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,%22:%22,//crumb )", 
            "title": "Crumb (secured Jenkins)"
        }, 
        {
            "location": "/jenkins-pipeline-examples/", 
            "text": "Jenkins Pipeline Examples\n\n\nPlease mind that these examples all assume the following:\n\n\n\n\nyou have Jenkins 2.32+\n\n\nyou have a recent set of pipeline plugins\n\n\nJenkins Pipeline Model\n\n\nJenkins Blue Ocean\n\n\nJenkins Pipeline Maven\n\n\nBuild timeout plugin\n\n\nCredentials Binding\n\n\nCredentials\n\n\nPipeline Multi-Branch\n\n\nSonarQube\n\n\nTimestamper\n\n\nPipeline Supporting APIs\n\n\nPipeline Shared Groovy Libraries", 
            "title": "Index"
        }, 
        {
            "location": "/jenkins-pipeline-examples/#jenkins-pipeline-examples", 
            "text": "Please mind that these examples all assume the following:   you have Jenkins 2.32+  you have a recent set of pipeline plugins  Jenkins Pipeline Model  Jenkins Blue Ocean  Jenkins Pipeline Maven  Build timeout plugin  Credentials Binding  Credentials  Pipeline Multi-Branch  SonarQube  Timestamper  Pipeline Supporting APIs  Pipeline Shared Groovy Libraries", 
            "title": "Jenkins Pipeline Examples"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-groovy-dsl/", 
            "text": "Maven Groovy DSL Example\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\nnode\n \n{\n\n    \ntimestamps\n \n{\n\n        \ntimeout\n(\ntime:\n \n15\n,\n \nunit:\n \nMINUTES\n)\n \n{\n\n            \ndeleteDir\n()\n\n            \nstage\n \nSCM\n\n            \n//git branch: \nmaster\n, credentialsId: \nflusso-gitlab\n, url: \nhttps://gitlab.flusso.nl/keep/keep-backend-spring.git\n\n            \ncheckout\n \nscm\n\n\n            \nenv\n.\nJAVA_HOME\n=\n${tool \nJDK 8 Latest\n}\n\n            \nenv\n.\nPATH\n=\n${env.JAVA_HOME}/bin:${env.PATH}\n\n            \nsh\n \njava -version\n\n\n            \ntry\n \n{\n\n                \ndef\n \ngradleHome\n \n=\n \ntool\n \nname:\n \nGradle Latest\n,\n \ntype:\n \nhudson.plugins.gradle.GradleInstallation\n\n                \nstage\n \nBuild\n\n\n                \nsh\n \n${gradleHome}/bin/gradle clean build javadoc\n\n                \nstep\n([\n$class\n:\n \nCheckStylePublisher\n,\n \ncanComputeNew:\n \nfalse\n,\n \ndefaultEncoding:\n \n,\n \nhealthy:\n \n,\n \npattern:\n \nbuild/reports/checkstyle/main.xml\n,\n \nunHealthy:\n \n])\n\n                \nstep\n([\n$class\n:\n \nJUnitResultArchiver\n,\n \ntestResults:\n \nbuild/test-results/*.xml\n])\n\n                \nstep\n([\n$class\n:\n \nJavadocArchiver\n,\n \njavadocDir:\n \nbuild/docs/javadoc\n])\n\n\n                \nstage\n \nSonarQube\n\n                \nsh\n \n${gradleHome}/bin/gradle sonarqube -Dsonar.host.url=http://sonarqube5-instance:9000\n\n\n                \nstash\n \nworkspace\n\n            \n}\n  \ncatch\n \n(\nerr\n)\n \n{\n\n                \narchive\n \nbuild/**/*.html\n\n                \necho\n \nCaught: ${err}\n\n                \ncurrentBuild\n.\nresult\n \n=\n \nFAILURE\n\n            \n}\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\nnode\n \n(\ndocker\n)\n \n{\n\n    \ntimestamps\n \n{\n\n        \ntimeout\n(\ntime:\n \n15\n,\n \nunit:\n \nMINUTES\n)\n \n{\n\n            \ndeleteDir\n()\n\n            \nunstash\n \nworkspace\n\n\n            \nstage\n \nBuild Docker image\n\n            \nsh\n \n./build.sh\n\n            \ndef\n \nimage\n \n=\n \ndocker\n.\nimage\n(\nkeep-backend-spring-img\n)\n\n\n            \nstage\n \nPush Docker image\n\n            \ntry\n \n{\n\n                \nsh\n \ndocker tag keep-backend-spring-img nexus.docker:18443/flusso/keep-backend-spring-img:latest\n\n                \nsh\n \ndocker push nexus.docker:18443/flusso/keep-backend-spring-img:latest\n\n            \n}\n  \ncatch\n \n(\nerr\n)\n \n{\n\n                \narchive\n \nbuild/**/*.html\n\n                \necho\n \nCaught: ${err}\n\n                \ncurrentBuild\n.\nresult\n \n=\n \nFAILURE\n\n            \n}\n\n\n        \n}\n\n    \n}\n\n\n}", 
            "title": "Maven Groovy DSL"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-groovy-dsl/#maven-groovy-dsl-example", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57 node   { \n     timestamps   { \n         timeout ( time:   15 ,   unit:   MINUTES )   { \n             deleteDir () \n             stage   SCM \n             //git branch:  master , credentialsId:  flusso-gitlab , url:  https://gitlab.flusso.nl/keep/keep-backend-spring.git \n             checkout   scm \n\n             env . JAVA_HOME = ${tool  JDK 8 Latest } \n             env . PATH = ${env.JAVA_HOME}/bin:${env.PATH} \n             sh   java -version \n\n             try   { \n                 def   gradleHome   =   tool   name:   Gradle Latest ,   type:   hudson.plugins.gradle.GradleInstallation \n                 stage   Build \n\n                 sh   ${gradleHome}/bin/gradle clean build javadoc \n                 step ([ $class :   CheckStylePublisher ,   canComputeNew:   false ,   defaultEncoding:   ,   healthy:   ,   pattern:   build/reports/checkstyle/main.xml ,   unHealthy:   ]) \n                 step ([ $class :   JUnitResultArchiver ,   testResults:   build/test-results/*.xml ]) \n                 step ([ $class :   JavadocArchiver ,   javadocDir:   build/docs/javadoc ]) \n\n                 stage   SonarQube \n                 sh   ${gradleHome}/bin/gradle sonarqube -Dsonar.host.url=http://sonarqube5-instance:9000 \n\n                 stash   workspace \n             }    catch   ( err )   { \n                 archive   build/**/*.html \n                 echo   Caught: ${err} \n                 currentBuild . result   =   FAILURE \n             } \n         } \n     }  }  node   ( docker )   { \n     timestamps   { \n         timeout ( time:   15 ,   unit:   MINUTES )   { \n             deleteDir () \n             unstash   workspace \n\n             stage   Build Docker image \n             sh   ./build.sh \n             def   image   =   docker . image ( keep-backend-spring-img ) \n\n             stage   Push Docker image \n             try   { \n                 sh   docker tag keep-backend-spring-img nexus.docker:18443/flusso/keep-backend-spring-img:latest \n                 sh   docker push nexus.docker:18443/flusso/keep-backend-spring-img:latest \n             }    catch   ( err )   { \n                 archive   build/**/*.html \n                 echo   Caught: ${err} \n                 currentBuild . result   =   FAILURE \n             } \n\n         } \n     }  }", 
            "title": "Maven Groovy DSL Example"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-declarative/", 
            "text": "Maven Declarative Examples\n\n\nBasics\n\n\nWe have to wrap the entire script in \npipeline { }\n, for it to be marked a declarative script.\n\n\nAs we will be using different agents for different stages, we select \nnone\n as the default.\n\n\nFor house keeping, we add the \noptions{}\n block, where we configure the following:\n\n\n\n\ntimeout: make sure this jobs succeeds in 10 minutes, else just cancel it\n\n\ntimestamps(): to make sure we have timestamps in our logs\n\n\nbuildDiscarder(): this will make sure we will only keep the latest 5 builds \n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\npipeline\n \n{\n\n    \nagent\n \nnone\n\n    \noptions\n \n{\n\n        \ntimeout\n(\ntime:\n \n10\n,\n \nunit:\n \nMINUTES\n)\n\n        \ntimestamps\n()\n\n        \nbuildDiscarder\n(\nlogRotator\n(\nnumToKeepStr:\n \n5\n))\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nCheckout\n\n\nThere are several ways to checkout the code.\n\n\nLet's assume our code is somewhere in a git repository.\n\n\nFull Checkout command\n\n\nThe main command for checking out is the Checkout command.\n\n\nIt will look like this.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nstage\n(\nSCM\n)\n \n{\n\n    \ncheckout\n([\n\n        \n$class\n:\n \nGitSCM\n,\n \n        \nbranches:\n \n[[\nname:\n \n*/master\n]],\n \n        \ndoGenerateSubmoduleConfigurations:\n \nfalse\n,\n \n        \nextensions:\n \n[],\n \n        \nsubmoduleCfg:\n \n[],\n \n        \nuserRemoteConfigs:\n \n            \n[[\ncredentialsId:\n \nMyCredentialsId\n,\n \n            \nurl:\n \nhttps://github.com/joostvdg/keep-watching\n]]\n\n    \n])\n\n\n}\n\n\n\n\n\n\n\nGit shorthand\n\n\nThats a lot of configuration for a simple checkout.\n\n\nSo what if I'm just using the master branch of a publicly accessible repository (as is the case with GitHub)?\n\n\n1\n2\n3\nstage\n(\nSCM\n)\n \n{\n\n    \ngit\n \nhttps://github.com/joostvdg/keep-watching\n\n\n}\n\n\n\n\n\n\n\nOr with a different branch and credentials:\n\n\n1\n2\n3\nstage\n(\nSCM\n)\n \n{\n\n    \ngit\n \ncredentialsId:\n \nMyCredentialsId\n,\n \nurl:\n \nhttps://github.com/joostvdg/keep-watching\n\n\n}\n\n\n\n\n\n\n\nThat's much better, but we can do even better.\n\n\nSCM shorthand\n\n\nIf you're starting this pipeline job via a SCM, you've already configured the SCM.\n\n\nSo assuming you've configured a pipeline job with 'Jenkinsfile from SCM' or an abstraction job - such as Multibranch-Pipeline, GitHub Organization or BitBucket Team/Project - you can do this.\n\n\n1\n2\n3\nstage\n(\nSCM\n)\n \n{\n\n    \ncheckout\n \nscm\n\n\n}\n\n\n\n\n\n\n\nThe \ncheckout scm\n line will use the checkout command we've used in the first example together with the object \nscm\n.\n\n\nThis scm object, will contain the SCM configuration of the Job and will be reused for checking out.\n\n\n\n\nWarning\n\n\nA pipeline job from SCM or abstraction, will only checkout your Jenkinsfile.\nYou will always need to checkout the rest of your code if you want to build it.\nFor that, just use \ncheckout scm\n                 \n\n\n\n\nDifferent Agent per Stage\n\n\nAs you could see on the top, we've set agent to none.\n\n\nSo for every stage we now need to tell it which agent to use - without it, the stage will fail.  \n\n\nAgent any\n\n\nIf you don't care what node it comes on, you specify any.\n\n\n1\n2\n3\n4\n5\n6\nstage\n(\nCheckout\n)\n \n{\n\n\n    \nagent\n \nany\n\n\n    \nsteps\n \n{\n\n        \ngit\n \nhttps://github.com/joostvdg/keep-watching\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nAgent via Label\n\n\nIf you want build on a node with a specific label - here \ndocker\n - you do so with \nagent { label '\nLABEL\n' }\n. \n\n\n1\n2\n3\n4\n5\n6\nstage\n(\nCheckout\n)\n \n{\n\n\n    \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n\n    \nsteps\n \n{\n\n        \ngit\n \nhttps://github.com/joostvdg/keep-watching\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nDocker Container as Agent\n\n\nMany developers are using docker for their CI/CD builds. \nSo being able to use docker containers as build agents is a requirement these days.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nstage\n(\nMaven Build\n)\n \n{\n\n    \nagent\n \n{\n\n\n        \ndocker\n \n{\n\n\n            \nimage\n \nmaven:3-alpine\n\n\n            \nlabel\n \ndocker\n\n\n            \nargs\n  \n-v /home/joost/.m2:/root/.m2\n\n\n        \n}\n\n    \n}\n\n    \nsteps\n \n{\n\n        \nsh\n \nmvn -B clean package\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nCache Maven repo\n\n\nWhen you're using a docker build container, it will be clean every time.\n\n\nSo if you want to avoid downloading the maven dependencies every build, you have to cache them.\n\n\nOne way to do this, is to map a volume into the container so the container will use that folder instead. \n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nstage\n(\nMaven Build\n)\n \n{\n\n    \nagent\n \n{\n\n        \ndocker\n \n{\n\n            \nimage\n \nmaven:3-alpine\n\n            \nlabel\n \ndocker\n\n\n            \nargs\n  \n-v /home/joost/.m2:/root/.m2\n\n\n        \n}\n\n    \n}\n\n    \nsteps\n \n{\n\n        \nsh\n \nmvn -B clean package\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nPost stage/build\n\n\nThe declarative pipeline allows for Post actions, on both stage and complete build level.\n\n\nFor both types there are different post hooks you can use, such as success, failure.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nstage\n(\nMaven Build\n)\n \n{\n\n    \nagent\n \n{\n\n        \ndocker\n \n{\n\n            \nimage\n \nmaven:3-alpine\n\n            \nlabel\n \ndocker\n\n            \nargs\n  \n-v /home/joost/.m2:/root/.m2\n\n        \n}\n\n    \n}\n\n    \nsteps\n \n{\n\n        \nsh\n \nmvn -B clean package\n\n    \n}\n\n\n    \npost\n \n{\n\n\n        \nsuccess\n \n{\n\n\n            \njunit\n \ntarget/surefire-reports/**/*.xml\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nstages\n \n{\n\n    \nstage\n(\nExample\n)\n \n{\n\n        \nsteps\n \n{\n\n            \necho\n \nHello World\n\n        \n}\n\n    \n}\n\n\n}\n\n\npost\n \n{\n\n\n    \nalways\n \n{\n\n\n        \necho\n \nThis will always run\n\n    \n}\n\n\n    \nsuccess\n \n{\n\n\n        \necho\n \nSUCCESS!\n\n    \n}\n\n\n    \nfailure\n \n{\n\n\n        \necho\n \nWe Failed\n\n    \n}\n\n\n    \nunstable\n \n{\n\n\n        \necho\n \nWe\nre unstable\n\n    \n}\n\n\n    \nchanged\n \n{\n\n\n        \necho\n \nStatus Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result]\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nEntire example\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\npipeline\n \n{\n\n    \nagent\n \nnone\n\n    \noptions\n \n{\n\n        \ntimeout\n(\ntime:\n \n10\n,\n \nunit:\n \nMINUTES\n)\n\n        \ntimestamps\n()\n\n        \nbuildDiscarder\n(\nlogRotator\n(\nnumToKeepStr:\n \n5\n))\n\n    \n}\n\n    \nstages\n \n{\n\n        \nstage\n(\nExample\n)\n \n{\n\n            \nsteps\n \n{\n\n                \necho\n \nHello World\n\n            \n}\n\n        \n}\n\n        \nstage\n(\nCheckout\n)\n \n{\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nsteps\n \n{\n\n                \ngit\n \nhttps://github.com/joostvdg/keep-watching\n\n            \n}\n\n        \n}\n\n        \nstage\n(\nMaven Build\n)\n \n{\n\n            \nagent\n \n{\n\n                \ndocker\n \n{\n\n                    \nimage\n \nmaven:3-alpine\n\n                    \nlabel\n \ndocker\n\n                    \nargs\n  \n-v /home/joost/.m2:/root/.m2\n\n                \n}\n\n            \n}\n\n            \nsteps\n \n{\n\n                \nsh\n \nmvn -B clean package\n\n            \n}\n\n            \npost\n \n{\n\n                \nsuccess\n \n{\n\n                    \njunit\n \ntarget/surefire-reports/**/*.xml\n\n                \n}\n\n            \n}\n\n        \n}\n\n        \nstage\n(\nDocker Build\n)\n \n{\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nsteps\n \n{\n\n                \nsh\n \ndocker build --tag=keep-watching-be .\n\n            \n}\n\n        \n}\n\n    \n}\n\n    \npost\n \n{\n\n        \nalways\n \n{\n\n            \necho\n \nThis will always run\n\n        \n}\n\n        \nsuccess\n \n{\n\n            \necho\n \nSUCCESS!\n\n        \n}\n\n        \nfailure\n \n{\n\n            \necho\n \nWe Failed\n\n        \n}\n\n        \nunstable\n \n{\n\n            \necho\n \nWe\nre unstable\n\n        \n}\n\n        \nchanged\n \n{\n\n            \necho\n \nStatus Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result]\n\n        \n}\n\n    \n}\n\n\n}", 
            "title": "Maven Declarative"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-declarative/#maven-declarative-examples", 
            "text": "", 
            "title": "Maven Declarative Examples"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-declarative/#basics", 
            "text": "We have to wrap the entire script in  pipeline { } , for it to be marked a declarative script.  As we will be using different agents for different stages, we select  none  as the default.  For house keeping, we add the  options{}  block, where we configure the following:   timeout: make sure this jobs succeeds in 10 minutes, else just cancel it  timestamps(): to make sure we have timestamps in our logs  buildDiscarder(): this will make sure we will only keep the latest 5 builds    1\n2\n3\n4\n5\n6\n7\n8 pipeline   { \n     agent   none \n     options   { \n         timeout ( time:   10 ,   unit:   MINUTES ) \n         timestamps () \n         buildDiscarder ( logRotator ( numToKeepStr:   5 )) \n     }  }", 
            "title": "Basics"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-declarative/#checkout", 
            "text": "There are several ways to checkout the code.  Let's assume our code is somewhere in a git repository.", 
            "title": "Checkout"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-declarative/#full-checkout-command", 
            "text": "The main command for checking out is the Checkout command.  It will look like this.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 stage ( SCM )   { \n     checkout ([ \n         $class :   GitSCM ,  \n         branches:   [[ name:   */master ]],  \n         doGenerateSubmoduleConfigurations:   false ,  \n         extensions:   [],  \n         submoduleCfg:   [],  \n         userRemoteConfigs:  \n             [[ credentialsId:   MyCredentialsId ,  \n             url:   https://github.com/joostvdg/keep-watching ]] \n     ])  }", 
            "title": "Full Checkout command"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-declarative/#git-shorthand", 
            "text": "Thats a lot of configuration for a simple checkout.  So what if I'm just using the master branch of a publicly accessible repository (as is the case with GitHub)?  1\n2\n3 stage ( SCM )   { \n     git   https://github.com/joostvdg/keep-watching  }    Or with a different branch and credentials:  1\n2\n3 stage ( SCM )   { \n     git   credentialsId:   MyCredentialsId ,   url:   https://github.com/joostvdg/keep-watching  }    That's much better, but we can do even better.", 
            "title": "Git shorthand"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-declarative/#scm-shorthand", 
            "text": "If you're starting this pipeline job via a SCM, you've already configured the SCM.  So assuming you've configured a pipeline job with 'Jenkinsfile from SCM' or an abstraction job - such as Multibranch-Pipeline, GitHub Organization or BitBucket Team/Project - you can do this.  1\n2\n3 stage ( SCM )   { \n     checkout   scm  }    The  checkout scm  line will use the checkout command we've used in the first example together with the object  scm .  This scm object, will contain the SCM configuration of the Job and will be reused for checking out.   Warning  A pipeline job from SCM or abstraction, will only checkout your Jenkinsfile.\nYou will always need to checkout the rest of your code if you want to build it.\nFor that, just use  checkout scm", 
            "title": "SCM shorthand"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-declarative/#different-agent-per-stage", 
            "text": "As you could see on the top, we've set agent to none.  So for every stage we now need to tell it which agent to use - without it, the stage will fail.", 
            "title": "Different Agent per Stage"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-declarative/#agent-any", 
            "text": "If you don't care what node it comes on, you specify any.  1\n2\n3\n4\n5\n6 stage ( Checkout )   {       agent   any       steps   { \n         git   https://github.com/joostvdg/keep-watching \n     }  }", 
            "title": "Agent any"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-declarative/#agent-via-label", 
            "text": "If you want build on a node with a specific label - here  docker  - you do so with  agent { label ' LABEL ' } .   1\n2\n3\n4\n5\n6 stage ( Checkout )   {       agent   {   label   docker   }       steps   { \n         git   https://github.com/joostvdg/keep-watching \n     }  }", 
            "title": "Agent via Label"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-declarative/#docker-container-as-agent", 
            "text": "Many developers are using docker for their CI/CD builds. \nSo being able to use docker containers as build agents is a requirement these days.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 stage ( Maven Build )   { \n     agent   {           docker   {               image   maven:3-alpine               label   docker               args    -v /home/joost/.m2:/root/.m2           } \n     } \n     steps   { \n         sh   mvn -B clean package \n     }  }", 
            "title": "Docker Container as Agent"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-declarative/#cache-maven-repo", 
            "text": "When you're using a docker build container, it will be clean every time.  So if you want to avoid downloading the maven dependencies every build, you have to cache them.  One way to do this, is to map a volume into the container so the container will use that folder instead.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 stage ( Maven Build )   { \n     agent   { \n         docker   { \n             image   maven:3-alpine \n             label   docker               args    -v /home/joost/.m2:/root/.m2           } \n     } \n     steps   { \n         sh   mvn -B clean package \n     }  }", 
            "title": "Cache Maven repo"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-declarative/#post-stagebuild", 
            "text": "The declarative pipeline allows for Post actions, on both stage and complete build level.  For both types there are different post hooks you can use, such as success, failure.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 stage ( Maven Build )   { \n     agent   { \n         docker   { \n             image   maven:3-alpine \n             label   docker \n             args    -v /home/joost/.m2:/root/.m2 \n         } \n     } \n     steps   { \n         sh   mvn -B clean package \n     }       post   {           success   {               junit   target/surefire-reports/**/*.xml \n         } \n     }  }     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 stages   { \n     stage ( Example )   { \n         steps   { \n             echo   Hello World \n         } \n     }  }  post   {       always   {           echo   This will always run \n     }       success   {           echo   SUCCESS! \n     }       failure   {           echo   We Failed \n     }       unstable   {           echo   We re unstable \n     }       changed   {           echo   Status Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result] \n     }  }", 
            "title": "Post stage/build"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-declarative/#entire-example", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61 pipeline   { \n     agent   none \n     options   { \n         timeout ( time:   10 ,   unit:   MINUTES ) \n         timestamps () \n         buildDiscarder ( logRotator ( numToKeepStr:   5 )) \n     } \n     stages   { \n         stage ( Example )   { \n             steps   { \n                 echo   Hello World \n             } \n         } \n         stage ( Checkout )   { \n             agent   {   label   docker   } \n             steps   { \n                 git   https://github.com/joostvdg/keep-watching \n             } \n         } \n         stage ( Maven Build )   { \n             agent   { \n                 docker   { \n                     image   maven:3-alpine \n                     label   docker \n                     args    -v /home/joost/.m2:/root/.m2 \n                 } \n             } \n             steps   { \n                 sh   mvn -B clean package \n             } \n             post   { \n                 success   { \n                     junit   target/surefire-reports/**/*.xml \n                 } \n             } \n         } \n         stage ( Docker Build )   { \n             agent   {   label   docker   } \n             steps   { \n                 sh   docker build --tag=keep-watching-be . \n             } \n         } \n     } \n     post   { \n         always   { \n             echo   This will always run \n         } \n         success   { \n             echo   SUCCESS! \n         } \n         failure   { \n             echo   We Failed \n         } \n         unstable   { \n             echo   We re unstable \n         } \n         changed   { \n             echo   Status Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result] \n         } \n     }  }", 
            "title": "Entire example"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-shared-library/", 
            "text": "Maven Shared Library Example", 
            "title": "Maven Shared Library"
        }, 
        {
            "location": "/jenkins-pipeline-examples/maven-shared-library/#maven-shared-library-example", 
            "text": "", 
            "title": "Maven Shared Library Example"
        }, 
        {
            "location": "/jenkins-pipeline-examples/docker-declarative/", 
            "text": "Docker Declarative Examples\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\npipeline\n \n{\n\n    \nagent\n \nnone\n\n    \noptions\n \n{\n\n        \ntimeout\n(\ntime:\n \n10\n,\n \nunit:\n \nMINUTES\n)\n\n        \ntimestamps\n()\n\n        \nbuildDiscarder\n(\nlogRotator\n(\nnumToKeepStr:\n \n5\n))\n\n    \n}\n\n    \nstages\n \n{\n\n        \nstage\n(\nPrepare\n){\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nsteps\n \n{\n\n                \nparallel\n \n(\n\n                    \nClean:\n \n{\n\n                        \ndeleteDir\n()\n\n                    \n},\n\n                    \nNotifySlack:\n \n{\n\n                        \nslackSend\n \nchannel:\n \ncicd\n,\n \ncolor:\n \n#FFFF00\n,\n \nmessage:\n \nSTARTED: Job \n${env.JOB_NAME} [${env.BUILD_NUMBER}]\n (${env.BUILD_URL})\n\n                    \n}\n\n                \n)\n\n            \n}\n\n        \n}\n\n        \nstage\n(\nCheckout\n){\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nsteps\n \n{\n\n                \ngit\n \ncredentialsId:\n \n355df378-e726-4abd-90fa-e723c5c21ad5\n,\n \nurl:\n \ngit@gitlab.flusso.nl:CICD/ci-cd-docs.git\n\n                \nscript\n \n{\n\n                    \nenv\n.\nGIT_COMMIT_HASH\n \n=\n \nsh\n \nreturnStdout:\n \ntrue\n,\n \nscript:\n \ngit rev-parse --verify HEAD\n\n                \n}\n\n            \n}\n\n        \n}\n\n        \nstage\n(\nBuild Docs\n)\n \n{\n\n            \nagent\n \n{\n\n                \ndocker\n \n{\n\n                    \nimage\n \ncaladreas/mkdocs-docker-build-container\n\n                    \nlabel\n \ndocker\n\n                \n}\n\n            \n}\n\n            \nsteps\n \n{\n\n                \nsh\n \nmkdocs build\n\n            \n}\n\n        \n}\n\n        \nstage\n(\nPrepare Docker Image\n){\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nsteps\n \n{\n\n                \nparallel\n \n(\n\n                    \nTestDockerfile:\n \n{\n\n                        \nscript\n \n{\n\n                            \ndef\n \nlintResult\n \n=\n \nsh\n \nreturnStdout:\n \ntrue\n,\n \nscript:\n \ndocker run --rm -i lukasmartinelli/hadolint \n Dockerfile\n\n                            \nif\n \n(\nlintResult\n.\ntrim\n()\n \n==\n \n)\n \n{\n\n                                \nprintln\n \nLint finished with no errors\n\n                            \n}\n \nelse\n \n{\n\n                                \nprintln\n \nError found in Lint\n\n                                \nprintln\n \n${lintResult}\n\n                                \ncurrentBuild\n.\nresult\n \n=\n \nUNSTABLE\n\n                            \n}\n\n                        \n}\n\n                    \n},\n \n// end test dockerfile\n\n                    \nBuildImage:\n \n{\n\n                        \nsh\n \nchmod +x build.sh\n\n                        \nsh\n \n./build.sh\n\n                    \n}\n \n                \n)\n\n            \n}\n\n            \npost\n \n{\n\n                \nsuccess\n \n{\n\n                    \nsh\n \nchmod +x push.sh\n\n                    \nsh\n \n./push.sh\n\n                \n}\n\n            \n}\n\n        \n}\n\n        \nstage\n(\nUpdate Docker Container\n)\n \n{\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nsteps\n \n{\n\n                \nsh\n \nchmod +x container-update.sh\n\n                \nsh\n \n./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH}\n\n            \n}\n\n        \n}\n\n    \n}\n\n    \npost\n \n{\n\n        \nsuccess\n \n{\n\n            \nslackSend\n \nchannel:\n \ncicd\n,\n \ncolor:\n \n#00FF00\n,\n \nmessage:\n \nSUCCESSFUL: Job \n${env.JOB_NAME} [${env.BUILD_NUMBER}]\n (${env.BUILD_URL})\n\n        \n}\n\n        \nfailure\n \n{\n\n            \nslackSend\n \nchannel:\n \ncicd\n,\n \ncolor:\n \n#FF0000\n,\n \nmessage:\n \nFAILED: Job \n${env.JOB_NAME} [${env.BUILD_NUMBER}]\n (${env.BUILD_URL})\n\n        \n}\n\n    \n}\n\n\n}", 
            "title": "Docker Declarative"
        }, 
        {
            "location": "/jenkins-pipeline-examples/docker-declarative/#docker-declarative-examples", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87 pipeline   { \n     agent   none \n     options   { \n         timeout ( time:   10 ,   unit:   MINUTES ) \n         timestamps () \n         buildDiscarder ( logRotator ( numToKeepStr:   5 )) \n     } \n     stages   { \n         stage ( Prepare ){ \n             agent   {   label   docker   } \n             steps   { \n                 parallel   ( \n                     Clean:   { \n                         deleteDir () \n                     }, \n                     NotifySlack:   { \n                         slackSend   channel:   cicd ,   color:   #FFFF00 ,   message:   STARTED: Job  ${env.JOB_NAME} [${env.BUILD_NUMBER}]  (${env.BUILD_URL}) \n                     } \n                 ) \n             } \n         } \n         stage ( Checkout ){ \n             agent   {   label   docker   } \n             steps   { \n                 git   credentialsId:   355df378-e726-4abd-90fa-e723c5c21ad5 ,   url:   git@gitlab.flusso.nl:CICD/ci-cd-docs.git \n                 script   { \n                     env . GIT_COMMIT_HASH   =   sh   returnStdout:   true ,   script:   git rev-parse --verify HEAD \n                 } \n             } \n         } \n         stage ( Build Docs )   { \n             agent   { \n                 docker   { \n                     image   caladreas/mkdocs-docker-build-container \n                     label   docker \n                 } \n             } \n             steps   { \n                 sh   mkdocs build \n             } \n         } \n         stage ( Prepare Docker Image ){ \n             agent   {   label   docker   } \n             steps   { \n                 parallel   ( \n                     TestDockerfile:   { \n                         script   { \n                             def   lintResult   =   sh   returnStdout:   true ,   script:   docker run --rm -i lukasmartinelli/hadolint   Dockerfile \n                             if   ( lintResult . trim ()   ==   )   { \n                                 println   Lint finished with no errors \n                             }   else   { \n                                 println   Error found in Lint \n                                 println   ${lintResult} \n                                 currentBuild . result   =   UNSTABLE \n                             } \n                         } \n                     },   // end test dockerfile \n                     BuildImage:   { \n                         sh   chmod +x build.sh \n                         sh   ./build.sh \n                     }  \n                 ) \n             } \n             post   { \n                 success   { \n                     sh   chmod +x push.sh \n                     sh   ./push.sh \n                 } \n             } \n         } \n         stage ( Update Docker Container )   { \n             agent   {   label   docker   } \n             steps   { \n                 sh   chmod +x container-update.sh \n                 sh   ./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH} \n             } \n         } \n     } \n     post   { \n         success   { \n             slackSend   channel:   cicd ,   color:   #00FF00 ,   message:   SUCCESSFUL: Job  ${env.JOB_NAME} [${env.BUILD_NUMBER}]  (${env.BUILD_URL}) \n         } \n         failure   { \n             slackSend   channel:   cicd ,   color:   #FF0000 ,   message:   FAILED: Job  ${env.JOB_NAME} [${env.BUILD_NUMBER}]  (${env.BUILD_URL}) \n         } \n     }  }", 
            "title": "Docker Declarative Examples"
        }
    ]
}