{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Joost van der Griendt's Software Development Docs This is a collection of knowledge that I have gathered over the years. I find it helps me learn better if I write it down, and often use the docs at different customers as reference. Call me J My full name is Joost van der Griendt , which unfortunately cannot be pronounced well in English. In order to facilitate non-Dutch speakers, you can refer to me as J (Jay). I've worked as a Java developer in the past, but currently I'm employed as a Consultant at CloudBees . My day-to-day work involves helping clients with CI/CD, Kubernetes and Software Development Management . Or, in simple words, how to make it easy and less painful to get software to customers/clients that they want to pay for at scale. In my spare time I keep my development skills active by developing in Go and Java mostly. But I'm also a big fan of automating the creation and management of CI/CD (self-service) platforms. I'm a big fan of Open Source Software and when it makes sense, Free Free software. Which is also why this site is completely open, and open source as well. Info Curious how this site is build? Read my explanation here Tracker Your browser will tell you there's a tracker. I'm curious to understand if people are reading my docs and if so, which pages. Feel free to block the tracker (Google Analytics), most browsers are able to do so. Main Topics CI / CD (Continuous Integration / Continous Delivery) Jenkins Jenkins X CloudBees Products (my current employer as of 2018) Containers (Docker, Kubernetes, ...) SWE : Software Engineering in all its facets (building, maintaining, social aspects, psychology, etc.) Other Docs Breakdown of a Spring Boot + ReactJS Application Continuous Integration A good definition can be found here: http://www.martinfowler.com/articles/continuousIntegration.html Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\" Continuous Delivery Continuous Delivery/deployment is the next step in getting yr software changes at the desired server in order to let your clients take a look at it. This article provides a good example of it: http://www.martinfowler.com/articles/continuousIntegration.html To do Continuous Integration you need multiple environments, one to run commit tests, one or more to run secondary tests. Since you are moving executables between these environments multiple times a day, you'll want to do this automatically. So it's important to have scripts that will allow you to deploy the application into any environment easily.","title":"Me"},{"location":"#joost-van-der-griendts-software-development-docs","text":"This is a collection of knowledge that I have gathered over the years. I find it helps me learn better if I write it down, and often use the docs at different customers as reference.","title":"Joost van der Griendt's Software Development Docs"},{"location":"#call-me-j","text":"My full name is Joost van der Griendt , which unfortunately cannot be pronounced well in English. In order to facilitate non-Dutch speakers, you can refer to me as J (Jay). I've worked as a Java developer in the past, but currently I'm employed as a Consultant at CloudBees . My day-to-day work involves helping clients with CI/CD, Kubernetes and Software Development Management . Or, in simple words, how to make it easy and less painful to get software to customers/clients that they want to pay for at scale. In my spare time I keep my development skills active by developing in Go and Java mostly. But I'm also a big fan of automating the creation and management of CI/CD (self-service) platforms. I'm a big fan of Open Source Software and when it makes sense, Free Free software. Which is also why this site is completely open, and open source as well. Info Curious how this site is build? Read my explanation here","title":"Call me J"},{"location":"#tracker","text":"Your browser will tell you there's a tracker. I'm curious to understand if people are reading my docs and if so, which pages. Feel free to block the tracker (Google Analytics), most browsers are able to do so.","title":"Tracker"},{"location":"#main-topics","text":"CI / CD (Continuous Integration / Continous Delivery) Jenkins Jenkins X CloudBees Products (my current employer as of 2018) Containers (Docker, Kubernetes, ...) SWE : Software Engineering in all its facets (building, maintaining, social aspects, psychology, etc.)","title":"Main Topics"},{"location":"#other-docs","text":"Breakdown of a Spring Boot + ReactJS Application","title":"Other Docs"},{"location":"#continuous-integration","text":"A good definition can be found here: http://www.martinfowler.com/articles/continuousIntegration.html Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\"","title":"Continuous Integration"},{"location":"#continuous-delivery","text":"Continuous Delivery/deployment is the next step in getting yr software changes at the desired server in order to let your clients take a look at it. This article provides a good example of it: http://www.martinfowler.com/articles/continuousIntegration.html To do Continuous Integration you need multiple environments, one to run commit tests, one or more to run secondary tests. Since you are moving executables between these environments multiple times a day, you'll want to do this automatically. So it's important to have scripts that will allow you to deploy the application into any environment easily.","title":"Continuous Delivery"},{"location":"blogs/docker-alternatives/","text":"Pipelines With Docker Alternatives Building pipelines with Jenkins on Docker has been common for a while. But accessing the docker socket has always been a bit tricky. The easiest solution is directly mounting the docker socket of the host into your build container. However, this is a big security risk and something that is undesirable at best and, pretty dangerous at worst. When you're using an orchestrator such as Kubernetes - which is where Jenkins is currently best put to work - the problem gets worse. Not only do you open up security holes, you also mess up the schedular's insight into available resources, and mess with it's ability to keep your cluster in a correct state. In some sense, using docker directly in a Kubernetes cluster, is likened to running with scissors . Potential Alternatives So, directly running docker containers via a docker engine inside your cluster is out. Let's look at some alternatives. Kubernetes Pod and External Node : the simple way out, use a cloud (such as EC2 Cloud) to provision a classic VM with docker engine as an agent and build there JIB : tool from Google, only works for Java applications and is supported directly from Gradle and Maven - Link Kaniko : tool from Google, works as 1-on-1 replacement for docker image build (except Windows Containers ) - Link IMG : tool from Jess Frazelle to avoid building docker images with a root user involved Link Kubernetes Pod and External Node One of the most used cloud environments is AWS, so I created this solution with AWS's Amazon EC2 Plugin . Warning Unfortunately, you cannot combine the Kubernetes plugin with external nodes (none pod container nodes) in a Declarative pipeline. So you have to use Scripted . This can be done with various different cloud providers such as Digital Ocean, Google, AWS or Azure. This guide will use AWS, as it has the most mature Jenkins Cloud plugin. Prerequisites AWS Account with rights to create AMI's and run EC2 instances Packer Jenkins with Amazon EC2 Plugin installed Steps create AMI with Packer install and configure Amazon EC2 plugin create a test pipeline Create AMI with Packer Packer needs to be able to access EC2 API's and be able to spin up an EC2 instance and create an AMI out of it. AWS setup for Packer You need to configure two things: account details for Packer to use security group where your EC2 instances will be running with this security group needs to open port 22 both Packer and Jenkins will use this for their connection 1 2 3 export AWS_DEFAULT_REGION = eu-west-1 export AWS_ACCESS_KEY_ID = XXX export AWS_SECRET_ACCESS_KEY = XXX 1 2 3 4 5 6 7 aws ec2 --profile myAwsProfile create-security-group \\ --description For building Docker images \\ --group-name docker { GroupId : sg-08079f78cXXXXXXX } Export the security group ID. 1 2 export SG_ID = sg-08079f78cXXXXXXX echo $SG_ID Enable port 22 1 2 3 4 5 6 7 aws ec2 \\ --profile myAwsProfile \\ authorize-security-group-ingress \\ --group-name docker \\ --protocol tcp \\ --port 22 \\ --cidr 0 .0.0.0/0 Packer AMI definition Here's an example definition for Packer for a Ubuntu 18.04 LTS base image with JDK 8 (required by Jenkins) and Docker. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { builders : [{ type : amazon-ebs , region : eu-west-1 , source_ami_filter : { filters : { virtualization-type : hvm , name : *ubuntu-bionic-18.04-amd64-server-* , root-device-type : ebs }, owners : [ 679593333241 ], most_recent : true }, instance_type : t2.micro , ssh_username : ubuntu , ami_name : docker , force_deregister : true }], provisioners : [{ type : shell , inline : [ sleep 15 , sudo apt-get clean , sudo apt-get update , sudo apt-get install -y apt-transport-https ca-certificates nfs-common , curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - , sudo add-apt-repository \\ deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\ , sudo add-apt-repository -y ppa:openjdk-r/ppa , sudo apt-get update , sudo apt-get install -y docker-ce , sudo usermod -aG docker ubuntu , sudo apt-get install -y openjdk-8-jdk , java -version , docker version ] }] } Build the new AMI with packer. 1 2 packer build docker-ami.json export AMI = ami-0212ab37f84e418f4 EC2 Key Pair Create EC2 key pair, this will be used by Jenkins to connect to the instances via ssh. 1 2 3 4 aws ec2 --profile myAwsProfile create-key-pair \\ --key-name jenkinsec2 \\ | jq -r .KeyMaterial \\ jenkins-ec2-proton.pem EC2 Cloud Configuration In a Jenkins' master main configuration, you add a new cloud . In this case, we will use a ec2 - cloud so we can instantiate our EC2 VM's with docker. use EC2 credentials for initial connection use key ( .pem ) for VM connection (jenkins agent) configure the following: AMI: ami-0212ab37f84e418f4 availability zone: eu-west-1a VPC SubnetID: subnet-aa54XXXX Remote user: ubuntu labels: docker ubuntu linux SecurityGroup Name: (the id) sg-08079f78cXXXXXXX public ip = true connect via public ip = true Pipeline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @Library ( jenkins-pipeline-library@master ) _ def scmVars def label = jenkins-slave-${UUID.randomUUID().toString()} podTemplate ( label: label , yaml: apiVersion: v1 kind: Pod spec: containers: - name: kubectl image: vfarcic/kubectl command: [ cat ] tty: true ) { node ( label ) { node ( docker ) { stage ( SCM Prepare ) { scmVars = checkout scm } stage ( Lint ) { dockerfileLint () } stage ( Build Docker ) { sh docker image build -t demo:rc-1 . } stage ( Tag Push Docker ) { IMAGE = ${DOCKER_IMAGE_NAME} TAG = ${DOCKER_IMAGE_TAG} FULL_NAME = ${FULL_IMAGE_NAME} withCredentials ([ usernamePassword ( credentialsId: dockerhub , usernameVariable: USER , passwordVariable: PASS )]) { sh docker login -u $USER -p $PASS } sh docker image tag ${IMAGE}:${TAG} ${FULL_NAME} sh docker image push ${FULL_NAME} } } // end node docker stage ( Prepare Pod ) { // have to checkout on our kubernetes pod aswell checkout scm } stage ( Check version ) { container ( kubectl ) { sh kubectl version } } } // end node random label } // end pod def Maven JIB If you use Java with either Gradle or Maven , you can use JIB to create docker image without requiring a docker client or docker engine. JIB will communicate with DockerHub and Google's container registry for sending image layers back and forth. The last layer will be created by the JIB plugin itself, adding your self-executing Jar to the layer and a Entrypoint with the correct flags. For more information about the runtime options, see either jib-maven-plugin documentation or jib-gradle-plugin . Prerequisites Java project build with Gradle or Maven Java project that can start it self, such as Spring Boot or Thorntail (previously Wildfly Swarm, from the JBoss family) able to build either gradle or maven applications The project used can be found at github.com/demomon/maven-spring-boot-demo . Steps configure the plugin for either Gradle or Maven build using an official docker image via the kubernetes pod template Pipeline Using a bit more elaborate pipeline example here. Using SonarCloud for static code analysis and then JIB to create a docker image. We could then use that image either directly in anything that runs docker or in the same cluster via a Helm Chart. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def scmVars def tag pipeline { options { buildDiscarder logRotator ( artifactDaysToKeepStr: 5 , artifactNumToKeepStr: 5 , daysToKeepStr: 5 , numToKeepStr: 5 ) } libraries { lib ( core@master ) lib ( maven@master ) } agent { kubernetes { label mypod defaultContainer jnlp yaml apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:3-jdk-11-slim command: - cat tty: true } } stages { stage ( Test versions ) { steps { container ( maven ) { sh uname -a sh mvn -version } } } stage ( Checkout ) { steps { script { scmVars = checkout scm } gitRemoteConfigByUrl ( scmVars . GIT_URL , githubtoken ) sh git config --global user.email jenkins@jenkins.io git config --global user.name Jenkins } } stage ( Build ) { steps { container ( maven ) { sh mvn clean verify -B -e } } } stage ( Version Analysis ) { parallel { stage ( Version Bump ) { when { branch master } environment { NEW_VERSION = gitNextSemverTagMaven ( pom.xml ) } steps { script { tag = ${NEW_VERSION} } container ( maven ) { sh mvn versions:set -DnewVersion=${NEW_VERSION} } gitTag ( v${NEW_VERSION} ) } } stage ( Sonar Analysis ) { when { branch master } environment { SONAR_HOST = https://sonarcloud.io KEY = spring-maven-demo ORG = demomon SONAR_TOKEN = credentials ( sonarcloud ) } steps { container ( maven ) { sh mvn sonar:sonar \\ -Dsonar.projectKey=${KEY} \\ -Dsonar.organization=${ORG} \\ -Dsonar.host.url=${SONAR_HOST} \\ -Dsonar.login=${SONAR_TOKEN} } } } } } stage ( Publish Artifact ) { when { branch master } environment { DHUB = credentials ( dockerhub ) } steps { container ( maven ) { // we should never come here if the tests have not run, as we run verify before sh mvn clean compile -B -e jib:build -Djib.to.auth.username=${DHUB_USR} -Djib.to.auth.password=${DHUB_PSW} -DskipTests } } } } post { always { cleanWs () } } } Kaniko Google loves Kubernetes and Google prefers people building docker images in Kubernetes without Docker. As JIB is only available for Java projects, there's needs to be an alternative for any other usecase/programming language. The answer to that is Kaniko , a specialized Docker image to create Docker images. Kaniko isn't the most secure way to create docker images, it barely beats mounting a Docker Socket, or might even be worse if you ask others (such as Jess Frazelle ). That said, it is gaining some traction being used by JenkinsX and having an example in Jenkins' Kubernetes Plugin . Info When building more than one image inside the kaniko container, make sure to use the --cleanup flag. So it cleans its temporary cache data before building the next image, as discussed in this google group . Prerequisites Steps Create docker registry secret Configure pod container template Configure stage Create docker registry secret This is an example for DockerHub inside the build namespace. 1 2 3 4 5 kubectl create secret docker-registry -n build regcred \\ --docker-server = index.docker.io \\ --docker-username = myDockerHubAccount \\ --docker-password = myDockerHubPassword \\ --docker-email = myDockerHub@Email.com Example Ppeline Warning Although multi-stage Dockerfile 's are supported, it did fail in my case. So I created a second Dockerfile which is only for running the application ( Dockerfile . run ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 pipeline { agent { kubernetes { //cloud kubernetes label kaniko yaml kind: Pod metadata: name: kaniko spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root - name: go-build-cache mountPath: /root/.cache/go-build - name: img-build-cache mountPath: /root/.local volumes: - name: go-build-cache emptyDir: {} - name: img-build-cache emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { environment { PATH = /busybox:$PATH } steps { container ( name: kaniko , shell: /busybox/sh ) { sh #!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=index.docker.io/caladreas/cat } } } } } IMG img is the brainchild of Jess Frazelle, a prominent figure in the container space. The goal is to be the safest and best way to build OCI compliant images, as outlined in her blog building container images securely on kubernetes . Not working (for me) yet It does not seem to work for me on AWS's EKS. To many little details with relation to runc, file permissions and other configuration. For those who want to give it a spin, here are some resources to take a look at. https://blog.jessfraz.com/post/building-container-images-securely-on-kubernetes/ https://github.com/genuinetools/img https://github.com/opencontainers/runc https://git.j3ss.co/genuinetools/img/+/d05b3e4e10cd0e3c074ffb03dc22d7bb6cde1e78 Pipeline Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 pipeline { agent { kubernetes { label img yaml kind: Pod metadata: name: img annotations: container.apparmor.security.beta.kubernetes.io/img: unconfined spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: img workingDir: /home/jenkins image: caladreas/img:0.5.1 imagePullPolicy: Always securityContext: rawProc: true privileged: true command: - cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: temp emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { steps { container ( img ) { sh mkdir cache sh img build -s ./cache -f Dockerfile.run -t caladreas/cat . } } } } }","title":"Pipelines with Docker Alternatives"},{"location":"blogs/docker-alternatives/#pipelines-with-docker-alternatives","text":"Building pipelines with Jenkins on Docker has been common for a while. But accessing the docker socket has always been a bit tricky. The easiest solution is directly mounting the docker socket of the host into your build container. However, this is a big security risk and something that is undesirable at best and, pretty dangerous at worst. When you're using an orchestrator such as Kubernetes - which is where Jenkins is currently best put to work - the problem gets worse. Not only do you open up security holes, you also mess up the schedular's insight into available resources, and mess with it's ability to keep your cluster in a correct state. In some sense, using docker directly in a Kubernetes cluster, is likened to running with scissors .","title":"Pipelines With Docker Alternatives"},{"location":"blogs/docker-alternatives/#potential-alternatives","text":"So, directly running docker containers via a docker engine inside your cluster is out. Let's look at some alternatives. Kubernetes Pod and External Node : the simple way out, use a cloud (such as EC2 Cloud) to provision a classic VM with docker engine as an agent and build there JIB : tool from Google, only works for Java applications and is supported directly from Gradle and Maven - Link Kaniko : tool from Google, works as 1-on-1 replacement for docker image build (except Windows Containers ) - Link IMG : tool from Jess Frazelle to avoid building docker images with a root user involved Link","title":"Potential Alternatives"},{"location":"blogs/docker-alternatives/#kubernetes-pod-and-external-node","text":"One of the most used cloud environments is AWS, so I created this solution with AWS's Amazon EC2 Plugin . Warning Unfortunately, you cannot combine the Kubernetes plugin with external nodes (none pod container nodes) in a Declarative pipeline. So you have to use Scripted . This can be done with various different cloud providers such as Digital Ocean, Google, AWS or Azure. This guide will use AWS, as it has the most mature Jenkins Cloud plugin.","title":"Kubernetes Pod and External Node"},{"location":"blogs/docker-alternatives/#prerequisites","text":"AWS Account with rights to create AMI's and run EC2 instances Packer Jenkins with Amazon EC2 Plugin installed","title":"Prerequisites"},{"location":"blogs/docker-alternatives/#steps","text":"create AMI with Packer install and configure Amazon EC2 plugin create a test pipeline","title":"Steps"},{"location":"blogs/docker-alternatives/#create-ami-with-packer","text":"Packer needs to be able to access EC2 API's and be able to spin up an EC2 instance and create an AMI out of it.","title":"Create AMI with Packer"},{"location":"blogs/docker-alternatives/#aws-setup-for-packer","text":"You need to configure two things: account details for Packer to use security group where your EC2 instances will be running with this security group needs to open port 22 both Packer and Jenkins will use this for their connection 1 2 3 export AWS_DEFAULT_REGION = eu-west-1 export AWS_ACCESS_KEY_ID = XXX export AWS_SECRET_ACCESS_KEY = XXX 1 2 3 4 5 6 7 aws ec2 --profile myAwsProfile create-security-group \\ --description For building Docker images \\ --group-name docker { GroupId : sg-08079f78cXXXXXXX } Export the security group ID. 1 2 export SG_ID = sg-08079f78cXXXXXXX echo $SG_ID","title":"AWS setup for Packer"},{"location":"blogs/docker-alternatives/#enable-port-22","text":"1 2 3 4 5 6 7 aws ec2 \\ --profile myAwsProfile \\ authorize-security-group-ingress \\ --group-name docker \\ --protocol tcp \\ --port 22 \\ --cidr 0 .0.0.0/0","title":"Enable port 22"},{"location":"blogs/docker-alternatives/#packer-ami-definition","text":"Here's an example definition for Packer for a Ubuntu 18.04 LTS base image with JDK 8 (required by Jenkins) and Docker. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { builders : [{ type : amazon-ebs , region : eu-west-1 , source_ami_filter : { filters : { virtualization-type : hvm , name : *ubuntu-bionic-18.04-amd64-server-* , root-device-type : ebs }, owners : [ 679593333241 ], most_recent : true }, instance_type : t2.micro , ssh_username : ubuntu , ami_name : docker , force_deregister : true }], provisioners : [{ type : shell , inline : [ sleep 15 , sudo apt-get clean , sudo apt-get update , sudo apt-get install -y apt-transport-https ca-certificates nfs-common , curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - , sudo add-apt-repository \\ deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\ , sudo add-apt-repository -y ppa:openjdk-r/ppa , sudo apt-get update , sudo apt-get install -y docker-ce , sudo usermod -aG docker ubuntu , sudo apt-get install -y openjdk-8-jdk , java -version , docker version ] }] } Build the new AMI with packer. 1 2 packer build docker-ami.json export AMI = ami-0212ab37f84e418f4","title":"Packer AMI definition"},{"location":"blogs/docker-alternatives/#ec2-key-pair","text":"Create EC2 key pair, this will be used by Jenkins to connect to the instances via ssh. 1 2 3 4 aws ec2 --profile myAwsProfile create-key-pair \\ --key-name jenkinsec2 \\ | jq -r .KeyMaterial \\ jenkins-ec2-proton.pem","title":"EC2 Key Pair"},{"location":"blogs/docker-alternatives/#ec2-cloud-configuration","text":"In a Jenkins' master main configuration, you add a new cloud . In this case, we will use a ec2 - cloud so we can instantiate our EC2 VM's with docker. use EC2 credentials for initial connection use key ( .pem ) for VM connection (jenkins agent) configure the following: AMI: ami-0212ab37f84e418f4 availability zone: eu-west-1a VPC SubnetID: subnet-aa54XXXX Remote user: ubuntu labels: docker ubuntu linux SecurityGroup Name: (the id) sg-08079f78cXXXXXXX public ip = true connect via public ip = true","title":"EC2 Cloud Configuration"},{"location":"blogs/docker-alternatives/#pipeline","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @Library ( jenkins-pipeline-library@master ) _ def scmVars def label = jenkins-slave-${UUID.randomUUID().toString()} podTemplate ( label: label , yaml: apiVersion: v1 kind: Pod spec: containers: - name: kubectl image: vfarcic/kubectl command: [ cat ] tty: true ) { node ( label ) { node ( docker ) { stage ( SCM Prepare ) { scmVars = checkout scm } stage ( Lint ) { dockerfileLint () } stage ( Build Docker ) { sh docker image build -t demo:rc-1 . } stage ( Tag Push Docker ) { IMAGE = ${DOCKER_IMAGE_NAME} TAG = ${DOCKER_IMAGE_TAG} FULL_NAME = ${FULL_IMAGE_NAME} withCredentials ([ usernamePassword ( credentialsId: dockerhub , usernameVariable: USER , passwordVariable: PASS )]) { sh docker login -u $USER -p $PASS } sh docker image tag ${IMAGE}:${TAG} ${FULL_NAME} sh docker image push ${FULL_NAME} } } // end node docker stage ( Prepare Pod ) { // have to checkout on our kubernetes pod aswell checkout scm } stage ( Check version ) { container ( kubectl ) { sh kubectl version } } } // end node random label } // end pod def","title":"Pipeline"},{"location":"blogs/docker-alternatives/#maven-jib","text":"If you use Java with either Gradle or Maven , you can use JIB to create docker image without requiring a docker client or docker engine. JIB will communicate with DockerHub and Google's container registry for sending image layers back and forth. The last layer will be created by the JIB plugin itself, adding your self-executing Jar to the layer and a Entrypoint with the correct flags. For more information about the runtime options, see either jib-maven-plugin documentation or jib-gradle-plugin .","title":"Maven JIB"},{"location":"blogs/docker-alternatives/#prerequisites_1","text":"Java project build with Gradle or Maven Java project that can start it self, such as Spring Boot or Thorntail (previously Wildfly Swarm, from the JBoss family) able to build either gradle or maven applications The project used can be found at github.com/demomon/maven-spring-boot-demo .","title":"Prerequisites"},{"location":"blogs/docker-alternatives/#steps_1","text":"configure the plugin for either Gradle or Maven build using an official docker image via the kubernetes pod template","title":"Steps"},{"location":"blogs/docker-alternatives/#pipeline_1","text":"Using a bit more elaborate pipeline example here. Using SonarCloud for static code analysis and then JIB to create a docker image. We could then use that image either directly in anything that runs docker or in the same cluster via a Helm Chart. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def scmVars def tag pipeline { options { buildDiscarder logRotator ( artifactDaysToKeepStr: 5 , artifactNumToKeepStr: 5 , daysToKeepStr: 5 , numToKeepStr: 5 ) } libraries { lib ( core@master ) lib ( maven@master ) } agent { kubernetes { label mypod defaultContainer jnlp yaml apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:3-jdk-11-slim command: - cat tty: true } } stages { stage ( Test versions ) { steps { container ( maven ) { sh uname -a sh mvn -version } } } stage ( Checkout ) { steps { script { scmVars = checkout scm } gitRemoteConfigByUrl ( scmVars . GIT_URL , githubtoken ) sh git config --global user.email jenkins@jenkins.io git config --global user.name Jenkins } } stage ( Build ) { steps { container ( maven ) { sh mvn clean verify -B -e } } } stage ( Version Analysis ) { parallel { stage ( Version Bump ) { when { branch master } environment { NEW_VERSION = gitNextSemverTagMaven ( pom.xml ) } steps { script { tag = ${NEW_VERSION} } container ( maven ) { sh mvn versions:set -DnewVersion=${NEW_VERSION} } gitTag ( v${NEW_VERSION} ) } } stage ( Sonar Analysis ) { when { branch master } environment { SONAR_HOST = https://sonarcloud.io KEY = spring-maven-demo ORG = demomon SONAR_TOKEN = credentials ( sonarcloud ) } steps { container ( maven ) { sh mvn sonar:sonar \\ -Dsonar.projectKey=${KEY} \\ -Dsonar.organization=${ORG} \\ -Dsonar.host.url=${SONAR_HOST} \\ -Dsonar.login=${SONAR_TOKEN} } } } } } stage ( Publish Artifact ) { when { branch master } environment { DHUB = credentials ( dockerhub ) } steps { container ( maven ) { // we should never come here if the tests have not run, as we run verify before sh mvn clean compile -B -e jib:build -Djib.to.auth.username=${DHUB_USR} -Djib.to.auth.password=${DHUB_PSW} -DskipTests } } } } post { always { cleanWs () } } }","title":"Pipeline"},{"location":"blogs/docker-alternatives/#kaniko","text":"Google loves Kubernetes and Google prefers people building docker images in Kubernetes without Docker. As JIB is only available for Java projects, there's needs to be an alternative for any other usecase/programming language. The answer to that is Kaniko , a specialized Docker image to create Docker images. Kaniko isn't the most secure way to create docker images, it barely beats mounting a Docker Socket, or might even be worse if you ask others (such as Jess Frazelle ). That said, it is gaining some traction being used by JenkinsX and having an example in Jenkins' Kubernetes Plugin . Info When building more than one image inside the kaniko container, make sure to use the --cleanup flag. So it cleans its temporary cache data before building the next image, as discussed in this google group .","title":"Kaniko"},{"location":"blogs/docker-alternatives/#prerequisites_2","text":"","title":"Prerequisites"},{"location":"blogs/docker-alternatives/#steps_2","text":"Create docker registry secret Configure pod container template Configure stage","title":"Steps"},{"location":"blogs/docker-alternatives/#create-docker-registry-secret","text":"This is an example for DockerHub inside the build namespace. 1 2 3 4 5 kubectl create secret docker-registry -n build regcred \\ --docker-server = index.docker.io \\ --docker-username = myDockerHubAccount \\ --docker-password = myDockerHubPassword \\ --docker-email = myDockerHub@Email.com","title":"Create docker registry secret"},{"location":"blogs/docker-alternatives/#example-ppeline","text":"Warning Although multi-stage Dockerfile 's are supported, it did fail in my case. So I created a second Dockerfile which is only for running the application ( Dockerfile . run ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 pipeline { agent { kubernetes { //cloud kubernetes label kaniko yaml kind: Pod metadata: name: kaniko spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root - name: go-build-cache mountPath: /root/.cache/go-build - name: img-build-cache mountPath: /root/.local volumes: - name: go-build-cache emptyDir: {} - name: img-build-cache emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { environment { PATH = /busybox:$PATH } steps { container ( name: kaniko , shell: /busybox/sh ) { sh #!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=index.docker.io/caladreas/cat } } } } }","title":"Example Ppeline"},{"location":"blogs/docker-alternatives/#img","text":"img is the brainchild of Jess Frazelle, a prominent figure in the container space. The goal is to be the safest and best way to build OCI compliant images, as outlined in her blog building container images securely on kubernetes .","title":"IMG"},{"location":"blogs/docker-alternatives/#not-working-for-me-yet","text":"It does not seem to work for me on AWS's EKS. To many little details with relation to runc, file permissions and other configuration. For those who want to give it a spin, here are some resources to take a look at. https://blog.jessfraz.com/post/building-container-images-securely-on-kubernetes/ https://github.com/genuinetools/img https://github.com/opencontainers/runc https://git.j3ss.co/genuinetools/img/+/d05b3e4e10cd0e3c074ffb03dc22d7bb6cde1e78","title":"Not working (for me) yet"},{"location":"blogs/docker-alternatives/#pipeline-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 pipeline { agent { kubernetes { label img yaml kind: Pod metadata: name: img annotations: container.apparmor.security.beta.kubernetes.io/img: unconfined spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: img workingDir: /home/jenkins image: caladreas/img:0.5.1 imagePullPolicy: Always securityContext: rawProc: true privileged: true command: - cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: temp emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { steps { container ( img ) { sh mkdir cache sh img build -s ./cache -f Dockerfile.run -t caladreas/cat . } } } } }","title":"Pipeline Example"},{"location":"blogs/dockercon-eu-2018/","text":"DockerCon EU 2018 - Recap Generally outline From my perspective, there were a few red threads throughout the conference. Security begins at the Developer A shift left of security, bringing the responsibility of knowing your dependencies and their quality firmly to the developer. Most of this tooling is still aimed at enterprises though, being part of paid solutions mostly. At least that which was shown at the conference. docker-assemble, that can build in an image from a Maven pom.xml and will include meta-data of all your dependencies (transitive included) JFrog X-Ray Docker EE tooling such as Docker Trusted Registry (DTR) Broader Automation More focus on the entire lifecycle of a system and not just an application. It seems people are starting to understand that doing CI/CD and Infrastructure As Code is not a single event for a single application. There is likely to be a few applications belonging together making a whole system which will land on more than one type of infrastructure and possibly more types of clusters. What we see is tools looking at either a broader scope, a higher level abstraction or more developer focussed (more love for the Dev in DevOps) to allow for easier integration with multiple platforms. For example, Pulumi will enable you to create any type of infrastructure - like Hashicorp's Terraform - but then in programming languages, you're used to (TypeScript, Python, Go). Pulumi Docker App CNAB Build-Kit Containerization Influences Everything Containerization has left deep and easy to spot imprints in our industry from startups building entirely on top of containers to programming languages changing their ways to stay relevant. There are new monitoring kings in the world, DataDog, Sysdig, Honeycomb.io and so on. They live and breathe containers and are not afraid of being thrown around different public clouds, networks and what not. In contrast to traditional monitoring tools, which are often bolting on container support and struggle with the dynamic nature of containerized clusters. Another extraordinary influence is that on the Java language. Declared dead a million times over and deemed obsolete in the container era due to its massive footprint in image size and runtime size. Both are being addressed, and we see a lot of work done on reducing footprint and static linking (JLink, Graal). The most significant influence might be on the software behemoth that has rejuvenated itself. Microsoft has sworn allegiance to open source, Linux and containers. Windows 2019 server can run container workloads natively and work as nodes alongside a Docker EE cluster - which can include Kubernetes workloads. The next step would be support for Kubernetes integration, and as in the case of Java, smaller container footprint. Java Docker Windows Container Windows Server Support Observability tools Kubernetes offerings everywhere... Docker Build with Build-Kit Instead of investing in improving docker image building via the Docker Client, Docker created a new API and client library. This library called BuildKit, is completely independent. With Docker 18.09, it is included in the Docker Client allowing anyone to use it as easily as the traditional docker image build . BuildKit is already used by some other tools, such as Buildah and IMG, and allows you to create custom DSL \"Frontends\". As long as the API of BuikdKit is adhered to, the resulting image will be OCI compliant. So further remarks below and how to use it. BuildKit In-Depth session Supercharged Docker Build with BuildKit Usable from Docker 18 . 09 HighLights: allows custom DSL for specifying image (BuildKit) to still be used with Docker client/daemon build cache for your own files during build, think Go, Maven, Gradle... much more optimized, builds less, quicker, with more cache in less time support mounts (cache) such as secrets, during build phase 1 2 3 # Set env variable to enable # Or configure docker s json config export DOCKER_BUILDKIT = 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # syntax=docker/dockerfile:experimental ####################################### ## 1. BUILD JAR WITH MAVEN FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src ! RUN --mount = type = cache,target = /root/.m2/ mvn clean package -e ####################################### ## 2. BUILD NATIVE IMAGE WITH GRAAL FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from = BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath ####################################### ## 3. BUILD DOCKER RUNTIME IMAGE FROM alpine:3.8 CMD [ jpc-graal ] COPY --from = NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal ####################################### Secure your Kubernetes https://www.openpolicyagent.org + admission controller Network Policies Service Accounts Broader Cloud Automation Two of the broader cloud automation initiatives that impressed me at DockerCon were Pulumi and CNAB. Where Pulumi is an attempt to provide a more developer-friendly alternative to Terraform, CNAB is an attempt to create an environment agnostic installer specification. Meaning, you could create a CNAB installer which uses Pulumi to install all required infrastructure, applications and other resources. CNAB: cloud native application bundle Bundle.json invocation image (oci) = installer https://cnab.io docker app implements it helm support https://github.com/deislabs Install an implementation There are currently two implementations - that I found. Duffle from DeisLabs - open source from Azure - and Docker App - From Docker Inc.. Duffle create a new repo clone the repo init a duffle bundle copy duffle bundle data to our repo folder 1 2 3 4 git clone git@github.com:demomon/cnab-duffle-demo-1.git duffle create cnab-duffle-demo-2 mv cnab-duffle-demo-2/cnab cnab-duffle-demo-1/ mv cnab-duffle-demo-2/duffle.json cnab-duffle-demo-1/ edit our install file ( cnab / run ) build our duffle bundle ( duffle build . ) We can now inspect our bundle with duffle. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 duffle show cnab-duffle-demo-1:0.1.0 -r -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA256 { name : cnab-duffle-demo-1 , version : 0.1.0 , description : A short description of your bundle , keywords : [ cnab-duffle-demo-1 , cnab , demo , joostvdg ] , maintainers : [ { name : Joost van der Griendt , email : joostvdg@gmail.com , url : https://joostvdg.github.io } ] , invocationImages : [ { imageType : docker , image : deislabs/cnab-duffle-demo-1-cnab:2965aad7406e1b651a98fffe1194fcaaec5e623c } ] , images : null, parameters : null, credentials : null } -----BEGIN PGP SIGNATURE----- wsDcBAEBCAAQBQJcL3L8CRDgq4YEOipZ/QAAjDYMAJI5o66SteUP2o4HsbVk+Viw 3Fd874vSVmpPKcmN3tRCEDWGdMdvqQiirDpa//ghx4y5oFTahK2ihQ35GbJLlq8S v9/CK6CKGJtp5g38s2LZrKIvESzEF2lTXwHB03PG8PJ37iWiYkHkxvMpyzded3Rs 4d+VgUnip0Cre7DemaUaz5+fTQjs88WNTIhqPg47YvgqFXV0s1y7yN3RTLr3ohQ1 9mkw87nWfOD+ULpbCUaq9FhNZ+v4dK5IZcWlkyv+yrtguyBBiA3MC54ueVBAdFCl 2OhxXgZjbBHPfQPV1mPqCQudOsWjK/+gqyNb6KTzKrAnyrumVQli/C/8BVk/SRC/ GS2o4EdTS2lfREc2Gl0/VTmMkqzFZZhWd7pwt/iMjl0bICFehSU0N6OqN1d+v6Sq vWIZ5ppxt1NnCzp05Y+NRfVZOxBc2xjYTquFwIa/+qGPrmXBKamw/irjmCOndvx+ l1tf/g0UVSQI2R2/19svl7dlMkYpDdlth1YGgZi/Hg == = 8Xwi -----END PGP SIGNATURE----- Demo I've created a demo on GitHub: github.com/demomon/cnab-duffle-demo-1 Its goal is to install CloudBees Core and its prerequisites in a (GKE) Kubernetes cluster. It wraps a Go (lang) binary that will execute the commands, for which you can find the source code on GitHub . Components A CNAB bundle has some components by default, for this demo we needed the following: duffle.json : Duffle configuration file Dockerfile : the CNAB installer runtime run (script): the installer script kgi (binary): the binary executable from my k8s-gitops-installer code, that we will leverage for the installation Dockerfile The installer tool ( kgi ) requires Helm and Kubectl , so we a Docker image that has those. As we might end up packaging the entire image as part of the full CNAB package, it should also be based on Alpine (or similar minimal Linux). There seems to be one very well maintained and widely used (according to GitHub and Dockerhub stats): dtzar / helm - kubect . So no need to roll our own. 1 2 3 4 5 6 7 FROM dtzar/helm-kubectl:2.12.1 COPY Dockerfile /cnab/Dockerfile COPY app /cnab/app COPY kgi /usr/bin RUN ls -lath /cnab/app/ RUN kgi --help CMD [ /cnab/app/run ] duffle.json The only thing we need to add beyond the auto-generated file, is the credentials section. 1 2 3 4 5 credentials : { kubeconfig : { path : /cnab/app/kube-config } } kgi I pre-build a binary suitable for Linux that works in Alpine and included it in the CNAB folder. run script First thing we need to make sure, is to configure the Kubeconfig location. 1 export KUBECONFIG = /cnab/app/kube-config This should match what we defined in the duffle . json configuration - as you might expect - to make sure it gets bound in the right location. The kubectl command now knows which file to use. For the rest, we can do what we want, but convention tells us we need at least support status , install and uninstall . I'm lazy and only implemented install at the moment. In the install action, we will use the kgi executable to install CloudBees Core and it's pre-requisites. 1 2 3 4 5 6 7 8 action = $CNAB_ACTION case $action in install ) echo [Install] kgi validate kubectl ;; esac For the rest, I recommend you look at the sources. Run the demo First, we have to build the bundle. 1 duffle build . Once the build succeeded, we can create a credentials configuration. This will be a separate configuration file managed by Duffle. This configuration config must then be used with any installation that requires it - which makes it reusable as well. We have to populate it with the credential. In this case a path to a kube-config file. If you do not have one that you can export - e.g. based on GCloud - you can create a new user/certificate with a script. This is taken from gravitational , which were nice enough to create a script for doing so. You can find the script on GitHub (get-kubeconfig.sh) . Once you have that, store the end result at a decent place and configure it as your credential. 1 duffle creds generate demo1 cnab-duffle-demo-1 With the above command we can create a credential config object based on the build bundle cnab - duffle - demo - 1 . The credential object will be demo - 1 , which we can now use for installing. 1 duffle install demo1 cnab-duffle-demo-1:1.0.0 -c demo1 Further reading Howto guide on creating a Duffle Bundle Howto on handling credentials Wordpress with Kubernetes and AWS demo by Bitnami Example bundles Pulumi Just pointing to the documentation and GitHub repository is a bit boring and most of all, lazy. So I've gone through the trouble to create a demo project that shows what Pulumi can do. One of the things to keep in mind is that Pulumi keeps its state in a remote location by default - Pulumi's SaaS service. Much like using an S3 bucket to store your Terraform state. The goal of the demo is to show what you can do with Pulumi. My personal opinion is that Pulumi is excellent for managing entire systems as code. From public cloud infrastructure to public open source resources right down to your applications and their configurations. The system that I've written with Pulumi is a CI/CD system running in a GKE cluster. The system contains Jenkins, Artifactory and a custom LDAP server. To make it interesting, we'll use different aspects of Pulumi to create the system. We will create the GKE cluster via Pulumi's wrapper for the gcloud cli - in a similar way as JenkinsX does -, install Jenkins and Artifactory via Helm and install the LDAP server via Pulumi's Kubernetes resource classes. Steps taken For more info Pulumi.io install: brew install pulumi clone demo: git clone https : // github . com / demomon / pulumi - demo - 1 init stack: pulumi stack init demomon - pulumi - demo - 1 connect to GitHub set kubernetes config pulumi config set kubernetes : context gke_ps - dev - 201405 _europe - west4_joostvdg - reg - dec18 - 1 pulumi config set isMinikube false install npm resources: npm install (I've used the TypeScript demo's) pulumi config set username administrator pulumi config set password 3 OvlgaockdnTsYRU5JAcgM1o --secret pulumi preview incase pulumi loses your stack: pulumi stack select demomon - pulumi - demo - 1 pulumi destroy Based on the following demo's: https://pulumi.io/quickstart/kubernetes/tutorial-exposed-deployment.html https://github.com/pulumi/examples/tree/master/kubernetes-ts-jenkins Artifactory via Helm To install the helm charts of Artifactory using Helm, we will first need to add JFrog's repository. helm repo add jfrog https : // charts . jfrog . io helm repo update GKE Cluster Below is the code for the cluster. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import * as gcp from @pulumi/gcp ; import * as k8s from @pulumi/kubernetes ; import * as pulumi from @pulumi/pulumi ; import { nodeCount , nodeMachineType , password , username } from ./gke-config ; export const k8sCluster = new gcp . container . Cluster ( gke-cluster , { name : joostvdg-dec-2018-pulumi , initialNodeCount : nodeCount , nodeVersion : latest , minMasterVersion : latest , nodeConfig : { machineType : nodeMachineType , oauthScopes : [ https://www.googleapis.com/auth/compute , https://www.googleapis.com/auth/devstorage.read_only , https://www.googleapis.com/auth/logging.write , https://www.googleapis.com/auth/monitoring ], }, }); GKE Config As you could see, we import variables from a configuration file gke - config . 1 2 3 4 5 6 7 8 import { Config } from @pulumi/pulumi ; const config = new Config (); export const nodeCount = config . getNumber ( nodeCount ) || 3 ; export const nodeMachineType = config . get ( nodeMachineType ) || n1-standard-2 ; // username is the admin username for the cluster. export const username = config . get ( username ) || admin ; // password is the password for the admin user in the cluster. export const password = config . require ( password ); Kubeconfig As you probably will want to install other things (Helm charts, services) inside this cluster, we need to make sure we get the kubeconfig file of our cluster - which is yet to be created. Below is the code - courtesy of Pulumi's GKE Example - that generates and exports the Kubernetes Client Configuration. This client configuration can then be used by Helm charts or other Kubernetes services. Pulumi will then also understand it depends on the cluster and create/update it first before moving on to the others. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // Manufacture a GKE-style Kubeconfig. Note that this is slightly different because of the way GKE requires // gcloud to be in the picture for cluster authentication (rather than using the client cert/key directly). export const k8sConfig = pulumi . all ([ k8sCluster . name , k8sCluster . endpoint , k8sCluster . masterAuth ]). apply (([ name , endpoint , auth ]) = { const context = ` ${ gcp . config . project } _ ${ gcp . config . zone } _ ${ name } ` ; return `apiVersion: v1 clusters: - cluster: certificate-authority-data: ${ auth . clusterCaCertificate } server: https:// ${ endpoint } name: ${ context } contexts: - context: cluster: ${ context } user: ${ context } name: ${ context } current-context: ${ context } kind: Config preferences: {} users: - name: ${ context } user: auth-provider: config: cmd-args: config config-helper --format=json cmd-path: gcloud expiry-key: {.credential.token_expiry} token-key: {.credential.access_token} name: gcp ` ; }); // Export a Kubernetes provider instance that uses our cluster from above. export const k8sProvider = new k8s . Provider ( gkeK8s , { kubeconfig : k8sConfig , }); Pulumi GCP Config https://github.com/pulumi/examples/blob/master/gcp-ts-gke/README.md 1 2 3 4 export GCP_PROJECT = ... export GCP_ZONE = europe-west4-a export CLUSTER_PASSWORD = ... export GCP_SA_NAME = ... Make sure you have a Google SA (Service Account) by that name first, as you can read here . For me it worked best to NOT set any environment variables mentioned. They invairably caused authentication or authorization issues. Just make sure the SA account and it's credential file (see below) are authorized and the gcloud cli works. 1 2 3 4 gcloud iam service-accounts keys create gcp-credentials.json \\ --iam-account ${ GCP_SA_NAME } @ ${ GCP_PROJECT } .iam.gserviceaccount.com gcloud auth activate-service-account --key-file gcp-credentials.json gcloud auth application-default login 1 2 3 pulumi config set gcp:project ${ GCP_PROJECT } pulumi config set gcp:zone ${ GCP_ZONE } pulumi config set password --secret ${ CLUSTER_PASSWORD } Post Cluster Creation 1 2 gcloud container clusters get-credentials joostvdg-dec-2018-pulumi kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $( gcloud config get-value account ) Install failed Failed to install kubernetes : rbac . authorization . k8s . io : Role artifactory - artifactory . Probably due to missing rights, so probably have to execute the admin binding before the helm charts. 1 error: Plan apply failed: roles.rbac.authorization.k8s.io artifactory-artifactory is forbidden: attempt to grant extra privileges: ... Helm Charts Using Pulumi to install a Helm Chart feels a bit like adding layers of wrapping upon wrapping. The power of Pulumi becomes visible when using more than one related service on the same cluster - for example a SDLC Tool Chain. This example application installs two helm charts, Jenkins and Artifactory, on a GKE cluster that is also created and managed by Pulumi. Below is an example of installing a Helm chart of Jenkins, where we provide the Kubernetes config from the GKE cluster as Provider. This way, Pulumi knows it must install the helm chart in that GKE cluster and not in the current Kubeconfig. 1 2 3 4 5 6 7 8 9 10 import { k8sProvider , k8sConfig } from ./gke-cluster ; const jenkins = new k8s . helm . v2 . Chart ( jenkins , { repo : stable , version : 0.25.1 , chart : jenkins , }, { providers : { kubernetes : k8sProvider } } ); Deployment Service First, make sure you have an interface for the configuration arguments. 1 2 3 4 5 export interface LdapArgs { readonly name : string , readonly imageName : string , readonly imageTag : string } Then, create a exportable Pulumi resource class that can be reused. 1 2 3 4 5 6 export class LdapInstallation extends pulumi . ComponentResource { public readonly deployment : k8s.apps.v1.Deployment ; public readonly service : k8s.core.v1.Service ; // constructor } Inside the constructor placehold we will create a constructor method. It will do all the configuration we need to do for this resource, in this case a Kubernetes Service and Deployment. 1 2 3 4 5 constructor ( args : LdapArgs ) { super ( k8stypes:service:LdapInstallation , args . name , {}); const labels = { app : args.name }; const name = args . name } First Kubernetes resource to create is a container specification for the Deployment. 1 2 3 4 5 6 7 8 9 10 11 12 const container : k8stypes.core.v1.Container = { name , image : args.imageName + : + args . imageTag , resources : { requests : { cpu : 100m , memory : 200Mi }, limits : { cpu : 100m , memory : 200Mi }, }, ports : [{ name : ldap , containerPort : 1389 , }, ] }; As the configuration arguments can be any TypeScript type, you can allow people to override entire segments (such as Resources). Which you would do as follows: 1 2 3 4 resources : args.resources || { requests : { cpu : 100m , memory : 200Mi }, limits : { cpu : 100m , memory : 200Mi }, }, The Deployment and Service construction are quite similar. 1 2 3 4 5 6 7 8 9 10 this . deployment = new k8s . apps . v1 . Deployment ( args . name , { spec : { selector : { matchLabels : labels }, replicas : 1 , template : { metadata : { labels : labels }, spec : { containers : [ container ] }, }, }, },{ provider : cluster.k8sProvider }); 1 2 3 4 5 6 7 8 9 10 11 12 13 this . service = new k8s . core . v1 . Service ( args . name , { metadata : { labels : this.deployment.metadata.apply ( meta = meta . labels ), }, spec : { ports : [{ name : ldap , port : 389 , targetPort : ldap , protocol : TCP }, ], selector : this.deployment.spec.apply ( spec = spec . template . metadata . labels ), type : ClusterIP , }, }, { provider : cluster.k8sProvider }); JFrog Jenkins Challenge Visited the stand of JFrog where they had stories about two main products: Artifactory and X-Ray. For both there is a Challenge, an X-Ray Challenge and a Jenkins Artifactory Challenge . Jenkins Challenge The instructions for the Challenge were simply, follow what is stated in their GitHub repository and email a screenshot of the result. The instruction were as follows: Get an Artifactory instance (you can start a free trial on prem or in the cloud) Install Jenkins Install Artifactory Jenkins Plugin Add Artifactory credentials to Jenkins Credentials Create a new pipeline job Use the Artifactory Plugin DSL documentation to complete the following script: With a Scripted Pipeline as starting point: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 node { def rtServer def rtGradle def buildInfo stage ( Preparation ) { git https://github.com/jbaruch/gradle-example.git // create a new Artifactory server using the credentials defined in Jenkins // create a new Gradle build // set the resolver to the Gradle build to resolve from Artifactory // set the deployer to the Gradle build to deploy to Artifactory // declare that your gradle script does not use Artifactory plugin // declare that your gradle script uses Gradle wrapper } stage ( Build ) { //run the artifactoryPublish gradle task and collect the build info } stage ( Publish Build Info ) { //collect the environment variables to build info //publish the build info } } I don't like scripted, so I opted for Declarative with Jenkins in Kubernetes with the Jenkins Kubernetes plugin. Steps I took: get a trial license from the JFrog website install Artifactory and copy in the license when prompted change admin password create local maven repo 'libs-snapshot-local' create remote maven repo 'jcenter' (default remote value is jcenter, so only have to set the name) install Jenkins Artifactory plugin Kubernetes plugin add Artifactory username/password as credential in Jenkins create a gradle application (Spring boot via start.spring.io) which you can find here create a Jenkinsfile Installing Artifactory I installed Artifactory via Helm. JFrog has their own Helm repository - of course, would weird otherwise tbh - and you have to add that first. 1 2 helm repo add jfrog https://charts.jfrog.io helm install --name artifactory stable/artifactory Jenkinsfile This uses the Gradle wrapper - as per instructions in the challenge. So we can use the standard JNLP container, which is default, so agent any will do. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 pipeline { agent any environment { rtServer = rtGradle = buildInfo = artifactoryServerAddress = http://..../artifactory } stages { stage ( Test Container ) { steps { container ( gradle ) { sh which gradle sh uname -a sh gradle -version } } } stage ( Checkout ){ steps { git https://github.com/demomon/gradle-jenkins-challenge.git } } stage ( Preparation ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: artifactoryServerAddress , credentialsId: art-admin // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: jcenter , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: libs-snapshot-local , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( Build ) { steps { script { //run the artifactoryPublish gradle task and collect the build info buildInfo = rtGradle . run buildFile: build.gradle , tasks: clean build artifactoryPublish } } } stage ( Publish Build Info ) { steps { script { //collect the environment variables to build info buildInfo . env . capture = true //publish the build info rtServer . publishBuildInfo buildInfo } } } } } Jenkinsfile without Gradle Wrapper I'd rather not install the Gradle tool if I can just use a pre-build container with it. Unfortunately, to use it correctly with the Artifactory plugin and a Jenkins Kubernetes plugin, we need to do two things. create a Gradle Tool in the Jenkins master because the Artifactory plugin expects a Jenkins Tool object, not a location Manage Jenkins - Global Tool Configuration - Gradle - Add As value supply / usr , the Artifactory build will add / gradle / bin to it automatically set the user of build Pod to id 1000 explicitly else the build will not be allowed to touch files in / home / jenkins / workspace 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 pipeline { agent { kubernetes { label mypod yaml apiVersion: v1 kind: Pod spec: securityContext: runAsUser: 1000 fsGroup: 1000 containers: - name: gradle image: gradle:4.10-jdk-alpine command: [ cat ] tty: true } } environment { rtServer = rtGradle = buildInfo = CONTAINER_GRADLE_TOOL = /usr/bin/gradle } stages { stage ( Test Container ) { steps { container ( gradle ) { sh which gradle sh uname -a sh gradle -version } } } stage ( Checkout ){ steps { // git https://github.com/demomon/gradle-jenkins-challenge.git checkout scm } } stage ( Preparation ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: http://35.204.238.14/artifactory , credentialsId: art-admin // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: jcenter , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: libs-snapshot-local , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( Build ) { //run the artifactoryPublish gradle task and collect the build info steps { script { buildInfo = rtGradle . run buildFile: build.gradle , tasks: clean build artifactoryPublish } } } stage ( Publish Build Info ) { //collect the environment variables to build info //publish the build info steps { script { buildInfo . env . capture = true rtServer . publishBuildInfo buildInfo } } } } } Docker security standards security takes place in every layer/lifecycle phase for scaling, security needs to be part of developer's day-to-day as everything is code, anything part of the sdlc should be secure and auditable use an admission controller network policies automate your security processes expand your security automation by adding learnings Docker Java CICD telepresence Distroless (google mini os) OpenJ9 Portala (for jdk 12) wagoodman/dive use jre for the runtime instead of jdk buildkit can use mounttarget for local caches add labels with Metadata (depency trees) grafeas kritis FindSecBugs org.owasp:dependency-check-maven arminc/clair-scanner jlink = in limbo Docker Windows specific base images for different use cases Docker capabilities heavily depend on Windows Server version Other Docker pipeline Dind + privileged mount socket windows linux Windows build agent provisioning with docker EE Jenkins Docker swarm update_config Idea: build a dynamic ci/cd platform with kubernetes jenkins evergreen + jcasc kubernetes plugin gitops pipeline AKS + virtual kubelet + ACI Jenkins + EC2 Pluging + ECS/Fargate jenkins agent as ecs task (fargate agent) docker on windows, only on ECS Apply Diplomacy to Code Review apply diplomacy to code review always positive remove human resistantance with inclusive language improvement focused persist, kindly Citizens Bank journey started with swarm, grew towards kubernetes (ucp) elk stack, centralised operations cluster Docker EE - Assemble Docker EE now has a binary called docker - assemble . This allows you to build a Docker image directly from something like a pom.xml, much like JIB. Other","title":"DockerCon EU 2018"},{"location":"blogs/dockercon-eu-2018/#dockercon-eu-2018-recap","text":"","title":"DockerCon EU 2018 - Recap"},{"location":"blogs/dockercon-eu-2018/#generally-outline","text":"From my perspective, there were a few red threads throughout the conference.","title":"Generally outline"},{"location":"blogs/dockercon-eu-2018/#security-begins-at-the-developer","text":"A shift left of security, bringing the responsibility of knowing your dependencies and their quality firmly to the developer. Most of this tooling is still aimed at enterprises though, being part of paid solutions mostly. At least that which was shown at the conference. docker-assemble, that can build in an image from a Maven pom.xml and will include meta-data of all your dependencies (transitive included) JFrog X-Ray Docker EE tooling such as Docker Trusted Registry (DTR)","title":"Security begins at the Developer"},{"location":"blogs/dockercon-eu-2018/#broader-automation","text":"More focus on the entire lifecycle of a system and not just an application. It seems people are starting to understand that doing CI/CD and Infrastructure As Code is not a single event for a single application. There is likely to be a few applications belonging together making a whole system which will land on more than one type of infrastructure and possibly more types of clusters. What we see is tools looking at either a broader scope, a higher level abstraction or more developer focussed (more love for the Dev in DevOps) to allow for easier integration with multiple platforms. For example, Pulumi will enable you to create any type of infrastructure - like Hashicorp's Terraform - but then in programming languages, you're used to (TypeScript, Python, Go). Pulumi Docker App CNAB Build-Kit","title":"Broader Automation"},{"location":"blogs/dockercon-eu-2018/#containerization-influences-everything","text":"Containerization has left deep and easy to spot imprints in our industry from startups building entirely on top of containers to programming languages changing their ways to stay relevant. There are new monitoring kings in the world, DataDog, Sysdig, Honeycomb.io and so on. They live and breathe containers and are not afraid of being thrown around different public clouds, networks and what not. In contrast to traditional monitoring tools, which are often bolting on container support and struggle with the dynamic nature of containerized clusters. Another extraordinary influence is that on the Java language. Declared dead a million times over and deemed obsolete in the container era due to its massive footprint in image size and runtime size. Both are being addressed, and we see a lot of work done on reducing footprint and static linking (JLink, Graal). The most significant influence might be on the software behemoth that has rejuvenated itself. Microsoft has sworn allegiance to open source, Linux and containers. Windows 2019 server can run container workloads natively and work as nodes alongside a Docker EE cluster - which can include Kubernetes workloads. The next step would be support for Kubernetes integration, and as in the case of Java, smaller container footprint. Java Docker Windows Container Windows Server Support Observability tools Kubernetes offerings everywhere...","title":"Containerization Influences Everything"},{"location":"blogs/dockercon-eu-2018/#docker-build-with-build-kit","text":"Instead of investing in improving docker image building via the Docker Client, Docker created a new API and client library. This library called BuildKit, is completely independent. With Docker 18.09, it is included in the Docker Client allowing anyone to use it as easily as the traditional docker image build . BuildKit is already used by some other tools, such as Buildah and IMG, and allows you to create custom DSL \"Frontends\". As long as the API of BuikdKit is adhered to, the resulting image will be OCI compliant. So further remarks below and how to use it. BuildKit In-Depth session Supercharged Docker Build with BuildKit Usable from Docker 18 . 09 HighLights: allows custom DSL for specifying image (BuildKit) to still be used with Docker client/daemon build cache for your own files during build, think Go, Maven, Gradle... much more optimized, builds less, quicker, with more cache in less time support mounts (cache) such as secrets, during build phase 1 2 3 # Set env variable to enable # Or configure docker s json config export DOCKER_BUILDKIT = 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # syntax=docker/dockerfile:experimental ####################################### ## 1. BUILD JAR WITH MAVEN FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src ! RUN --mount = type = cache,target = /root/.m2/ mvn clean package -e ####################################### ## 2. BUILD NATIVE IMAGE WITH GRAAL FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from = BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath ####################################### ## 3. BUILD DOCKER RUNTIME IMAGE FROM alpine:3.8 CMD [ jpc-graal ] COPY --from = NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal #######################################","title":"Docker Build with Build-Kit"},{"location":"blogs/dockercon-eu-2018/#secure-your-kubernetes","text":"https://www.openpolicyagent.org + admission controller Network Policies Service Accounts","title":"Secure your Kubernetes"},{"location":"blogs/dockercon-eu-2018/#broader-cloud-automation","text":"Two of the broader cloud automation initiatives that impressed me at DockerCon were Pulumi and CNAB. Where Pulumi is an attempt to provide a more developer-friendly alternative to Terraform, CNAB is an attempt to create an environment agnostic installer specification. Meaning, you could create a CNAB installer which uses Pulumi to install all required infrastructure, applications and other resources.","title":"Broader Cloud Automation"},{"location":"blogs/dockercon-eu-2018/#cnab-cloud-native-application-bundle","text":"Bundle.json invocation image (oci) = installer https://cnab.io docker app implements it helm support https://github.com/deislabs","title":"CNAB: cloud native application bundle"},{"location":"blogs/dockercon-eu-2018/#install-an-implementation","text":"There are currently two implementations - that I found. Duffle from DeisLabs - open source from Azure - and Docker App - From Docker Inc..","title":"Install an implementation"},{"location":"blogs/dockercon-eu-2018/#duffle","text":"create a new repo clone the repo init a duffle bundle copy duffle bundle data to our repo folder 1 2 3 4 git clone git@github.com:demomon/cnab-duffle-demo-1.git duffle create cnab-duffle-demo-2 mv cnab-duffle-demo-2/cnab cnab-duffle-demo-1/ mv cnab-duffle-demo-2/duffle.json cnab-duffle-demo-1/ edit our install file ( cnab / run ) build our duffle bundle ( duffle build . ) We can now inspect our bundle with duffle. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 duffle show cnab-duffle-demo-1:0.1.0 -r -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA256 { name : cnab-duffle-demo-1 , version : 0.1.0 , description : A short description of your bundle , keywords : [ cnab-duffle-demo-1 , cnab , demo , joostvdg ] , maintainers : [ { name : Joost van der Griendt , email : joostvdg@gmail.com , url : https://joostvdg.github.io } ] , invocationImages : [ { imageType : docker , image : deislabs/cnab-duffle-demo-1-cnab:2965aad7406e1b651a98fffe1194fcaaec5e623c } ] , images : null, parameters : null, credentials : null } -----BEGIN PGP SIGNATURE----- wsDcBAEBCAAQBQJcL3L8CRDgq4YEOipZ/QAAjDYMAJI5o66SteUP2o4HsbVk+Viw 3Fd874vSVmpPKcmN3tRCEDWGdMdvqQiirDpa//ghx4y5oFTahK2ihQ35GbJLlq8S v9/CK6CKGJtp5g38s2LZrKIvESzEF2lTXwHB03PG8PJ37iWiYkHkxvMpyzded3Rs 4d+VgUnip0Cre7DemaUaz5+fTQjs88WNTIhqPg47YvgqFXV0s1y7yN3RTLr3ohQ1 9mkw87nWfOD+ULpbCUaq9FhNZ+v4dK5IZcWlkyv+yrtguyBBiA3MC54ueVBAdFCl 2OhxXgZjbBHPfQPV1mPqCQudOsWjK/+gqyNb6KTzKrAnyrumVQli/C/8BVk/SRC/ GS2o4EdTS2lfREc2Gl0/VTmMkqzFZZhWd7pwt/iMjl0bICFehSU0N6OqN1d+v6Sq vWIZ5ppxt1NnCzp05Y+NRfVZOxBc2xjYTquFwIa/+qGPrmXBKamw/irjmCOndvx+ l1tf/g0UVSQI2R2/19svl7dlMkYpDdlth1YGgZi/Hg == = 8Xwi -----END PGP SIGNATURE-----","title":"Duffle"},{"location":"blogs/dockercon-eu-2018/#demo","text":"I've created a demo on GitHub: github.com/demomon/cnab-duffle-demo-1 Its goal is to install CloudBees Core and its prerequisites in a (GKE) Kubernetes cluster. It wraps a Go (lang) binary that will execute the commands, for which you can find the source code on GitHub .","title":"Demo"},{"location":"blogs/dockercon-eu-2018/#components","text":"A CNAB bundle has some components by default, for this demo we needed the following: duffle.json : Duffle configuration file Dockerfile : the CNAB installer runtime run (script): the installer script kgi (binary): the binary executable from my k8s-gitops-installer code, that we will leverage for the installation","title":"Components"},{"location":"blogs/dockercon-eu-2018/#dockerfile","text":"The installer tool ( kgi ) requires Helm and Kubectl , so we a Docker image that has those. As we might end up packaging the entire image as part of the full CNAB package, it should also be based on Alpine (or similar minimal Linux). There seems to be one very well maintained and widely used (according to GitHub and Dockerhub stats): dtzar / helm - kubect . So no need to roll our own. 1 2 3 4 5 6 7 FROM dtzar/helm-kubectl:2.12.1 COPY Dockerfile /cnab/Dockerfile COPY app /cnab/app COPY kgi /usr/bin RUN ls -lath /cnab/app/ RUN kgi --help CMD [ /cnab/app/run ]","title":"Dockerfile"},{"location":"blogs/dockercon-eu-2018/#dufflejson","text":"The only thing we need to add beyond the auto-generated file, is the credentials section. 1 2 3 4 5 credentials : { kubeconfig : { path : /cnab/app/kube-config } }","title":"duffle.json"},{"location":"blogs/dockercon-eu-2018/#kgi","text":"I pre-build a binary suitable for Linux that works in Alpine and included it in the CNAB folder.","title":"kgi"},{"location":"blogs/dockercon-eu-2018/#run-script","text":"First thing we need to make sure, is to configure the Kubeconfig location. 1 export KUBECONFIG = /cnab/app/kube-config This should match what we defined in the duffle . json configuration - as you might expect - to make sure it gets bound in the right location. The kubectl command now knows which file to use. For the rest, we can do what we want, but convention tells us we need at least support status , install and uninstall . I'm lazy and only implemented install at the moment. In the install action, we will use the kgi executable to install CloudBees Core and it's pre-requisites. 1 2 3 4 5 6 7 8 action = $CNAB_ACTION case $action in install ) echo [Install] kgi validate kubectl ;; esac For the rest, I recommend you look at the sources.","title":"run script"},{"location":"blogs/dockercon-eu-2018/#run-the-demo","text":"First, we have to build the bundle. 1 duffle build . Once the build succeeded, we can create a credentials configuration. This will be a separate configuration file managed by Duffle. This configuration config must then be used with any installation that requires it - which makes it reusable as well. We have to populate it with the credential. In this case a path to a kube-config file. If you do not have one that you can export - e.g. based on GCloud - you can create a new user/certificate with a script. This is taken from gravitational , which were nice enough to create a script for doing so. You can find the script on GitHub (get-kubeconfig.sh) . Once you have that, store the end result at a decent place and configure it as your credential. 1 duffle creds generate demo1 cnab-duffle-demo-1 With the above command we can create a credential config object based on the build bundle cnab - duffle - demo - 1 . The credential object will be demo - 1 , which we can now use for installing. 1 duffle install demo1 cnab-duffle-demo-1:1.0.0 -c demo1","title":"Run the demo"},{"location":"blogs/dockercon-eu-2018/#further-reading","text":"Howto guide on creating a Duffle Bundle Howto on handling credentials Wordpress with Kubernetes and AWS demo by Bitnami Example bundles","title":"Further reading"},{"location":"blogs/dockercon-eu-2018/#pulumi","text":"Just pointing to the documentation and GitHub repository is a bit boring and most of all, lazy. So I've gone through the trouble to create a demo project that shows what Pulumi can do. One of the things to keep in mind is that Pulumi keeps its state in a remote location by default - Pulumi's SaaS service. Much like using an S3 bucket to store your Terraform state. The goal of the demo is to show what you can do with Pulumi. My personal opinion is that Pulumi is excellent for managing entire systems as code. From public cloud infrastructure to public open source resources right down to your applications and their configurations. The system that I've written with Pulumi is a CI/CD system running in a GKE cluster. The system contains Jenkins, Artifactory and a custom LDAP server. To make it interesting, we'll use different aspects of Pulumi to create the system. We will create the GKE cluster via Pulumi's wrapper for the gcloud cli - in a similar way as JenkinsX does -, install Jenkins and Artifactory via Helm and install the LDAP server via Pulumi's Kubernetes resource classes.","title":"Pulumi"},{"location":"blogs/dockercon-eu-2018/#steps-taken","text":"For more info Pulumi.io install: brew install pulumi clone demo: git clone https : // github . com / demomon / pulumi - demo - 1 init stack: pulumi stack init demomon - pulumi - demo - 1 connect to GitHub set kubernetes config pulumi config set kubernetes : context gke_ps - dev - 201405 _europe - west4_joostvdg - reg - dec18 - 1 pulumi config set isMinikube false install npm resources: npm install (I've used the TypeScript demo's) pulumi config set username administrator pulumi config set password 3 OvlgaockdnTsYRU5JAcgM1o --secret pulumi preview incase pulumi loses your stack: pulumi stack select demomon - pulumi - demo - 1 pulumi destroy Based on the following demo's: https://pulumi.io/quickstart/kubernetes/tutorial-exposed-deployment.html https://github.com/pulumi/examples/tree/master/kubernetes-ts-jenkins","title":"Steps taken"},{"location":"blogs/dockercon-eu-2018/#artifactory-via-helm","text":"To install the helm charts of Artifactory using Helm, we will first need to add JFrog's repository. helm repo add jfrog https : // charts . jfrog . io helm repo update","title":"Artifactory via Helm"},{"location":"blogs/dockercon-eu-2018/#gke-cluster","text":"Below is the code for the cluster. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import * as gcp from @pulumi/gcp ; import * as k8s from @pulumi/kubernetes ; import * as pulumi from @pulumi/pulumi ; import { nodeCount , nodeMachineType , password , username } from ./gke-config ; export const k8sCluster = new gcp . container . Cluster ( gke-cluster , { name : joostvdg-dec-2018-pulumi , initialNodeCount : nodeCount , nodeVersion : latest , minMasterVersion : latest , nodeConfig : { machineType : nodeMachineType , oauthScopes : [ https://www.googleapis.com/auth/compute , https://www.googleapis.com/auth/devstorage.read_only , https://www.googleapis.com/auth/logging.write , https://www.googleapis.com/auth/monitoring ], }, });","title":"GKE Cluster"},{"location":"blogs/dockercon-eu-2018/#gke-config","text":"As you could see, we import variables from a configuration file gke - config . 1 2 3 4 5 6 7 8 import { Config } from @pulumi/pulumi ; const config = new Config (); export const nodeCount = config . getNumber ( nodeCount ) || 3 ; export const nodeMachineType = config . get ( nodeMachineType ) || n1-standard-2 ; // username is the admin username for the cluster. export const username = config . get ( username ) || admin ; // password is the password for the admin user in the cluster. export const password = config . require ( password );","title":"GKE Config"},{"location":"blogs/dockercon-eu-2018/#kubeconfig","text":"As you probably will want to install other things (Helm charts, services) inside this cluster, we need to make sure we get the kubeconfig file of our cluster - which is yet to be created. Below is the code - courtesy of Pulumi's GKE Example - that generates and exports the Kubernetes Client Configuration. This client configuration can then be used by Helm charts or other Kubernetes services. Pulumi will then also understand it depends on the cluster and create/update it first before moving on to the others. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // Manufacture a GKE-style Kubeconfig. Note that this is slightly different because of the way GKE requires // gcloud to be in the picture for cluster authentication (rather than using the client cert/key directly). export const k8sConfig = pulumi . all ([ k8sCluster . name , k8sCluster . endpoint , k8sCluster . masterAuth ]). apply (([ name , endpoint , auth ]) = { const context = ` ${ gcp . config . project } _ ${ gcp . config . zone } _ ${ name } ` ; return `apiVersion: v1 clusters: - cluster: certificate-authority-data: ${ auth . clusterCaCertificate } server: https:// ${ endpoint } name: ${ context } contexts: - context: cluster: ${ context } user: ${ context } name: ${ context } current-context: ${ context } kind: Config preferences: {} users: - name: ${ context } user: auth-provider: config: cmd-args: config config-helper --format=json cmd-path: gcloud expiry-key: {.credential.token_expiry} token-key: {.credential.access_token} name: gcp ` ; }); // Export a Kubernetes provider instance that uses our cluster from above. export const k8sProvider = new k8s . Provider ( gkeK8s , { kubeconfig : k8sConfig , });","title":"Kubeconfig"},{"location":"blogs/dockercon-eu-2018/#pulumi-gcp-config","text":"https://github.com/pulumi/examples/blob/master/gcp-ts-gke/README.md 1 2 3 4 export GCP_PROJECT = ... export GCP_ZONE = europe-west4-a export CLUSTER_PASSWORD = ... export GCP_SA_NAME = ... Make sure you have a Google SA (Service Account) by that name first, as you can read here . For me it worked best to NOT set any environment variables mentioned. They invairably caused authentication or authorization issues. Just make sure the SA account and it's credential file (see below) are authorized and the gcloud cli works. 1 2 3 4 gcloud iam service-accounts keys create gcp-credentials.json \\ --iam-account ${ GCP_SA_NAME } @ ${ GCP_PROJECT } .iam.gserviceaccount.com gcloud auth activate-service-account --key-file gcp-credentials.json gcloud auth application-default login 1 2 3 pulumi config set gcp:project ${ GCP_PROJECT } pulumi config set gcp:zone ${ GCP_ZONE } pulumi config set password --secret ${ CLUSTER_PASSWORD }","title":"Pulumi GCP Config"},{"location":"blogs/dockercon-eu-2018/#post-cluster-creation","text":"1 2 gcloud container clusters get-credentials joostvdg-dec-2018-pulumi kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $( gcloud config get-value account )","title":"Post Cluster Creation"},{"location":"blogs/dockercon-eu-2018/#install-failed","text":"Failed to install kubernetes : rbac . authorization . k8s . io : Role artifactory - artifactory . Probably due to missing rights, so probably have to execute the admin binding before the helm charts. 1 error: Plan apply failed: roles.rbac.authorization.k8s.io artifactory-artifactory is forbidden: attempt to grant extra privileges: ...","title":"Install failed"},{"location":"blogs/dockercon-eu-2018/#helm-charts","text":"Using Pulumi to install a Helm Chart feels a bit like adding layers of wrapping upon wrapping. The power of Pulumi becomes visible when using more than one related service on the same cluster - for example a SDLC Tool Chain. This example application installs two helm charts, Jenkins and Artifactory, on a GKE cluster that is also created and managed by Pulumi. Below is an example of installing a Helm chart of Jenkins, where we provide the Kubernetes config from the GKE cluster as Provider. This way, Pulumi knows it must install the helm chart in that GKE cluster and not in the current Kubeconfig. 1 2 3 4 5 6 7 8 9 10 import { k8sProvider , k8sConfig } from ./gke-cluster ; const jenkins = new k8s . helm . v2 . Chart ( jenkins , { repo : stable , version : 0.25.1 , chart : jenkins , }, { providers : { kubernetes : k8sProvider } } );","title":"Helm Charts"},{"location":"blogs/dockercon-eu-2018/#deployment-service","text":"First, make sure you have an interface for the configuration arguments. 1 2 3 4 5 export interface LdapArgs { readonly name : string , readonly imageName : string , readonly imageTag : string } Then, create a exportable Pulumi resource class that can be reused. 1 2 3 4 5 6 export class LdapInstallation extends pulumi . ComponentResource { public readonly deployment : k8s.apps.v1.Deployment ; public readonly service : k8s.core.v1.Service ; // constructor } Inside the constructor placehold we will create a constructor method. It will do all the configuration we need to do for this resource, in this case a Kubernetes Service and Deployment. 1 2 3 4 5 constructor ( args : LdapArgs ) { super ( k8stypes:service:LdapInstallation , args . name , {}); const labels = { app : args.name }; const name = args . name } First Kubernetes resource to create is a container specification for the Deployment. 1 2 3 4 5 6 7 8 9 10 11 12 const container : k8stypes.core.v1.Container = { name , image : args.imageName + : + args . imageTag , resources : { requests : { cpu : 100m , memory : 200Mi }, limits : { cpu : 100m , memory : 200Mi }, }, ports : [{ name : ldap , containerPort : 1389 , }, ] }; As the configuration arguments can be any TypeScript type, you can allow people to override entire segments (such as Resources). Which you would do as follows: 1 2 3 4 resources : args.resources || { requests : { cpu : 100m , memory : 200Mi }, limits : { cpu : 100m , memory : 200Mi }, }, The Deployment and Service construction are quite similar. 1 2 3 4 5 6 7 8 9 10 this . deployment = new k8s . apps . v1 . Deployment ( args . name , { spec : { selector : { matchLabels : labels }, replicas : 1 , template : { metadata : { labels : labels }, spec : { containers : [ container ] }, }, }, },{ provider : cluster.k8sProvider }); 1 2 3 4 5 6 7 8 9 10 11 12 13 this . service = new k8s . core . v1 . Service ( args . name , { metadata : { labels : this.deployment.metadata.apply ( meta = meta . labels ), }, spec : { ports : [{ name : ldap , port : 389 , targetPort : ldap , protocol : TCP }, ], selector : this.deployment.spec.apply ( spec = spec . template . metadata . labels ), type : ClusterIP , }, }, { provider : cluster.k8sProvider });","title":"Deployment &amp; Service"},{"location":"blogs/dockercon-eu-2018/#jfrog-jenkins-challenge","text":"Visited the stand of JFrog where they had stories about two main products: Artifactory and X-Ray. For both there is a Challenge, an X-Ray Challenge and a Jenkins Artifactory Challenge .","title":"JFrog Jenkins Challenge"},{"location":"blogs/dockercon-eu-2018/#jenkins-challenge","text":"The instructions for the Challenge were simply, follow what is stated in their GitHub repository and email a screenshot of the result. The instruction were as follows: Get an Artifactory instance (you can start a free trial on prem or in the cloud) Install Jenkins Install Artifactory Jenkins Plugin Add Artifactory credentials to Jenkins Credentials Create a new pipeline job Use the Artifactory Plugin DSL documentation to complete the following script: With a Scripted Pipeline as starting point: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 node { def rtServer def rtGradle def buildInfo stage ( Preparation ) { git https://github.com/jbaruch/gradle-example.git // create a new Artifactory server using the credentials defined in Jenkins // create a new Gradle build // set the resolver to the Gradle build to resolve from Artifactory // set the deployer to the Gradle build to deploy to Artifactory // declare that your gradle script does not use Artifactory plugin // declare that your gradle script uses Gradle wrapper } stage ( Build ) { //run the artifactoryPublish gradle task and collect the build info } stage ( Publish Build Info ) { //collect the environment variables to build info //publish the build info } } I don't like scripted, so I opted for Declarative with Jenkins in Kubernetes with the Jenkins Kubernetes plugin. Steps I took: get a trial license from the JFrog website install Artifactory and copy in the license when prompted change admin password create local maven repo 'libs-snapshot-local' create remote maven repo 'jcenter' (default remote value is jcenter, so only have to set the name) install Jenkins Artifactory plugin Kubernetes plugin add Artifactory username/password as credential in Jenkins create a gradle application (Spring boot via start.spring.io) which you can find here create a Jenkinsfile","title":"Jenkins Challenge"},{"location":"blogs/dockercon-eu-2018/#installing-artifactory","text":"I installed Artifactory via Helm. JFrog has their own Helm repository - of course, would weird otherwise tbh - and you have to add that first. 1 2 helm repo add jfrog https://charts.jfrog.io helm install --name artifactory stable/artifactory","title":"Installing Artifactory"},{"location":"blogs/dockercon-eu-2018/#jenkinsfile","text":"This uses the Gradle wrapper - as per instructions in the challenge. So we can use the standard JNLP container, which is default, so agent any will do. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 pipeline { agent any environment { rtServer = rtGradle = buildInfo = artifactoryServerAddress = http://..../artifactory } stages { stage ( Test Container ) { steps { container ( gradle ) { sh which gradle sh uname -a sh gradle -version } } } stage ( Checkout ){ steps { git https://github.com/demomon/gradle-jenkins-challenge.git } } stage ( Preparation ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: artifactoryServerAddress , credentialsId: art-admin // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: jcenter , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: libs-snapshot-local , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( Build ) { steps { script { //run the artifactoryPublish gradle task and collect the build info buildInfo = rtGradle . run buildFile: build.gradle , tasks: clean build artifactoryPublish } } } stage ( Publish Build Info ) { steps { script { //collect the environment variables to build info buildInfo . env . capture = true //publish the build info rtServer . publishBuildInfo buildInfo } } } } }","title":"Jenkinsfile"},{"location":"blogs/dockercon-eu-2018/#jenkinsfile-without-gradle-wrapper","text":"I'd rather not install the Gradle tool if I can just use a pre-build container with it. Unfortunately, to use it correctly with the Artifactory plugin and a Jenkins Kubernetes plugin, we need to do two things. create a Gradle Tool in the Jenkins master because the Artifactory plugin expects a Jenkins Tool object, not a location Manage Jenkins - Global Tool Configuration - Gradle - Add As value supply / usr , the Artifactory build will add / gradle / bin to it automatically set the user of build Pod to id 1000 explicitly else the build will not be allowed to touch files in / home / jenkins / workspace 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 pipeline { agent { kubernetes { label mypod yaml apiVersion: v1 kind: Pod spec: securityContext: runAsUser: 1000 fsGroup: 1000 containers: - name: gradle image: gradle:4.10-jdk-alpine command: [ cat ] tty: true } } environment { rtServer = rtGradle = buildInfo = CONTAINER_GRADLE_TOOL = /usr/bin/gradle } stages { stage ( Test Container ) { steps { container ( gradle ) { sh which gradle sh uname -a sh gradle -version } } } stage ( Checkout ){ steps { // git https://github.com/demomon/gradle-jenkins-challenge.git checkout scm } } stage ( Preparation ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: http://35.204.238.14/artifactory , credentialsId: art-admin // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: jcenter , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: libs-snapshot-local , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( Build ) { //run the artifactoryPublish gradle task and collect the build info steps { script { buildInfo = rtGradle . run buildFile: build.gradle , tasks: clean build artifactoryPublish } } } stage ( Publish Build Info ) { //collect the environment variables to build info //publish the build info steps { script { buildInfo . env . capture = true rtServer . publishBuildInfo buildInfo } } } } }","title":"Jenkinsfile without Gradle Wrapper"},{"location":"blogs/dockercon-eu-2018/#docker-security-standards","text":"security takes place in every layer/lifecycle phase for scaling, security needs to be part of developer's day-to-day as everything is code, anything part of the sdlc should be secure and auditable use an admission controller network policies automate your security processes expand your security automation by adding learnings","title":"Docker security &amp; standards"},{"location":"blogs/dockercon-eu-2018/#docker-java-cicd","text":"telepresence Distroless (google mini os) OpenJ9 Portala (for jdk 12) wagoodman/dive use jre for the runtime instead of jdk buildkit can use mounttarget for local caches add labels with Metadata (depency trees) grafeas kritis FindSecBugs org.owasp:dependency-check-maven arminc/clair-scanner jlink = in limbo","title":"Docker &amp; Java &amp; CICD"},{"location":"blogs/dockercon-eu-2018/#docker-windows","text":"specific base images for different use cases Docker capabilities heavily depend on Windows Server version","title":"Docker &amp; Windows"},{"location":"blogs/dockercon-eu-2018/#other","text":"","title":"Other"},{"location":"blogs/dockercon-eu-2018/#docker-pipeline","text":"Dind + privileged mount socket windows linux Windows build agent provisioning with docker EE Jenkins Docker swarm update_config","title":"Docker pipeline"},{"location":"blogs/dockercon-eu-2018/#idea-build-a-dynamic-cicd-platform-with-kubernetes","text":"jenkins evergreen + jcasc kubernetes plugin gitops pipeline AKS + virtual kubelet + ACI Jenkins + EC2 Pluging + ECS/Fargate jenkins agent as ecs task (fargate agent) docker on windows, only on ECS","title":"Idea: build a dynamic ci/cd platform with kubernetes"},{"location":"blogs/dockercon-eu-2018/#apply-diplomacy-to-code-review","text":"apply diplomacy to code review always positive remove human resistantance with inclusive language improvement focused persist, kindly","title":"Apply Diplomacy to Code Review"},{"location":"blogs/dockercon-eu-2018/#citizens-bank-journey","text":"started with swarm, grew towards kubernetes (ucp) elk stack, centralised operations cluster","title":"Citizens Bank journey"},{"location":"blogs/dockercon-eu-2018/#docker-ee-assemble","text":"Docker EE now has a binary called docker - assemble . This allows you to build a Docker image directly from something like a pom.xml, much like JIB.","title":"Docker EE - Assemble"},{"location":"blogs/dockercon-eu-2018/#other_1","text":"","title":"Other"},{"location":"blogs/gitops-pipeline/","text":"GitOps Pipeline with Jenkins on Kubernetes Missing pieces secrets via Vault service mesh infrastructure as code (around the clusters) multiple clusters Pulumi Rancher","title":"GitOps Pipeline with Jenkins on Kubernetes"},{"location":"blogs/gitops-pipeline/#gitops-pipeline-with-jenkins-on-kubernetes","text":"","title":"GitOps Pipeline with Jenkins on Kubernetes"},{"location":"blogs/gitops-pipeline/#missing-pieces","text":"secrets via Vault service mesh infrastructure as code (around the clusters) multiple clusters Pulumi Rancher","title":"Missing pieces"},{"location":"blogs/graceful-shutdown/","text":"Gracefully Shutting Down Applications in Docker I'm not sure about you, but I prefer it when my neighbors leave our shared spaces clean and don't take up parking spaces when they don't need them. Imagine you live in an apartment complex with the above-mentioned parking lot. Some tenants go away and never come back. If nothing is done to clean up after them - to reclaim their apartment and parking space - then after some time, more and more apartments are unavailable for no reason, and the parking lot fills with cars which belong to no one. Some tenants did not get a parking lot and are getting frustrated that none are becoming available. When they moved in, they were told that when others leave, they would be next in line. While they're waiting, they have to park outside the complex. Eventually, the entrance gets blocked and no one can enter or leave. The end result is a completely unlivable apartment block with trapped tenants - never to be seen or heard from again. If you agree with me that when a tenant leaves, they should clean the apartment and free the parking spot to make it ready for the next inhabitant; then please read on. We're going to dive into the equivalent of doing this with containers. We will explore running our containers with Docker (run, compose, swarm) and Kubernetes. Even if you use another way to run your containers, this article should provide you with enough insight to get you on your way. The case for graceful shutdown We're in an age where many applications are running in Docker containers across a multitude of clusters. These applications are then confronted with new concerns to tackle such as more moving parts, networking between these parts, remote storage and others. One significant way we defend ourselves against the perils of this distributed nature is to make our applications more robust - able to survive errors. However, even then there is still no guarantee your application is always up and running. So another concern we should tackle is how it responds when it needs to shut down. Where we can differentiate between an unexpected shutdown - we crashed - or an expected shutdown. On top of that, failing instead of trying to recover when something bad happens also adheres to \"fail fast\" - as strongly advocated by Michael Nygard in ReleaseIt. Shutting down can happen for a variety of reasons, in this post we dive into how to deal with an expected shutdown such as it being told to stop by an orchestrator such as Kubernetes. Containers can be purposefully shut down for a variety of reasons, including but not limited too: your application's health check fails your application consumed more resources than allowed the application is scaling down Just as cleaning up when leaving makes you a better tenant, having your application clean up connections, resources Moreover, the more tenants behaving in a good way increases the quality of living for all tenants. In our case, it improves the reliability and consistency of our cluster. Graceful shutdown is not unique to Docker, as it has been part of Linux's best practices for quite some years before Docker's existence. However, applying them to Docker container adds extra dimensions. Start Good So You Can End Well When you sign up for an apartment, you probably have to sign a contract detailing your rights and obligations. The more you state explicitly, the easier it is to deal with bad behaving neighbors. The same is true when running processes; we should make sure that we set the rules, obligations, and expectations from the start. As we say in Dutch: a good beginning is half the work. We will start with how you can run a process in a container with a process that shuts down gracefully. There are many ways to start a process in a container. In this article, we look at processes started by commands defined in a Dockerfile. There are two ways to specify this: CMD : runs a command when the container gets started ENTRYPOINT : provides the location (entrypoint) from where commands get run when the container starts You need at least one ENTRYPOINT or CMD in a Dockerfile for it to be valid. They can be used in collaboration but they can do similar things. For more information on the details of these commands, read Docker's docs on Entrypoint vs. CMD . Docker Shell form example We start with the shell form and see if it can do what we want; begin in such a way, we can stop it nicely. Shell form means we define a shell command without any special format or keywords. Please create Dockerfile with the content that follows. 1 2 FROM ubuntu:18.04 ENTRYPOINT top -b Then build an image and run a container. 1 2 docker image build --tag shell-form . docker run --name shell-form --rm shell-form The above command yields the following output. 1 2 3 4 5 6 7 8 9 top - 16 :34:56 up 1 day, 5 :15, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 541984 free, 302668 used, 1202280 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1579380 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 4624 760 696 S 0 .0 0 .0 0 :00.05 sh 6 root 20 0 36480 2928 2580 R 0 .0 0 .1 0 :00.01 top As you can see, two processes are running, sh and top . Meaning, that killing the process, with ctrl+c for example, terminates the sh process, but not top . This happens because the sh process forked the top process, but the termination will only be send to PID 1 - in this case sh . As sh will not stop the top process for us it will continue running and leave the container alive. To kill this container, open a second terminal and execute the following command. 1 docker rm -f shell-form Shell form doesn't do what we need. Starting a process with shell form will only lead us to the disaster of parking lots filling up unless there's a someone actively cleaning up. Docker exec form example This leads us to the exec form. Hopefully, this gets us somewhere. The exec form is written as an array of parameters: ENTRYPOINT [ top , -b ] To continue in the same line of examples, we will create a Dockerfile, build and run it. 1 2 FROM ubuntu:18.04 ENTRYPOINT [ top , -b ] Then build and run it. 1 2 docker image build --tag exec-form . docker run --name exec-form --rm exec-form This yields the following output. 1 2 3 4 5 6 7 8 top - 18 :12:30 up 1 day, 6 :53, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 535896 free, 307196 used, 1203840 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1574880 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 36480 2940 2584 R 0 .0 0 .1 0 :00.03 top Now we got something we can work with. If something would tell this Container to stop, it will tell our only running process so it is sure to reach the correct one! Gotchas Knowing we can use the exec form for our goal - gracefully shutting down our container - we can move on to the next part of our efforts. For the sake of imparting you with some hard learned lessons, we will explore two gotchas. They're optional, so you can also choose to skip to Make Sure Your Process Listens . Docker exec form with parameters A caveat with the exec form is that it doesn't interpolate parameters. You can try the following: 1 2 3 FROM ubuntu:18.04 ENV PARAM = -b ENTRYPOINT [ top , ${PARAM} ] Then build and run it: 1 2 docker image build --tag exec-param . docker run --name exec-form --rm exec-param This should yield the following: 1 /bin/sh: 1 : [ top: not found This is where Docker created a mix between the two styles. It allows you to create an Entrypoint with a shell command - performing interpolation - but executing it as an exec form. This can be done by prefixing the shell form, with, you guessed it, exec . 1 2 3 FROM ubuntu:18.04 ENV PARAM = -b ENTRYPOINT exec top ${ PARAM } Then build and run it: 1 2 docker image build --tag exec-param . docker run --name exec-form --rm exec-param This will return the exact same as if we would've run ENTRYPOINT [ top , -b ] . Now you can also override the param, by using the environment variable flag. 1 2 docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = help exec-param Resulting in top's help string. The special case of Alpine One of the main best practices for Dockerfiles , is to make them as small as possible. The easiest way to do this is to start with a minimal image. This is where Alpine Linux comes in. We will revisit out shell form example, but replace ubuntu with alpine. Create the following Dockerfile. 1 2 FROM alpine:3.8 ENTRYPOINT top -b Then build and run it. 1 2 docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = help exec-param This yields the following output. 1 2 3 4 5 Mem: 1509068K used, 537864K free, 640K shrd, 126756K buff, 1012436K cached CPU: 0 % usr 0 % sys 0 % nic 100 % idle 0 % io 0 % irq 0 % sirq Load average: 0 .00 0 .00 0 .00 2 /404 5 PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND 1 0 root R 1516 0 % 0 0 % top -b Aside from top 's output looking a bit different, there is only one command. Alpine Linux helps us avoid the problem of shell form altogether! Make Sure Your Process Listens It is excellent if your tenants are all signed up, know their rights and obligations. But you can't contact them when something happens, how will they ever know when to act? Translating that into our process. It starts and can be told to shut down, but does it process listen? Can it interpret the message it gets from Docker or Kubernetes? And if it does, can it relay the message correctly to its Child Processes? In order for your process to gracefully shutdown, it should know when to do so. As such, it should listen not only for itself but also on behalf of its children - yours never do anything wrong though! Some processes do, but many aren't designed to listen or tell their Children . They expect someone else to listen for them and tell them and their children - process managers. In order to listen to these signals , we can call in the help of others. We will look at two options. we let Docker manage the process and its children we use a process manager Let Docker manage it for us If you're not using Docker to run or manage your containers, you should skip to Depend on a process manager . Docker has a build in feature, that it uses a lightweight process manager to help you. So if you're running your images with Docker itself, either directly or via Compose or Swarm, you're fine. You can use the init flag in your run command or your compose file. Please, note that the below examples require a certain minimum version of Docker. run - 1.13+ compose (v 2.2) - 1.13.0+ swarm (v 3.7) - 18.06.0+ With Docker Run 1 docker run --rm -ti --init caladreas/dui With Docker Compose 1 2 3 4 5 version : 2.2 services : web : image : caladreas/java-docker-signal-demo:no-tini init : true With Docker Swarm 1 2 3 4 5 version : 3.7 services : web : image : caladreas/java-docker-signal-demo:no-tini init : true Relying on Docker does create a dependency on how your container runs. It only runs correctly in Docker-related technologies (run, compose, swarm) and only if the proper versions are available. Creating either a different experience for users running your application somewhere else or not able to meet the version requirements. So maybe another solution is to bake a process manager into your image and guarantee its behavior. Depend on a process manager One of our goals for Docker images is to keep them small. We should look for a lightweight process manager. It does not have too many a whole machine worth or processes, just one and perhaps some children. Here we would like to introduce you to Tini , a lightweight process manager designed for this purpose . It is a very successful and widely adopted process manager in the Docker world. So successful, that the before mentioned init flags from Docker are implemented by baking Tini into Docker . Debian example For brevity, the build process is excluded, and for image size, we use Debian slim instead of default Debian. 1 2 3 4 5 6 FROM debian:stable-slim ENV TINI_VERSION v0.18.0 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui , -XX:+UseCGroupMemoryLimitForHeap , -XX:+UnlockExperimentalVMOptions ] COPY --from = build /usr/bin/dui-image/ /usr/bin/dui Alpine example Alpine Linux works wonders for Docker images, so to improve our lives, you can very easily install it if you want. 1 2 3 4 FROM alpine RUN apk add --no-cache tini ENTRYPOINT [ /sbin/tini , -vv , -g , -s , -- ] CMD [ top -b ] How To Be Told What You Want To Hear You've made it this far; your tenets are reachable so you can inform them if they need to act. However, there's another problem lurking around the corner. Do they speak your language? Our process now starts knowing it can be talked to, it has someone who takes care of listening for it and its children. Now we need to make sure it can understand what it hears, it should be able to handle the incoming signals. We have two main ways of doing this. Handle signals as they come : we should make sure our process deal with the signals as they come State the signals we want : we can also tell up front, which signals we want to hear and put the burden of translation on our callers For more details on the subject of Signals and Docker, please read this excellent blog from Grigorii Chudnov . Handle signals as they come Handling process signals depend on your application, programming language or framework. State the signals we want Sometimes your language or framework of choice, doesn't handle signals all that well. It might be very rigid in what it does with specific signals, removing your ability to do the right thing. Of course, not all languages or frameworks are designed with Docker container or Microservices in mind, are yet to catch up to this more dynamic environment. Luckily Docker and Kubernetes allow you to specify what signal too sent to your process. Docker run 1 2 docker run --rm -ti --init --stop-signal = SIGINT \\ caladreas/java-docker-signal-demo Docker compose/swarm Docker's compose file format allows you to specify a stop signal . This is the signal sent when the container is stopped in a normal fashion. Normal in this case, meaning docker stop or when docker itself determines it should stop the container. If you forcefully remove the container, for example with docker rm - f it will directly kill the process, so don't do that. 1 2 3 4 5 6 version : 2.2 services : web : image : caladreas/java-docker-signal-demo stop_signal : SIGINT stop_grace_period : 15s If you run this with docker - compose up and then in a second terminal, stop the container, you will see something like this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 web_1 | HelloWorld! web_1 | Shutdown hook called! web_1 | We re told to stop early... web_1 | java.lang.InterruptedException: sleep interrupted web_1 | at java.base/java.lang.Thread.sleep(Native Method) web_1 | at joostvdg.demo.signal@1.0/com.github.joostvdg.demo.signal.HelloWorld.printHelloWorld(Unknown Source) web_1 | at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) web_1 | at java.base/java.util.concurrent.FutureTask.run(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) web_1 | at java.base/java.lang.Thread.run(Unknown Source) web_1 | [DEBUG tini (1)] Passing signal: Interrupt web_1 | [DEBUG tini (1)] Received SIGCHLD web_1 | [DEBUG tini (1)] Reaped child with pid: 7 web_1 | [INFO tini (1)] Main child exited with signal (with signal Interrupt ) Kubernetes In Kubernetes we can make use of Container Lifecycle Hooks to manage how our container should be stopped. We could, for example, send a SIGINT (interrupt) to tell our application to stop. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : apps/v1 kind : Deployment metadata : name : java-signal-demo namespace : default labels : app : java-signal-demo spec : replicas : 1 template : metadata : labels : app : java-signal-demo spec : containers : - name : main image : caladreas/java-docker-signal-demo lifecycle : preStop : exec : command : [ killall , java , -INT ] terminationGracePeriodSeconds : 60 When you create this as deployment.yml, create and delete it - kubectl apply - f deployment . yml / kubectl delete - f deployment . yml - you will see the same behavior. How To Be Told When You Want To Hear It Our process now will now start knowing it will hear what it wants to hear. But we now have to make sure we hear it when we need to hear it. An intervention is excellent when you can still be saved, but it is a bit useless if you're already dead. Docker You can either configure your health check in your Dockerfile or configure it in your docker-compose.yml for either compose or swarm. Considering only Docker can use the health check in your Dockerfile, it is strongly recommended to have health checks in your application and document how they can be used. Kubernetes In Kubernetes we have the concept of Container Probes . This allows you to configure whether your container is ready (readinessProbe) to be used and if it is still working as expected (livenessProbe). Examples How to actually listen to the signals and determine which one to use will depend on your programming language. There's three examples I have worked out, one for Go (lang) and two for Java: pojo and Spring Boot. Go Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 # build stage FROM golang:latest AS build-env RUN go get -v github.com/docker/docker/client/... RUN go get -v github.com/docker/docker/api/... ADD src/ $GOPATH /flow-proxy-service-lister WORKDIR $GOPATH /flow-proxy-service-lister RUN go build -o main -tags netgo main.go # final stage FROM alpine ENTRYPOINT [ /app/main ] COPY --from = build-env /go/flow-proxy-service-lister/main /app/ RUN chmod +x /app/main Go code for graceful shutdown The following is a way for Go to shutdown a http server when receiving a termination signal. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func main () { c := make ( chan bool ) // make channel for main -- webserver communication go webserver . Start ( 7777 , webserverData , c ) // ignore the missing data stop := make ( chan os . Signal , 1 ) // make a channel that listens to is signals signal . Notify ( stop , syscall . SIGINT , syscall . SIGTERM ) // we listen to some specific syscall signals for i := 1 ; ; i ++ { // this is still infinite t := time . NewTicker ( time . Second * 30 ) // set a timer for the polling select { case - stop : // this means we got a os signal on our channel break // so we can stop case - t . C : // our timer expired, refresh our data continue // and continue with the loop } break } fmt . Println ( Shutting down webserver ) // if we got here, we have to inform the webserver to close shop c - true // we do this by sending a message on the channel if b := - c ; b { // when we get true back, that means the webserver is doing with a graceful shutdown fmt . Println ( Webserver shut down ) // webserver is done } fmt . Println ( Shut down app ) // we can close shop ourselves now } Java plain (Docker Swarm) This application is a Java 9 modular application, which can be found on github, github.com/joostvdg . Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name *.java ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.1.0 LABEL description = Docker image for playing with java applications in a concurrent, parallel and distributed manor. # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui ] ENV DATE_CHANGED = 20180120-1525 COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules Handling code The code first initializes the server which and when started, creates the Shutdown Hook . Java handles certain signals in specific ways, as can be found in this table for linux. For more information, you can read the docs from Oracle . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 public class DockerApp { public static void main ( String [] args ) { ServiceLoader Logger loggers = ServiceLoader . load ( Logger . class ); Logger logger = loggers . findFirst (). isPresent () ? loggers . findFirst (). get () : null ; if ( logger == null ) { System . err . println ( Did not find any loggers, quiting ); System . exit ( 1 ); } logger . start ( LogLevel . INFO ); int pseudoRandom = new Random (). nextInt ( ProtocolConstants . POTENTIAL_SERVER_NAMES . length - 1 ); String serverName = ProtocolConstants . POTENTIAL_SERVER_NAMES [ pseudoRandom ]; int listenPort = ProtocolConstants . EXTERNAL_COMMUNICATION_PORT_A ; String multicastGroup = ProtocolConstants . MULTICAST_GROUP ; DuiServer distributedServer = DuiServerFactory . newDistributedServer ( listenPort , multicastGroup , serverName , logger ); distributedServer . logMembership (); ExecutorService executorService = Executors . newFixedThreadPool ( 1 ); executorService . submit ( distributedServer :: startServer ); long threadId = Thread . currentThread (). getId (); Runtime . getRuntime (). addShutdownHook ( new Thread (() - { System . out . println ( Shutdown hook called! ); logger . log ( LogLevel . WARN , App , ShotdownHook , threadId , Shutting down at request of Docker ); distributedServer . stopServer (); distributedServer . closeServer (); executorService . shutdown (); try { Thread . sleep ( 100 ); executorService . shutdownNow (); logger . stop (); } catch ( InterruptedException e ) { e . printStackTrace (); } })); } } Java Plain (Kubernetes) So far we've utilized the utilities from Docker itself in conjunction with it's native Docker Swarm orchestrator. Unfortunately, when it comes to popularity Kubernetes beats Swarm hands down . So this isn't complete if it doesn't also do graceful shutdown in Kubernetes. In Dockerfile Our original file had to be changed, as Debian's Slim image doesn't actually contain the kill package. And we need a kill package, as we cannot instruct Kubernetes to issue a specific SIGNAL. Instead, we can issue a PreStop exec command , which we can utilise to execute a killall java -INT . The command will be specified in the Kubernetes deployment definition below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name *.java ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.1.0 LABEL description = Docker image for playing with java applications in a concurrent, parallel and distributed manor. # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui ] ENV DATE_CHANGED = 20180120-1525 RUN apt-get update apt-get install --no-install-recommends -y psmisc = 22 .* rm -rf /var/lib/apt/lists/* COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules Kubernetes Deployment So here we have the image's K8s Deployment descriptor. Including the Pod's lifecycle preStop with a exec style command. You should know by now why we prefer that . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion : extensions/v1beta1 kind : Deployment metadata : name : dui-deployment namespace : default labels : k8s-app : dui spec : replicas : 3 template : metadata : labels : k8s-app : dui spec : containers : - name : master image : caladreas/buming ports : - name : http containerPort : 7777 lifecycle : preStop : exec : command : [ killall , java , -INT ] terminationGracePeriodSeconds : 60 Java Spring Boot (1.x) This example is for Spring Boot 1.x, in time we will have an example for 2.x. This example is for the scenario of a Fat Jar with Tomcat as container [^8]. Execute example 1 docker-compose build Execute the following command: 1 docker run --rm -ti --name test spring-boot-graceful Exit the application/container via ctrl + c and you should see the application shutting down gracefully. 1 2 3 2018 -01-30 13 :35:46.327 INFO 7 --- [ Thread-3 ] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@6e5e91e4: startup date [ Tue Jan 30 13 :35:42 GMT 2018 ] ; root of context hierarchy 2018 -01-30 13 :35:46.405 INFO 7 --- [ Thread-3 ] BootGracefulApplication $GracefulShutdown : Tomcat was shutdown gracefully within the allotted time. 2018 -01-30 13 :35:46.408 INFO 7 --- [ Thread-3 ] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans on shutdown Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 FROM maven:3-jdk-8 AS build ENV MAVEN_OPTS = -Dmaven.repo.local = /usr/share/maven/repository ENV WORKDIR = /usr/src/graceful RUN mkdir $WORKDIR WORKDIR $WORKDIR COPY pom.xml $WORKDIR RUN mvn -B -e org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline COPY . $WORKSPACE RUN mvn -B -e clean verify FROM anapsix/alpine-java:8_jdk_unlimited LABEL authors = Joost van der Griendt joostvdg@gmail.com ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- ] ENV DATE_CHANGED = 20180120-1525 COPY --from = build /usr/src/graceful/target/spring-boot-graceful.jar /app.jar CMD [ java , -Xms256M , -Xmx480M , -Djava.security.egd=file:/dev/./urandom , -jar , /app.jar ] Docker compose file 1 2 3 4 5 6 7 version : 3.5 services : web : image : spring-boot-graceful build : . stop_signal : SIGINT Java handling code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 package com.github.joostvdg.demo.springbootgraceful ; import org.springframework.boot.SpringApplication ; import org.springframework.boot.autoconfigure.SpringBootApplication ; import org.apache.catalina.connector.Connector ; import org.apache.tomcat.util.threads.ThreadPoolExecutor ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer ; import org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatConnectorCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory ; import org.springframework.context.ApplicationListener ; import org.springframework.context.annotation.Bean ; import org.springframework.context.event.ContextClosedEvent ; import java.util.concurrent.Executor ; import java.util.concurrent.TimeUnit ; @SpringBootApplication public class SpringBootGracefulApplication { public static void main ( String [] args ) { SpringApplication . run ( SpringBootGracefulApplication . class , args ); } @Bean public GracefulShutdown gracefulShutdown () { return new GracefulShutdown (); } @Bean public EmbeddedServletContainerCustomizer tomcatCustomizer () { return new EmbeddedServletContainerCustomizer () { @Override public void customize ( ConfigurableEmbeddedServletContainer container ) { if ( container instanceof TomcatEmbeddedServletContainerFactory ) { (( TomcatEmbeddedServletContainerFactory ) container ) . addConnectorCustomizers ( gracefulShutdown ()); } } }; } private static class GracefulShutdown implements TomcatConnectorCustomizer , ApplicationListener ContextClosedEvent { private static final Logger log = LoggerFactory . getLogger ( GracefulShutdown . class ); private volatile Connector connector ; @Override public void customize ( Connector connector ) { this . connector = connector ; } @Override public void onApplicationEvent ( ContextClosedEvent event ) { this . connector . pause (); Executor executor = this . connector . getProtocolHandler (). getExecutor (); if ( executor instanceof ThreadPoolExecutor ) { try { ThreadPoolExecutor threadPoolExecutor = ( ThreadPoolExecutor ) executor ; threadPoolExecutor . shutdown (); if (! threadPoolExecutor . awaitTermination ( 30 , TimeUnit . SECONDS )) { log . warn ( Tomcat thread pool did not shut down gracefully within + 30 seconds. Proceeding with forceful shutdown ); } else { log . info ( Tomcat was shutdown gracefully within the allotted time. ); } } catch ( InterruptedException ex ) { Thread . currentThread (). interrupt (); } } } } } Example with Docker Swarm For now there's only an example with docker swarm , in time there will also be a Kubernetes example. Now that you can create Java applications packaged neatly in Docker images that support graceful shutdown, it would be nice to utilize. A good scenario would be a microservices architecture where services can come and go, but are registered in a service registry such as Eureka . Or a membership based protocol where members interact with each other and perhaps shard data. In these cases, of course the interactions are designed to be fault tolerant and discover faulty nodes on their own. But wouldn't it be better that if you knew you're going to quit, you inform the rest? We can reuse the caladreas / buming image and make it a docker swarm stack and run the service on every node. This way, we can easily see members coming and going and reduce the time to detect failure by notifying our peers of our impeding end. Docker swarm cluster Setting up a docker swarm cluster is easy, but has some requirements: virtual box 4.x+ docker-machine 1.12+ docker 17.06+ Warn Make sure this is the first and only virtualbox docker-machine VM being created/running, so that the ip range starts with 192.168.99.100 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 docker-machine create --driver virtualbox dui-1 docker-machine create --driver virtualbox dui-2 docker-machine create --driver virtualbox dui-3 eval $( docker-machine env dui-1 ) IP = 192 .168.99.100 docker swarm init --advertise-addr $IP TOKEN = $( docker swarm join-token -q worker ) eval $( docker-machine env dui-2 ) docker swarm join --token ${ TOKEN } ${ IP } :2377 eval $( docker-machine env dui-3 ) docker swarm join --token ${ TOKEN } ${ IP } :2377 eval $( docker-machine env dui-1 ) docker node ls Docker swarm network and multicast Unfortunately, docker swarm's swarm mode network overlay does not support multicast [ 9][ 10]. Why is this a problem? Well, the application I use to test the graceful shutdown requires this, sorry. Luckily there is a very easy solution for this, its by using Weavenet 's docker network plugin. Don't want to know about it or how you install it? Don't worry, just execute the script below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/usr/bin/env bash echo = Prepare dui-2 eval $( docker-machine env dui-2 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo = Prepare dui-3 eval $( docker-machine env dui-3 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo = Prepare dui-1 eval $( docker-machine env dui-1 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 docker network create --driver = weaveworks/net-plugin:2.1.3 --opt works.weave.multicast = true --attachable dui Docker stack Now to create a service that runs on every node it is the easiest to create a docker stack . Compose file (docker-stack.yml) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 version : 3.5 services : dui : image : caladreas/buming build : . stop_signal : SIGINT networks : - dui deploy : mode : global networks : dui : external : true Create stack 1 docker stack deploy --compose-file docker-stack.yml buming Execute example Now that we have a docker swarm cluster and a stack - which has a service running on every node - we can showcase the power of graceful shutdown in a cluster of dependent services. Confirm the service is running correctly on every node, first lets check our nodes. 1 2 eval $( docker-machine env dui-1 ) docker node ls Which should look like this: 1 2 3 4 ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS f21ilm4thxegn5xbentmss5ur * dui-1 Ready Active Leader y7475bo5uplt2b58d050b4wfd dui-2 Ready Active 6ssxola6y1i6h9p8256pi7bfv dui-3 Ready Active Then check the service. 1 docker service ps buming_dui Which should look like this. 1 2 3 4 ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 3mrpr0jg31x1 buming_dui.6ssxola6y1i6h9p8256pi7bfv dui:latest dui-3 Running Running 17 seconds ago pfubtiy4j7vo buming_dui.f21ilm4thxegn5xbentmss5ur dui:latest dui-1 Running Running 17 seconds ago f4gjnmhoe3y4 buming_dui.y7475bo5uplt2b58d050b4wfd dui:latest dui-2 Running Running 17 seconds ago Now open a second terminal window. In window one, follow the service logs: 1 2 eval $( docker-machine env dui-1 ) docker service logs -f buming_dui In window two, go to a different node and stop the container. 1 2 3 eval $( docker-machine env dui-2 ) docker ps docker stop buming_dui.y7475bo5uplt2b58d050b4wfd.pnoui2x6elrz0tvkjz51njz94 In this case, you will see the other nodes receiving a leave notice and then the node stopping. 1 2 3 4 5 6 buming_dui.0.ryd8szexxku3@dui-3 | [ Server-John D. Carmack ] [ WARN ] [ 14 :19:02.604011 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = 83918f6ad817 , ip = 10.0.0.7 , name = Ken Thompson } buming_dui.0.so5m14sz8ksh@dui-1 | [ Server-Alan Kay ] [ WARN ] [ 14 :19:02.602082 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = 83918f6ad817 , ip = 10.0.0.7 , name = Ken Thompson } buming_dui.0.pnoui2x6elrz@dui-2 | Shutdown hook called! buming_dui.0.pnoui2x6elrz@dui-2 | [ App ] [ WARN ] [ 14 :19:02.598759 ] [ 1 ] [ ShotdownHook ] Shutting down at request of Docker buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.598858 ] [ 12 ] [ Main ] Stopping buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.601008 ] [ 12 ] [ Main ] Closing Further reading Wikipedia page on reboots Microsoft about graceful shutdown Gracefully stopping docker containers What to know about Java and shutdown hooks https://www.weave.works/blog/docker-container-networking-multicast-fast/ https://www.weave.works/docs/net/latest/install/plugin/plugin-how-it-works/ https://www.weave.works/docs/net/latest/install/plugin/plugin-v2/ https://www.auzias.net/en/docker-network-multihost/ https://forums.docker.com/t/cannot-get-zookeeper-to-work-running-in-docker-using-swarm-mode/27109 https://github.com/docker/libnetwork/issues/740","title":"Docker Graceful Shutdown"},{"location":"blogs/graceful-shutdown/#gracefully-shutting-down-applications-in-docker","text":"I'm not sure about you, but I prefer it when my neighbors leave our shared spaces clean and don't take up parking spaces when they don't need them. Imagine you live in an apartment complex with the above-mentioned parking lot. Some tenants go away and never come back. If nothing is done to clean up after them - to reclaim their apartment and parking space - then after some time, more and more apartments are unavailable for no reason, and the parking lot fills with cars which belong to no one. Some tenants did not get a parking lot and are getting frustrated that none are becoming available. When they moved in, they were told that when others leave, they would be next in line. While they're waiting, they have to park outside the complex. Eventually, the entrance gets blocked and no one can enter or leave. The end result is a completely unlivable apartment block with trapped tenants - never to be seen or heard from again. If you agree with me that when a tenant leaves, they should clean the apartment and free the parking spot to make it ready for the next inhabitant; then please read on. We're going to dive into the equivalent of doing this with containers. We will explore running our containers with Docker (run, compose, swarm) and Kubernetes. Even if you use another way to run your containers, this article should provide you with enough insight to get you on your way.","title":"Gracefully Shutting Down Applications in Docker"},{"location":"blogs/graceful-shutdown/#the-case-for-graceful-shutdown","text":"We're in an age where many applications are running in Docker containers across a multitude of clusters. These applications are then confronted with new concerns to tackle such as more moving parts, networking between these parts, remote storage and others. One significant way we defend ourselves against the perils of this distributed nature is to make our applications more robust - able to survive errors. However, even then there is still no guarantee your application is always up and running. So another concern we should tackle is how it responds when it needs to shut down. Where we can differentiate between an unexpected shutdown - we crashed - or an expected shutdown. On top of that, failing instead of trying to recover when something bad happens also adheres to \"fail fast\" - as strongly advocated by Michael Nygard in ReleaseIt. Shutting down can happen for a variety of reasons, in this post we dive into how to deal with an expected shutdown such as it being told to stop by an orchestrator such as Kubernetes. Containers can be purposefully shut down for a variety of reasons, including but not limited too: your application's health check fails your application consumed more resources than allowed the application is scaling down Just as cleaning up when leaving makes you a better tenant, having your application clean up connections, resources Moreover, the more tenants behaving in a good way increases the quality of living for all tenants. In our case, it improves the reliability and consistency of our cluster. Graceful shutdown is not unique to Docker, as it has been part of Linux's best practices for quite some years before Docker's existence. However, applying them to Docker container adds extra dimensions.","title":"The case for graceful shutdown"},{"location":"blogs/graceful-shutdown/#start-good-so-you-can-end-well","text":"When you sign up for an apartment, you probably have to sign a contract detailing your rights and obligations. The more you state explicitly, the easier it is to deal with bad behaving neighbors. The same is true when running processes; we should make sure that we set the rules, obligations, and expectations from the start. As we say in Dutch: a good beginning is half the work. We will start with how you can run a process in a container with a process that shuts down gracefully. There are many ways to start a process in a container. In this article, we look at processes started by commands defined in a Dockerfile. There are two ways to specify this: CMD : runs a command when the container gets started ENTRYPOINT : provides the location (entrypoint) from where commands get run when the container starts You need at least one ENTRYPOINT or CMD in a Dockerfile for it to be valid. They can be used in collaboration but they can do similar things. For more information on the details of these commands, read Docker's docs on Entrypoint vs. CMD .","title":"Start Good So You Can End Well"},{"location":"blogs/graceful-shutdown/#docker-shell-form-example","text":"We start with the shell form and see if it can do what we want; begin in such a way, we can stop it nicely. Shell form means we define a shell command without any special format or keywords. Please create Dockerfile with the content that follows. 1 2 FROM ubuntu:18.04 ENTRYPOINT top -b Then build an image and run a container. 1 2 docker image build --tag shell-form . docker run --name shell-form --rm shell-form The above command yields the following output. 1 2 3 4 5 6 7 8 9 top - 16 :34:56 up 1 day, 5 :15, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 541984 free, 302668 used, 1202280 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1579380 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 4624 760 696 S 0 .0 0 .0 0 :00.05 sh 6 root 20 0 36480 2928 2580 R 0 .0 0 .1 0 :00.01 top As you can see, two processes are running, sh and top . Meaning, that killing the process, with ctrl+c for example, terminates the sh process, but not top . This happens because the sh process forked the top process, but the termination will only be send to PID 1 - in this case sh . As sh will not stop the top process for us it will continue running and leave the container alive. To kill this container, open a second terminal and execute the following command. 1 docker rm -f shell-form Shell form doesn't do what we need. Starting a process with shell form will only lead us to the disaster of parking lots filling up unless there's a someone actively cleaning up.","title":"Docker Shell form example"},{"location":"blogs/graceful-shutdown/#docker-exec-form-example","text":"This leads us to the exec form. Hopefully, this gets us somewhere. The exec form is written as an array of parameters: ENTRYPOINT [ top , -b ] To continue in the same line of examples, we will create a Dockerfile, build and run it. 1 2 FROM ubuntu:18.04 ENTRYPOINT [ top , -b ] Then build and run it. 1 2 docker image build --tag exec-form . docker run --name exec-form --rm exec-form This yields the following output. 1 2 3 4 5 6 7 8 top - 18 :12:30 up 1 day, 6 :53, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 535896 free, 307196 used, 1203840 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1574880 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 36480 2940 2584 R 0 .0 0 .1 0 :00.03 top Now we got something we can work with. If something would tell this Container to stop, it will tell our only running process so it is sure to reach the correct one!","title":"Docker exec form example"},{"location":"blogs/graceful-shutdown/#gotchas","text":"Knowing we can use the exec form for our goal - gracefully shutting down our container - we can move on to the next part of our efforts. For the sake of imparting you with some hard learned lessons, we will explore two gotchas. They're optional, so you can also choose to skip to Make Sure Your Process Listens .","title":"Gotchas"},{"location":"blogs/graceful-shutdown/#docker-exec-form-with-parameters","text":"A caveat with the exec form is that it doesn't interpolate parameters. You can try the following: 1 2 3 FROM ubuntu:18.04 ENV PARAM = -b ENTRYPOINT [ top , ${PARAM} ] Then build and run it: 1 2 docker image build --tag exec-param . docker run --name exec-form --rm exec-param This should yield the following: 1 /bin/sh: 1 : [ top: not found This is where Docker created a mix between the two styles. It allows you to create an Entrypoint with a shell command - performing interpolation - but executing it as an exec form. This can be done by prefixing the shell form, with, you guessed it, exec . 1 2 3 FROM ubuntu:18.04 ENV PARAM = -b ENTRYPOINT exec top ${ PARAM } Then build and run it: 1 2 docker image build --tag exec-param . docker run --name exec-form --rm exec-param This will return the exact same as if we would've run ENTRYPOINT [ top , -b ] . Now you can also override the param, by using the environment variable flag. 1 2 docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = help exec-param Resulting in top's help string.","title":"Docker exec form with parameters"},{"location":"blogs/graceful-shutdown/#the-special-case-of-alpine","text":"One of the main best practices for Dockerfiles , is to make them as small as possible. The easiest way to do this is to start with a minimal image. This is where Alpine Linux comes in. We will revisit out shell form example, but replace ubuntu with alpine. Create the following Dockerfile. 1 2 FROM alpine:3.8 ENTRYPOINT top -b Then build and run it. 1 2 docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = help exec-param This yields the following output. 1 2 3 4 5 Mem: 1509068K used, 537864K free, 640K shrd, 126756K buff, 1012436K cached CPU: 0 % usr 0 % sys 0 % nic 100 % idle 0 % io 0 % irq 0 % sirq Load average: 0 .00 0 .00 0 .00 2 /404 5 PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND 1 0 root R 1516 0 % 0 0 % top -b Aside from top 's output looking a bit different, there is only one command. Alpine Linux helps us avoid the problem of shell form altogether!","title":"The special case of Alpine"},{"location":"blogs/graceful-shutdown/#make-sure-your-process-listens","text":"It is excellent if your tenants are all signed up, know their rights and obligations. But you can't contact them when something happens, how will they ever know when to act? Translating that into our process. It starts and can be told to shut down, but does it process listen? Can it interpret the message it gets from Docker or Kubernetes? And if it does, can it relay the message correctly to its Child Processes? In order for your process to gracefully shutdown, it should know when to do so. As such, it should listen not only for itself but also on behalf of its children - yours never do anything wrong though! Some processes do, but many aren't designed to listen or tell their Children . They expect someone else to listen for them and tell them and their children - process managers. In order to listen to these signals , we can call in the help of others. We will look at two options. we let Docker manage the process and its children we use a process manager","title":"Make Sure Your Process Listens"},{"location":"blogs/graceful-shutdown/#let-docker-manage-it-for-us","text":"If you're not using Docker to run or manage your containers, you should skip to Depend on a process manager . Docker has a build in feature, that it uses a lightweight process manager to help you. So if you're running your images with Docker itself, either directly or via Compose or Swarm, you're fine. You can use the init flag in your run command or your compose file. Please, note that the below examples require a certain minimum version of Docker. run - 1.13+ compose (v 2.2) - 1.13.0+ swarm (v 3.7) - 18.06.0+","title":"Let Docker manage it for us"},{"location":"blogs/graceful-shutdown/#with-docker-run","text":"1 docker run --rm -ti --init caladreas/dui","title":"With Docker Run"},{"location":"blogs/graceful-shutdown/#with-docker-compose","text":"1 2 3 4 5 version : 2.2 services : web : image : caladreas/java-docker-signal-demo:no-tini init : true","title":"With Docker Compose"},{"location":"blogs/graceful-shutdown/#with-docker-swarm","text":"1 2 3 4 5 version : 3.7 services : web : image : caladreas/java-docker-signal-demo:no-tini init : true Relying on Docker does create a dependency on how your container runs. It only runs correctly in Docker-related technologies (run, compose, swarm) and only if the proper versions are available. Creating either a different experience for users running your application somewhere else or not able to meet the version requirements. So maybe another solution is to bake a process manager into your image and guarantee its behavior.","title":"With Docker Swarm"},{"location":"blogs/graceful-shutdown/#depend-on-a-process-manager","text":"One of our goals for Docker images is to keep them small. We should look for a lightweight process manager. It does not have too many a whole machine worth or processes, just one and perhaps some children. Here we would like to introduce you to Tini , a lightweight process manager designed for this purpose . It is a very successful and widely adopted process manager in the Docker world. So successful, that the before mentioned init flags from Docker are implemented by baking Tini into Docker .","title":"Depend on a process manager"},{"location":"blogs/graceful-shutdown/#debian-example","text":"For brevity, the build process is excluded, and for image size, we use Debian slim instead of default Debian. 1 2 3 4 5 6 FROM debian:stable-slim ENV TINI_VERSION v0.18.0 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui , -XX:+UseCGroupMemoryLimitForHeap , -XX:+UnlockExperimentalVMOptions ] COPY --from = build /usr/bin/dui-image/ /usr/bin/dui","title":"Debian example"},{"location":"blogs/graceful-shutdown/#alpine-example","text":"Alpine Linux works wonders for Docker images, so to improve our lives, you can very easily install it if you want. 1 2 3 4 FROM alpine RUN apk add --no-cache tini ENTRYPOINT [ /sbin/tini , -vv , -g , -s , -- ] CMD [ top -b ]","title":"Alpine example"},{"location":"blogs/graceful-shutdown/#how-to-be-told-what-you-want-to-hear","text":"You've made it this far; your tenets are reachable so you can inform them if they need to act. However, there's another problem lurking around the corner. Do they speak your language? Our process now starts knowing it can be talked to, it has someone who takes care of listening for it and its children. Now we need to make sure it can understand what it hears, it should be able to handle the incoming signals. We have two main ways of doing this. Handle signals as they come : we should make sure our process deal with the signals as they come State the signals we want : we can also tell up front, which signals we want to hear and put the burden of translation on our callers For more details on the subject of Signals and Docker, please read this excellent blog from Grigorii Chudnov .","title":"How To Be Told What You Want To Hear"},{"location":"blogs/graceful-shutdown/#handle-signals-as-they-come","text":"Handling process signals depend on your application, programming language or framework.","title":"Handle signals as they come"},{"location":"blogs/graceful-shutdown/#state-the-signals-we-want","text":"Sometimes your language or framework of choice, doesn't handle signals all that well. It might be very rigid in what it does with specific signals, removing your ability to do the right thing. Of course, not all languages or frameworks are designed with Docker container or Microservices in mind, are yet to catch up to this more dynamic environment. Luckily Docker and Kubernetes allow you to specify what signal too sent to your process.","title":"State the signals we want"},{"location":"blogs/graceful-shutdown/#docker-run","text":"1 2 docker run --rm -ti --init --stop-signal = SIGINT \\ caladreas/java-docker-signal-demo","title":"Docker run"},{"location":"blogs/graceful-shutdown/#docker-composeswarm","text":"Docker's compose file format allows you to specify a stop signal . This is the signal sent when the container is stopped in a normal fashion. Normal in this case, meaning docker stop or when docker itself determines it should stop the container. If you forcefully remove the container, for example with docker rm - f it will directly kill the process, so don't do that. 1 2 3 4 5 6 version : 2.2 services : web : image : caladreas/java-docker-signal-demo stop_signal : SIGINT stop_grace_period : 15s If you run this with docker - compose up and then in a second terminal, stop the container, you will see something like this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 web_1 | HelloWorld! web_1 | Shutdown hook called! web_1 | We re told to stop early... web_1 | java.lang.InterruptedException: sleep interrupted web_1 | at java.base/java.lang.Thread.sleep(Native Method) web_1 | at joostvdg.demo.signal@1.0/com.github.joostvdg.demo.signal.HelloWorld.printHelloWorld(Unknown Source) web_1 | at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) web_1 | at java.base/java.util.concurrent.FutureTask.run(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) web_1 | at java.base/java.lang.Thread.run(Unknown Source) web_1 | [DEBUG tini (1)] Passing signal: Interrupt web_1 | [DEBUG tini (1)] Received SIGCHLD web_1 | [DEBUG tini (1)] Reaped child with pid: 7 web_1 | [INFO tini (1)] Main child exited with signal (with signal Interrupt )","title":"Docker compose/swarm"},{"location":"blogs/graceful-shutdown/#kubernetes","text":"In Kubernetes we can make use of Container Lifecycle Hooks to manage how our container should be stopped. We could, for example, send a SIGINT (interrupt) to tell our application to stop. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : apps/v1 kind : Deployment metadata : name : java-signal-demo namespace : default labels : app : java-signal-demo spec : replicas : 1 template : metadata : labels : app : java-signal-demo spec : containers : - name : main image : caladreas/java-docker-signal-demo lifecycle : preStop : exec : command : [ killall , java , -INT ] terminationGracePeriodSeconds : 60 When you create this as deployment.yml, create and delete it - kubectl apply - f deployment . yml / kubectl delete - f deployment . yml - you will see the same behavior.","title":"Kubernetes"},{"location":"blogs/graceful-shutdown/#how-to-be-told-when-you-want-to-hear-it","text":"Our process now will now start knowing it will hear what it wants to hear. But we now have to make sure we hear it when we need to hear it. An intervention is excellent when you can still be saved, but it is a bit useless if you're already dead.","title":"How To Be Told When You Want To Hear It"},{"location":"blogs/graceful-shutdown/#docker","text":"You can either configure your health check in your Dockerfile or configure it in your docker-compose.yml for either compose or swarm. Considering only Docker can use the health check in your Dockerfile, it is strongly recommended to have health checks in your application and document how they can be used.","title":"Docker"},{"location":"blogs/graceful-shutdown/#kubernetes_1","text":"In Kubernetes we have the concept of Container Probes . This allows you to configure whether your container is ready (readinessProbe) to be used and if it is still working as expected (livenessProbe).","title":"Kubernetes"},{"location":"blogs/graceful-shutdown/#examples","text":"How to actually listen to the signals and determine which one to use will depend on your programming language. There's three examples I have worked out, one for Go (lang) and two for Java: pojo and Spring Boot.","title":"Examples"},{"location":"blogs/graceful-shutdown/#go","text":"","title":"Go"},{"location":"blogs/graceful-shutdown/#dockerfile","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 # build stage FROM golang:latest AS build-env RUN go get -v github.com/docker/docker/client/... RUN go get -v github.com/docker/docker/api/... ADD src/ $GOPATH /flow-proxy-service-lister WORKDIR $GOPATH /flow-proxy-service-lister RUN go build -o main -tags netgo main.go # final stage FROM alpine ENTRYPOINT [ /app/main ] COPY --from = build-env /go/flow-proxy-service-lister/main /app/ RUN chmod +x /app/main","title":"Dockerfile"},{"location":"blogs/graceful-shutdown/#go-code-for-graceful-shutdown","text":"The following is a way for Go to shutdown a http server when receiving a termination signal. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func main () { c := make ( chan bool ) // make channel for main -- webserver communication go webserver . Start ( 7777 , webserverData , c ) // ignore the missing data stop := make ( chan os . Signal , 1 ) // make a channel that listens to is signals signal . Notify ( stop , syscall . SIGINT , syscall . SIGTERM ) // we listen to some specific syscall signals for i := 1 ; ; i ++ { // this is still infinite t := time . NewTicker ( time . Second * 30 ) // set a timer for the polling select { case - stop : // this means we got a os signal on our channel break // so we can stop case - t . C : // our timer expired, refresh our data continue // and continue with the loop } break } fmt . Println ( Shutting down webserver ) // if we got here, we have to inform the webserver to close shop c - true // we do this by sending a message on the channel if b := - c ; b { // when we get true back, that means the webserver is doing with a graceful shutdown fmt . Println ( Webserver shut down ) // webserver is done } fmt . Println ( Shut down app ) // we can close shop ourselves now }","title":"Go code for graceful shutdown"},{"location":"blogs/graceful-shutdown/#java-plain-docker-swarm","text":"This application is a Java 9 modular application, which can be found on github, github.com/joostvdg .","title":"Java plain (Docker Swarm)"},{"location":"blogs/graceful-shutdown/#dockerfile_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name *.java ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.1.0 LABEL description = Docker image for playing with java applications in a concurrent, parallel and distributed manor. # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui ] ENV DATE_CHANGED = 20180120-1525 COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules","title":"Dockerfile"},{"location":"blogs/graceful-shutdown/#handling-code","text":"The code first initializes the server which and when started, creates the Shutdown Hook . Java handles certain signals in specific ways, as can be found in this table for linux. For more information, you can read the docs from Oracle . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 public class DockerApp { public static void main ( String [] args ) { ServiceLoader Logger loggers = ServiceLoader . load ( Logger . class ); Logger logger = loggers . findFirst (). isPresent () ? loggers . findFirst (). get () : null ; if ( logger == null ) { System . err . println ( Did not find any loggers, quiting ); System . exit ( 1 ); } logger . start ( LogLevel . INFO ); int pseudoRandom = new Random (). nextInt ( ProtocolConstants . POTENTIAL_SERVER_NAMES . length - 1 ); String serverName = ProtocolConstants . POTENTIAL_SERVER_NAMES [ pseudoRandom ]; int listenPort = ProtocolConstants . EXTERNAL_COMMUNICATION_PORT_A ; String multicastGroup = ProtocolConstants . MULTICAST_GROUP ; DuiServer distributedServer = DuiServerFactory . newDistributedServer ( listenPort , multicastGroup , serverName , logger ); distributedServer . logMembership (); ExecutorService executorService = Executors . newFixedThreadPool ( 1 ); executorService . submit ( distributedServer :: startServer ); long threadId = Thread . currentThread (). getId (); Runtime . getRuntime (). addShutdownHook ( new Thread (() - { System . out . println ( Shutdown hook called! ); logger . log ( LogLevel . WARN , App , ShotdownHook , threadId , Shutting down at request of Docker ); distributedServer . stopServer (); distributedServer . closeServer (); executorService . shutdown (); try { Thread . sleep ( 100 ); executorService . shutdownNow (); logger . stop (); } catch ( InterruptedException e ) { e . printStackTrace (); } })); } }","title":"Handling code"},{"location":"blogs/graceful-shutdown/#java-plain-kubernetes","text":"So far we've utilized the utilities from Docker itself in conjunction with it's native Docker Swarm orchestrator. Unfortunately, when it comes to popularity Kubernetes beats Swarm hands down . So this isn't complete if it doesn't also do graceful shutdown in Kubernetes.","title":"Java Plain (Kubernetes)"},{"location":"blogs/graceful-shutdown/#in-dockerfile","text":"Our original file had to be changed, as Debian's Slim image doesn't actually contain the kill package. And we need a kill package, as we cannot instruct Kubernetes to issue a specific SIGNAL. Instead, we can issue a PreStop exec command , which we can utilise to execute a killall java -INT . The command will be specified in the Kubernetes deployment definition below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name *.java ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.1.0 LABEL description = Docker image for playing with java applications in a concurrent, parallel and distributed manor. # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui ] ENV DATE_CHANGED = 20180120-1525 RUN apt-get update apt-get install --no-install-recommends -y psmisc = 22 .* rm -rf /var/lib/apt/lists/* COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules","title":"In Dockerfile"},{"location":"blogs/graceful-shutdown/#kubernetes-deployment","text":"So here we have the image's K8s Deployment descriptor. Including the Pod's lifecycle preStop with a exec style command. You should know by now why we prefer that . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion : extensions/v1beta1 kind : Deployment metadata : name : dui-deployment namespace : default labels : k8s-app : dui spec : replicas : 3 template : metadata : labels : k8s-app : dui spec : containers : - name : master image : caladreas/buming ports : - name : http containerPort : 7777 lifecycle : preStop : exec : command : [ killall , java , -INT ] terminationGracePeriodSeconds : 60","title":"Kubernetes Deployment"},{"location":"blogs/graceful-shutdown/#java-spring-boot-1x","text":"This example is for Spring Boot 1.x, in time we will have an example for 2.x. This example is for the scenario of a Fat Jar with Tomcat as container [^8].","title":"Java Spring Boot (1.x)"},{"location":"blogs/graceful-shutdown/#execute-example","text":"1 docker-compose build Execute the following command: 1 docker run --rm -ti --name test spring-boot-graceful Exit the application/container via ctrl + c and you should see the application shutting down gracefully. 1 2 3 2018 -01-30 13 :35:46.327 INFO 7 --- [ Thread-3 ] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@6e5e91e4: startup date [ Tue Jan 30 13 :35:42 GMT 2018 ] ; root of context hierarchy 2018 -01-30 13 :35:46.405 INFO 7 --- [ Thread-3 ] BootGracefulApplication $GracefulShutdown : Tomcat was shutdown gracefully within the allotted time. 2018 -01-30 13 :35:46.408 INFO 7 --- [ Thread-3 ] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans on shutdown","title":"Execute example"},{"location":"blogs/graceful-shutdown/#dockerfile_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 FROM maven:3-jdk-8 AS build ENV MAVEN_OPTS = -Dmaven.repo.local = /usr/share/maven/repository ENV WORKDIR = /usr/src/graceful RUN mkdir $WORKDIR WORKDIR $WORKDIR COPY pom.xml $WORKDIR RUN mvn -B -e org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline COPY . $WORKSPACE RUN mvn -B -e clean verify FROM anapsix/alpine-java:8_jdk_unlimited LABEL authors = Joost van der Griendt joostvdg@gmail.com ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- ] ENV DATE_CHANGED = 20180120-1525 COPY --from = build /usr/src/graceful/target/spring-boot-graceful.jar /app.jar CMD [ java , -Xms256M , -Xmx480M , -Djava.security.egd=file:/dev/./urandom , -jar , /app.jar ]","title":"Dockerfile"},{"location":"blogs/graceful-shutdown/#docker-compose-file","text":"1 2 3 4 5 6 7 version : 3.5 services : web : image : spring-boot-graceful build : . stop_signal : SIGINT","title":"Docker compose file"},{"location":"blogs/graceful-shutdown/#java-handling-code","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 package com.github.joostvdg.demo.springbootgraceful ; import org.springframework.boot.SpringApplication ; import org.springframework.boot.autoconfigure.SpringBootApplication ; import org.apache.catalina.connector.Connector ; import org.apache.tomcat.util.threads.ThreadPoolExecutor ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer ; import org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatConnectorCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory ; import org.springframework.context.ApplicationListener ; import org.springframework.context.annotation.Bean ; import org.springframework.context.event.ContextClosedEvent ; import java.util.concurrent.Executor ; import java.util.concurrent.TimeUnit ; @SpringBootApplication public class SpringBootGracefulApplication { public static void main ( String [] args ) { SpringApplication . run ( SpringBootGracefulApplication . class , args ); } @Bean public GracefulShutdown gracefulShutdown () { return new GracefulShutdown (); } @Bean public EmbeddedServletContainerCustomizer tomcatCustomizer () { return new EmbeddedServletContainerCustomizer () { @Override public void customize ( ConfigurableEmbeddedServletContainer container ) { if ( container instanceof TomcatEmbeddedServletContainerFactory ) { (( TomcatEmbeddedServletContainerFactory ) container ) . addConnectorCustomizers ( gracefulShutdown ()); } } }; } private static class GracefulShutdown implements TomcatConnectorCustomizer , ApplicationListener ContextClosedEvent { private static final Logger log = LoggerFactory . getLogger ( GracefulShutdown . class ); private volatile Connector connector ; @Override public void customize ( Connector connector ) { this . connector = connector ; } @Override public void onApplicationEvent ( ContextClosedEvent event ) { this . connector . pause (); Executor executor = this . connector . getProtocolHandler (). getExecutor (); if ( executor instanceof ThreadPoolExecutor ) { try { ThreadPoolExecutor threadPoolExecutor = ( ThreadPoolExecutor ) executor ; threadPoolExecutor . shutdown (); if (! threadPoolExecutor . awaitTermination ( 30 , TimeUnit . SECONDS )) { log . warn ( Tomcat thread pool did not shut down gracefully within + 30 seconds. Proceeding with forceful shutdown ); } else { log . info ( Tomcat was shutdown gracefully within the allotted time. ); } } catch ( InterruptedException ex ) { Thread . currentThread (). interrupt (); } } } } }","title":"Java handling code"},{"location":"blogs/graceful-shutdown/#example-with-docker-swarm","text":"For now there's only an example with docker swarm , in time there will also be a Kubernetes example. Now that you can create Java applications packaged neatly in Docker images that support graceful shutdown, it would be nice to utilize. A good scenario would be a microservices architecture where services can come and go, but are registered in a service registry such as Eureka . Or a membership based protocol where members interact with each other and perhaps shard data. In these cases, of course the interactions are designed to be fault tolerant and discover faulty nodes on their own. But wouldn't it be better that if you knew you're going to quit, you inform the rest? We can reuse the caladreas / buming image and make it a docker swarm stack and run the service on every node. This way, we can easily see members coming and going and reduce the time to detect failure by notifying our peers of our impeding end.","title":"Example with Docker Swarm"},{"location":"blogs/graceful-shutdown/#docker-swarm-cluster","text":"Setting up a docker swarm cluster is easy, but has some requirements: virtual box 4.x+ docker-machine 1.12+ docker 17.06+ Warn Make sure this is the first and only virtualbox docker-machine VM being created/running, so that the ip range starts with 192.168.99.100 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 docker-machine create --driver virtualbox dui-1 docker-machine create --driver virtualbox dui-2 docker-machine create --driver virtualbox dui-3 eval $( docker-machine env dui-1 ) IP = 192 .168.99.100 docker swarm init --advertise-addr $IP TOKEN = $( docker swarm join-token -q worker ) eval $( docker-machine env dui-2 ) docker swarm join --token ${ TOKEN } ${ IP } :2377 eval $( docker-machine env dui-3 ) docker swarm join --token ${ TOKEN } ${ IP } :2377 eval $( docker-machine env dui-1 ) docker node ls","title":"Docker swarm cluster"},{"location":"blogs/graceful-shutdown/#docker-swarm-network-and-multicast","text":"Unfortunately, docker swarm's swarm mode network overlay does not support multicast [ 9][ 10]. Why is this a problem? Well, the application I use to test the graceful shutdown requires this, sorry. Luckily there is a very easy solution for this, its by using Weavenet 's docker network plugin. Don't want to know about it or how you install it? Don't worry, just execute the script below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/usr/bin/env bash echo = Prepare dui-2 eval $( docker-machine env dui-2 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo = Prepare dui-3 eval $( docker-machine env dui-3 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo = Prepare dui-1 eval $( docker-machine env dui-1 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 docker network create --driver = weaveworks/net-plugin:2.1.3 --opt works.weave.multicast = true --attachable dui","title":"Docker swarm network and multicast"},{"location":"blogs/graceful-shutdown/#docker-stack","text":"Now to create a service that runs on every node it is the easiest to create a docker stack .","title":"Docker stack"},{"location":"blogs/graceful-shutdown/#compose-file-docker-stackyml","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 version : 3.5 services : dui : image : caladreas/buming build : . stop_signal : SIGINT networks : - dui deploy : mode : global networks : dui : external : true","title":"Compose file (docker-stack.yml)"},{"location":"blogs/graceful-shutdown/#create-stack","text":"1 docker stack deploy --compose-file docker-stack.yml buming","title":"Create stack"},{"location":"blogs/graceful-shutdown/#execute-example_1","text":"Now that we have a docker swarm cluster and a stack - which has a service running on every node - we can showcase the power of graceful shutdown in a cluster of dependent services. Confirm the service is running correctly on every node, first lets check our nodes. 1 2 eval $( docker-machine env dui-1 ) docker node ls Which should look like this: 1 2 3 4 ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS f21ilm4thxegn5xbentmss5ur * dui-1 Ready Active Leader y7475bo5uplt2b58d050b4wfd dui-2 Ready Active 6ssxola6y1i6h9p8256pi7bfv dui-3 Ready Active Then check the service. 1 docker service ps buming_dui Which should look like this. 1 2 3 4 ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 3mrpr0jg31x1 buming_dui.6ssxola6y1i6h9p8256pi7bfv dui:latest dui-3 Running Running 17 seconds ago pfubtiy4j7vo buming_dui.f21ilm4thxegn5xbentmss5ur dui:latest dui-1 Running Running 17 seconds ago f4gjnmhoe3y4 buming_dui.y7475bo5uplt2b58d050b4wfd dui:latest dui-2 Running Running 17 seconds ago Now open a second terminal window. In window one, follow the service logs: 1 2 eval $( docker-machine env dui-1 ) docker service logs -f buming_dui In window two, go to a different node and stop the container. 1 2 3 eval $( docker-machine env dui-2 ) docker ps docker stop buming_dui.y7475bo5uplt2b58d050b4wfd.pnoui2x6elrz0tvkjz51njz94 In this case, you will see the other nodes receiving a leave notice and then the node stopping. 1 2 3 4 5 6 buming_dui.0.ryd8szexxku3@dui-3 | [ Server-John D. Carmack ] [ WARN ] [ 14 :19:02.604011 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = 83918f6ad817 , ip = 10.0.0.7 , name = Ken Thompson } buming_dui.0.so5m14sz8ksh@dui-1 | [ Server-Alan Kay ] [ WARN ] [ 14 :19:02.602082 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = 83918f6ad817 , ip = 10.0.0.7 , name = Ken Thompson } buming_dui.0.pnoui2x6elrz@dui-2 | Shutdown hook called! buming_dui.0.pnoui2x6elrz@dui-2 | [ App ] [ WARN ] [ 14 :19:02.598759 ] [ 1 ] [ ShotdownHook ] Shutting down at request of Docker buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.598858 ] [ 12 ] [ Main ] Stopping buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.601008 ] [ 12 ] [ Main ] Closing","title":"Execute example"},{"location":"blogs/graceful-shutdown/#further-reading","text":"Wikipedia page on reboots Microsoft about graceful shutdown Gracefully stopping docker containers What to know about Java and shutdown hooks https://www.weave.works/blog/docker-container-networking-multicast-fast/ https://www.weave.works/docs/net/latest/install/plugin/plugin-how-it-works/ https://www.weave.works/docs/net/latest/install/plugin/plugin-v2/ https://www.auzias.net/en/docker-network-multihost/ https://forums.docker.com/t/cannot-get-zookeeper-to-work-running-in-docker-using-swarm-mode/27109 https://github.com/docker/libnetwork/issues/740","title":"Further reading"},{"location":"blogs/jenkins-pipeline-support-tool/","text":"Jenkins Pipeline Support Tools With Jenkins now becoming Cloud Native and a first class citizen of Kubernetes, it is time to review how we use build tools. This content assumes you're using Jenkins in a Kubernetes cluster, but most of it should also work in other Docker-based environments. Ideal Pipeline Anyway, one thing we often see people do wrong with Jenkins pipelines is to use the Groovy Scripts as a general-purpose programming language. This creates many problems, bloated complicated pipelines, much more stress on the master instead of on the build agent and generally making things unreliable. A much better way is to use Jenkins pipelines only as orchestration and lean heavily on your build tools - e.g., Maven, Gradle, Yarn, Bazel - and shell scripts. Alas, if you created complicated pipelines in Groovy scripts, it is likely you'll end up the same with Bash scripts. An even better solution would be to create custom CLI applications that take care of large operations and convoluted logic. You can test and reuse them. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 pipeline { agent any stages { stage ( Build ) { steps { sh ./build.sh } } stage ( Test ) { steps { sh ./test.sh } } stage ( Deploy ) { steps { sh ./deploy.sh } } } post { success { sh ./successNotification.sh } failure { sh ./failureNotification.sh } } } Now, this might look a bit like a pipe dream, but it illustrates how you should use Jenkins Pipeline. The groovy script engine allows for a lot of freedom, but only rarely is its use justified. To create robust, modular and generic pipelines, it is recommended to use build tools, shell scripts, Shared Libraries and custom CLI's . It was always a bit difficult to manage generic scripts and tools across instances of Jenkins, pipelines, and teams. But with Pod Templates we have an excellent mechanism for using, versioning and distributing them with ease. Kubernetes Pods When Jenkins runs in Kubernetes, it can leverage it via the Kubernetes Plugin . I realize Jenkins conjures up mixed emotions when it comes to plugins, but this setup might replace most of them. How so? By using a Kubernetes Pod as the agent where instead of putting all your tools into a single VM you can use multiple small scoped containers. You can specify Pod Templates in multiple ways, where my personal favorite is to define it as yaml inside a declarative pipeline - see example below. For each tool you need, you specify the container and its configuration - if required. By default, you will always get a container with a Jenkins JNLP client and the workspace mounted as a volume in the pod. This allows you to create several tiny containers, each containing only the tools you need for a specific job. Now, it could happen you use two or more tools together a lot - let's say npm and maven - so it is ok to sometimes deviate from this to lower the overall memory of the pod. When you need custom logic, you will have to create a script or tool. This is where PodTemplate, Docker images and our desire for small narrow focus tools come together. PodTemplate example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 pipeline { agent { kubernetes { label mypod defaultContainer jnlp yaml apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:alpine command: - cat tty: true - name: busybox image: busybox command: - cat tty: true } } stages { stage ( Run maven ) { steps { container ( maven ) { sh mvn -version } container ( busybox ) { sh /bin/busybox } } } } } Java Example I bet most people do not think about Java when it comes lightweight CLI applications, but I think that is a shame. Java has excellent tooling to help you build well-tested applications which can be understood and maintained by a vast majority of developers. To make the images small, we will use some of the new tools available in Java land. We will first dive into using Java Modularity and JLink to create a compact and strict binary package, and then we move onto Graal for creating a Native image. Custom JDK Image All the source code of this example application is at github.com/joostvdg/jpb . It is a small CLI which does only one thing; it parses a git commit log to see which folders changed. Quite a useful tool for Monorepo's or other repositories containing more than one changeable resource. Such a CLI should have specific characteristics: * testable * small memory footprint * small disk footprint * quick start * easy to setup * easy to maintain These points sound like an excellent use case for Java Modules and JLink. For those who don't know, read up on Java Modules here and read up on JLink here . JLink will create a binary image that we can use with Alpine Linux to form a minimal Java (Docker) Image. Unfortunately, the plugins for Maven (JMod and JLink) seem to have died. The support on Gradle side is not much better. So I created a solution myself with a multi-stage Docker build. Which does detract a bit from the ease of setup. But overall, it hits the other characteristics spot on. Application Model For ease of maintenance and testing, we separate the parts of the CLI into Java Modules, as you can see in the model below. For using JLink, we need to be a module ourselves. So I figured to expand the exercise to use it to not only create boundaries via packages but also with Modules. Build The current LTS version of Java is 11, which means we need at least that if we want to be up-to-date. As we want to run the application in Alpine Linux, we need to build it with Alpine Linux - if you create a custom JDK image its OS specific. To my surprise, the official LTS release is not released for Alpine, so we use OpenJDK 12. Everything is built via a Multi-Stage Docker Build. This Dockerfile can be divided into five segments. creation of the base with a JDK 11+ on Alpine Linux compiling our Java Modules in into Module Jars test our code create our custom JDK image with just our code and whatever we need from the JDK create the runtime Docker image The Dockerfile looks a bit complicated, but we did get a Java runtime that is about 44MB in size and can run as a direct binary with no startup time. The Dockerfile can be much short if we use only a single module, but as our logic grows it is a thoughtful way to separate different concerns. Still, I'm not too happy with this for creating many small CLI's. To much handwork goes into creating the images like this. Relying on unmaintained Maven or Gradle Plugins doesn't seem a better choice. Luckily, there's a new game in town, GraalVM . We'll make an image with Graal next, stay tuned. Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 ############################################################### ############################################################### ##### 1. CREATE ALPINE BASE WITH JDK11+ #### OpenJDK image produces weird results with JLink (400mb + sizes) FROM alpine:3.8 AS build ENV JAVA_HOME = /opt/jdk \\ PATH = ${ PATH } :/opt/jdk/bin \\ LANG = C.UTF-8 RUN set -ex \\ apk add --no-cache bash \\ wget https://download.java.net/java/early_access/alpine/18/binaries/openjdk-12-ea+18_linux-x64-musl_bin.tar.gz -O jdk.tar.gz \\ mkdir -p /opt/jdk \\ tar zxvf jdk.tar.gz -C /opt/jdk --strip-components = 1 \\ rm jdk.tar.gz \\ rm /opt/jdk/lib/src.zip #################################### ## 2.a PREPARE COMPILE PHASE RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src ## 2.b COMPILE ALL JAVA FILES RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name *.java ) ## 2.c CREATE ALL JAVA MODULE JARS RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.jpb.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.core.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.jpb.core . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.cli.jar --module-version 1 .0 -e com.github.joostvdg.jpb.cli.JpbApp \\ -C /usr/src/mods/compiled/joostvdg.jpb.cli . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.core.test.jar --module-version 1 .0 -e com.github.joostvdg.jpb.core.test.ParseChangeListTest \\ -C /usr/src/mods/compiled/joostvdg.jpb.core.test . #################################### ## 3 RUN TESTS RUN rm -rf /usr/bin/jpb-test-image RUN jlink \\ --verbose \\ --compress 2 \\ --no-header-files \\ --no-man-pages \\ --strip-debug \\ --limit-modules java.base \\ --launcher jpb-test = joostvdg.jpb.core.test \\ --module-path /usr/src/mods/jars/: $JAVA_HOME /jmods \\ --add-modules joostvdg.jpb.core.test \\ --add-modules joostvdg.jpb.core \\ --add-modules joostvdg.jpb.api \\ --output /usr/bin/jpb-test-image RUN /usr/bin/jpb-test-image/bin/java --list-modules RUN /usr/bin/jpb-test-image/bin/jpb-test #################################### ## 4 BUILD RUNTIME - CUSTOM JDK IMAGE RUN rm -rf /usr/bin/jpb-image RUN jlink \\ --verbose \\ --compress 2 \\ --no-header-files \\ --no-man-pages \\ --strip-debug \\ --limit-modules java.base \\ --launcher jpb = joostvdg.jpb.cli \\ --module-path /usr/src/mods/jars/: $JAVA_HOME /jmods \\ --add-modules joostvdg.jpb.cli \\ --add-modules joostvdg.jpb.api \\ --add-modules joostvdg.jpb.core \\ --output /usr/bin/jpb-image RUN /usr/bin/jpb-image/bin/java --list-modules #################################### ##### 5. RUNTIME IMAGE - ALPINE FROM panga/alpine:3.8-glibc2.27 LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.1.0 LABEL description = Docker image for running Jenkins Pipeline Binary ENV DATE_CHANGED = 20181014-2035 ENV JAVA_OPTS = -XX:+UseCGroupMemoryLimitForHeap -XX:+UnlockExperimentalVMOptions COPY --from = build /usr/bin/jpb-image/ /usr/bin/jpb ENTRYPOINT [ /usr/bin/jpb/bin/jpb ] Image disk size 1 2 REPOSITORY TAG IMAGE ID CREATED SIZE jpb latest af7dda45732a About a minute ago 43 .8MB Graal GraalVM is a universal virtual machine for running applications written in JavaScript, Python, Ruby, R, JVM-based languages like Java, Scala, Kotlin, Clojure, and LLVM-based languages such as C and C++. - graalvm.org Ok, that doesn't tell you why using GraalVM is excellent for creating small CLI docker images. Maybe this quote helps: Native images compiled with GraalVM ahead-of-time improve the startup time and reduce the memory footprint of JVM-based applications. Where JLink allows you to create a custom JDK image and embed your application as a runtime binary, Graal goes one step further. It replaces the VM altogether and uses Substrate VM to run your binary. It can't do a lot of the fantastic things the JVM can do and isn't suited for long running applications or those with a large memory footprint and so on. Well, our CLI applications are single shot executions with low memory footprint, the perfect fit for Graal/Substrate! All the code from this example can is on GitHub at github.com/demomon/jpc-graal--maven . Application Model While building modular Java applications is excellent, the current tooling support terrible. So this time the application is a single Jar - Graal can create images from classes or jars - where packages do the separation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README . md \u251c\u2500\u2500 docker - graal - build . sh \u251c\u2500\u2500 pom . xml \u2514\u2500\u2500 src \u251c\u2500\u2500 main \u2514\u2500\u2500 java \u2514\u2500\u2500 com \u2514\u2500\u2500 github \u2514\u2500\u2500 joostvdg \u2514\u2500\u2500 demo \u251c\u2500\u2500 App . java \u2514\u2500\u2500 Hello . java Build Graal can build a native image based on a Jar file. This allows us to use any standard Java build tool such as Maven or Gradle to build the jar. The actual Graal build will be done in a Dockerfile. The people over at Oracle have created an official Docker image reducing effort spend on our side. The Dockerfile has three segments: build the jar with Maven build the native image with Graal assembly the runtime Docker image based on Alpine As you can see below, the Graal image is only half the size of the JLink image! Let's see how that stacks up to other languages such as Go and Python. Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ####################################### ## 1. BUILD JAR WITH MAVEN FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src RUN mvn clean package -e ####################################### ## 2. BUILD NATIVE IMAGE WITH GRAAL FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from = BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath ####################################### ## 3. BUILD DOCKER RUNTIME IMAGE FROM alpine:3.8 CMD [ jpc-graal ] COPY --from = NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal ####################################### Image disk size 1 2 REPOSITORY TAG IMAGE ID CREATED SIZE jpc-graal-maven latest dc33ebb10813 About an hour ago 19 .6MB Go Example Application Model Build Dockerfile Image Disk size 1 2 REPOSITORY TAG IMAGE ID CREATED SIZE jpc-go latest bb4a8e546601 6 minutes ago 12 .3MB Python Example Application Model Build Dockerfile Image Disk size Container footprint 1 2 3 4 5 6 kubectl top pods mypod-s4wpb-7dz4q --containers POD NAME CPU ( cores ) MEMORY ( bytes ) mypod-7lxnk-gw1sj jpc-go 0m 0Mi mypod-7lxnk-gw1sj java-jlink 0m 0Mi mypod-7lxnk-gw1sj java-graal 0m 0Mi mypod-7lxnk-gw1sj jnlp 150m 96Mi So, the 0Mi memory seems wrong. So I decided to dive into the Google Cloud Console, to see if there's any information in there. What I found there, is the data you can see below. The memory is indeed 0Mi, as they're using between 329 and 815 Kilobytes and not hitting the MB threshold (and thus get listed as 0Mi). We do see that graal uses more CPU and slightly less memory than the JLink setup. Both are still significantly larger than the Go CLI tool, but as long as the JNLP container takes ~100MB, I don't think we should worry about 400-500KB. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 CPU container/cpu/usage_time:gke_container:REDUCE_SUM ( , ps-dev-201405 ) : 0 .24 java-graal: 5e-4 java-jlink: 3e-3 jnlp: 0 .23 jpc-go: 2e-4 Memory java-graal: 729 ,088.00 java-jlink: 815 ,104.00 jnlp: 101 .507M jpc-go: 327 ,680.00 Disk java-graal: 49 ,152.00 java-jlink: 49 ,152.00 jnlp: 94 ,208.00 jpc-go: 49 ,152.00 Pipeline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 pipeline { agent { kubernetes { label mypod defaultContainer jnlp yaml apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: java-graal image: caladreas/jpc-graal:0.1.0-maven-b1 command: - cat tty: true - name: java-jlink image: caladreas/jpc-jlink:0.1.0-b1 command: - cat tty: true - name: jpc-go image: caladreas/jpc-go:0.1.0-b1 command: - cat tty: true } } stages { stage ( Test Versions ) { steps { container ( java-graal ) { echo java-graal sh /usr/local/bin/jpc-graal sleep 5 } container ( java-jlink ) { echo java-jlink sh /usr/bin/jpb/bin/jpb GitChangeListToFolder abc abc sleep 5 } container ( jpc-go ) { sh jpc-go sayHello -n joost sleep 5 } sleep 60 } } } }","title":"Jenkins Pipeline Support Tools"},{"location":"blogs/jenkins-pipeline-support-tool/#jenkins-pipeline-support-tools","text":"With Jenkins now becoming Cloud Native and a first class citizen of Kubernetes, it is time to review how we use build tools. This content assumes you're using Jenkins in a Kubernetes cluster, but most of it should also work in other Docker-based environments.","title":"Jenkins Pipeline Support Tools"},{"location":"blogs/jenkins-pipeline-support-tool/#ideal-pipeline","text":"Anyway, one thing we often see people do wrong with Jenkins pipelines is to use the Groovy Scripts as a general-purpose programming language. This creates many problems, bloated complicated pipelines, much more stress on the master instead of on the build agent and generally making things unreliable. A much better way is to use Jenkins pipelines only as orchestration and lean heavily on your build tools - e.g., Maven, Gradle, Yarn, Bazel - and shell scripts. Alas, if you created complicated pipelines in Groovy scripts, it is likely you'll end up the same with Bash scripts. An even better solution would be to create custom CLI applications that take care of large operations and convoluted logic. You can test and reuse them. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 pipeline { agent any stages { stage ( Build ) { steps { sh ./build.sh } } stage ( Test ) { steps { sh ./test.sh } } stage ( Deploy ) { steps { sh ./deploy.sh } } } post { success { sh ./successNotification.sh } failure { sh ./failureNotification.sh } } } Now, this might look a bit like a pipe dream, but it illustrates how you should use Jenkins Pipeline. The groovy script engine allows for a lot of freedom, but only rarely is its use justified. To create robust, modular and generic pipelines, it is recommended to use build tools, shell scripts, Shared Libraries and custom CLI's . It was always a bit difficult to manage generic scripts and tools across instances of Jenkins, pipelines, and teams. But with Pod Templates we have an excellent mechanism for using, versioning and distributing them with ease.","title":"Ideal Pipeline"},{"location":"blogs/jenkins-pipeline-support-tool/#kubernetes-pods","text":"When Jenkins runs in Kubernetes, it can leverage it via the Kubernetes Plugin . I realize Jenkins conjures up mixed emotions when it comes to plugins, but this setup might replace most of them. How so? By using a Kubernetes Pod as the agent where instead of putting all your tools into a single VM you can use multiple small scoped containers. You can specify Pod Templates in multiple ways, where my personal favorite is to define it as yaml inside a declarative pipeline - see example below. For each tool you need, you specify the container and its configuration - if required. By default, you will always get a container with a Jenkins JNLP client and the workspace mounted as a volume in the pod. This allows you to create several tiny containers, each containing only the tools you need for a specific job. Now, it could happen you use two or more tools together a lot - let's say npm and maven - so it is ok to sometimes deviate from this to lower the overall memory of the pod. When you need custom logic, you will have to create a script or tool. This is where PodTemplate, Docker images and our desire for small narrow focus tools come together.","title":"Kubernetes Pods"},{"location":"blogs/jenkins-pipeline-support-tool/#podtemplate-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 pipeline { agent { kubernetes { label mypod defaultContainer jnlp yaml apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:alpine command: - cat tty: true - name: busybox image: busybox command: - cat tty: true } } stages { stage ( Run maven ) { steps { container ( maven ) { sh mvn -version } container ( busybox ) { sh /bin/busybox } } } } }","title":"PodTemplate example"},{"location":"blogs/jenkins-pipeline-support-tool/#java-example","text":"I bet most people do not think about Java when it comes lightweight CLI applications, but I think that is a shame. Java has excellent tooling to help you build well-tested applications which can be understood and maintained by a vast majority of developers. To make the images small, we will use some of the new tools available in Java land. We will first dive into using Java Modularity and JLink to create a compact and strict binary package, and then we move onto Graal for creating a Native image.","title":"Java Example"},{"location":"blogs/jenkins-pipeline-support-tool/#custom-jdk-image","text":"All the source code of this example application is at github.com/joostvdg/jpb . It is a small CLI which does only one thing; it parses a git commit log to see which folders changed. Quite a useful tool for Monorepo's or other repositories containing more than one changeable resource. Such a CLI should have specific characteristics: * testable * small memory footprint * small disk footprint * quick start * easy to setup * easy to maintain These points sound like an excellent use case for Java Modules and JLink. For those who don't know, read up on Java Modules here and read up on JLink here . JLink will create a binary image that we can use with Alpine Linux to form a minimal Java (Docker) Image. Unfortunately, the plugins for Maven (JMod and JLink) seem to have died. The support on Gradle side is not much better. So I created a solution myself with a multi-stage Docker build. Which does detract a bit from the ease of setup. But overall, it hits the other characteristics spot on.","title":"Custom JDK Image"},{"location":"blogs/jenkins-pipeline-support-tool/#application-model","text":"For ease of maintenance and testing, we separate the parts of the CLI into Java Modules, as you can see in the model below. For using JLink, we need to be a module ourselves. So I figured to expand the exercise to use it to not only create boundaries via packages but also with Modules.","title":"Application Model"},{"location":"blogs/jenkins-pipeline-support-tool/#build","text":"The current LTS version of Java is 11, which means we need at least that if we want to be up-to-date. As we want to run the application in Alpine Linux, we need to build it with Alpine Linux - if you create a custom JDK image its OS specific. To my surprise, the official LTS release is not released for Alpine, so we use OpenJDK 12. Everything is built via a Multi-Stage Docker Build. This Dockerfile can be divided into five segments. creation of the base with a JDK 11+ on Alpine Linux compiling our Java Modules in into Module Jars test our code create our custom JDK image with just our code and whatever we need from the JDK create the runtime Docker image The Dockerfile looks a bit complicated, but we did get a Java runtime that is about 44MB in size and can run as a direct binary with no startup time. The Dockerfile can be much short if we use only a single module, but as our logic grows it is a thoughtful way to separate different concerns. Still, I'm not too happy with this for creating many small CLI's. To much handwork goes into creating the images like this. Relying on unmaintained Maven or Gradle Plugins doesn't seem a better choice. Luckily, there's a new game in town, GraalVM . We'll make an image with Graal next, stay tuned.","title":"Build"},{"location":"blogs/jenkins-pipeline-support-tool/#dockerfile","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 ############################################################### ############################################################### ##### 1. CREATE ALPINE BASE WITH JDK11+ #### OpenJDK image produces weird results with JLink (400mb + sizes) FROM alpine:3.8 AS build ENV JAVA_HOME = /opt/jdk \\ PATH = ${ PATH } :/opt/jdk/bin \\ LANG = C.UTF-8 RUN set -ex \\ apk add --no-cache bash \\ wget https://download.java.net/java/early_access/alpine/18/binaries/openjdk-12-ea+18_linux-x64-musl_bin.tar.gz -O jdk.tar.gz \\ mkdir -p /opt/jdk \\ tar zxvf jdk.tar.gz -C /opt/jdk --strip-components = 1 \\ rm jdk.tar.gz \\ rm /opt/jdk/lib/src.zip #################################### ## 2.a PREPARE COMPILE PHASE RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src ## 2.b COMPILE ALL JAVA FILES RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name *.java ) ## 2.c CREATE ALL JAVA MODULE JARS RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.jpb.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.core.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.jpb.core . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.cli.jar --module-version 1 .0 -e com.github.joostvdg.jpb.cli.JpbApp \\ -C /usr/src/mods/compiled/joostvdg.jpb.cli . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.core.test.jar --module-version 1 .0 -e com.github.joostvdg.jpb.core.test.ParseChangeListTest \\ -C /usr/src/mods/compiled/joostvdg.jpb.core.test . #################################### ## 3 RUN TESTS RUN rm -rf /usr/bin/jpb-test-image RUN jlink \\ --verbose \\ --compress 2 \\ --no-header-files \\ --no-man-pages \\ --strip-debug \\ --limit-modules java.base \\ --launcher jpb-test = joostvdg.jpb.core.test \\ --module-path /usr/src/mods/jars/: $JAVA_HOME /jmods \\ --add-modules joostvdg.jpb.core.test \\ --add-modules joostvdg.jpb.core \\ --add-modules joostvdg.jpb.api \\ --output /usr/bin/jpb-test-image RUN /usr/bin/jpb-test-image/bin/java --list-modules RUN /usr/bin/jpb-test-image/bin/jpb-test #################################### ## 4 BUILD RUNTIME - CUSTOM JDK IMAGE RUN rm -rf /usr/bin/jpb-image RUN jlink \\ --verbose \\ --compress 2 \\ --no-header-files \\ --no-man-pages \\ --strip-debug \\ --limit-modules java.base \\ --launcher jpb = joostvdg.jpb.cli \\ --module-path /usr/src/mods/jars/: $JAVA_HOME /jmods \\ --add-modules joostvdg.jpb.cli \\ --add-modules joostvdg.jpb.api \\ --add-modules joostvdg.jpb.core \\ --output /usr/bin/jpb-image RUN /usr/bin/jpb-image/bin/java --list-modules #################################### ##### 5. RUNTIME IMAGE - ALPINE FROM panga/alpine:3.8-glibc2.27 LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.1.0 LABEL description = Docker image for running Jenkins Pipeline Binary ENV DATE_CHANGED = 20181014-2035 ENV JAVA_OPTS = -XX:+UseCGroupMemoryLimitForHeap -XX:+UnlockExperimentalVMOptions COPY --from = build /usr/bin/jpb-image/ /usr/bin/jpb ENTRYPOINT [ /usr/bin/jpb/bin/jpb ]","title":"Dockerfile"},{"location":"blogs/jenkins-pipeline-support-tool/#image-disk-size","text":"1 2 REPOSITORY TAG IMAGE ID CREATED SIZE jpb latest af7dda45732a About a minute ago 43 .8MB","title":"Image disk size"},{"location":"blogs/jenkins-pipeline-support-tool/#graal","text":"GraalVM is a universal virtual machine for running applications written in JavaScript, Python, Ruby, R, JVM-based languages like Java, Scala, Kotlin, Clojure, and LLVM-based languages such as C and C++. - graalvm.org Ok, that doesn't tell you why using GraalVM is excellent for creating small CLI docker images. Maybe this quote helps: Native images compiled with GraalVM ahead-of-time improve the startup time and reduce the memory footprint of JVM-based applications. Where JLink allows you to create a custom JDK image and embed your application as a runtime binary, Graal goes one step further. It replaces the VM altogether and uses Substrate VM to run your binary. It can't do a lot of the fantastic things the JVM can do and isn't suited for long running applications or those with a large memory footprint and so on. Well, our CLI applications are single shot executions with low memory footprint, the perfect fit for Graal/Substrate! All the code from this example can is on GitHub at github.com/demomon/jpc-graal--maven .","title":"Graal"},{"location":"blogs/jenkins-pipeline-support-tool/#application-model_1","text":"While building modular Java applications is excellent, the current tooling support terrible. So this time the application is a single Jar - Graal can create images from classes or jars - where packages do the separation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README . md \u251c\u2500\u2500 docker - graal - build . sh \u251c\u2500\u2500 pom . xml \u2514\u2500\u2500 src \u251c\u2500\u2500 main \u2514\u2500\u2500 java \u2514\u2500\u2500 com \u2514\u2500\u2500 github \u2514\u2500\u2500 joostvdg \u2514\u2500\u2500 demo \u251c\u2500\u2500 App . java \u2514\u2500\u2500 Hello . java","title":"Application Model"},{"location":"blogs/jenkins-pipeline-support-tool/#build_1","text":"Graal can build a native image based on a Jar file. This allows us to use any standard Java build tool such as Maven or Gradle to build the jar. The actual Graal build will be done in a Dockerfile. The people over at Oracle have created an official Docker image reducing effort spend on our side. The Dockerfile has three segments: build the jar with Maven build the native image with Graal assembly the runtime Docker image based on Alpine As you can see below, the Graal image is only half the size of the JLink image! Let's see how that stacks up to other languages such as Go and Python.","title":"Build"},{"location":"blogs/jenkins-pipeline-support-tool/#dockerfile_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ####################################### ## 1. BUILD JAR WITH MAVEN FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src RUN mvn clean package -e ####################################### ## 2. BUILD NATIVE IMAGE WITH GRAAL FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from = BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath ####################################### ## 3. BUILD DOCKER RUNTIME IMAGE FROM alpine:3.8 CMD [ jpc-graal ] COPY --from = NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal #######################################","title":"Dockerfile"},{"location":"blogs/jenkins-pipeline-support-tool/#image-disk-size_1","text":"1 2 REPOSITORY TAG IMAGE ID CREATED SIZE jpc-graal-maven latest dc33ebb10813 About an hour ago 19 .6MB","title":"Image disk size"},{"location":"blogs/jenkins-pipeline-support-tool/#go-example","text":"","title":"Go Example"},{"location":"blogs/jenkins-pipeline-support-tool/#application-model_2","text":"","title":"Application Model"},{"location":"blogs/jenkins-pipeline-support-tool/#build_2","text":"","title":"Build"},{"location":"blogs/jenkins-pipeline-support-tool/#dockerfile_2","text":"","title":"Dockerfile"},{"location":"blogs/jenkins-pipeline-support-tool/#image-disk-size_2","text":"1 2 REPOSITORY TAG IMAGE ID CREATED SIZE jpc-go latest bb4a8e546601 6 minutes ago 12 .3MB","title":"Image Disk size"},{"location":"blogs/jenkins-pipeline-support-tool/#python-example","text":"","title":"Python Example"},{"location":"blogs/jenkins-pipeline-support-tool/#application-model_3","text":"","title":"Application Model"},{"location":"blogs/jenkins-pipeline-support-tool/#build_3","text":"","title":"Build"},{"location":"blogs/jenkins-pipeline-support-tool/#dockerfile_3","text":"","title":"Dockerfile"},{"location":"blogs/jenkins-pipeline-support-tool/#image-disk-size_3","text":"","title":"Image Disk size"},{"location":"blogs/jenkins-pipeline-support-tool/#container-footprint","text":"1 2 3 4 5 6 kubectl top pods mypod-s4wpb-7dz4q --containers POD NAME CPU ( cores ) MEMORY ( bytes ) mypod-7lxnk-gw1sj jpc-go 0m 0Mi mypod-7lxnk-gw1sj java-jlink 0m 0Mi mypod-7lxnk-gw1sj java-graal 0m 0Mi mypod-7lxnk-gw1sj jnlp 150m 96Mi So, the 0Mi memory seems wrong. So I decided to dive into the Google Cloud Console, to see if there's any information in there. What I found there, is the data you can see below. The memory is indeed 0Mi, as they're using between 329 and 815 Kilobytes and not hitting the MB threshold (and thus get listed as 0Mi). We do see that graal uses more CPU and slightly less memory than the JLink setup. Both are still significantly larger than the Go CLI tool, but as long as the JNLP container takes ~100MB, I don't think we should worry about 400-500KB. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 CPU container/cpu/usage_time:gke_container:REDUCE_SUM ( , ps-dev-201405 ) : 0 .24 java-graal: 5e-4 java-jlink: 3e-3 jnlp: 0 .23 jpc-go: 2e-4 Memory java-graal: 729 ,088.00 java-jlink: 815 ,104.00 jnlp: 101 .507M jpc-go: 327 ,680.00 Disk java-graal: 49 ,152.00 java-jlink: 49 ,152.00 jnlp: 94 ,208.00 jpc-go: 49 ,152.00","title":"Container footprint"},{"location":"blogs/jenkins-pipeline-support-tool/#pipeline","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 pipeline { agent { kubernetes { label mypod defaultContainer jnlp yaml apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: java-graal image: caladreas/jpc-graal:0.1.0-maven-b1 command: - cat tty: true - name: java-jlink image: caladreas/jpc-jlink:0.1.0-b1 command: - cat tty: true - name: jpc-go image: caladreas/jpc-go:0.1.0-b1 command: - cat tty: true } } stages { stage ( Test Versions ) { steps { container ( java-graal ) { echo java-graal sh /usr/local/bin/jpc-graal sleep 5 } container ( java-jlink ) { echo java-jlink sh /usr/bin/jpb/bin/jpb GitChangeListToFolder abc abc sleep 5 } container ( jpc-go ) { sh jpc-go sayHello -n joost sleep 5 } sleep 60 } } } }","title":"Pipeline"},{"location":"blogs/jenkins-x/","text":"Jenkins X Choose your distribution GKE via JX binary 1 2 3 4 5 6 7 export JX_TOOL_PSW = ZfwYM0odeI5W41GGzXgGqFmP export MACHINE_TYPE = n1-standard-2 export GKE_ZONE = europe-west4-a export K8S_VERSION = 1 .11.2-gke.18 export GKE_NAME = joostvdg-jx-nov18-1 export GIT_API_TOKEN = export PROJECT_ID = 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 jx create cluster gke \\ --cluster-name = ${ GKE_NAME } \\ --default-admin-password = ${ JX_TOOL_PSW } \\ --domain = kearos.net \\ --git-api-token = ${ GIT_API_TOKEN } \\ --git-username = joostvdg \\ --no-tiller \\ --project-id = ${ PROJECT_ID } \\ --prow = true \\ --vault = true \\ --zone = ${ GKE_ZONE } \\ --machine-type = ${ MACHINE_TYPE } \\ --labels = owner=jvandergriendt,purpose=practice,team=ps \\ --skip-login = true \\ --max-num-nodes = 3 \\ --min-num-nodes = 2 \\ --kubernetes-version = ${ K8S_VERSION } \\ --batch-mode = true 1 2 --skip-installation = false: Provision cluster only, don t install Jenkins X into it --skip-login = false: Skip Google auth if already logged in via gcloud auth","title":"Jenkins X"},{"location":"blogs/jenkins-x/#jenkins-x","text":"","title":"Jenkins X"},{"location":"blogs/jenkins-x/#choose-your-distribution","text":"","title":"Choose your distribution"},{"location":"blogs/jenkins-x/#gke-via-jx-binary","text":"1 2 3 4 5 6 7 export JX_TOOL_PSW = ZfwYM0odeI5W41GGzXgGqFmP export MACHINE_TYPE = n1-standard-2 export GKE_ZONE = europe-west4-a export K8S_VERSION = 1 .11.2-gke.18 export GKE_NAME = joostvdg-jx-nov18-1 export GIT_API_TOKEN = export PROJECT_ID = 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 jx create cluster gke \\ --cluster-name = ${ GKE_NAME } \\ --default-admin-password = ${ JX_TOOL_PSW } \\ --domain = kearos.net \\ --git-api-token = ${ GIT_API_TOKEN } \\ --git-username = joostvdg \\ --no-tiller \\ --project-id = ${ PROJECT_ID } \\ --prow = true \\ --vault = true \\ --zone = ${ GKE_ZONE } \\ --machine-type = ${ MACHINE_TYPE } \\ --labels = owner=jvandergriendt,purpose=practice,team=ps \\ --skip-login = true \\ --max-num-nodes = 3 \\ --min-num-nodes = 2 \\ --kubernetes-version = ${ K8S_VERSION } \\ --batch-mode = true 1 2 --skip-installation = false: Provision cluster only, don t install Jenkins X into it --skip-login = false: Skip Google auth if already logged in via gcloud auth","title":"GKE via JX binary"},{"location":"blogs/k8s-controller/","text":"Create your own custom Kubernetes controller Before we dive into the why and how of creating a Kubernetes Controller, let's take a brief look at what it does. What is a Controller I will only briefly touch on what a controller is. If you already know what it is you can safely skip this paragraph. In applications of robotics and automation, a control loop is a non-terminating loop that regulates the state of the system. In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the API server and makes changes attempting to move the current state towards the desired state. Examples of controllers that ship with Kubernetes today are the replication controller, endpoints controller, namespace controller, and serviceaccounts controller. So the purpose of controllers is to control - hence the name - the state of the system - our Kubernetes cluster. A controller is generally created to watch a single resource type and make sure that its desired state is met. As there is already very well written material on the details of controllers, I'll leave it at this. For more information on controllers and how they work, I recommend reading bitnami's deepdive and the kubernetes documentation . When to create your controller Great, you're still reading this. So when would you put in the effort to create your controller? I'm pretty sure there will be more cases, but the following two are the main ones. Process events on Core resources, Core being the resources any Kubernetes ships with Process events on Customer resources Examples of customer controllers for the first use case are tools such as Kubediff, which will compare resources in the cluster with their definition in a Git repository. For the second use case - custom controller for custom resource - there are many more examples. As most custom resources will have their controller to act on the events of the resources because existing controllers will not process the custom resource. Additionally, in most cases having resources sitting in a cluster with nothing happening is a bit of a waste. So we write a controller to match the resource. How to create your controller When it comes to making a controller, it will be some Go (lang) code using the Kubernetes client library. This is straightforward if you're creating a controller for the core resources, but quite a few steps if you write a custom controller. Write a core resource controller To ease ourselves into it lets first create a core resource controller. We're aiming for a controller that can read our ConfigMaps resources. To be able to do this, we need the following: Handler : for the events (Created, Deleted, Updated) Controller : retrieves events from an informer, puts work on a queue, and delegates the events to the handler Entrypoint : typically a main.go file, that creates a connection to the Kubernetes API server and ties all of the resources together Dockerfile : to package our binary for running inside the cluster Resource Definition YAML : typical Kubernetes resource definition file, in our case a Deployment, so our controller will run as a pod/container Handler Controller Entrypoint Dockerfile Resource Definition Resources https://medium.com/@trstringer/create-kubernetes-controllers-for-core-and-custom-resources-62fc35ad64a3 https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/ https://coreos.com/blog/introducing-operators.html https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html https://github.com/joostvdg/k8s-core-resource-controller https://github.com/kubernetes/sample-controller/blob/master/controller.go","title":"Create your own custom Kubernetes controller"},{"location":"blogs/k8s-controller/#create-your-own-custom-kubernetes-controller","text":"Before we dive into the why and how of creating a Kubernetes Controller, let's take a brief look at what it does.","title":"Create your own custom Kubernetes controller"},{"location":"blogs/k8s-controller/#what-is-a-controller","text":"I will only briefly touch on what a controller is. If you already know what it is you can safely skip this paragraph. In applications of robotics and automation, a control loop is a non-terminating loop that regulates the state of the system. In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the API server and makes changes attempting to move the current state towards the desired state. Examples of controllers that ship with Kubernetes today are the replication controller, endpoints controller, namespace controller, and serviceaccounts controller. So the purpose of controllers is to control - hence the name - the state of the system - our Kubernetes cluster. A controller is generally created to watch a single resource type and make sure that its desired state is met. As there is already very well written material on the details of controllers, I'll leave it at this. For more information on controllers and how they work, I recommend reading bitnami's deepdive and the kubernetes documentation .","title":"What is a Controller"},{"location":"blogs/k8s-controller/#when-to-create-your-controller","text":"Great, you're still reading this. So when would you put in the effort to create your controller? I'm pretty sure there will be more cases, but the following two are the main ones. Process events on Core resources, Core being the resources any Kubernetes ships with Process events on Customer resources Examples of customer controllers for the first use case are tools such as Kubediff, which will compare resources in the cluster with their definition in a Git repository. For the second use case - custom controller for custom resource - there are many more examples. As most custom resources will have their controller to act on the events of the resources because existing controllers will not process the custom resource. Additionally, in most cases having resources sitting in a cluster with nothing happening is a bit of a waste. So we write a controller to match the resource.","title":"When to create your controller"},{"location":"blogs/k8s-controller/#how-to-create-your-controller","text":"When it comes to making a controller, it will be some Go (lang) code using the Kubernetes client library. This is straightforward if you're creating a controller for the core resources, but quite a few steps if you write a custom controller.","title":"How to create your controller"},{"location":"blogs/k8s-controller/#write-a-core-resource-controller","text":"To ease ourselves into it lets first create a core resource controller. We're aiming for a controller that can read our ConfigMaps resources. To be able to do this, we need the following: Handler : for the events (Created, Deleted, Updated) Controller : retrieves events from an informer, puts work on a queue, and delegates the events to the handler Entrypoint : typically a main.go file, that creates a connection to the Kubernetes API server and ties all of the resources together Dockerfile : to package our binary for running inside the cluster Resource Definition YAML : typical Kubernetes resource definition file, in our case a Deployment, so our controller will run as a pod/container","title":"Write a core resource controller"},{"location":"blogs/k8s-controller/#handler","text":"","title":"Handler"},{"location":"blogs/k8s-controller/#controller","text":"","title":"Controller"},{"location":"blogs/k8s-controller/#entrypoint","text":"","title":"Entrypoint"},{"location":"blogs/k8s-controller/#dockerfile","text":"","title":"Dockerfile"},{"location":"blogs/k8s-controller/#resource-definition","text":"","title":"Resource Definition"},{"location":"blogs/k8s-controller/#resources","text":"https://medium.com/@trstringer/create-kubernetes-controllers-for-core-and-custom-resources-62fc35ad64a3 https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/ https://coreos.com/blog/introducing-operators.html https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html https://github.com/joostvdg/k8s-core-resource-controller https://github.com/kubernetes/sample-controller/blob/master/controller.go","title":"Resources"},{"location":"blogs/k8s-crd/","text":"Kubernetes CRD Kubernetes is a fantastic platform that allows you to run a lot of different workloads in various ways. It has APIs front and center, allowing you to choose different implementation as they suit you. Sometimes you feel something is missing. There is a concept with your application or something you want from the cluster that isn't (however) available in Kubernetes. It is then that you can look for extending Kubernetes itself. Either its API or by creating a new kind of resource: a Custom Resource Definition or CRD. What you need resource definition : the yaml definition of your custom resource custom controller : a controller to interact with your custom resource Resource Definition As with any Kubernetes resource, you need a yaml file that defines it with the lexicon of Kubernetes. In this case, the Kind is CustomerResourceDefinition . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : apiextensions.k8s.io/v1beta1 kind : CustomResourceDefinition metadata : name : manifests.cat.kearos.net spec : group : cat.kearos.net version : v1 names : kind : ApplicationManifest plural : applicationmanifests singular : applicationmanifest shortNames : - cam scope : Namespaced apiVersion : as the name implies, it's an API extension kind : has to be CustomResourceDefinition else it wouldn't be a CRD name : name must match the spec fields below, and be in the form: . group : API group name so that you can group multiple resources somewhat together names : kind : the resource kind, used for other resource definitions plural is the official name used in the Kubernetes API, also the default for interaction with kubectl singular : alias for the API usage in kubectl and used as the display value shortNames : shortNames allow a shorter string to match your resource on the CLI scope : can either be Namespaced , tied to a specific namespace, or Cluster where it must be cluster-wide unique Install CRD Taking the above example and saving it as application - manifest . yml , we can install the CRD into the cluster as follows. 1 kubectl create -f application-manifest.yml Resource Usage Example 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : cat.kearos.net/v1 kind : ApplicationManifest metadata : name : manifest-cat spec : name : cat description : Central Application Tracker namespace : cat artifactIDs : - github.com/joostvdg/cat sources : - git@github.com:joostvdg/cat.git Looking at this example, you might wonder how this works. There is a specification in there - spec - with all kinds of custom fields. But where do they come from? Nowhere really, so you cannot validate this with the CRD alone. You can put any arbitrary field in there. So what do you do with the CRD then? You can create a custom controller that processes your custom resources. Because creating a custom controller for your custom resources is complicated and takes several steps, we will do this in a separate article.","title":"Create your own k8s resource"},{"location":"blogs/k8s-crd/#kubernetes-crd","text":"Kubernetes is a fantastic platform that allows you to run a lot of different workloads in various ways. It has APIs front and center, allowing you to choose different implementation as they suit you. Sometimes you feel something is missing. There is a concept with your application or something you want from the cluster that isn't (however) available in Kubernetes. It is then that you can look for extending Kubernetes itself. Either its API or by creating a new kind of resource: a Custom Resource Definition or CRD.","title":"Kubernetes CRD"},{"location":"blogs/k8s-crd/#what-you-need","text":"resource definition : the yaml definition of your custom resource custom controller : a controller to interact with your custom resource","title":"What you need"},{"location":"blogs/k8s-crd/#resource-definition","text":"As with any Kubernetes resource, you need a yaml file that defines it with the lexicon of Kubernetes. In this case, the Kind is CustomerResourceDefinition . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : apiextensions.k8s.io/v1beta1 kind : CustomResourceDefinition metadata : name : manifests.cat.kearos.net spec : group : cat.kearos.net version : v1 names : kind : ApplicationManifest plural : applicationmanifests singular : applicationmanifest shortNames : - cam scope : Namespaced apiVersion : as the name implies, it's an API extension kind : has to be CustomResourceDefinition else it wouldn't be a CRD name : name must match the spec fields below, and be in the form: . group : API group name so that you can group multiple resources somewhat together names : kind : the resource kind, used for other resource definitions plural is the official name used in the Kubernetes API, also the default for interaction with kubectl singular : alias for the API usage in kubectl and used as the display value shortNames : shortNames allow a shorter string to match your resource on the CLI scope : can either be Namespaced , tied to a specific namespace, or Cluster where it must be cluster-wide unique","title":"Resource Definition"},{"location":"blogs/k8s-crd/#install-crd","text":"Taking the above example and saving it as application - manifest . yml , we can install the CRD into the cluster as follows. 1 kubectl create -f application-manifest.yml","title":"Install CRD"},{"location":"blogs/k8s-crd/#resource-usage-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : cat.kearos.net/v1 kind : ApplicationManifest metadata : name : manifest-cat spec : name : cat description : Central Application Tracker namespace : cat artifactIDs : - github.com/joostvdg/cat sources : - git@github.com:joostvdg/cat.git Looking at this example, you might wonder how this works. There is a specification in there - spec - with all kinds of custom fields. But where do they come from? Nowhere really, so you cannot validate this with the CRD alone. You can put any arbitrary field in there. So what do you do with the CRD then? You can create a custom controller that processes your custom resources. Because creating a custom controller for your custom resources is complicated and takes several steps, we will do this in a separate article.","title":"Resource Usage Example"},{"location":"blogs/k8s-lets-encrypt/","text":"Let's Encrypt for Kubernetes Let's Encrypt is a free, automated and open Certificate Authority. The kind of service you need if you want to have a secure website with https - yes I know that requires more than that - and it's now more straightforward to use than ever. This about using Let's Encrypt for generating a certificate for your service on Kubernetes. There are several ways to do this, with more or less automation, cluster-wide or namespace bound or with a DNS or HTTP validation check. I'll choose the route that was the easiest for me, and then I'll briefly look at the other options. Prerequisites There are some prerequisites required, that are best discussed on their own. So we will continue with the assumption that you have these in order. valid Class A or CNAME domain name kubernetes cluster with ingress controller (such as nginx) with helm and tiller installed in the cluster web application Steps The steps to take to get a web application to get a certificate from Let's Encrypt are the following. install cert-manager from the official helm chart deploy a Issuer resource deploy a certificate resource confirm certificate and secret are created/filled use in web app Install Cert Manager For more details on Cert Manager, I recommend reading their introduction . In essence, it's a tool that helps you initiate a certificate request with a service such as Let's Encrypt. You can install it via Helm, and it's meant to be installed only once per cluster. The once per cluster restriction is due to the usage of Custom Resource Definitions (CRD's) which will block any (re-)installation. To confirm if there are any CRD's from cert-manager, you can issue the following command. 1 kubectl get customresourcedefinitions.apiextensions.k8s.io The CRD's belonging to cert-manager are the following: certificates.certmanager.k8s.io clusterissuers.certmanager.k8s.io issuers.certmanager.k8s.io You will notice later, that we will use these CRD's for getting our certificate, so keep them in mind. To remove them in case of a re-install, you can issue the command below. 1 2 3 4 kubectl delete customresourcedefinitions.apiextensions.k8s.io \\ certificates.certmanager.k8s.io \\ clusterissuers.certmanager.k8s.io \\ issuers.certmanager.k8s.io When you're sure there are no CRD's left, you can install cert-manager via it's helm chart. It has some options in case you need them, you read about them here , but in my case that wasn't needed. 1 helm install --name cert-manager --namespace default stable/cert-manager Deploy Issuer To be able to use a certificate we need to have a Certificate Issuer. If you remember from our cert - manager , there are two CRD's that can take this role: ClusterIssuer : clusterissuers.certmanager.k8s.io Issuer : issuers.certmanager.k8s.io Both issuer type can use two ways of providing the proof of ownership, either by dns - 01 or http - 01 . We'll be using the http - 01 method, for the dns - 01 method, refer to the cert-manager documenation . ClusterIssuer As the resource Kind implies, a ClusterIssuer is a cluster-wide resource and not bound to a specific namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : # The ACME server URL server : https://acme-staging.api.letsencrypt.org/directory # Email address used for ACME registration email : user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} Issuer Not everyone wants a cluster-wide resource, and not everyone has the rights to install something elsewhere than their namespace. I prefer having as much as possible tied to a namespace - either a team or an application - I will use this type. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : myapp-letsencrypt-staging namespace : myapp spec : acme : # The ACME server URL server : https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email : myadmin@myapp.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : myapp-letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} There's a few things to note here: server : this refers to the server executing the ACME test, in this case: Let's Encrypt Staging (with the v2 API) email : this will be the account it will use for registering the certificate privateKeySecretRef : this is the Kubernetes secret resource in which the privateKey will be stored, just in case you need or want to remove it Deploy Certificate Resource Next up is our Certificate resource, this is where cert - manager will store our certificate details to be used by our application. In case you forgot, this is one of the three CRD's provided by cert-manager. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : myapp-example-com namespace : myapp spec : secretName : myapp-tls dnsNames : - myapp.example.com acme : config : - http01 : ingressClass : nginx domains : - myapp.example.com issuerRef : name : myapp-letsencrypt-staging kind : Issuer The things to note here: name : so far I've found it a naming convention to write the domain name where - replaces the . 's. secretName : the name of the Kubernetes secret that will house the certificate and certificate key dnsNames : you can specify more than one name, in our case just a single one, should match acme . config . domains acme.config : this defines the configuration for how the ownership proof should be done, this should match the method defined in the Issuer issuerRef : in good Kubernetes fashion, we reference the Issuer that should issue our certificate, the name and kind should match our Issue resource Confirm Resources We have defined our set of resources that should create our valid - though not trusted, as it is staging - certificate. Before we use it, we should confirm our secret and our certificate are both valid. 1 kubectl describe certificate myapp-example-com --namespace myapp The response includes the latest status, which looks like this: 1 2 3 4 5 6 7 8 9 10 Status : Acme : Order : URL : https://acme-staging-v02.api.letsencrypt.org/acme/order/705.../960... Conditions : Last Transition Time : 2018-10-02T21:17:34Z Message : Certificate issued successfully Reason : CertIssued Status : True Type : Ready Next up is our secret, containing the actual certificate and the certificate key. 1 kubectl describe secret myapp-tls --namespace myapp Which results in something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Name: myapp-tls Namespace: myapp Labels: certmanager.k8s.io/certificate-name = myapp-example-com Annotations: certmanager.k8s.io/alt-names: myapp.example.com certmanager.k8s.io/common-name: myapp.example.com certmanager.k8s.io/issuer-kind: Issuer certmanager.k8s.io/issuer-name: myapp-letsencrypt-staging Type: kubernetes.io/tls Data ==== tls.crt: 3797 bytes tls.key: 1679 bytes Use certificate to enable https Assuming the secret and the certificate are correct, we can use them to enable https on our web app. We put the tls certificate on the ingress of the application, in this case, Nginx, which assumes the following about the app: it has a deployment or stateful set it has a service which provides and endpoint to one or more instances it has an nginx ingress which points to the service Deployment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 kind : Deployment apiVersion : apps/v1 metadata : name : myapp namespace : myapp labels : app : myapp spec : replicas : 3 selector : matchLabels : app : myapp template : metadata : labels : app : myapp spec : containers : - name : myapp image : caladreas/catnip-master imagePullPolicy : Always ports : - containerPort : 8087 You should always include livenessprobe, resource limits and so on. But for the sake of brevity, these are omitted! Service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : v1 kind : Service metadata : name : myapp namespace : myapp labels : app : myapp spec : selector : app : myapp ports : - name : http port : 80 targetPort : 8087 protocol : TCP Ingress for Issuer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/ssl-redirect : true certmanager.k8s.io/issuer : myapp-letsencrypt-staging certmanager.k8s.io/acme-challenge-type : http01 spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls Ingress for ClusterIssuer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/ssl-redirect : true certmanager.k8s.io/cluster-issuer : letsencrypt-staging certmanager.k8s.io/acme-challenge-type : http01 spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls Further resources How Does Let's Encrypt Work Tutorial that inspired this page Amazone EKS Ingress Guide Kuberetes EKS Ingress and TLS How To Configure LTS for Nginx Ingress","title":"Let's Encrypt for Kubernetes Apps"},{"location":"blogs/k8s-lets-encrypt/#lets-encrypt-for-kubernetes","text":"Let's Encrypt is a free, automated and open Certificate Authority. The kind of service you need if you want to have a secure website with https - yes I know that requires more than that - and it's now more straightforward to use than ever. This about using Let's Encrypt for generating a certificate for your service on Kubernetes. There are several ways to do this, with more or less automation, cluster-wide or namespace bound or with a DNS or HTTP validation check. I'll choose the route that was the easiest for me, and then I'll briefly look at the other options.","title":"Let's Encrypt for Kubernetes"},{"location":"blogs/k8s-lets-encrypt/#prerequisites","text":"There are some prerequisites required, that are best discussed on their own. So we will continue with the assumption that you have these in order. valid Class A or CNAME domain name kubernetes cluster with ingress controller (such as nginx) with helm and tiller installed in the cluster web application","title":"Prerequisites"},{"location":"blogs/k8s-lets-encrypt/#steps","text":"The steps to take to get a web application to get a certificate from Let's Encrypt are the following. install cert-manager from the official helm chart deploy a Issuer resource deploy a certificate resource confirm certificate and secret are created/filled use in web app","title":"Steps"},{"location":"blogs/k8s-lets-encrypt/#install-cert-manager","text":"For more details on Cert Manager, I recommend reading their introduction . In essence, it's a tool that helps you initiate a certificate request with a service such as Let's Encrypt. You can install it via Helm, and it's meant to be installed only once per cluster. The once per cluster restriction is due to the usage of Custom Resource Definitions (CRD's) which will block any (re-)installation. To confirm if there are any CRD's from cert-manager, you can issue the following command. 1 kubectl get customresourcedefinitions.apiextensions.k8s.io The CRD's belonging to cert-manager are the following: certificates.certmanager.k8s.io clusterissuers.certmanager.k8s.io issuers.certmanager.k8s.io You will notice later, that we will use these CRD's for getting our certificate, so keep them in mind. To remove them in case of a re-install, you can issue the command below. 1 2 3 4 kubectl delete customresourcedefinitions.apiextensions.k8s.io \\ certificates.certmanager.k8s.io \\ clusterissuers.certmanager.k8s.io \\ issuers.certmanager.k8s.io When you're sure there are no CRD's left, you can install cert-manager via it's helm chart. It has some options in case you need them, you read about them here , but in my case that wasn't needed. 1 helm install --name cert-manager --namespace default stable/cert-manager","title":"Install Cert Manager"},{"location":"blogs/k8s-lets-encrypt/#deploy-issuer","text":"To be able to use a certificate we need to have a Certificate Issuer. If you remember from our cert - manager , there are two CRD's that can take this role: ClusterIssuer : clusterissuers.certmanager.k8s.io Issuer : issuers.certmanager.k8s.io Both issuer type can use two ways of providing the proof of ownership, either by dns - 01 or http - 01 . We'll be using the http - 01 method, for the dns - 01 method, refer to the cert-manager documenation .","title":"Deploy Issuer"},{"location":"blogs/k8s-lets-encrypt/#clusterissuer","text":"As the resource Kind implies, a ClusterIssuer is a cluster-wide resource and not bound to a specific namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : # The ACME server URL server : https://acme-staging.api.letsencrypt.org/directory # Email address used for ACME registration email : user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {}","title":"ClusterIssuer"},{"location":"blogs/k8s-lets-encrypt/#issuer","text":"Not everyone wants a cluster-wide resource, and not everyone has the rights to install something elsewhere than their namespace. I prefer having as much as possible tied to a namespace - either a team or an application - I will use this type. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : myapp-letsencrypt-staging namespace : myapp spec : acme : # The ACME server URL server : https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email : myadmin@myapp.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : myapp-letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} There's a few things to note here: server : this refers to the server executing the ACME test, in this case: Let's Encrypt Staging (with the v2 API) email : this will be the account it will use for registering the certificate privateKeySecretRef : this is the Kubernetes secret resource in which the privateKey will be stored, just in case you need or want to remove it","title":"Issuer"},{"location":"blogs/k8s-lets-encrypt/#deploy-certificate-resource","text":"Next up is our Certificate resource, this is where cert - manager will store our certificate details to be used by our application. In case you forgot, this is one of the three CRD's provided by cert-manager. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : myapp-example-com namespace : myapp spec : secretName : myapp-tls dnsNames : - myapp.example.com acme : config : - http01 : ingressClass : nginx domains : - myapp.example.com issuerRef : name : myapp-letsencrypt-staging kind : Issuer The things to note here: name : so far I've found it a naming convention to write the domain name where - replaces the . 's. secretName : the name of the Kubernetes secret that will house the certificate and certificate key dnsNames : you can specify more than one name, in our case just a single one, should match acme . config . domains acme.config : this defines the configuration for how the ownership proof should be done, this should match the method defined in the Issuer issuerRef : in good Kubernetes fashion, we reference the Issuer that should issue our certificate, the name and kind should match our Issue resource","title":"Deploy Certificate Resource"},{"location":"blogs/k8s-lets-encrypt/#confirm-resources","text":"We have defined our set of resources that should create our valid - though not trusted, as it is staging - certificate. Before we use it, we should confirm our secret and our certificate are both valid. 1 kubectl describe certificate myapp-example-com --namespace myapp The response includes the latest status, which looks like this: 1 2 3 4 5 6 7 8 9 10 Status : Acme : Order : URL : https://acme-staging-v02.api.letsencrypt.org/acme/order/705.../960... Conditions : Last Transition Time : 2018-10-02T21:17:34Z Message : Certificate issued successfully Reason : CertIssued Status : True Type : Ready Next up is our secret, containing the actual certificate and the certificate key. 1 kubectl describe secret myapp-tls --namespace myapp Which results in something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Name: myapp-tls Namespace: myapp Labels: certmanager.k8s.io/certificate-name = myapp-example-com Annotations: certmanager.k8s.io/alt-names: myapp.example.com certmanager.k8s.io/common-name: myapp.example.com certmanager.k8s.io/issuer-kind: Issuer certmanager.k8s.io/issuer-name: myapp-letsencrypt-staging Type: kubernetes.io/tls Data ==== tls.crt: 3797 bytes tls.key: 1679 bytes","title":"Confirm Resources"},{"location":"blogs/k8s-lets-encrypt/#use-certificate-to-enable-https","text":"Assuming the secret and the certificate are correct, we can use them to enable https on our web app. We put the tls certificate on the ingress of the application, in this case, Nginx, which assumes the following about the app: it has a deployment or stateful set it has a service which provides and endpoint to one or more instances it has an nginx ingress which points to the service","title":"Use certificate to enable https"},{"location":"blogs/k8s-lets-encrypt/#deployment","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 kind : Deployment apiVersion : apps/v1 metadata : name : myapp namespace : myapp labels : app : myapp spec : replicas : 3 selector : matchLabels : app : myapp template : metadata : labels : app : myapp spec : containers : - name : myapp image : caladreas/catnip-master imagePullPolicy : Always ports : - containerPort : 8087 You should always include livenessprobe, resource limits and so on. But for the sake of brevity, these are omitted!","title":"Deployment"},{"location":"blogs/k8s-lets-encrypt/#service","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : v1 kind : Service metadata : name : myapp namespace : myapp labels : app : myapp spec : selector : app : myapp ports : - name : http port : 80 targetPort : 8087 protocol : TCP","title":"Service"},{"location":"blogs/k8s-lets-encrypt/#ingress-for-issuer","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/ssl-redirect : true certmanager.k8s.io/issuer : myapp-letsencrypt-staging certmanager.k8s.io/acme-challenge-type : http01 spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls","title":"Ingress for Issuer"},{"location":"blogs/k8s-lets-encrypt/#ingress-for-clusterissuer","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/ssl-redirect : true certmanager.k8s.io/cluster-issuer : letsencrypt-staging certmanager.k8s.io/acme-challenge-type : http01 spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls","title":"Ingress for ClusterIssuer"},{"location":"blogs/k8s-lets-encrypt/#further-resources","text":"How Does Let's Encrypt Work Tutorial that inspired this page Amazone EKS Ingress Guide Kuberetes EKS Ingress and TLS How To Configure LTS for Nginx Ingress","title":"Further resources"},{"location":"blogs/kubernetes-post-install/","text":"Kubernetes Post Install What to do after you've installed your Kubernetes cluster, whether that was EKS via eksctl or GKE via gcloud. make network more secure with encryption weavenet for example install package manager install helm tiller use nginx for ssl termination together with Let's Encrypt install nginx install cert-manager Helm Tiller Helm is the defacto standard package manager for Kubernetes. Its current iteration is version 2, which has a client component - Helm - and a serverside component, Tiller. There's a problem with that, due this setup with Helm and Tiller, Tiller is aking to a cluster admin. This isn't very secure and there are several ways around that. JenkinsX : its binary ( jx ) can install helm charts without using Tiller. It generates the kubernetes resource files and installs these directly custom RBAC setup : you can also setup RBAC in such a way that every separate namespace gets its own Tiller, limiting the reach of any Tiller Tiller Custom RBAC Example Namespaces 1 2 3 4 5 6 7 8 9 10 11 12 13 14 kind : Namespace apiVersion : v1 metadata : name : sre --- kind : Namespace apiVersion : v1 metadata : name : dev1 --- kind : Namespace apiVersion : v1 metadata : name : dev2 Service Accounts 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kind : ServiceAccount apiVersion : v1 metadata : name : tiller namespace : dev1 --- kind : ServiceAccount apiVersion : v1 metadata : name : tiller namespace : dev2 --- kind : ServiceAccount apiVersion : v1 metadata : name : helm namespace : sre Roles 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-manager namespace : dev1 rules : - apiGroups : [ , batch , extensions , apps ] resources : [ * ] verbs : [ * ] --- kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-manager namespace : dev2 rules : - apiGroups : [ , batch , extensions , apps ] resources : [ * ] verbs : [ * ] --- kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1 metadata : name : helm-clusterrole rules : - apiGroups : [ ] resources : [ pods/portforward ] verbs : [ create ] - apiGroups : [ ] resources : [ pods ] verbs : [ list , get ] RoleBindings 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-binding namespace : dev1 subjects : - kind : ServiceAccount name : tiller namespace : dev1 roleRef : kind : Role name : tiller-manager apiGroup : rbac.authorization.k8s.io --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-binding namespace : dev2 subjects : - kind : ServiceAccount name : tiller namespace : dev2 roleRef : kind : Role name : tiller-manager apiGroup : rbac.authorization.k8s.io --- kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : helm-clusterrolebinding roleRef : kind : ClusterRole apiGroup : rbac.authorization.k8s.io name : helm-clusterrole subjects : - kind : ServiceAccount name : helm namespace : sre Install Tiller 1 2 helm init --service-account tiller --tiller-namespace dev1 helm init --service-account tiller --tiller-namespace dev2 Create KubeConfig for Helm client 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # Find the secret associated with the Service Account SECRET = $( kubectl -n sre get sa helm -o jsonpath = {.secrets[].name} ) # Retrieve the token from the secret and decode it TOKEN = $( kubectl get secrets -n sre $SECRET -o jsonpath = {.data.token} | base64 -D ) # Retrieve the CA from the secret, decode it and write it to disk kubectl get secrets -n sre $SECRET -o jsonpath = {.data.ca\\.crt} | base64 -D ca.crt # Retrieve the current context CONTEXT = $( kubectl config current-context ) # Retrieve the cluster name CLUSTER_NAME = $( kubectl config get-contexts $CONTEXT --no-headers = true | awk {print $3} ) # Retrieve the API endpoint SERVER = $( kubectl config view -o jsonpath = {.clusters[?(@.name == \\ ${ CLUSTER_NAME } \\ )].cluster.server} ) # Set up variables KUBECONFIG_FILE = config USER = helm CA = ca.crt # Set up config kubectl config set-cluster $CLUSTER_NAME \\ --kubeconfig = $KUBECONFIG_FILE \\ --server = $SERVER \\ --certificate-authority = $CA \\ --embed-certs = true # Set token credentials kubectl config set-credentials \\ $USER \\ --kubeconfig = $KUBECONFIG_FILE \\ --token = $TOKEN # Set context entry kubectl config set-context \\ $USER \\ --kubeconfig = $KUBECONFIG_FILE \\ --cluster = $CLUSTER_NAME \\ --user = $USER # Set the current-context kubectl config use-context $USER \\ --kubeconfig = $KUBECONFIG_FILE Helm Install 1 2 3 4 5 6 7 8 9 10 11 12 helm install \\ --name prometheus \\ stable/prometheus \\ --tiller-namespace dev1 \\ --kubeconfig config \\ --namespace dev1 \\ --set rbac.create = false NAME: prometheus LAST DEPLOYED: Sun Oct 28 16 :22:46 2018 NAMESPACE: dev1 STATUS: DEPLOYED 1 2 3 4 5 6 7 8 9 10 11 12 helm install --name grafana \\ stable/grafana \\ --tiller-namespace dev2 \\ --kubeconfig config \\ --namespace dev2 \\ --set rbac.pspEnabled = false \\ --set rbac.create = false NAME: grafana LAST DEPLOYED: Sun Oct 28 16 :25:18 2018 NAMESPACE: dev2 STATUS: DEPLOYED References https://medium.com/@elijudah/configuring-minimal-rbac-permissions-for-helm-and-tiller-e7d792511d10 https://medium.com/virtuslab/think-twice-before-using-helm-25fbb18bc822 https://jenkins-x.io/architecture/helm3/ https://gist.github.com/innovia/fbba8259042f71db98ea8d4ad19bd708#file-kubernetes_add_service_account_kubeconfig-sh","title":"Kubernetes Post Install"},{"location":"blogs/kubernetes-post-install/#kubernetes-post-install","text":"What to do after you've installed your Kubernetes cluster, whether that was EKS via eksctl or GKE via gcloud. make network more secure with encryption weavenet for example install package manager install helm tiller use nginx for ssl termination together with Let's Encrypt install nginx install cert-manager","title":"Kubernetes Post Install"},{"location":"blogs/kubernetes-post-install/#helm-tiller","text":"Helm is the defacto standard package manager for Kubernetes. Its current iteration is version 2, which has a client component - Helm - and a serverside component, Tiller. There's a problem with that, due this setup with Helm and Tiller, Tiller is aking to a cluster admin. This isn't very secure and there are several ways around that. JenkinsX : its binary ( jx ) can install helm charts without using Tiller. It generates the kubernetes resource files and installs these directly custom RBAC setup : you can also setup RBAC in such a way that every separate namespace gets its own Tiller, limiting the reach of any Tiller","title":"Helm &amp; Tiller"},{"location":"blogs/kubernetes-post-install/#tiller-custom-rbac-example","text":"","title":"Tiller Custom RBAC Example"},{"location":"blogs/kubernetes-post-install/#namespaces","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 kind : Namespace apiVersion : v1 metadata : name : sre --- kind : Namespace apiVersion : v1 metadata : name : dev1 --- kind : Namespace apiVersion : v1 metadata : name : dev2","title":"Namespaces"},{"location":"blogs/kubernetes-post-install/#service-accounts","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kind : ServiceAccount apiVersion : v1 metadata : name : tiller namespace : dev1 --- kind : ServiceAccount apiVersion : v1 metadata : name : tiller namespace : dev2 --- kind : ServiceAccount apiVersion : v1 metadata : name : helm namespace : sre","title":"Service Accounts"},{"location":"blogs/kubernetes-post-install/#roles","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-manager namespace : dev1 rules : - apiGroups : [ , batch , extensions , apps ] resources : [ * ] verbs : [ * ] --- kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-manager namespace : dev2 rules : - apiGroups : [ , batch , extensions , apps ] resources : [ * ] verbs : [ * ] --- kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1 metadata : name : helm-clusterrole rules : - apiGroups : [ ] resources : [ pods/portforward ] verbs : [ create ] - apiGroups : [ ] resources : [ pods ] verbs : [ list , get ]","title":"Roles"},{"location":"blogs/kubernetes-post-install/#rolebindings","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-binding namespace : dev1 subjects : - kind : ServiceAccount name : tiller namespace : dev1 roleRef : kind : Role name : tiller-manager apiGroup : rbac.authorization.k8s.io --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-binding namespace : dev2 subjects : - kind : ServiceAccount name : tiller namespace : dev2 roleRef : kind : Role name : tiller-manager apiGroup : rbac.authorization.k8s.io --- kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : helm-clusterrolebinding roleRef : kind : ClusterRole apiGroup : rbac.authorization.k8s.io name : helm-clusterrole subjects : - kind : ServiceAccount name : helm namespace : sre","title":"RoleBindings"},{"location":"blogs/kubernetes-post-install/#install-tiller","text":"1 2 helm init --service-account tiller --tiller-namespace dev1 helm init --service-account tiller --tiller-namespace dev2","title":"Install Tiller"},{"location":"blogs/kubernetes-post-install/#create-kubeconfig-for-helm-client","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # Find the secret associated with the Service Account SECRET = $( kubectl -n sre get sa helm -o jsonpath = {.secrets[].name} ) # Retrieve the token from the secret and decode it TOKEN = $( kubectl get secrets -n sre $SECRET -o jsonpath = {.data.token} | base64 -D ) # Retrieve the CA from the secret, decode it and write it to disk kubectl get secrets -n sre $SECRET -o jsonpath = {.data.ca\\.crt} | base64 -D ca.crt # Retrieve the current context CONTEXT = $( kubectl config current-context ) # Retrieve the cluster name CLUSTER_NAME = $( kubectl config get-contexts $CONTEXT --no-headers = true | awk {print $3} ) # Retrieve the API endpoint SERVER = $( kubectl config view -o jsonpath = {.clusters[?(@.name == \\ ${ CLUSTER_NAME } \\ )].cluster.server} ) # Set up variables KUBECONFIG_FILE = config USER = helm CA = ca.crt # Set up config kubectl config set-cluster $CLUSTER_NAME \\ --kubeconfig = $KUBECONFIG_FILE \\ --server = $SERVER \\ --certificate-authority = $CA \\ --embed-certs = true # Set token credentials kubectl config set-credentials \\ $USER \\ --kubeconfig = $KUBECONFIG_FILE \\ --token = $TOKEN # Set context entry kubectl config set-context \\ $USER \\ --kubeconfig = $KUBECONFIG_FILE \\ --cluster = $CLUSTER_NAME \\ --user = $USER # Set the current-context kubectl config use-context $USER \\ --kubeconfig = $KUBECONFIG_FILE","title":"Create KubeConfig for Helm client"},{"location":"blogs/kubernetes-post-install/#helm-install","text":"1 2 3 4 5 6 7 8 9 10 11 12 helm install \\ --name prometheus \\ stable/prometheus \\ --tiller-namespace dev1 \\ --kubeconfig config \\ --namespace dev1 \\ --set rbac.create = false NAME: prometheus LAST DEPLOYED: Sun Oct 28 16 :22:46 2018 NAMESPACE: dev1 STATUS: DEPLOYED 1 2 3 4 5 6 7 8 9 10 11 12 helm install --name grafana \\ stable/grafana \\ --tiller-namespace dev2 \\ --kubeconfig config \\ --namespace dev2 \\ --set rbac.pspEnabled = false \\ --set rbac.create = false NAME: grafana LAST DEPLOYED: Sun Oct 28 16 :25:18 2018 NAMESPACE: dev2 STATUS: DEPLOYED","title":"Helm Install"},{"location":"blogs/kubernetes-post-install/#references","text":"https://medium.com/@elijudah/configuring-minimal-rbac-permissions-for-helm-and-tiller-e7d792511d10 https://medium.com/virtuslab/think-twice-before-using-helm-25fbb18bc822 https://jenkins-x.io/architecture/helm3/ https://gist.github.com/innovia/fbba8259042f71db98ea8d4ad19bd708#file-kubernetes_add_service_account_kubeconfig-sh","title":"References"},{"location":"blogs/kubernetes-sso-keycloak/","text":"Single Sign On on Kubernetes with Keycloak This article is about setting up an Apache Keycloak 1 instance for Single Sign-On 2 (SSO) on Kubernetes. Important This guide is created to help you show how you can do this from a technical point of view. If you are in an enterprise, please consult your in-house - if available - or external security professionals before running tools such as Keycloak in production. Goal The goal is to show why using Keycloak as SSO can be valuable and guide on how to do so. For what's in the How , look at the next paragraph: Steps . At the outset, this article is not about comparing SSO solutions or about claiming Keycloak is the best. Its a solution, and if you're interested in using this, it helps you with the how. Below are some links to alternatives as reference, but are not discussed. Audience The expected audience are those who are using Kubernetes 3 and are: looking to host a SSO solution themselves want to use LDAP 4 as User Federation for Keycloak want to setup a SSO with Jenkins 5 and/or SonarQube 6 Steps These are the steps we will execute. configure an LDAP server with a test data set package LDAP server in a Docker container and run it as an kubernetes Deployment you might think, why not a StatefulSet , but our LDAP will have static data and the idea of this setup, is that we do not control LDAP, meaning, we should expect LDAP's data to be immutable install Keycloak with Helm 8 we use Helm 2, as Helm 3 9 is still in beta at the time of writing (September 2019) configure Keycloak to use LDAP for User Federation install Jenkins and SonarQube with Helm 8 configure SSO with Keycloak in Jenkins configure SSO with Keycloak in SonarQube LDAP We will use OpenDJ 7 as LDAP implementation. There are many alternatives out there, feel free to use those. But for this guide, we will use OpenDJ's community edition, it works well and is easy to configure. We need the following: configured with a test data set including users and groups Docker container image definition Kubernetes Deployment definition to run Kubernetes Service definition to access a stable address Test Data Set example.ldiff This a full example. The different parts will be explained below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 dn : dc = example , dc = com objectclass : top objectclass : domain dc : example dn : ou = People , dc = example , dc = com objectclass : top objectclass : organizationalunit ou : People aci : ( targetattr = *||+ )( version 3.0 ; acl IDM Access ; allow ( all ) userdn = ldap:///uid=idm,ou=Administrators,dc=example,dc=com ;) dn : uid = cptjack , ou = People , dc = example , dc = com cn : cpt . Jack Sparrow sn : Sparrow givenname : Jack objectclass : top objectclass : person objectclass : organizationalPerson objectclass : inetOrgPerson ou : Operations ou : People l : Caribbean uid : cptjack mail : jack @ example . com telephonenumber : + 421 910 123456 facsimiletelephonenumber : + 1 408 555 1111 roomnumber : 666 userpassword : MyAwesomePassword dn : uid = djones , ou = People , dc = example , dc = com cn : Davy Jones sn : Jones givenname : Davy objectclass : top objectclass : person objectclass : organizationalPerson objectclass : inetOrgPerson ou : Operations ou : People l : Caribbean uid : djones mail : d . jones @ example . com telephonenumber : + 421 910 382735 facsimiletelephonenumber : + 1 408 555 1112 roomnumber : 112 userpassword : MyAwesomePassword dn : ou = Groups , dc = example , dc = com objectclass : top objectclass : organizationalunit ou : Groups aci : ( targetattr = *||+ )( version 3.0 ; acl IDM Access ; allow ( all ) userdn = ldap:///uid=idm,ou=Administrators,dc=example,dc=com ;) dn : cn = Pirates , ou = Groups , dc = example , dc = com objectclass : top objectclass : groupOfUniqueNames cn : Pirates ou : Groups uniquemember : uid = cptjack , ou = People , dc = example , dc = com uniquemember : uid = djones , ou = People , dc = example , dc = com description : Arrrrr ! dn : cn = Catmins , ou = Groups , dc = example , dc = com objectclass : top objectclass : groupOfUniqueNames cn : Catmins ou : Groups uniquemember : uid = djones , ou = People , dc = example , dc = com description : Purrrr ! dn : ou = Administrators , dc = example , dc = com objectclass : top objectclass : organizationalunit ou : Administrators dn : uid = idm , ou = Administrators , dc = example , dc = com objectclass : top objectclass : person objectclass : organizationalPerson objectclass : inetOrgPerson uid : idm cn : IDM Administrator sn : IDM Administrator description : Special LDAP acccount used by the IDM to access the LDAP data . ou : Administrators userPassword : MySecretAdminPassword ds - privilege - name : unindexed - search ds - privilege - name : password - reset Basics We need some basic meta data for our server. The two most important elements being: the base dn ( Distinguished Name ), where all of our data lives build up out of dc ( domainComponent ) elements, representing a domain name, usually companyName.com the dn where our user data will live and which user owns it (the userdn assigned in the aci ) 1 2 3 4 5 6 7 8 9 10 11 dn : dc = example , dc = com objectclass : top objectclass : domain dc : example dn : ou = People , dc = example , dc = com objectclass : top objectclass : organizationalunit ou : People aci : ( targetattr = *||+ )( version 3.0 ; acl IDM Access ; allow ( all ) userdn = ldap:///uid=idm,ou=Administrators,dc=example,dc=com ;) User Entries After these two main metadata dn elements, we have all the user entries. We start with the identifier of the entry, designated by the dn , followed by attributes and classifications. While LDAP stands for Lightweight , it has a lot of attributes which are not easily understood because they are commonly used by their two-letter acronym only. Below we explain a few of the attributes, but there are more complete lists available 10 . dn : the full identifier of the data entry, in reverse tree uid : the unique id of the user within the organizational structure cn : Common Name , generally how humans would identify this resource l : Location ou : Organizational Unit , these commonly represent departments objectclass : defines a object hierarchy, derived from Object Oriented Programming top : is commonly the root object class, every other object inheriting from there object classes are used to create typed objects which have their own schema this allows you to define what values are required and which are optional object classes are also used in search filters, so you can distinguish different value types (users, groups) Organizational Tree 1 2 3 4 5 6 . \u251c\u2500\u2500 com \u2502 \u2514\u2500\u2500 example \u2502 \u251c\u2500\u2500 Operations \u2502 \u2514\u2500\u2500 People \u2502 \u2514\u2500\u2500 jwick 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 dn : uid = jwick , ou = People , dc = example , dc = com cn : John Wick sn : Wick givenname : John objectclass : top objectclass : person objectclass : organizationalPerson objectclass : inetOrgPerson ou : Operations ou : People l : New York uid : jwick mail : jwick @ example . com telephonenumber : + 1 408 555 1236 facsimiletelephonenumber : + 1 408 555 4323 roomnumber : 233 userpassword : myawesomepassword Group We first create the OU housing our Groups, including which object classes it represents. Then we can attach groups to this OU by creating dn 's under it. There are two flavors of groups, we have groupOfNames and groupOfUniqueNames , I'm sure you understand what the difference is. We explain wich kind of group we are by the objectclass : groupOfUniqueNames , and include a list of members. In the case of groupOfNames we use member : member dn , in our case we user uniquemember : uid = cptjack , ou = People , dc = example , dc = com . To make it a list, add more than one entry, each with a unique value. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 dn : ou = Groups , dc = example , dc = com objectclass : top objectclass : organizationalunit ou : Groups aci : ( targetattr = *||+ )( version 3.0 ; acl IDM Access ; allow ( all ) userdn = ldap:///uid=idm,ou=Administrators,dc=example,dc=com ;) dn : cn = Pirates , ou = Groups , dc = example , dc = com objectclass : top objectclass : groupOfUniqueNames cn : Pirates ou : Groups uniquemember : uid = cptjack , ou = People , dc = example , dc = com uniquemember : uid = will , ou = People , dc = example , dc = com uniquemember : uid = djones , ou = People , dc = example , dc = com description : Arrrrr ! Administrator We have to configure our main administrator, as we've already given it a lot of privileges to manage the other resources (via the aci rules). As usual, we start with defining a ou that houses our Administrators . In this case, we give it two more special permissions via ds - privilege - name . So this user can also reset passwords, just in case. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 dn : ou = Administrators , dc = example , dc = com objectclass : top objectclass : organizationalunit ou : Administrators dn : uid = idm , ou = Administrators , dc = example , dc = com objectclass : top objectclass : person objectclass : organizationalPerson objectclass : inetOrgPerson uid : idm cn : IDM Administrator sn : IDM Administrator description : Special LDAP acccount used by the IDM to access the LDAP data . ou : Administrators userPassword : MySecretAdminPassword ds - privilege - name : unindexed - search ds - privilege - name : password - reset Configure OpenDJ One of the reasons I like OpenDJ is because it is very easy to operate. It has many tools to help you manage the server. Some are separate tools that interact with a running server, others interact with the configure while the server is offline. We have to do two configuration actions, 1) we intialize it with our root dn , which port to use and so on, 2) we add our test data set, so we have our groups and users to work with. 1 2 3 4 5 6 7 8 9 10 11 12 /opt/opendj/setup --cli \\ -p 1389 \\ --ldapsPort 1636 \\ --enableStartTLS \\ --generateSelfSignedCertificate \\ --baseDN dc = example,dc = com \\ -h localhost \\ --rootUserDN $ROOT_USER_DN \\ --rootUserPassword $ROOT_PASSWORD \\ --acceptLicense \\ --no-prompt \\ --doNotStart 1 2 3 4 5 /opt/opendj/bin/import-ldif \\ --includeBranch dc = example,dc = com \\ --backendID userRoot \\ --offline \\ --ldifFile example.ldiff Dockerfile For completeness I will also include my Docker image. We use Tini 11 order to manage the process of OpenDJ nicely, even in the face of being shutdown. For more information, read my article on Docker Graceful Shutdown . We use a jre as we only need to run Java, so no need for a JDK. We use the OpenJDK distribution from Azul 15 in order to avoid possible violations of Oracle's License 12 . We use Azul's Alpine 13 based image as it is much smaller than those based of full-fledged OS's. This makes the image smaller in size on disk and in memory and reduces the attack vector and recommended by the likes of Docker and Snyk 14 . We download the latest version available which, as of this writing in September 2019, is 4 . 4 . 3 which you can retrieve from the OpenDJ Community's releases page on GitHub 13 . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 FROM azul/zulu-openjdk-alpine:8u222-jre LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.2.0 LABEL description = OpenDJ container WORKDIR /opt EXPOSE 1389 1636 4444 ENV CHANGE_DATE = 20190916-2100 ENV JAVA_HOME /usr/lib/jvm/zulu-8 ENV OPENDJ_JAVA_HOME /usr/lib/jvm/zulu-8 ENV VERSION = 4 .4.3 ENV ROOT_USER_DN = cn=admin ENV ROOT_PASSWORD = changeme RUN apk add --no-cache tini ENTRYPOINT [ /sbin/tini , -vv , -g , -s , -- ] CMD [ /opt/opendj/bin/start-ds , --nodetach ] RUN wget --quiet \\ https://github.com/OpenIdentityPlatform/OpenDJ/releases/download/ $VERSION /opendj- $VERSION .zip \\ unzip opendj- $VERSION .zip \\ rm -r opendj- $VERSION .zip RUN /opt/opendj/setup --cli \\ -p 1389 \\ --ldapsPort 1636 \\ --enableStartTLS \\ --generateSelfSignedCertificate \\ --baseDN dc = example,dc = com \\ -h localhost \\ --rootUserDN $ROOT_USER_DN \\ --rootUserPassword $ROOT_PASSWORD \\ --acceptLicense \\ --no-prompt \\ --doNotStart ADD Example.ldif /var/tmp/example.ldiff # RUN /opt/opendj/bin/import-ldif --help RUN /opt/opendj/bin/import-ldif --includeBranch dc = example,dc = com --backendID userRoot --offline --ldifFile /var/tmp/example.ldiff Kubernetes Deployment In production you might want to run LDAP in a StatefulSet and give it some permanent storage. But in this guide the goal of LDAP is to show how to use it with Keycloak and we stick to a Deployment as it is easier. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion : apps/v1 kind : Deployment metadata : name : opendj4 labels : app : opendj4 spec : replicas : 1 selector : matchLabels : app : opendj4 template : metadata : labels : app : opendj4 spec : containers : - name : opendj4 image : caladreas/opendj:4.4.3-1 ports : - containerPort : 1389 name : ldap resources : requests : memory : 250Mi cpu : 50m limits : memory : 500Mi cpu : 250m Kubernetes Service When we use a Deployment our container instance will have a generated name and a new ip address on every (re-)start. So we use a Service to create a stable endpoint, which means that we will now access our LDAP server via our service: ldap : // opendj4 : 389 . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Service metadata : labels : app : opendj4 name : opendj4 spec : ports : - name : http port : 389 targetPort : 1389 protocol : TCP selector : app : opendj4 Caution If you deploy the Service and Deployment in a different namespace than where you want to access them from, you will have to add the namespace to the access url. If you've configured them in namespace ldap , the access url becomes ldap : // opendj4 . ldap : 389 . Homegrown Helm chart with GitHub You can also package the above explained Docker Image + Kubernetes Yaml definition as a Helm package in a GitHub repository 16 . So if you do not want to use any of the above, feel free to use my personal Helm Repository 17 . 1 2 helm repo add joostvdg https://raw.githubusercontent.com/joostvdg/helm-repo/master/ helm repo update 1 helm install joostvdg/opendj4 --name ldap --namespace ldap The service, retrieved by kubectl get service now has a different name. 1 2 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ldap-opendj4 ClusterIP 10 .100.19.29 none 389 /TCP 105s So now we access the LDAP server via ldap : // ldap - opendj4 : 389 . Keycloak Pre-requisites Helm installed TLS certificate Using a tool as Keycloak to do SSO well, feels wrong without using TLS certificates. So I wholeheartedly recommend configuring Keycloak with a Domain name and TLS Certificate. For the TLS certificate, you use Let's Encrypt 20 with Cert Manager 19 , if you unsure how to proceed in Kubernetes read my guide on Let's Encrypt on Kubernetes . Install Via Helm 1 2 helm repo add codecentric https://codecentric.github.io/helm-charts helm repo update 1 kubectl apply -f keycloak-certificate.yaml 1 helm install --name keycloak codecentric/keycloak -f keycloak-values.yaml Note Make sure you replace the dns name keycloak . my . domain . com with your own domain. If you do not have a domain, you can use `nip.io 18 . keycloak-certificate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : keycloak.my.domain.com spec : secretName : tls-keycloak dnsNames : - keycloak.my.domain.com acme : config : - http01 : ingressClass : nginx domains : - keycloak.my.domain.com issuerRef : name : letsencrypt-prod kind : ClusterIssuer keycloak-values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 keycloak : password : notsosecret ingress : enabled : true path : / annotations : kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : true ingress.kubernetes.io/affinity : cookie hosts : - keycloak.my.domain.com tls : - hosts : - keycloak.my.domain.com secretName : tls-keycloak LDAP As User Federation Assuming you have Keycloak running now, login with the admin user, keycloak , and the password you set in the keycloak - values . yaml . In the left hand menu, you can select User Federation , this is where we can add ldap and kerberos providers. As Keycloak supports multiple sources, these will be listed by their priority (the smaller the number, the higher). The federations are consulted in the order according to their priority. Create LDAP Let's create the new LDAP provider, select the Add provider dropdown - top right - and choose ldap . You will now see a whole list of values to fill in, don't worry, many can be kept as default. And to be sure, I will list them all here. For more information, each field has a little question mark, hover over it. Info When you've filled in the Connection URL , you should test the configuration with the Test connection button. The same is true for when you've configured the fields Users DN , Bind Type , Bind DN , Bind Credential with the button Test authentication . Console Display Name : display name, should be a name that tells you what this provider is Priority : the priority of this provider, will be used for the order that the federation is accessed Import Users : wether or not to import the users, I just leave this to On as to cache the users Import Users : READ_ONLY , in this guide we assume the LDAP server is not under your control, so read only Vendor : Other Username LDAP attribute : uid RDN LDAP attribute : uid UUID LDAP attribute : entryUUID User Object Classes : inetOrgPerson , organizationalPerson Connection URL : ldap : // opendj4 : 389 Users DN : dc = example , dc = com Bind Type : simple Bind DN : uid = idm , ou = Administrators , dc = example , dc = com Bind Credential : secret - or what ever you've set it in the example . diff Custom User LDAP Filter : `` Search Scope : Subtree Validate Password Policy : OFF Use Truststore SPI : Only for ldap Connection Pooling : ON Connection Timeout : `` Read Timeout : `` Pagination : ON Sync Settings: Cache Policy : 1000 Periodic Full Sync : On Full Sync Period : 604800 Periodic Changed Users Sync : On Changed Users Sync Period : 86400 Cache Settings: Cache Policy : DEFAULT Add Group Mapping If the value is not mentioned, the default value should be fine. Some values listed here are default, but listed all the same. Name : groups Mapper Type : group - ldap - mapper LDAP Groups DN : ou = Groups , dc = example , dc = com Group Name LDAP Attribute : cn Group Object Classes : groupOfUniqueNames Membership LDAP Attribute : uniquemember User Groups Retrieve Strategy : LOAD_GROUPS_BY_MEMBER_ATTRIBUTE When you hit Save , you can synchronize the groups to Keycloak - if you don't need to, it will confirm the configuration works. Hit the Sync LDAP Groups To Keycloak button, and on top there should be a temporary banner stating how many groups were synchronized (if all categories are 0 , something is wrong). Note Everytime you change a value, you first have to save the page before you can synchronize again. Mix LDAP Users With Other Sources One of the reasons for this guide to exist, is to be able to encapsulate an LDAP over which you have no control and add additional accounts and groups. There's multiple ways forward here, you can use Identity Providers and User Federation to create more sources of user accounts. Perhaps the simplest way is to manage these extra accounts in Keycloak itself. It has its own User database and Groups database. In addition to that, it allows you to assign users created in Keycloak to be a member of a group derived from LDAP - if you've synched them. UI We can use the UI in Keycloak to manage Users and Groups. We initiate this by going to the Users view and hit Add user . We can then fill in all the details of the User. REST API Keycloak has a rich REST API with good decent documentation 21 . The thing missing is some examples for how to use them correctly. I recommend using HTTPie 28 rather than cUrl, as it is easier to use for these more complex calls. Get Token Warning The Bearer Token is only valid for a short period of time. If you wait too long, you will get 401 unauthorized . The default REALM is master . httpie 1 http --form POST ${ KEYCLOAK_URL } /auth/realms/ ${ REALM } /protocol/openid-connect/token username = keycloak password = ${ PASS } client_id = admin-cli grant_type = password curl 1 curl ${ KEYCLOAK_URL } /auth/realms/ ${ REALM } /protocol/openid-connect/token -u keycloak: ${ PASS } Get Users httpie 1 http ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users Authorization: Bearer $TOKEN httpie - get user 1 http ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users/ ${ userId } Authorization: Bearer $TOKEN curl 1 curl -v ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users -H Authorization: Bearer $TOKEN | jq Create User httpie 1 2 3 4 5 6 7 8 9 10 http POST ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users \\ Authorization: Bearer $TOKEN \\ credentials: = [{\\ value\\ : \\ mypass\\ , \\ type\\ : \\ password\\ }] \\ email = user@example.com \\ firstName = hannibal \\ lastName = lecter \\ username = hlecter \\ groups: = [ Robots ] \\ emailVerified: = true \\ enabled: = true curl 1 curl -v ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users -H Authorization: Bearer $TOKEN | jq Verify To verify everything is working as it should, you can go to the Groups and Users pages within the Manage menu (left hand side). With Groups you have View all groups , which should have the groups from LDAP and any group you have created within Keycloak - if not yet, you can do so here as well. To view details of a Group, double click the name - it's not obvious you can do so - and you will go to the details page. Here you can also see the members of the Group. You can do the same with the Users menu item. By default the page is empty, if your list of users is not too large, click on View all users . You can view the details page of a user by clicking the link of the User ID. Within the Groups tab you can add the user to more groups, this should contain all the groups known to Keycloak - both Keycloak internal and from LDAP. SSO with Jenkins To configure Jenkins to use Keycloak we have two plugins at our disposal, OpenId Connect( oic - auth ) 22 and Keycloak 23 . While the Keycloak plugin is easier to configure for authentication, I found it difficult to configure groups. As I feel the group management is mandatory we're going with the OpenId Connect plugin. You can install plugins in Jenkins via the UI 24 , via the new Jenkins CLI 25 , or via the values . yaml when installing via the Helm Chart 27 . Important Install the OpenId Connect plugin before configuring the next parts, and restart your Jenkins instance for best results. In order to configure OpenId Connect with Jenkins, it is the easiest to use the well - known endpoint url. This endpoint contains all the configuration information the plugin needs to configure itself. Usually, this is ${ KEYCLOAK_URL } /auth/realms/ ${ REALM } /.well-known/openid-configuration . From Keycloak's perspective we have to register Jenkins as a Client. Keycloak Client We go to the Clients screen in Keycloak and hit the Create button. In the next screen, we need to supply three values: Client ID : the name of your client, jenkins would be good example Client Protocol : openid - connect is recommended Root URL : the main url of your installation, for example https : // jenkins . my . domain . com Main Settings Once we hit save, we get a details view. We have to change some values here. Access Type : we need a Client ID and Client Secret, we only get this when we select confidential Valid Redirect URIs : confirm this is ${ yourJenkinsURL } /* Once you hit save, you get a new Tab in the Details screen of this Client. It is called Credentials , and here you can see the Client Secret which we will need to enter in Jenkins. Group Mappings Just as we had to add the Group mapping to the LDAP configuration, we will need to configure the Group mapping in the Client. If we go into the Client details, we see there's a Tab called Mappers . We create a new Mapping here by hitting the Create button. Give the mapping a name and then select the Mapper Type Group Membership . The Token Claim Name is important as well, we use this in our Jenkins configuration, give it a descriptive name such as group - membership . Configure Via UI In Jenkins we go to Manage Jenkins - Configure Global Security and here we select Login with Openid Connect in the Security Realm block. Client id : the client we've configured in Keycloak, if you've followed this guide, it should be jenkins Client secret : the secret of the client configured in Keycloak, if you've lost it, go back to Keycloak - Clients - jenkins - Credentials and copy the value in the field Secret . We now get a Configuration mode block where we can select either Automatic configuration on Manual configuration . We select Automatic and enter our Well - known configuration endpoint URL from Keycloak we've written down earlier. If you don't remember, the format is usually this: ${ KEYCLOAK_URL } /auth/realms/ ${ REALM } /.well-known/openid-configuration . User name field name : preferred_username Full name field name : name Email field name : email Groups field name : group - membership Note The field Groups field name refers back to the Token Claim Name we configured in Keycloak within our Client's Mapping for Group Membership. Configure Via Configuration-as-Code We can use the amazing Jenkins Configuration-as-Code 26 to make sure our SSO configuration is configured out-of-the-box! In order to avoid the having to store the Client ID and Client Secret, we're going to create these as Kubernetes Secrets first. There's more ways to create these, but this is to keep it simple. 1 2 3 4 5 kubectl create secret generic oic-auth \\ --from-literal = clientID = ${ CLIENT_ID } \\ --from-literal = clientSecret = ${ CLIENT_SECRET } \\ --from-literal = keycloakUrl = ${ keycloakUrl } \\ --namespace jenkins JCASC-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 jenkins : securityRealm : oic : clientId : ${clientID} clientSecret : ${clientSecret} wellKnownOpenIDConfigurationUrl : ${keycloakUrl}/auth/realms/master/.well-known/openid-configuration tokenServerUrl : ${keycloakUrl}auth/realms/master/protocol/openid-connect/token authorizationServerUrl : ${keycloakUrl}/auth/realms/master/protocol/openid-connect/auth userInfoServerUrl : ${keycloakUrl}/auth/realms/master/protocol/openid-connect/userinfo userNameField : preferred_username fullNameFieldName : name emailFieldName : email groupsFieldName : group-membership scopes : web-origins address phone openid offline_access profile roles microprofile-jwt email disableSslVerification : false logoutFromOpenidProvider : true endSessionUrl : ${keycloakUrl}/auth/realms/master/protocol/openid-connect/logout postLogoutRedirectUrl : escapeHatchEnabled : false escapeHatchSecret : complete-jenkins-helm-values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 master : ingress : enabled : true hostName : jenkins.my.doamain.com csrf : defaultCrumbIssuer : enabled : true proxyCompatability : true cli : false installPlugins : - kubernetes:latest - kubernetes-credentials:latest - workflow-aggregator:latest - workflow-job:latest - credentials-binding:latest - git:latest - blueocean:latest - prometheus:latest - matrix-auth:latest - keycloak:latest - oic-auth:latest JCasC : enabled : true pluginVersion : 1.30 configScripts : welcome-message : | jenkins: systemMessage: Welcome, this Jenkins is configured and managed as code. ldap-settings : | jenkins: securityRealm: oic: clientId: ${clientID} clientSecret: ${clientSecret} wellKnownOpenIDConfigurationUrl: ${keycloakUrl}/auth/realms/master/.well-known/openid-configuration tokenServerUrl: ${keycloakUrl}/auth/realms/master/protocol/openid-connect/token authorizationServerUrl: ${keycloakUrl}/auth/realms/master/protocol/openid-connect/auth userInfoServerUrl: ${keycloakUrl}/auth/realms/master/protocol/openid-connect/userinfo userNameField: preferred_username fullNameFieldName: name emailFieldName: email groupsFieldName: group-membership scopes: web-origins address phone openid offline_access profile roles microprofile-jwt email disableSslVerification: false logoutFromOpenidProvider: true endSessionUrl: ${keycloakUrl}/auth/realms/master/protocol/openid-connect/logout postLogoutRedirectUrl: escapeHatchEnabled: false escapeHatchSecret: matrix-auth : | jenkins: authorizationStrategy: globalMatrix: permissions: - Overall/Read:authenticated - Overall/Administer:barbossa - Overall/Administer:Catmins - Overall/Administer:Robots - Overall/Administer:/Catmins - Overall/Administer:/Robots persistence : enabled : true volumes : - name : oic-auth-clientid secret : secretName : oic-auth items : - key : clientID path : clientID - name : oic-auth-clientsecret secret : secretName : oic-auth items : - key : clientSecret path : clientSecret - name : oic-auth-keycloakurl secret : secretName : oic-auth items : - key : keycloakUrl path : keycloakUrl mounts : - name : oic-auth-clientid mountPath : /run/secrets/clientID subPath : clientID - name : oic-auth-clientsecret mountPath : /run/secrets/clientSecret subPath : clientSecret - name : oic-auth-keycloakurl mountPath : /run/secrets/keycloakUrl subPath : keycloakUrl Use Keycloak Groups In Jenkins Warning Unfortuantely, there is one caveat about using Keycloak as intermediary between LDAP and Jenkins. Groups do not come across the same as before, they're now prefixed with / . I'm sure it is down to a misconfiguration on my end, so please let me know how to resolve that if you figure it out. This means, that a LDAP group called Catmins will have to be used in Jenkins via / Catmins in Matrix , Project Matrix or other authorization schemes. Verify To verify, take the leap of faith to save the configuration, logout (top right) and then log back in. If everything goes well, you will be redirected to Keycloak, once successfully logged in, redirected back to Jenkins! Note One thing to remark, that if you're logged into Jenkins and you want to see the Groups, you can select your User (top right, click on your name). Alternatively, you can go to ${ JENKINS_URL } /whoAmI (notice the capital casing of the 'A' and 'I', it is required). References Apache Keycloak Home Wikipedia Definition on Single sign-on Kubernetes Home Lightweight Directory Access Protocol (LDAP) Jenkins Home SonarQube Home OpenDJ Community Edition Helm - Package Manager for Kubernetes Helm 3 Beta Common Used LDAP Attributes Explained tini - process manager Overops Article on Java License in Docker images Alpine Docker Image Snyk - 10 docker image security best practices Azul OpenJDK Alpine Image Hosting Helm Private Repository from GitHub Joost van der Griendt's Helm Repository Nip Io - Dead simple wildcard DNS for any IP Address Cert Manager Let's Encrypt - Let's Encrypt the world! Keycloak Admin REST API Jenkins OpenId Connect Plugin Jenkins Keycloak Plugin Jenkins Plugin Management Jenkins JCLI Jenkins Configuration As Code Jenkins Helm Chart HTTPie http commandline interface","title":"Kubernetes SSO Keycloak"},{"location":"blogs/kubernetes-sso-keycloak/#single-sign-on-on-kubernetes-with-keycloak","text":"This article is about setting up an Apache Keycloak 1 instance for Single Sign-On 2 (SSO) on Kubernetes. Important This guide is created to help you show how you can do this from a technical point of view. If you are in an enterprise, please consult your in-house - if available - or external security professionals before running tools such as Keycloak in production.","title":"Single Sign On on Kubernetes with Keycloak"},{"location":"blogs/kubernetes-sso-keycloak/#goal","text":"The goal is to show why using Keycloak as SSO can be valuable and guide on how to do so. For what's in the How , look at the next paragraph: Steps . At the outset, this article is not about comparing SSO solutions or about claiming Keycloak is the best. Its a solution, and if you're interested in using this, it helps you with the how. Below are some links to alternatives as reference, but are not discussed.","title":"Goal"},{"location":"blogs/kubernetes-sso-keycloak/#audience","text":"The expected audience are those who are using Kubernetes 3 and are: looking to host a SSO solution themselves want to use LDAP 4 as User Federation for Keycloak want to setup a SSO with Jenkins 5 and/or SonarQube 6","title":"Audience"},{"location":"blogs/kubernetes-sso-keycloak/#steps","text":"These are the steps we will execute. configure an LDAP server with a test data set package LDAP server in a Docker container and run it as an kubernetes Deployment you might think, why not a StatefulSet , but our LDAP will have static data and the idea of this setup, is that we do not control LDAP, meaning, we should expect LDAP's data to be immutable install Keycloak with Helm 8 we use Helm 2, as Helm 3 9 is still in beta at the time of writing (September 2019) configure Keycloak to use LDAP for User Federation install Jenkins and SonarQube with Helm 8 configure SSO with Keycloak in Jenkins configure SSO with Keycloak in SonarQube","title":"Steps"},{"location":"blogs/kubernetes-sso-keycloak/#ldap","text":"We will use OpenDJ 7 as LDAP implementation. There are many alternatives out there, feel free to use those. But for this guide, we will use OpenDJ's community edition, it works well and is easy to configure. We need the following: configured with a test data set including users and groups Docker container image definition Kubernetes Deployment definition to run Kubernetes Service definition to access a stable address","title":"LDAP"},{"location":"blogs/kubernetes-sso-keycloak/#test-data-set","text":"example.ldiff This a full example. The different parts will be explained below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 dn : dc = example , dc = com objectclass : top objectclass : domain dc : example dn : ou = People , dc = example , dc = com objectclass : top objectclass : organizationalunit ou : People aci : ( targetattr = *||+ )( version 3.0 ; acl IDM Access ; allow ( all ) userdn = ldap:///uid=idm,ou=Administrators,dc=example,dc=com ;) dn : uid = cptjack , ou = People , dc = example , dc = com cn : cpt . Jack Sparrow sn : Sparrow givenname : Jack objectclass : top objectclass : person objectclass : organizationalPerson objectclass : inetOrgPerson ou : Operations ou : People l : Caribbean uid : cptjack mail : jack @ example . com telephonenumber : + 421 910 123456 facsimiletelephonenumber : + 1 408 555 1111 roomnumber : 666 userpassword : MyAwesomePassword dn : uid = djones , ou = People , dc = example , dc = com cn : Davy Jones sn : Jones givenname : Davy objectclass : top objectclass : person objectclass : organizationalPerson objectclass : inetOrgPerson ou : Operations ou : People l : Caribbean uid : djones mail : d . jones @ example . com telephonenumber : + 421 910 382735 facsimiletelephonenumber : + 1 408 555 1112 roomnumber : 112 userpassword : MyAwesomePassword dn : ou = Groups , dc = example , dc = com objectclass : top objectclass : organizationalunit ou : Groups aci : ( targetattr = *||+ )( version 3.0 ; acl IDM Access ; allow ( all ) userdn = ldap:///uid=idm,ou=Administrators,dc=example,dc=com ;) dn : cn = Pirates , ou = Groups , dc = example , dc = com objectclass : top objectclass : groupOfUniqueNames cn : Pirates ou : Groups uniquemember : uid = cptjack , ou = People , dc = example , dc = com uniquemember : uid = djones , ou = People , dc = example , dc = com description : Arrrrr ! dn : cn = Catmins , ou = Groups , dc = example , dc = com objectclass : top objectclass : groupOfUniqueNames cn : Catmins ou : Groups uniquemember : uid = djones , ou = People , dc = example , dc = com description : Purrrr ! dn : ou = Administrators , dc = example , dc = com objectclass : top objectclass : organizationalunit ou : Administrators dn : uid = idm , ou = Administrators , dc = example , dc = com objectclass : top objectclass : person objectclass : organizationalPerson objectclass : inetOrgPerson uid : idm cn : IDM Administrator sn : IDM Administrator description : Special LDAP acccount used by the IDM to access the LDAP data . ou : Administrators userPassword : MySecretAdminPassword ds - privilege - name : unindexed - search ds - privilege - name : password - reset","title":"Test Data Set"},{"location":"blogs/kubernetes-sso-keycloak/#basics","text":"We need some basic meta data for our server. The two most important elements being: the base dn ( Distinguished Name ), where all of our data lives build up out of dc ( domainComponent ) elements, representing a domain name, usually companyName.com the dn where our user data will live and which user owns it (the userdn assigned in the aci ) 1 2 3 4 5 6 7 8 9 10 11 dn : dc = example , dc = com objectclass : top objectclass : domain dc : example dn : ou = People , dc = example , dc = com objectclass : top objectclass : organizationalunit ou : People aci : ( targetattr = *||+ )( version 3.0 ; acl IDM Access ; allow ( all ) userdn = ldap:///uid=idm,ou=Administrators,dc=example,dc=com ;)","title":"Basics"},{"location":"blogs/kubernetes-sso-keycloak/#user-entries","text":"After these two main metadata dn elements, we have all the user entries. We start with the identifier of the entry, designated by the dn , followed by attributes and classifications. While LDAP stands for Lightweight , it has a lot of attributes which are not easily understood because they are commonly used by their two-letter acronym only. Below we explain a few of the attributes, but there are more complete lists available 10 . dn : the full identifier of the data entry, in reverse tree uid : the unique id of the user within the organizational structure cn : Common Name , generally how humans would identify this resource l : Location ou : Organizational Unit , these commonly represent departments objectclass : defines a object hierarchy, derived from Object Oriented Programming top : is commonly the root object class, every other object inheriting from there object classes are used to create typed objects which have their own schema this allows you to define what values are required and which are optional object classes are also used in search filters, so you can distinguish different value types (users, groups) Organizational Tree 1 2 3 4 5 6 . \u251c\u2500\u2500 com \u2502 \u2514\u2500\u2500 example \u2502 \u251c\u2500\u2500 Operations \u2502 \u2514\u2500\u2500 People \u2502 \u2514\u2500\u2500 jwick 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 dn : uid = jwick , ou = People , dc = example , dc = com cn : John Wick sn : Wick givenname : John objectclass : top objectclass : person objectclass : organizationalPerson objectclass : inetOrgPerson ou : Operations ou : People l : New York uid : jwick mail : jwick @ example . com telephonenumber : + 1 408 555 1236 facsimiletelephonenumber : + 1 408 555 4323 roomnumber : 233 userpassword : myawesomepassword","title":"User Entries"},{"location":"blogs/kubernetes-sso-keycloak/#group","text":"We first create the OU housing our Groups, including which object classes it represents. Then we can attach groups to this OU by creating dn 's under it. There are two flavors of groups, we have groupOfNames and groupOfUniqueNames , I'm sure you understand what the difference is. We explain wich kind of group we are by the objectclass : groupOfUniqueNames , and include a list of members. In the case of groupOfNames we use member : member dn , in our case we user uniquemember : uid = cptjack , ou = People , dc = example , dc = com . To make it a list, add more than one entry, each with a unique value. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 dn : ou = Groups , dc = example , dc = com objectclass : top objectclass : organizationalunit ou : Groups aci : ( targetattr = *||+ )( version 3.0 ; acl IDM Access ; allow ( all ) userdn = ldap:///uid=idm,ou=Administrators,dc=example,dc=com ;) dn : cn = Pirates , ou = Groups , dc = example , dc = com objectclass : top objectclass : groupOfUniqueNames cn : Pirates ou : Groups uniquemember : uid = cptjack , ou = People , dc = example , dc = com uniquemember : uid = will , ou = People , dc = example , dc = com uniquemember : uid = djones , ou = People , dc = example , dc = com description : Arrrrr !","title":"Group"},{"location":"blogs/kubernetes-sso-keycloak/#administrator","text":"We have to configure our main administrator, as we've already given it a lot of privileges to manage the other resources (via the aci rules). As usual, we start with defining a ou that houses our Administrators . In this case, we give it two more special permissions via ds - privilege - name . So this user can also reset passwords, just in case. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 dn : ou = Administrators , dc = example , dc = com objectclass : top objectclass : organizationalunit ou : Administrators dn : uid = idm , ou = Administrators , dc = example , dc = com objectclass : top objectclass : person objectclass : organizationalPerson objectclass : inetOrgPerson uid : idm cn : IDM Administrator sn : IDM Administrator description : Special LDAP acccount used by the IDM to access the LDAP data . ou : Administrators userPassword : MySecretAdminPassword ds - privilege - name : unindexed - search ds - privilege - name : password - reset","title":"Administrator"},{"location":"blogs/kubernetes-sso-keycloak/#configure-opendj","text":"One of the reasons I like OpenDJ is because it is very easy to operate. It has many tools to help you manage the server. Some are separate tools that interact with a running server, others interact with the configure while the server is offline. We have to do two configuration actions, 1) we intialize it with our root dn , which port to use and so on, 2) we add our test data set, so we have our groups and users to work with. 1 2 3 4 5 6 7 8 9 10 11 12 /opt/opendj/setup --cli \\ -p 1389 \\ --ldapsPort 1636 \\ --enableStartTLS \\ --generateSelfSignedCertificate \\ --baseDN dc = example,dc = com \\ -h localhost \\ --rootUserDN $ROOT_USER_DN \\ --rootUserPassword $ROOT_PASSWORD \\ --acceptLicense \\ --no-prompt \\ --doNotStart 1 2 3 4 5 /opt/opendj/bin/import-ldif \\ --includeBranch dc = example,dc = com \\ --backendID userRoot \\ --offline \\ --ldifFile example.ldiff","title":"Configure OpenDJ"},{"location":"blogs/kubernetes-sso-keycloak/#dockerfile","text":"For completeness I will also include my Docker image. We use Tini 11 order to manage the process of OpenDJ nicely, even in the face of being shutdown. For more information, read my article on Docker Graceful Shutdown . We use a jre as we only need to run Java, so no need for a JDK. We use the OpenJDK distribution from Azul 15 in order to avoid possible violations of Oracle's License 12 . We use Azul's Alpine 13 based image as it is much smaller than those based of full-fledged OS's. This makes the image smaller in size on disk and in memory and reduces the attack vector and recommended by the likes of Docker and Snyk 14 . We download the latest version available which, as of this writing in September 2019, is 4 . 4 . 3 which you can retrieve from the OpenDJ Community's releases page on GitHub 13 . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 FROM azul/zulu-openjdk-alpine:8u222-jre LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.2.0 LABEL description = OpenDJ container WORKDIR /opt EXPOSE 1389 1636 4444 ENV CHANGE_DATE = 20190916-2100 ENV JAVA_HOME /usr/lib/jvm/zulu-8 ENV OPENDJ_JAVA_HOME /usr/lib/jvm/zulu-8 ENV VERSION = 4 .4.3 ENV ROOT_USER_DN = cn=admin ENV ROOT_PASSWORD = changeme RUN apk add --no-cache tini ENTRYPOINT [ /sbin/tini , -vv , -g , -s , -- ] CMD [ /opt/opendj/bin/start-ds , --nodetach ] RUN wget --quiet \\ https://github.com/OpenIdentityPlatform/OpenDJ/releases/download/ $VERSION /opendj- $VERSION .zip \\ unzip opendj- $VERSION .zip \\ rm -r opendj- $VERSION .zip RUN /opt/opendj/setup --cli \\ -p 1389 \\ --ldapsPort 1636 \\ --enableStartTLS \\ --generateSelfSignedCertificate \\ --baseDN dc = example,dc = com \\ -h localhost \\ --rootUserDN $ROOT_USER_DN \\ --rootUserPassword $ROOT_PASSWORD \\ --acceptLicense \\ --no-prompt \\ --doNotStart ADD Example.ldif /var/tmp/example.ldiff # RUN /opt/opendj/bin/import-ldif --help RUN /opt/opendj/bin/import-ldif --includeBranch dc = example,dc = com --backendID userRoot --offline --ldifFile /var/tmp/example.ldiff","title":"Dockerfile"},{"location":"blogs/kubernetes-sso-keycloak/#kubernetes-deployment","text":"In production you might want to run LDAP in a StatefulSet and give it some permanent storage. But in this guide the goal of LDAP is to show how to use it with Keycloak and we stick to a Deployment as it is easier. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion : apps/v1 kind : Deployment metadata : name : opendj4 labels : app : opendj4 spec : replicas : 1 selector : matchLabels : app : opendj4 template : metadata : labels : app : opendj4 spec : containers : - name : opendj4 image : caladreas/opendj:4.4.3-1 ports : - containerPort : 1389 name : ldap resources : requests : memory : 250Mi cpu : 50m limits : memory : 500Mi cpu : 250m","title":"Kubernetes Deployment"},{"location":"blogs/kubernetes-sso-keycloak/#kubernetes-service","text":"When we use a Deployment our container instance will have a generated name and a new ip address on every (re-)start. So we use a Service to create a stable endpoint, which means that we will now access our LDAP server via our service: ldap : // opendj4 : 389 . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Service metadata : labels : app : opendj4 name : opendj4 spec : ports : - name : http port : 389 targetPort : 1389 protocol : TCP selector : app : opendj4 Caution If you deploy the Service and Deployment in a different namespace than where you want to access them from, you will have to add the namespace to the access url. If you've configured them in namespace ldap , the access url becomes ldap : // opendj4 . ldap : 389 .","title":"Kubernetes Service"},{"location":"blogs/kubernetes-sso-keycloak/#homegrown-helm-chart-with-github","text":"You can also package the above explained Docker Image + Kubernetes Yaml definition as a Helm package in a GitHub repository 16 . So if you do not want to use any of the above, feel free to use my personal Helm Repository 17 . 1 2 helm repo add joostvdg https://raw.githubusercontent.com/joostvdg/helm-repo/master/ helm repo update 1 helm install joostvdg/opendj4 --name ldap --namespace ldap The service, retrieved by kubectl get service now has a different name. 1 2 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ldap-opendj4 ClusterIP 10 .100.19.29 none 389 /TCP 105s So now we access the LDAP server via ldap : // ldap - opendj4 : 389 .","title":"Homegrown Helm chart with GitHub"},{"location":"blogs/kubernetes-sso-keycloak/#keycloak","text":"","title":"Keycloak"},{"location":"blogs/kubernetes-sso-keycloak/#pre-requisites","text":"Helm installed TLS certificate Using a tool as Keycloak to do SSO well, feels wrong without using TLS certificates. So I wholeheartedly recommend configuring Keycloak with a Domain name and TLS Certificate. For the TLS certificate, you use Let's Encrypt 20 with Cert Manager 19 , if you unsure how to proceed in Kubernetes read my guide on Let's Encrypt on Kubernetes .","title":"Pre-requisites"},{"location":"blogs/kubernetes-sso-keycloak/#install-via-helm","text":"1 2 helm repo add codecentric https://codecentric.github.io/helm-charts helm repo update 1 kubectl apply -f keycloak-certificate.yaml 1 helm install --name keycloak codecentric/keycloak -f keycloak-values.yaml Note Make sure you replace the dns name keycloak . my . domain . com with your own domain. If you do not have a domain, you can use `nip.io 18 . keycloak-certificate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : keycloak.my.domain.com spec : secretName : tls-keycloak dnsNames : - keycloak.my.domain.com acme : config : - http01 : ingressClass : nginx domains : - keycloak.my.domain.com issuerRef : name : letsencrypt-prod kind : ClusterIssuer keycloak-values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 keycloak : password : notsosecret ingress : enabled : true path : / annotations : kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : true ingress.kubernetes.io/affinity : cookie hosts : - keycloak.my.domain.com tls : - hosts : - keycloak.my.domain.com secretName : tls-keycloak","title":"Install Via Helm"},{"location":"blogs/kubernetes-sso-keycloak/#ldap-as-user-federation","text":"Assuming you have Keycloak running now, login with the admin user, keycloak , and the password you set in the keycloak - values . yaml . In the left hand menu, you can select User Federation , this is where we can add ldap and kerberos providers. As Keycloak supports multiple sources, these will be listed by their priority (the smaller the number, the higher). The federations are consulted in the order according to their priority.","title":"LDAP As User Federation"},{"location":"blogs/kubernetes-sso-keycloak/#create-ldap","text":"Let's create the new LDAP provider, select the Add provider dropdown - top right - and choose ldap . You will now see a whole list of values to fill in, don't worry, many can be kept as default. And to be sure, I will list them all here. For more information, each field has a little question mark, hover over it. Info When you've filled in the Connection URL , you should test the configuration with the Test connection button. The same is true for when you've configured the fields Users DN , Bind Type , Bind DN , Bind Credential with the button Test authentication . Console Display Name : display name, should be a name that tells you what this provider is Priority : the priority of this provider, will be used for the order that the federation is accessed Import Users : wether or not to import the users, I just leave this to On as to cache the users Import Users : READ_ONLY , in this guide we assume the LDAP server is not under your control, so read only Vendor : Other Username LDAP attribute : uid RDN LDAP attribute : uid UUID LDAP attribute : entryUUID User Object Classes : inetOrgPerson , organizationalPerson Connection URL : ldap : // opendj4 : 389 Users DN : dc = example , dc = com Bind Type : simple Bind DN : uid = idm , ou = Administrators , dc = example , dc = com Bind Credential : secret - or what ever you've set it in the example . diff Custom User LDAP Filter : `` Search Scope : Subtree Validate Password Policy : OFF Use Truststore SPI : Only for ldap Connection Pooling : ON Connection Timeout : `` Read Timeout : `` Pagination : ON Sync Settings: Cache Policy : 1000 Periodic Full Sync : On Full Sync Period : 604800 Periodic Changed Users Sync : On Changed Users Sync Period : 86400 Cache Settings: Cache Policy : DEFAULT","title":"Create LDAP"},{"location":"blogs/kubernetes-sso-keycloak/#add-group-mapping","text":"If the value is not mentioned, the default value should be fine. Some values listed here are default, but listed all the same. Name : groups Mapper Type : group - ldap - mapper LDAP Groups DN : ou = Groups , dc = example , dc = com Group Name LDAP Attribute : cn Group Object Classes : groupOfUniqueNames Membership LDAP Attribute : uniquemember User Groups Retrieve Strategy : LOAD_GROUPS_BY_MEMBER_ATTRIBUTE When you hit Save , you can synchronize the groups to Keycloak - if you don't need to, it will confirm the configuration works. Hit the Sync LDAP Groups To Keycloak button, and on top there should be a temporary banner stating how many groups were synchronized (if all categories are 0 , something is wrong). Note Everytime you change a value, you first have to save the page before you can synchronize again.","title":"Add Group Mapping"},{"location":"blogs/kubernetes-sso-keycloak/#mix-ldap-users-with-other-sources","text":"One of the reasons for this guide to exist, is to be able to encapsulate an LDAP over which you have no control and add additional accounts and groups. There's multiple ways forward here, you can use Identity Providers and User Federation to create more sources of user accounts. Perhaps the simplest way is to manage these extra accounts in Keycloak itself. It has its own User database and Groups database. In addition to that, it allows you to assign users created in Keycloak to be a member of a group derived from LDAP - if you've synched them.","title":"Mix LDAP Users With Other Sources"},{"location":"blogs/kubernetes-sso-keycloak/#ui","text":"We can use the UI in Keycloak to manage Users and Groups. We initiate this by going to the Users view and hit Add user . We can then fill in all the details of the User.","title":"UI"},{"location":"blogs/kubernetes-sso-keycloak/#rest-api","text":"Keycloak has a rich REST API with good decent documentation 21 . The thing missing is some examples for how to use them correctly. I recommend using HTTPie 28 rather than cUrl, as it is easier to use for these more complex calls.","title":"REST API"},{"location":"blogs/kubernetes-sso-keycloak/#get-token","text":"Warning The Bearer Token is only valid for a short period of time. If you wait too long, you will get 401 unauthorized . The default REALM is master . httpie 1 http --form POST ${ KEYCLOAK_URL } /auth/realms/ ${ REALM } /protocol/openid-connect/token username = keycloak password = ${ PASS } client_id = admin-cli grant_type = password curl 1 curl ${ KEYCLOAK_URL } /auth/realms/ ${ REALM } /protocol/openid-connect/token -u keycloak: ${ PASS }","title":"Get Token"},{"location":"blogs/kubernetes-sso-keycloak/#get-users","text":"httpie 1 http ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users Authorization: Bearer $TOKEN httpie - get user 1 http ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users/ ${ userId } Authorization: Bearer $TOKEN curl 1 curl -v ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users -H Authorization: Bearer $TOKEN | jq","title":"Get Users"},{"location":"blogs/kubernetes-sso-keycloak/#create-user","text":"httpie 1 2 3 4 5 6 7 8 9 10 http POST ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users \\ Authorization: Bearer $TOKEN \\ credentials: = [{\\ value\\ : \\ mypass\\ , \\ type\\ : \\ password\\ }] \\ email = user@example.com \\ firstName = hannibal \\ lastName = lecter \\ username = hlecter \\ groups: = [ Robots ] \\ emailVerified: = true \\ enabled: = true curl 1 curl -v ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users -H Authorization: Bearer $TOKEN | jq","title":"Create User"},{"location":"blogs/kubernetes-sso-keycloak/#verify","text":"To verify everything is working as it should, you can go to the Groups and Users pages within the Manage menu (left hand side). With Groups you have View all groups , which should have the groups from LDAP and any group you have created within Keycloak - if not yet, you can do so here as well. To view details of a Group, double click the name - it's not obvious you can do so - and you will go to the details page. Here you can also see the members of the Group. You can do the same with the Users menu item. By default the page is empty, if your list of users is not too large, click on View all users . You can view the details page of a user by clicking the link of the User ID. Within the Groups tab you can add the user to more groups, this should contain all the groups known to Keycloak - both Keycloak internal and from LDAP.","title":"Verify"},{"location":"blogs/kubernetes-sso-keycloak/#sso-with-jenkins","text":"To configure Jenkins to use Keycloak we have two plugins at our disposal, OpenId Connect( oic - auth ) 22 and Keycloak 23 . While the Keycloak plugin is easier to configure for authentication, I found it difficult to configure groups. As I feel the group management is mandatory we're going with the OpenId Connect plugin. You can install plugins in Jenkins via the UI 24 , via the new Jenkins CLI 25 , or via the values . yaml when installing via the Helm Chart 27 . Important Install the OpenId Connect plugin before configuring the next parts, and restart your Jenkins instance for best results. In order to configure OpenId Connect with Jenkins, it is the easiest to use the well - known endpoint url. This endpoint contains all the configuration information the plugin needs to configure itself. Usually, this is ${ KEYCLOAK_URL } /auth/realms/ ${ REALM } /.well-known/openid-configuration . From Keycloak's perspective we have to register Jenkins as a Client.","title":"SSO with Jenkins"},{"location":"blogs/kubernetes-sso-keycloak/#keycloak-client","text":"We go to the Clients screen in Keycloak and hit the Create button. In the next screen, we need to supply three values: Client ID : the name of your client, jenkins would be good example Client Protocol : openid - connect is recommended Root URL : the main url of your installation, for example https : // jenkins . my . domain . com","title":"Keycloak Client"},{"location":"blogs/kubernetes-sso-keycloak/#main-settings","text":"Once we hit save, we get a details view. We have to change some values here. Access Type : we need a Client ID and Client Secret, we only get this when we select confidential Valid Redirect URIs : confirm this is ${ yourJenkinsURL } /* Once you hit save, you get a new Tab in the Details screen of this Client. It is called Credentials , and here you can see the Client Secret which we will need to enter in Jenkins.","title":"Main Settings"},{"location":"blogs/kubernetes-sso-keycloak/#group-mappings","text":"Just as we had to add the Group mapping to the LDAP configuration, we will need to configure the Group mapping in the Client. If we go into the Client details, we see there's a Tab called Mappers . We create a new Mapping here by hitting the Create button. Give the mapping a name and then select the Mapper Type Group Membership . The Token Claim Name is important as well, we use this in our Jenkins configuration, give it a descriptive name such as group - membership .","title":"Group Mappings"},{"location":"blogs/kubernetes-sso-keycloak/#configure-via-ui","text":"In Jenkins we go to Manage Jenkins - Configure Global Security and here we select Login with Openid Connect in the Security Realm block. Client id : the client we've configured in Keycloak, if you've followed this guide, it should be jenkins Client secret : the secret of the client configured in Keycloak, if you've lost it, go back to Keycloak - Clients - jenkins - Credentials and copy the value in the field Secret . We now get a Configuration mode block where we can select either Automatic configuration on Manual configuration . We select Automatic and enter our Well - known configuration endpoint URL from Keycloak we've written down earlier. If you don't remember, the format is usually this: ${ KEYCLOAK_URL } /auth/realms/ ${ REALM } /.well-known/openid-configuration . User name field name : preferred_username Full name field name : name Email field name : email Groups field name : group - membership Note The field Groups field name refers back to the Token Claim Name we configured in Keycloak within our Client's Mapping for Group Membership.","title":"Configure Via UI"},{"location":"blogs/kubernetes-sso-keycloak/#configure-via-configuration-as-code","text":"We can use the amazing Jenkins Configuration-as-Code 26 to make sure our SSO configuration is configured out-of-the-box! In order to avoid the having to store the Client ID and Client Secret, we're going to create these as Kubernetes Secrets first. There's more ways to create these, but this is to keep it simple. 1 2 3 4 5 kubectl create secret generic oic-auth \\ --from-literal = clientID = ${ CLIENT_ID } \\ --from-literal = clientSecret = ${ CLIENT_SECRET } \\ --from-literal = keycloakUrl = ${ keycloakUrl } \\ --namespace jenkins JCASC-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 jenkins : securityRealm : oic : clientId : ${clientID} clientSecret : ${clientSecret} wellKnownOpenIDConfigurationUrl : ${keycloakUrl}/auth/realms/master/.well-known/openid-configuration tokenServerUrl : ${keycloakUrl}auth/realms/master/protocol/openid-connect/token authorizationServerUrl : ${keycloakUrl}/auth/realms/master/protocol/openid-connect/auth userInfoServerUrl : ${keycloakUrl}/auth/realms/master/protocol/openid-connect/userinfo userNameField : preferred_username fullNameFieldName : name emailFieldName : email groupsFieldName : group-membership scopes : web-origins address phone openid offline_access profile roles microprofile-jwt email disableSslVerification : false logoutFromOpenidProvider : true endSessionUrl : ${keycloakUrl}/auth/realms/master/protocol/openid-connect/logout postLogoutRedirectUrl : escapeHatchEnabled : false escapeHatchSecret : complete-jenkins-helm-values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 master : ingress : enabled : true hostName : jenkins.my.doamain.com csrf : defaultCrumbIssuer : enabled : true proxyCompatability : true cli : false installPlugins : - kubernetes:latest - kubernetes-credentials:latest - workflow-aggregator:latest - workflow-job:latest - credentials-binding:latest - git:latest - blueocean:latest - prometheus:latest - matrix-auth:latest - keycloak:latest - oic-auth:latest JCasC : enabled : true pluginVersion : 1.30 configScripts : welcome-message : | jenkins: systemMessage: Welcome, this Jenkins is configured and managed as code. ldap-settings : | jenkins: securityRealm: oic: clientId: ${clientID} clientSecret: ${clientSecret} wellKnownOpenIDConfigurationUrl: ${keycloakUrl}/auth/realms/master/.well-known/openid-configuration tokenServerUrl: ${keycloakUrl}/auth/realms/master/protocol/openid-connect/token authorizationServerUrl: ${keycloakUrl}/auth/realms/master/protocol/openid-connect/auth userInfoServerUrl: ${keycloakUrl}/auth/realms/master/protocol/openid-connect/userinfo userNameField: preferred_username fullNameFieldName: name emailFieldName: email groupsFieldName: group-membership scopes: web-origins address phone openid offline_access profile roles microprofile-jwt email disableSslVerification: false logoutFromOpenidProvider: true endSessionUrl: ${keycloakUrl}/auth/realms/master/protocol/openid-connect/logout postLogoutRedirectUrl: escapeHatchEnabled: false escapeHatchSecret: matrix-auth : | jenkins: authorizationStrategy: globalMatrix: permissions: - Overall/Read:authenticated - Overall/Administer:barbossa - Overall/Administer:Catmins - Overall/Administer:Robots - Overall/Administer:/Catmins - Overall/Administer:/Robots persistence : enabled : true volumes : - name : oic-auth-clientid secret : secretName : oic-auth items : - key : clientID path : clientID - name : oic-auth-clientsecret secret : secretName : oic-auth items : - key : clientSecret path : clientSecret - name : oic-auth-keycloakurl secret : secretName : oic-auth items : - key : keycloakUrl path : keycloakUrl mounts : - name : oic-auth-clientid mountPath : /run/secrets/clientID subPath : clientID - name : oic-auth-clientsecret mountPath : /run/secrets/clientSecret subPath : clientSecret - name : oic-auth-keycloakurl mountPath : /run/secrets/keycloakUrl subPath : keycloakUrl","title":"Configure Via Configuration-as-Code"},{"location":"blogs/kubernetes-sso-keycloak/#use-keycloak-groups-in-jenkins","text":"Warning Unfortuantely, there is one caveat about using Keycloak as intermediary between LDAP and Jenkins. Groups do not come across the same as before, they're now prefixed with / . I'm sure it is down to a misconfiguration on my end, so please let me know how to resolve that if you figure it out. This means, that a LDAP group called Catmins will have to be used in Jenkins via / Catmins in Matrix , Project Matrix or other authorization schemes.","title":"Use Keycloak Groups In Jenkins"},{"location":"blogs/kubernetes-sso-keycloak/#verify_1","text":"To verify, take the leap of faith to save the configuration, logout (top right) and then log back in. If everything goes well, you will be redirected to Keycloak, once successfully logged in, redirected back to Jenkins! Note One thing to remark, that if you're logged into Jenkins and you want to see the Groups, you can select your User (top right, click on your name). Alternatively, you can go to ${ JENKINS_URL } /whoAmI (notice the capital casing of the 'A' and 'I', it is required).","title":"Verify"},{"location":"blogs/kubernetes-sso-keycloak/#references","text":"Apache Keycloak Home Wikipedia Definition on Single sign-on Kubernetes Home Lightweight Directory Access Protocol (LDAP) Jenkins Home SonarQube Home OpenDJ Community Edition Helm - Package Manager for Kubernetes Helm 3 Beta Common Used LDAP Attributes Explained tini - process manager Overops Article on Java License in Docker images Alpine Docker Image Snyk - 10 docker image security best practices Azul OpenJDK Alpine Image Hosting Helm Private Repository from GitHub Joost van der Griendt's Helm Repository Nip Io - Dead simple wildcard DNS for any IP Address Cert Manager Let's Encrypt - Let's Encrypt the world! Keycloak Admin REST API Jenkins OpenId Connect Plugin Jenkins Keycloak Plugin Jenkins Plugin Management Jenkins JCLI Jenkins Configuration As Code Jenkins Helm Chart HTTPie http commandline interface","title":"References"},{"location":"blogs/monitor-jenkins-on-k8s/","text":"Monitor Jenkins on Kubernetes Additional Metrics metrics-diskusage disk-usage Next Steps Replace Node Builds make them a single metric, and calculate builds per label","title":"Monitor Jenkins on Kubernetes"},{"location":"blogs/monitor-jenkins-on-k8s/#monitor-jenkins-on-kubernetes","text":"","title":"Monitor Jenkins on Kubernetes"},{"location":"blogs/monitor-jenkins-on-k8s/#additional-metrics","text":"metrics-diskusage disk-usage","title":"Additional Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/#next-steps","text":"","title":"Next Steps"},{"location":"blogs/monitor-jenkins-on-k8s/#replace-node-builds","text":"make them a single metric, and calculate builds per label","title":"Replace Node Builds"},{"location":"blogs/sso-azure-ad/","text":"Azure AD CloudBees Core In this article we're going to configure Azure AD as Single-Sign-Solution for CloudBees Core. The configuration rests on three points; 1) Azure AD, 2) Jenkins' SAML plugin, and 3) CloudBees Core's Role Base Access Control or RBAC for short. Important Currently there's a discrepency between the TimeToLive (TTL) of the RefreshToken in Azure AD and the default configuration in the SAML plugin. This creates the problem that you can be logged in, in Azure, but when going to Jenkins you get the Logged Out page. The only way to log back in, is to logout in Azure and back in. The reason is as follows: The max lifetime of the Access Token in Azure AD seems to be 24 hours where the refresh token can live for a maximum of 14 days (if the access token expires the refresh token is used to try to obtain a new access token). The Jenkins setting in Configure Global Security SAML Identity Provider Settings Maximum Authentication Lifetime is 24 hours (86400 in seconds) upping this to 1209600 (which is 14 days in seconds/the max lifetime of the Refresh Token). The recommended resolution is to set Maximum Authentication Lifetime to the same value of your RefreshToken TTL. If you haven't changed this in Azure AD, it is 1209600 . Manage Jenkins - Configure Global Security SAML Identity Provider Settings Maximum Authentication Lifetime = 1209600 Prerequisites Before we start, there are some requirements. a running CloudBees Core Operations Center instance this instance is accessible via https . if you do not have a valid certificate and you're running on Kubernetes take a look at my cert-manager guide Let's Encrypt can now also work with nip.io addresses active Azure subscription have an Azure subscription Administrator on hand Configure Azure Warning We use https : // cloudbees - core . example . com as the example URL for CloudBees Core. Please replace the domain to your actual domain in all the examples! Steps to execute We do the following steps on the Azure side. create the Azure Active Directory create users and groups create App Registration URL: https : // cloudbees - core . example . com / cjoc / securityRealm / finishLogin replace example . com with your domain, https is required! update manifest (for groups) change: groupMembershipClaims : null , (usually line 11) to: groupMembershipClaims : SecurityGroup , create SP ID / App ID URI grant admin consent Info If you use the Azure AD plugin, you also create a client secret. Information To Note Down The following information is unique to your installation, so you need to record them as you go along. App ID URI Object ID 's of Users and Groups you want to give rights Federation Metadata Document Endpoint Azure AD - App Registrations - - Endpoints (circular icon on top) you can use either the URL or the document contents make sure the URL contains the Tenant ID of your Azure Active Directory URL example: https://login.microsoftonline.com/ ${ TENANT_ID } /federationmetadata/2007-06/federationmetadata.xml You can find your Tenant ID in Azure Active Directory - Properties - Directory ID (different name, same ID) Visual Guide Below is a visual guide with screenshots. Pay attention to these hints in the screenshots. red : this is the main thing orange : this is how we got to the current page/view blue : while you're in this screen, there might be other things you could do Create New App Registration If you have an Azure account, you have an Azure Active Directory (Azure AD) pre-created. This guide assumes you have an Azure AD ready to use. That means the next step is to create an Application Registration. Give the registration a useful name, select who can authenticate and the redirect URL . This URL needs to be configured and MUST be the actual URL of your CloudBees Core installation. Important To have Client Masters as a fallback for login, incase Operations Center is down, you have to add another redirect URL for each Master. Azure AD - App Registrations - - Authentication - Web - https : // example . com / teams - cat / securityRealm / finishLogin App Registration Data To Write Down Depending on the plugin you're going to use (SAML or Azure AD) you need information from your Azure AD / App Registration. Tentant ID Object ID Client ID Federation Metadata Document you can use the document XML content or the URL Click on the Endpoints button to open the side-bar with the links. App ID We need the App ID - even if the SAML plugin doesn't mention it. Azure generates an APP ID URI for you. You can also use CloudBees Core's URL as the URI. The URI is shown when there is an error, so it recommended to use a value you can recognize. Info App ID must match in both Azure AD (set as App ID URI ) and the SAML plugin (set as Entity ID ) configuration in Jenkins. So write it down. API Permissions Of course, things wouldn't be proper IAM/LDAP/AD without giving some permissions. If we want to retrieve group information and other fields, we need to be able to read the Directory information. You find the Directory information via the Microsoft Graph api button. We select Application Permissions and then check Directory . Read . All . We don't need to write. The Permissions have changed, so we require an Administrator account to consent with the new permissions. Update Manifest As with the permissions, the default Manifest doesn't give us all the information we want. We want the groups so we can configure RBAC, and thus we have to set the groupMembershipsClaims claim attribute. We change the null to SecurityGroup . Please consult the Microsoft docs (see reference below) for other options. We can download, edit, and upload the manifest file. Alternatively, we can edit inline and hit save on top. Retrieve Group Object ID If we want to assign Azure AD groups to groups or roles in Jenkins' RBAC, we need to use the Object ID 's. Each Group and User has an Object ID , which have a handy Copy this button on the end of the value box! Configure Jenkins We now get to the point where we configure Jenkins. The SAML plugin is opensource, and thus it's configuration can also be used for Jenkins or CloudBees Jenkins Distribution . Steps Here are the steps we perform to configure Azure AD as the Single-Sign-On solution. install the SAML plugin I assume you know how to install plugins, so we skip this if you don't know Read the Managing Plugins Guide configure saml 2.0 in Jenkins setup groups (RBAC) administrators - admin group browsers - all other groups Visual Guide Below is a visual guide with screenshots. Please pay attention to the hints in the screenshots. Red : this is the main thing Orange : this is how we got to the current page/view Blue : while you're in this screen, there might be other things you could do Configure Security To go to Jenkins' security configuration, follow this route: login with an Admin user go to the Operations Center Manage Jenkins - Global Security Configuration Configure RBAC The SAML plugin configuration pollutes the screen with fields. My advice is to enable RBAC first. If you haven't got any groups/roles yet, I recommend using the Typical Initial Setup from the dropdown. The logged-in user is automatically registered as administrator. So if your Azure AD configuration doesn't work, this user can still manage Jenkins. Important Make sure you know the credentials of the current admin user. It will automatically be added to the Administrators group, and it will be your go-to account when you mess up the SAML configuration and you have to reset security. For how to reset the security configuration, see the For When You Mess Up paragraph. Configure SAML Select SAML 2 . 0 from the Security Realm options. Here we first supply our Federation Metadata Document content or it's URL. Each option - document content or URL - has its own Validate ... button, hit it and confirm it says Success . Info You can leave Displayname empty, which gives you the default naming scheme from Azure AD. I think this is ugly, as it amounts to something like ${ EMAIL_ADDRESS } _ ${ AD_DOMAIN } _ ${ AZURE_CORP_DOMAIN } . There are other options, I've settled for givenname , as there isn't a fullname by default, and well, I prefer Joost to a long hard to recognize string. Fields Displayname : http : // schemas . xmlsoap . org / ws / 2005 / 05 / identity / claims / givenname Group : http : // schemas . microsoft . com / ws / 2008 / 06 / identity / claims / groups Username : http : // schemas . xmlsoap . org / ws / 2005 / 05 / identity / claims / name Email : http : // schemas . xmlsoap . org / ws / 2005 / 05 / identity / claims / emailaddress SP Entity ID : the App ID URI you configured in Azure AD (hidden behind Advanced Configuration ) Configure RBAC Groups Tip Once Azure AD is configured, and it works, you can configure groups for RBAC just as you're used to. Both for classic RBAC and Team Masters. Just make sure you use the Azure AD Object ID 's of the groups to map them. Bonus tip, add every Azure AD group to Browsers , so you can directly map their groups to Team Master roles without problems. XML Config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 useSecurity true /useSecurity authorizationStrategy class= nectar.plugins.rbac.strategy.RoleMatrixAuthorizationStrategyImpl / securityRealm class= org.jenkinsci.plugins.saml.SamlSecurityRealm plugin= saml@1.1.2 displayNameAttributeName http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname /displayNameAttributeName groupsAttributeName http://schemas.microsoft.com/ws/2008/06/identity/claims/groups /groupsAttributeName maximumAuthenticationLifetime 86400 /maximumAuthenticationLifetime emailAttributeName http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress /emailAttributeName usernameCaseConversion none /usernameCaseConversion usernameAttributeName http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name /usernameAttributeName binding urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect /binding advancedConfiguration forceAuthn false /forceAuthn spEntityId https://cloudbees-core.kearos.net /spEntityId /advancedConfiguration idpMetadataConfiguration xml /xml url https://login.microsoftonline.com/95b46e09-0307-488b-a6fc-1d2717ba9c49/federationmetadata/2007-06/federationmetadata.xml /url period 5 /period /idpMetadataConfiguration /securityRealm disableRememberMe false /disableRememberMe Logout URL Depending on the requirements, you may want to specify a logout url in the SAML configuration to log you completely out of SAML, not just Core. An example https : // login . windows . net / tenant_id_of_your_app / oauth2 / logout ? post_logout_redirect_uri = logout_URL_of_your_app / logout For When You Mess Up This is the default config for security in CloudBees Core. This file is in ${ JENKINS_HOME } /config.xml , the XML tags we want to look at are quite near the top. 1 2 3 4 5 6 7 8 9 useSecurity true /useSecurity authorizationStrategy class= hudson.security.FullControlOnceLoggedInAuthorizationStrategy denyAnonymousReadAccess true /denyAnonymousReadAccess /authorizationStrategy securityRealm class= hudson.security.HudsonPrivateSecurityRealm disableSignup true /disableSignup enableCaptcha false /enableCaptcha /securityRealm disableRememberMe false /disableRememberMe On CloudBees Core Modern / Kubernetes To rectify a failed configuration, execute the following steps: exec into the cjoc - 0 container: kubectl exec - ti cjoc - 0 -- bash open config . xml : vi / var / jenkins_home / config . xml replace conflicting lines with the above snippet save the changes exit the container: exit kill the pod: kubectl delete po cjoc - 0 Tip For removing a whole line, stay in \"normal\" mode, and press d d (two times the d key). To add the new lines, go into insert mode by pressing the i key. Go back to \"normal\" mode by pressing the esc key. Then, save and quit, by writing: : wq followed by enter . References CloudBees Guide on Azure AD for Core SSO (outdated) SAML Plugin Docs for Azure AD (outdated) Microsoft Doc for Azure AD Tokens Microsoft Doc for Azure AD Optional Tokens Microsoft Doc for Azure AD Custom Tokens Alternative Azure AD Plugin (very new) Info Currently, there is a limitation which requires you to use the Object ID 's which make searching groups and people less than ideal. When the Alternative Azure AD Plugin is approved this may provide a more satisfactory solution.","title":"CloudBees SSO Azure AD"},{"location":"blogs/sso-azure-ad/#azure-ad-cloudbees-core","text":"In this article we're going to configure Azure AD as Single-Sign-Solution for CloudBees Core. The configuration rests on three points; 1) Azure AD, 2) Jenkins' SAML plugin, and 3) CloudBees Core's Role Base Access Control or RBAC for short. Important Currently there's a discrepency between the TimeToLive (TTL) of the RefreshToken in Azure AD and the default configuration in the SAML plugin. This creates the problem that you can be logged in, in Azure, but when going to Jenkins you get the Logged Out page. The only way to log back in, is to logout in Azure and back in. The reason is as follows: The max lifetime of the Access Token in Azure AD seems to be 24 hours where the refresh token can live for a maximum of 14 days (if the access token expires the refresh token is used to try to obtain a new access token). The Jenkins setting in Configure Global Security SAML Identity Provider Settings Maximum Authentication Lifetime is 24 hours (86400 in seconds) upping this to 1209600 (which is 14 days in seconds/the max lifetime of the Refresh Token). The recommended resolution is to set Maximum Authentication Lifetime to the same value of your RefreshToken TTL. If you haven't changed this in Azure AD, it is 1209600 . Manage Jenkins - Configure Global Security SAML Identity Provider Settings Maximum Authentication Lifetime = 1209600","title":"Azure AD &amp; CloudBees Core"},{"location":"blogs/sso-azure-ad/#prerequisites","text":"Before we start, there are some requirements. a running CloudBees Core Operations Center instance this instance is accessible via https . if you do not have a valid certificate and you're running on Kubernetes take a look at my cert-manager guide Let's Encrypt can now also work with nip.io addresses active Azure subscription have an Azure subscription Administrator on hand","title":"Prerequisites"},{"location":"blogs/sso-azure-ad/#configure-azure","text":"Warning We use https : // cloudbees - core . example . com as the example URL for CloudBees Core. Please replace the domain to your actual domain in all the examples!","title":"Configure Azure"},{"location":"blogs/sso-azure-ad/#steps-to-execute","text":"We do the following steps on the Azure side. create the Azure Active Directory create users and groups create App Registration URL: https : // cloudbees - core . example . com / cjoc / securityRealm / finishLogin replace example . com with your domain, https is required! update manifest (for groups) change: groupMembershipClaims : null , (usually line 11) to: groupMembershipClaims : SecurityGroup , create SP ID / App ID URI grant admin consent Info If you use the Azure AD plugin, you also create a client secret.","title":"Steps to execute"},{"location":"blogs/sso-azure-ad/#information-to-note-down","text":"The following information is unique to your installation, so you need to record them as you go along. App ID URI Object ID 's of Users and Groups you want to give rights Federation Metadata Document Endpoint Azure AD - App Registrations - - Endpoints (circular icon on top) you can use either the URL or the document contents make sure the URL contains the Tenant ID of your Azure Active Directory URL example: https://login.microsoftonline.com/ ${ TENANT_ID } /federationmetadata/2007-06/federationmetadata.xml You can find your Tenant ID in Azure Active Directory - Properties - Directory ID (different name, same ID)","title":"Information To Note Down"},{"location":"blogs/sso-azure-ad/#visual-guide","text":"Below is a visual guide with screenshots. Pay attention to these hints in the screenshots. red : this is the main thing orange : this is how we got to the current page/view blue : while you're in this screen, there might be other things you could do","title":"Visual Guide"},{"location":"blogs/sso-azure-ad/#create-new-app-registration","text":"If you have an Azure account, you have an Azure Active Directory (Azure AD) pre-created. This guide assumes you have an Azure AD ready to use. That means the next step is to create an Application Registration. Give the registration a useful name, select who can authenticate and the redirect URL . This URL needs to be configured and MUST be the actual URL of your CloudBees Core installation. Important To have Client Masters as a fallback for login, incase Operations Center is down, you have to add another redirect URL for each Master. Azure AD - App Registrations - - Authentication - Web - https : // example . com / teams - cat / securityRealm / finishLogin","title":"Create New App Registration"},{"location":"blogs/sso-azure-ad/#app-registration-data-to-write-down","text":"Depending on the plugin you're going to use (SAML or Azure AD) you need information from your Azure AD / App Registration. Tentant ID Object ID Client ID Federation Metadata Document you can use the document XML content or the URL Click on the Endpoints button to open the side-bar with the links.","title":"App Registration Data To Write Down"},{"location":"blogs/sso-azure-ad/#app-id","text":"We need the App ID - even if the SAML plugin doesn't mention it. Azure generates an APP ID URI for you. You can also use CloudBees Core's URL as the URI. The URI is shown when there is an error, so it recommended to use a value you can recognize. Info App ID must match in both Azure AD (set as App ID URI ) and the SAML plugin (set as Entity ID ) configuration in Jenkins. So write it down.","title":"App ID"},{"location":"blogs/sso-azure-ad/#api-permissions","text":"Of course, things wouldn't be proper IAM/LDAP/AD without giving some permissions. If we want to retrieve group information and other fields, we need to be able to read the Directory information. You find the Directory information via the Microsoft Graph api button. We select Application Permissions and then check Directory . Read . All . We don't need to write. The Permissions have changed, so we require an Administrator account to consent with the new permissions.","title":"API Permissions"},{"location":"blogs/sso-azure-ad/#update-manifest","text":"As with the permissions, the default Manifest doesn't give us all the information we want. We want the groups so we can configure RBAC, and thus we have to set the groupMembershipsClaims claim attribute. We change the null to SecurityGroup . Please consult the Microsoft docs (see reference below) for other options. We can download, edit, and upload the manifest file. Alternatively, we can edit inline and hit save on top.","title":"Update Manifest"},{"location":"blogs/sso-azure-ad/#retrieve-group-object-id","text":"If we want to assign Azure AD groups to groups or roles in Jenkins' RBAC, we need to use the Object ID 's. Each Group and User has an Object ID , which have a handy Copy this button on the end of the value box!","title":"Retrieve Group Object ID"},{"location":"blogs/sso-azure-ad/#configure-jenkins","text":"We now get to the point where we configure Jenkins. The SAML plugin is opensource, and thus it's configuration can also be used for Jenkins or CloudBees Jenkins Distribution .","title":"Configure Jenkins"},{"location":"blogs/sso-azure-ad/#steps","text":"Here are the steps we perform to configure Azure AD as the Single-Sign-On solution. install the SAML plugin I assume you know how to install plugins, so we skip this if you don't know Read the Managing Plugins Guide configure saml 2.0 in Jenkins setup groups (RBAC) administrators - admin group browsers - all other groups","title":"Steps"},{"location":"blogs/sso-azure-ad/#visual-guide_1","text":"Below is a visual guide with screenshots. Please pay attention to the hints in the screenshots. Red : this is the main thing Orange : this is how we got to the current page/view Blue : while you're in this screen, there might be other things you could do","title":"Visual Guide"},{"location":"blogs/sso-azure-ad/#configure-security","text":"To go to Jenkins' security configuration, follow this route: login with an Admin user go to the Operations Center Manage Jenkins - Global Security Configuration","title":"Configure Security"},{"location":"blogs/sso-azure-ad/#configure-rbac","text":"The SAML plugin configuration pollutes the screen with fields. My advice is to enable RBAC first. If you haven't got any groups/roles yet, I recommend using the Typical Initial Setup from the dropdown. The logged-in user is automatically registered as administrator. So if your Azure AD configuration doesn't work, this user can still manage Jenkins. Important Make sure you know the credentials of the current admin user. It will automatically be added to the Administrators group, and it will be your go-to account when you mess up the SAML configuration and you have to reset security. For how to reset the security configuration, see the For When You Mess Up paragraph.","title":"Configure RBAC"},{"location":"blogs/sso-azure-ad/#configure-saml","text":"Select SAML 2 . 0 from the Security Realm options. Here we first supply our Federation Metadata Document content or it's URL. Each option - document content or URL - has its own Validate ... button, hit it and confirm it says Success . Info You can leave Displayname empty, which gives you the default naming scheme from Azure AD. I think this is ugly, as it amounts to something like ${ EMAIL_ADDRESS } _ ${ AD_DOMAIN } _ ${ AZURE_CORP_DOMAIN } . There are other options, I've settled for givenname , as there isn't a fullname by default, and well, I prefer Joost to a long hard to recognize string.","title":"Configure SAML"},{"location":"blogs/sso-azure-ad/#fields","text":"Displayname : http : // schemas . xmlsoap . org / ws / 2005 / 05 / identity / claims / givenname Group : http : // schemas . microsoft . com / ws / 2008 / 06 / identity / claims / groups Username : http : // schemas . xmlsoap . org / ws / 2005 / 05 / identity / claims / name Email : http : // schemas . xmlsoap . org / ws / 2005 / 05 / identity / claims / emailaddress SP Entity ID : the App ID URI you configured in Azure AD (hidden behind Advanced Configuration )","title":"Fields"},{"location":"blogs/sso-azure-ad/#configure-rbac-groups","text":"Tip Once Azure AD is configured, and it works, you can configure groups for RBAC just as you're used to. Both for classic RBAC and Team Masters. Just make sure you use the Azure AD Object ID 's of the groups to map them. Bonus tip, add every Azure AD group to Browsers , so you can directly map their groups to Team Master roles without problems.","title":"Configure RBAC Groups"},{"location":"blogs/sso-azure-ad/#xml-config","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 useSecurity true /useSecurity authorizationStrategy class= nectar.plugins.rbac.strategy.RoleMatrixAuthorizationStrategyImpl / securityRealm class= org.jenkinsci.plugins.saml.SamlSecurityRealm plugin= saml@1.1.2 displayNameAttributeName http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname /displayNameAttributeName groupsAttributeName http://schemas.microsoft.com/ws/2008/06/identity/claims/groups /groupsAttributeName maximumAuthenticationLifetime 86400 /maximumAuthenticationLifetime emailAttributeName http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress /emailAttributeName usernameCaseConversion none /usernameCaseConversion usernameAttributeName http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name /usernameAttributeName binding urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect /binding advancedConfiguration forceAuthn false /forceAuthn spEntityId https://cloudbees-core.kearos.net /spEntityId /advancedConfiguration idpMetadataConfiguration xml /xml url https://login.microsoftonline.com/95b46e09-0307-488b-a6fc-1d2717ba9c49/federationmetadata/2007-06/federationmetadata.xml /url period 5 /period /idpMetadataConfiguration /securityRealm disableRememberMe false /disableRememberMe","title":"XML Config"},{"location":"blogs/sso-azure-ad/#logout-url","text":"Depending on the requirements, you may want to specify a logout url in the SAML configuration to log you completely out of SAML, not just Core. An example https : // login . windows . net / tenant_id_of_your_app / oauth2 / logout ? post_logout_redirect_uri = logout_URL_of_your_app / logout","title":"Logout URL"},{"location":"blogs/sso-azure-ad/#for-when-you-mess-up","text":"This is the default config for security in CloudBees Core. This file is in ${ JENKINS_HOME } /config.xml , the XML tags we want to look at are quite near the top. 1 2 3 4 5 6 7 8 9 useSecurity true /useSecurity authorizationStrategy class= hudson.security.FullControlOnceLoggedInAuthorizationStrategy denyAnonymousReadAccess true /denyAnonymousReadAccess /authorizationStrategy securityRealm class= hudson.security.HudsonPrivateSecurityRealm disableSignup true /disableSignup enableCaptcha false /enableCaptcha /securityRealm disableRememberMe false /disableRememberMe","title":"For When You Mess Up"},{"location":"blogs/sso-azure-ad/#on-cloudbees-core-modern-kubernetes","text":"To rectify a failed configuration, execute the following steps: exec into the cjoc - 0 container: kubectl exec - ti cjoc - 0 -- bash open config . xml : vi / var / jenkins_home / config . xml replace conflicting lines with the above snippet save the changes exit the container: exit kill the pod: kubectl delete po cjoc - 0 Tip For removing a whole line, stay in \"normal\" mode, and press d d (two times the d key). To add the new lines, go into insert mode by pressing the i key. Go back to \"normal\" mode by pressing the esc key. Then, save and quit, by writing: : wq followed by enter .","title":"On CloudBees Core Modern / Kubernetes"},{"location":"blogs/sso-azure-ad/#references","text":"CloudBees Guide on Azure AD for Core SSO (outdated) SAML Plugin Docs for Azure AD (outdated) Microsoft Doc for Azure AD Tokens Microsoft Doc for Azure AD Optional Tokens Microsoft Doc for Azure AD Custom Tokens Alternative Azure AD Plugin (very new) Info Currently, there is a limitation which requires you to use the Object ID 's which make searching groups and people less than ideal. When the Alternative Azure AD Plugin is approved this may provide a more satisfactory solution.","title":"References"},{"location":"blogs/teams-automation/","text":"Core Modern Teams Automation CloudBees Core on Modern (meaning, on Kubernetes) has two main types of Jenkins Masters, a Managed Master, and a Team Master. In this article, we're going to automate the creation and management of the Team Masters. Hint If you do not want to read any of the code here, or just want to take a look at the end result - pipeline wise - you can find working examples on GitHub. Template Repository - creates a new team template and a PR to the GitOps repository GitOps Repository - applies GitOps principles to manage the CloudBees Core Team Masters Goals Just automating the creation of a Team Master is relatively easy, as this can be done via the Client Jar . So we're going to set some additional goals to create a decent challenge. GitOps : I want to be able to create and delete Team Masters by managing configuration in a Git repository Configuration-as-Code : as much of the configuration as possible should be stored in the Git repository Namespace : one of the major reasons for Team Masters to exist is to increase (Product) Team autonomy, which in Kubernetes should correspond to a namespace . So I want each Team Master to be in its own Namespace! Self-Service : the solution should cater to (semi-)autonomous teams and lower the workload of the team managing CloudBees Core. So requesting a Team Master should be doable by everyone Before We Start Some assumptions need to be taken care off before we start. Kubernetes cluster in which you are ClusterAdmin if you don't have one yet, there are guides on this elsewhere on the site your cluster has enough capacity (at least two nodes of 4gb memory) your cluster has CloudBees Core Modern installed if you don't have this yet look at one of the guides on this site or look at the guides on CloudBees.com have administrator access to CloudBees Core Cloud Operations Center Code Examples The more extensive Code Examples, such as Kubernetes Yaml files, are collapsed by default. You can open them by clicking on them. On the right, the code snippet will have a [ ] copy icon. Below is an example. Code Snippet Example Here's a code snippet. 1 2 3 4 5 6 7 8 9 10 pipeline { agent any stages { stage ( Hello ) { steps { echo Hello World! } } } } Bootstrapping All right, so we want to use GitOps and to process the changes we need a Pipeline that can be triggered by a Webhook. I believe in everything as code - except Secrets and such - which includes the Pipeline. Unfortunately, Operations Center cannot run such pipelines. To get over this hurdle, we will create a special Ops Team Master. This Master will be configured to be able to Manage the other Team Masters for us. Log into your Operations Center with a user that has administrative access. Create API Token Create a new API Token for your administrator user by clicking on the user's name - top right corner. Select the Configuration menu on the left and then you should see a section where you can Create a API Token . This Token will disappear, so write it down. Get Configure Client Jar Replace the values marked by ... . The Operations Center URL should look like this: http : // cbcore . mydomain . com / cjoc . Setup the connection variables. 1 OC_URL = your operations center url 1 2 USR = your username TKN = api token Download the Client Jar. 1 curl ${ OC_URL } /jnlpJars/jenkins-cli.jar -o jenkins-cli.jar Create Alias Test 1 alias cboc = java -jar jenkins-cli.jar -noKeyAuth -auth ${ USR } : ${ TKN } -s ${ OC_URL } 1 cboc version Create Team Ops As the tasks of the Team Masters for managing Operations are quite specific and demand special rights, I'd recommend putting this in its own namespace . To do so properly, we need to configure a few things. allows Operations Center access to this namespace (so it can create the Team Master) give the ServiceAccount the permissions to create namespace 's for the other Team Masters add config map for the Jenkins Agents temporarily change Operations Center's operating Namespace (where it will spawn resources in) use the CLI to create the team - ops Team Master reset Operations Center's operating Namespace Update Create Kubernetes Namespaces Create Team Ops Namespace 1 kubectl apply -f team-ops-namespace.yaml team-ops-namespace.yaml This creates the team - ops namespace including all the resources required such as ResourceQuota , ServiceAccount and so on. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 apiVersion : v1 kind : Namespace metadata : name : team-ops --- apiVersion : v1 kind : ResourceQuota metadata : name : resource-quota namespace : team-ops spec : hard : pods : 20 requests.cpu : 4 requests.memory : 6Gi limits.cpu : 5 limits.memory : 10Gi services.loadbalancers : 0 services.nodeports : 0 persistentvolumeclaims : 10 --- apiVersion : v1 kind : ServiceAccount metadata : name : jenkins namespace : team-ops --- kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : pods-all namespace : team-ops rules : - apiGroups : [ ] resources : [ pods ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/exec ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/log ] verbs : [ get , list , watch ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : jenkins namespace : team-ops roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pods-all subjects : - kind : ServiceAccount name : jenkins namespace : team-ops --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : cjoc roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : master-management subjects : - kind : ServiceAccount name : jenkins namespace : team-ops --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : create-namespaces rules : - apiGroups : [ * ] resources : [ serviceaccounts , rolebindings , roles , resourcequotas , namespaces ] verbs : [ create , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ configmaps , rolebindings , roles , resourcequotas , namespaces ] verbs : [ create , get , list ] - apiGroups : [ ] resources : [ events ] verbs : [ get , list , watch ] - apiGroups : [ ] resources : [ persistentvolumeclaims , pods , pods/exec , services , statefulsets , ingresses , extensions ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/log ] verbs : [ get , list , watch ] - apiGroups : [ ] resources : [ secrets ] verbs : [ list ] - apiGroups : [ apps ] resources : [ statefulsets ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ extensions ] resources : [ ingresses ] verbs : [ create , delete , get , list , patch , update , watch ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : ops-namespace roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : create-namespaces subjects : - kind : ServiceAccount name : jenkins namespace : team-ops Update Operation Center ServiceAccount The ServiceAccount under which Operation Center runs, only has rights in it's own namespace . Which means it cannot create our Team Ops Master. Below is the .yaml file for Kubernetes and the command to apply it. Warning I assume you're using the default cloudbees - core as per Cloudbees' documentation. If this is not the case, change the last line, namespace : cloudbees - core with the namespace your Operation Center runs in. 1 kubectl apply -f patch-oc-serviceaccount.yaml -n team-ops patch-oc-serviceaccount.yaml This patches the existing Operation Center's ServiceAccount to also have the correct rights in the team - ops namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : master-management rules : - apiGroups : [ ] resources : [ pods ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/exec ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/log ] verbs : [ get , list , watch ] - apiGroups : [ apps ] resources : [ statefulsets ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ services ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ persistentvolumeclaims ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ extensions ] resources : [ ingresses ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ secrets ] verbs : [ list ] - apiGroups : [ ] resources : [ events ] verbs : [ get , list , watch ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : cjoc roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : master-management subjects : - kind : ServiceAccount name : cjoc namespace : cloudbees-core Jenkins Agent ConfigMap 1 kubectl apply -f jenkins-agent-config-map.yaml -n team-ops jenkins-agent-config-map.yaml Creates the Jenkins Agent ConfigMap, which contains the information the Jenkins Agent - within a PodTemplate - uses to connect to the Jenkins Master. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 apiVersion : v1 kind : ConfigMap metadata : name : jenkins-agent data : jenkins-agent : | #!/usr/bin/env sh # The MIT License # # Copyright (c) 2015, CloudBees, Inc. # # Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files (the Software ), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED AS IS , WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN # THE SOFTWARE. # Usage jenkins-slave.sh [options] -url http://jenkins [SECRET] [AGENT_NAME] # Optional environment variables : # * JENKINS_TUNNEL : HOST:PORT for a tunnel to route TCP traffic to jenkins host, when jenkins can t be directly accessed over network # * JENKINS_URL : alternate jenkins URL # * JENKINS_SECRET : agent secret, if not set as an argument # * JENKINS_AGENT_NAME : agent name, if not set as an argument if [ $# -eq 1 ]; then # if `docker run` only has one arguments, we assume user is running alternate command like `bash` to inspect the image exec $@ else # if -tunnel is not provided try env vars case $@ in * -tunnel *) ;; *) if [ ! -z $JENKINS_TUNNEL ]; then TUNNEL= -tunnel $JENKINS_TUNNEL fi ;; esac if [ -n $JENKINS_URL ]; then URL= -url $JENKINS_URL fi if [ -n $JENKINS_NAME ]; then JENKINS_AGENT_NAME= $JENKINS_NAME fi if [ -z $JNLP_PROTOCOL_OPTS ]; then echo Warning: JnlpProtocol3 is disabled by default, use JNLP_PROTOCOL_OPTS to alter the behavior JNLP_PROTOCOL_OPTS= -Dorg.jenkinsci.remoting.engine.JnlpProtocol3.disabled=true fi # If both required options are defined, do not pass the parameters OPT_JENKINS_SECRET= if [ -n $JENKINS_SECRET ]; then case $@ in * ${JENKINS_SECRET} *) echo Warning: SECRET is defined twice in command-line arguments and the environment variable ;; *) OPT_JENKINS_SECRET= ${JENKINS_SECRET} ;; esac fi OPT_JENKINS_AGENT_NAME= if [ -n $JENKINS_AGENT_NAME ]; then case $@ in * ${JENKINS_AGENT_NAME} *) echo Warning: AGENT_NAME is defined twice in command-line arguments and the environment variable ;; *) OPT_JENKINS_AGENT_NAME= ${JENKINS_AGENT_NAME} ;; esac fi SLAVE_JAR=/usr/share/jenkins/slave.jar if [ ! -f $SLAVE_JAR ]; then tmpfile=$(mktemp) if hash wget /dev/null 2 1; then wget -O $tmpfile $JENKINS_URL/jnlpJars/slave.jar elif hash curl /dev/null 2 1; then curl -o $tmpfile $JENKINS_URL/jnlpJars/slave.jar else echo Image does not include $SLAVE_JAR and could not find wget or curl to download it return 1 fi SLAVE_JAR=$tmpfile fi #TODO: Handle the case when the command-line and Environment variable contain different values. #It is fine it blows up for now since it should lead to an error anyway. exec java $JAVA_OPTS $JNLP_PROTOCOL_OPTS -cp $SLAVE_JAR hudson.remoting.jnlp.Main -headless $TUNNEL $URL $OPT_JENKINS_SECRET $OPT_JENKINS_AGENT_NAME $@ fi Create Initial Master To make it easier to change the namespace if needed, its extracted out from the command. 1 OriginalNamespace = cloudbees-core This script changes the Operations Center's operating namespace , creates a Team Master with the name ops , and then resets the namespace. 1 2 3 cboc groovy = configure-oc-namespace.groovy team-ops cboc teams ops --put team-ops.json cboc groovy = configure-oc-namespace.groovy $OriginalNamespace team-ops.json This json file that describes a team. By default there are three roles defined on a team, TEAM_ADMIN , TEAM_MEMBER , and TEAM_GUEST . Don't forget to change the id 's to Group ID's from your Single-Sign-On solution. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { version : 1 , data : { name : ops , displayName : Operations , provisioningRecipe : basic , members : [{ id : Catmins , roles : [ TEAM_ADMIN ] }, { id : Pirates , roles : [ TEAM_MEMBER ] }, { id : Continental , roles : [ TEAM_GUEST ] } ], icon : { name : hexagons , color : #8d7ec1 } } } configure-oc-namespace.groovy This is a Jenkins Configuration or System Groovy script. It will change the namespace Operation Center uses to create resources. You can change this in the UI by going to Operations Center - Manage Jenkins - System Configuration - Master Provisioning - Namespace . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import hudson.* import hudson.util.Secret ; import hudson.util.Scrambler ; import hudson.util.FormValidation ; import jenkins.* import jenkins.model.* import hudson.security.* import com.cloudbees.masterprovisioning.kubernetes.KubernetesMasterProvisioning import com.cloudbees.masterprovisioning.kubernetes.KubernetesClusterEndpoint println === KubernetesMasterProvisioning Configuration - start println == Retrieving main configuration def descriptor = Jenkins . getInstance (). getInjector (). getInstance ( KubernetesMasterProvisioning . DescriptorImpl . class ) def namespace = this . args [ 0 ] def currentKubernetesClusterEndpoint = descriptor . getClusterEndpoints (). get ( 0 ) println = Found current endpoint println = + currentKubernetesClusterEndpoint . toString () def id = currentKubernetesClusterEndpoint . getId () def name = currentKubernetesClusterEndpoint . getName () def url = currentKubernetesClusterEndpoint . getUrl () def credentialsId = currentKubernetesClusterEndpoint . getCredentialsId () println == Setting Namspace to + namespace def updatedKubernetesClusterEndpoint = new KubernetesClusterEndpoint ( id , name , url , credentialsId , namespace ) def clusterEndpoints = new ArrayList KubernetesClusterEndpoint () clusterEndpoints . add ( updatedKubernetesClusterEndpoint ) descriptor . setClusterEndpoints ( clusterEndpoints ) println == Saving Jenkins configuration descriptor . save () println === KubernetesMasterProvisioning Configuration - finish Configure Team Ops Master Now that we've created the Operations Team Master (Team Ops), we can configure it. The Pipelines we need will require credentials, we describe them below. githubtoken_token : GitHub API Token only, credentials type Secret Text (for the PR pipeline) githubtoken : GitHub username and API Token jenkins-api : Username and API Token for Operations Center. Just like the one we used for Client Jar. We also need to have a Global Pipeline Library defined by the name github . com / joostvdg / jpl - core . This, as the name suggests, should point to https : // github . com / joostvdg / jpl - core . git . Create GitOps Pipeline In total, we need two repositories and two or three Pipelines. You can either use my CLI Docker Image or roll your own. I will proceed as if you will create your own. CLI Image Pipeline : this will create a CLI Docker Image that is used to talk to Operations Center via the Client Jar (CLI) PR Pipeline : I like the idea of Self-Service, but in order to keep things in check, you might want to provide that via a PullRequest (PR) rather than a direct write to the Master branch. This is also Repository One, as I prefer having each pipeline in their own Repository, but you don't need to. Main Pipeline : will trigger on a commit to the Master branch and create the new team. I'll even throw in a free manage your Team Recipes for free as well. Create CLI Image Pipeline In a Kubernetes cluster, you should not build with Docker directly, use an in-cluster builder such as Kaniko or Buildah . You can read more about the why and how elsewhere on this site . Tip If you do not want to create your own, you can re-use my images. There should be one available for every recent version of CloudBees Core that will work with your Operations Center. The images are available in DockerHub at caladreas/cbcore-cli Kaniko Configuration Kaniko uses a Docker Image to build your Docker Image in cluster. It does however need to directly communicate to your Docker Registry. This requires a Kubernetes Secret of type docker - registry . How you can do this and more, you can read on the CloudBees Core Docs . Pipeline Now that you have Kaniko configured, you can use this Pipeline to create your own CLI Images. Caution Make sure you replace the environment variables with values that make sense to you. CJOC_URL internal url in Kubernets, usually http : // cjoc . namespace / cjoc REGISTRY : index.docker.io = DockerHub REPO : docker repository name IMAGE : docker image name Jenkinsfile Jenkins Declarative Pipeline for the CLI Image geberation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 pipeline { agent { kubernetes { //cloud kubernetes label test yaml kind: Pod metadata: name: test spec: containers: - name: curl image: byrnedo/alpine-curl command: - cat tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: jenkins-docker-cfg projected: sources: - secret: name: docker-credentials items: - key: .dockerconfigjson path: .docker/config.json } } environment { CJOC_URL = http://cjoc.cloudbees-core/cjoc CLI_VERSION = REGISTRY = index.docker.io REPO = caladreas IMAGE = cbcore-cli } stages { stage ( Download CLI ) { steps { container ( curl ) { sh curl --version sh echo ${CJOC_URL}/jnlpJars/jenkins-cli.jar sh curl ${CJOC_URL}/jnlpJars/jenkins-cli.jar --output jenkins-cli.jar sh ls -lath } } } stage ( Prepare ) { parallel { stage ( Verify CLI ) { environment { CREDS = credentials ( jenkins-api ) CLI = java -jar jenkins-cli.jar -noKeyAuth -s ${CJOC_URL} -auth } steps { sh echo ${CLI} script { CLI_VERSION = sh returnStdout: true , script: ${CLI} ${CREDS} version } sh echo ${CLI_VERSION} } } stage ( Prepare Dockerfile ) { steps { writeFile encoding: UTF-8 , file: Dockerfile , text: FROM mcr.microsoft.com/java/jre-headless:8u192-zulu-alpine WORKDIR /usr/bin ADD jenkins-cli.jar . RUN pwd RUN ls -lath } } } } stage ( Build with Kaniko ) { environment { PATH = /busybox:/kaniko:$PATH TAG = ${CLI_VERSION} } steps { sh echo image fqn=${REGISTRY}/${REPO}/${IMAGE}:${TAG} container ( name: kaniko , shell: /busybox/sh ) { sh #!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:${TAG} /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:latest } } } } } PR Pipeline Caution The PR Pipeline example builds upon the GitHub API, if you're not using GItHub, you will have to figure out another way to make the PR. Tools Used yq : commandline tool for processing Yaml files jq commandline tool for pressing Json files Kustomize templating tool for Kubernetes Yaml, as of Kubernetes 1 . 13 , this is part of the Client (note, your server can be older, don't worry!) Hub commandline client for GitHub Repository Layout folder: team - master - template with file simple . json folder: namespace - creation with folder: kustomize this contains the Kustomize configuration Simple.json This is a template for the team JSON definition. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { version : 1 , data : { name : NAME , displayName : DISPLAY_NAME , provisioningRecipe : RECIPE , members : [ { id : ADMINS , roles : [ TEAM_ADMIN ] }, { id : MEMBERS , roles : [ TEAM_MEMBER ] }, { id : GUESTS , roles : [ TEAM_GUEST ] } ], icon : { name : ICON , color : HEX_COLOR } } } Kustomize Configuration Kustomize is a tool for template Kubernetes YAML definitions, which is what we need here. However, only for the namespace creation configuration. So if you don't want to do that, you can skip this. The Kustomize configuration has two parts, a folder called team - example with a kustomization . yaml . This will be what we configure to generate a new yaml definition. The main template is in the folder base , where the entrypoint will be again kustomization . yaml . This time, the kustomization . yaml will link to all the template files we need. As posting all these yaml files again is a bit much, I'll link to my example repo. Feel free to fork it instead: cb-team-gitops-template configmap.yaml : the Jenkins Agent ConfigMap namespace.yaml : the new namespace resource-quota.yaml : resource quota's for the namespace role-binding-cjoc.yaml : a role binding for the CJOC ServiceAccount, so it create create the new Master in the new namespace role-binding.yaml : the role binding for the jenkins ServiceAccount, which allows the new Master to create and manage Pods (for PodTemplates) role-cjoc.yaml : the role for CJOC for the ability to create a Master in the new Namspace role.yaml : the role for the jenkins ServiceAccount for the new Master service-account.yaml : the ServiceAccount, jenkins , used by the new Master Pipeline The Pipeline will do the following: capture input parameters to be used to customize the Team Master update the Kustomize template to make sure every resource is correct for the new namespace ( teams - name of team ) execute Kustomize to generate a single yaml file that defines the configuration for the new Team Masters' namespace process the simple . json to generate a team . json file for the new Team Master for use with the Jenkins CLI checkout your GIT_REPO that contains your team definitions create a new PR to your GIT_REPO for the new team Jenkinsfile Variables to update: GIT_REPO : the GitHub repository in which the Team Definitions are stored RESET_NAMESPACE : the namespace Operations Center should use as default 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 pipeline { agent { kubernetes { label team-automation yaml kind: Pod spec: containers: - name: hub image: caladreas/hub command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 150m limits: memory: 50Mi cpu: 150m - name: kubectl image: bitnami/kubectl:latest command: [ cat ] tty: true securityContext: runAsUser: 1000 fsGroup: 1000 resources: requests: memory: 50Mi cpu: 100m limits: memory: 150Mi cpu: 200m - name: yq image: mikefarah/yq command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m - name: jq image: colstrom/jq command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m } } libraries { lib ( github.com/joostvdg/jpl-core ) } options { disableConcurrentBuilds () // we don t want more than one at a time checkoutToSubdirectory templates // we need to do two checkouts buildDiscarder logRotator ( artifactDaysToKeepStr: , artifactNumToKeepStr: , daysToKeepStr: 5 , numToKeepStr: 5 ) // always clean up } environment { envGitInfo = RESET_NAMESPACE = jx-production TEAM_BASE_NAME = NAMESPACE_TO_CREATE = DISPLAY_NAME = TEAM_RECIPE = ICON = ICON_COLOR_CODE = ADMINS_ROLE = MEMBERS_ROLE = GUESTS_ROLE = RECORD_LOC = GIT_REPO = } stages { stage ( Team Details ) { input { message Please enter the team details. ok Looks good, proceed parameters { string ( name: Name , defaultValue: hex , description: Please specify a team name ) string ( name: DisplayName , defaultValue: Hex , description: Please specify a team display name ) choice choices: [ joostvdg , basic , java-web ], description: Please select a Team Recipe , name: TeamRecipe choice choices: [ anchor , bear , bowler-hat , briefcase , bug , calculator , calculatorcart , clock , cloud , cloudbees , connect , dollar-bill , dollar-symbol , file , flag , flower-carnation , flower-daisy , help , hexagon , high-heels , jenkins , key , marker , monocle , mustache , office , panther , paw-print , teacup , tiger , truck ], description: Please select an Icon , name: Icon string ( name: IconColorCode , defaultValue: #CCCCCC , description: Please specify a valid html hexcode for the color (https://htmlcolorcodes.com/) ) string ( name: Admins , defaultValue: Catmins , description: Please specify a groupid or userid for the TEAM_ADMIN role ) string ( name: Members , defaultValue: Pirates , description: Please specify a groupid or userid for the TEAM_MEMBER role ) string ( name: Guests , defaultValue: Continental , description: Please specify a groupid or userid for the TEAM_GUEST role ) } } steps { println Name=${Name} println DisplayName=${DisplayName} println TeamRecipe=${TeamRecipe} println Icon=${Icon} println IconColorCode=${IconColorCode} println Admins=${Admins} println Members=${Members} println Guests=${Guests} script { TEAM_BASE_NAME = ${Name} NAMESPACE_TO_CREATE = cb-teams-${Name} DISPLAY_NAME = ${DisplayName} TEAM_RECIPE = ${TeamRecipe} ICON = ${Icon} ICON_COLOR_CODE = ${IconColorCode} ADMINS_ROLE = ${Admins} MEMBERS_ROLE = ${Members} GUESTS_ROLE = ${Guests} RECORD_LOC = templates/teams/${Name} sh mkdir -p ${RECORD_LOC} } } } stage ( Create Team Config ) { environment { BASE = templates/namespace-creation/kustomize NAMESPACE = ${NAMESPACE_TO_CREATE} RECORD_LOC = templates/teams/${TEAM_BASE_NAME} } parallel { stage ( Namespace ) { steps { container ( yq ) { sh yq w -i ${BASE}/base/role-binding.yaml subjects[0].namespace ${NAMESPACE} sh yq w -i ${BASE}/base/namespace.yaml metadata.name ${NAMESPACE} sh yq w -i ${BASE}/team-example/kustomization.yaml namespace ${NAMESPACE} } container ( kubectl ) { sh kubectl kustomize ${BASE}/team-example ${RECORD_LOC}/team.yaml cat ${RECORD_LOC}/team.yaml } } } stage ( Team Master JSON ) { steps { container ( jq ) { sh jq \\ .data.name = ${TEAM_BASE_NAME} |\\ .data.displayName = ${DISPLAY_NAME} |\\ .data.provisioningRecipe = ${TEAM_RECIPE} |\\ .data.icon.name = ${ICON} |\\ .data.icon.color = ${ICON_COLOR_CODE} |\\ .data.members[0].id = ${ADMINS_ROLE} |\\ .data.members[1].id = ${MEMBERS_ROLE} |\\ .data.members[2].id = ${GUESTS_ROLE} \\ templates/team-master-template/simple.json ${RECORD_LOC}/team.json } sh cat ${RECORD_LOC}/team.json } } } } stage ( Create PR ) { when { branch master } environment { RECORD_OLD_LOC = templates/teams/${TEAM_BASE_NAME} RECORD_LOC = teams/${TEAM_BASE_NAME} PR_CHANGE_NAME = add_team_${TEAM_BASE_NAME} } steps { container ( hub ) { dir ( cb-team-gitops ) { script { envGitInfo = git ${GIT_REPO} } sh git checkout -b ${PR_CHANGE_NAME} sh ls -lath ../${RECORD_OLD_LOC} sh cp -R ../${RECORD_OLD_LOC} ./teams sh ls -lath sh ls -lath teams/ gitRemoteConfigByUrl ( envGitInfo . GIT_URL , githubtoken_token ) // must be a API Token ONLY - secret text sh git config --global user.email jenkins@jenkins.io git config --global user.name Jenkins git add ${RECORD_LOC} git status git commit -m add team ${TEAM_BASE_NAME} git push origin ${PR_CHANGE_NAME} // has to be indented like that, else the indents will be in the pr description writeFile encoding: UTF-8 , file: pr-info.md , text: Add ${TEAM_BASE_NAME} \\n This pr is automatically generated via CloudBees.\\\\n \\n The job: ${env.JOB_URL} // TODO: unfortunately, environment {} s credentials have fixed environment variable names // TODO: in this case, they need to be EXACTLY GITHUB_PASSWORD and GITHUB_USER script { withCredentials ([ usernamePassword ( credentialsId: githubtoken , passwordVariable: GITHUB_PASSWORD , usernameVariable: GITHUB_USER )]) { sh set +x hub pull-request --force -F pr-info.md -l ${TEAM_BASE_NAME} --no-edit } } } } } } } } Main Pipeline The main Pipeline should be part of a repository. The Repository should look like this: recipes (folder) recipes . json - current complete list of CloudBees Core Team Recipes definition teams (folder) folder per team team . json - CloudBees Core Team definition team . yaml - Kubernetes YAML definition of the namespace and all its resources Process The pipeline can be a bit hard to grasp, so let me break it down into individual steps. We have the following stages: Create Team - which is broken into sub-stages via the sequential stages feature . * Parse Changelog * Create Namespace * Change OC Namespace * Create Team Master Test CLI Connection Update Team Recipes Notable Statements disableConcurrentBuilds We change the namespace of Operation Center to a different value only for the duration of creating this master. This is something that should probably be part of the Team Master creation, but as it is a single configuration option for all that Operation Center does, we need to be careful. By ensuring we only run one build concurrently, we reduce the risk of this blowing up in our face. 1 2 3 options { disableConcurrentBuilds () } when { } The When Directive allows us to creating effective conditions for when a stage should be executed. The snippet below shows the use of a combination of both the branch and changeset built-in filters. changeset looks at the commit being build and validates that there was a change in that file path. 1 when { allOf { branch master ; changeset teams/**/team.* } } post { always { } } The Post Directive allows us to run certain commands after the main pipeline has run depending on the outcome (compared or not to the previous outcome). In this case, we want to make sure we reset the namespace used by Operations Center to the original value. By using post { always {} } , it will ALWAYS run, regardless of the status of the pipeline. So we should be safe. 1 2 3 4 5 6 7 post { always { container ( cli ) { sh ${CLI} ${CREDS} groovy = resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE} } } } stages { stage { parallel { stage() { stages { stage { Oke, you might've noticed this massive indenting depth and probably have some questions. By combining sequential stages with parallel stages we can create a set of stages that will be executed in sequence but can be controlled by a single when {} statement whether or not they get executed. This prevents mistakes being made in the condition and accidentally running one or other but not all the required steps. 1 2 3 4 5 6 stages { stage ( Create Team ) { parallel { stage ( Main ) { stages { stage ( Parse Changelog ) { changetSetData container('jpb') {} Alright, so even if we know a team was added in / teams / team - name , we still don't know the following two things: 1) what is the name of this team, 2) was this team changed or deleted? So we have to process the changelog to be able to answer these questions as well. There are different ways of getting the changelog and parsing it. I've written one you can do on ANY machine, regardless of Jenkins by leveraging Git and my own custom binary ( jpb - Jenkins Pipeline Binary). The code for my binary is at GitHub: github.com/joostvdg/jpb . An alternative approach is described by CloudBees Support here , which leverages Jenkins groovy powers. 1 2 3 4 5 6 7 COMMIT_INFO = ${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT} def changeSetData = sh returnStdout: true , script: git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO} changeSetData = changeSetData . replace ( \\n , \\\\n ) container ( jpb ) { changeSetFolders = sh returnStdout: true , script: /usr/bin/jpb/bin/jpb GitChangeListToFolder ${changeSetData} teams/ changeSetFolders = changeSetFolders . split ( , ) } Files recipes.json The default Team Recipes that ships with CloudBees Core Modern. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { version : 1 , data : [{ name : basic , displayName : Basic , description : The minimalistic setup. , plugins : [ bluesteel-master , cloudbees-folders-plus , cloudbees-jsync-archiver , cloudbees-monitoring , cloudbees-nodes-plus , cloudbees-ssh-slaves , cloudbees-support , cloudbees-workflow-template , credentials-binding , email-ext , git , git-client , github-branch-source , github-organization-folder , infradna-backup , ldap , mailer , operations-center-analytics-reporter , operations-center-cloud , pipeline-model-definition , ssh-credentials , wikitext , workflow-aggregator , workflow-cps-checkpoint ], default : true }, { name : java-web , displayName : Java Web Development , description : The essential tools to build, release and deploy Java Web applications including integration with Maven, Gradle and Node JS. , plugins : [ bluesteel-master , cloudbees-folders-plus , cloudbees-jsync-archiver , cloudbees-monitoring , cloudbees-nodes-plus , cloudbees-ssh-slaves , cloudbees-support , cloudbees-workflow-template , credentials-binding , email-ext , git , git-client , github-branch-source , github-organization-folder , infradna-backup , ldap , mailer , operations-center-analytics-reporter , operations-center-cloud , pipeline-model-definition , ssh-credentials , wikitext , workflow-aggregator , workflow-cps-checkpoint , config-file-provider , cloudbees-aws-cli , cloudbees-cloudfoundry-cli , findbugs , gradle , jira , junit , nodejs , openshift-cli , pipeline-maven , tasks , warnings ], default : false }] } Jenkinsfile This is the pipeline that will process the commit to the repository and, if it detects a new team is created will apply the changes. Variables to overwrite: GIT_REPO : the https url to the Git Repository your GitOps code/configuration is stored RESET_NAMESPACE : the namespace your Operation Center normally operates in CLI : this command depends on the namespace Operation Center is in ( http : // service name . namespace / cjoc ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 pipeline { agent { kubernetes { label jenkins-agent yaml apiVersion: v1 kind: Pod spec: serviceAccountName: jenkins containers: - name: cli image: caladreas/cbcore-cli:2.176.2.3 imagePullPolicy: Always command: - cat tty: true resources: requests: memory: 50Mi cpu: 150m limits: memory: 50Mi cpu: 150m - name: kubectl image: bitnami/kubectl:latest command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 150Mi cpu: 200m - name: yq image: mikefarah/yq command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m - name: jpb image: caladreas/jpb command: - cat tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m securityContext: runAsUser: 1000 fsGroup: 1000 } } options { disableConcurrentBuilds () buildDiscarder logRotator ( artifactDaysToKeepStr: , artifactNumToKeepStr: , daysToKeepStr: 5 , numToKeepStr: 5 ) } environment { RESET_NAMESPACE = cloudbees-core CREDS = credentials ( jenkins-api ) CLI = java -jar /usr/bin/jenkins-cli.jar -noKeyAuth -s http://cjoc.cloudbees-core/cjoc -auth COMMIT_INFO = TEAM = GIT_REPO = } stages { stage ( Create Team ) { when { allOf { branch master ; changeset teams/**/team.* } } parallel { stage ( Main ) { stages { stage ( Parse Changelog ) { steps { // Alternative approach: https://support.cloudbees.com/hc/en-us/articles/217630098-How-to-access-Changelogs-in-a-Pipeline-Job- // However, that runs on the master, JPB runs in an agent! script { scmVars = git ${GIT_REPO} COMMIT_INFO = ${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT} def changeSetData = sh returnStdout: true , script: git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO} changeSetData = changeSetData . replace ( \\n , \\\\n ) container ( jpb ) { changeSetFolders = sh returnStdout: true , script: /usr/bin/jpb/bin/jpb GitChangeListToFolder ${changeSetData} teams/ changeSetFolders = changeSetFolders . split ( , ) } if ( changeSetFolders . length 0 ) { TEAM = changeSetFolders [ 0 ] TEAM = TEAM . trim () // to protect against a team being removed def exists = fileExists teams/${TEAM}/team.yaml if (! exists ) { TEAM = } } else { TEAM = } echo Team that changed: |${TEAM}| } } } stage ( Create Namespace ) { when { expression { return ! TEAM . equals ( ) } } environment { NAMESPACE = cb-teams-${TEAM} RECORD_LOC = teams/${TEAM} } steps { container ( kubectl ) { sh cat ${RECORD_LOC}/team.yaml kubectl apply -f ${RECORD_LOC}/team.yaml } } } stage ( Change OC Namespace ) { when { expression { return ! TEAM . equals ( ) } } environment { NAMESPACE = cb-teams-${TEAM} } steps { container ( cli ) { sh echo ${NAMESPACE} script { def response = sh encoding: UTF-8 , label: create team , returnStatus: true , script: ${CLI} ${CREDS} groovy = resources/bootstrap/configure-oc-namespace.groovy ${NAMESPACE} println Response: ${response} } } } } stage ( Create Team Master ) { when { expression { return ! TEAM . equals ( ) } } environment { TEAM_NAME = ${TEAM} } steps { container ( cli ) { println TEAM_NAME=${TEAM_NAME} sh ls -lath sh ls -lath teams/ script { def response = sh encoding: UTF-8 , label: create team , returnStatus: true , script: ${CLI} ${CREDS} teams ${TEAM_NAME} --put teams/${TEAM_NAME}/team.json println Response: ${response} } } } } } } } } stage ( Test CLI Connection ) { steps { container ( cli ) { script { def response = sh encoding: UTF-8 , label: retrieve version , returnStatus: true , script: ${CLI} ${CREDS} version println Response: ${response} } } } } stage ( Update Team Recipes ) { when { allOf { branch master ; changeset recipes/recipes.json } } steps { container ( cli ) { sh ls -lath sh ls -lath recipes/ script { def response = sh encoding: UTF-8 , label: update team recipe , returnStatus: true , script: ${CLI} ${CREDS} team-creation-recipes --put recipes/recipes.json println Response: ${response} } } } } } post { always { container ( cli ) { sh ${CLI} ${CREDS} groovy = resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE} } } } }","title":"CloudBees Automate Teams"},{"location":"blogs/teams-automation/#core-modern-teams-automation","text":"CloudBees Core on Modern (meaning, on Kubernetes) has two main types of Jenkins Masters, a Managed Master, and a Team Master. In this article, we're going to automate the creation and management of the Team Masters. Hint If you do not want to read any of the code here, or just want to take a look at the end result - pipeline wise - you can find working examples on GitHub. Template Repository - creates a new team template and a PR to the GitOps repository GitOps Repository - applies GitOps principles to manage the CloudBees Core Team Masters","title":"Core Modern Teams Automation"},{"location":"blogs/teams-automation/#goals","text":"Just automating the creation of a Team Master is relatively easy, as this can be done via the Client Jar . So we're going to set some additional goals to create a decent challenge. GitOps : I want to be able to create and delete Team Masters by managing configuration in a Git repository Configuration-as-Code : as much of the configuration as possible should be stored in the Git repository Namespace : one of the major reasons for Team Masters to exist is to increase (Product) Team autonomy, which in Kubernetes should correspond to a namespace . So I want each Team Master to be in its own Namespace! Self-Service : the solution should cater to (semi-)autonomous teams and lower the workload of the team managing CloudBees Core. So requesting a Team Master should be doable by everyone","title":"Goals"},{"location":"blogs/teams-automation/#before-we-start","text":"Some assumptions need to be taken care off before we start. Kubernetes cluster in which you are ClusterAdmin if you don't have one yet, there are guides on this elsewhere on the site your cluster has enough capacity (at least two nodes of 4gb memory) your cluster has CloudBees Core Modern installed if you don't have this yet look at one of the guides on this site or look at the guides on CloudBees.com have administrator access to CloudBees Core Cloud Operations Center Code Examples The more extensive Code Examples, such as Kubernetes Yaml files, are collapsed by default. You can open them by clicking on them. On the right, the code snippet will have a [ ] copy icon. Below is an example. Code Snippet Example Here's a code snippet. 1 2 3 4 5 6 7 8 9 10 pipeline { agent any stages { stage ( Hello ) { steps { echo Hello World! } } } }","title":"Before We Start"},{"location":"blogs/teams-automation/#bootstrapping","text":"All right, so we want to use GitOps and to process the changes we need a Pipeline that can be triggered by a Webhook. I believe in everything as code - except Secrets and such - which includes the Pipeline. Unfortunately, Operations Center cannot run such pipelines. To get over this hurdle, we will create a special Ops Team Master. This Master will be configured to be able to Manage the other Team Masters for us. Log into your Operations Center with a user that has administrative access.","title":"Bootstrapping"},{"location":"blogs/teams-automation/#create-api-token","text":"Create a new API Token for your administrator user by clicking on the user's name - top right corner. Select the Configuration menu on the left and then you should see a section where you can Create a API Token . This Token will disappear, so write it down.","title":"Create API Token"},{"location":"blogs/teams-automation/#get-configure-client-jar","text":"Replace the values marked by ... . The Operations Center URL should look like this: http : // cbcore . mydomain . com / cjoc . Setup the connection variables. 1 OC_URL = your operations center url 1 2 USR = your username TKN = api token Download the Client Jar. 1 curl ${ OC_URL } /jnlpJars/jenkins-cli.jar -o jenkins-cli.jar","title":"Get &amp; Configure Client Jar"},{"location":"blogs/teams-automation/#create-alias-test","text":"1 alias cboc = java -jar jenkins-cli.jar -noKeyAuth -auth ${ USR } : ${ TKN } -s ${ OC_URL } 1 cboc version","title":"Create Alias &amp; Test"},{"location":"blogs/teams-automation/#create-team-ops","text":"As the tasks of the Team Masters for managing Operations are quite specific and demand special rights, I'd recommend putting this in its own namespace . To do so properly, we need to configure a few things. allows Operations Center access to this namespace (so it can create the Team Master) give the ServiceAccount the permissions to create namespace 's for the other Team Masters add config map for the Jenkins Agents temporarily change Operations Center's operating Namespace (where it will spawn resources in) use the CLI to create the team - ops Team Master reset Operations Center's operating Namespace","title":"Create Team Ops"},{"location":"blogs/teams-automation/#update-create-kubernetes-namespaces","text":"","title":"Update &amp; Create Kubernetes Namespaces"},{"location":"blogs/teams-automation/#create-team-ops-namespace","text":"1 kubectl apply -f team-ops-namespace.yaml team-ops-namespace.yaml This creates the team - ops namespace including all the resources required such as ResourceQuota , ServiceAccount and so on. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 apiVersion : v1 kind : Namespace metadata : name : team-ops --- apiVersion : v1 kind : ResourceQuota metadata : name : resource-quota namespace : team-ops spec : hard : pods : 20 requests.cpu : 4 requests.memory : 6Gi limits.cpu : 5 limits.memory : 10Gi services.loadbalancers : 0 services.nodeports : 0 persistentvolumeclaims : 10 --- apiVersion : v1 kind : ServiceAccount metadata : name : jenkins namespace : team-ops --- kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : pods-all namespace : team-ops rules : - apiGroups : [ ] resources : [ pods ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/exec ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/log ] verbs : [ get , list , watch ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : jenkins namespace : team-ops roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pods-all subjects : - kind : ServiceAccount name : jenkins namespace : team-ops --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : cjoc roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : master-management subjects : - kind : ServiceAccount name : jenkins namespace : team-ops --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : create-namespaces rules : - apiGroups : [ * ] resources : [ serviceaccounts , rolebindings , roles , resourcequotas , namespaces ] verbs : [ create , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ configmaps , rolebindings , roles , resourcequotas , namespaces ] verbs : [ create , get , list ] - apiGroups : [ ] resources : [ events ] verbs : [ get , list , watch ] - apiGroups : [ ] resources : [ persistentvolumeclaims , pods , pods/exec , services , statefulsets , ingresses , extensions ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/log ] verbs : [ get , list , watch ] - apiGroups : [ ] resources : [ secrets ] verbs : [ list ] - apiGroups : [ apps ] resources : [ statefulsets ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ extensions ] resources : [ ingresses ] verbs : [ create , delete , get , list , patch , update , watch ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : ops-namespace roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : create-namespaces subjects : - kind : ServiceAccount name : jenkins namespace : team-ops","title":"Create Team Ops Namespace"},{"location":"blogs/teams-automation/#update-operation-center-serviceaccount","text":"The ServiceAccount under which Operation Center runs, only has rights in it's own namespace . Which means it cannot create our Team Ops Master. Below is the .yaml file for Kubernetes and the command to apply it. Warning I assume you're using the default cloudbees - core as per Cloudbees' documentation. If this is not the case, change the last line, namespace : cloudbees - core with the namespace your Operation Center runs in. 1 kubectl apply -f patch-oc-serviceaccount.yaml -n team-ops patch-oc-serviceaccount.yaml This patches the existing Operation Center's ServiceAccount to also have the correct rights in the team - ops namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : master-management rules : - apiGroups : [ ] resources : [ pods ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/exec ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/log ] verbs : [ get , list , watch ] - apiGroups : [ apps ] resources : [ statefulsets ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ services ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ persistentvolumeclaims ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ extensions ] resources : [ ingresses ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ secrets ] verbs : [ list ] - apiGroups : [ ] resources : [ events ] verbs : [ get , list , watch ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : cjoc roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : master-management subjects : - kind : ServiceAccount name : cjoc namespace : cloudbees-core","title":"Update Operation Center ServiceAccount"},{"location":"blogs/teams-automation/#jenkins-agent-configmap","text":"1 kubectl apply -f jenkins-agent-config-map.yaml -n team-ops jenkins-agent-config-map.yaml Creates the Jenkins Agent ConfigMap, which contains the information the Jenkins Agent - within a PodTemplate - uses to connect to the Jenkins Master. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 apiVersion : v1 kind : ConfigMap metadata : name : jenkins-agent data : jenkins-agent : | #!/usr/bin/env sh # The MIT License # # Copyright (c) 2015, CloudBees, Inc. # # Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files (the Software ), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED AS IS , WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN # THE SOFTWARE. # Usage jenkins-slave.sh [options] -url http://jenkins [SECRET] [AGENT_NAME] # Optional environment variables : # * JENKINS_TUNNEL : HOST:PORT for a tunnel to route TCP traffic to jenkins host, when jenkins can t be directly accessed over network # * JENKINS_URL : alternate jenkins URL # * JENKINS_SECRET : agent secret, if not set as an argument # * JENKINS_AGENT_NAME : agent name, if not set as an argument if [ $# -eq 1 ]; then # if `docker run` only has one arguments, we assume user is running alternate command like `bash` to inspect the image exec $@ else # if -tunnel is not provided try env vars case $@ in * -tunnel *) ;; *) if [ ! -z $JENKINS_TUNNEL ]; then TUNNEL= -tunnel $JENKINS_TUNNEL fi ;; esac if [ -n $JENKINS_URL ]; then URL= -url $JENKINS_URL fi if [ -n $JENKINS_NAME ]; then JENKINS_AGENT_NAME= $JENKINS_NAME fi if [ -z $JNLP_PROTOCOL_OPTS ]; then echo Warning: JnlpProtocol3 is disabled by default, use JNLP_PROTOCOL_OPTS to alter the behavior JNLP_PROTOCOL_OPTS= -Dorg.jenkinsci.remoting.engine.JnlpProtocol3.disabled=true fi # If both required options are defined, do not pass the parameters OPT_JENKINS_SECRET= if [ -n $JENKINS_SECRET ]; then case $@ in * ${JENKINS_SECRET} *) echo Warning: SECRET is defined twice in command-line arguments and the environment variable ;; *) OPT_JENKINS_SECRET= ${JENKINS_SECRET} ;; esac fi OPT_JENKINS_AGENT_NAME= if [ -n $JENKINS_AGENT_NAME ]; then case $@ in * ${JENKINS_AGENT_NAME} *) echo Warning: AGENT_NAME is defined twice in command-line arguments and the environment variable ;; *) OPT_JENKINS_AGENT_NAME= ${JENKINS_AGENT_NAME} ;; esac fi SLAVE_JAR=/usr/share/jenkins/slave.jar if [ ! -f $SLAVE_JAR ]; then tmpfile=$(mktemp) if hash wget /dev/null 2 1; then wget -O $tmpfile $JENKINS_URL/jnlpJars/slave.jar elif hash curl /dev/null 2 1; then curl -o $tmpfile $JENKINS_URL/jnlpJars/slave.jar else echo Image does not include $SLAVE_JAR and could not find wget or curl to download it return 1 fi SLAVE_JAR=$tmpfile fi #TODO: Handle the case when the command-line and Environment variable contain different values. #It is fine it blows up for now since it should lead to an error anyway. exec java $JAVA_OPTS $JNLP_PROTOCOL_OPTS -cp $SLAVE_JAR hudson.remoting.jnlp.Main -headless $TUNNEL $URL $OPT_JENKINS_SECRET $OPT_JENKINS_AGENT_NAME $@ fi","title":"Jenkins Agent ConfigMap"},{"location":"blogs/teams-automation/#create-initial-master","text":"To make it easier to change the namespace if needed, its extracted out from the command. 1 OriginalNamespace = cloudbees-core This script changes the Operations Center's operating namespace , creates a Team Master with the name ops , and then resets the namespace. 1 2 3 cboc groovy = configure-oc-namespace.groovy team-ops cboc teams ops --put team-ops.json cboc groovy = configure-oc-namespace.groovy $OriginalNamespace team-ops.json This json file that describes a team. By default there are three roles defined on a team, TEAM_ADMIN , TEAM_MEMBER , and TEAM_GUEST . Don't forget to change the id 's to Group ID's from your Single-Sign-On solution. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { version : 1 , data : { name : ops , displayName : Operations , provisioningRecipe : basic , members : [{ id : Catmins , roles : [ TEAM_ADMIN ] }, { id : Pirates , roles : [ TEAM_MEMBER ] }, { id : Continental , roles : [ TEAM_GUEST ] } ], icon : { name : hexagons , color : #8d7ec1 } } } configure-oc-namespace.groovy This is a Jenkins Configuration or System Groovy script. It will change the namespace Operation Center uses to create resources. You can change this in the UI by going to Operations Center - Manage Jenkins - System Configuration - Master Provisioning - Namespace . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import hudson.* import hudson.util.Secret ; import hudson.util.Scrambler ; import hudson.util.FormValidation ; import jenkins.* import jenkins.model.* import hudson.security.* import com.cloudbees.masterprovisioning.kubernetes.KubernetesMasterProvisioning import com.cloudbees.masterprovisioning.kubernetes.KubernetesClusterEndpoint println === KubernetesMasterProvisioning Configuration - start println == Retrieving main configuration def descriptor = Jenkins . getInstance (). getInjector (). getInstance ( KubernetesMasterProvisioning . DescriptorImpl . class ) def namespace = this . args [ 0 ] def currentKubernetesClusterEndpoint = descriptor . getClusterEndpoints (). get ( 0 ) println = Found current endpoint println = + currentKubernetesClusterEndpoint . toString () def id = currentKubernetesClusterEndpoint . getId () def name = currentKubernetesClusterEndpoint . getName () def url = currentKubernetesClusterEndpoint . getUrl () def credentialsId = currentKubernetesClusterEndpoint . getCredentialsId () println == Setting Namspace to + namespace def updatedKubernetesClusterEndpoint = new KubernetesClusterEndpoint ( id , name , url , credentialsId , namespace ) def clusterEndpoints = new ArrayList KubernetesClusterEndpoint () clusterEndpoints . add ( updatedKubernetesClusterEndpoint ) descriptor . setClusterEndpoints ( clusterEndpoints ) println == Saving Jenkins configuration descriptor . save () println === KubernetesMasterProvisioning Configuration - finish","title":"Create Initial Master"},{"location":"blogs/teams-automation/#configure-team-ops-master","text":"Now that we've created the Operations Team Master (Team Ops), we can configure it. The Pipelines we need will require credentials, we describe them below. githubtoken_token : GitHub API Token only, credentials type Secret Text (for the PR pipeline) githubtoken : GitHub username and API Token jenkins-api : Username and API Token for Operations Center. Just like the one we used for Client Jar. We also need to have a Global Pipeline Library defined by the name github . com / joostvdg / jpl - core . This, as the name suggests, should point to https : // github . com / joostvdg / jpl - core . git .","title":"Configure Team Ops Master"},{"location":"blogs/teams-automation/#create-gitops-pipeline","text":"In total, we need two repositories and two or three Pipelines. You can either use my CLI Docker Image or roll your own. I will proceed as if you will create your own. CLI Image Pipeline : this will create a CLI Docker Image that is used to talk to Operations Center via the Client Jar (CLI) PR Pipeline : I like the idea of Self-Service, but in order to keep things in check, you might want to provide that via a PullRequest (PR) rather than a direct write to the Master branch. This is also Repository One, as I prefer having each pipeline in their own Repository, but you don't need to. Main Pipeline : will trigger on a commit to the Master branch and create the new team. I'll even throw in a free manage your Team Recipes for free as well.","title":"Create GitOps Pipeline"},{"location":"blogs/teams-automation/#create-cli-image-pipeline","text":"In a Kubernetes cluster, you should not build with Docker directly, use an in-cluster builder such as Kaniko or Buildah . You can read more about the why and how elsewhere on this site . Tip If you do not want to create your own, you can re-use my images. There should be one available for every recent version of CloudBees Core that will work with your Operations Center. The images are available in DockerHub at caladreas/cbcore-cli","title":"Create CLI Image Pipeline"},{"location":"blogs/teams-automation/#kaniko-configuration","text":"Kaniko uses a Docker Image to build your Docker Image in cluster. It does however need to directly communicate to your Docker Registry. This requires a Kubernetes Secret of type docker - registry . How you can do this and more, you can read on the CloudBees Core Docs .","title":"Kaniko Configuration"},{"location":"blogs/teams-automation/#pipeline","text":"Now that you have Kaniko configured, you can use this Pipeline to create your own CLI Images. Caution Make sure you replace the environment variables with values that make sense to you. CJOC_URL internal url in Kubernets, usually http : // cjoc . namespace / cjoc REGISTRY : index.docker.io = DockerHub REPO : docker repository name IMAGE : docker image name Jenkinsfile Jenkins Declarative Pipeline for the CLI Image geberation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 pipeline { agent { kubernetes { //cloud kubernetes label test yaml kind: Pod metadata: name: test spec: containers: - name: curl image: byrnedo/alpine-curl command: - cat tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: jenkins-docker-cfg projected: sources: - secret: name: docker-credentials items: - key: .dockerconfigjson path: .docker/config.json } } environment { CJOC_URL = http://cjoc.cloudbees-core/cjoc CLI_VERSION = REGISTRY = index.docker.io REPO = caladreas IMAGE = cbcore-cli } stages { stage ( Download CLI ) { steps { container ( curl ) { sh curl --version sh echo ${CJOC_URL}/jnlpJars/jenkins-cli.jar sh curl ${CJOC_URL}/jnlpJars/jenkins-cli.jar --output jenkins-cli.jar sh ls -lath } } } stage ( Prepare ) { parallel { stage ( Verify CLI ) { environment { CREDS = credentials ( jenkins-api ) CLI = java -jar jenkins-cli.jar -noKeyAuth -s ${CJOC_URL} -auth } steps { sh echo ${CLI} script { CLI_VERSION = sh returnStdout: true , script: ${CLI} ${CREDS} version } sh echo ${CLI_VERSION} } } stage ( Prepare Dockerfile ) { steps { writeFile encoding: UTF-8 , file: Dockerfile , text: FROM mcr.microsoft.com/java/jre-headless:8u192-zulu-alpine WORKDIR /usr/bin ADD jenkins-cli.jar . RUN pwd RUN ls -lath } } } } stage ( Build with Kaniko ) { environment { PATH = /busybox:/kaniko:$PATH TAG = ${CLI_VERSION} } steps { sh echo image fqn=${REGISTRY}/${REPO}/${IMAGE}:${TAG} container ( name: kaniko , shell: /busybox/sh ) { sh #!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:${TAG} /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:latest } } } } }","title":"Pipeline"},{"location":"blogs/teams-automation/#pr-pipeline","text":"Caution The PR Pipeline example builds upon the GitHub API, if you're not using GItHub, you will have to figure out another way to make the PR.","title":"PR Pipeline"},{"location":"blogs/teams-automation/#tools-used","text":"yq : commandline tool for processing Yaml files jq commandline tool for pressing Json files Kustomize templating tool for Kubernetes Yaml, as of Kubernetes 1 . 13 , this is part of the Client (note, your server can be older, don't worry!) Hub commandline client for GitHub","title":"Tools Used"},{"location":"blogs/teams-automation/#repository-layout","text":"folder: team - master - template with file simple . json folder: namespace - creation with folder: kustomize this contains the Kustomize configuration Simple.json This is a template for the team JSON definition. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { version : 1 , data : { name : NAME , displayName : DISPLAY_NAME , provisioningRecipe : RECIPE , members : [ { id : ADMINS , roles : [ TEAM_ADMIN ] }, { id : MEMBERS , roles : [ TEAM_MEMBER ] }, { id : GUESTS , roles : [ TEAM_GUEST ] } ], icon : { name : ICON , color : HEX_COLOR } } }","title":"Repository Layout"},{"location":"blogs/teams-automation/#kustomize-configuration","text":"Kustomize is a tool for template Kubernetes YAML definitions, which is what we need here. However, only for the namespace creation configuration. So if you don't want to do that, you can skip this. The Kustomize configuration has two parts, a folder called team - example with a kustomization . yaml . This will be what we configure to generate a new yaml definition. The main template is in the folder base , where the entrypoint will be again kustomization . yaml . This time, the kustomization . yaml will link to all the template files we need. As posting all these yaml files again is a bit much, I'll link to my example repo. Feel free to fork it instead: cb-team-gitops-template configmap.yaml : the Jenkins Agent ConfigMap namespace.yaml : the new namespace resource-quota.yaml : resource quota's for the namespace role-binding-cjoc.yaml : a role binding for the CJOC ServiceAccount, so it create create the new Master in the new namespace role-binding.yaml : the role binding for the jenkins ServiceAccount, which allows the new Master to create and manage Pods (for PodTemplates) role-cjoc.yaml : the role for CJOC for the ability to create a Master in the new Namspace role.yaml : the role for the jenkins ServiceAccount for the new Master service-account.yaml : the ServiceAccount, jenkins , used by the new Master","title":"Kustomize Configuration"},{"location":"blogs/teams-automation/#pipeline_1","text":"The Pipeline will do the following: capture input parameters to be used to customize the Team Master update the Kustomize template to make sure every resource is correct for the new namespace ( teams - name of team ) execute Kustomize to generate a single yaml file that defines the configuration for the new Team Masters' namespace process the simple . json to generate a team . json file for the new Team Master for use with the Jenkins CLI checkout your GIT_REPO that contains your team definitions create a new PR to your GIT_REPO for the new team Jenkinsfile Variables to update: GIT_REPO : the GitHub repository in which the Team Definitions are stored RESET_NAMESPACE : the namespace Operations Center should use as default 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 pipeline { agent { kubernetes { label team-automation yaml kind: Pod spec: containers: - name: hub image: caladreas/hub command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 150m limits: memory: 50Mi cpu: 150m - name: kubectl image: bitnami/kubectl:latest command: [ cat ] tty: true securityContext: runAsUser: 1000 fsGroup: 1000 resources: requests: memory: 50Mi cpu: 100m limits: memory: 150Mi cpu: 200m - name: yq image: mikefarah/yq command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m - name: jq image: colstrom/jq command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m } } libraries { lib ( github.com/joostvdg/jpl-core ) } options { disableConcurrentBuilds () // we don t want more than one at a time checkoutToSubdirectory templates // we need to do two checkouts buildDiscarder logRotator ( artifactDaysToKeepStr: , artifactNumToKeepStr: , daysToKeepStr: 5 , numToKeepStr: 5 ) // always clean up } environment { envGitInfo = RESET_NAMESPACE = jx-production TEAM_BASE_NAME = NAMESPACE_TO_CREATE = DISPLAY_NAME = TEAM_RECIPE = ICON = ICON_COLOR_CODE = ADMINS_ROLE = MEMBERS_ROLE = GUESTS_ROLE = RECORD_LOC = GIT_REPO = } stages { stage ( Team Details ) { input { message Please enter the team details. ok Looks good, proceed parameters { string ( name: Name , defaultValue: hex , description: Please specify a team name ) string ( name: DisplayName , defaultValue: Hex , description: Please specify a team display name ) choice choices: [ joostvdg , basic , java-web ], description: Please select a Team Recipe , name: TeamRecipe choice choices: [ anchor , bear , bowler-hat , briefcase , bug , calculator , calculatorcart , clock , cloud , cloudbees , connect , dollar-bill , dollar-symbol , file , flag , flower-carnation , flower-daisy , help , hexagon , high-heels , jenkins , key , marker , monocle , mustache , office , panther , paw-print , teacup , tiger , truck ], description: Please select an Icon , name: Icon string ( name: IconColorCode , defaultValue: #CCCCCC , description: Please specify a valid html hexcode for the color (https://htmlcolorcodes.com/) ) string ( name: Admins , defaultValue: Catmins , description: Please specify a groupid or userid for the TEAM_ADMIN role ) string ( name: Members , defaultValue: Pirates , description: Please specify a groupid or userid for the TEAM_MEMBER role ) string ( name: Guests , defaultValue: Continental , description: Please specify a groupid or userid for the TEAM_GUEST role ) } } steps { println Name=${Name} println DisplayName=${DisplayName} println TeamRecipe=${TeamRecipe} println Icon=${Icon} println IconColorCode=${IconColorCode} println Admins=${Admins} println Members=${Members} println Guests=${Guests} script { TEAM_BASE_NAME = ${Name} NAMESPACE_TO_CREATE = cb-teams-${Name} DISPLAY_NAME = ${DisplayName} TEAM_RECIPE = ${TeamRecipe} ICON = ${Icon} ICON_COLOR_CODE = ${IconColorCode} ADMINS_ROLE = ${Admins} MEMBERS_ROLE = ${Members} GUESTS_ROLE = ${Guests} RECORD_LOC = templates/teams/${Name} sh mkdir -p ${RECORD_LOC} } } } stage ( Create Team Config ) { environment { BASE = templates/namespace-creation/kustomize NAMESPACE = ${NAMESPACE_TO_CREATE} RECORD_LOC = templates/teams/${TEAM_BASE_NAME} } parallel { stage ( Namespace ) { steps { container ( yq ) { sh yq w -i ${BASE}/base/role-binding.yaml subjects[0].namespace ${NAMESPACE} sh yq w -i ${BASE}/base/namespace.yaml metadata.name ${NAMESPACE} sh yq w -i ${BASE}/team-example/kustomization.yaml namespace ${NAMESPACE} } container ( kubectl ) { sh kubectl kustomize ${BASE}/team-example ${RECORD_LOC}/team.yaml cat ${RECORD_LOC}/team.yaml } } } stage ( Team Master JSON ) { steps { container ( jq ) { sh jq \\ .data.name = ${TEAM_BASE_NAME} |\\ .data.displayName = ${DISPLAY_NAME} |\\ .data.provisioningRecipe = ${TEAM_RECIPE} |\\ .data.icon.name = ${ICON} |\\ .data.icon.color = ${ICON_COLOR_CODE} |\\ .data.members[0].id = ${ADMINS_ROLE} |\\ .data.members[1].id = ${MEMBERS_ROLE} |\\ .data.members[2].id = ${GUESTS_ROLE} \\ templates/team-master-template/simple.json ${RECORD_LOC}/team.json } sh cat ${RECORD_LOC}/team.json } } } } stage ( Create PR ) { when { branch master } environment { RECORD_OLD_LOC = templates/teams/${TEAM_BASE_NAME} RECORD_LOC = teams/${TEAM_BASE_NAME} PR_CHANGE_NAME = add_team_${TEAM_BASE_NAME} } steps { container ( hub ) { dir ( cb-team-gitops ) { script { envGitInfo = git ${GIT_REPO} } sh git checkout -b ${PR_CHANGE_NAME} sh ls -lath ../${RECORD_OLD_LOC} sh cp -R ../${RECORD_OLD_LOC} ./teams sh ls -lath sh ls -lath teams/ gitRemoteConfigByUrl ( envGitInfo . GIT_URL , githubtoken_token ) // must be a API Token ONLY - secret text sh git config --global user.email jenkins@jenkins.io git config --global user.name Jenkins git add ${RECORD_LOC} git status git commit -m add team ${TEAM_BASE_NAME} git push origin ${PR_CHANGE_NAME} // has to be indented like that, else the indents will be in the pr description writeFile encoding: UTF-8 , file: pr-info.md , text: Add ${TEAM_BASE_NAME} \\n This pr is automatically generated via CloudBees.\\\\n \\n The job: ${env.JOB_URL} // TODO: unfortunately, environment {} s credentials have fixed environment variable names // TODO: in this case, they need to be EXACTLY GITHUB_PASSWORD and GITHUB_USER script { withCredentials ([ usernamePassword ( credentialsId: githubtoken , passwordVariable: GITHUB_PASSWORD , usernameVariable: GITHUB_USER )]) { sh set +x hub pull-request --force -F pr-info.md -l ${TEAM_BASE_NAME} --no-edit } } } } } } } }","title":"Pipeline"},{"location":"blogs/teams-automation/#main-pipeline","text":"The main Pipeline should be part of a repository. The Repository should look like this: recipes (folder) recipes . json - current complete list of CloudBees Core Team Recipes definition teams (folder) folder per team team . json - CloudBees Core Team definition team . yaml - Kubernetes YAML definition of the namespace and all its resources","title":"Main Pipeline"},{"location":"blogs/teams-automation/#process","text":"The pipeline can be a bit hard to grasp, so let me break it down into individual steps. We have the following stages: Create Team - which is broken into sub-stages via the sequential stages feature . * Parse Changelog * Create Namespace * Change OC Namespace * Create Team Master Test CLI Connection Update Team Recipes","title":"Process"},{"location":"blogs/teams-automation/#notable-statements","text":"disableConcurrentBuilds We change the namespace of Operation Center to a different value only for the duration of creating this master. This is something that should probably be part of the Team Master creation, but as it is a single configuration option for all that Operation Center does, we need to be careful. By ensuring we only run one build concurrently, we reduce the risk of this blowing up in our face. 1 2 3 options { disableConcurrentBuilds () } when { } The When Directive allows us to creating effective conditions for when a stage should be executed. The snippet below shows the use of a combination of both the branch and changeset built-in filters. changeset looks at the commit being build and validates that there was a change in that file path. 1 when { allOf { branch master ; changeset teams/**/team.* } } post { always { } } The Post Directive allows us to run certain commands after the main pipeline has run depending on the outcome (compared or not to the previous outcome). In this case, we want to make sure we reset the namespace used by Operations Center to the original value. By using post { always {} } , it will ALWAYS run, regardless of the status of the pipeline. So we should be safe. 1 2 3 4 5 6 7 post { always { container ( cli ) { sh ${CLI} ${CREDS} groovy = resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE} } } } stages { stage { parallel { stage() { stages { stage { Oke, you might've noticed this massive indenting depth and probably have some questions. By combining sequential stages with parallel stages we can create a set of stages that will be executed in sequence but can be controlled by a single when {} statement whether or not they get executed. This prevents mistakes being made in the condition and accidentally running one or other but not all the required steps. 1 2 3 4 5 6 stages { stage ( Create Team ) { parallel { stage ( Main ) { stages { stage ( Parse Changelog ) { changetSetData container('jpb') {} Alright, so even if we know a team was added in / teams / team - name , we still don't know the following two things: 1) what is the name of this team, 2) was this team changed or deleted? So we have to process the changelog to be able to answer these questions as well. There are different ways of getting the changelog and parsing it. I've written one you can do on ANY machine, regardless of Jenkins by leveraging Git and my own custom binary ( jpb - Jenkins Pipeline Binary). The code for my binary is at GitHub: github.com/joostvdg/jpb . An alternative approach is described by CloudBees Support here , which leverages Jenkins groovy powers. 1 2 3 4 5 6 7 COMMIT_INFO = ${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT} def changeSetData = sh returnStdout: true , script: git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO} changeSetData = changeSetData . replace ( \\n , \\\\n ) container ( jpb ) { changeSetFolders = sh returnStdout: true , script: /usr/bin/jpb/bin/jpb GitChangeListToFolder ${changeSetData} teams/ changeSetFolders = changeSetFolders . split ( , ) }","title":"Notable Statements"},{"location":"blogs/teams-automation/#files","text":"recipes.json The default Team Recipes that ships with CloudBees Core Modern. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { version : 1 , data : [{ name : basic , displayName : Basic , description : The minimalistic setup. , plugins : [ bluesteel-master , cloudbees-folders-plus , cloudbees-jsync-archiver , cloudbees-monitoring , cloudbees-nodes-plus , cloudbees-ssh-slaves , cloudbees-support , cloudbees-workflow-template , credentials-binding , email-ext , git , git-client , github-branch-source , github-organization-folder , infradna-backup , ldap , mailer , operations-center-analytics-reporter , operations-center-cloud , pipeline-model-definition , ssh-credentials , wikitext , workflow-aggregator , workflow-cps-checkpoint ], default : true }, { name : java-web , displayName : Java Web Development , description : The essential tools to build, release and deploy Java Web applications including integration with Maven, Gradle and Node JS. , plugins : [ bluesteel-master , cloudbees-folders-plus , cloudbees-jsync-archiver , cloudbees-monitoring , cloudbees-nodes-plus , cloudbees-ssh-slaves , cloudbees-support , cloudbees-workflow-template , credentials-binding , email-ext , git , git-client , github-branch-source , github-organization-folder , infradna-backup , ldap , mailer , operations-center-analytics-reporter , operations-center-cloud , pipeline-model-definition , ssh-credentials , wikitext , workflow-aggregator , workflow-cps-checkpoint , config-file-provider , cloudbees-aws-cli , cloudbees-cloudfoundry-cli , findbugs , gradle , jira , junit , nodejs , openshift-cli , pipeline-maven , tasks , warnings ], default : false }] } Jenkinsfile This is the pipeline that will process the commit to the repository and, if it detects a new team is created will apply the changes. Variables to overwrite: GIT_REPO : the https url to the Git Repository your GitOps code/configuration is stored RESET_NAMESPACE : the namespace your Operation Center normally operates in CLI : this command depends on the namespace Operation Center is in ( http : // service name . namespace / cjoc ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 pipeline { agent { kubernetes { label jenkins-agent yaml apiVersion: v1 kind: Pod spec: serviceAccountName: jenkins containers: - name: cli image: caladreas/cbcore-cli:2.176.2.3 imagePullPolicy: Always command: - cat tty: true resources: requests: memory: 50Mi cpu: 150m limits: memory: 50Mi cpu: 150m - name: kubectl image: bitnami/kubectl:latest command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 150Mi cpu: 200m - name: yq image: mikefarah/yq command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m - name: jpb image: caladreas/jpb command: - cat tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m securityContext: runAsUser: 1000 fsGroup: 1000 } } options { disableConcurrentBuilds () buildDiscarder logRotator ( artifactDaysToKeepStr: , artifactNumToKeepStr: , daysToKeepStr: 5 , numToKeepStr: 5 ) } environment { RESET_NAMESPACE = cloudbees-core CREDS = credentials ( jenkins-api ) CLI = java -jar /usr/bin/jenkins-cli.jar -noKeyAuth -s http://cjoc.cloudbees-core/cjoc -auth COMMIT_INFO = TEAM = GIT_REPO = } stages { stage ( Create Team ) { when { allOf { branch master ; changeset teams/**/team.* } } parallel { stage ( Main ) { stages { stage ( Parse Changelog ) { steps { // Alternative approach: https://support.cloudbees.com/hc/en-us/articles/217630098-How-to-access-Changelogs-in-a-Pipeline-Job- // However, that runs on the master, JPB runs in an agent! script { scmVars = git ${GIT_REPO} COMMIT_INFO = ${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT} def changeSetData = sh returnStdout: true , script: git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO} changeSetData = changeSetData . replace ( \\n , \\\\n ) container ( jpb ) { changeSetFolders = sh returnStdout: true , script: /usr/bin/jpb/bin/jpb GitChangeListToFolder ${changeSetData} teams/ changeSetFolders = changeSetFolders . split ( , ) } if ( changeSetFolders . length 0 ) { TEAM = changeSetFolders [ 0 ] TEAM = TEAM . trim () // to protect against a team being removed def exists = fileExists teams/${TEAM}/team.yaml if (! exists ) { TEAM = } } else { TEAM = } echo Team that changed: |${TEAM}| } } } stage ( Create Namespace ) { when { expression { return ! TEAM . equals ( ) } } environment { NAMESPACE = cb-teams-${TEAM} RECORD_LOC = teams/${TEAM} } steps { container ( kubectl ) { sh cat ${RECORD_LOC}/team.yaml kubectl apply -f ${RECORD_LOC}/team.yaml } } } stage ( Change OC Namespace ) { when { expression { return ! TEAM . equals ( ) } } environment { NAMESPACE = cb-teams-${TEAM} } steps { container ( cli ) { sh echo ${NAMESPACE} script { def response = sh encoding: UTF-8 , label: create team , returnStatus: true , script: ${CLI} ${CREDS} groovy = resources/bootstrap/configure-oc-namespace.groovy ${NAMESPACE} println Response: ${response} } } } } stage ( Create Team Master ) { when { expression { return ! TEAM . equals ( ) } } environment { TEAM_NAME = ${TEAM} } steps { container ( cli ) { println TEAM_NAME=${TEAM_NAME} sh ls -lath sh ls -lath teams/ script { def response = sh encoding: UTF-8 , label: create team , returnStatus: true , script: ${CLI} ${CREDS} teams ${TEAM_NAME} --put teams/${TEAM_NAME}/team.json println Response: ${response} } } } } } } } } stage ( Test CLI Connection ) { steps { container ( cli ) { script { def response = sh encoding: UTF-8 , label: retrieve version , returnStatus: true , script: ${CLI} ${CREDS} version println Response: ${response} } } } } stage ( Update Team Recipes ) { when { allOf { branch master ; changeset recipes/recipes.json } } steps { container ( cli ) { sh ls -lath sh ls -lath recipes/ script { def response = sh encoding: UTF-8 , label: update team recipe , returnStatus: true , script: ${CLI} ${CREDS} team-creation-recipes --put recipes/recipes.json println Response: ${response} } } } } } post { always { container ( cli ) { sh ${CLI} ${CREDS} groovy = resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE} } } } }","title":"Files"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/","text":"Alerts With Alertmanager List Of Alerts health score 0.95 ingress performance vm heap usage ratio 80% file descriptor 75% job queue 10 over x minutes job success ratio 50% master executor count 0 good http request ratio 90% offline nodes 5 over 30 minutes healtcheck duration 0.002 plugin updates available 10 An alert that triggers if any of the health reports are failing An alert that triggers if the file descriptor usage on the master goes above 80% vm . file . descriptor . ratio - vm_file_descriptor_ratio An alert that triggers if the JVM heap memory usage is over 80% for more than a minute vm . memory . heap . usage - vm_memory_heap_usage An alert that triggers if the 5 minute average of HTTP/404 responses goes above 10 per minute for more than five minutes http . responseCodes . badRequest - http_responseCodes_badRequest Alert Manager Configuration We can configure Alert Manager via the Prometheus Helm Chart. All the configuration elements below are part of the prom - values . yaml we used to when installing Prometheus via Helm. Get Slack Endpoint There are many ways to get the Alerts out, for all options you can read the Prometheus documentation . In this guide, I've chosen to use slack, as I find it convenient personally. Slack has a guide on creating webhooks , once you've created an App you can retrieve an endpoint which you can use directly in the Alertmanager configuration. Alerts Configuration We configure the alerts within Prometheus itself via a ConfigMap . We configure the body of the alert configuration file via serverFiles . alerts . groups and serverFiles . rules . We can have a list of rules and a list of groups of rules. For more information how you can configure these rules, consult the Prometheus documentation . 1 2 3 4 5 serverFiles : alerts : groups : # alerts come here rules : {} Alert Example Below is an example of an Alert. We have the following fields: alert : the name of the alert expr : the query that should evaluate to true or false for (optional): duration of the expressions equating to true before it fires labels (optional): you can add key-value pairs to encode more information on the alert, you can use this to select different receiver (e.g., email vs. slack, or different slack channels) annotations : we're expected to fill in summary and description as shown below, they will header and body of the alert 1 2 3 4 5 6 7 8 - alert : JenkinsTooManyJobsQueued expr : sum(jenkins_queue_size_value) 5 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} too many jobs queued description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs stuck in the queue Alertmanager Configuration We use Alertmanager for what to do with alerts once they happen. We configure this in the same prom - values . yaml file, in this under alertmanagerFiles . alertmanager . yml . We can create different routes that match on labels or other values. For simplicity sake - this guide is not on Alertmanager's capabilities - we stick to the most straightforward example without any such matching or grouping. For more information on configuring routes, please read the Prometheus configuration documentation . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 alertmanagerFiles : alertmanager.yml : global : {} route : group_by : [ alertname , app_kubernetes_io_instance ] receiver : default receivers : - name : default slack_configs : - api_url : REPLACE_WITH_YOUR_SLACK_API_ENDPOINT username : Alertmanager channel : #notify send_resolved : true title : {{ .CommonAnnotations.summary }} text : {{ .CommonAnnotations.description }} {{ .CommonLabels.app_kubernetes_io_instance}} title_link : http://my-prometheus.com/alerts Group Alerts You can group alerts if they are similar or the same with different trigger values (warning vs critical). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 serverFiles : alerts : groups : - name : healthcheck rules : - alert : JenkinsHealthScoreToLow # alert info - alert : JenkinsTooSlowHealthCheck # alert info - name : jobs rules : - alert : JenkinsTooManyJobsQueued # alert info - alert : JenkinsTooManyJobsStuckInQueue # alert info The Alerts Behold, my awesome - eh, simple example - alerts. These are by no means the best alerts to create and are by no means alerts you should directly put into production. Please see them as examples to learn from! Caution One thing to note especially, the values for the exp and for are generally set very low. This is intentional, so they are easy to copy past and test. They should be relatively easy to trigger so you can learn about the relationship between the situation in your master and the alert firing. Too Many Jobs Queued If there are too many Jobs queued in the Jenkins Master. This event fires if there's more than 10 jobs in the queue for at least 10 minutes. 1 2 3 4 5 6 7 8 - alert : JenkinsTooManyJobsQueued expr : sum(jenkins_queue_size_value) 10 for : 10m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} too many jobs queued description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs stuck in the queue Jobs Stuck In Queue Sometimes Jobs depend on other Jobs, which means they're not just in the queue, they're stuck in the queue. 1 2 3 4 5 6 7 8 - alert : JenkinsTooManyJobsStuckInQueue expr : sum(jenkins_queue_stuck_value) by (app_kubernetes_io_instance) 5 for : 5m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} too many jobs queued description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs in queue Jobs Waiting Too Long To Start If Jobs are generally waiting a long time to start, waiting for a build agent to be available or otherwise, we want to know. This value is not very useful - although not completely useless - if you only have PodTemplates as build agents. When you use PodTemplates, this value is the time between the job being scheduled and when the Pod is scheduled in Kubernetes. 1 2 3 4 5 6 7 8 - alert : JenkinsWaitingTooMuchOnJobStart expr : sum (jenkins_job_waiting_duration) by (app_kubernetes_io_instance) 0.05 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} waits too long for jobs description : {{ $labels.app_kubernetes_io_instance }} is waiting on average {{ $value }} seconds to start a job health score 1 By default, each Jenkins Master has a health check consisting out of four values. Some plugins will add an entry, such as the CloudBees ElasticSearch Reporter for CloudBees Core. This values range from 0-1, and likely will show 0 . 25 , 0 . 50 , 0 . 75 and 1 as values. 1 2 3 4 5 6 7 8 - alert : JenkinsHealthScoreToLow expr : sum(jenkins_health_check_score) by (app_kubernetes_io_instance) 1 for : 5m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has a to low health score description : {{ $labels.app_kubernetes_io_instance }} a health score lower than 100% Ingress Too Slow This alert looks at the ingress controller request duration. It fires if the request duration in 0.25 seconds or faster is not achieved for the 95% percentile. 1 2 3 4 5 6 7 8 - alert : AppTooSlow expr : sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{le= 0.25 }[5m])) by (ingress) / sum(rate(nginx_ingress_controller_request_duration_seconds_count[5m])) by (ingress) 0.95 for : 5m labels : severity : notify annotations : summary : Application - {{ $labels.ingress }} - is too slow description : {{ $labels.ingress }} - More then 5% of requests are slower than 0.25s HTTP Requests Too Slow These are the HTTP requests in Jenkins' webserver itself. We should hold this by must stricter standards than the Ingress controller - which goes through many more layers. 1 2 3 4 5 6 7 8 - alert : JenkinsTooSlow expr : sum(http_requests{quantile= 0.99 } ) by (app_kubernetes_io_instance) 1 for : 3m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} is too slow description : {{ $labels.app_kubernetes_io_instance }} More then 1% of requests are slower than 1s (request time: {{ $value }}) Too Many Plugin Updates I always prefer having my instance up-to-date, don't you? So why not send an alert if there's more than X number of plugins waiting for an update. 1 2 3 4 5 6 7 8 - alert : JenkinsTooManyPluginsNeedUpate expr : sum(jenkins_plugins_withUpdate) by (app_kubernetes_io_instance) 3 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} too many plugins updates description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} plugins that require an update File Descriptor Ratio 40% According to CloudBees' documentation, the File Descriptor Ratio should not exceed 40%. Warning I don't truly know the correct value level of this metric. So wether this should be 0 . 0040 or 0 . 40 I'm not sure. Also, does this make sense in Containers with remote storage? So before you put this in production, please re-evaluate this! 1 2 3 4 5 6 7 8 - alert : JenkinsToManyOpenFiles expr : sum(vm_file_descriptor_ratio) by (app_kubernetes_io_instance) 0.040 for : 5m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has a to many open files description : {{ $labels.app_kubernetes_io_instance }} instance has used {{ $value }} of available open files Job Success Ratio 50% Please, please do not use Job success ratios to punish people. But if it is at all possible - which it almost certainly is - keep a respectable level of success. When practicing Continuous Integration, a broken build is a stop the world event, fix it before moving on. 100% success rate should be strived for. It is ok, not to achieve it, yet, one should be as close as possible and not let broken builds rot. 1 2 3 4 5 6 7 8 - alert : JenkinsTooLowJobSuccessRate expr : sum(jenkins_runs_success_total) by (app_kubernetes_io_instance) / sum(jenkins_runs_total_total) by (app_kubernetes_io_instance) 0.5 for : 5m labels : severity : notify annotations : summary : {{$labels.app_kubernetes_io_instance}} has a too low job success rate description : {{$labels.app_kubernetes_io_instance}} instance has less than 50% of jobs being successful Offline nodes 5 over 10 minutes Having nodes offline for quite some time is usually a bad sign. It can be a static agent that can be enabled or reconnect at will, so it isn't bad on its own. Having multiple offline for a long period is likely an issue somewhere, though. 1 2 3 4 5 6 7 8 - alert : JenkinsTooManyOfflineNodes expr : sum(jenkins_node_offline_value) by (app_kubernetes_io_instance) 5 for : 10m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has a too many offline nodes description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} nodes that are offline for some time (5 minutes) healtcheck duration 0.002 The health check within Jenkins is talking to itself. This means it is generally really fast. We should be very very strict here, if Jenkins start having trouble measuring its own health, it is a first sign of trouble. 1 2 3 4 5 6 7 8 9 - alert : JenkinsTooSlowHealthCheck expr : sum(jenkins_health_check_duration{quantile= 0.999 }) by (app_kubernetes_io_instance) 0.001 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} responds too slow to health check description : {{ $labels.app_kubernetes_io_instance }} is responding too slow to the regular health check GC ThroughPut Too Low Ok, here I am on thin ice. I'm not a JVM expert, so this is just an inspiration. I do not know what would be a reasonable value for triggering an alert here. I'd say, test it! 1 2 3 4 5 6 7 8 - alert : JenkinsTooManyPluginsNeedUpate expr : 1 - sum(vm_gc_G1_Young_Generation_time)by (app_kubernetes_io_instance) / sum (vm_uptime_milliseconds) by (app_kubernetes_io_instance) 0.99 for : 30m labels : severity : notify annotations : summary : {{ $labels.instance }} too low GC throughput description : {{ $labels.instance }} has too low Garbage Collection throughput vm heap usage ratio 70% According to the CloudBees guide on tuning the JVM - which redirects to Oracle - the ration of JVM Heap memory usage should not exceed about 60%. So if we get over 70% for quite some time, expect trouble. As with any of these values, please do not take my word on it, and understand it yourself. 1 2 3 4 5 6 7 8 - alert : JenkinsVMMemoryRationTooHigh expr : sum(vm_memory_heap_usage) by (app_kubernetes_io_instance) 0.70 for : 3m labels : severity : notify annotations : summary : {{$labels.app_kubernetes_io_instance}} too high memory ration description : {{$labels.app_kubernetes_io_instance}} has a too high VM memory ration Uptime Less Than Two Hours I absolutely love servers that have excellent uptime. Running services in Containers makes that a thing of the past, such a shame. Still, I'd like my applications - such as Jenkins - to be up for reasonable lengths of time. In this case we can get notifications on Masters that have restart - for example, when OOMKilled by Kubernetes. We also get an alert when a new Master is created, which if there's selfservice involved is a nice bonus. 1 2 3 4 5 6 7 8 - alert : JenkinsNewOrRestarted expr : sum(vm_uptime_milliseconds) by (app_kubernetes_io_instance) / 3600000 2 for : 3m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has low uptime description : {{ $labels.app_kubernetes_io_instance }} has low uptime and was either restarted or is a new instance (uptime: {{ $value }} hours) Full Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 server : ingress : enabled : true annotations : ingress.kubernetes.io/ssl-redirect : false nginx.ingress.kubernetes.io/ssl-redirect : false resources : limits : cpu : 100m memory : 1000Mi requests : cpu : 10m memory : 500Mi alertmanager : ingress : enabled : true annotations : ingress.kubernetes.io/ssl-redirect : false nginx.ingress.kubernetes.io/ssl-redirect : false resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi kubeStateMetrics : resources : limits : cpu : 10m memory : 50Mi requests : cpu : 5m memory : 25Mi nodeExporter : resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi pushgateway : resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi serverFiles : alerts : groups : - name : jobs rules : - alert : JenkinsTooManyJobsQueued expr : sum(jenkins_queue_size_value) 5 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} too many jobs queued description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs stuck in the queue - alert : JenkinsTooManyJobsStuckInQueue expr : sum(jenkins_queue_stuck_value) by (app_kubernetes_io_instance) 5 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} too many jobs queued description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs in queue - alert : JenkinsWaitingTooMuchOnJobStart expr : sum (jenkins_job_waiting_duration) by (app_kubernetes_io_instance) 0.05 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} waits too long for jobs description : {{ $labels.app_kubernetes_io_instance }} is waiting on average {{ $value }} seconds to start a job - alert : JenkinsTooLowJobSuccessRate expr : sum(jenkins_runs_success_total) by (app_kubernetes_io_instance) / sum(jenkins_runs_total_total) by (app_kubernetes_io_instance) 0.60 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has a too low job success rate description : {{ $labels.app_kubernetes_io_instance }} instance has {{ $value }}% of jobs being successful - name : uptime rules : - alert : JenkinsNewOrRestarted expr : sum(vm_uptime_milliseconds) by (app_kubernetes_io_instance) / 3600000 2 for : 3m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has low uptime description : {{ $labels.app_kubernetes_io_instance }} has low uptime and was either restarted or is a new instance (uptime: {{ $value }} hours) - name : plugins rules : - alert : JenkinsTooManyPluginsNeedUpate expr : sum(jenkins_plugins_withUpdate) by (app_kubernetes_io_instance) 3 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} too many plugins updates description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} plugins that require an update - name : jvm rules : - alert : JenkinsToManyOpenFiles expr : sum(vm_file_descriptor_ratio) by (app_kubernetes_io_instance) 0.040 for : 5m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has a to many open files description : {{ $labels.app_kubernetes_io_instance }} instance has used {{ $value }} of available open files - alert : JenkinsVMMemoryRationTooHigh expr : sum(vm_memory_heap_usage) by (app_kubernetes_io_instance) 0.70 for : 3m labels : severity : notify annotations : summary : {{$labels.app_kubernetes_io_instance}} too high memory ration description : {{$labels.app_kubernetes_io_instance}} has a too high VM memory ration - alert : JenkinsTooManyPluginsNeedUpate expr : 1 - sum(vm_gc_G1_Young_Generation_time)by (app_kubernetes_io_instance) / sum (vm_uptime_milliseconds) by (app_kubernetes_io_instance) 0.99 for : 30m labels : severity : notify annotations : summary : {{ $labels.instance }} too low GC throughput description : {{ $labels.instance }} has too low Garbage Collection throughput - name : web rules : - alert : JenkinsTooSlow expr : sum(http_requests{quantile= 0.99 } ) by (app_kubernetes_io_instance) 1 for : 3m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} is too slow description : {{ $labels.app_kubernetes_io_instance }} More then 1% of requests are slower than 1s (request time: {{ $value }}) - alert : AppTooSlow expr : sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{le= 0.25 }[5m])) by (ingress) / sum(rate(nginx_ingress_controller_request_duration_seconds_count[5m])) by (ingress) 0.95 for : 5m labels : severity : notify annotations : summary : Application - {{ $labels.ingress }} - is too slow description : {{ $labels.ingress }} - More then 5% of requests are slower than 0.25s - name : healthcheck rules : - alert : JenkinsHealthScoreToLow expr : sum(jenkins_health_check_score) by (app_kubernetes_io_instance) 1 for : 5m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has a to low health score description : {{ $labels.app_kubernetes_io_instance }} a health score lower than 100% - alert : JenkinsTooSlowHealthCheck expr : sum(jenkins_health_check_duration{quantile= 0.999 }) by (app_kubernetes_io_instance) 0.001 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} responds too slow to health check description : {{ $labels.app_kubernetes_io_instance }} is responding too slow to the regular health check - name : nodes rules : - alert : JenkinsTooManyOfflineNodes expr : sum(jenkins_node_offline_value) by (app_kubernetes_io_instance) 3 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has a too many offline nodes description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} nodes that are offline for some time (5 minutes) alertmanagerFiles : alertmanager.yml : global : {} route : group_by : [ alertname , app_kubernetes_io_instance ] receiver : default receivers : - name : default slack_configs : - api_url : REPLACE_WITH_YOUR_SLACK_API_URL username : Alertmanager channel : #notify send_resolved : true title : {{ .CommonAnnotations.summary }} text : {{ .CommonAnnotations.description }} {{ .CommonLabels.app_kubernetes_io_instance}} title_link : http://my-prometheus.com/alerts","title":"Alerts"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#alerts-with-alertmanager","text":"","title":"Alerts With Alertmanager"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#list-of-alerts","text":"health score 0.95 ingress performance vm heap usage ratio 80% file descriptor 75% job queue 10 over x minutes job success ratio 50% master executor count 0 good http request ratio 90% offline nodes 5 over 30 minutes healtcheck duration 0.002 plugin updates available 10 An alert that triggers if any of the health reports are failing An alert that triggers if the file descriptor usage on the master goes above 80% vm . file . descriptor . ratio - vm_file_descriptor_ratio An alert that triggers if the JVM heap memory usage is over 80% for more than a minute vm . memory . heap . usage - vm_memory_heap_usage An alert that triggers if the 5 minute average of HTTP/404 responses goes above 10 per minute for more than five minutes http . responseCodes . badRequest - http_responseCodes_badRequest","title":"List Of Alerts"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#alert-manager-configuration","text":"We can configure Alert Manager via the Prometheus Helm Chart. All the configuration elements below are part of the prom - values . yaml we used to when installing Prometheus via Helm.","title":"Alert Manager Configuration"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#get-slack-endpoint","text":"There are many ways to get the Alerts out, for all options you can read the Prometheus documentation . In this guide, I've chosen to use slack, as I find it convenient personally. Slack has a guide on creating webhooks , once you've created an App you can retrieve an endpoint which you can use directly in the Alertmanager configuration.","title":"Get Slack Endpoint"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#alerts-configuration","text":"We configure the alerts within Prometheus itself via a ConfigMap . We configure the body of the alert configuration file via serverFiles . alerts . groups and serverFiles . rules . We can have a list of rules and a list of groups of rules. For more information how you can configure these rules, consult the Prometheus documentation . 1 2 3 4 5 serverFiles : alerts : groups : # alerts come here rules : {}","title":"Alerts Configuration"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#alert-example","text":"Below is an example of an Alert. We have the following fields: alert : the name of the alert expr : the query that should evaluate to true or false for (optional): duration of the expressions equating to true before it fires labels (optional): you can add key-value pairs to encode more information on the alert, you can use this to select different receiver (e.g., email vs. slack, or different slack channels) annotations : we're expected to fill in summary and description as shown below, they will header and body of the alert 1 2 3 4 5 6 7 8 - alert : JenkinsTooManyJobsQueued expr : sum(jenkins_queue_size_value) 5 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} too many jobs queued description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs stuck in the queue","title":"Alert Example"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#alertmanager-configuration","text":"We use Alertmanager for what to do with alerts once they happen. We configure this in the same prom - values . yaml file, in this under alertmanagerFiles . alertmanager . yml . We can create different routes that match on labels or other values. For simplicity sake - this guide is not on Alertmanager's capabilities - we stick to the most straightforward example without any such matching or grouping. For more information on configuring routes, please read the Prometheus configuration documentation . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 alertmanagerFiles : alertmanager.yml : global : {} route : group_by : [ alertname , app_kubernetes_io_instance ] receiver : default receivers : - name : default slack_configs : - api_url : REPLACE_WITH_YOUR_SLACK_API_ENDPOINT username : Alertmanager channel : #notify send_resolved : true title : {{ .CommonAnnotations.summary }} text : {{ .CommonAnnotations.description }} {{ .CommonLabels.app_kubernetes_io_instance}} title_link : http://my-prometheus.com/alerts","title":"Alertmanager Configuration"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#group-alerts","text":"You can group alerts if they are similar or the same with different trigger values (warning vs critical). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 serverFiles : alerts : groups : - name : healthcheck rules : - alert : JenkinsHealthScoreToLow # alert info - alert : JenkinsTooSlowHealthCheck # alert info - name : jobs rules : - alert : JenkinsTooManyJobsQueued # alert info - alert : JenkinsTooManyJobsStuckInQueue # alert info","title":"Group Alerts"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#the-alerts","text":"Behold, my awesome - eh, simple example - alerts. These are by no means the best alerts to create and are by no means alerts you should directly put into production. Please see them as examples to learn from! Caution One thing to note especially, the values for the exp and for are generally set very low. This is intentional, so they are easy to copy past and test. They should be relatively easy to trigger so you can learn about the relationship between the situation in your master and the alert firing.","title":"The Alerts"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#too-many-jobs-queued","text":"If there are too many Jobs queued in the Jenkins Master. This event fires if there's more than 10 jobs in the queue for at least 10 minutes. 1 2 3 4 5 6 7 8 - alert : JenkinsTooManyJobsQueued expr : sum(jenkins_queue_size_value) 10 for : 10m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} too many jobs queued description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs stuck in the queue","title":"Too Many Jobs Queued"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#jobs-stuck-in-queue","text":"Sometimes Jobs depend on other Jobs, which means they're not just in the queue, they're stuck in the queue. 1 2 3 4 5 6 7 8 - alert : JenkinsTooManyJobsStuckInQueue expr : sum(jenkins_queue_stuck_value) by (app_kubernetes_io_instance) 5 for : 5m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} too many jobs queued description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs in queue","title":"Jobs Stuck In Queue"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#jobs-waiting-too-long-to-start","text":"If Jobs are generally waiting a long time to start, waiting for a build agent to be available or otherwise, we want to know. This value is not very useful - although not completely useless - if you only have PodTemplates as build agents. When you use PodTemplates, this value is the time between the job being scheduled and when the Pod is scheduled in Kubernetes. 1 2 3 4 5 6 7 8 - alert : JenkinsWaitingTooMuchOnJobStart expr : sum (jenkins_job_waiting_duration) by (app_kubernetes_io_instance) 0.05 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} waits too long for jobs description : {{ $labels.app_kubernetes_io_instance }} is waiting on average {{ $value }} seconds to start a job","title":"Jobs Waiting Too Long To Start"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#health-score-1","text":"By default, each Jenkins Master has a health check consisting out of four values. Some plugins will add an entry, such as the CloudBees ElasticSearch Reporter for CloudBees Core. This values range from 0-1, and likely will show 0 . 25 , 0 . 50 , 0 . 75 and 1 as values. 1 2 3 4 5 6 7 8 - alert : JenkinsHealthScoreToLow expr : sum(jenkins_health_check_score) by (app_kubernetes_io_instance) 1 for : 5m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has a to low health score description : {{ $labels.app_kubernetes_io_instance }} a health score lower than 100%","title":"health score &lt; 1"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#ingress-too-slow","text":"This alert looks at the ingress controller request duration. It fires if the request duration in 0.25 seconds or faster is not achieved for the 95% percentile. 1 2 3 4 5 6 7 8 - alert : AppTooSlow expr : sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{le= 0.25 }[5m])) by (ingress) / sum(rate(nginx_ingress_controller_request_duration_seconds_count[5m])) by (ingress) 0.95 for : 5m labels : severity : notify annotations : summary : Application - {{ $labels.ingress }} - is too slow description : {{ $labels.ingress }} - More then 5% of requests are slower than 0.25s","title":"Ingress Too Slow"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#http-requests-too-slow","text":"These are the HTTP requests in Jenkins' webserver itself. We should hold this by must stricter standards than the Ingress controller - which goes through many more layers. 1 2 3 4 5 6 7 8 - alert : JenkinsTooSlow expr : sum(http_requests{quantile= 0.99 } ) by (app_kubernetes_io_instance) 1 for : 3m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} is too slow description : {{ $labels.app_kubernetes_io_instance }} More then 1% of requests are slower than 1s (request time: {{ $value }})","title":"HTTP Requests Too Slow"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#too-many-plugin-updates","text":"I always prefer having my instance up-to-date, don't you? So why not send an alert if there's more than X number of plugins waiting for an update. 1 2 3 4 5 6 7 8 - alert : JenkinsTooManyPluginsNeedUpate expr : sum(jenkins_plugins_withUpdate) by (app_kubernetes_io_instance) 3 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} too many plugins updates description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} plugins that require an update","title":"Too Many Plugin Updates"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#file-descriptor-ratio-40","text":"According to CloudBees' documentation, the File Descriptor Ratio should not exceed 40%. Warning I don't truly know the correct value level of this metric. So wether this should be 0 . 0040 or 0 . 40 I'm not sure. Also, does this make sense in Containers with remote storage? So before you put this in production, please re-evaluate this! 1 2 3 4 5 6 7 8 - alert : JenkinsToManyOpenFiles expr : sum(vm_file_descriptor_ratio) by (app_kubernetes_io_instance) 0.040 for : 5m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has a to many open files description : {{ $labels.app_kubernetes_io_instance }} instance has used {{ $value }} of available open files","title":"File Descriptor Ratio &gt; 40%"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#job-success-ratio-50","text":"Please, please do not use Job success ratios to punish people. But if it is at all possible - which it almost certainly is - keep a respectable level of success. When practicing Continuous Integration, a broken build is a stop the world event, fix it before moving on. 100% success rate should be strived for. It is ok, not to achieve it, yet, one should be as close as possible and not let broken builds rot. 1 2 3 4 5 6 7 8 - alert : JenkinsTooLowJobSuccessRate expr : sum(jenkins_runs_success_total) by (app_kubernetes_io_instance) / sum(jenkins_runs_total_total) by (app_kubernetes_io_instance) 0.5 for : 5m labels : severity : notify annotations : summary : {{$labels.app_kubernetes_io_instance}} has a too low job success rate description : {{$labels.app_kubernetes_io_instance}} instance has less than 50% of jobs being successful","title":"Job Success Ratio &lt; 50%"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#offline-nodes-5-over-10-minutes","text":"Having nodes offline for quite some time is usually a bad sign. It can be a static agent that can be enabled or reconnect at will, so it isn't bad on its own. Having multiple offline for a long period is likely an issue somewhere, though. 1 2 3 4 5 6 7 8 - alert : JenkinsTooManyOfflineNodes expr : sum(jenkins_node_offline_value) by (app_kubernetes_io_instance) 5 for : 10m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has a too many offline nodes description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} nodes that are offline for some time (5 minutes)","title":"Offline nodes &gt; 5 over 10 minutes"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#healtcheck-duration-0002","text":"The health check within Jenkins is talking to itself. This means it is generally really fast. We should be very very strict here, if Jenkins start having trouble measuring its own health, it is a first sign of trouble. 1 2 3 4 5 6 7 8 9 - alert : JenkinsTooSlowHealthCheck expr : sum(jenkins_health_check_duration{quantile= 0.999 }) by (app_kubernetes_io_instance) 0.001 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} responds too slow to health check description : {{ $labels.app_kubernetes_io_instance }} is responding too slow to the regular health check","title":"healtcheck duration &gt; 0.002"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#gc-throughput-too-low","text":"Ok, here I am on thin ice. I'm not a JVM expert, so this is just an inspiration. I do not know what would be a reasonable value for triggering an alert here. I'd say, test it! 1 2 3 4 5 6 7 8 - alert : JenkinsTooManyPluginsNeedUpate expr : 1 - sum(vm_gc_G1_Young_Generation_time)by (app_kubernetes_io_instance) / sum (vm_uptime_milliseconds) by (app_kubernetes_io_instance) 0.99 for : 30m labels : severity : notify annotations : summary : {{ $labels.instance }} too low GC throughput description : {{ $labels.instance }} has too low Garbage Collection throughput","title":"GC ThroughPut Too Low"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#vm-heap-usage-ratio-70","text":"According to the CloudBees guide on tuning the JVM - which redirects to Oracle - the ration of JVM Heap memory usage should not exceed about 60%. So if we get over 70% for quite some time, expect trouble. As with any of these values, please do not take my word on it, and understand it yourself. 1 2 3 4 5 6 7 8 - alert : JenkinsVMMemoryRationTooHigh expr : sum(vm_memory_heap_usage) by (app_kubernetes_io_instance) 0.70 for : 3m labels : severity : notify annotations : summary : {{$labels.app_kubernetes_io_instance}} too high memory ration description : {{$labels.app_kubernetes_io_instance}} has a too high VM memory ration","title":"vm heap usage ratio &gt; 70%"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#uptime-less-than-two-hours","text":"I absolutely love servers that have excellent uptime. Running services in Containers makes that a thing of the past, such a shame. Still, I'd like my applications - such as Jenkins - to be up for reasonable lengths of time. In this case we can get notifications on Masters that have restart - for example, when OOMKilled by Kubernetes. We also get an alert when a new Master is created, which if there's selfservice involved is a nice bonus. 1 2 3 4 5 6 7 8 - alert : JenkinsNewOrRestarted expr : sum(vm_uptime_milliseconds) by (app_kubernetes_io_instance) / 3600000 2 for : 3m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has low uptime description : {{ $labels.app_kubernetes_io_instance }} has low uptime and was either restarted or is a new instance (uptime: {{ $value }} hours)","title":"Uptime Less Than Two Hours"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#full-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 server : ingress : enabled : true annotations : ingress.kubernetes.io/ssl-redirect : false nginx.ingress.kubernetes.io/ssl-redirect : false resources : limits : cpu : 100m memory : 1000Mi requests : cpu : 10m memory : 500Mi alertmanager : ingress : enabled : true annotations : ingress.kubernetes.io/ssl-redirect : false nginx.ingress.kubernetes.io/ssl-redirect : false resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi kubeStateMetrics : resources : limits : cpu : 10m memory : 50Mi requests : cpu : 5m memory : 25Mi nodeExporter : resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi pushgateway : resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi serverFiles : alerts : groups : - name : jobs rules : - alert : JenkinsTooManyJobsQueued expr : sum(jenkins_queue_size_value) 5 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} too many jobs queued description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs stuck in the queue - alert : JenkinsTooManyJobsStuckInQueue expr : sum(jenkins_queue_stuck_value) by (app_kubernetes_io_instance) 5 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} too many jobs queued description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs in queue - alert : JenkinsWaitingTooMuchOnJobStart expr : sum (jenkins_job_waiting_duration) by (app_kubernetes_io_instance) 0.05 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} waits too long for jobs description : {{ $labels.app_kubernetes_io_instance }} is waiting on average {{ $value }} seconds to start a job - alert : JenkinsTooLowJobSuccessRate expr : sum(jenkins_runs_success_total) by (app_kubernetes_io_instance) / sum(jenkins_runs_total_total) by (app_kubernetes_io_instance) 0.60 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has a too low job success rate description : {{ $labels.app_kubernetes_io_instance }} instance has {{ $value }}% of jobs being successful - name : uptime rules : - alert : JenkinsNewOrRestarted expr : sum(vm_uptime_milliseconds) by (app_kubernetes_io_instance) / 3600000 2 for : 3m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has low uptime description : {{ $labels.app_kubernetes_io_instance }} has low uptime and was either restarted or is a new instance (uptime: {{ $value }} hours) - name : plugins rules : - alert : JenkinsTooManyPluginsNeedUpate expr : sum(jenkins_plugins_withUpdate) by (app_kubernetes_io_instance) 3 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} too many plugins updates description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} plugins that require an update - name : jvm rules : - alert : JenkinsToManyOpenFiles expr : sum(vm_file_descriptor_ratio) by (app_kubernetes_io_instance) 0.040 for : 5m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has a to many open files description : {{ $labels.app_kubernetes_io_instance }} instance has used {{ $value }} of available open files - alert : JenkinsVMMemoryRationTooHigh expr : sum(vm_memory_heap_usage) by (app_kubernetes_io_instance) 0.70 for : 3m labels : severity : notify annotations : summary : {{$labels.app_kubernetes_io_instance}} too high memory ration description : {{$labels.app_kubernetes_io_instance}} has a too high VM memory ration - alert : JenkinsTooManyPluginsNeedUpate expr : 1 - sum(vm_gc_G1_Young_Generation_time)by (app_kubernetes_io_instance) / sum (vm_uptime_milliseconds) by (app_kubernetes_io_instance) 0.99 for : 30m labels : severity : notify annotations : summary : {{ $labels.instance }} too low GC throughput description : {{ $labels.instance }} has too low Garbage Collection throughput - name : web rules : - alert : JenkinsTooSlow expr : sum(http_requests{quantile= 0.99 } ) by (app_kubernetes_io_instance) 1 for : 3m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} is too slow description : {{ $labels.app_kubernetes_io_instance }} More then 1% of requests are slower than 1s (request time: {{ $value }}) - alert : AppTooSlow expr : sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{le= 0.25 }[5m])) by (ingress) / sum(rate(nginx_ingress_controller_request_duration_seconds_count[5m])) by (ingress) 0.95 for : 5m labels : severity : notify annotations : summary : Application - {{ $labels.ingress }} - is too slow description : {{ $labels.ingress }} - More then 5% of requests are slower than 0.25s - name : healthcheck rules : - alert : JenkinsHealthScoreToLow expr : sum(jenkins_health_check_score) by (app_kubernetes_io_instance) 1 for : 5m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has a to low health score description : {{ $labels.app_kubernetes_io_instance }} a health score lower than 100% - alert : JenkinsTooSlowHealthCheck expr : sum(jenkins_health_check_duration{quantile= 0.999 }) by (app_kubernetes_io_instance) 0.001 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} responds too slow to health check description : {{ $labels.app_kubernetes_io_instance }} is responding too slow to the regular health check - name : nodes rules : - alert : JenkinsTooManyOfflineNodes expr : sum(jenkins_node_offline_value) by (app_kubernetes_io_instance) 3 for : 1m labels : severity : notify annotations : summary : {{ $labels.app_kubernetes_io_instance }} has a too many offline nodes description : {{ $labels.app_kubernetes_io_instance }} has {{ $value }} nodes that are offline for some time (5 minutes) alertmanagerFiles : alertmanager.yml : global : {} route : group_by : [ alertname , app_kubernetes_io_instance ] receiver : default receivers : - name : default slack_configs : - api_url : REPLACE_WITH_YOUR_SLACK_API_URL username : Alertmanager channel : #notify send_resolved : true title : {{ .CommonAnnotations.summary }} text : {{ .CommonAnnotations.description }} {{ .CommonLabels.app_kubernetes_io_instance}} title_link : http://my-prometheus.com/alerts","title":"Full Example"},{"location":"blogs/monitor-jenkins-on-k8s/cloudbees/","text":"CloudBees Core add prometheus plugin to team-recipe update CJOC's Master Provisioning with prometheus annotations 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : apps/v1 kind : StatefulSet spec : template : metadata : annotations : prometheus.io/path : /${name}/prometheus prometheus.io/port : 8080 prometheus.io/scrape : true labels : app.kubernetes.io/component : Managed-Master app.kubernetes.io/instance : ${name} app.kubernetes.io/managed-by : CloudBees-Core-Cloud-Operations-Center app.kubernetes.io/name : ${name}","title":"CloudBees"},{"location":"blogs/monitor-jenkins-on-k8s/cloudbees/#cloudbees-core","text":"add prometheus plugin to team-recipe update CJOC's Master Provisioning with prometheus annotations 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : apps/v1 kind : StatefulSet spec : template : metadata : annotations : prometheus.io/path : /${name}/prometheus prometheus.io/port : 8080 prometheus.io/scrape : true labels : app.kubernetes.io/component : Managed-Master app.kubernetes.io/instance : ${name} app.kubernetes.io/managed-by : CloudBees-Core-Cloud-Operations-Center app.kubernetes.io/name : ${name}","title":"CloudBees Core"},{"location":"blogs/monitor-jenkins-on-k8s/dashboard/","text":"Grafana Dashboard Example Screenshot Grafana Variables cluster type : datasource datasource type : prometheus node type : query query : label_values ( kube_node_info { component = kube-state-metrics } , node ) label : K8S Node multivalue include all namespace type : query query : label_values ( jenkins_health_check_duration , kubernetes_namespace ) label : Namespace multivalue include all instance type : query query : label_values ( jenkins_health_check_duration , app_kubernetes_io_instance ) label : Master multivalue include all Dashboard json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 { annotations : { list : [ { builtIn : 1 , datasource : -- Grafana -- , enable : true , hide : true , iconColor : rgba(0, 211, 255, 1) , name : Annotations Alerts , type : dashboard } ] }, description : Dashboard for when you are using multiple Jenkins Masters , editable : true , gnetId : null , graphTooltip : 1 , id : 9 , iteration : 1565906968208 , links : [], panels : [ { collapsed : false , gridPos : { h : 1 , w : 24 , x : 0 , y : 0 }, id : 20 , panels : [], title : Performance , type : row }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , decimals : 3 , description : The ratio of ok (200) request out of all requests. , fill : 1 , gridPos : { h : 6 , w : 6 , x : 0 , y : 1 }, id : 36 , legend : { alignAsTable : true , avg : false , current : true , max : false , min : false , rightSide : true , show : true , total : false , values : true }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(http_responseCodes_ok_total{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) \\nby (app_kubernetes_io_instance) / \\nsum(http_requests_count{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) \\nby (app_kubernetes_io_instance) , format : time_series , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [ { colorMode : warning , fill : true , line : true , op : lt , value : 0.991 , yaxis : left }, { colorMode : critical , fill : true , line : true , op : lt , value : 0.981 , yaxis : left } ], timeFrom : null , timeRegions : [], timeShift : null , title : Good HTTP Request Ratio , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { decimals : 3 , format : percentunit , label : null , logBase : 1 , max : 1 , min : 0.95 , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : false } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , decimals : 0 , description : Http Server Errors (500) , fill : 1 , gridPos : { h : 6 , w : 5 , x : 6 , y : 1 }, id : 38 , legend : { alignAsTable : true , avg : false , current : true , hideEmpty : true , hideZero : false , max : false , min : false , rightSide : true , show : true , total : false , values : true }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : http_responseCodes_serverError_total{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ } , format : time_series , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [ { colorMode : warning , fill : true , line : true , op : gt , value : 1 , yaxis : left } ], timeFrom : null , timeRegions : [], timeShift : null , title : Server Errors , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , fill : 1 , gridPos : { h : 6 , w : 6 , x : 11 , y : 1 }, id : 42 , interval : 1m , legend : { avg : false , current : false , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum (jenkins_job_waiting_duration{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) by (app_kubernetes_io_instance) , format : time_series , instant : false , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [], timeFrom : null , timeRegions : [], timeShift : null , title : Job Wait Duration , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : s , label : null , logBase : 2 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : false } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , fill : 1 , gridPos : { h : 6 , w : 6 , x : 17 , y : 1 }, id : 44 , interval : 1m , legend : { avg : false , current : false , hideEmpty : false , hideZero : false , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(jenkins_job_building_duration{quantile=\\ 0.5\\ ,kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ })\\nby (app_kubernetes_io_instance) / 60 , format : time_series , instant : false , interval : , intervalFactor : 1 , legendFormat : p50-{{app_kubernetes_io_instance}} , refId : A }, { expr : sum(jenkins_node_builds{quantile=\\ 0.999\\ ,kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ })\\nby (app_kubernetes_io_instance) / 60 , format : time_series , intervalFactor : 1 , legendFormat : p999-{{app_kubernetes_io_instance}} , refId : B } ], thresholds : [], timeFrom : null , timeRegions : [], timeShift : null , title : Avg Build Duration Minutes , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : m , label : null , logBase : 2 , max : null , min : null , show : true }, { decimals : 1 , format : m , label : null , logBase : 1 , max : null , min : null , show : false } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , description : How long the health check takes to complete at the 99th percentile.\\nHigher numbers signify problems , fill : 1 , gridPos : { h : 6 , w : 7 , x : 0 , y : 7 }, id : 18 , interval : 1m , legend : { avg : false , current : false , hideEmpty : true , hideZero : true , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : null as zero , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(jenkins_health_check_duration{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ , quantile=\\ 0.99\\ }) \\n by (app_kubernetes_io_instance) , format : time_series , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [], timeFrom : null , timeRegions : [], timeShift : null , title : Health Check Duration (99%) , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , description : The 99% percentile of HTTP Requests handled by Jenkins masters. , fill : 1 , gridPos : { h : 6 , w : 8 , x : 7 , y : 7 }, id : 52 , legend : { avg : false , current : false , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(http_requests{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ ,quantile=\\ 0.99\\ } ) by (app_kubernetes_io_instance) , format : time_series , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [], timeFrom : null , timeRegions : [], timeShift : null , title : HTTP Request Duration (99%) , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , description : Shows performance of Ingress Controller connection that lasts longer than 250 milliseconds , fill : 1 , gridPos : { h : 6 , w : 8 , x : 15 , y : 7 }, id : 32 , legend : { avg : false , current : false , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(rate(\\n nginx_ingress_controller_request_duration_seconds_bucket{\\n le=\\ 0.25\\ ,\\n namespace=~\\ $namespace\\ ,\\n ingress=~\\ jenkins*\\ \\n }[5m]\\n)) \\nby (ingress) / \\nsum(rate(\\n nginx_ingress_controller_request_duration_seconds_count{\\n namespace=~\\ $namespace\\ ,\\n ingress=~\\ jenkins*\\ \\n }[5m]\\n)) \\nby (ingress) , format : time_series , intervalFactor : 1 , legendFormat : {{ingress}} , refId : A } ], thresholds : [ { colorMode : custom , fill : false , fillColor : rgba(50, 116, 217, 0.2) , line : true , lineColor : #B877D9 , op : gt , value : 0.5 , yaxis : left }, { colorMode : warning , fill : false , fillColor : rgba(50, 116, 217, 0.2) , line : true , lineColor : rgba(31, 96, 196, 0.6) , op : gt , value : 1.5 , yaxis : left }, { colorMode : critical , fill : false , line : true , op : gt , value : 3 , yaxis : left } ], timeFrom : null , timeRegions : [], timeShift : null , title : Ingress Perfomance , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { collapsed : false , gridPos : { h : 1 , w : 24 , x : 0 , y : 13 }, id : 12 , panels : [], title : General Info , type : row }, { cacheTimeout : null , colorBackground : true , colorPostfix : false , colorValue : false , colors : [ #F2495C , #FFCB7D , #5794F2 ], description : Amount of Masters healthy , format : none , gauge : { maxValue : 100 , minValue : 0 , show : false , thresholdLabels : false , thresholdMarkers : true }, gridPos : { h : 6 , w : 4 , x : 0 , y : 14 }, id : 26 , interval : null , links : [], mappingType : 1 , mappingTypes : [ { name : value to text , value : 1 }, { name : range to text , value : 2 } ], maxDataPoints : 100 , nullPointMode : connected , nullText : null , options : {}, pluginVersion : 6.2.4 , postfix : , postfixFontSize : 50% , prefix : , prefixFontSize : 50% , rangeMaps : [ { from : null , text : N/A , to : null } ], sparkline : { fillColor : rgba(31, 118, 189, 0.18) , full : false , lineColor : rgb(31, 120, 193) , show : false }, tableColumn : , targets : [ { expr : sum(jenkins_health_check_score{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) , format : time_series , intervalFactor : 1 , legendFormat : , refId : A } ], thresholds : 0,1 , timeFrom : null , timeShift : null , title : Number of Masters , type : singlestat , valueFontSize : 200% , valueMaps : [ { op : = , text : N/A , value : null } ], valueName : current }, { cacheTimeout : null , columns : [ { text : Avg , value : avg } ], description : Dropwizard based Health Score derived from other metrics , fontSize : 100% , gridPos : { h : 6 , w : 5 , x : 4 , y : 14 }, id : 40 , links : [], options : {}, pageSize : 5 , pluginVersion : 6.2.4 , scroll : true , showHeader : true , sort : { col : 0 , desc : true }, styles : [ { alias : Score , colorMode : row , colors : [ #5794F2 , #FF9830 , #F2495C ], decimals : 0 , pattern : /.*/ , thresholds : [ 90 , 95 ], type : number , unit : percentunit } ], targets : [ { expr : jenkins_health_check_score{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ } , format : time_series , instant : true , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : Health Score , transform : timeseries_aggregations , type : table }, { gridPos : { h : 6 , w : 14 , x : 9 , y : 14 }, id : 46 , links : [], options : { fieldOptions : { calcs : [ last ], defaults : { decimals : 1 , max : 100 , min : 0 , title : , unit : percent }, mappings : [], override : {}, thresholds : [ { color : red , index : 0 , value : null }, { color : purple , index : 1 , value : 50 }, { color : blue , index : 2 , value : 75 } ], values : false }, orientation : auto , showThresholdLabels : false , showThresholdMarkers : true }, pluginVersion : 6.2.4 , targets : [ { expr : sum(jenkins_runs_success_total{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ })\\nby (app_kubernetes_io_instance) /\\nsum(jenkins_runs_total_total{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ })\\nby (app_kubernetes_io_instance) * 100 , format : time_series , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : Job Success Ratio , type : gauge }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , decimals : 0 , description : Amount of Jobs Currenty in the Queue , fill : 1 , gridPos : { h : 6 , w : 7 , x : 0 , y : 20 }, id : 30 , legend : { alignAsTable : false , avg : false , current : false , hideEmpty : false , hideZero : true , max : false , min : false , rightSide : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(jenkins_queue_size_value{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) by (app_kubernetes_io_instance) , format : time_series , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [ { colorMode : critical , fill : false , line : true , op : gt , value : 10 , yaxis : left }, { colorMode : warning , fill : false , line : true , op : gt , value : 5 , yaxis : left } ], timeFrom : null , timeRegions : [], timeShift : null , title : Job Queue , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , cacheTimeout : null , dashLength : 10 , dashes : false , decimals : 0 , fill : 1 , gridPos : { h : 6 , w : 8 , x : 7 , y : 20 }, hideTimeOverride : false , id : 50 , interval : , legend : { alignAsTable : true , avg : false , current : false , hideEmpty : true , hideZero : true , max : true , min : false , rightSide : true , show : true , total : false , values : true }, lines : true , linewidth : 2 , links : [], nullPointMode : null , options : {}, percentage : false , pluginVersion : 6.2.4 , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(increase(jenkins_runs_total_total{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }[24h])) by (app_kubernetes_io_instance) , format : time_series , interval : , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}}-total , refId : A }, { expr : sum(increase(jenkins_runs_failure_total{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }[24h])) by (app_kubernetes_io_instance) , format : time_series , interval : , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}}-failed , refId : B }, { expr : sum(increase(jenkins_runs_aborted_total{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }[24h])) by (app_kubernetes_io_instance) , format : time_series , interval : , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}}-aborted , refId : C } ], thresholds : [], timeFrom : 12h , timeRegions : [], timeShift : null , title : Runs Per Day , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { decimals : 0 , format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { decimals : 0 , format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , description : Active Build Runs , fill : 1 , gridPos : { h : 6 , w : 8 , x : 15 , y : 20 }, id : 56 , legend : { avg : false , current : false , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(jenkins_executor_in_use_history{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) by (app_kubernetes_io_instance) , format : time_series , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [], timeFrom : null , timeRegions : [], timeShift : null , title : Build Runs , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { cards : { cardPadding : null , cardRound : null }, color : { cardColor : #F2495C , colorScale : sqrt , colorScheme : interpolateSpectral , exponent : 0.8 , max : null , min : 0 , mode : spectrum }, dataFormat : tsbuckets , description : Heatmap of when Jobs are scheduled , gridPos : { h : 7 , w : 23 , x : 0 , y : 26 }, heatmap : {}, hideTimeOverride : false , hideZeroBuckets : false , highlightCards : true , id : 58 , interval : 5m , legend : { show : true }, links : [], options : {}, reverseYBuckets : false , targets : [ { expr : sum(increase(jenkins_runs_total_total{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }[2h])) by (app_kubernetes_io_instance) , format : heatmap , instant : false , interval : , intervalFactor : 10 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : 168h , timeShift : null , title : Job Runs Heatmap , tooltip : { show : true , showHistogram : false }, type : heatmap , xAxis : { show : true }, xBucketNumber : null , xBucketSize : null , yAxis : { decimals : 0 , format : short , logBase : 1 , max : null , min : 0 , show : true , splitFactor : null }, yBucketBound : auto , yBucketNumber : null , yBucketSize : null }, { columns : [ { text : Current , value : current } ], description : Jenkins Master Plugin Count , fontSize : 100% , gridPos : { h : 5 , w : 7 , x : 0 , y : 33 }, id : 14 , interval : 1m , links : [], options : {}, pageSize : null , pluginVersion : 6.2.4 , scroll : true , showHeader : true , sort : { col : 0 , desc : true }, styles : [ { alias : Time , dateFormat : YYYY-MM-DD HH:mm:ss , pattern : Time , type : date }, { alias : Master , colorMode : row , colors : [ #5794F2 , #FF9830 , #F2495C ], decimals : 0 , link : false , pattern : /.*/ , thresholds : [ 95 , 130 ], type : number , unit : short } ], targets : [ { expr : jenkins_plugins_active{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ } , format : time_series , instant : true , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : Plugin Count , transform : timeseries_to_rows , type : table }, { columns : [ { text : Current , value : current } ], description : Amount of plugins that are available for updating , fontSize : 100% , gridPos : { h : 5 , w : 8 , x : 7 , y : 33 }, id : 22 , links : [], options : {}, pageSize : null , scroll : true , showHeader : true , sort : { col : 0 , desc : true }, styles : [ { alias : Time , dateFormat : YYYY-MM-DD HH:mm:ss , pattern : Time , type : date }, { alias : , colorMode : row , colors : [ #5794F2 , #FF9830 , #F2495C ], decimals : 0 , pattern : /.*/ , thresholds : [ 3 , 10 ], type : number , unit : short } ], targets : [ { expr : sum(jenkins_plugins_withUpdate{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) by (app_kubernetes_io_instance) , format : time_series , instant : true , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : Plugin Updates Available , transform : timeseries_aggregations , type : table }, { columns : [ { text : Current , value : current }, { text : Max , value : max } ], description : Jenkins Master Job Count , fontSize : 100% , gridPos : { h : 5 , w : 8 , x : 15 , y : 33 }, id : 16 , links : [], options : {}, pageSize : null , scroll : true , showHeader : true , sort : { col : 0 , desc : true }, styles : [ { alias : Time , dateFormat : YYYY-MM-DD HH:mm:ss , pattern : Time , type : date }, { alias : , colorMode : row , colors : [ #F2495C , #FF9830 , #5794F2 ], decimals : 0 , pattern : /.*/ , thresholds : [ 1 ], type : number , unit : short } ], targets : [ { expr : jenkins_job_count_value{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ } , format : time_series , instant : true , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : Job Count , transform : timeseries_aggregations , type : table }, { columns : [ { text : Current , value : current } ], description : Counts offline build nodes that are connected to this master , fontSize : 100% , gridPos : { h : 6 , w : 7 , x : 0 , y : 38 }, id : 24 , links : [], options : {}, pageSize : null , scroll : true , showHeader : true , sort : { col : 0 , desc : true }, styles : [ { alias : Time , dateFormat : YYYY-MM-DD HH:mm:ss , pattern : Time , type : date }, { alias : , colorMode : row , colors : [ #5794F2 , #B877D9 , #F2495C ], decimals : 2 , pattern : /.*/ , thresholds : [ 1 , 3 ], type : number , unit : short } ], targets : [ { expr : sum(jenkins_node_offline_value{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) by (app_kubernetes_io_instance) , format : time_series , instant : true , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : Offline Nodes , transform : timeseries_aggregations , type : table }, { cacheTimeout : null , columns : [ { text : Current , value : current } ], description : Uptime in hours , fontSize : 100% , gridPos : { h : 6 , w : 8 , x : 7 , y : 38 }, id : 6 , links : [], options : {}, pageSize : null , pluginVersion : 6.2.4 , scroll : true , showHeader : true , sort : { col : null , desc : false }, styles : [ { alias : Time , dateFormat : YYYY-MM-DD HH:mm:ss , pattern : Time , type : date }, { alias : , colorMode : row , colors : [ #F2495C , #FF9830 , #5794F2 ], decimals : 0 , pattern : /.*/ , thresholds : [ 1 , 24 ], type : number , unit : short } ], targets : [ { expr : vm_uptime_milliseconds{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ } / 3600000 , format : time_series , instant : true , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : Uptime , transform : timeseries_aggregations , type : table }, { columns : [ { text : Current , value : current } ], description : The current Master executor count, masters should not have executors, so only 0 is green. , fontSize : 100% , gridPos : { h : 6 , w : 8 , x : 15 , y : 38 }, id : 34 , interval : , links : [], options : {}, pageSize : null , scroll : true , showHeader : true , sort : { col : 0 , desc : true }, styles : [ { alias : Time , dateFormat : YYYY-MM-DD HH:mm:ss , pattern : Time , type : date }, { alias : , colorMode : row , colors : [ #F2495C , #FF9830 , #5794F2 ], decimals : 2 , pattern : /.*/ , thresholds : [ 0 , 0 , 1 ], type : number , unit : short } ], targets : [ { expr : sum(jenkins_executor_count_value{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) by (app_kubernetes_io_instance) , format : time_series , instant : true , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : Master Executor Count , transform : timeseries_aggregations , type : table }, { collapsed : false , gridPos : { h : 1 , w : 24 , x : 0 , y : 44 }, id : 2 , panels : [], title : JVM Metrics , type : row }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , fill : 1 , gridPos : { h : 7 , w : 10 , x : 0 , y : 45 }, id : 48 , legend : { avg : false , current : false , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : 1 - sum(vm_gc_G1_Young_Generation_time{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ })by (app_kubernetes_io_instance) \\n/ \\nsum (vm_uptime_milliseconds{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) by (app_kubernetes_io_instance) , format : time_series , instant : false , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [ { colorMode : warning , fill : true , line : true , op : lt , value : 0.998 , yaxis : left }, { colorMode : critical , fill : true , line : true , op : lt , value : 0.98 , yaxis : left } ], timeFrom : null , timeRegions : [], timeShift : null , title : JVM GC Throughput , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { decimals : 5 , format : percentunit , label : null , logBase : 1 , max : 1 , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : false } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , description : Ratio of JVM Memory used , fill : 1 , gridPos : { h : 7 , w : 13 , x : 10 , y : 45 }, id : 10 , legend : { alignAsTable : false , avg : false , current : false , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 2 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(vm_memory_heap_usage{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) by (app_kubernetes_io_instance) , format : time_series , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [ { colorMode : critical , fill : true , line : true , op : gt , value : 0.75 , yaxis : left } ], timeFrom : null , timeRegions : [], timeShift : null , title : Memory Ratio , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : percentunit , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : false } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , decimals : 4 , description : JVM CPU Load , fill : 1 , gridPos : { h : 7 , w : 10 , x : 0 , y : 52 }, id : 4 , legend : { alignAsTable : true , avg : false , current : true , hideEmpty : true , hideZero : true , max : true , min : false , rightSide : true , show : false , total : false , values : true }, lines : true , linewidth : 2 , links : [], nullPointMode : connected , options : {}, percentage : false , pointradius : 1 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : true , targets : [ { expr : vm_cpu_load{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ } , format : time_series , instant : false , intervalFactor : 5 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [], timeFrom : null , timeRegions : [], timeShift : null , title : CPU Load , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : , logBase : 1 , max : null , min : null , show : true }, { format : short , label : , logBase : 1 , max : null , min : null , show : false } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , decimals : null , description : JVM Memory usage , fill : 2 , gridPos : { h : 7 , w : 13 , x : 10 , y : 52 }, id : 8 , legend : { alignAsTable : true , avg : false , current : true , max : true , min : false , rightSide : true , show : true , total : false , values : true }, lines : true , linewidth : 2 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : vm_memory_total_used{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ } , format : time_series , intervalFactor : 2 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [], timeFrom : null , timeRegions : [], timeShift : null , title : Memory , tooltip : { shared : true , sort : 1 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , description : Lists memory usage of the Pod vs. Kubernetes Requests , fill : 1 , gridPos : { h : 8 , w : 10 , x : 0 , y : 59 }, id : 54 , legend : { avg : false , current : false , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : connected , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum (label_join(container_memory_usage_bytes{\\n container_name=\\ jenkins\\ ,\\n namespace=~\\ $namespace\\ \\n }, \\n \\ pod\\ , \\n \\ ,\\ , \\n \\ pod_name\\ \\n)) by (pod) / \\nsum (kube_pod_container_resource_requests_memory_bytes { \\n container=\\ jenkins\\ ,\\n namespace=~\\ $namespace\\ \\n }\\n) by (pod) , format : time_series , intervalFactor : 1 , legendFormat : {{pod}} , refId : A } ], thresholds : [], timeFrom : null , timeRegions : [], timeShift : null , title : Memory Usage vs. Request , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { description : The ratio of open file descriptors\\nSee: https://support.cloudbees.com/hc/en-us/articles/204246140-Too-many-open-files , gridPos : { h : 8 , w : 5 , x : 10 , y : 59 }, id : 28 , links : [], options : { displayMode : basic , fieldOptions : { calcs : [ last ], defaults : { max : 1 , min : 0 , unit : percentunit }, mappings : [], override : {}, thresholds : [ { color : blue , index : 0 , value : null }, { color : orange , index : 1 , value : 60 }, { color : red , index : 2 , value : 80 } ], values : false }, orientation : horizontal }, targets : [ { expr : vm_file_descriptor_ratio{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ } , format : time_series , instant : true , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : File Descriptor Ratio , type : bargauge } ], refresh : 30s , schemaVersion : 18 , style : dark , tags : [ Jenkins , Prometheus ], templating : { list : [ { current : { text : Prometheus , value : Prometheus }, hide : 0 , includeAll : false , label : null , multi : false , name : cluster , options : [], query : prometheus , refresh : 1 , regex : , skipUrlSync : false , type : datasource }, { allValue : null , current : { text : All , value : [ $__all ] }, datasource : Prometheus , definition : label_values(jenkins_health_check_duration, app_kubernetes_io_instance) , hide : 0 , includeAll : true , label : Master , multi : true , name : instance , options : [], query : label_values(jenkins_health_check_duration, app_kubernetes_io_instance) , refresh : 2 , regex : , skipUrlSync : false , sort : 1 , tagValuesQuery : , tags : [], tagsQuery : , type : query , useTags : false }, { allValue : null , current : { text : All , value : $__all }, datasource : Prometheus , definition : label_values(jenkins_health_check_duration, kubernetes_namespace) , hide : 0 , includeAll : true , label : Namespace , multi : true , name : namespace , options : [], query : label_values(jenkins_health_check_duration, kubernetes_namespace) , refresh : 2 , regex : , skipUrlSync : false , sort : 1 , tagValuesQuery : , tags : [], tagsQuery : , type : query , useTags : false }, { allValue : null , current : { text : All , value : $__all }, datasource : Prometheus , definition : label_values(kube_node_info{component=\\ kube-state-metrics\\ }, node) , hide : 0 , includeAll : true , label : K8S Node , multi : true , name : node , options : [], query : label_values(kube_node_info{component=\\ kube-state-metrics\\ }, node) , refresh : 1 , regex : , skipUrlSync : false , sort : 5 , tagValuesQuery : , tags : [], tagsQuery : , type : query , useTags : false }, { allValue : null , current : { text : jx-production , value : jx-production }, datasource : Prometheus , definition : label_values(jenkins_health_check_duration, kubernetes_namespace) , hide : 0 , includeAll : false , label : null , multi : false , name : Test , options : [], query : label_values(jenkins_health_check_duration, kubernetes_namespace) , refresh : 2 , regex : , skipUrlSync : false , sort : 1 , tagValuesQuery : , tags : [], tagsQuery : , type : query , useTags : false } ] }, time : { from : now-6h , to : now }, timepicker : { refresh_intervals : [ 5s , 10s , 30s , 1m , 5m , 15m , 30m , 1h , 2h , 1d ], time_options : [ 5m , 15m , 1h , 6h , 12h , 24h , 2d , 7d , 30d ] }, timezone : , title : Jenkins Masters , uid : 8Z9-POHWz , version : 9 }","title":"Dashboard"},{"location":"blogs/monitor-jenkins-on-k8s/dashboard/#grafana-dashboard","text":"","title":"Grafana Dashboard"},{"location":"blogs/monitor-jenkins-on-k8s/dashboard/#example-screenshot","text":"","title":"Example Screenshot"},{"location":"blogs/monitor-jenkins-on-k8s/dashboard/#grafana-variables","text":"cluster type : datasource datasource type : prometheus node type : query query : label_values ( kube_node_info { component = kube-state-metrics } , node ) label : K8S Node multivalue include all namespace type : query query : label_values ( jenkins_health_check_duration , kubernetes_namespace ) label : Namespace multivalue include all instance type : query query : label_values ( jenkins_health_check_duration , app_kubernetes_io_instance ) label : Master multivalue include all","title":"Grafana Variables"},{"location":"blogs/monitor-jenkins-on-k8s/dashboard/#dashboard-json","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 { annotations : { list : [ { builtIn : 1 , datasource : -- Grafana -- , enable : true , hide : true , iconColor : rgba(0, 211, 255, 1) , name : Annotations Alerts , type : dashboard } ] }, description : Dashboard for when you are using multiple Jenkins Masters , editable : true , gnetId : null , graphTooltip : 1 , id : 9 , iteration : 1565906968208 , links : [], panels : [ { collapsed : false , gridPos : { h : 1 , w : 24 , x : 0 , y : 0 }, id : 20 , panels : [], title : Performance , type : row }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , decimals : 3 , description : The ratio of ok (200) request out of all requests. , fill : 1 , gridPos : { h : 6 , w : 6 , x : 0 , y : 1 }, id : 36 , legend : { alignAsTable : true , avg : false , current : true , max : false , min : false , rightSide : true , show : true , total : false , values : true }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(http_responseCodes_ok_total{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) \\nby (app_kubernetes_io_instance) / \\nsum(http_requests_count{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) \\nby (app_kubernetes_io_instance) , format : time_series , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [ { colorMode : warning , fill : true , line : true , op : lt , value : 0.991 , yaxis : left }, { colorMode : critical , fill : true , line : true , op : lt , value : 0.981 , yaxis : left } ], timeFrom : null , timeRegions : [], timeShift : null , title : Good HTTP Request Ratio , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { decimals : 3 , format : percentunit , label : null , logBase : 1 , max : 1 , min : 0.95 , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : false } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , decimals : 0 , description : Http Server Errors (500) , fill : 1 , gridPos : { h : 6 , w : 5 , x : 6 , y : 1 }, id : 38 , legend : { alignAsTable : true , avg : false , current : true , hideEmpty : true , hideZero : false , max : false , min : false , rightSide : true , show : true , total : false , values : true }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : http_responseCodes_serverError_total{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ } , format : time_series , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [ { colorMode : warning , fill : true , line : true , op : gt , value : 1 , yaxis : left } ], timeFrom : null , timeRegions : [], timeShift : null , title : Server Errors , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , fill : 1 , gridPos : { h : 6 , w : 6 , x : 11 , y : 1 }, id : 42 , interval : 1m , legend : { avg : false , current : false , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum (jenkins_job_waiting_duration{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) by (app_kubernetes_io_instance) , format : time_series , instant : false , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [], timeFrom : null , timeRegions : [], timeShift : null , title : Job Wait Duration , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : s , label : null , logBase : 2 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : false } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , fill : 1 , gridPos : { h : 6 , w : 6 , x : 17 , y : 1 }, id : 44 , interval : 1m , legend : { avg : false , current : false , hideEmpty : false , hideZero : false , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(jenkins_job_building_duration{quantile=\\ 0.5\\ ,kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ })\\nby (app_kubernetes_io_instance) / 60 , format : time_series , instant : false , interval : , intervalFactor : 1 , legendFormat : p50-{{app_kubernetes_io_instance}} , refId : A }, { expr : sum(jenkins_node_builds{quantile=\\ 0.999\\ ,kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ })\\nby (app_kubernetes_io_instance) / 60 , format : time_series , intervalFactor : 1 , legendFormat : p999-{{app_kubernetes_io_instance}} , refId : B } ], thresholds : [], timeFrom : null , timeRegions : [], timeShift : null , title : Avg Build Duration Minutes , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : m , label : null , logBase : 2 , max : null , min : null , show : true }, { decimals : 1 , format : m , label : null , logBase : 1 , max : null , min : null , show : false } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , description : How long the health check takes to complete at the 99th percentile.\\nHigher numbers signify problems , fill : 1 , gridPos : { h : 6 , w : 7 , x : 0 , y : 7 }, id : 18 , interval : 1m , legend : { avg : false , current : false , hideEmpty : true , hideZero : true , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : null as zero , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(jenkins_health_check_duration{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ , quantile=\\ 0.99\\ }) \\n by (app_kubernetes_io_instance) , format : time_series , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [], timeFrom : null , timeRegions : [], timeShift : null , title : Health Check Duration (99%) , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , description : The 99% percentile of HTTP Requests handled by Jenkins masters. , fill : 1 , gridPos : { h : 6 , w : 8 , x : 7 , y : 7 }, id : 52 , legend : { avg : false , current : false , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(http_requests{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ ,quantile=\\ 0.99\\ } ) by (app_kubernetes_io_instance) , format : time_series , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [], timeFrom : null , timeRegions : [], timeShift : null , title : HTTP Request Duration (99%) , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , description : Shows performance of Ingress Controller connection that lasts longer than 250 milliseconds , fill : 1 , gridPos : { h : 6 , w : 8 , x : 15 , y : 7 }, id : 32 , legend : { avg : false , current : false , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(rate(\\n nginx_ingress_controller_request_duration_seconds_bucket{\\n le=\\ 0.25\\ ,\\n namespace=~\\ $namespace\\ ,\\n ingress=~\\ jenkins*\\ \\n }[5m]\\n)) \\nby (ingress) / \\nsum(rate(\\n nginx_ingress_controller_request_duration_seconds_count{\\n namespace=~\\ $namespace\\ ,\\n ingress=~\\ jenkins*\\ \\n }[5m]\\n)) \\nby (ingress) , format : time_series , intervalFactor : 1 , legendFormat : {{ingress}} , refId : A } ], thresholds : [ { colorMode : custom , fill : false , fillColor : rgba(50, 116, 217, 0.2) , line : true , lineColor : #B877D9 , op : gt , value : 0.5 , yaxis : left }, { colorMode : warning , fill : false , fillColor : rgba(50, 116, 217, 0.2) , line : true , lineColor : rgba(31, 96, 196, 0.6) , op : gt , value : 1.5 , yaxis : left }, { colorMode : critical , fill : false , line : true , op : gt , value : 3 , yaxis : left } ], timeFrom : null , timeRegions : [], timeShift : null , title : Ingress Perfomance , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { collapsed : false , gridPos : { h : 1 , w : 24 , x : 0 , y : 13 }, id : 12 , panels : [], title : General Info , type : row }, { cacheTimeout : null , colorBackground : true , colorPostfix : false , colorValue : false , colors : [ #F2495C , #FFCB7D , #5794F2 ], description : Amount of Masters healthy , format : none , gauge : { maxValue : 100 , minValue : 0 , show : false , thresholdLabels : false , thresholdMarkers : true }, gridPos : { h : 6 , w : 4 , x : 0 , y : 14 }, id : 26 , interval : null , links : [], mappingType : 1 , mappingTypes : [ { name : value to text , value : 1 }, { name : range to text , value : 2 } ], maxDataPoints : 100 , nullPointMode : connected , nullText : null , options : {}, pluginVersion : 6.2.4 , postfix : , postfixFontSize : 50% , prefix : , prefixFontSize : 50% , rangeMaps : [ { from : null , text : N/A , to : null } ], sparkline : { fillColor : rgba(31, 118, 189, 0.18) , full : false , lineColor : rgb(31, 120, 193) , show : false }, tableColumn : , targets : [ { expr : sum(jenkins_health_check_score{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) , format : time_series , intervalFactor : 1 , legendFormat : , refId : A } ], thresholds : 0,1 , timeFrom : null , timeShift : null , title : Number of Masters , type : singlestat , valueFontSize : 200% , valueMaps : [ { op : = , text : N/A , value : null } ], valueName : current }, { cacheTimeout : null , columns : [ { text : Avg , value : avg } ], description : Dropwizard based Health Score derived from other metrics , fontSize : 100% , gridPos : { h : 6 , w : 5 , x : 4 , y : 14 }, id : 40 , links : [], options : {}, pageSize : 5 , pluginVersion : 6.2.4 , scroll : true , showHeader : true , sort : { col : 0 , desc : true }, styles : [ { alias : Score , colorMode : row , colors : [ #5794F2 , #FF9830 , #F2495C ], decimals : 0 , pattern : /.*/ , thresholds : [ 90 , 95 ], type : number , unit : percentunit } ], targets : [ { expr : jenkins_health_check_score{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ } , format : time_series , instant : true , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : Health Score , transform : timeseries_aggregations , type : table }, { gridPos : { h : 6 , w : 14 , x : 9 , y : 14 }, id : 46 , links : [], options : { fieldOptions : { calcs : [ last ], defaults : { decimals : 1 , max : 100 , min : 0 , title : , unit : percent }, mappings : [], override : {}, thresholds : [ { color : red , index : 0 , value : null }, { color : purple , index : 1 , value : 50 }, { color : blue , index : 2 , value : 75 } ], values : false }, orientation : auto , showThresholdLabels : false , showThresholdMarkers : true }, pluginVersion : 6.2.4 , targets : [ { expr : sum(jenkins_runs_success_total{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ })\\nby (app_kubernetes_io_instance) /\\nsum(jenkins_runs_total_total{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ })\\nby (app_kubernetes_io_instance) * 100 , format : time_series , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : Job Success Ratio , type : gauge }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , decimals : 0 , description : Amount of Jobs Currenty in the Queue , fill : 1 , gridPos : { h : 6 , w : 7 , x : 0 , y : 20 }, id : 30 , legend : { alignAsTable : false , avg : false , current : false , hideEmpty : false , hideZero : true , max : false , min : false , rightSide : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(jenkins_queue_size_value{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) by (app_kubernetes_io_instance) , format : time_series , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [ { colorMode : critical , fill : false , line : true , op : gt , value : 10 , yaxis : left }, { colorMode : warning , fill : false , line : true , op : gt , value : 5 , yaxis : left } ], timeFrom : null , timeRegions : [], timeShift : null , title : Job Queue , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , cacheTimeout : null , dashLength : 10 , dashes : false , decimals : 0 , fill : 1 , gridPos : { h : 6 , w : 8 , x : 7 , y : 20 }, hideTimeOverride : false , id : 50 , interval : , legend : { alignAsTable : true , avg : false , current : false , hideEmpty : true , hideZero : true , max : true , min : false , rightSide : true , show : true , total : false , values : true }, lines : true , linewidth : 2 , links : [], nullPointMode : null , options : {}, percentage : false , pluginVersion : 6.2.4 , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(increase(jenkins_runs_total_total{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }[24h])) by (app_kubernetes_io_instance) , format : time_series , interval : , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}}-total , refId : A }, { expr : sum(increase(jenkins_runs_failure_total{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }[24h])) by (app_kubernetes_io_instance) , format : time_series , interval : , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}}-failed , refId : B }, { expr : sum(increase(jenkins_runs_aborted_total{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }[24h])) by (app_kubernetes_io_instance) , format : time_series , interval : , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}}-aborted , refId : C } ], thresholds : [], timeFrom : 12h , timeRegions : [], timeShift : null , title : Runs Per Day , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { decimals : 0 , format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { decimals : 0 , format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , description : Active Build Runs , fill : 1 , gridPos : { h : 6 , w : 8 , x : 15 , y : 20 }, id : 56 , legend : { avg : false , current : false , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(jenkins_executor_in_use_history{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) by (app_kubernetes_io_instance) , format : time_series , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [], timeFrom : null , timeRegions : [], timeShift : null , title : Build Runs , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { cards : { cardPadding : null , cardRound : null }, color : { cardColor : #F2495C , colorScale : sqrt , colorScheme : interpolateSpectral , exponent : 0.8 , max : null , min : 0 , mode : spectrum }, dataFormat : tsbuckets , description : Heatmap of when Jobs are scheduled , gridPos : { h : 7 , w : 23 , x : 0 , y : 26 }, heatmap : {}, hideTimeOverride : false , hideZeroBuckets : false , highlightCards : true , id : 58 , interval : 5m , legend : { show : true }, links : [], options : {}, reverseYBuckets : false , targets : [ { expr : sum(increase(jenkins_runs_total_total{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }[2h])) by (app_kubernetes_io_instance) , format : heatmap , instant : false , interval : , intervalFactor : 10 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : 168h , timeShift : null , title : Job Runs Heatmap , tooltip : { show : true , showHistogram : false }, type : heatmap , xAxis : { show : true }, xBucketNumber : null , xBucketSize : null , yAxis : { decimals : 0 , format : short , logBase : 1 , max : null , min : 0 , show : true , splitFactor : null }, yBucketBound : auto , yBucketNumber : null , yBucketSize : null }, { columns : [ { text : Current , value : current } ], description : Jenkins Master Plugin Count , fontSize : 100% , gridPos : { h : 5 , w : 7 , x : 0 , y : 33 }, id : 14 , interval : 1m , links : [], options : {}, pageSize : null , pluginVersion : 6.2.4 , scroll : true , showHeader : true , sort : { col : 0 , desc : true }, styles : [ { alias : Time , dateFormat : YYYY-MM-DD HH:mm:ss , pattern : Time , type : date }, { alias : Master , colorMode : row , colors : [ #5794F2 , #FF9830 , #F2495C ], decimals : 0 , link : false , pattern : /.*/ , thresholds : [ 95 , 130 ], type : number , unit : short } ], targets : [ { expr : jenkins_plugins_active{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ } , format : time_series , instant : true , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : Plugin Count , transform : timeseries_to_rows , type : table }, { columns : [ { text : Current , value : current } ], description : Amount of plugins that are available for updating , fontSize : 100% , gridPos : { h : 5 , w : 8 , x : 7 , y : 33 }, id : 22 , links : [], options : {}, pageSize : null , scroll : true , showHeader : true , sort : { col : 0 , desc : true }, styles : [ { alias : Time , dateFormat : YYYY-MM-DD HH:mm:ss , pattern : Time , type : date }, { alias : , colorMode : row , colors : [ #5794F2 , #FF9830 , #F2495C ], decimals : 0 , pattern : /.*/ , thresholds : [ 3 , 10 ], type : number , unit : short } ], targets : [ { expr : sum(jenkins_plugins_withUpdate{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) by (app_kubernetes_io_instance) , format : time_series , instant : true , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : Plugin Updates Available , transform : timeseries_aggregations , type : table }, { columns : [ { text : Current , value : current }, { text : Max , value : max } ], description : Jenkins Master Job Count , fontSize : 100% , gridPos : { h : 5 , w : 8 , x : 15 , y : 33 }, id : 16 , links : [], options : {}, pageSize : null , scroll : true , showHeader : true , sort : { col : 0 , desc : true }, styles : [ { alias : Time , dateFormat : YYYY-MM-DD HH:mm:ss , pattern : Time , type : date }, { alias : , colorMode : row , colors : [ #F2495C , #FF9830 , #5794F2 ], decimals : 0 , pattern : /.*/ , thresholds : [ 1 ], type : number , unit : short } ], targets : [ { expr : jenkins_job_count_value{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ } , format : time_series , instant : true , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : Job Count , transform : timeseries_aggregations , type : table }, { columns : [ { text : Current , value : current } ], description : Counts offline build nodes that are connected to this master , fontSize : 100% , gridPos : { h : 6 , w : 7 , x : 0 , y : 38 }, id : 24 , links : [], options : {}, pageSize : null , scroll : true , showHeader : true , sort : { col : 0 , desc : true }, styles : [ { alias : Time , dateFormat : YYYY-MM-DD HH:mm:ss , pattern : Time , type : date }, { alias : , colorMode : row , colors : [ #5794F2 , #B877D9 , #F2495C ], decimals : 2 , pattern : /.*/ , thresholds : [ 1 , 3 ], type : number , unit : short } ], targets : [ { expr : sum(jenkins_node_offline_value{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) by (app_kubernetes_io_instance) , format : time_series , instant : true , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : Offline Nodes , transform : timeseries_aggregations , type : table }, { cacheTimeout : null , columns : [ { text : Current , value : current } ], description : Uptime in hours , fontSize : 100% , gridPos : { h : 6 , w : 8 , x : 7 , y : 38 }, id : 6 , links : [], options : {}, pageSize : null , pluginVersion : 6.2.4 , scroll : true , showHeader : true , sort : { col : null , desc : false }, styles : [ { alias : Time , dateFormat : YYYY-MM-DD HH:mm:ss , pattern : Time , type : date }, { alias : , colorMode : row , colors : [ #F2495C , #FF9830 , #5794F2 ], decimals : 0 , pattern : /.*/ , thresholds : [ 1 , 24 ], type : number , unit : short } ], targets : [ { expr : vm_uptime_milliseconds{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ } / 3600000 , format : time_series , instant : true , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : Uptime , transform : timeseries_aggregations , type : table }, { columns : [ { text : Current , value : current } ], description : The current Master executor count, masters should not have executors, so only 0 is green. , fontSize : 100% , gridPos : { h : 6 , w : 8 , x : 15 , y : 38 }, id : 34 , interval : , links : [], options : {}, pageSize : null , scroll : true , showHeader : true , sort : { col : 0 , desc : true }, styles : [ { alias : Time , dateFormat : YYYY-MM-DD HH:mm:ss , pattern : Time , type : date }, { alias : , colorMode : row , colors : [ #F2495C , #FF9830 , #5794F2 ], decimals : 2 , pattern : /.*/ , thresholds : [ 0 , 0 , 1 ], type : number , unit : short } ], targets : [ { expr : sum(jenkins_executor_count_value{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) by (app_kubernetes_io_instance) , format : time_series , instant : true , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : Master Executor Count , transform : timeseries_aggregations , type : table }, { collapsed : false , gridPos : { h : 1 , w : 24 , x : 0 , y : 44 }, id : 2 , panels : [], title : JVM Metrics , type : row }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , fill : 1 , gridPos : { h : 7 , w : 10 , x : 0 , y : 45 }, id : 48 , legend : { avg : false , current : false , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : 1 - sum(vm_gc_G1_Young_Generation_time{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ })by (app_kubernetes_io_instance) \\n/ \\nsum (vm_uptime_milliseconds{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) by (app_kubernetes_io_instance) , format : time_series , instant : false , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [ { colorMode : warning , fill : true , line : true , op : lt , value : 0.998 , yaxis : left }, { colorMode : critical , fill : true , line : true , op : lt , value : 0.98 , yaxis : left } ], timeFrom : null , timeRegions : [], timeShift : null , title : JVM GC Throughput , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { decimals : 5 , format : percentunit , label : null , logBase : 1 , max : 1 , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : false } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , description : Ratio of JVM Memory used , fill : 1 , gridPos : { h : 7 , w : 13 , x : 10 , y : 45 }, id : 10 , legend : { alignAsTable : false , avg : false , current : false , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 2 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum(vm_memory_heap_usage{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ }) by (app_kubernetes_io_instance) , format : time_series , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [ { colorMode : critical , fill : true , line : true , op : gt , value : 0.75 , yaxis : left } ], timeFrom : null , timeRegions : [], timeShift : null , title : Memory Ratio , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : percentunit , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : false } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , decimals : 4 , description : JVM CPU Load , fill : 1 , gridPos : { h : 7 , w : 10 , x : 0 , y : 52 }, id : 4 , legend : { alignAsTable : true , avg : false , current : true , hideEmpty : true , hideZero : true , max : true , min : false , rightSide : true , show : false , total : false , values : true }, lines : true , linewidth : 2 , links : [], nullPointMode : connected , options : {}, percentage : false , pointradius : 1 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : true , targets : [ { expr : vm_cpu_load{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ } , format : time_series , instant : false , intervalFactor : 5 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [], timeFrom : null , timeRegions : [], timeShift : null , title : CPU Load , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : , logBase : 1 , max : null , min : null , show : true }, { format : short , label : , logBase : 1 , max : null , min : null , show : false } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , decimals : null , description : JVM Memory usage , fill : 2 , gridPos : { h : 7 , w : 13 , x : 10 , y : 52 }, id : 8 , legend : { alignAsTable : true , avg : false , current : true , max : true , min : false , rightSide : true , show : true , total : false , values : true }, lines : true , linewidth : 2 , links : [], nullPointMode : null , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : vm_memory_total_used{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ } , format : time_series , intervalFactor : 2 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], thresholds : [], timeFrom : null , timeRegions : [], timeShift : null , title : Memory , tooltip : { shared : true , sort : 1 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { aliasColors : {}, bars : false , dashLength : 10 , dashes : false , description : Lists memory usage of the Pod vs. Kubernetes Requests , fill : 1 , gridPos : { h : 8 , w : 10 , x : 0 , y : 59 }, id : 54 , legend : { avg : false , current : false , max : false , min : false , show : true , total : false , values : false }, lines : true , linewidth : 1 , links : [], nullPointMode : connected , options : {}, percentage : false , pointradius : 2 , points : false , renderer : flot , seriesOverrides : [], spaceLength : 10 , stack : false , steppedLine : false , targets : [ { expr : sum (label_join(container_memory_usage_bytes{\\n container_name=\\ jenkins\\ ,\\n namespace=~\\ $namespace\\ \\n }, \\n \\ pod\\ , \\n \\ ,\\ , \\n \\ pod_name\\ \\n)) by (pod) / \\nsum (kube_pod_container_resource_requests_memory_bytes { \\n container=\\ jenkins\\ ,\\n namespace=~\\ $namespace\\ \\n }\\n) by (pod) , format : time_series , intervalFactor : 1 , legendFormat : {{pod}} , refId : A } ], thresholds : [], timeFrom : null , timeRegions : [], timeShift : null , title : Memory Usage vs. Request , tooltip : { shared : true , sort : 0 , value_type : individual }, type : graph , xaxis : { buckets : null , mode : time , name : null , show : true , values : [] }, yaxes : [ { format : short , label : null , logBase : 1 , max : null , min : null , show : true }, { format : short , label : null , logBase : 1 , max : null , min : null , show : true } ], yaxis : { align : false , alignLevel : null } }, { description : The ratio of open file descriptors\\nSee: https://support.cloudbees.com/hc/en-us/articles/204246140-Too-many-open-files , gridPos : { h : 8 , w : 5 , x : 10 , y : 59 }, id : 28 , links : [], options : { displayMode : basic , fieldOptions : { calcs : [ last ], defaults : { max : 1 , min : 0 , unit : percentunit }, mappings : [], override : {}, thresholds : [ { color : blue , index : 0 , value : null }, { color : orange , index : 1 , value : 60 }, { color : red , index : 2 , value : 80 } ], values : false }, orientation : horizontal }, targets : [ { expr : vm_file_descriptor_ratio{kubernetes_namespace=~\\ $namespace\\ , app_kubernetes_io_instance=~\\ $instance\\ } , format : time_series , instant : true , intervalFactor : 1 , legendFormat : {{app_kubernetes_io_instance}} , refId : A } ], timeFrom : null , timeShift : null , title : File Descriptor Ratio , type : bargauge } ], refresh : 30s , schemaVersion : 18 , style : dark , tags : [ Jenkins , Prometheus ], templating : { list : [ { current : { text : Prometheus , value : Prometheus }, hide : 0 , includeAll : false , label : null , multi : false , name : cluster , options : [], query : prometheus , refresh : 1 , regex : , skipUrlSync : false , type : datasource }, { allValue : null , current : { text : All , value : [ $__all ] }, datasource : Prometheus , definition : label_values(jenkins_health_check_duration, app_kubernetes_io_instance) , hide : 0 , includeAll : true , label : Master , multi : true , name : instance , options : [], query : label_values(jenkins_health_check_duration, app_kubernetes_io_instance) , refresh : 2 , regex : , skipUrlSync : false , sort : 1 , tagValuesQuery : , tags : [], tagsQuery : , type : query , useTags : false }, { allValue : null , current : { text : All , value : $__all }, datasource : Prometheus , definition : label_values(jenkins_health_check_duration, kubernetes_namespace) , hide : 0 , includeAll : true , label : Namespace , multi : true , name : namespace , options : [], query : label_values(jenkins_health_check_duration, kubernetes_namespace) , refresh : 2 , regex : , skipUrlSync : false , sort : 1 , tagValuesQuery : , tags : [], tagsQuery : , type : query , useTags : false }, { allValue : null , current : { text : All , value : $__all }, datasource : Prometheus , definition : label_values(kube_node_info{component=\\ kube-state-metrics\\ }, node) , hide : 0 , includeAll : true , label : K8S Node , multi : true , name : node , options : [], query : label_values(kube_node_info{component=\\ kube-state-metrics\\ }, node) , refresh : 1 , regex : , skipUrlSync : false , sort : 5 , tagValuesQuery : , tags : [], tagsQuery : , type : query , useTags : false }, { allValue : null , current : { text : jx-production , value : jx-production }, datasource : Prometheus , definition : label_values(jenkins_health_check_duration, kubernetes_namespace) , hide : 0 , includeAll : false , label : null , multi : false , name : Test , options : [], query : label_values(jenkins_health_check_duration, kubernetes_namespace) , refresh : 2 , regex : , skipUrlSync : false , sort : 1 , tagValuesQuery : , tags : [], tagsQuery : , type : query , useTags : false } ] }, time : { from : now-6h , to : now }, timepicker : { refresh_intervals : [ 5s , 10s , 30s , 1m , 5m , 15m , 30m , 1h , 2h , 1d ], time_options : [ 5m , 15m , 1h , 6h , 12h , 24h , 2d , 7d , 30d ] }, timezone : , title : Jenkins Masters , uid : 8Z9-POHWz , version : 9 }","title":"Dashboard json"},{"location":"blogs/monitor-jenkins-on-k8s/install/","text":"Install Components for Monitoring This chapter is about installing all the tools we need for completing this guide. If you already have these tools installed, feel free to skip the actual installations. However, do make sure to confirm you have a compatible configuration. Important This guide is written during August/September 2019, during which Helm 3 entered Beta. This guide assumes Helm 2, be mindful of the Helm version you are running! Prepare First, we make sure we have hostnames for our services, including Prometheus, Alertmanager, and Grafana. 1 export DOMAIN = 1 2 3 export PROM_ADDR = mon. ${ DOMAIN } export AM_ADDR = alertmanager. ${ DOMAIN } export GRAFANA_ADDR = grafana. ${ DOMAIN } Then we create a namespace to host the monitoring tools. 1 2 kubectl create namespace mon kubens mon Install Prometheus Alertmanager By default, the Helm chart of Prometheus installs Alertmanager as well. To access the UI of Alertmanager, we also set its Ingress' hostname. 1 2 3 4 5 6 7 helm upgrade -i prometheus \\ stable/prometheus \\ --namespace mon \\ --version 7 .1.3 \\ --set server.ingress.hosts ={ $PROM_ADDR } \\ --set alertmanager.ingress.hosts ={ $AM_ADDR } \\ -f prom-values.yaml Use the below command to wait for the deployment of Prometheus to be completed. 1 2 3 kubectl -n mon \\ rollout status \\ deploy prometheus-server prom-values.yaml Below is an example helm values . yaml for Prometheus. It shows how to set resources limits and request, some alerts, and how to configure sending these alerts to Slack. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 server : ingress : enabled : true annotations : ingress.kubernetes.io/ssl-redirect : false nginx.ingress.kubernetes.io/ssl-redirect : false resources : limits : cpu : 100m memory : 1000Mi requests : cpu : 10m memory : 500Mi alertmanager : ingress : enabled : true annotations : ingress.kubernetes.io/ssl-redirect : false nginx.ingress.kubernetes.io/ssl-redirect : false resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi kubeStateMetrics : resources : limits : cpu : 10m memory : 50Mi requests : cpu : 5m memory : 25Mi nodeExporter : resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi pushgateway : resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi serverFiles : alerts : groups : - name : nodes rules : - alert : JenkinsToManyJobsQueued expr : sum(jenkins_queue_size_value) 5 for : 3m labels : severity : notify annotations : summary : Jenkins to many jobs queued description : A Jenkins instance is failing a health check alertmanagerFiles : alertmanager.yml : global : {} route : group_wait : 10s group_interval : 5m receiver : slack repeat_interval : 3h routes : - receiver : slack repeat_interval : 5d match : severity : notify frequency : low receivers : - name : slack slack_configs : - api_url : XXXXXXXXXX send_resolved : true title : {{ .CommonAnnotations.summary }} text : {{ .CommonAnnotations.description }} title_link : http://example.com Install Grafana !!! note At the time of writing (September 2019) we cannot use the latest version of the Grafana helm chart. 1 2 * https : // github . com / helm / charts / pull / 15702 * https : // github . com / helm / charts / issues / 15725 We install Grafana in the same namespace as Prometheus and Alertmanager. 1 2 3 4 5 helm upgrade -i grafana stable/grafana \\ --version 3 .5.5 \\ --namespace mon \\ --set ingress.hosts = { $GRAFANA_ADDR } \\ --values grafana-values.yaml 1 kubectl -n mon rollout status deployment grafana Once the deployment is rolled out, we can either directly open the Grafana UI or echo the address and copy paste it. 1 echo http:// $GRAFANA_ADDR 1 open http:// $GRAFANA_ADDR By default, the Grafana helm chart generates a password for you, with the command below you can retrieve it. 1 2 3 4 kubectl -n mon \\ get secret grafana \\ -o jsonpath = {.data.admin-password} \\ | base64 --decode ; echo 1 open https://grafana.com/dashboards grafana-values.yaml Below is an example configuration for a helm values . yaml , which also includes some useful dashboards by default. We've also configured a default Datasource, pointing to the Prometheus installed earlier. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 ingress : enabled : true persistence : enabled : true accessModes : - ReadWriteOnce size : 1Gi resources : limits : cpu : 20m memory : 50Mi requests : cpu : 5m memory : 25Mi datasources : datasources.yaml : apiVersion : 1 datasources : - name : Prometheus type : prometheus url : http://prometheus-server access : proxy isDefault : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : Default orgId : 1 folder : default type : file disableDeletion : true editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : Costs-Pod : gnetId : 6879 revision : 1 datasource : Prometheus Costs : gnetId : 8670 revision : 1 datasource : Prometheus Summary : gnetId : 8685 revision : 1 datasource : Prometheus Capacity : gnetId : 5228 revision : 6 datasource : Prometheus Deployments : gnetId : 8588 revision : 1 datasource : Prometheus Volumes : gnetId : 6739 revision : 1 datasource : Prometheus Install Jenkins Now that we've taken care of the monitoring tools, we can install Jenkins. We start by creating a namespace for Jenkins to land in. 1 2 kubectl create namespace jenkins kubens jenkins There are many ways of installing Jenkins. There is a very well maintained Helm chart, which is well suited for what we want to achieve. !!! note It is recommended to spread teams and applications across Jenkins masters rather than put everything into a single instance. So in this guide we create two identical Jenkins Masters, each with a unique hostname, to simulate this and show that the alerts and dashboards work for one or more Jenkins masters. Although the Helm chart is a very good starting point, we still need a values . yaml file to configure a few things. Helm Values Explained Let's explain some of the values: installPlugins : we want blueocean for a more beautiful Pipeline UI and prometheus to expose the metrics in a Prometheus format resources : always specify your resources, if these are wrong, our monitoring alerts and dashboard should help use tweak these values javaOpts : for some reason, the default configuration doesn't have the recommended JVM and Garbage Collection configuration, so we have to specify this, see CloudBees' JVM Troubleshoot Guide for more details ingress : because I believe every publicly available service should only be accessible via TLS, we have to configure TLS and certmanager annotations (as we're using Certmanager to manage our certificate) podAnnotations : the default metrics endpoint that Prometheus scrapes from is / metrics , unfortunately, the by default included Metrics Plugin exposes the metrics on that endpoint in the wrong format. This means we have to inform Prometheus how to retrieve the metrics Make sure both jenkins - values . X . yaml and jenkins - certificate . X . yaml are created according to the template files below. Replace the X for each master, if you want three, you'll have .1.yaml , .2.yaml and .3.yaml for each of the files. Replace the ReplaceWithYourDNS with your DNS Host name and the X with the appropriate number. For example, if your host name is example . com , you will have the following: 1 2 3 4 5 hostName : jenkins1.example.com tls : - secretName : tls-jenkins-1 hosts : - jenkins1.example.com Use this file as the starting point for each of the masters. I would recommend making your changes in this file first and then make two copies and update the X value with 1 and 2 respectively. jenkins-values.X.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 master : serviceType : ClusterIP installPlugins : - blueocean:1.17.0 - prometheus:2.0.0 - kubernetes:1.17.2 resources : requests : cpu : 250m memory : 1024Mi limits : cpu : 1000m memory : 2048Mi javaOpts : -XX:+AlwaysPreTouch -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+ParallelRefProcEnabled -XX:+DisableExplicitGC -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions ingress : enabled : true hostName : jenkinsX. ReplaceWithYourDNS tls : - secretName : tls-jenkins-X hosts : - jenkinsX. ReplaceWithYourDNS annotations : certmanager.k8s.io/cluster-issuer : letsencrypt-prod kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : false nginx.ingress.kubernetes.io/proxy-body-size : 50m nginx.ingress.kubernetes.io/proxy-request-buffering : off nginx.ingress.kubernetes.io/ssl-redirect : true podAnnotations : prometheus.io/path : /prometheus prometheus.io/port : 8080 prometheus.io/scrape : true agent : enabled : true rbac : create : true If you want to use TLS for Jenkins, this is an example Certificate. If you don't already have certmanager configured, take a look at my guide on leveraging Let's Encrypt in Kubernetes . jenkins-certificate.X.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : jenkinsX. ReplaceWithYourDNS spec : secretName : tls-jenkins-X dnsNames : - jenkinsX. ReplaceWithYourDNS acme : config : - http01 : ingressClass : nginx domains : - jenkinsX. ReplaceWithYourDNS issuerRef : name : letsencrypt-prod kind : ClusterIssuer Master One Assuming you've created unique Helm values files for both Master One and Master Two, we can start with creating the first one. 1 2 3 4 helm upgrade -i jenkins \\ stable/jenkins \\ --namespace jenkins \\ -f jenkins-values.1.yaml Apply Certificate If you have the certificate, apply it to the cluster. 1 kubectl apply -f jenkins-certificate.1.yaml Wait for rollout If you want to wait for the Jenkins deployment to be completed, use the following command. 1 kubectl -n jenkins rollout status deployment jenkins1 Retrieve Password The Jenkins Helm chart also generates a admin password for you. See the command below on how to retrieve it. 1 printf $( kubectl get secret --namespace jenkins jenkins1 -o jsonpath = {.data.jenkins-admin-password} | base64 --decode ) ; echo Master Two Let's create Master Two as well, same deal as before. The commands are here for convenience, so you can use the [] in the top right to copy and paste easily. 1 2 3 4 helm upgrade -i jenkins2 \\ stable/jenkins \\ --namespace jenkins \\ -f jenkins-values.2.yaml Wait for rollout 1 kubectl -n jenkins rollout status deployment jenkins2 Apply Certificate 1 kubectl apply -f jenkins-certificate.2.yaml Retrieve Password 1 printf $( kubectl get secret --namespace jenkins jenkins2 -o jsonpath = {.data.jenkins-admin-password} | base64 --decode ) ; echo","title":"Install Tools"},{"location":"blogs/monitor-jenkins-on-k8s/install/#install-components-for-monitoring","text":"This chapter is about installing all the tools we need for completing this guide. If you already have these tools installed, feel free to skip the actual installations. However, do make sure to confirm you have a compatible configuration. Important This guide is written during August/September 2019, during which Helm 3 entered Beta. This guide assumes Helm 2, be mindful of the Helm version you are running!","title":"Install Components for Monitoring"},{"location":"blogs/monitor-jenkins-on-k8s/install/#prepare","text":"First, we make sure we have hostnames for our services, including Prometheus, Alertmanager, and Grafana. 1 export DOMAIN = 1 2 3 export PROM_ADDR = mon. ${ DOMAIN } export AM_ADDR = alertmanager. ${ DOMAIN } export GRAFANA_ADDR = grafana. ${ DOMAIN } Then we create a namespace to host the monitoring tools. 1 2 kubectl create namespace mon kubens mon","title":"Prepare"},{"location":"blogs/monitor-jenkins-on-k8s/install/#install-prometheus-alertmanager","text":"By default, the Helm chart of Prometheus installs Alertmanager as well. To access the UI of Alertmanager, we also set its Ingress' hostname. 1 2 3 4 5 6 7 helm upgrade -i prometheus \\ stable/prometheus \\ --namespace mon \\ --version 7 .1.3 \\ --set server.ingress.hosts ={ $PROM_ADDR } \\ --set alertmanager.ingress.hosts ={ $AM_ADDR } \\ -f prom-values.yaml Use the below command to wait for the deployment of Prometheus to be completed. 1 2 3 kubectl -n mon \\ rollout status \\ deploy prometheus-server prom-values.yaml Below is an example helm values . yaml for Prometheus. It shows how to set resources limits and request, some alerts, and how to configure sending these alerts to Slack. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 server : ingress : enabled : true annotations : ingress.kubernetes.io/ssl-redirect : false nginx.ingress.kubernetes.io/ssl-redirect : false resources : limits : cpu : 100m memory : 1000Mi requests : cpu : 10m memory : 500Mi alertmanager : ingress : enabled : true annotations : ingress.kubernetes.io/ssl-redirect : false nginx.ingress.kubernetes.io/ssl-redirect : false resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi kubeStateMetrics : resources : limits : cpu : 10m memory : 50Mi requests : cpu : 5m memory : 25Mi nodeExporter : resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi pushgateway : resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi serverFiles : alerts : groups : - name : nodes rules : - alert : JenkinsToManyJobsQueued expr : sum(jenkins_queue_size_value) 5 for : 3m labels : severity : notify annotations : summary : Jenkins to many jobs queued description : A Jenkins instance is failing a health check alertmanagerFiles : alertmanager.yml : global : {} route : group_wait : 10s group_interval : 5m receiver : slack repeat_interval : 3h routes : - receiver : slack repeat_interval : 5d match : severity : notify frequency : low receivers : - name : slack slack_configs : - api_url : XXXXXXXXXX send_resolved : true title : {{ .CommonAnnotations.summary }} text : {{ .CommonAnnotations.description }} title_link : http://example.com","title":"Install Prometheus &amp; Alertmanager"},{"location":"blogs/monitor-jenkins-on-k8s/install/#install-grafana","text":"!!! note At the time of writing (September 2019) we cannot use the latest version of the Grafana helm chart. 1 2 * https : // github . com / helm / charts / pull / 15702 * https : // github . com / helm / charts / issues / 15725 We install Grafana in the same namespace as Prometheus and Alertmanager. 1 2 3 4 5 helm upgrade -i grafana stable/grafana \\ --version 3 .5.5 \\ --namespace mon \\ --set ingress.hosts = { $GRAFANA_ADDR } \\ --values grafana-values.yaml 1 kubectl -n mon rollout status deployment grafana Once the deployment is rolled out, we can either directly open the Grafana UI or echo the address and copy paste it. 1 echo http:// $GRAFANA_ADDR 1 open http:// $GRAFANA_ADDR By default, the Grafana helm chart generates a password for you, with the command below you can retrieve it. 1 2 3 4 kubectl -n mon \\ get secret grafana \\ -o jsonpath = {.data.admin-password} \\ | base64 --decode ; echo 1 open https://grafana.com/dashboards grafana-values.yaml Below is an example configuration for a helm values . yaml , which also includes some useful dashboards by default. We've also configured a default Datasource, pointing to the Prometheus installed earlier. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 ingress : enabled : true persistence : enabled : true accessModes : - ReadWriteOnce size : 1Gi resources : limits : cpu : 20m memory : 50Mi requests : cpu : 5m memory : 25Mi datasources : datasources.yaml : apiVersion : 1 datasources : - name : Prometheus type : prometheus url : http://prometheus-server access : proxy isDefault : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : Default orgId : 1 folder : default type : file disableDeletion : true editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : Costs-Pod : gnetId : 6879 revision : 1 datasource : Prometheus Costs : gnetId : 8670 revision : 1 datasource : Prometheus Summary : gnetId : 8685 revision : 1 datasource : Prometheus Capacity : gnetId : 5228 revision : 6 datasource : Prometheus Deployments : gnetId : 8588 revision : 1 datasource : Prometheus Volumes : gnetId : 6739 revision : 1 datasource : Prometheus","title":"Install Grafana"},{"location":"blogs/monitor-jenkins-on-k8s/install/#install-jenkins","text":"Now that we've taken care of the monitoring tools, we can install Jenkins. We start by creating a namespace for Jenkins to land in. 1 2 kubectl create namespace jenkins kubens jenkins There are many ways of installing Jenkins. There is a very well maintained Helm chart, which is well suited for what we want to achieve. !!! note It is recommended to spread teams and applications across Jenkins masters rather than put everything into a single instance. So in this guide we create two identical Jenkins Masters, each with a unique hostname, to simulate this and show that the alerts and dashboards work for one or more Jenkins masters. Although the Helm chart is a very good starting point, we still need a values . yaml file to configure a few things.","title":"Install Jenkins"},{"location":"blogs/monitor-jenkins-on-k8s/install/#helm-values-explained","text":"Let's explain some of the values: installPlugins : we want blueocean for a more beautiful Pipeline UI and prometheus to expose the metrics in a Prometheus format resources : always specify your resources, if these are wrong, our monitoring alerts and dashboard should help use tweak these values javaOpts : for some reason, the default configuration doesn't have the recommended JVM and Garbage Collection configuration, so we have to specify this, see CloudBees' JVM Troubleshoot Guide for more details ingress : because I believe every publicly available service should only be accessible via TLS, we have to configure TLS and certmanager annotations (as we're using Certmanager to manage our certificate) podAnnotations : the default metrics endpoint that Prometheus scrapes from is / metrics , unfortunately, the by default included Metrics Plugin exposes the metrics on that endpoint in the wrong format. This means we have to inform Prometheus how to retrieve the metrics Make sure both jenkins - values . X . yaml and jenkins - certificate . X . yaml are created according to the template files below. Replace the X for each master, if you want three, you'll have .1.yaml , .2.yaml and .3.yaml for each of the files. Replace the ReplaceWithYourDNS with your DNS Host name and the X with the appropriate number. For example, if your host name is example . com , you will have the following: 1 2 3 4 5 hostName : jenkins1.example.com tls : - secretName : tls-jenkins-1 hosts : - jenkins1.example.com Use this file as the starting point for each of the masters. I would recommend making your changes in this file first and then make two copies and update the X value with 1 and 2 respectively. jenkins-values.X.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 master : serviceType : ClusterIP installPlugins : - blueocean:1.17.0 - prometheus:2.0.0 - kubernetes:1.17.2 resources : requests : cpu : 250m memory : 1024Mi limits : cpu : 1000m memory : 2048Mi javaOpts : -XX:+AlwaysPreTouch -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+ParallelRefProcEnabled -XX:+DisableExplicitGC -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions ingress : enabled : true hostName : jenkinsX. ReplaceWithYourDNS tls : - secretName : tls-jenkins-X hosts : - jenkinsX. ReplaceWithYourDNS annotations : certmanager.k8s.io/cluster-issuer : letsencrypt-prod kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : false nginx.ingress.kubernetes.io/proxy-body-size : 50m nginx.ingress.kubernetes.io/proxy-request-buffering : off nginx.ingress.kubernetes.io/ssl-redirect : true podAnnotations : prometheus.io/path : /prometheus prometheus.io/port : 8080 prometheus.io/scrape : true agent : enabled : true rbac : create : true If you want to use TLS for Jenkins, this is an example Certificate. If you don't already have certmanager configured, take a look at my guide on leveraging Let's Encrypt in Kubernetes . jenkins-certificate.X.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : jenkinsX. ReplaceWithYourDNS spec : secretName : tls-jenkins-X dnsNames : - jenkinsX. ReplaceWithYourDNS acme : config : - http01 : ingressClass : nginx domains : - jenkinsX. ReplaceWithYourDNS issuerRef : name : letsencrypt-prod kind : ClusterIssuer","title":"Helm Values Explained"},{"location":"blogs/monitor-jenkins-on-k8s/install/#master-one","text":"Assuming you've created unique Helm values files for both Master One and Master Two, we can start with creating the first one. 1 2 3 4 helm upgrade -i jenkins \\ stable/jenkins \\ --namespace jenkins \\ -f jenkins-values.1.yaml","title":"Master One"},{"location":"blogs/monitor-jenkins-on-k8s/install/#apply-certificate","text":"If you have the certificate, apply it to the cluster. 1 kubectl apply -f jenkins-certificate.1.yaml","title":"Apply Certificate"},{"location":"blogs/monitor-jenkins-on-k8s/install/#wait-for-rollout","text":"If you want to wait for the Jenkins deployment to be completed, use the following command. 1 kubectl -n jenkins rollout status deployment jenkins1","title":"Wait for rollout"},{"location":"blogs/monitor-jenkins-on-k8s/install/#retrieve-password","text":"The Jenkins Helm chart also generates a admin password for you. See the command below on how to retrieve it. 1 printf $( kubectl get secret --namespace jenkins jenkins1 -o jsonpath = {.data.jenkins-admin-password} | base64 --decode ) ; echo","title":"Retrieve Password"},{"location":"blogs/monitor-jenkins-on-k8s/install/#master-two","text":"Let's create Master Two as well, same deal as before. The commands are here for convenience, so you can use the [] in the top right to copy and paste easily. 1 2 3 4 helm upgrade -i jenkins2 \\ stable/jenkins \\ --namespace jenkins \\ -f jenkins-values.2.yaml","title":"Master Two"},{"location":"blogs/monitor-jenkins-on-k8s/install/#wait-for-rollout_1","text":"1 kubectl -n jenkins rollout status deployment jenkins2","title":"Wait for rollout"},{"location":"blogs/monitor-jenkins-on-k8s/install/#apply-certificate_1","text":"1 kubectl apply -f jenkins-certificate.2.yaml","title":"Apply Certificate"},{"location":"blogs/monitor-jenkins-on-k8s/install/#retrieve-password_1","text":"1 printf $( kubectl get secret --namespace jenkins jenkins2 -o jsonpath = {.data.jenkins-admin-password} | base64 --decode ) ; echo","title":"Retrieve Password"},{"location":"blogs/monitor-jenkins-on-k8s/introduction/","text":"Introduction This is a guide on monitoring Jenkins on Kubernetes with Prometheus and Grafana. There are other solutions out there, and if you want to learn those, I suggest to look elsewhere. If you're interested in diving in Jenkins Metrics and how to make sense of them with Prometheus and Grafana, read on! What We Will Do The outline of the steps to take is below. Each has its own page, so if you feel you have create a Kubernetes cluster configure the cluster (e.g. Helm) install Prometheus and Grafana install one or more Jenkins instances get metrics from running Jenkins instance(s) have queries for understanding the state and performance of the Jenkins instance(s) have a dashboard to aid debugging an issue or determine new alerts have alerts that fire when (potential) problematic conditions occur get metrics from Jenkins Pipelines Resources The list below is both a shout out to the resources I learned from and as a reference for you if you want to learn more. https://go.cloudbees.com/docs/solutions/jvm-troubleshooting/ https://go.cloudbees.com/docs/cloudbees-documentation/devoptics-user-guide/run_insights/ https://medium.com/@eng.mohamed.m.saeed/monitoring-jenkins-with-grafana-and-prometheus-a7e037cbb376 https://stackoverflow.com/questions/52230653/graphite-jenkins-job-level-metrics https://towardsdatascience.com/jenkins-events-logs-and-metrics-7c3e8b28962b https://github.com/nvgoldin/jenkins-graphite https://www.weave.works/blog/promql-queries-for-the-rest-of-us/ https://medium.com/quiq-blog/prometheus-relabeling-tricks-6ae62c56cbda https://docs.google.com/presentation/d/1gtqEfTKM3oLr1N9zjAeXtOcS1eAQS--Xz0D4hwlo_KQ/edit#slide=id.g5bbd4fcccc_10_10 https://go.cloudbees.com/docs/cloudbees-documentation/devoptics-user-guide/security_privacy/#_verification_of_connection_to_the_devoptics_service https://sysdig.com/blog/golden-signals-kubernetes/ https://stackoverflow.com/questions/47141967/how-to-use-the-selected-period-of-time-in-a-query/47173828#47173828 https://www.robustperception.io/rate-then-sum-never-sum-then-rate https://www.innoq.com/en/blog/prometheus-counters/ https://www.robustperception.io/dont-put-the-value-in-alert-labels https://blog.pvincent.io/2017/12/prometheus-blog-series-part-5-alerting-rules/ https://docs.google.com/document/d/199PqyG3UsyXlwieHaqbGiWVa8eMWi8zzAn0YfcApr8Q/edit https://wiki.jenkins.io/display/JENKINS/Metrics+Plugin https://www.robustperception.io/controlling-the-instance-label https://www.robustperception.io/target-labels-are-for-life-not-just-for-christmas https://prometheus.io/docs/alerting/notifications/ https://piotrminkowski.wordpress.com/2017/08/29/visualizing-jenkins-pipeline-results-in-grafana/ https://medium.com/@jotak/designing-prometheus-metrics-72dcff88c2e5 https://github.com/prometheus/pushgateway https://github.com/prometheus/client_golang https://blog.pvincent.io/2017/12/prometheus-blog-series-part-2-metric-types/ https://prometheus.io/docs/concepts/jobs_instances/","title":"Introduction"},{"location":"blogs/monitor-jenkins-on-k8s/introduction/#introduction","text":"This is a guide on monitoring Jenkins on Kubernetes with Prometheus and Grafana. There are other solutions out there, and if you want to learn those, I suggest to look elsewhere. If you're interested in diving in Jenkins Metrics and how to make sense of them with Prometheus and Grafana, read on!","title":"Introduction"},{"location":"blogs/monitor-jenkins-on-k8s/introduction/#what-we-will-do","text":"The outline of the steps to take is below. Each has its own page, so if you feel you have create a Kubernetes cluster configure the cluster (e.g. Helm) install Prometheus and Grafana install one or more Jenkins instances get metrics from running Jenkins instance(s) have queries for understanding the state and performance of the Jenkins instance(s) have a dashboard to aid debugging an issue or determine new alerts have alerts that fire when (potential) problematic conditions occur get metrics from Jenkins Pipelines","title":"What We Will Do"},{"location":"blogs/monitor-jenkins-on-k8s/introduction/#resources","text":"The list below is both a shout out to the resources I learned from and as a reference for you if you want to learn more. https://go.cloudbees.com/docs/solutions/jvm-troubleshooting/ https://go.cloudbees.com/docs/cloudbees-documentation/devoptics-user-guide/run_insights/ https://medium.com/@eng.mohamed.m.saeed/monitoring-jenkins-with-grafana-and-prometheus-a7e037cbb376 https://stackoverflow.com/questions/52230653/graphite-jenkins-job-level-metrics https://towardsdatascience.com/jenkins-events-logs-and-metrics-7c3e8b28962b https://github.com/nvgoldin/jenkins-graphite https://www.weave.works/blog/promql-queries-for-the-rest-of-us/ https://medium.com/quiq-blog/prometheus-relabeling-tricks-6ae62c56cbda https://docs.google.com/presentation/d/1gtqEfTKM3oLr1N9zjAeXtOcS1eAQS--Xz0D4hwlo_KQ/edit#slide=id.g5bbd4fcccc_10_10 https://go.cloudbees.com/docs/cloudbees-documentation/devoptics-user-guide/security_privacy/#_verification_of_connection_to_the_devoptics_service https://sysdig.com/blog/golden-signals-kubernetes/ https://stackoverflow.com/questions/47141967/how-to-use-the-selected-period-of-time-in-a-query/47173828#47173828 https://www.robustperception.io/rate-then-sum-never-sum-then-rate https://www.innoq.com/en/blog/prometheus-counters/ https://www.robustperception.io/dont-put-the-value-in-alert-labels https://blog.pvincent.io/2017/12/prometheus-blog-series-part-5-alerting-rules/ https://docs.google.com/document/d/199PqyG3UsyXlwieHaqbGiWVa8eMWi8zzAn0YfcApr8Q/edit https://wiki.jenkins.io/display/JENKINS/Metrics+Plugin https://www.robustperception.io/controlling-the-instance-label https://www.robustperception.io/target-labels-are-for-life-not-just-for-christmas https://prometheus.io/docs/alerting/notifications/ https://piotrminkowski.wordpress.com/2017/08/29/visualizing-jenkins-pipeline-results-in-grafana/ https://medium.com/@jotak/designing-prometheus-metrics-72dcff88c2e5 https://github.com/prometheus/pushgateway https://github.com/prometheus/client_golang https://blog.pvincent.io/2017/12/prometheus-blog-series-part-2-metric-types/ https://prometheus.io/docs/concepts/jobs_instances/","title":"Resources"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/","text":"Metrics Jenkins is a Java Web application, in this case running in Kubernetes. Let's categorize the metrics we want to look at and deal with each group individually. JVM : the JVM metrics are exposed, we should leverage this for particular queries and alerts Jenkins Configuration : the default configuration exposes some configuration elements, a few of these have strong recommended values, such as Master Executor Slots (should always be 0 ) Jenkins Usage : jobs running in Jenkins, or Jobs not running in Jenkins can also tell us about (potential) problems Web : although it is not the primary function of Jenkins, web access gives hints about performance trends Pod Metrics : any generic metric from a Kubernetes Pod perspective can be helpful to look at Kubernetes Labels This guide assumes you install Jenkins via the Helm chart as explained elsewhere in the guide. This means it assumes the Jenkins instances all have the label app . kubernetes . io / instance . In Prometheus, this becomes app_kubernetes_io_instance . If you install your applications (such as Jenkins) in other ways, either change the queries presented here accordingly or add the label. Types of Metrics to evaluate We are in the age where SRE, DevOps Engineer, and Platform Engineer are hyped terms. Hyped they may be, there is a good reason people are making noise about monitoring. A lot is written about which kinds of metrics to observe and which to ignore. There's enough written about this - including Viktor Farcic's excellent DevOps Toolkit 2.5 - so we skip diving into these. In case you haven't read anything about it, let's briefly look at the types of metrics. Latency : response times of your application, in our case, both external access via Ingress and internal access. We can measure latency on internal access via Jenkins' own metrics, which also has percentile information (for example, p99) Errors : we can take a look at network errors such as HTTP 500, which we get straight from Jenkins' webserver (Netty) and at failed jobs Traffic : the number of connections to our service, in our case we have web traffic and jobs running, both we get from Jenkins Saturation : how much the system is used compared to the available resources, core resources such as CPU and Memory primarily depend on your Kubernetes Nodes. However, we can take a look at the Pod's limits vs. requests and Jenkins' job wait time - which roughly translates to saturation JVM Metrics We have some basic JVM metrics, such as CPU and Memory usage, and uptime. Uptime In the age of containers, uptime is not a very useful or sexy metric to observe. I include it because we can use uptime as a proxy metric. For example, if a service never goes beyond a particular value - Prometheus records Max values - it can signify a problem elsewhere. 1 vm_cpu_load 1 ( vm_memory_total_max - vm_memory_total_used ) / vm_memory_total_max * 100 . 0 1 vm_uptime_milliseconds Garbage Collection For fine-tuning the JVM's garbage collection for Jenkins, there are two central guides from CloudBees. Which also explain the JVM_OPTIONS in the jenkins - values . yaml we used for the Helm installation. Guide On Preparing Jenkins For Support JVM Troubleshooting Guide The second article contains much information on how to analyze the Garbage Collection logs and metrics. To process the data thoroughly requires experts with specially designed tools. I am not such an expert, nor is this the document to guide you through this. Summarizing the two guides: measure core metrics and Garbage Collection Throughput. If you need more, consult experts. Garbage Collection Throughput 1 2 3 4 5 6 7 8 9 1 - sum ( rate ( vm_gc_G1_Young_Generation_time [ 5 m ] ) ) by ( app_kubernetes_io_instance ) / sum ( vm_uptime_milliseconds ) by ( app_kubernetes_io_instance ) Check for too many open files When looking at the CloudBees guide on tuning performance on Linux , one of the main things to look are core metrics (Memory and CPU) and Open Files. There's even an explicit guide on monitoring the number of open files . 1 vm_file_descriptor_ratio Jenkins Config Metrics Some of the metrics are derived from the configuration of a Jenkins Master. Plugins While Jenkins' extensive community is often praised for the number of plugins created and maintained, the plugins are also a big source of risk. You probably want to set a baseline and determine a value for when to send an alert. 1 jenkins_plugins_active Jenkins Build Nodes Jenkins should never build on a master, always on a node or agent. 1 jenkins_executor_count_value You might use static agents or, while we're in Kubernetes, only have dynamic agents. Either way, having nodes offline for a while signifies a problem. Maybe the Node configuration is wrong, or the PodTemplate has a mistake, or maybe your ServiceAccount doesn't have the correct permissions. 1 jenkins_node_offline_value Jenkins Usage Metrics Most of Jenkins' metrics relate to its usage, though. Think about metrics regarding HTTP request duration, number server errors (HTTP 500), and all the metrics related to builds. Builds Per Day 1 sum ( increase ( jenkins_runs_total_total [ 24 h ])) by ( app_kubernetes_io_instance ) Job duration 1 default_jenkins_builds_last_build_duration_milliseconds Job Count 1 jenkins_job_count_value Jobs in Queue If a Jenkins master is overloaded, it is likely to fall behind building jobs that are scheduled. Jenkins observes the duration a job spends in the queue ( jenkins_job_queuing_duration ) and the current queue size ( jenkins_queue_size_value ). 1 jenkins_job_queuing_duration 1 sum ( jenkins_queue_size_value ) by ( app_kubernetes_io_instance ) Web Metrics As Jenkins is also a web application, it makes to look at its HTTP related metrics as well. Route Of External Traffic It is important to note that the HTTP traffic of user interaction with Jenkins when running in Kubernetes can contain quite a lot of layers. Problems can arise in any of these layers, so it is crucial to monitor traffic to a service on multiple layers to speed debug time. Tracing is a great solution but out of scope for this guide. HTTP Requests The 99 th percentile of HTTP Requests handled by Jenkins masters. 1 sum ( http_requests { quantile = 0.99 } ) by ( app_kubernetes_io_instance ) 99 th percentile We look at percentiles because average times are not very helpful. For more information on why this is so, please consult the Google SRE book which is free online. Health Check Duration How long the health check takes to complete at the 99 th percentile. Higher numbers signify problems. 1 2 sum ( rate ( jenkins_health_check_duration { quantile = 0.99 }[ 5 m ])) by ( app_kubernetes_io_instance ) Ingress Performance In this case, we look at the metrics of the Nginx Ingress Controller. If you use a different controller, rewrite the query to a sensible alternative. 1 2 3 4 5 6 7 8 9 10 sum ( rate ( nginx_ingress_controller_request_duration_seconds_bucket { le = 0.25 } [ 5 m ] )) by ( ingress ) / sum ( rate ( nginx_ingress_controller_request_duration_seconds_count [ 5 m ] )) by ( ingress ) Number of Good Request vs. Request 1 2 3 4 sum ( http_responseCodes_ok_total ) by ( kubernetes_pod_name ) / sum ( http_requests_count ) by ( kubernetes_pod_name ) Pod Metrics These metrics are purely related to the Kubernetes Pods. They are as such, applicable to more applications than just Jenkins. CPU Usage 1 2 3 4 5 6 sum ( rate ( container_cpu_usage_seconds_total { container_name = jenkins* } [ 5 m ] )) by ( pod_name ) Query Filters In this case, we filter on those containers with name jenkins * , which means any container whose name has jenkins as the prefix. If you want to have more than one prefix or suffix, you can use || . So, if you would want to combine Jenkins with, let's say, prometheus , you will get the following. 1 container_name = jenkins*||prometheus* Oversubscription of Pod memory While requests are not meant to be binding, if you think your application requests around 1GB and it is using well over 3GB, something is off. Either you are too naive and should update the requests , or something is wrong, and you need to take action. 1 2 3 4 5 6 7 8 9 10 11 sum ( label_join ( container_memory_usage_bytes { container_name = jenkins } , pod , , , pod_name )) by ( pod ) / sum ( kube_pod_container_resource_requests_memory_bytes { container = jenkins } ) by ( pod ) DevOptics Metrics CloudBees made parts of its DevOptics product free. This product contains - amongst other things - a feature set called Run Insights . This is a monitoring solution where your Jenkins Master uploads its metrics to the CloudBees service, and you get a dashboard with many of the same things already discussed. You might not want to leverage this free service but like some of its dashboard features. I've tried to recreate some of these - in a minimal fashion. Active Runs To know how many current builds there are, we can watch the executors that are in use. 1 sum ( jenkins_executor_in_use_history ) by ( app_kubernetes_io_instance ) Idle Executors When using Kubernetes' Pods as an agent, the only idle executors we'll have are Pods that are done with their build and in the process of being terminated. Not very useful, but in case you want to know how: 1 sum ( jenkins_executor_free_history ) by ( app_kubernetes_io_instance ) Average Time Waiting to Start With Kubernetes PodTemplates we cannot calculate this. The only wait time we get is the one that is between queue'ing a job and requesting the Pod, which isn't very meaningful. Completed Runs Per Day 1 sum ( increase ( jenkins_runs_total_total [ 24 h ])) by ( app_kubernetes_io_instance ) Average Time to complete 1 2 sum ( jenkins_job_building_duration ) by ( app_kubernetes_io_instance ) / sum ( jenkins_job_building_duration_count ) by ( app_kubernetes_io_instance )","title":"Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#metrics","text":"Jenkins is a Java Web application, in this case running in Kubernetes. Let's categorize the metrics we want to look at and deal with each group individually. JVM : the JVM metrics are exposed, we should leverage this for particular queries and alerts Jenkins Configuration : the default configuration exposes some configuration elements, a few of these have strong recommended values, such as Master Executor Slots (should always be 0 ) Jenkins Usage : jobs running in Jenkins, or Jobs not running in Jenkins can also tell us about (potential) problems Web : although it is not the primary function of Jenkins, web access gives hints about performance trends Pod Metrics : any generic metric from a Kubernetes Pod perspective can be helpful to look at Kubernetes Labels This guide assumes you install Jenkins via the Helm chart as explained elsewhere in the guide. This means it assumes the Jenkins instances all have the label app . kubernetes . io / instance . In Prometheus, this becomes app_kubernetes_io_instance . If you install your applications (such as Jenkins) in other ways, either change the queries presented here accordingly or add the label.","title":"Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#types-of-metrics-to-evaluate","text":"We are in the age where SRE, DevOps Engineer, and Platform Engineer are hyped terms. Hyped they may be, there is a good reason people are making noise about monitoring. A lot is written about which kinds of metrics to observe and which to ignore. There's enough written about this - including Viktor Farcic's excellent DevOps Toolkit 2.5 - so we skip diving into these. In case you haven't read anything about it, let's briefly look at the types of metrics. Latency : response times of your application, in our case, both external access via Ingress and internal access. We can measure latency on internal access via Jenkins' own metrics, which also has percentile information (for example, p99) Errors : we can take a look at network errors such as HTTP 500, which we get straight from Jenkins' webserver (Netty) and at failed jobs Traffic : the number of connections to our service, in our case we have web traffic and jobs running, both we get from Jenkins Saturation : how much the system is used compared to the available resources, core resources such as CPU and Memory primarily depend on your Kubernetes Nodes. However, we can take a look at the Pod's limits vs. requests and Jenkins' job wait time - which roughly translates to saturation","title":"Types of Metrics to evaluate"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#jvm-metrics","text":"We have some basic JVM metrics, such as CPU and Memory usage, and uptime. Uptime In the age of containers, uptime is not a very useful or sexy metric to observe. I include it because we can use uptime as a proxy metric. For example, if a service never goes beyond a particular value - Prometheus records Max values - it can signify a problem elsewhere. 1 vm_cpu_load 1 ( vm_memory_total_max - vm_memory_total_used ) / vm_memory_total_max * 100 . 0 1 vm_uptime_milliseconds","title":"JVM Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#garbage-collection","text":"For fine-tuning the JVM's garbage collection for Jenkins, there are two central guides from CloudBees. Which also explain the JVM_OPTIONS in the jenkins - values . yaml we used for the Helm installation. Guide On Preparing Jenkins For Support JVM Troubleshooting Guide The second article contains much information on how to analyze the Garbage Collection logs and metrics. To process the data thoroughly requires experts with specially designed tools. I am not such an expert, nor is this the document to guide you through this. Summarizing the two guides: measure core metrics and Garbage Collection Throughput. If you need more, consult experts. Garbage Collection Throughput 1 2 3 4 5 6 7 8 9 1 - sum ( rate ( vm_gc_G1_Young_Generation_time [ 5 m ] ) ) by ( app_kubernetes_io_instance ) / sum ( vm_uptime_milliseconds ) by ( app_kubernetes_io_instance )","title":"Garbage Collection"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#check-for-too-many-open-files","text":"When looking at the CloudBees guide on tuning performance on Linux , one of the main things to look are core metrics (Memory and CPU) and Open Files. There's even an explicit guide on monitoring the number of open files . 1 vm_file_descriptor_ratio","title":"Check for too many open files"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#jenkins-config-metrics","text":"Some of the metrics are derived from the configuration of a Jenkins Master.","title":"Jenkins Config Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#plugins","text":"While Jenkins' extensive community is often praised for the number of plugins created and maintained, the plugins are also a big source of risk. You probably want to set a baseline and determine a value for when to send an alert. 1 jenkins_plugins_active","title":"Plugins"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#jenkins-build-nodes","text":"Jenkins should never build on a master, always on a node or agent. 1 jenkins_executor_count_value You might use static agents or, while we're in Kubernetes, only have dynamic agents. Either way, having nodes offline for a while signifies a problem. Maybe the Node configuration is wrong, or the PodTemplate has a mistake, or maybe your ServiceAccount doesn't have the correct permissions. 1 jenkins_node_offline_value","title":"Jenkins Build Nodes"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#jenkins-usage-metrics","text":"Most of Jenkins' metrics relate to its usage, though. Think about metrics regarding HTTP request duration, number server errors (HTTP 500), and all the metrics related to builds.","title":"Jenkins Usage Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#builds-per-day","text":"1 sum ( increase ( jenkins_runs_total_total [ 24 h ])) by ( app_kubernetes_io_instance )","title":"Builds Per Day"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#job-duration","text":"1 default_jenkins_builds_last_build_duration_milliseconds","title":"Job duration"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#job-count","text":"1 jenkins_job_count_value","title":"Job Count"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#jobs-in-queue","text":"If a Jenkins master is overloaded, it is likely to fall behind building jobs that are scheduled. Jenkins observes the duration a job spends in the queue ( jenkins_job_queuing_duration ) and the current queue size ( jenkins_queue_size_value ). 1 jenkins_job_queuing_duration 1 sum ( jenkins_queue_size_value ) by ( app_kubernetes_io_instance )","title":"Jobs in Queue"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#web-metrics","text":"As Jenkins is also a web application, it makes to look at its HTTP related metrics as well. Route Of External Traffic It is important to note that the HTTP traffic of user interaction with Jenkins when running in Kubernetes can contain quite a lot of layers. Problems can arise in any of these layers, so it is crucial to monitor traffic to a service on multiple layers to speed debug time. Tracing is a great solution but out of scope for this guide.","title":"Web Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#http-requests","text":"The 99 th percentile of HTTP Requests handled by Jenkins masters. 1 sum ( http_requests { quantile = 0.99 } ) by ( app_kubernetes_io_instance ) 99 th percentile We look at percentiles because average times are not very helpful. For more information on why this is so, please consult the Google SRE book which is free online.","title":"HTTP Requests"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#health-check-duration","text":"How long the health check takes to complete at the 99 th percentile. Higher numbers signify problems. 1 2 sum ( rate ( jenkins_health_check_duration { quantile = 0.99 }[ 5 m ])) by ( app_kubernetes_io_instance )","title":"Health Check Duration"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#ingress-performance","text":"In this case, we look at the metrics of the Nginx Ingress Controller. If you use a different controller, rewrite the query to a sensible alternative. 1 2 3 4 5 6 7 8 9 10 sum ( rate ( nginx_ingress_controller_request_duration_seconds_bucket { le = 0.25 } [ 5 m ] )) by ( ingress ) / sum ( rate ( nginx_ingress_controller_request_duration_seconds_count [ 5 m ] )) by ( ingress )","title":"Ingress Performance"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#number-of-good-request-vs-request","text":"1 2 3 4 sum ( http_responseCodes_ok_total ) by ( kubernetes_pod_name ) / sum ( http_requests_count ) by ( kubernetes_pod_name )","title":"Number of Good Request vs. Request"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#pod-metrics","text":"These metrics are purely related to the Kubernetes Pods. They are as such, applicable to more applications than just Jenkins.","title":"Pod Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#cpu-usage","text":"1 2 3 4 5 6 sum ( rate ( container_cpu_usage_seconds_total { container_name = jenkins* } [ 5 m ] )) by ( pod_name ) Query Filters In this case, we filter on those containers with name jenkins * , which means any container whose name has jenkins as the prefix. If you want to have more than one prefix or suffix, you can use || . So, if you would want to combine Jenkins with, let's say, prometheus , you will get the following. 1 container_name = jenkins*||prometheus*","title":"CPU Usage"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#oversubscription-of-pod-memory","text":"While requests are not meant to be binding, if you think your application requests around 1GB and it is using well over 3GB, something is off. Either you are too naive and should update the requests , or something is wrong, and you need to take action. 1 2 3 4 5 6 7 8 9 10 11 sum ( label_join ( container_memory_usage_bytes { container_name = jenkins } , pod , , , pod_name )) by ( pod ) / sum ( kube_pod_container_resource_requests_memory_bytes { container = jenkins } ) by ( pod )","title":"Oversubscription of Pod memory"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#devoptics-metrics","text":"CloudBees made parts of its DevOptics product free. This product contains - amongst other things - a feature set called Run Insights . This is a monitoring solution where your Jenkins Master uploads its metrics to the CloudBees service, and you get a dashboard with many of the same things already discussed. You might not want to leverage this free service but like some of its dashboard features. I've tried to recreate some of these - in a minimal fashion.","title":"DevOptics Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#active-runs","text":"To know how many current builds there are, we can watch the executors that are in use. 1 sum ( jenkins_executor_in_use_history ) by ( app_kubernetes_io_instance )","title":"Active Runs"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#idle-executors","text":"When using Kubernetes' Pods as an agent, the only idle executors we'll have are Pods that are done with their build and in the process of being terminated. Not very useful, but in case you want to know how: 1 sum ( jenkins_executor_free_history ) by ( app_kubernetes_io_instance )","title":"Idle Executors"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#average-time-waiting-to-start","text":"With Kubernetes PodTemplates we cannot calculate this. The only wait time we get is the one that is between queue'ing a job and requesting the Pod, which isn't very meaningful.","title":"Average Time Waiting to Start"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#completed-runs-per-day","text":"1 sum ( increase ( jenkins_runs_total_total [ 24 h ])) by ( app_kubernetes_io_instance )","title":"Completed Runs Per Day"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#average-time-to-complete","text":"1 2 sum ( jenkins_job_building_duration ) by ( app_kubernetes_io_instance ) / sum ( jenkins_job_building_duration_count ) by ( app_kubernetes_io_instance )","title":"Average Time to complete"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/","text":"Track Metrics of Pipelines Get Data From Jobs Use Prometheus Push Gateway ** via shared lib JX sh step - tekton - write interceptor Configure Prometheus Push Gateway Make sure it is enabled in the prometheus helm chart 1 2 pushgateway : enabled : true Identification Data canonical FQN: application ID source URI Questions to answer Metrics to gather test coverage shared libraries used duration of stages duration of job status of job status of stage time-to-fix git source node label languages (github languages parser) Test Coverage Send a Gauge with coverage as value. Potential Labels: Application ID Source URI Job Instance RunId Send Metric From Jenkins Pipeline Bash Simple 1 echo some_metric 3.14 | curl --data-binary @- http://pushgateway.example.org:9091/metrics/job/some_job Complex 1 2 3 4 5 6 7 cat EOF | curl --data-binary @- http://pushgateway.example.org:9091/metrics/job/some_job/instance/some_instance # TYPE some_metric counter some_metric{label= val1 } 42 # TYPE another_metric gauge # HELP another_metric Just an example. another_metric 2398.283 EOF Delete by instance 1 curl -X DELETE http://pushgateway.example.org:9091/metrics/job/some_job/instance/some_instance Delete by (prometheus) job 1 curl -X DELETE http://pushgateway.example.org:9091/metrics/job/some_job Go lang client Queries Total Stages Duration In Seconds 1 sum ( jenkins_pipeline_run_stages_hist_sum ) by ( jobName , runId ) / 1000 Coverage Metric 1 jenkins_pipeline_run_test_coverage Last Push Of Metric 1 time () - push_time_seconds Stage Statusses 1 m ?? 1 2 ( sum ( jenkins_pipeline_run_hist_sum ) by ( jobName ) / 1000 ) / sum ( jenkins_pipeline_run_hist_count ) by ( jobName ) 1 sum ( jenkins_pipeline_run_hist_sum ) by ( jobName , runId ) 1 sum ( jenkins_pipeline_run_hist_count ) by ( instance , appId ) 1 sum ( jenkins_pipeline_run_stages_hist_sum ) by ( instance , jobName , runId ) 1 sum ( jenkins_pipeline_run_hist_count offset 3 d ) by ( jobName ) 1 sum ( jenkins_pipeline_run_hist_count ) by ( jobName ) Success Rate of Stages 1 count ( jenkins_pipeline_run_hist_sum { result = SUCCESS } ) by ( jobName , runId ) / count ( jenkins_pipeline_run_hist_sum ) by ( jobName , runId ) Things to look at Scraping of Gateway means metrics are retrieved more often than they are created you can reduce the error by creating a rewrite rule https://www.robustperception.io/aggregating-across-batch-job-runs-with-push_time_seconds Counter does not aggregate https://stackoverflow.com/questions/50923880/prometheus-intrumentation-for-distributed-accumulated-batch-jobs if you want aggregation, use Prometheus Aggregation Gateway from Weaveworks https://github.com/weaveworks/prom-aggregation-gateway Pipeline Example - Curl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 pipeline { agent { kubernetes { label jpb-mon yaml kind: Pod metadata: labels: build: prom-test spec: containers: - name: curl image: byrnedo/alpine-curl command: [ cat ] tty: true - name: golang image: golang:1.9 command: [ cat ] tty: true } } environment { CREDS = credentials ( api ) TEST_COVERAGE = PROM_URL = http://prometheus-pushgateway.obs:9091/metrics/job/devops25 } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/go-demo-5.git } } stage ( Prepare ) { steps { container ( golang ) { sh go get -d -v -t } } } stage ( Build Test ) { steps { container ( golang ) { script { sh go build def coverage = sh encoding: UTF-8 , label: go test , returnStdout: true , script: go test --cover -v ./... --run UnitTest | grep coverage: coverage = coverage . trim () coverage = coverage . replace ( coverage: , ) coverage = coverage . replace ( % of statements , ) TEST_COVERAGE = ${coverage} println coverage=${coverage} } sh ls -lath } } } stage ( Push Metrics ) { environment { COVERAGE = ${TEST_COVERAGE} } steps { println COVERAGE=${COVERAGE} container ( curl ) { sh echo TEST_COVERAGE=${COVERAGE} sh echo PROM_URL=${PROM_URL} sh echo BUILD_ID=${BUILD_ID} sh echo JOB_NAME=${JOB_NAME} sh echo jenkins_pipeline_run_test_coverage{instance=\\ $JENKINS_URL\\ ,jobName=\\ $JOB_NAME\\ , run=\\ $BUILD_ID\\ } ${TEST_COVERAGE} | curl --data-binary @- ${PROM_URL} } } } } } Pipeline Example - CLI The tool Jenkins Pipeline Binary - Monitoring will retrieve the Stages Nodes from Jenkins and translate them to Gauges in Prometheus. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 pipeline { agent { kubernetes { label jpb-mon yaml kind: Pod metadata: labels: build: prom-test-4 spec: containers: - name: jpb image: caladreas/jpb-mon:0.23.0 command: [ /bin/jpb-mon , sleep , --sleep , 3m ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m } } environment { CREDS = credentials ( api ) } stages { stage ( Test1 ) { steps { print Hello World } } stage ( Test2 ) { environment { MASTER = jenkins1 } steps { sh echo Hello World! } } } post { always { container ( jpb ) { sh /bin/jpb-mon get-run --verbose --host ${JENKINS_URL} --job ${JOB_NAME} --run ${BUILD_ID} --username ${CREDS_USR} --password ${CREDS_PSW} --push } } } } Resources https://stackoverflow.com/questions/37009906/access-stage-results-in-workflow-pipeline-plugin https://github.com/jenkinsci/blueocean-plugin/tree/master/blueocean-rest#get-pipeline-run-nodes https://github.com/jenkinsci/pipeline-model-definition-plugin/wiki/Getting-Started","title":"Pipeline"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#track-metrics-of-pipelines","text":"","title":"Track Metrics of Pipelines"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#get-data-from-jobs","text":"Use Prometheus Push Gateway ** via shared lib JX sh step - tekton - write interceptor","title":"Get Data From Jobs"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#configure-prometheus-push-gateway","text":"Make sure it is enabled in the prometheus helm chart 1 2 pushgateway : enabled : true","title":"Configure Prometheus Push Gateway"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#identification-data","text":"canonical FQN: application ID source URI","title":"Identification Data"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#questions-to-answer","text":"","title":"Questions to answer"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#metrics-to-gather","text":"test coverage shared libraries used duration of stages duration of job status of job status of stage time-to-fix git source node label languages (github languages parser)","title":"Metrics to gather"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#test-coverage","text":"Send a Gauge with coverage as value. Potential Labels: Application ID Source URI Job Instance RunId","title":"Test Coverage"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#send-metric-from-jenkins-pipeline","text":"","title":"Send Metric From Jenkins Pipeline"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#bash","text":"","title":"Bash"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#simple","text":"1 echo some_metric 3.14 | curl --data-binary @- http://pushgateway.example.org:9091/metrics/job/some_job","title":"Simple"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#complex","text":"1 2 3 4 5 6 7 cat EOF | curl --data-binary @- http://pushgateway.example.org:9091/metrics/job/some_job/instance/some_instance # TYPE some_metric counter some_metric{label= val1 } 42 # TYPE another_metric gauge # HELP another_metric Just an example. another_metric 2398.283 EOF","title":"Complex"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#delete-by-instance","text":"1 curl -X DELETE http://pushgateway.example.org:9091/metrics/job/some_job/instance/some_instance","title":"Delete by instance"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#delete-by-prometheus-job","text":"1 curl -X DELETE http://pushgateway.example.org:9091/metrics/job/some_job","title":"Delete by (prometheus) job"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#go-lang-client","text":"","title":"Go lang client"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#queries","text":"","title":"Queries"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#total-stages-duration-in-seconds","text":"1 sum ( jenkins_pipeline_run_stages_hist_sum ) by ( jobName , runId ) / 1000","title":"Total Stages Duration In Seconds"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#coverage-metric","text":"1 jenkins_pipeline_run_test_coverage","title":"Coverage Metric"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#last-push-of-metric","text":"1 time () - push_time_seconds","title":"Last Push Of Metric"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#stage-statusses","text":"1 m","title":"Stage Statusses"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#_1","text":"1 2 ( sum ( jenkins_pipeline_run_hist_sum ) by ( jobName ) / 1000 ) / sum ( jenkins_pipeline_run_hist_count ) by ( jobName ) 1 sum ( jenkins_pipeline_run_hist_sum ) by ( jobName , runId ) 1 sum ( jenkins_pipeline_run_hist_count ) by ( instance , appId ) 1 sum ( jenkins_pipeline_run_stages_hist_sum ) by ( instance , jobName , runId ) 1 sum ( jenkins_pipeline_run_hist_count offset 3 d ) by ( jobName ) 1 sum ( jenkins_pipeline_run_hist_count ) by ( jobName )","title":"??"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#success-rate-of-stages","text":"1 count ( jenkins_pipeline_run_hist_sum { result = SUCCESS } ) by ( jobName , runId ) / count ( jenkins_pipeline_run_hist_sum ) by ( jobName , runId )","title":"Success Rate of Stages"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#things-to-look-at","text":"Scraping of Gateway means metrics are retrieved more often than they are created you can reduce the error by creating a rewrite rule https://www.robustperception.io/aggregating-across-batch-job-runs-with-push_time_seconds Counter does not aggregate https://stackoverflow.com/questions/50923880/prometheus-intrumentation-for-distributed-accumulated-batch-jobs if you want aggregation, use Prometheus Aggregation Gateway from Weaveworks https://github.com/weaveworks/prom-aggregation-gateway","title":"Things to look at"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#pipeline-example-curl","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 pipeline { agent { kubernetes { label jpb-mon yaml kind: Pod metadata: labels: build: prom-test spec: containers: - name: curl image: byrnedo/alpine-curl command: [ cat ] tty: true - name: golang image: golang:1.9 command: [ cat ] tty: true } } environment { CREDS = credentials ( api ) TEST_COVERAGE = PROM_URL = http://prometheus-pushgateway.obs:9091/metrics/job/devops25 } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/go-demo-5.git } } stage ( Prepare ) { steps { container ( golang ) { sh go get -d -v -t } } } stage ( Build Test ) { steps { container ( golang ) { script { sh go build def coverage = sh encoding: UTF-8 , label: go test , returnStdout: true , script: go test --cover -v ./... --run UnitTest | grep coverage: coverage = coverage . trim () coverage = coverage . replace ( coverage: , ) coverage = coverage . replace ( % of statements , ) TEST_COVERAGE = ${coverage} println coverage=${coverage} } sh ls -lath } } } stage ( Push Metrics ) { environment { COVERAGE = ${TEST_COVERAGE} } steps { println COVERAGE=${COVERAGE} container ( curl ) { sh echo TEST_COVERAGE=${COVERAGE} sh echo PROM_URL=${PROM_URL} sh echo BUILD_ID=${BUILD_ID} sh echo JOB_NAME=${JOB_NAME} sh echo jenkins_pipeline_run_test_coverage{instance=\\ $JENKINS_URL\\ ,jobName=\\ $JOB_NAME\\ , run=\\ $BUILD_ID\\ } ${TEST_COVERAGE} | curl --data-binary @- ${PROM_URL} } } } } }","title":"Pipeline Example - Curl"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#pipeline-example-cli","text":"The tool Jenkins Pipeline Binary - Monitoring will retrieve the Stages Nodes from Jenkins and translate them to Gauges in Prometheus. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 pipeline { agent { kubernetes { label jpb-mon yaml kind: Pod metadata: labels: build: prom-test-4 spec: containers: - name: jpb image: caladreas/jpb-mon:0.23.0 command: [ /bin/jpb-mon , sleep , --sleep , 3m ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m } } environment { CREDS = credentials ( api ) } stages { stage ( Test1 ) { steps { print Hello World } } stage ( Test2 ) { environment { MASTER = jenkins1 } steps { sh echo Hello World! } } } post { always { container ( jpb ) { sh /bin/jpb-mon get-run --verbose --host ${JENKINS_URL} --job ${JOB_NAME} --run ${BUILD_ID} --username ${CREDS_USR} --password ${CREDS_PSW} --push } } } }","title":"Pipeline Example - CLI"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#resources","text":"https://stackoverflow.com/questions/37009906/access-stage-results-in-workflow-pipeline-plugin https://github.com/jenkinsci/blueocean-plugin/tree/master/blueocean-rest#get-pipeline-run-nodes https://github.com/jenkinsci/pipeline-model-definition-plugin/wiki/Getting-Started","title":"Resources"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/","text":"Prepare Environment This is a guide on monitoring Jenkins on Kubernetes, which makes it rather handy to have a Kubernetes cluster at hand. There are many ways to create a Kubernetes cluster, below is a guide on creating a cluster with Google Cloud's GKE. Elsewhere on this site, there are alternatives, such as Azure's AKS and AWS's EKS . Things To Do create a cluster install and configure Helm for easily installing other applications on the cluster install and configure Certmanager for managing TLS certificates with Let's Encrypt Create GKE Cluster Enough talk about what we should be doing, let's create the cluster! Prerequisites gcloud command-line utility Google Cloud account that is activated Variables Variables we need for the gcloud create cluster command. To make it easy to copy and paste the command. 1 2 3 4 K8S_VERSION = 1 .13.7-gke.8 REGION = europe-west4 CLUSTER_NAME = your cluster name PROJECT_ID = your google project id Query available versions If you want to see which versions are available in your Google Cloud Region, set the REGION variable and execute the command below. The list you get back will contain two lists, one for worker nodes and one for master nodes . Only the versions for master nodes can be used to create a cluster. 1 gcloud container get-server-config --region $REGION Create Cluster 1 2 3 4 5 6 7 8 9 10 gcloud container clusters create ${ CLUSTER_NAME } \\ --region ${ REGION } \\ --cluster-version ${ K8S_VERSION } \\ --num-nodes 2 --machine-type n1-standard-2 \\ --addons = HorizontalPodAutoscaling \\ --min-nodes 2 --max-nodes 3 \\ --enable-autoupgrade \\ --enable-autoscaling \\ --enable-network-policy \\ --labels = purpose = practice Set ClusterAdmin For some later commands, such as Helm, we need to be ClusterAdmin. 1 2 3 4 kubectl create clusterrolebinding \\ cluster-admin-binding \\ --clusterrole cluster-admin \\ --user $( gcloud config get-value account ) Install Ingress Controller An ingress controller is what allows you to access the applications you install on your Kubernetes cluster from the outside. We need to do this for the tools we will use. So we need to install an ingress controller. Any will do, but ingress - nginx (based on the widely use nginx application) is the most commonly used. 1 2 kubectl apply \\ -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/1cd17cd12c98563407ad03812aebac46ca4442f2/deploy/mandatory.yaml 1 2 kubectl apply \\ -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/1cd17cd12c98563407ad03812aebac46ca4442f2/deploy/provider/cloud-generic.yaml For exposing our applications to the outside, we need to have a valid DNS name. For that, we need to have the IP address of our LoadBalancer. The command below retrieves that address. If it is empty, wait a few minutes and try again. 1 2 3 4 5 export LB_IP = $( kubectl -n ingress-nginx \\ get svc ingress-nginx \\ -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) echo $LB_IP Tip If you don't get an address back, check to see if your ingress controller has a service and that the service has an EXTERNAL IP address. 1 kubectl get svc -n ingress-nginx -o wide The response should look something like this: 1 2 NAME TYPE CLUSTER - IP EXTERNAL - IP PORT ( S ) AGE ingress - nginx LoadBalancer 10 . 48 . 14 . 43 34 . 90 . 67 . 21 80 : 32762 / TCP , 443 : 31389 / TCP 21 d Install Helm Helm is a, or the , package manager for Kubernetes. We will use it to install the other applications. Read more here . 1 2 3 kubectl create \\ -f https://raw.githubusercontent.com/vfarcic/k8s-specs/master/helm/tiller-rbac.yml \\ --record --save-config Now that we've installed Helm, we can initialize the server component via a helm init . 1 helm init --service-account tiller And now we wait. 1 2 kubectl -n kube-system \\ rollout status deploy tiller-deploy Install Cert-Manager Cert-manager will help users automate installing TLS Certificates. Read more about cert-manager here . This creates the cert-manager specific resource definitions, also call CustomerResourceDefinitions or ***CRD***s. 1 kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8/deploy/manifests/00-crds.yaml Due to how cert-manager works, it is best installed into its own namespace. There's a chicken and egg problem because it needs a Root Certificate Authority (or, RootCA) to exist, but every Certificate needs to be validated against this Certificate. Which is why we add the special label. 1 2 kubectl create namespace cert-manager kubectl label namespace cert-manager certmanager.k8s.io/disable-validation = true Now that the CRD's and the namespace are ready, we can install cert-manager. Well, almost. The Helm Chart - that is how we call Helm packages - is in another Castle, eh, Helm Repository. So we first have to tell Helm where to get the Chart. 1 2 helm repo add jetstack https://charts.jetstack.io helm repo update We can now install cert-manager via Helm! 1 2 3 4 5 helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.8.0 \\ jetstack/cert-manager Configure ClusterIssuer Cert-manager can leverage Let's Encrypt to generate valid certificates. We need to instruct cert-manager which service to use, we do that by creating a ClusterIssuer resource. 1 kubectl apply -f cluster-issuer.yaml cluster-issuer.yaml Don't forget to replace replacewith your email address with an actual email address you can access. 1 2 3 4 5 6 7 8 9 10 11 apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-prod spec : acme : email : replacewith your email address server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-prod http01 : {}","title":"Prepare Environment"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#prepare-environment","text":"This is a guide on monitoring Jenkins on Kubernetes, which makes it rather handy to have a Kubernetes cluster at hand. There are many ways to create a Kubernetes cluster, below is a guide on creating a cluster with Google Cloud's GKE. Elsewhere on this site, there are alternatives, such as Azure's AKS and AWS's EKS .","title":"Prepare Environment"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#things-to-do","text":"create a cluster install and configure Helm for easily installing other applications on the cluster install and configure Certmanager for managing TLS certificates with Let's Encrypt","title":"Things To Do"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#create-gke-cluster","text":"Enough talk about what we should be doing, let's create the cluster!","title":"Create GKE Cluster"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#prerequisites","text":"gcloud command-line utility Google Cloud account that is activated","title":"Prerequisites"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#variables","text":"Variables we need for the gcloud create cluster command. To make it easy to copy and paste the command. 1 2 3 4 K8S_VERSION = 1 .13.7-gke.8 REGION = europe-west4 CLUSTER_NAME = your cluster name PROJECT_ID = your google project id","title":"Variables"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#query-available-versions","text":"If you want to see which versions are available in your Google Cloud Region, set the REGION variable and execute the command below. The list you get back will contain two lists, one for worker nodes and one for master nodes . Only the versions for master nodes can be used to create a cluster. 1 gcloud container get-server-config --region $REGION","title":"Query available versions"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#create-cluster","text":"1 2 3 4 5 6 7 8 9 10 gcloud container clusters create ${ CLUSTER_NAME } \\ --region ${ REGION } \\ --cluster-version ${ K8S_VERSION } \\ --num-nodes 2 --machine-type n1-standard-2 \\ --addons = HorizontalPodAutoscaling \\ --min-nodes 2 --max-nodes 3 \\ --enable-autoupgrade \\ --enable-autoscaling \\ --enable-network-policy \\ --labels = purpose = practice","title":"Create Cluster"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#set-clusteradmin","text":"For some later commands, such as Helm, we need to be ClusterAdmin. 1 2 3 4 kubectl create clusterrolebinding \\ cluster-admin-binding \\ --clusterrole cluster-admin \\ --user $( gcloud config get-value account )","title":"Set ClusterAdmin"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#install-ingress-controller","text":"An ingress controller is what allows you to access the applications you install on your Kubernetes cluster from the outside. We need to do this for the tools we will use. So we need to install an ingress controller. Any will do, but ingress - nginx (based on the widely use nginx application) is the most commonly used. 1 2 kubectl apply \\ -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/1cd17cd12c98563407ad03812aebac46ca4442f2/deploy/mandatory.yaml 1 2 kubectl apply \\ -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/1cd17cd12c98563407ad03812aebac46ca4442f2/deploy/provider/cloud-generic.yaml For exposing our applications to the outside, we need to have a valid DNS name. For that, we need to have the IP address of our LoadBalancer. The command below retrieves that address. If it is empty, wait a few minutes and try again. 1 2 3 4 5 export LB_IP = $( kubectl -n ingress-nginx \\ get svc ingress-nginx \\ -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) echo $LB_IP Tip If you don't get an address back, check to see if your ingress controller has a service and that the service has an EXTERNAL IP address. 1 kubectl get svc -n ingress-nginx -o wide The response should look something like this: 1 2 NAME TYPE CLUSTER - IP EXTERNAL - IP PORT ( S ) AGE ingress - nginx LoadBalancer 10 . 48 . 14 . 43 34 . 90 . 67 . 21 80 : 32762 / TCP , 443 : 31389 / TCP 21 d","title":"Install Ingress Controller"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#install-helm","text":"Helm is a, or the , package manager for Kubernetes. We will use it to install the other applications. Read more here . 1 2 3 kubectl create \\ -f https://raw.githubusercontent.com/vfarcic/k8s-specs/master/helm/tiller-rbac.yml \\ --record --save-config Now that we've installed Helm, we can initialize the server component via a helm init . 1 helm init --service-account tiller And now we wait. 1 2 kubectl -n kube-system \\ rollout status deploy tiller-deploy","title":"Install Helm"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#install-cert-manager","text":"Cert-manager will help users automate installing TLS Certificates. Read more about cert-manager here . This creates the cert-manager specific resource definitions, also call CustomerResourceDefinitions or ***CRD***s. 1 kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8/deploy/manifests/00-crds.yaml Due to how cert-manager works, it is best installed into its own namespace. There's a chicken and egg problem because it needs a Root Certificate Authority (or, RootCA) to exist, but every Certificate needs to be validated against this Certificate. Which is why we add the special label. 1 2 kubectl create namespace cert-manager kubectl label namespace cert-manager certmanager.k8s.io/disable-validation = true Now that the CRD's and the namespace are ready, we can install cert-manager. Well, almost. The Helm Chart - that is how we call Helm packages - is in another Castle, eh, Helm Repository. So we first have to tell Helm where to get the Chart. 1 2 helm repo add jetstack https://charts.jetstack.io helm repo update We can now install cert-manager via Helm! 1 2 3 4 5 helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.8.0 \\ jetstack/cert-manager","title":"Install Cert-Manager"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#configure-clusterissuer","text":"Cert-manager can leverage Let's Encrypt to generate valid certificates. We need to instruct cert-manager which service to use, we do that by creating a ClusterIssuer resource. 1 kubectl apply -f cluster-issuer.yaml cluster-issuer.yaml Don't forget to replace replacewith your email address with an actual email address you can access. 1 2 3 4 5 6 7 8 9 10 11 apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-prod spec : acme : email : replacewith your email address server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-prod http01 : {}","title":"Configure ClusterIssuer"},{"location":"certificates/lets-encrypt-k8s/","text":"Let's Encrypt for Kubernetes Let's Encrypt is a free, automated and open Certificate Authority. The kind of service you need if you want to have a secure website with https - yes I know that requires more than that - and it's now more straightforward to use than ever. This about using Let's Encrypt for generating a certificate for your service on Kubernetes. There are several ways to do this, with more or less automation, cluster-wide or namespace bound or with a DNS or HTTP validation check. I'll choose the route that was the easiest for me, and then I'll briefly look at the other options. Prerequisites There are some prerequisites required, that are best discussed on their own. So we will continue with the assumption that you have these in order. valid Class A or CNAME domain name kubernetes cluster with ingress controller (such as nginx) with helm and tiller installed in the cluster web application Steps The steps to take to get a web application to get a certificate from Let's Encrypt are the following. install cert-manager from the official helm chart deploy a Issuer resource deploy a certificate resource confirm certificate and secret are created/filled use in web app Install Cert Manager For more details on Cert Manager, I recommend reading their introduction . In essence, it's a tool that helps you initiate a certificate request with a service such as Let's Encrypt. You can install it via Helm, and it's meant to be installed only once per cluster. The once per cluster restriction is due to the usage of Custom Resource Definitions (CRD's) which will block any (re-)installation. To confirm if there are any CRD's from cert-manager, you can issue the following command. 1 kubectl get customresourcedefinitions.apiextensions.k8s.io The CRD's belonging to cert-manager are the following: certificates.certmanager.k8s.io clusterissuers.certmanager.k8s.io issuers.certmanager.k8s.io You will notice later, that we will use these CRD's for getting our certificate, so keep them in mind. To remove them in case of a re-install, you can issue the command below. 1 2 3 4 kubectl delete customresourcedefinitions.apiextensions.k8s.io \\ certificates.certmanager.k8s.io \\ clusterissuers.certmanager.k8s.io \\ issuers.certmanager.k8s.io When you're sure there are no CRD's left, you can install cert-manager via it's helm chart. It has some options in case you need them, you read about them here , but in my case that wasn't needed. 1 helm install --name cert-manager --namespace default stable/cert-manager Deploy Issuer To be able to use a certificate we need to have a Certificate Issuer. If you remember from our cert - manager , there are two CRD's that can take this role: ClusterIssuer : clusterissuers.certmanager.k8s.io Issuer : issuers.certmanager.k8s.io Both issuer type can use two ways of providing the proof of ownership, either by dns - 01 or http - 01 . We'll be using the http - 01 method, for the dns - 01 method, refer to the cert-manager documenation . ClusterIssuer As the resource Kind implies, a ClusterIssuer is a cluster-wide resource and not bound to a specific namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : # The ACME server URL server : https://acme-staging.api.letsencrypt.org/directory # Email address used for ACME registration email : user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} Issuer Not everyone wants a cluster-wide resource, and not everyone has the rights to install something elsewhere than their namespace. I prefer having as much as possible tied to a namespace - either a team or an application - I will use this type. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : myapp-letsencrypt-staging namespace : myapp spec : acme : # The ACME server URL server : https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email : myadmin@myapp.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : myapp-letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} There's a few things to note here: server : this refers to the server executing the ACME test, in this case: Let's Encrypt Staging (with the v2 API) email : this will be the account it will use for registering the certificate privateKeySecretRef : this is the Kubernetes secret resource in which the privateKey will be stored, just in case you need or want to remove it Deploy Certificate Resource Next up is our Certificate resource, this is where cert - manager will store our certificate details to be used by our application. In case you forgot, this is one of the three CRD's provided by cert-manager. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : myapp-example-com namespace : myapp spec : secretName : myapp-tls dnsNames : - myapp.example.com acme : config : - http01 : ingressClass : nginx domains : - myapp.example.com issuerRef : name : myapp-letsencrypt-staging kind : Issuer The things to note here: name : so far I've found it a naming convention to write the domain name where - replaces the . 's. secretName : the name of the Kubernetes secret that will house the certificate and certificate key dnsNames : you can specify more than one name, in our case just a single one, should match acme . config . domains acme.config : this defines the configuration for how the ownership proof should be done, this should match the method defined in the Issuer issuerRef : in good Kubernetes fashion, we reference the Issuer that should issue our certificate, the name and kind should match our Issue resource Confirm Resources We have defined our set of resources that should create our valid - though not trusted, as it is staging - certificate. Before we use it, we should confirm our secret and our certificate are both valid. 1 kubectl describe certificate myapp-example-com --namespace myapp The response includes the latest status, which looks like this: 1 2 3 4 5 6 7 8 9 10 Status : Acme : Order : URL : https://acme-staging-v02.api.letsencrypt.org/acme/order/705.../960... Conditions : Last Transition Time : 2018-10-02T21:17:34Z Message : Certificate issued successfully Reason : CertIssued Status : True Type : Ready Next up is our secret, containing the actual certificate and the certificate key. 1 kubectl describe secret myapp-tls --namespace myapp Which results in something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Name: myapp-tls Namespace: myapp Labels: certmanager.k8s.io/certificate-name = myapp-example-com Annotations: certmanager.k8s.io/alt-names: myapp.example.com certmanager.k8s.io/common-name: myapp.example.com certmanager.k8s.io/issuer-kind: Issuer certmanager.k8s.io/issuer-name: myapp-letsencrypt-staging Type: kubernetes.io/tls Data ==== tls.crt: 3797 bytes tls.key: 1679 bytes Use certificate to enable https Assuming the secret and the certificate are correct, we can use them to enable https on our web app. We put the tls certificate on the ingress of the application, in this case, Nginx, which assumes the following about the app: it has a deployment or stateful set it has a service which provides and endpoint to one or more instances it has an nginx ingress which points to the service Deployment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 kind : Deployment apiVersion : apps/v1 metadata : name : myapp namespace : myapp labels : app : myapp spec : replicas : 3 selector : matchLabels : app : myapp template : metadata : labels : app : myapp spec : containers : - name : myapp image : caladreas/catnip-master imagePullPolicy : Always ports : - containerPort : 8087 You should always include livenessprobe, resource limits and so on. But for the sake of brevity, these are omitted! Service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : v1 kind : Service metadata : name : myapp namespace : myapp labels : app : myapp spec : selector : app : myapp ports : - name : http port : 80 targetPort : 8087 protocol : TCP Ingress 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/ssl-redirect : true certmanager.k8s.io/issuer-kind : Issuer certmanager.k8s.io/issuer-name : myapp-letsencrypt-staging spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls Further resources How Does Let's Encrypt Work Tutorial that inspired this page Amazone EKS Ingress Guide Kuberetes EKS Ingress and TLS How To Configure LTS for Nginx Ingress","title":"Let's Encrypt K8S"},{"location":"certificates/lets-encrypt-k8s/#lets-encrypt-for-kubernetes","text":"Let's Encrypt is a free, automated and open Certificate Authority. The kind of service you need if you want to have a secure website with https - yes I know that requires more than that - and it's now more straightforward to use than ever. This about using Let's Encrypt for generating a certificate for your service on Kubernetes. There are several ways to do this, with more or less automation, cluster-wide or namespace bound or with a DNS or HTTP validation check. I'll choose the route that was the easiest for me, and then I'll briefly look at the other options.","title":"Let's Encrypt for Kubernetes"},{"location":"certificates/lets-encrypt-k8s/#prerequisites","text":"There are some prerequisites required, that are best discussed on their own. So we will continue with the assumption that you have these in order. valid Class A or CNAME domain name kubernetes cluster with ingress controller (such as nginx) with helm and tiller installed in the cluster web application","title":"Prerequisites"},{"location":"certificates/lets-encrypt-k8s/#steps","text":"The steps to take to get a web application to get a certificate from Let's Encrypt are the following. install cert-manager from the official helm chart deploy a Issuer resource deploy a certificate resource confirm certificate and secret are created/filled use in web app","title":"Steps"},{"location":"certificates/lets-encrypt-k8s/#install-cert-manager","text":"For more details on Cert Manager, I recommend reading their introduction . In essence, it's a tool that helps you initiate a certificate request with a service such as Let's Encrypt. You can install it via Helm, and it's meant to be installed only once per cluster. The once per cluster restriction is due to the usage of Custom Resource Definitions (CRD's) which will block any (re-)installation. To confirm if there are any CRD's from cert-manager, you can issue the following command. 1 kubectl get customresourcedefinitions.apiextensions.k8s.io The CRD's belonging to cert-manager are the following: certificates.certmanager.k8s.io clusterissuers.certmanager.k8s.io issuers.certmanager.k8s.io You will notice later, that we will use these CRD's for getting our certificate, so keep them in mind. To remove them in case of a re-install, you can issue the command below. 1 2 3 4 kubectl delete customresourcedefinitions.apiextensions.k8s.io \\ certificates.certmanager.k8s.io \\ clusterissuers.certmanager.k8s.io \\ issuers.certmanager.k8s.io When you're sure there are no CRD's left, you can install cert-manager via it's helm chart. It has some options in case you need them, you read about them here , but in my case that wasn't needed. 1 helm install --name cert-manager --namespace default stable/cert-manager","title":"Install Cert Manager"},{"location":"certificates/lets-encrypt-k8s/#deploy-issuer","text":"To be able to use a certificate we need to have a Certificate Issuer. If you remember from our cert - manager , there are two CRD's that can take this role: ClusterIssuer : clusterissuers.certmanager.k8s.io Issuer : issuers.certmanager.k8s.io Both issuer type can use two ways of providing the proof of ownership, either by dns - 01 or http - 01 . We'll be using the http - 01 method, for the dns - 01 method, refer to the cert-manager documenation .","title":"Deploy Issuer"},{"location":"certificates/lets-encrypt-k8s/#clusterissuer","text":"As the resource Kind implies, a ClusterIssuer is a cluster-wide resource and not bound to a specific namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : # The ACME server URL server : https://acme-staging.api.letsencrypt.org/directory # Email address used for ACME registration email : user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {}","title":"ClusterIssuer"},{"location":"certificates/lets-encrypt-k8s/#issuer","text":"Not everyone wants a cluster-wide resource, and not everyone has the rights to install something elsewhere than their namespace. I prefer having as much as possible tied to a namespace - either a team or an application - I will use this type. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : myapp-letsencrypt-staging namespace : myapp spec : acme : # The ACME server URL server : https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email : myadmin@myapp.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : myapp-letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} There's a few things to note here: server : this refers to the server executing the ACME test, in this case: Let's Encrypt Staging (with the v2 API) email : this will be the account it will use for registering the certificate privateKeySecretRef : this is the Kubernetes secret resource in which the privateKey will be stored, just in case you need or want to remove it","title":"Issuer"},{"location":"certificates/lets-encrypt-k8s/#deploy-certificate-resource","text":"Next up is our Certificate resource, this is where cert - manager will store our certificate details to be used by our application. In case you forgot, this is one of the three CRD's provided by cert-manager. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : myapp-example-com namespace : myapp spec : secretName : myapp-tls dnsNames : - myapp.example.com acme : config : - http01 : ingressClass : nginx domains : - myapp.example.com issuerRef : name : myapp-letsencrypt-staging kind : Issuer The things to note here: name : so far I've found it a naming convention to write the domain name where - replaces the . 's. secretName : the name of the Kubernetes secret that will house the certificate and certificate key dnsNames : you can specify more than one name, in our case just a single one, should match acme . config . domains acme.config : this defines the configuration for how the ownership proof should be done, this should match the method defined in the Issuer issuerRef : in good Kubernetes fashion, we reference the Issuer that should issue our certificate, the name and kind should match our Issue resource","title":"Deploy Certificate Resource"},{"location":"certificates/lets-encrypt-k8s/#confirm-resources","text":"We have defined our set of resources that should create our valid - though not trusted, as it is staging - certificate. Before we use it, we should confirm our secret and our certificate are both valid. 1 kubectl describe certificate myapp-example-com --namespace myapp The response includes the latest status, which looks like this: 1 2 3 4 5 6 7 8 9 10 Status : Acme : Order : URL : https://acme-staging-v02.api.letsencrypt.org/acme/order/705.../960... Conditions : Last Transition Time : 2018-10-02T21:17:34Z Message : Certificate issued successfully Reason : CertIssued Status : True Type : Ready Next up is our secret, containing the actual certificate and the certificate key. 1 kubectl describe secret myapp-tls --namespace myapp Which results in something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Name: myapp-tls Namespace: myapp Labels: certmanager.k8s.io/certificate-name = myapp-example-com Annotations: certmanager.k8s.io/alt-names: myapp.example.com certmanager.k8s.io/common-name: myapp.example.com certmanager.k8s.io/issuer-kind: Issuer certmanager.k8s.io/issuer-name: myapp-letsencrypt-staging Type: kubernetes.io/tls Data ==== tls.crt: 3797 bytes tls.key: 1679 bytes","title":"Confirm Resources"},{"location":"certificates/lets-encrypt-k8s/#use-certificate-to-enable-https","text":"Assuming the secret and the certificate are correct, we can use them to enable https on our web app. We put the tls certificate on the ingress of the application, in this case, Nginx, which assumes the following about the app: it has a deployment or stateful set it has a service which provides and endpoint to one or more instances it has an nginx ingress which points to the service","title":"Use certificate to enable https"},{"location":"certificates/lets-encrypt-k8s/#deployment","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 kind : Deployment apiVersion : apps/v1 metadata : name : myapp namespace : myapp labels : app : myapp spec : replicas : 3 selector : matchLabels : app : myapp template : metadata : labels : app : myapp spec : containers : - name : myapp image : caladreas/catnip-master imagePullPolicy : Always ports : - containerPort : 8087 You should always include livenessprobe, resource limits and so on. But for the sake of brevity, these are omitted!","title":"Deployment"},{"location":"certificates/lets-encrypt-k8s/#service","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : v1 kind : Service metadata : name : myapp namespace : myapp labels : app : myapp spec : selector : app : myapp ports : - name : http port : 80 targetPort : 8087 protocol : TCP","title":"Service"},{"location":"certificates/lets-encrypt-k8s/#ingress","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/ssl-redirect : true certmanager.k8s.io/issuer-kind : Issuer certmanager.k8s.io/issuer-name : myapp-letsencrypt-staging spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls","title":"Ingress"},{"location":"certificates/lets-encrypt-k8s/#further-resources","text":"How Does Let's Encrypt Work Tutorial that inspired this page Amazone EKS Ingress Guide Kuberetes EKS Ingress and TLS How To Configure LTS for Nginx Ingress","title":"Further resources"},{"location":"cloud/automation-pulumi/","text":"Pulumi Just pointing to the documentation and GitHub repository is a bit boring and most of all, lazy. So I've gone through the trouble to create a demo project that shows what Pulumi can do. One of the things to keep in mind is that Pulumi keeps its state in a remote location by default - Pulumi's SaaS service. Much like using an S3 bucket to store your Terraform state. The goal of the demo is to show what you can do with Pulumi. My personal opinion is that Pulumi is excellent for managing entire systems as code. From public cloud infrastructure to public open source resources right down to your applications and their configurations. The system that I've written with Pulumi is a CI/CD system running in a GKE cluster. The system contains Jenkins, Artifactory and a custom LDAP server. To make it interesting, we'll use different aspects of Pulumi to create the system. We will create the GKE cluster via Pulumi's wrapper for the gcloud cli - in a similar way as JenkinsX does -, install Jenkins and Artifactory via Helm and install the LDAP server via Pulumi's Kubernetes resource classes. Steps taken For more info Pulumi.io install: brew install pulumi clone demo: git clone https : // github . com / demomon / pulumi - demo - 1 init stack: pulumi stack init demomon - pulumi - demo - 1 connect to GitHub set kubernetes config pulumi config set kubernetes : context gke_ps - dev - 201405 _europe - west4_joostvdg - reg - dec18 - 1 pulumi config set isMinikube false install npm resources: npm install (I've used the TypeScript demo's) pulumi config set username administrator pulumi config set password 3 OvlgaockdnTsYRU5JAcgM1o --secret pulumi preview incase pulumi loses your stack: pulumi stack select demomon - pulumi - demo - 1 pulumi destroy Based on the following demo's: https://pulumi.io/quickstart/kubernetes/tutorial-exposed-deployment.html https://github.com/pulumi/examples/tree/master/kubernetes-ts-jenkins Artifactory via Helm To install the helm charts of Artifactory using Helm, we will first need to add JFrog's repository. helm repo add jfrog https : // charts . jfrog . io helm repo update GKE Cluster Below is the code for the cluster. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import * as gcp from @pulumi/gcp ; import * as k8s from @pulumi/kubernetes ; import * as pulumi from @pulumi/pulumi ; import { nodeCount , nodeMachineType , password , username } from ./gke-config ; export const k8sCluster = new gcp . container . Cluster ( gke-cluster , { name : joostvdg-dec-2018-pulumi , initialNodeCount : nodeCount , nodeVersion : latest , minMasterVersion : latest , nodeConfig : { machineType : nodeMachineType , oauthScopes : [ https://www.googleapis.com/auth/compute , https://www.googleapis.com/auth/devstorage.read_only , https://www.googleapis.com/auth/logging.write , https://www.googleapis.com/auth/monitoring ], }, }); GKE Config As you could see, we import variables from a configuration file gke - config . 1 2 3 4 5 6 7 8 import { Config } from @pulumi/pulumi ; const config = new Config (); export const nodeCount = config . getNumber ( nodeCount ) || 3 ; export const nodeMachineType = config . get ( nodeMachineType ) || n1-standard-2 ; // username is the admin username for the cluster. export const username = config . get ( username ) || admin ; // password is the password for the admin user in the cluster. export const password = config . require ( password ); Kubeconfig As you probably will want to install other things (Helm charts, services) inside this cluster, we need to make sure we get the kubeconfig file of our cluster - which is yet to be created. Below is the code - courtesy of Pulumi's GKE Example - that generates and exports the Kubernetes Client Configuration. This client configuration can then be used by Helm charts or other Kubernetes services. Pulumi will then also understand it depends on the cluster and create/update it first before moving on to the others. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // Manufacture a GKE-style Kubeconfig. Note that this is slightly different because of the way GKE requires // gcloud to be in the picture for cluster authentication (rather than using the client cert/key directly). export const k8sConfig = pulumi . all ([ k8sCluster . name , k8sCluster . endpoint , k8sCluster . masterAuth ]). apply (([ name , endpoint , auth ]) = { const context = ` ${ gcp . config . project } _ ${ gcp . config . zone } _ ${ name } ` ; return `apiVersion: v1 clusters: - cluster: certificate-authority-data: ${ auth . clusterCaCertificate } server: https:// ${ endpoint } name: ${ context } contexts: - context: cluster: ${ context } user: ${ context } name: ${ context } current-context: ${ context } kind: Config preferences: {} users: - name: ${ context } user: auth-provider: config: cmd-args: config config-helper --format=json cmd-path: gcloud expiry-key: {.credential.token_expiry} token-key: {.credential.access_token} name: gcp ` ; }); // Export a Kubernetes provider instance that uses our cluster from above. export const k8sProvider = new k8s . Provider ( gkeK8s , { kubeconfig : k8sConfig , }); Pulumi GCP Config https://github.com/pulumi/examples/blob/master/gcp-ts-gke/README.md 1 2 3 4 export GCP_PROJECT = ... export GCP_ZONE = europe-west4-a export CLUSTER_PASSWORD = ... export GCP_SA_NAME = ... Make sure you have a Google SA (Service Account) by that name first, as you can read here . For me it worked best to NOT set any environment variables mentioned. They invairably caused authentication or authorization issues. Just make sure the SA account and it's credential file (see below) are authorized and the gcloud cli works. 1 2 3 4 gcloud iam service-accounts keys create gcp-credentials.json \\ --iam-account ${ GCP_SA_NAME } @ ${ GCP_PROJECT } .iam.gserviceaccount.com gcloud auth activate-service-account --key-file gcp-credentials.json gcloud auth application-default login 1 2 3 pulumi config set gcp:project ${ GCP_PROJECT } pulumi config set gcp:zone ${ GCP_ZONE } pulumi config set password --secret ${ CLUSTER_PASSWORD } Post Cluster Creation 1 2 gcloud container clusters get-credentials joostvdg-dec-2018-pulumi kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $( gcloud config get-value account ) Install failed Failed to install kubernetes : rbac . authorization . k8s . io : Role artifactory - artifactory . Probably due to missing rights, so probably have to execute the admin binding before the helm charts. 1 error: Plan apply failed: roles.rbac.authorization.k8s.io artifactory-artifactory is forbidden: attempt to grant extra privileges: ... Helm Charts Using Pulumi to install a Helm Chart feels a bit like adding layers of wrapping upon wrapping. The power of Pulumi becomes visible when using more than one related service on the same cluster - for example a SDLC Tool Chain. This example application installs two helm charts, Jenkins and Artifactory, on a GKE cluster that is also created and managed by Pulumi. Below is an example of installing a Helm chart of Jenkins, where we provide the Kubernetes config from the GKE cluster as Provider. This way, Pulumi knows it must install the helm chart in that GKE cluster and not in the current Kubeconfig. 1 2 3 4 5 6 7 8 9 10 import { k8sProvider , k8sConfig } from ./gke-cluster ; const jenkins = new k8s . helm . v2 . Chart ( jenkins , { repo : stable , version : 0.25.1 , chart : jenkins , }, { providers : { kubernetes : k8sProvider } } ); Deployment Service First, make sure you have an interface for the configuration arguments. 1 2 3 4 5 export interface LdapArgs { readonly name : string , readonly imageName : string , readonly imageTag : string } Then, create a exportable Pulumi resource class that can be reused. 1 2 3 4 5 6 export class LdapInstallation extends pulumi . ComponentResource { public readonly deployment : k8s.apps.v1.Deployment ; public readonly service : k8s.core.v1.Service ; // constructor } Inside the constructor placehold we will create a constructor method. It will do all the configuration we need to do for this resource, in this case a Kubernetes Service and Deployment. 1 2 3 4 5 constructor ( args : LdapArgs ) { super ( k8stypes:service:LdapInstallation , args . name , {}); const labels = { app : args.name }; const name = args . name } First Kubernetes resource to create is a container specification for the Deployment. 1 2 3 4 5 6 7 8 9 10 11 12 const container : k8stypes.core.v1.Container = { name , image : args.imageName + : + args . imageTag , resources : { requests : { cpu : 100m , memory : 200Mi }, limits : { cpu : 100m , memory : 200Mi }, }, ports : [{ name : ldap , containerPort : 1389 , }, ] }; As the configuration arguments can be any TypeScript type, you can allow people to override entire segments (such as Resources). Which you would do as follows: 1 2 3 4 resources : args.resources || { requests : { cpu : 100m , memory : 200Mi }, limits : { cpu : 100m , memory : 200Mi }, }, The Deployment and Service construction are quite similar. 1 2 3 4 5 6 7 8 9 10 this . deployment = new k8s . apps . v1 . Deployment ( args . name , { spec : { selector : { matchLabels : labels }, replicas : 1 , template : { metadata : { labels : labels }, spec : { containers : [ container ] }, }, }, },{ provider : cluster.k8sProvider }); 1 2 3 4 5 6 7 8 9 10 11 12 13 this . service = new k8s . core . v1 . Service ( args . name , { metadata : { labels : this.deployment.metadata.apply ( meta = meta . labels ), }, spec : { ports : [{ name : ldap , port : 389 , targetPort : ldap , protocol : TCP }, ], selector : this.deployment.spec.apply ( spec = spec . template . metadata . labels ), type : ClusterIP , }, }, { provider : cluster.k8sProvider });","title":"Automation - Pulumi"},{"location":"cloud/automation-pulumi/#pulumi","text":"Just pointing to the documentation and GitHub repository is a bit boring and most of all, lazy. So I've gone through the trouble to create a demo project that shows what Pulumi can do. One of the things to keep in mind is that Pulumi keeps its state in a remote location by default - Pulumi's SaaS service. Much like using an S3 bucket to store your Terraform state. The goal of the demo is to show what you can do with Pulumi. My personal opinion is that Pulumi is excellent for managing entire systems as code. From public cloud infrastructure to public open source resources right down to your applications and their configurations. The system that I've written with Pulumi is a CI/CD system running in a GKE cluster. The system contains Jenkins, Artifactory and a custom LDAP server. To make it interesting, we'll use different aspects of Pulumi to create the system. We will create the GKE cluster via Pulumi's wrapper for the gcloud cli - in a similar way as JenkinsX does -, install Jenkins and Artifactory via Helm and install the LDAP server via Pulumi's Kubernetes resource classes.","title":"Pulumi"},{"location":"cloud/automation-pulumi/#steps-taken","text":"For more info Pulumi.io install: brew install pulumi clone demo: git clone https : // github . com / demomon / pulumi - demo - 1 init stack: pulumi stack init demomon - pulumi - demo - 1 connect to GitHub set kubernetes config pulumi config set kubernetes : context gke_ps - dev - 201405 _europe - west4_joostvdg - reg - dec18 - 1 pulumi config set isMinikube false install npm resources: npm install (I've used the TypeScript demo's) pulumi config set username administrator pulumi config set password 3 OvlgaockdnTsYRU5JAcgM1o --secret pulumi preview incase pulumi loses your stack: pulumi stack select demomon - pulumi - demo - 1 pulumi destroy Based on the following demo's: https://pulumi.io/quickstart/kubernetes/tutorial-exposed-deployment.html https://github.com/pulumi/examples/tree/master/kubernetes-ts-jenkins","title":"Steps taken"},{"location":"cloud/automation-pulumi/#artifactory-via-helm","text":"To install the helm charts of Artifactory using Helm, we will first need to add JFrog's repository. helm repo add jfrog https : // charts . jfrog . io helm repo update","title":"Artifactory via Helm"},{"location":"cloud/automation-pulumi/#gke-cluster","text":"Below is the code for the cluster. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import * as gcp from @pulumi/gcp ; import * as k8s from @pulumi/kubernetes ; import * as pulumi from @pulumi/pulumi ; import { nodeCount , nodeMachineType , password , username } from ./gke-config ; export const k8sCluster = new gcp . container . Cluster ( gke-cluster , { name : joostvdg-dec-2018-pulumi , initialNodeCount : nodeCount , nodeVersion : latest , minMasterVersion : latest , nodeConfig : { machineType : nodeMachineType , oauthScopes : [ https://www.googleapis.com/auth/compute , https://www.googleapis.com/auth/devstorage.read_only , https://www.googleapis.com/auth/logging.write , https://www.googleapis.com/auth/monitoring ], }, });","title":"GKE Cluster"},{"location":"cloud/automation-pulumi/#gke-config","text":"As you could see, we import variables from a configuration file gke - config . 1 2 3 4 5 6 7 8 import { Config } from @pulumi/pulumi ; const config = new Config (); export const nodeCount = config . getNumber ( nodeCount ) || 3 ; export const nodeMachineType = config . get ( nodeMachineType ) || n1-standard-2 ; // username is the admin username for the cluster. export const username = config . get ( username ) || admin ; // password is the password for the admin user in the cluster. export const password = config . require ( password );","title":"GKE Config"},{"location":"cloud/automation-pulumi/#kubeconfig","text":"As you probably will want to install other things (Helm charts, services) inside this cluster, we need to make sure we get the kubeconfig file of our cluster - which is yet to be created. Below is the code - courtesy of Pulumi's GKE Example - that generates and exports the Kubernetes Client Configuration. This client configuration can then be used by Helm charts or other Kubernetes services. Pulumi will then also understand it depends on the cluster and create/update it first before moving on to the others. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // Manufacture a GKE-style Kubeconfig. Note that this is slightly different because of the way GKE requires // gcloud to be in the picture for cluster authentication (rather than using the client cert/key directly). export const k8sConfig = pulumi . all ([ k8sCluster . name , k8sCluster . endpoint , k8sCluster . masterAuth ]). apply (([ name , endpoint , auth ]) = { const context = ` ${ gcp . config . project } _ ${ gcp . config . zone } _ ${ name } ` ; return `apiVersion: v1 clusters: - cluster: certificate-authority-data: ${ auth . clusterCaCertificate } server: https:// ${ endpoint } name: ${ context } contexts: - context: cluster: ${ context } user: ${ context } name: ${ context } current-context: ${ context } kind: Config preferences: {} users: - name: ${ context } user: auth-provider: config: cmd-args: config config-helper --format=json cmd-path: gcloud expiry-key: {.credential.token_expiry} token-key: {.credential.access_token} name: gcp ` ; }); // Export a Kubernetes provider instance that uses our cluster from above. export const k8sProvider = new k8s . Provider ( gkeK8s , { kubeconfig : k8sConfig , });","title":"Kubeconfig"},{"location":"cloud/automation-pulumi/#pulumi-gcp-config","text":"https://github.com/pulumi/examples/blob/master/gcp-ts-gke/README.md 1 2 3 4 export GCP_PROJECT = ... export GCP_ZONE = europe-west4-a export CLUSTER_PASSWORD = ... export GCP_SA_NAME = ... Make sure you have a Google SA (Service Account) by that name first, as you can read here . For me it worked best to NOT set any environment variables mentioned. They invairably caused authentication or authorization issues. Just make sure the SA account and it's credential file (see below) are authorized and the gcloud cli works. 1 2 3 4 gcloud iam service-accounts keys create gcp-credentials.json \\ --iam-account ${ GCP_SA_NAME } @ ${ GCP_PROJECT } .iam.gserviceaccount.com gcloud auth activate-service-account --key-file gcp-credentials.json gcloud auth application-default login 1 2 3 pulumi config set gcp:project ${ GCP_PROJECT } pulumi config set gcp:zone ${ GCP_ZONE } pulumi config set password --secret ${ CLUSTER_PASSWORD }","title":"Pulumi GCP Config"},{"location":"cloud/automation-pulumi/#post-cluster-creation","text":"1 2 gcloud container clusters get-credentials joostvdg-dec-2018-pulumi kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $( gcloud config get-value account )","title":"Post Cluster Creation"},{"location":"cloud/automation-pulumi/#install-failed","text":"Failed to install kubernetes : rbac . authorization . k8s . io : Role artifactory - artifactory . Probably due to missing rights, so probably have to execute the admin binding before the helm charts. 1 error: Plan apply failed: roles.rbac.authorization.k8s.io artifactory-artifactory is forbidden: attempt to grant extra privileges: ...","title":"Install failed"},{"location":"cloud/automation-pulumi/#helm-charts","text":"Using Pulumi to install a Helm Chart feels a bit like adding layers of wrapping upon wrapping. The power of Pulumi becomes visible when using more than one related service on the same cluster - for example a SDLC Tool Chain. This example application installs two helm charts, Jenkins and Artifactory, on a GKE cluster that is also created and managed by Pulumi. Below is an example of installing a Helm chart of Jenkins, where we provide the Kubernetes config from the GKE cluster as Provider. This way, Pulumi knows it must install the helm chart in that GKE cluster and not in the current Kubeconfig. 1 2 3 4 5 6 7 8 9 10 import { k8sProvider , k8sConfig } from ./gke-cluster ; const jenkins = new k8s . helm . v2 . Chart ( jenkins , { repo : stable , version : 0.25.1 , chart : jenkins , }, { providers : { kubernetes : k8sProvider } } );","title":"Helm Charts"},{"location":"cloud/automation-pulumi/#deployment-service","text":"First, make sure you have an interface for the configuration arguments. 1 2 3 4 5 export interface LdapArgs { readonly name : string , readonly imageName : string , readonly imageTag : string } Then, create a exportable Pulumi resource class that can be reused. 1 2 3 4 5 6 export class LdapInstallation extends pulumi . ComponentResource { public readonly deployment : k8s.apps.v1.Deployment ; public readonly service : k8s.core.v1.Service ; // constructor } Inside the constructor placehold we will create a constructor method. It will do all the configuration we need to do for this resource, in this case a Kubernetes Service and Deployment. 1 2 3 4 5 constructor ( args : LdapArgs ) { super ( k8stypes:service:LdapInstallation , args . name , {}); const labels = { app : args.name }; const name = args . name } First Kubernetes resource to create is a container specification for the Deployment. 1 2 3 4 5 6 7 8 9 10 11 12 const container : k8stypes.core.v1.Container = { name , image : args.imageName + : + args . imageTag , resources : { requests : { cpu : 100m , memory : 200Mi }, limits : { cpu : 100m , memory : 200Mi }, }, ports : [{ name : ldap , containerPort : 1389 , }, ] }; As the configuration arguments can be any TypeScript type, you can allow people to override entire segments (such as Resources). Which you would do as follows: 1 2 3 4 resources : args.resources || { requests : { cpu : 100m , memory : 200Mi }, limits : { cpu : 100m , memory : 200Mi }, }, The Deployment and Service construction are quite similar. 1 2 3 4 5 6 7 8 9 10 this . deployment = new k8s . apps . v1 . Deployment ( args . name , { spec : { selector : { matchLabels : labels }, replicas : 1 , template : { metadata : { labels : labels }, spec : { containers : [ container ] }, }, }, },{ provider : cluster.k8sProvider }); 1 2 3 4 5 6 7 8 9 10 11 12 13 this . service = new k8s . core . v1 . Service ( args . name , { metadata : { labels : this.deployment.metadata.apply ( meta = meta . labels ), }, spec : { ports : [{ name : ldap , port : 389 , targetPort : ldap , protocol : TCP }, ], selector : this.deployment.spec.apply ( spec = spec . template . metadata . labels ), type : ClusterIP , }, }, { provider : cluster.k8sProvider });","title":"Deployment &amp; Service"},{"location":"cloudbees/cbc-eks/","text":"CloudBees Core on AWS EKS https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/eks-install/# https://go.cloudbees.com/docs/cloudbees-core/cloud-reference-architecture/ra-for-eks/#_ingress_tls_termination Create EKS Cluster See my guide on creating a EKS cluster with EKSCTL Certmanager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 echo apiVersion: certmanager.k8s.io/v1alpha1 kind: Certificate metadata: name: cloudbeescore-kearos-net namespace: cje spec: secretName: cjoc-tls-prd dnsNames: - cloudbees-core.kearos.net acme: config: - http01: ingressClass: nginx domains: - cloudbees-core.kearos.net issuerRef: name: letsencrypt-prd kind: ClusterIssuer cjoc-cert.yml eks apply -f cjoc-cert.yml Create Namespace CJE 1 2 3 4 5 6 7 8 9 echo apiVersion: v1 kind: Namespace metadata: labels: name: cje name: cje cje-namespace.yaml eks create -f cje-namespace.yaml eks config set-context $( eks config current-context ) --namespace = cje CB Core Install Download from downloads.cloudbees.com Configure DNS 1 2 DOMAIN_NAME = cloudbees-core.kearos.net sed -e s,cje.example.com, $DOMAIN_NAME ,g cloudbees-core.yml tmp mv tmp cloudbees-core.yml Configure k8s yaml file: add certmanager . k8s . io / cluster - issuer : letsencrypt - prd to cjoc ingress's metadata . annotations add secretName : cjoc - tls - prd to cjoc ingress' spec . tls . host [ 0 ] confirm cjoc ingress's host and tls host is cloudbees - core . kearos . net Install 1 2 eks apply -f cloudbees-core.yml eks rollout status sts cjoc Retrieve initial password 1 eks exec cjoc-0 -- cat /var/jenkins_home/secrets/initialAdminPassword Jenkins CLI 1 2 export CJOC_URL = https://cloudbees-core.kearos.net/cjoc/ http --download ${ CJOC_URL } /jnlpJars/jenkins-cli.jar --verify false 1 2 export USR = jvandergriendt export TKN = 11b1016f80ddbb8a35bcbb5389599f7881 1 2 alias cbc = java -jar jenkins-cli.jar -noKeyAuth -auth ${ USR } : ${ TKN } -s ${ CJOC_URL } cbc teams Create team CAT 1 cbc teams cat --put team-cat.json Use EFS https://go.cloudbees.com/docs/cloudbees-core/cloud-reference-architecture/kubernetes-efs/ Create EFS in AWS performance: general purpose throughput: provisioned, 160mb/s encrypted: yes 1 EFS_KEY_ARN = arn:aws:kms:eu-west-1:324005994172:key/4bfd8d70-c7de-4e7a-ab83-10792be5daaa Destroy cluster External Client - The Hard Way create a new master (hat) confirm remoting works on expected port 50000+n, where n is incremental count of number of masters for example, if hat is the first new \"team\", it will be 50001 create a new node external-agent launch via java webstart download client jar confirm port is NOT accessable open port on LB confirm port is open Open Port on LB 1 2 3 4 5 export DOMAIN_NAME = cloudbees-core.example.com export TEAM_NAME = hat export MASTER_NAME = teams- ${ TEAM_NAME } export USR = export PSS = Test Port 1 Get Remoting Port 1 http --print = hH --auth $USR : $PSS https:// $DOMAIN_NAME / $MASTER_NAME / | grep X-Jenkins-CLI-Port Configure Config Map If you already configured tcp-services before, you will need to retrieve the current configmap using kubectl get configmap tcp-services -n ingress-nginx -o yaml tcp-services.yaml and edit it accordingly 1 kubectl get configmap tcp-services -n ingress-nginx -o yaml tcp-services.yaml Else: 1 export JNLP_MASTER_PORT = 50001 1 2 3 4 5 6 7 apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : ingress-nginx data : $JNLP_MASTER_PORT : \\ $NAMESPACE/$MASTER_NAME:$JNLP_MASTER_PORT:PROXY\\ For example: 1 2 3 4 5 6 7 apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : ingress-nginx data : 50001 : default/teams-hat:50001:PROXY Create Patch for Deployment (ingress) 1 2 3 4 5 6 7 8 9 spec : template : spec : containers : - name : nginx-ingress-controller ports : - containerPort : $JNLP_MASTER_PORT name : $JNLP_MASTER_PORT-tcp protocol : TCP Example: 1 2 3 4 5 6 7 8 9 spec : template : spec : containers : - name : nginx-ingress-controller ports : - containerPort : 50001 name : 50001-tcp protocol : TCP Create Patch for Service (ingress) 1 2 3 4 5 6 spec : ports : - name : $JNLP_MASTER_PORT-tcp port : $JNLP_MASTER_PORT protocol : TCP targetPort : $JNLP_MASTER_PORT-tcp Example: 1 2 3 4 5 6 spec : ports : - name : 50001-tcp port : 50001 protocol : TCP targetPort : 50001-tcp Apply patches 1 2 3 4 5 export NGINX_POD = $( kubectl get deployment -l app.kubernetes.io/name = ingress-nginx -n ingress-nginx -o jsonpath = {.items[0].metadata.name} ) kubectl apply -f tcp-services.yaml kubectl patch deployment ${ NGINX_POD } -n ingress-nginx -p $( cat deployment-patch.yaml ) kubectl patch service ingress-nginx -n ingress-nginx -p $( cat service-patch.yaml ) kubectl annotate -n ingress-nginx service/ingress-nginx service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout = 3600 --overwrite Delete cluster 1 aws cloudformation delete-stack --stack-name ${ STACK_NAME } --region ${ REGION } --profile ${ PROFILE }","title":"CloudBees Core on EKS"},{"location":"cloudbees/cbc-eks/#cloudbees-core-on-aws-eks","text":"https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/eks-install/# https://go.cloudbees.com/docs/cloudbees-core/cloud-reference-architecture/ra-for-eks/#_ingress_tls_termination","title":"CloudBees Core on AWS EKS"},{"location":"cloudbees/cbc-eks/#create-eks-cluster","text":"See my guide on creating a EKS cluster with EKSCTL","title":"Create EKS Cluster"},{"location":"cloudbees/cbc-eks/#certmanager","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 echo apiVersion: certmanager.k8s.io/v1alpha1 kind: Certificate metadata: name: cloudbeescore-kearos-net namespace: cje spec: secretName: cjoc-tls-prd dnsNames: - cloudbees-core.kearos.net acme: config: - http01: ingressClass: nginx domains: - cloudbees-core.kearos.net issuerRef: name: letsencrypt-prd kind: ClusterIssuer cjoc-cert.yml eks apply -f cjoc-cert.yml","title":"Certmanager"},{"location":"cloudbees/cbc-eks/#create-namespace-cje","text":"1 2 3 4 5 6 7 8 9 echo apiVersion: v1 kind: Namespace metadata: labels: name: cje name: cje cje-namespace.yaml eks create -f cje-namespace.yaml eks config set-context $( eks config current-context ) --namespace = cje","title":"Create Namespace CJE"},{"location":"cloudbees/cbc-eks/#cb-core-install","text":"Download from downloads.cloudbees.com","title":"CB Core Install"},{"location":"cloudbees/cbc-eks/#configure-dns","text":"1 2 DOMAIN_NAME = cloudbees-core.kearos.net sed -e s,cje.example.com, $DOMAIN_NAME ,g cloudbees-core.yml tmp mv tmp cloudbees-core.yml Configure k8s yaml file: add certmanager . k8s . io / cluster - issuer : letsencrypt - prd to cjoc ingress's metadata . annotations add secretName : cjoc - tls - prd to cjoc ingress' spec . tls . host [ 0 ] confirm cjoc ingress's host and tls host is cloudbees - core . kearos . net","title":"Configure DNS"},{"location":"cloudbees/cbc-eks/#install","text":"1 2 eks apply -f cloudbees-core.yml eks rollout status sts cjoc","title":"Install"},{"location":"cloudbees/cbc-eks/#retrieve-initial-password","text":"1 eks exec cjoc-0 -- cat /var/jenkins_home/secrets/initialAdminPassword","title":"Retrieve initial password"},{"location":"cloudbees/cbc-eks/#jenkins-cli","text":"1 2 export CJOC_URL = https://cloudbees-core.kearos.net/cjoc/ http --download ${ CJOC_URL } /jnlpJars/jenkins-cli.jar --verify false 1 2 export USR = jvandergriendt export TKN = 11b1016f80ddbb8a35bcbb5389599f7881 1 2 alias cbc = java -jar jenkins-cli.jar -noKeyAuth -auth ${ USR } : ${ TKN } -s ${ CJOC_URL } cbc teams","title":"Jenkins CLI"},{"location":"cloudbees/cbc-eks/#create-team-cat","text":"1 cbc teams cat --put team-cat.json","title":"Create team CAT"},{"location":"cloudbees/cbc-eks/#use-efs","text":"https://go.cloudbees.com/docs/cloudbees-core/cloud-reference-architecture/kubernetes-efs/ Create EFS in AWS performance: general purpose throughput: provisioned, 160mb/s encrypted: yes 1 EFS_KEY_ARN = arn:aws:kms:eu-west-1:324005994172:key/4bfd8d70-c7de-4e7a-ab83-10792be5daaa","title":"Use EFS"},{"location":"cloudbees/cbc-eks/#destroy-cluster","text":"","title":"Destroy cluster"},{"location":"cloudbees/cbc-eks/#external-client-the-hard-way","text":"create a new master (hat) confirm remoting works on expected port 50000+n, where n is incremental count of number of masters for example, if hat is the first new \"team\", it will be 50001 create a new node external-agent launch via java webstart download client jar confirm port is NOT accessable open port on LB confirm port is open","title":"External Client - The Hard Way"},{"location":"cloudbees/cbc-eks/#open-port-on-lb","text":"1 2 3 4 5 export DOMAIN_NAME = cloudbees-core.example.com export TEAM_NAME = hat export MASTER_NAME = teams- ${ TEAM_NAME } export USR = export PSS =","title":"Open Port on LB"},{"location":"cloudbees/cbc-eks/#test-port","text":"1","title":"Test Port"},{"location":"cloudbees/cbc-eks/#get-remoting-port","text":"1 http --print = hH --auth $USR : $PSS https:// $DOMAIN_NAME / $MASTER_NAME / | grep X-Jenkins-CLI-Port","title":"Get Remoting Port"},{"location":"cloudbees/cbc-eks/#configure-config-map","text":"If you already configured tcp-services before, you will need to retrieve the current configmap using kubectl get configmap tcp-services -n ingress-nginx -o yaml tcp-services.yaml and edit it accordingly 1 kubectl get configmap tcp-services -n ingress-nginx -o yaml tcp-services.yaml Else: 1 export JNLP_MASTER_PORT = 50001 1 2 3 4 5 6 7 apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : ingress-nginx data : $JNLP_MASTER_PORT : \\ $NAMESPACE/$MASTER_NAME:$JNLP_MASTER_PORT:PROXY\\ For example: 1 2 3 4 5 6 7 apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : ingress-nginx data : 50001 : default/teams-hat:50001:PROXY","title":"Configure Config Map"},{"location":"cloudbees/cbc-eks/#create-patch-for-deployment-ingress","text":"1 2 3 4 5 6 7 8 9 spec : template : spec : containers : - name : nginx-ingress-controller ports : - containerPort : $JNLP_MASTER_PORT name : $JNLP_MASTER_PORT-tcp protocol : TCP Example: 1 2 3 4 5 6 7 8 9 spec : template : spec : containers : - name : nginx-ingress-controller ports : - containerPort : 50001 name : 50001-tcp protocol : TCP","title":"Create Patch for Deployment (ingress)"},{"location":"cloudbees/cbc-eks/#create-patch-for-service-ingress","text":"1 2 3 4 5 6 spec : ports : - name : $JNLP_MASTER_PORT-tcp port : $JNLP_MASTER_PORT protocol : TCP targetPort : $JNLP_MASTER_PORT-tcp Example: 1 2 3 4 5 6 spec : ports : - name : 50001-tcp port : 50001 protocol : TCP targetPort : 50001-tcp","title":"Create Patch for Service (ingress)"},{"location":"cloudbees/cbc-eks/#apply-patches","text":"1 2 3 4 5 export NGINX_POD = $( kubectl get deployment -l app.kubernetes.io/name = ingress-nginx -n ingress-nginx -o jsonpath = {.items[0].metadata.name} ) kubectl apply -f tcp-services.yaml kubectl patch deployment ${ NGINX_POD } -n ingress-nginx -p $( cat deployment-patch.yaml ) kubectl patch service ingress-nginx -n ingress-nginx -p $( cat service-patch.yaml ) kubectl annotate -n ingress-nginx service/ingress-nginx service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout = 3600 --overwrite","title":"Apply patches"},{"location":"cloudbees/cbc-eks/#delete-cluster","text":"1 aws cloudformation delete-stack --stack-name ${ STACK_NAME } --region ${ REGION } --profile ${ PROFILE }","title":"Delete cluster"},{"location":"cloudbees/cbc-gke-helm/","text":"Install CloudBees Core On GKE Prerequisite Have a GKE cluster in which you're ClusterAdmin . Don't have one yet? Read here how to create one! Prepare 1 kubectl create namespace cloudbees-core 1 2 helm repo add cloudbees https://charts.cloudbees.com/public/cloudbees helm repo update 1 kubens cloudbees-core Info kubense is a subcommand of the kubecontext tool. Install ClusterIssuer/Cert This assumes you have Cert-Manager installed. 1 2 kubectl apply -f clusterissuer.yaml kubectl apply -f certificate.yaml clusterissuer.yaml Make sure to replace REPLACE_WITH_YOUR_EMAIL_ADDRESS with your own email address. Let's Encrypt will use this to register the certificate and will notify you there when it expires. 1 2 3 4 5 6 7 8 9 10 11 apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-prod spec : acme : email : REPLACE_WITH_YOUR_EMAIL_ADDRESS server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-prod http01 : {} certificate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : MyHostName namespace : cloudbees-core spec : secretName : tls-cloudbees-core-kearos-net dnsNames : - cloudbees-core.kearos.net acme : config : - http01 : ingressClass : nginx domains : - cloudbees-core.kearos.net issuerRef : name : letsencrypt-prod kind : ClusterIssuer Install with values.yaml 1 2 3 4 helm install --name cloudbees-core \\ -f cloudbees-core-values.yaml \\ --namespace = cloudbees-core \\ cloudbees/cloudbees-core 1 kubectl rollout status statefulset cjoc 1 kubectl get po cjoc-0 1 kubectl logs -f cjoc-0 1 stern cjoc Get Initial Password 1 kubectl -n cloudbees-core exec cjoc-0 cat /var/jenkins_home/secrets/initialAdminPassword values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # A helm example values file for standard kubernetes install. # An nginx-ingress controller is not installed and ssl isn t installed. # Install an nginx-ingress controller nginx-ingress : Enabled : false OperationsCenter : # Set the HostName for the Operation Center HostName : cloudbees-core.kearos.net # Setting ServiceType to ClusterIP creates ingress ServiceType : ClusterIP CSRF : ProxyCompatibility : true Ingress : Annotations : certmanager.k8s.io/cluster-issuer : letsencrypt-prod kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : false nginx.ingress.kubernetes.io/app-root : https://$best_http_host/cjoc/teams-check/ nginx.ingress.kubernetes.io/proxy-body-size : 50m nginx.ingress.kubernetes.io/proxy-request-buffering : off nginx.ingress.kubernetes.io/ssl-redirect : true tls : ## Set this to true in order to enable TLS on the ingress record Enable : true # Create a certificate kubernetes and use it here. SecretName : tls-cloudbees-core-kearos-net Host : cloudbees-core.kearos.net Core Post Install Setup API Token Go to http : // MyHostName / cjoc , login with your admin user. Click on the user's name (top right corner) - Configure - Generate Token . Warning You will see this token only once, so copy it and store it somewhere. Get CLI 1 2 export CJOC_URL = https:// MyHostName /cjoc/ http --download ${ CJOC_URL } /jnlpJars/jenkins-cli.jar --verify false Alias CLI 1 2 USR = admin TKN = 1 alias cboc = java -jar jenkins-cli.jar -noKeyAuth -auth ${ USR } : ${ TKN } -s ${ CJOC_URL } 1 cboc version For More CLI Go to http : // MyHostName / cjoc / cli","title":"CloudBees Core on GKE"},{"location":"cloudbees/cbc-gke-helm/#install-cloudbees-core-on-gke","text":"","title":"Install CloudBees Core On GKE"},{"location":"cloudbees/cbc-gke-helm/#prerequisite","text":"Have a GKE cluster in which you're ClusterAdmin . Don't have one yet? Read here how to create one!","title":"Prerequisite"},{"location":"cloudbees/cbc-gke-helm/#prepare","text":"1 kubectl create namespace cloudbees-core 1 2 helm repo add cloudbees https://charts.cloudbees.com/public/cloudbees helm repo update 1 kubens cloudbees-core Info kubense is a subcommand of the kubecontext tool.","title":"Prepare"},{"location":"cloudbees/cbc-gke-helm/#install-clusterissuercert","text":"This assumes you have Cert-Manager installed. 1 2 kubectl apply -f clusterissuer.yaml kubectl apply -f certificate.yaml","title":"Install ClusterIssuer/Cert"},{"location":"cloudbees/cbc-gke-helm/#clusterissueryaml","text":"Make sure to replace REPLACE_WITH_YOUR_EMAIL_ADDRESS with your own email address. Let's Encrypt will use this to register the certificate and will notify you there when it expires. 1 2 3 4 5 6 7 8 9 10 11 apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-prod spec : acme : email : REPLACE_WITH_YOUR_EMAIL_ADDRESS server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-prod http01 : {}","title":"clusterissuer.yaml"},{"location":"cloudbees/cbc-gke-helm/#certificateyaml","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : MyHostName namespace : cloudbees-core spec : secretName : tls-cloudbees-core-kearos-net dnsNames : - cloudbees-core.kearos.net acme : config : - http01 : ingressClass : nginx domains : - cloudbees-core.kearos.net issuerRef : name : letsencrypt-prod kind : ClusterIssuer","title":"certificate.yaml"},{"location":"cloudbees/cbc-gke-helm/#install-with-valuesyaml","text":"1 2 3 4 helm install --name cloudbees-core \\ -f cloudbees-core-values.yaml \\ --namespace = cloudbees-core \\ cloudbees/cloudbees-core 1 kubectl rollout status statefulset cjoc 1 kubectl get po cjoc-0 1 kubectl logs -f cjoc-0 1 stern cjoc","title":"Install with values.yaml"},{"location":"cloudbees/cbc-gke-helm/#get-initial-password","text":"1 kubectl -n cloudbees-core exec cjoc-0 cat /var/jenkins_home/secrets/initialAdminPassword","title":"Get Initial Password"},{"location":"cloudbees/cbc-gke-helm/#valuesyaml","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # A helm example values file for standard kubernetes install. # An nginx-ingress controller is not installed and ssl isn t installed. # Install an nginx-ingress controller nginx-ingress : Enabled : false OperationsCenter : # Set the HostName for the Operation Center HostName : cloudbees-core.kearos.net # Setting ServiceType to ClusterIP creates ingress ServiceType : ClusterIP CSRF : ProxyCompatibility : true Ingress : Annotations : certmanager.k8s.io/cluster-issuer : letsencrypt-prod kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : false nginx.ingress.kubernetes.io/app-root : https://$best_http_host/cjoc/teams-check/ nginx.ingress.kubernetes.io/proxy-body-size : 50m nginx.ingress.kubernetes.io/proxy-request-buffering : off nginx.ingress.kubernetes.io/ssl-redirect : true tls : ## Set this to true in order to enable TLS on the ingress record Enable : true # Create a certificate kubernetes and use it here. SecretName : tls-cloudbees-core-kearos-net Host : cloudbees-core.kearos.net","title":"values.yaml"},{"location":"cloudbees/cbc-gke-helm/#core-post-install","text":"","title":"Core Post Install"},{"location":"cloudbees/cbc-gke-helm/#setup-api-token","text":"Go to http : // MyHostName / cjoc , login with your admin user. Click on the user's name (top right corner) - Configure - Generate Token . Warning You will see this token only once, so copy it and store it somewhere.","title":"Setup API Token"},{"location":"cloudbees/cbc-gke-helm/#get-cli","text":"1 2 export CJOC_URL = https:// MyHostName /cjoc/ http --download ${ CJOC_URL } /jnlpJars/jenkins-cli.jar --verify false","title":"Get CLI"},{"location":"cloudbees/cbc-gke-helm/#alias-cli","text":"1 2 USR = admin TKN = 1 alias cboc = java -jar jenkins-cli.jar -noKeyAuth -auth ${ USR } : ${ TKN } -s ${ CJOC_URL } 1 cboc version","title":"Alias CLI"},{"location":"cloudbees/cbc-gke-helm/#for-more-cli","text":"Go to http : // MyHostName / cjoc / cli","title":"For More CLI"},{"location":"cloudbees/cbc-post-install-tips/","text":"CloudBees Core Post Install Tips","title":"CloudBees Core Post Install Tips"},{"location":"cloudbees/cbc-post-install-tips/#cloudbees-core-post-install-tips","text":"","title":"CloudBees Core Post Install Tips"},{"location":"cloudbees/sso-azure-ad/","text":"Azure AD CloudBees Core In this article we're going to configure Azure AD as Single-Sign-Solution for CloudBees Core. The configuration rests on three points; 1) Azure AD, 2) Jenkins' SAML plugin, and 3) CloudBees Core's Role Base Access Control or RBAC for short. Important Currently there's a discrepency between the TimeToLive (TTL) of the RefreshToken in Azure AD and the default configuration in the SAML plugin. This creates the problem that you can be logged in, in Azure, but when going to Jenkins you get the Logged Out page. The only way to log back in, is to logout in Azure and back in. The reason is as follows: The max lifetime of the Access Token in Azure AD seems to be 24 hours where the refresh token can live for a maximum of 14 days (if the access token expires the refresh token is used to try to obtain a new access token). The Jenkins setting in Configure Global Security SAML Identity Provider Settings Maximum Authentication Lifetime is 24 hours (86400 in seconds) upping this to 1209600 (which is 14 days in seconds/the max lifetime of the Refresh Token). The recommended resolution is to set Maximum Authentication Lifetime to the same value of your RefreshToken TTL. If you haven't changed this in Azure AD, it is 1209600 . Manage Jenkins - Configure Global Security SAML Identity Provider Settings Maximum Authentication Lifetime = 1209600 Prerequisites Before we start, there are some requirements. a running CloudBees Core Operations Center instance this instance is accessible via https . if you do not have a valid certificate and you're running on Kubernetes take a look at my cert-manager guide Let's Encrypt can now also work with nip.io addresses active Azure subscription have an Azure subscription Administrator on hand Configure Azure Warning We use https : // cloudbees - core . example . com as the example URL for CloudBees Core. Please replace the domain to your actual domain in all the examples! Steps to execute We do the following steps on the Azure side. create the Azure Active Directory create users and groups create App Registration URL: https : // cloudbees - core . example . com / cjoc / securityRealm / finishLogin replace example . com with your domain, https is required! update manifest (for groups) change: groupMembershipClaims : null , (usually line 11) to: groupMembershipClaims : SecurityGroup , create SP ID / App ID URI grant admin consent Info If you use the Azure AD plugin, you also create a client secret. Information To Note Down The following information is unique to your installation, so you need to record them as you go along. App ID URI Object ID 's of Users and Groups you want to give rights Federation Metadata Document Endpoint Azure AD - App Registrations - - Endpoints (circular icon on top) you can use either the URL or the document contents make sure the URL contains the Tenant ID of your Azure Active Directory URL example: https://login.microsoftonline.com/ ${ TENANT_ID } /federationmetadata/2007-06/federationmetadata.xml You can find your Tenant ID in Azure Active Directory - Properties - Directory ID (different name, same ID) Visual Guide Below is a visual guide with screenshots. Pay attention to these hints in the screenshots. red : this is the main thing orange : this is how we got to the current page/view blue : while you're in this screen, there might be other things you could do Create New App Registration If you have an Azure account, you have an Azure Active Directory (Azure AD) pre-created. This guide assumes you have an Azure AD ready to use. That means the next step is to create an Application Registration. Give the registration a useful name, select who can authenticate and the redirect URL . This URL needs to be configured and MUST be the actual URL of your CloudBees Core installation. Important To have Client Masters as a fallback for login, incase Operations Center is down, you have to add another redirect URL for each Master. Azure AD - App Registrations - - Authentication - Web - https : // example . com / teams - cat / securityRealm / finishLogin App Registration Data To Write Down Depending on the plugin you're going to use (SAML or Azure AD) you need information from your Azure AD / App Registration. Tentant ID Object ID Client ID Federation Metadata Document you can use the document XML content or the URL Click on the Endpoints button to open the side-bar with the links. App ID We need the App ID - even if the SAML plugin doesn't mention it. Azure generates an APP ID URI for you. You can also use CloudBees Core's URL as the URI. The URI is shown when there is an error, so it recommended to use a value you can recognize. Info App ID must match in both Azure AD (set as App ID URI ) and the SAML plugin (set as Entity ID ) configuration in Jenkins. So write it down. API Permissions Of course, things wouldn't be proper IAM/LDAP/AD without giving some permissions. If we want to retrieve group information and other fields, we need to be able to read the Directory information. You find the Directory information via the Microsoft Graph api button. We select Application Permissions and then check Directory . Read . All . We don't need to write. The Permissions have changed, so we require an Administrator account to consent with the new permissions. Update Manifest As with the permissions, the default Manifest doesn't give us all the information we want. We want the groups so we can configure RBAC, and thus we have to set the groupMembershipsClaims claim attribute. We change the null to SecurityGroup . Please consult the Microsoft docs (see reference below) for other options. We can download, edit, and upload the manifest file. Alternatively, we can edit inline and hit save on top. Retrieve Group Object ID If we want to assign Azure AD groups to groups or roles in Jenkins' RBAC, we need to use the Object ID 's. Each Group and User has an Object ID , which have a handy Copy this button on the end of the value box! Configure Jenkins We now get to the point where we configure Jenkins. The SAML plugin is opensource, and thus it's configuration can also be used for Jenkins or CloudBees Jenkins Distribution . Steps Here are the steps we perform to configure Azure AD as the Single-Sign-On solution. install the SAML plugin I assume you know how to install plugins, so we skip this if you don't know Read the Managing Plugins Guide configure saml 2.0 in Jenkins setup groups (RBAC) administrators - admin group browsers - all other groups Visual Guide Below is a visual guide with screenshots. Please pay attention to the hints in the screenshots. Red : this is the main thing Orange : this is how we got to the current page/view Blue : while you're in this screen, there might be other things you could do Configure Security To go to Jenkins' security configuration, follow this route: login with an Admin user go to the Operations Center Manage Jenkins - Global Security Configuration Configure RBAC The SAML plugin configuration pollutes the screen with fields. My advice is to enable RBAC first. If you haven't got any groups/roles yet, I recommend using the Typical Initial Setup from the dropdown. The logged-in user is automatically registered as administrator. So if your Azure AD configuration doesn't work, this user can still manage Jenkins. Important Make sure you know the credentials of the current admin user. It will automatically be added to the Administrators group, and it will be your go-to account when you mess up the SAML configuration and you have to reset security. For how to reset the security configuration, see the For When You Mess Up paragraph. Configure SAML Select SAML 2 . 0 from the Security Realm options. Here we first supply our Federation Metadata Document content or it's URL. Each option - document content or URL - has its own Validate ... button, hit it and confirm it says Success . Info You can leave Displayname empty, which gives you the default naming scheme from Azure AD. I think this is ugly, as it amounts to something like ${ EMAIL_ADDRESS } _ ${ AD_DOMAIN } _ ${ AZURE_CORP_DOMAIN } . There are other options, I've settled for givenname , as there isn't a fullname by default, and well, I prefer Joost to a long hard to recognize string. Fields Displayname : http : // schemas . xmlsoap . org / ws / 2005 / 05 / identity / claims / givenname Group : http : // schemas . microsoft . com / ws / 2008 / 06 / identity / claims / groups Username : http : // schemas . xmlsoap . org / ws / 2005 / 05 / identity / claims / name Email : http : // schemas . xmlsoap . org / ws / 2005 / 05 / identity / claims / emailaddress SP Entity ID : the App ID URI you configured in Azure AD (hidden behind Advanced Configuration ) Configure RBAC Groups Tip Once Azure AD is configured, and it works, you can configure groups for RBAC just as you're used to. Both for classic RBAC and Team Masters. Just make sure you use the Azure AD Object ID 's of the groups to map them. Bonus tip, add every Azure AD group to Browsers , so you can directly map their groups to Team Master roles without problems. XML Config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 useSecurity true /useSecurity authorizationStrategy class= nectar.plugins.rbac.strategy.RoleMatrixAuthorizationStrategyImpl / securityRealm class= org.jenkinsci.plugins.saml.SamlSecurityRealm plugin= saml@1.1.2 displayNameAttributeName http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname /displayNameAttributeName groupsAttributeName http://schemas.microsoft.com/ws/2008/06/identity/claims/groups /groupsAttributeName maximumAuthenticationLifetime 86400 /maximumAuthenticationLifetime emailAttributeName http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress /emailAttributeName usernameCaseConversion none /usernameCaseConversion usernameAttributeName http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name /usernameAttributeName binding urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect /binding advancedConfiguration forceAuthn false /forceAuthn spEntityId https://cloudbees-core.kearos.net /spEntityId /advancedConfiguration idpMetadataConfiguration xml /xml url https://login.microsoftonline.com/95b46e09-0307-488b-a6fc-1d2717ba9c49/federationmetadata/2007-06/federationmetadata.xml /url period 5 /period /idpMetadataConfiguration /securityRealm disableRememberMe false /disableRememberMe Logout URL Depending on the requirements, you may want to specify a logout url in the SAML configuration to log you completely out of SAML, not just Core. An example https : // login . windows . net / tenant_id_of_your_app / oauth2 / logout ? post_logout_redirect_uri = logout_URL_of_your_app / logout For When You Mess Up This is the default config for security in CloudBees Core. This file is in ${ JENKINS_HOME } /config.xml , the XML tags we want to look at are quite near the top. 1 2 3 4 5 6 7 8 9 useSecurity true /useSecurity authorizationStrategy class= hudson.security.FullControlOnceLoggedInAuthorizationStrategy denyAnonymousReadAccess true /denyAnonymousReadAccess /authorizationStrategy securityRealm class= hudson.security.HudsonPrivateSecurityRealm disableSignup true /disableSignup enableCaptcha false /enableCaptcha /securityRealm disableRememberMe false /disableRememberMe On CloudBees Core Modern / Kubernetes To rectify a failed configuration, execute the following steps: exec into the cjoc - 0 container: kubectl exec - ti cjoc - 0 -- bash open config . xml : vi / var / jenkins_home / config . xml replace conflicting lines with the above snippet save the changes exit the container: exit kill the pod: kubectl delete po cjoc - 0 Tip For removing a whole line, stay in \"normal\" mode, and press d d (two times the d key). To add the new lines, go into insert mode by pressing the i key. Go back to \"normal\" mode by pressing the esc key. Then, save and quit, by writing: : wq followed by enter . References CloudBees Guide on Azure AD for Core SSO (outdated) SAML Plugin Docs for Azure AD (outdated) Microsoft Doc for Azure AD Tokens Microsoft Doc for Azure AD Optional Tokens Microsoft Doc for Azure AD Custom Tokens Alternative Azure AD Plugin (very new) Info Currently, there is a limitation which requires you to use the Object ID 's which make searching groups and people less than ideal. When the Alternative Azure AD Plugin is approved this may provide a more satisfactory solution.","title":"Single Sign On - AzureAD"},{"location":"cloudbees/sso-azure-ad/#azure-ad-cloudbees-core","text":"In this article we're going to configure Azure AD as Single-Sign-Solution for CloudBees Core. The configuration rests on three points; 1) Azure AD, 2) Jenkins' SAML plugin, and 3) CloudBees Core's Role Base Access Control or RBAC for short. Important Currently there's a discrepency between the TimeToLive (TTL) of the RefreshToken in Azure AD and the default configuration in the SAML plugin. This creates the problem that you can be logged in, in Azure, but when going to Jenkins you get the Logged Out page. The only way to log back in, is to logout in Azure and back in. The reason is as follows: The max lifetime of the Access Token in Azure AD seems to be 24 hours where the refresh token can live for a maximum of 14 days (if the access token expires the refresh token is used to try to obtain a new access token). The Jenkins setting in Configure Global Security SAML Identity Provider Settings Maximum Authentication Lifetime is 24 hours (86400 in seconds) upping this to 1209600 (which is 14 days in seconds/the max lifetime of the Refresh Token). The recommended resolution is to set Maximum Authentication Lifetime to the same value of your RefreshToken TTL. If you haven't changed this in Azure AD, it is 1209600 . Manage Jenkins - Configure Global Security SAML Identity Provider Settings Maximum Authentication Lifetime = 1209600","title":"Azure AD &amp; CloudBees Core"},{"location":"cloudbees/sso-azure-ad/#prerequisites","text":"Before we start, there are some requirements. a running CloudBees Core Operations Center instance this instance is accessible via https . if you do not have a valid certificate and you're running on Kubernetes take a look at my cert-manager guide Let's Encrypt can now also work with nip.io addresses active Azure subscription have an Azure subscription Administrator on hand","title":"Prerequisites"},{"location":"cloudbees/sso-azure-ad/#configure-azure","text":"Warning We use https : // cloudbees - core . example . com as the example URL for CloudBees Core. Please replace the domain to your actual domain in all the examples!","title":"Configure Azure"},{"location":"cloudbees/sso-azure-ad/#steps-to-execute","text":"We do the following steps on the Azure side. create the Azure Active Directory create users and groups create App Registration URL: https : // cloudbees - core . example . com / cjoc / securityRealm / finishLogin replace example . com with your domain, https is required! update manifest (for groups) change: groupMembershipClaims : null , (usually line 11) to: groupMembershipClaims : SecurityGroup , create SP ID / App ID URI grant admin consent Info If you use the Azure AD plugin, you also create a client secret.","title":"Steps to execute"},{"location":"cloudbees/sso-azure-ad/#information-to-note-down","text":"The following information is unique to your installation, so you need to record them as you go along. App ID URI Object ID 's of Users and Groups you want to give rights Federation Metadata Document Endpoint Azure AD - App Registrations - - Endpoints (circular icon on top) you can use either the URL or the document contents make sure the URL contains the Tenant ID of your Azure Active Directory URL example: https://login.microsoftonline.com/ ${ TENANT_ID } /federationmetadata/2007-06/federationmetadata.xml You can find your Tenant ID in Azure Active Directory - Properties - Directory ID (different name, same ID)","title":"Information To Note Down"},{"location":"cloudbees/sso-azure-ad/#visual-guide","text":"Below is a visual guide with screenshots. Pay attention to these hints in the screenshots. red : this is the main thing orange : this is how we got to the current page/view blue : while you're in this screen, there might be other things you could do","title":"Visual Guide"},{"location":"cloudbees/sso-azure-ad/#create-new-app-registration","text":"If you have an Azure account, you have an Azure Active Directory (Azure AD) pre-created. This guide assumes you have an Azure AD ready to use. That means the next step is to create an Application Registration. Give the registration a useful name, select who can authenticate and the redirect URL . This URL needs to be configured and MUST be the actual URL of your CloudBees Core installation. Important To have Client Masters as a fallback for login, incase Operations Center is down, you have to add another redirect URL for each Master. Azure AD - App Registrations - - Authentication - Web - https : // example . com / teams - cat / securityRealm / finishLogin","title":"Create New App Registration"},{"location":"cloudbees/sso-azure-ad/#app-registration-data-to-write-down","text":"Depending on the plugin you're going to use (SAML or Azure AD) you need information from your Azure AD / App Registration. Tentant ID Object ID Client ID Federation Metadata Document you can use the document XML content or the URL Click on the Endpoints button to open the side-bar with the links.","title":"App Registration Data To Write Down"},{"location":"cloudbees/sso-azure-ad/#app-id","text":"We need the App ID - even if the SAML plugin doesn't mention it. Azure generates an APP ID URI for you. You can also use CloudBees Core's URL as the URI. The URI is shown when there is an error, so it recommended to use a value you can recognize. Info App ID must match in both Azure AD (set as App ID URI ) and the SAML plugin (set as Entity ID ) configuration in Jenkins. So write it down.","title":"App ID"},{"location":"cloudbees/sso-azure-ad/#api-permissions","text":"Of course, things wouldn't be proper IAM/LDAP/AD without giving some permissions. If we want to retrieve group information and other fields, we need to be able to read the Directory information. You find the Directory information via the Microsoft Graph api button. We select Application Permissions and then check Directory . Read . All . We don't need to write. The Permissions have changed, so we require an Administrator account to consent with the new permissions.","title":"API Permissions"},{"location":"cloudbees/sso-azure-ad/#update-manifest","text":"As with the permissions, the default Manifest doesn't give us all the information we want. We want the groups so we can configure RBAC, and thus we have to set the groupMembershipsClaims claim attribute. We change the null to SecurityGroup . Please consult the Microsoft docs (see reference below) for other options. We can download, edit, and upload the manifest file. Alternatively, we can edit inline and hit save on top.","title":"Update Manifest"},{"location":"cloudbees/sso-azure-ad/#retrieve-group-object-id","text":"If we want to assign Azure AD groups to groups or roles in Jenkins' RBAC, we need to use the Object ID 's. Each Group and User has an Object ID , which have a handy Copy this button on the end of the value box!","title":"Retrieve Group Object ID"},{"location":"cloudbees/sso-azure-ad/#configure-jenkins","text":"We now get to the point where we configure Jenkins. The SAML plugin is opensource, and thus it's configuration can also be used for Jenkins or CloudBees Jenkins Distribution .","title":"Configure Jenkins"},{"location":"cloudbees/sso-azure-ad/#steps","text":"Here are the steps we perform to configure Azure AD as the Single-Sign-On solution. install the SAML plugin I assume you know how to install plugins, so we skip this if you don't know Read the Managing Plugins Guide configure saml 2.0 in Jenkins setup groups (RBAC) administrators - admin group browsers - all other groups","title":"Steps"},{"location":"cloudbees/sso-azure-ad/#visual-guide_1","text":"Below is a visual guide with screenshots. Please pay attention to the hints in the screenshots. Red : this is the main thing Orange : this is how we got to the current page/view Blue : while you're in this screen, there might be other things you could do","title":"Visual Guide"},{"location":"cloudbees/sso-azure-ad/#configure-security","text":"To go to Jenkins' security configuration, follow this route: login with an Admin user go to the Operations Center Manage Jenkins - Global Security Configuration","title":"Configure Security"},{"location":"cloudbees/sso-azure-ad/#configure-rbac","text":"The SAML plugin configuration pollutes the screen with fields. My advice is to enable RBAC first. If you haven't got any groups/roles yet, I recommend using the Typical Initial Setup from the dropdown. The logged-in user is automatically registered as administrator. So if your Azure AD configuration doesn't work, this user can still manage Jenkins. Important Make sure you know the credentials of the current admin user. It will automatically be added to the Administrators group, and it will be your go-to account when you mess up the SAML configuration and you have to reset security. For how to reset the security configuration, see the For When You Mess Up paragraph.","title":"Configure RBAC"},{"location":"cloudbees/sso-azure-ad/#configure-saml","text":"Select SAML 2 . 0 from the Security Realm options. Here we first supply our Federation Metadata Document content or it's URL. Each option - document content or URL - has its own Validate ... button, hit it and confirm it says Success . Info You can leave Displayname empty, which gives you the default naming scheme from Azure AD. I think this is ugly, as it amounts to something like ${ EMAIL_ADDRESS } _ ${ AD_DOMAIN } _ ${ AZURE_CORP_DOMAIN } . There are other options, I've settled for givenname , as there isn't a fullname by default, and well, I prefer Joost to a long hard to recognize string.","title":"Configure SAML"},{"location":"cloudbees/sso-azure-ad/#fields","text":"Displayname : http : // schemas . xmlsoap . org / ws / 2005 / 05 / identity / claims / givenname Group : http : // schemas . microsoft . com / ws / 2008 / 06 / identity / claims / groups Username : http : // schemas . xmlsoap . org / ws / 2005 / 05 / identity / claims / name Email : http : // schemas . xmlsoap . org / ws / 2005 / 05 / identity / claims / emailaddress SP Entity ID : the App ID URI you configured in Azure AD (hidden behind Advanced Configuration )","title":"Fields"},{"location":"cloudbees/sso-azure-ad/#configure-rbac-groups","text":"Tip Once Azure AD is configured, and it works, you can configure groups for RBAC just as you're used to. Both for classic RBAC and Team Masters. Just make sure you use the Azure AD Object ID 's of the groups to map them. Bonus tip, add every Azure AD group to Browsers , so you can directly map their groups to Team Master roles without problems.","title":"Configure RBAC Groups"},{"location":"cloudbees/sso-azure-ad/#xml-config","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 useSecurity true /useSecurity authorizationStrategy class= nectar.plugins.rbac.strategy.RoleMatrixAuthorizationStrategyImpl / securityRealm class= org.jenkinsci.plugins.saml.SamlSecurityRealm plugin= saml@1.1.2 displayNameAttributeName http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname /displayNameAttributeName groupsAttributeName http://schemas.microsoft.com/ws/2008/06/identity/claims/groups /groupsAttributeName maximumAuthenticationLifetime 86400 /maximumAuthenticationLifetime emailAttributeName http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress /emailAttributeName usernameCaseConversion none /usernameCaseConversion usernameAttributeName http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name /usernameAttributeName binding urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect /binding advancedConfiguration forceAuthn false /forceAuthn spEntityId https://cloudbees-core.kearos.net /spEntityId /advancedConfiguration idpMetadataConfiguration xml /xml url https://login.microsoftonline.com/95b46e09-0307-488b-a6fc-1d2717ba9c49/federationmetadata/2007-06/federationmetadata.xml /url period 5 /period /idpMetadataConfiguration /securityRealm disableRememberMe false /disableRememberMe","title":"XML Config"},{"location":"cloudbees/sso-azure-ad/#logout-url","text":"Depending on the requirements, you may want to specify a logout url in the SAML configuration to log you completely out of SAML, not just Core. An example https : // login . windows . net / tenant_id_of_your_app / oauth2 / logout ? post_logout_redirect_uri = logout_URL_of_your_app / logout","title":"Logout URL"},{"location":"cloudbees/sso-azure-ad/#for-when-you-mess-up","text":"This is the default config for security in CloudBees Core. This file is in ${ JENKINS_HOME } /config.xml , the XML tags we want to look at are quite near the top. 1 2 3 4 5 6 7 8 9 useSecurity true /useSecurity authorizationStrategy class= hudson.security.FullControlOnceLoggedInAuthorizationStrategy denyAnonymousReadAccess true /denyAnonymousReadAccess /authorizationStrategy securityRealm class= hudson.security.HudsonPrivateSecurityRealm disableSignup true /disableSignup enableCaptcha false /enableCaptcha /securityRealm disableRememberMe false /disableRememberMe","title":"For When You Mess Up"},{"location":"cloudbees/sso-azure-ad/#on-cloudbees-core-modern-kubernetes","text":"To rectify a failed configuration, execute the following steps: exec into the cjoc - 0 container: kubectl exec - ti cjoc - 0 -- bash open config . xml : vi / var / jenkins_home / config . xml replace conflicting lines with the above snippet save the changes exit the container: exit kill the pod: kubectl delete po cjoc - 0 Tip For removing a whole line, stay in \"normal\" mode, and press d d (two times the d key). To add the new lines, go into insert mode by pressing the i key. Go back to \"normal\" mode by pressing the esc key. Then, save and quit, by writing: : wq followed by enter .","title":"On CloudBees Core Modern / Kubernetes"},{"location":"cloudbees/sso-azure-ad/#references","text":"CloudBees Guide on Azure AD for Core SSO (outdated) SAML Plugin Docs for Azure AD (outdated) Microsoft Doc for Azure AD Tokens Microsoft Doc for Azure AD Optional Tokens Microsoft Doc for Azure AD Custom Tokens Alternative Azure AD Plugin (very new) Info Currently, there is a limitation which requires you to use the Object ID 's which make searching groups and people less than ideal. When the Alternative Azure AD Plugin is approved this may provide a more satisfactory solution.","title":"References"},{"location":"cloudbees/teams-automation/","text":"Core Modern Teams Automation CloudBees Core on Modern (meaning, on Kubernetes) has two main types of Jenkins Masters, a Managed Master, and a Team Master. In this article, we're going to automate the creation and management of the Team Masters. Hint If you do not want to read any of the code here, or just want to take a look at the end result - pipeline wise - you can find working examples on GitHub. Template Repository - creates a new team template and a PR to the GitOps repository GitOps Repository - applies GitOps principles to manage the CloudBees Core Team Masters Goals Just automating the creation of a Team Master is relatively easy, as this can be done via the Client Jar . So we're going to set some additional goals to create a decent challenge. GitOps : I want to be able to create and delete Team Masters by managing configuration in a Git repository Configuration-as-Code : as much of the configuration as possible should be stored in the Git repository Namespace : one of the major reasons for Team Masters to exist is to increase (Product) Team autonomy, which in Kubernetes should correspond to a namespace . So I want each Team Master to be in its own Namespace! Self-Service : the solution should cater to (semi-)autonomous teams and lower the workload of the team managing CloudBees Core. So requesting a Team Master should be doable by everyone Before We Start Some assumptions need to be taken care off before we start. Kubernetes cluster in which you are ClusterAdmin if you don't have one yet, there are guides on this elsewhere on the site your cluster has enough capacity (at least two nodes of 4gb memory) your cluster has CloudBees Core Modern installed if you don't have this yet look at one of the guides on this site or look at the guides on CloudBees.com have administrator access to CloudBees Core Cloud Operations Center Code Examples The more extensive Code Examples, such as Kubernetes Yaml files, are collapsed by default. You can open them by clicking on them. On the right, the code snippet will have a [ ] copy icon. Below is an example. Code Snippet Example Here's a code snippet. 1 2 3 4 5 6 7 8 9 10 pipeline { agent any stages { stage ( Hello ) { steps { echo Hello World! } } } } Bootstrapping All right, so we want to use GitOps and to process the changes we need a Pipeline that can be triggered by a Webhook. I believe in everything as code - except Secrets and such - which includes the Pipeline. Unfortunately, Operations Center cannot run such pipelines. To get over this hurdle, we will create a special Ops Team Master. This Master will be configured to be able to Manage the other Team Masters for us. Log into your Operations Center with a user that has administrative access. Create API Token Create a new API Token for your administrator user by clicking on the user's name - top right corner. Select the Configuration menu on the left and then you should see a section where you can Create a API Token . This Token will disappear, so write it down. Get Configure Client Jar Replace the values marked by ... . The Operations Center URL should look like this: http : // cbcore . mydomain . com / cjoc . Setup the connection variables. 1 OC_URL = your operations center url 1 2 USR = your username TKN = api token Download the Client Jar. 1 curl ${ OC_URL } /jnlpJars/jenkins-cli.jar -o jenkins-cli.jar Create Alias Test 1 alias cboc = java -jar jenkins-cli.jar -noKeyAuth -auth ${ USR } : ${ TKN } -s ${ OC_URL } 1 cboc version Create Team Ops As the tasks of the Team Masters for managing Operations are quite specific and demand special rights, I'd recommend putting this in its own namespace . To do so properly, we need to configure a few things. allows Operations Center access to this namespace (so it can create the Team Master) give the ServiceAccount the permissions to create namespace 's for the other Team Masters add config map for the Jenkins Agents temporarily change Operations Center's operating Namespace (where it will spawn resources in) use the CLI to create the team - ops Team Master reset Operations Center's operating Namespace Update Create Kubernetes Namespaces Create Team Ops Namespace 1 kubectl apply -f team-ops-namespace.yaml team-ops-namespace.yaml This creates the team - ops namespace including all the resources required such as ResourceQuota , ServiceAccount and so on. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 apiVersion : v1 kind : Namespace metadata : name : team-ops --- apiVersion : v1 kind : ResourceQuota metadata : name : resource-quota namespace : team-ops spec : hard : pods : 20 requests.cpu : 4 requests.memory : 6Gi limits.cpu : 5 limits.memory : 10Gi services.loadbalancers : 0 services.nodeports : 0 persistentvolumeclaims : 10 --- apiVersion : v1 kind : ServiceAccount metadata : name : jenkins namespace : team-ops --- kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : pods-all namespace : team-ops rules : - apiGroups : [ ] resources : [ pods ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/exec ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/log ] verbs : [ get , list , watch ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : jenkins namespace : team-ops roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pods-all subjects : - kind : ServiceAccount name : jenkins namespace : team-ops --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : cjoc roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : master-management subjects : - kind : ServiceAccount name : jenkins namespace : team-ops --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : create-namespaces rules : - apiGroups : [ * ] resources : [ serviceaccounts , rolebindings , roles , resourcequotas , namespaces ] verbs : [ create , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ configmaps , rolebindings , roles , resourcequotas , namespaces ] verbs : [ create , get , list ] - apiGroups : [ ] resources : [ events ] verbs : [ get , list , watch ] - apiGroups : [ ] resources : [ persistentvolumeclaims , pods , pods/exec , services , statefulsets , ingresses , extensions ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/log ] verbs : [ get , list , watch ] - apiGroups : [ ] resources : [ secrets ] verbs : [ list ] - apiGroups : [ apps ] resources : [ statefulsets ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ extensions ] resources : [ ingresses ] verbs : [ create , delete , get , list , patch , update , watch ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : ops-namespace roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : create-namespaces subjects : - kind : ServiceAccount name : jenkins namespace : team-ops Update Operation Center ServiceAccount The ServiceAccount under which Operation Center runs, only has rights in it's own namespace . Which means it cannot create our Team Ops Master. Below is the .yaml file for Kubernetes and the command to apply it. Warning I assume you're using the default cloudbees - core as per Cloudbees' documentation. If this is not the case, change the last line, namespace : cloudbees - core with the namespace your Operation Center runs in. 1 kubectl apply -f patch-oc-serviceaccount.yaml -n team-ops patch-oc-serviceaccount.yaml This patches the existing Operation Center's ServiceAccount to also have the correct rights in the team - ops namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : master-management rules : - apiGroups : [ ] resources : [ pods ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/exec ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/log ] verbs : [ get , list , watch ] - apiGroups : [ apps ] resources : [ statefulsets ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ services ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ persistentvolumeclaims ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ extensions ] resources : [ ingresses ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ secrets ] verbs : [ list ] - apiGroups : [ ] resources : [ events ] verbs : [ get , list , watch ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : cjoc roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : master-management subjects : - kind : ServiceAccount name : cjoc namespace : cloudbees-core Jenkins Agent ConfigMap 1 kubectl apply -f jenkins-agent-config-map.yaml -n team-ops jenkins-agent-config-map.yaml Creates the Jenkins Agent ConfigMap, which contains the information the Jenkins Agent - within a PodTemplate - uses to connect to the Jenkins Master. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 apiVersion : v1 kind : ConfigMap metadata : name : jenkins-agent data : jenkins-agent : | #!/usr/bin/env sh # The MIT License # # Copyright (c) 2015, CloudBees, Inc. # # Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files (the Software ), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED AS IS , WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN # THE SOFTWARE. # Usage jenkins-slave.sh [options] -url http://jenkins [SECRET] [AGENT_NAME] # Optional environment variables : # * JENKINS_TUNNEL : HOST:PORT for a tunnel to route TCP traffic to jenkins host, when jenkins can t be directly accessed over network # * JENKINS_URL : alternate jenkins URL # * JENKINS_SECRET : agent secret, if not set as an argument # * JENKINS_AGENT_NAME : agent name, if not set as an argument if [ $# -eq 1 ]; then # if `docker run` only has one arguments, we assume user is running alternate command like `bash` to inspect the image exec $@ else # if -tunnel is not provided try env vars case $@ in * -tunnel *) ;; *) if [ ! -z $JENKINS_TUNNEL ]; then TUNNEL= -tunnel $JENKINS_TUNNEL fi ;; esac if [ -n $JENKINS_URL ]; then URL= -url $JENKINS_URL fi if [ -n $JENKINS_NAME ]; then JENKINS_AGENT_NAME= $JENKINS_NAME fi if [ -z $JNLP_PROTOCOL_OPTS ]; then echo Warning: JnlpProtocol3 is disabled by default, use JNLP_PROTOCOL_OPTS to alter the behavior JNLP_PROTOCOL_OPTS= -Dorg.jenkinsci.remoting.engine.JnlpProtocol3.disabled=true fi # If both required options are defined, do not pass the parameters OPT_JENKINS_SECRET= if [ -n $JENKINS_SECRET ]; then case $@ in * ${JENKINS_SECRET} *) echo Warning: SECRET is defined twice in command-line arguments and the environment variable ;; *) OPT_JENKINS_SECRET= ${JENKINS_SECRET} ;; esac fi OPT_JENKINS_AGENT_NAME= if [ -n $JENKINS_AGENT_NAME ]; then case $@ in * ${JENKINS_AGENT_NAME} *) echo Warning: AGENT_NAME is defined twice in command-line arguments and the environment variable ;; *) OPT_JENKINS_AGENT_NAME= ${JENKINS_AGENT_NAME} ;; esac fi SLAVE_JAR=/usr/share/jenkins/slave.jar if [ ! -f $SLAVE_JAR ]; then tmpfile=$(mktemp) if hash wget /dev/null 2 1; then wget -O $tmpfile $JENKINS_URL/jnlpJars/slave.jar elif hash curl /dev/null 2 1; then curl -o $tmpfile $JENKINS_URL/jnlpJars/slave.jar else echo Image does not include $SLAVE_JAR and could not find wget or curl to download it return 1 fi SLAVE_JAR=$tmpfile fi #TODO: Handle the case when the command-line and Environment variable contain different values. #It is fine it blows up for now since it should lead to an error anyway. exec java $JAVA_OPTS $JNLP_PROTOCOL_OPTS -cp $SLAVE_JAR hudson.remoting.jnlp.Main -headless $TUNNEL $URL $OPT_JENKINS_SECRET $OPT_JENKINS_AGENT_NAME $@ fi Create Initial Master To make it easier to change the namespace if needed, its extracted out from the command. 1 OriginalNamespace = cloudbees-core This script changes the Operations Center's operating namespace , creates a Team Master with the name ops , and then resets the namespace. 1 2 3 cboc groovy = configure-oc-namespace.groovy team-ops cboc teams ops --put team-ops.json cboc groovy = configure-oc-namespace.groovy $OriginalNamespace team-ops.json This json file that describes a team. By default there are three roles defined on a team, TEAM_ADMIN , TEAM_MEMBER , and TEAM_GUEST . Don't forget to change the id 's to Group ID's from your Single-Sign-On solution. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { version : 1 , data : { name : ops , displayName : Operations , provisioningRecipe : basic , members : [{ id : Catmins , roles : [ TEAM_ADMIN ] }, { id : Pirates , roles : [ TEAM_MEMBER ] }, { id : Continental , roles : [ TEAM_GUEST ] } ], icon : { name : hexagons , color : #8d7ec1 } } } configure-oc-namespace.groovy This is a Jenkins Configuration or System Groovy script. It will change the namespace Operation Center uses to create resources. You can change this in the UI by going to Operations Center - Manage Jenkins - System Configuration - Master Provisioning - Namespace . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import hudson.* import hudson.util.Secret ; import hudson.util.Scrambler ; import hudson.util.FormValidation ; import jenkins.* import jenkins.model.* import hudson.security.* import com.cloudbees.masterprovisioning.kubernetes.KubernetesMasterProvisioning import com.cloudbees.masterprovisioning.kubernetes.KubernetesClusterEndpoint println === KubernetesMasterProvisioning Configuration - start println == Retrieving main configuration def descriptor = Jenkins . getInstance (). getInjector (). getInstance ( KubernetesMasterProvisioning . DescriptorImpl . class ) def namespace = this . args [ 0 ] def currentKubernetesClusterEndpoint = descriptor . getClusterEndpoints (). get ( 0 ) println = Found current endpoint println = + currentKubernetesClusterEndpoint . toString () def id = currentKubernetesClusterEndpoint . getId () def name = currentKubernetesClusterEndpoint . getName () def url = currentKubernetesClusterEndpoint . getUrl () def credentialsId = currentKubernetesClusterEndpoint . getCredentialsId () println == Setting Namspace to + namespace def updatedKubernetesClusterEndpoint = new KubernetesClusterEndpoint ( id , name , url , credentialsId , namespace ) def clusterEndpoints = new ArrayList KubernetesClusterEndpoint () clusterEndpoints . add ( updatedKubernetesClusterEndpoint ) descriptor . setClusterEndpoints ( clusterEndpoints ) println == Saving Jenkins configuration descriptor . save () println === KubernetesMasterProvisioning Configuration - finish Configure Team Ops Master Now that we've created the Operations Team Master (Team Ops), we can configure it. The Pipelines we need will require credentials, we describe them below. githubtoken_token : GitHub API Token only, credentials type Secret Text (for the PR pipeline) githubtoken : GitHub username and API Token jenkins-api : Username and API Token for Operations Center. Just like the one we used for Client Jar. We also need to have a Global Pipeline Library defined by the name github . com / joostvdg / jpl - core . This, as the name suggests, should point to https : // github . com / joostvdg / jpl - core . git . Create GitOps Pipeline In total, we need two repositories and two or three Pipelines. You can either use my CLI Docker Image or roll your own. I will proceed as if you will create your own. CLI Image Pipeline : this will create a CLI Docker Image that is used to talk to Operations Center via the Client Jar (CLI) PR Pipeline : I like the idea of Self-Service, but in order to keep things in check, you might want to provide that via a PullRequest (PR) rather than a direct write to the Master branch. This is also Repository One, as I prefer having each pipeline in their own Repository, but you don't need to. Main Pipeline : will trigger on a commit to the Master branch and create the new team. I'll even throw in a free manage your Team Recipes for free as well. Create CLI Image Pipeline In a Kubernetes cluster, you should not build with Docker directly, use an in-cluster builder such as Kaniko or Buildah . You can read more about the why and how elsewhere on this site . Tip If you do not want to create your own, you can re-use my images. There should be one available for every recent version of CloudBees Core that will work with your Operations Center. The images are available in DockerHub at caladreas/cbcore-cli Kaniko Configuration Kaniko uses a Docker Image to build your Docker Image in cluster. It does however need to directly communicate to your Docker Registry. This requires a Kubernetes Secret of type docker - registry . How you can do this and more, you can read on the CloudBees Core Docs . Pipeline Now that you have Kaniko configured, you can use this Pipeline to create your own CLI Images. Caution Make sure you replace the environment variables with values that make sense to you. CJOC_URL internal url in Kubernets, usually http : // cjoc . namespace / cjoc REGISTRY : index.docker.io = DockerHub REPO : docker repository name IMAGE : docker image name Jenkinsfile Jenkins Declarative Pipeline for the CLI Image geberation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 pipeline { agent { kubernetes { //cloud kubernetes label test yaml kind: Pod metadata: name: test spec: containers: - name: curl image: byrnedo/alpine-curl command: - cat tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: jenkins-docker-cfg projected: sources: - secret: name: docker-credentials items: - key: .dockerconfigjson path: .docker/config.json } } environment { CJOC_URL = http://cjoc.cloudbees-core/cjoc CLI_VERSION = REGISTRY = index.docker.io REPO = caladreas IMAGE = cbcore-cli } stages { stage ( Download CLI ) { steps { container ( curl ) { sh curl --version sh echo ${CJOC_URL}/jnlpJars/jenkins-cli.jar sh curl ${CJOC_URL}/jnlpJars/jenkins-cli.jar --output jenkins-cli.jar sh ls -lath } } } stage ( Prepare ) { parallel { stage ( Verify CLI ) { environment { CREDS = credentials ( jenkins-api ) CLI = java -jar jenkins-cli.jar -noKeyAuth -s ${CJOC_URL} -auth } steps { sh echo ${CLI} script { CLI_VERSION = sh returnStdout: true , script: ${CLI} ${CREDS} version } sh echo ${CLI_VERSION} } } stage ( Prepare Dockerfile ) { steps { writeFile encoding: UTF-8 , file: Dockerfile , text: FROM mcr.microsoft.com/java/jre-headless:8u192-zulu-alpine WORKDIR /usr/bin ADD jenkins-cli.jar . RUN pwd RUN ls -lath } } } } stage ( Build with Kaniko ) { environment { PATH = /busybox:/kaniko:$PATH TAG = ${CLI_VERSION} } steps { sh echo image fqn=${REGISTRY}/${REPO}/${IMAGE}:${TAG} container ( name: kaniko , shell: /busybox/sh ) { sh #!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:${TAG} /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:latest } } } } } PR Pipeline Caution The PR Pipeline example builds upon the GitHub API, if you're not using GItHub, you will have to figure out another way to make the PR. Tools Used yq : commandline tool for processing Yaml files jq commandline tool for pressing Json files Kustomize templating tool for Kubernetes Yaml, as of Kubernetes 1 . 13 , this is part of the Client (note, your server can be older, don't worry!) Hub commandline client for GitHub Repository Layout folder: team - master - template with file simple . json folder: namespace - creation with folder: kustomize this contains the Kustomize configuration Simple.json This is a template for the team JSON definition. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { version : 1 , data : { name : NAME , displayName : DISPLAY_NAME , provisioningRecipe : RECIPE , members : [ { id : ADMINS , roles : [ TEAM_ADMIN ] }, { id : MEMBERS , roles : [ TEAM_MEMBER ] }, { id : GUESTS , roles : [ TEAM_GUEST ] } ], icon : { name : ICON , color : HEX_COLOR } } } Kustomize Configuration Kustomize is a tool for template Kubernetes YAML definitions, which is what we need here. However, only for the namespace creation configuration. So if you don't want to do that, you can skip this. The Kustomize configuration has two parts, a folder called team - example with a kustomization . yaml . This will be what we configure to generate a new yaml definition. The main template is in the folder base , where the entrypoint will be again kustomization . yaml . This time, the kustomization . yaml will link to all the template files we need. As posting all these yaml files again is a bit much, I'll link to my example repo. Feel free to fork it instead: cb-team-gitops-template configmap.yaml : the Jenkins Agent ConfigMap namespace.yaml : the new namespace resource-quota.yaml : resource quota's for the namespace role-binding-cjoc.yaml : a role binding for the CJOC ServiceAccount, so it create create the new Master in the new namespace role-binding.yaml : the role binding for the jenkins ServiceAccount, which allows the new Master to create and manage Pods (for PodTemplates) role-cjoc.yaml : the role for CJOC for the ability to create a Master in the new Namspace role.yaml : the role for the jenkins ServiceAccount for the new Master service-account.yaml : the ServiceAccount, jenkins , used by the new Master Pipeline The Pipeline will do the following: capture input parameters to be used to customize the Team Master update the Kustomize template to make sure every resource is correct for the new namespace ( teams - name of team ) execute Kustomize to generate a single yaml file that defines the configuration for the new Team Masters' namespace process the simple . json to generate a team . json file for the new Team Master for use with the Jenkins CLI checkout your GIT_REPO that contains your team definitions create a new PR to your GIT_REPO for the new team Jenkinsfile Variables to update: GIT_REPO : the GitHub repository in which the Team Definitions are stored RESET_NAMESPACE : the namespace Operations Center should use as default 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 pipeline { agent { kubernetes { label team-automation yaml kind: Pod spec: containers: - name: hub image: caladreas/hub command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 150m limits: memory: 50Mi cpu: 150m - name: kubectl image: bitnami/kubectl:latest command: [ cat ] tty: true securityContext: runAsUser: 1000 fsGroup: 1000 resources: requests: memory: 50Mi cpu: 100m limits: memory: 150Mi cpu: 200m - name: yq image: mikefarah/yq command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m - name: jq image: colstrom/jq command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m } } libraries { lib ( github.com/joostvdg/jpl-core ) } options { disableConcurrentBuilds () // we don t want more than one at a time checkoutToSubdirectory templates // we need to do two checkouts buildDiscarder logRotator ( artifactDaysToKeepStr: , artifactNumToKeepStr: , daysToKeepStr: 5 , numToKeepStr: 5 ) // always clean up } environment { envGitInfo = RESET_NAMESPACE = jx-production TEAM_BASE_NAME = NAMESPACE_TO_CREATE = DISPLAY_NAME = TEAM_RECIPE = ICON = ICON_COLOR_CODE = ADMINS_ROLE = MEMBERS_ROLE = GUESTS_ROLE = RECORD_LOC = GIT_REPO = } stages { stage ( Team Details ) { input { message Please enter the team details. ok Looks good, proceed parameters { string ( name: Name , defaultValue: hex , description: Please specify a team name ) string ( name: DisplayName , defaultValue: Hex , description: Please specify a team display name ) choice choices: [ joostvdg , basic , java-web ], description: Please select a Team Recipe , name: TeamRecipe choice choices: [ anchor , bear , bowler-hat , briefcase , bug , calculator , calculatorcart , clock , cloud , cloudbees , connect , dollar-bill , dollar-symbol , file , flag , flower-carnation , flower-daisy , help , hexagon , high-heels , jenkins , key , marker , monocle , mustache , office , panther , paw-print , teacup , tiger , truck ], description: Please select an Icon , name: Icon string ( name: IconColorCode , defaultValue: #CCCCCC , description: Please specify a valid html hexcode for the color (https://htmlcolorcodes.com/) ) string ( name: Admins , defaultValue: Catmins , description: Please specify a groupid or userid for the TEAM_ADMIN role ) string ( name: Members , defaultValue: Pirates , description: Please specify a groupid or userid for the TEAM_MEMBER role ) string ( name: Guests , defaultValue: Continental , description: Please specify a groupid or userid for the TEAM_GUEST role ) } } steps { println Name=${Name} println DisplayName=${DisplayName} println TeamRecipe=${TeamRecipe} println Icon=${Icon} println IconColorCode=${IconColorCode} println Admins=${Admins} println Members=${Members} println Guests=${Guests} script { TEAM_BASE_NAME = ${Name} NAMESPACE_TO_CREATE = cb-teams-${Name} DISPLAY_NAME = ${DisplayName} TEAM_RECIPE = ${TeamRecipe} ICON = ${Icon} ICON_COLOR_CODE = ${IconColorCode} ADMINS_ROLE = ${Admins} MEMBERS_ROLE = ${Members} GUESTS_ROLE = ${Guests} RECORD_LOC = templates/teams/${Name} sh mkdir -p ${RECORD_LOC} } } } stage ( Create Team Config ) { environment { BASE = templates/namespace-creation/kustomize NAMESPACE = ${NAMESPACE_TO_CREATE} RECORD_LOC = templates/teams/${TEAM_BASE_NAME} } parallel { stage ( Namespace ) { steps { container ( yq ) { sh yq w -i ${BASE}/base/role-binding.yaml subjects[0].namespace ${NAMESPACE} sh yq w -i ${BASE}/base/namespace.yaml metadata.name ${NAMESPACE} sh yq w -i ${BASE}/team-example/kustomization.yaml namespace ${NAMESPACE} } container ( kubectl ) { sh kubectl kustomize ${BASE}/team-example ${RECORD_LOC}/team.yaml cat ${RECORD_LOC}/team.yaml } } } stage ( Team Master JSON ) { steps { container ( jq ) { sh jq \\ .data.name = ${TEAM_BASE_NAME} |\\ .data.displayName = ${DISPLAY_NAME} |\\ .data.provisioningRecipe = ${TEAM_RECIPE} |\\ .data.icon.name = ${ICON} |\\ .data.icon.color = ${ICON_COLOR_CODE} |\\ .data.members[0].id = ${ADMINS_ROLE} |\\ .data.members[1].id = ${MEMBERS_ROLE} |\\ .data.members[2].id = ${GUESTS_ROLE} \\ templates/team-master-template/simple.json ${RECORD_LOC}/team.json } sh cat ${RECORD_LOC}/team.json } } } } stage ( Create PR ) { when { branch master } environment { RECORD_OLD_LOC = templates/teams/${TEAM_BASE_NAME} RECORD_LOC = teams/${TEAM_BASE_NAME} PR_CHANGE_NAME = add_team_${TEAM_BASE_NAME} } steps { container ( hub ) { dir ( cb-team-gitops ) { script { envGitInfo = git ${GIT_REPO} } sh git checkout -b ${PR_CHANGE_NAME} sh ls -lath ../${RECORD_OLD_LOC} sh cp -R ../${RECORD_OLD_LOC} ./teams sh ls -lath sh ls -lath teams/ gitRemoteConfigByUrl ( envGitInfo . GIT_URL , githubtoken_token ) // must be a API Token ONLY - secret text sh git config --global user.email jenkins@jenkins.io git config --global user.name Jenkins git add ${RECORD_LOC} git status git commit -m add team ${TEAM_BASE_NAME} git push origin ${PR_CHANGE_NAME} // has to be indented like that, else the indents will be in the pr description writeFile encoding: UTF-8 , file: pr-info.md , text: Add ${TEAM_BASE_NAME} \\n This pr is automatically generated via CloudBees.\\\\n \\n The job: ${env.JOB_URL} // TODO: unfortunately, environment {} s credentials have fixed environment variable names // TODO: in this case, they need to be EXACTLY GITHUB_PASSWORD and GITHUB_USER script { withCredentials ([ usernamePassword ( credentialsId: githubtoken , passwordVariable: GITHUB_PASSWORD , usernameVariable: GITHUB_USER )]) { sh set +x hub pull-request --force -F pr-info.md -l ${TEAM_BASE_NAME} --no-edit } } } } } } } } Main Pipeline The main Pipeline should be part of a repository. The Repository should look like this: recipes (folder) recipes . json - current complete list of CloudBees Core Team Recipes definition teams (folder) folder per team team . json - CloudBees Core Team definition team . yaml - Kubernetes YAML definition of the namespace and all its resources Process The pipeline can be a bit hard to grasp, so let me break it down into individual steps. We have the following stages: Create Team - which is broken into sub-stages via the sequential stages feature . * Parse Changelog * Create Namespace * Change OC Namespace * Create Team Master Test CLI Connection Update Team Recipes Notable Statements disableConcurrentBuilds We change the namespace of Operation Center to a different value only for the duration of creating this master. This is something that should probably be part of the Team Master creation, but as it is a single configuration option for all that Operation Center does, we need to be careful. By ensuring we only run one build concurrently, we reduce the risk of this blowing up in our face. 1 2 3 options { disableConcurrentBuilds () } when { } The When Directive allows us to creating effective conditions for when a stage should be executed. The snippet below shows the use of a combination of both the branch and changeset built-in filters. changeset looks at the commit being build and validates that there was a change in that file path. 1 when { allOf { branch master ; changeset teams/**/team.* } } post { always { } } The Post Directive allows us to run certain commands after the main pipeline has run depending on the outcome (compared or not to the previous outcome). In this case, we want to make sure we reset the namespace used by Operations Center to the original value. By using post { always {} } , it will ALWAYS run, regardless of the status of the pipeline. So we should be safe. 1 2 3 4 5 6 7 post { always { container ( cli ) { sh ${CLI} ${CREDS} groovy = resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE} } } } stages { stage { parallel { stage() { stages { stage { Oke, you might've noticed this massive indenting depth and probably have some questions. By combining sequential stages with parallel stages we can create a set of stages that will be executed in sequence but can be controlled by a single when {} statement whether or not they get executed. This prevents mistakes being made in the condition and accidentally running one or other but not all the required steps. 1 2 3 4 5 6 stages { stage ( Create Team ) { parallel { stage ( Main ) { stages { stage ( Parse Changelog ) { changetSetData container('jpb') {} Alright, so even if we know a team was added in / teams / team - name , we still don't know the following two things: 1) what is the name of this team, 2) was this team changed or deleted? So we have to process the changelog to be able to answer these questions as well. There are different ways of getting the changelog and parsing it. I've written one you can do on ANY machine, regardless of Jenkins by leveraging Git and my own custom binary ( jpb - Jenkins Pipeline Binary). The code for my binary is at GitHub: github.com/joostvdg/jpb . An alternative approach is described by CloudBees Support here , which leverages Jenkins groovy powers. 1 2 3 4 5 6 7 COMMIT_INFO = ${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT} def changeSetData = sh returnStdout: true , script: git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO} changeSetData = changeSetData . replace ( \\n , \\\\n ) container ( jpb ) { changeSetFolders = sh returnStdout: true , script: /usr/bin/jpb/bin/jpb GitChangeListToFolder ${changeSetData} teams/ changeSetFolders = changeSetFolders . split ( , ) } Files recipes.json The default Team Recipes that ships with CloudBees Core Modern. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { version : 1 , data : [{ name : basic , displayName : Basic , description : The minimalistic setup. , plugins : [ bluesteel-master , cloudbees-folders-plus , cloudbees-jsync-archiver , cloudbees-monitoring , cloudbees-nodes-plus , cloudbees-ssh-slaves , cloudbees-support , cloudbees-workflow-template , credentials-binding , email-ext , git , git-client , github-branch-source , github-organization-folder , infradna-backup , ldap , mailer , operations-center-analytics-reporter , operations-center-cloud , pipeline-model-definition , ssh-credentials , wikitext , workflow-aggregator , workflow-cps-checkpoint ], default : true }, { name : java-web , displayName : Java Web Development , description : The essential tools to build, release and deploy Java Web applications including integration with Maven, Gradle and Node JS. , plugins : [ bluesteel-master , cloudbees-folders-plus , cloudbees-jsync-archiver , cloudbees-monitoring , cloudbees-nodes-plus , cloudbees-ssh-slaves , cloudbees-support , cloudbees-workflow-template , credentials-binding , email-ext , git , git-client , github-branch-source , github-organization-folder , infradna-backup , ldap , mailer , operations-center-analytics-reporter , operations-center-cloud , pipeline-model-definition , ssh-credentials , wikitext , workflow-aggregator , workflow-cps-checkpoint , config-file-provider , cloudbees-aws-cli , cloudbees-cloudfoundry-cli , findbugs , gradle , jira , junit , nodejs , openshift-cli , pipeline-maven , tasks , warnings ], default : false }] } Jenkinsfile This is the pipeline that will process the commit to the repository and, if it detects a new team is created will apply the changes. Variables to overwrite: GIT_REPO : the https url to the Git Repository your GitOps code/configuration is stored RESET_NAMESPACE : the namespace your Operation Center normally operates in CLI : this command depends on the namespace Operation Center is in ( http : // service name . namespace / cjoc ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 pipeline { agent { kubernetes { label jenkins-agent yaml apiVersion: v1 kind: Pod spec: serviceAccountName: jenkins containers: - name: cli image: caladreas/cbcore-cli:2.176.2.3 imagePullPolicy: Always command: - cat tty: true resources: requests: memory: 50Mi cpu: 150m limits: memory: 50Mi cpu: 150m - name: kubectl image: bitnami/kubectl:latest command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 150Mi cpu: 200m - name: yq image: mikefarah/yq command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m - name: jpb image: caladreas/jpb command: - cat tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m securityContext: runAsUser: 1000 fsGroup: 1000 } } options { disableConcurrentBuilds () buildDiscarder logRotator ( artifactDaysToKeepStr: , artifactNumToKeepStr: , daysToKeepStr: 5 , numToKeepStr: 5 ) } environment { RESET_NAMESPACE = cloudbees-core CREDS = credentials ( jenkins-api ) CLI = java -jar /usr/bin/jenkins-cli.jar -noKeyAuth -s http://cjoc.cloudbees-core/cjoc -auth COMMIT_INFO = TEAM = GIT_REPO = } stages { stage ( Create Team ) { when { allOf { branch master ; changeset teams/**/team.* } } parallel { stage ( Main ) { stages { stage ( Parse Changelog ) { steps { // Alternative approach: https://support.cloudbees.com/hc/en-us/articles/217630098-How-to-access-Changelogs-in-a-Pipeline-Job- // However, that runs on the master, JPB runs in an agent! script { scmVars = git ${GIT_REPO} COMMIT_INFO = ${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT} def changeSetData = sh returnStdout: true , script: git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO} changeSetData = changeSetData . replace ( \\n , \\\\n ) container ( jpb ) { changeSetFolders = sh returnStdout: true , script: /usr/bin/jpb/bin/jpb GitChangeListToFolder ${changeSetData} teams/ changeSetFolders = changeSetFolders . split ( , ) } if ( changeSetFolders . length 0 ) { TEAM = changeSetFolders [ 0 ] TEAM = TEAM . trim () // to protect against a team being removed def exists = fileExists teams/${TEAM}/team.yaml if (! exists ) { TEAM = } } else { TEAM = } echo Team that changed: |${TEAM}| } } } stage ( Create Namespace ) { when { expression { return ! TEAM . equals ( ) } } environment { NAMESPACE = cb-teams-${TEAM} RECORD_LOC = teams/${TEAM} } steps { container ( kubectl ) { sh cat ${RECORD_LOC}/team.yaml kubectl apply -f ${RECORD_LOC}/team.yaml } } } stage ( Change OC Namespace ) { when { expression { return ! TEAM . equals ( ) } } environment { NAMESPACE = cb-teams-${TEAM} } steps { container ( cli ) { sh echo ${NAMESPACE} script { def response = sh encoding: UTF-8 , label: create team , returnStatus: true , script: ${CLI} ${CREDS} groovy = resources/bootstrap/configure-oc-namespace.groovy ${NAMESPACE} println Response: ${response} } } } } stage ( Create Team Master ) { when { expression { return ! TEAM . equals ( ) } } environment { TEAM_NAME = ${TEAM} } steps { container ( cli ) { println TEAM_NAME=${TEAM_NAME} sh ls -lath sh ls -lath teams/ script { def response = sh encoding: UTF-8 , label: create team , returnStatus: true , script: ${CLI} ${CREDS} teams ${TEAM_NAME} --put teams/${TEAM_NAME}/team.json println Response: ${response} } } } } } } } } stage ( Test CLI Connection ) { steps { container ( cli ) { script { def response = sh encoding: UTF-8 , label: retrieve version , returnStatus: true , script: ${CLI} ${CREDS} version println Response: ${response} } } } } stage ( Update Team Recipes ) { when { allOf { branch master ; changeset recipes/recipes.json } } steps { container ( cli ) { sh ls -lath sh ls -lath recipes/ script { def response = sh encoding: UTF-8 , label: update team recipe , returnStatus: true , script: ${CLI} ${CREDS} team-creation-recipes --put recipes/recipes.json println Response: ${response} } } } } } post { always { container ( cli ) { sh ${CLI} ${CREDS} groovy = resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE} } } } }","title":"CloudBees Automate Teams"},{"location":"cloudbees/teams-automation/#core-modern-teams-automation","text":"CloudBees Core on Modern (meaning, on Kubernetes) has two main types of Jenkins Masters, a Managed Master, and a Team Master. In this article, we're going to automate the creation and management of the Team Masters. Hint If you do not want to read any of the code here, or just want to take a look at the end result - pipeline wise - you can find working examples on GitHub. Template Repository - creates a new team template and a PR to the GitOps repository GitOps Repository - applies GitOps principles to manage the CloudBees Core Team Masters","title":"Core Modern Teams Automation"},{"location":"cloudbees/teams-automation/#goals","text":"Just automating the creation of a Team Master is relatively easy, as this can be done via the Client Jar . So we're going to set some additional goals to create a decent challenge. GitOps : I want to be able to create and delete Team Masters by managing configuration in a Git repository Configuration-as-Code : as much of the configuration as possible should be stored in the Git repository Namespace : one of the major reasons for Team Masters to exist is to increase (Product) Team autonomy, which in Kubernetes should correspond to a namespace . So I want each Team Master to be in its own Namespace! Self-Service : the solution should cater to (semi-)autonomous teams and lower the workload of the team managing CloudBees Core. So requesting a Team Master should be doable by everyone","title":"Goals"},{"location":"cloudbees/teams-automation/#before-we-start","text":"Some assumptions need to be taken care off before we start. Kubernetes cluster in which you are ClusterAdmin if you don't have one yet, there are guides on this elsewhere on the site your cluster has enough capacity (at least two nodes of 4gb memory) your cluster has CloudBees Core Modern installed if you don't have this yet look at one of the guides on this site or look at the guides on CloudBees.com have administrator access to CloudBees Core Cloud Operations Center Code Examples The more extensive Code Examples, such as Kubernetes Yaml files, are collapsed by default. You can open them by clicking on them. On the right, the code snippet will have a [ ] copy icon. Below is an example. Code Snippet Example Here's a code snippet. 1 2 3 4 5 6 7 8 9 10 pipeline { agent any stages { stage ( Hello ) { steps { echo Hello World! } } } }","title":"Before We Start"},{"location":"cloudbees/teams-automation/#bootstrapping","text":"All right, so we want to use GitOps and to process the changes we need a Pipeline that can be triggered by a Webhook. I believe in everything as code - except Secrets and such - which includes the Pipeline. Unfortunately, Operations Center cannot run such pipelines. To get over this hurdle, we will create a special Ops Team Master. This Master will be configured to be able to Manage the other Team Masters for us. Log into your Operations Center with a user that has administrative access.","title":"Bootstrapping"},{"location":"cloudbees/teams-automation/#create-api-token","text":"Create a new API Token for your administrator user by clicking on the user's name - top right corner. Select the Configuration menu on the left and then you should see a section where you can Create a API Token . This Token will disappear, so write it down.","title":"Create API Token"},{"location":"cloudbees/teams-automation/#get-configure-client-jar","text":"Replace the values marked by ... . The Operations Center URL should look like this: http : // cbcore . mydomain . com / cjoc . Setup the connection variables. 1 OC_URL = your operations center url 1 2 USR = your username TKN = api token Download the Client Jar. 1 curl ${ OC_URL } /jnlpJars/jenkins-cli.jar -o jenkins-cli.jar","title":"Get &amp; Configure Client Jar"},{"location":"cloudbees/teams-automation/#create-alias-test","text":"1 alias cboc = java -jar jenkins-cli.jar -noKeyAuth -auth ${ USR } : ${ TKN } -s ${ OC_URL } 1 cboc version","title":"Create Alias &amp; Test"},{"location":"cloudbees/teams-automation/#create-team-ops","text":"As the tasks of the Team Masters for managing Operations are quite specific and demand special rights, I'd recommend putting this in its own namespace . To do so properly, we need to configure a few things. allows Operations Center access to this namespace (so it can create the Team Master) give the ServiceAccount the permissions to create namespace 's for the other Team Masters add config map for the Jenkins Agents temporarily change Operations Center's operating Namespace (where it will spawn resources in) use the CLI to create the team - ops Team Master reset Operations Center's operating Namespace","title":"Create Team Ops"},{"location":"cloudbees/teams-automation/#update-create-kubernetes-namespaces","text":"","title":"Update &amp; Create Kubernetes Namespaces"},{"location":"cloudbees/teams-automation/#create-team-ops-namespace","text":"1 kubectl apply -f team-ops-namespace.yaml team-ops-namespace.yaml This creates the team - ops namespace including all the resources required such as ResourceQuota , ServiceAccount and so on. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 apiVersion : v1 kind : Namespace metadata : name : team-ops --- apiVersion : v1 kind : ResourceQuota metadata : name : resource-quota namespace : team-ops spec : hard : pods : 20 requests.cpu : 4 requests.memory : 6Gi limits.cpu : 5 limits.memory : 10Gi services.loadbalancers : 0 services.nodeports : 0 persistentvolumeclaims : 10 --- apiVersion : v1 kind : ServiceAccount metadata : name : jenkins namespace : team-ops --- kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : pods-all namespace : team-ops rules : - apiGroups : [ ] resources : [ pods ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/exec ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/log ] verbs : [ get , list , watch ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : jenkins namespace : team-ops roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pods-all subjects : - kind : ServiceAccount name : jenkins namespace : team-ops --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : cjoc roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : master-management subjects : - kind : ServiceAccount name : jenkins namespace : team-ops --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : create-namespaces rules : - apiGroups : [ * ] resources : [ serviceaccounts , rolebindings , roles , resourcequotas , namespaces ] verbs : [ create , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ configmaps , rolebindings , roles , resourcequotas , namespaces ] verbs : [ create , get , list ] - apiGroups : [ ] resources : [ events ] verbs : [ get , list , watch ] - apiGroups : [ ] resources : [ persistentvolumeclaims , pods , pods/exec , services , statefulsets , ingresses , extensions ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/log ] verbs : [ get , list , watch ] - apiGroups : [ ] resources : [ secrets ] verbs : [ list ] - apiGroups : [ apps ] resources : [ statefulsets ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ extensions ] resources : [ ingresses ] verbs : [ create , delete , get , list , patch , update , watch ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : ops-namespace roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : create-namespaces subjects : - kind : ServiceAccount name : jenkins namespace : team-ops","title":"Create Team Ops Namespace"},{"location":"cloudbees/teams-automation/#update-operation-center-serviceaccount","text":"The ServiceAccount under which Operation Center runs, only has rights in it's own namespace . Which means it cannot create our Team Ops Master. Below is the .yaml file for Kubernetes and the command to apply it. Warning I assume you're using the default cloudbees - core as per Cloudbees' documentation. If this is not the case, change the last line, namespace : cloudbees - core with the namespace your Operation Center runs in. 1 kubectl apply -f patch-oc-serviceaccount.yaml -n team-ops patch-oc-serviceaccount.yaml This patches the existing Operation Center's ServiceAccount to also have the correct rights in the team - ops namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : master-management rules : - apiGroups : [ ] resources : [ pods ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/exec ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ pods/log ] verbs : [ get , list , watch ] - apiGroups : [ apps ] resources : [ statefulsets ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ services ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ persistentvolumeclaims ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ extensions ] resources : [ ingresses ] verbs : [ create , delete , get , list , patch , update , watch ] - apiGroups : [ ] resources : [ secrets ] verbs : [ list ] - apiGroups : [ ] resources : [ events ] verbs : [ get , list , watch ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : cjoc roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : master-management subjects : - kind : ServiceAccount name : cjoc namespace : cloudbees-core","title":"Update Operation Center ServiceAccount"},{"location":"cloudbees/teams-automation/#jenkins-agent-configmap","text":"1 kubectl apply -f jenkins-agent-config-map.yaml -n team-ops jenkins-agent-config-map.yaml Creates the Jenkins Agent ConfigMap, which contains the information the Jenkins Agent - within a PodTemplate - uses to connect to the Jenkins Master. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 apiVersion : v1 kind : ConfigMap metadata : name : jenkins-agent data : jenkins-agent : | #!/usr/bin/env sh # The MIT License # # Copyright (c) 2015, CloudBees, Inc. # # Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files (the Software ), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED AS IS , WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN # THE SOFTWARE. # Usage jenkins-slave.sh [options] -url http://jenkins [SECRET] [AGENT_NAME] # Optional environment variables : # * JENKINS_TUNNEL : HOST:PORT for a tunnel to route TCP traffic to jenkins host, when jenkins can t be directly accessed over network # * JENKINS_URL : alternate jenkins URL # * JENKINS_SECRET : agent secret, if not set as an argument # * JENKINS_AGENT_NAME : agent name, if not set as an argument if [ $# -eq 1 ]; then # if `docker run` only has one arguments, we assume user is running alternate command like `bash` to inspect the image exec $@ else # if -tunnel is not provided try env vars case $@ in * -tunnel *) ;; *) if [ ! -z $JENKINS_TUNNEL ]; then TUNNEL= -tunnel $JENKINS_TUNNEL fi ;; esac if [ -n $JENKINS_URL ]; then URL= -url $JENKINS_URL fi if [ -n $JENKINS_NAME ]; then JENKINS_AGENT_NAME= $JENKINS_NAME fi if [ -z $JNLP_PROTOCOL_OPTS ]; then echo Warning: JnlpProtocol3 is disabled by default, use JNLP_PROTOCOL_OPTS to alter the behavior JNLP_PROTOCOL_OPTS= -Dorg.jenkinsci.remoting.engine.JnlpProtocol3.disabled=true fi # If both required options are defined, do not pass the parameters OPT_JENKINS_SECRET= if [ -n $JENKINS_SECRET ]; then case $@ in * ${JENKINS_SECRET} *) echo Warning: SECRET is defined twice in command-line arguments and the environment variable ;; *) OPT_JENKINS_SECRET= ${JENKINS_SECRET} ;; esac fi OPT_JENKINS_AGENT_NAME= if [ -n $JENKINS_AGENT_NAME ]; then case $@ in * ${JENKINS_AGENT_NAME} *) echo Warning: AGENT_NAME is defined twice in command-line arguments and the environment variable ;; *) OPT_JENKINS_AGENT_NAME= ${JENKINS_AGENT_NAME} ;; esac fi SLAVE_JAR=/usr/share/jenkins/slave.jar if [ ! -f $SLAVE_JAR ]; then tmpfile=$(mktemp) if hash wget /dev/null 2 1; then wget -O $tmpfile $JENKINS_URL/jnlpJars/slave.jar elif hash curl /dev/null 2 1; then curl -o $tmpfile $JENKINS_URL/jnlpJars/slave.jar else echo Image does not include $SLAVE_JAR and could not find wget or curl to download it return 1 fi SLAVE_JAR=$tmpfile fi #TODO: Handle the case when the command-line and Environment variable contain different values. #It is fine it blows up for now since it should lead to an error anyway. exec java $JAVA_OPTS $JNLP_PROTOCOL_OPTS -cp $SLAVE_JAR hudson.remoting.jnlp.Main -headless $TUNNEL $URL $OPT_JENKINS_SECRET $OPT_JENKINS_AGENT_NAME $@ fi","title":"Jenkins Agent ConfigMap"},{"location":"cloudbees/teams-automation/#create-initial-master","text":"To make it easier to change the namespace if needed, its extracted out from the command. 1 OriginalNamespace = cloudbees-core This script changes the Operations Center's operating namespace , creates a Team Master with the name ops , and then resets the namespace. 1 2 3 cboc groovy = configure-oc-namespace.groovy team-ops cboc teams ops --put team-ops.json cboc groovy = configure-oc-namespace.groovy $OriginalNamespace team-ops.json This json file that describes a team. By default there are three roles defined on a team, TEAM_ADMIN , TEAM_MEMBER , and TEAM_GUEST . Don't forget to change the id 's to Group ID's from your Single-Sign-On solution. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { version : 1 , data : { name : ops , displayName : Operations , provisioningRecipe : basic , members : [{ id : Catmins , roles : [ TEAM_ADMIN ] }, { id : Pirates , roles : [ TEAM_MEMBER ] }, { id : Continental , roles : [ TEAM_GUEST ] } ], icon : { name : hexagons , color : #8d7ec1 } } } configure-oc-namespace.groovy This is a Jenkins Configuration or System Groovy script. It will change the namespace Operation Center uses to create resources. You can change this in the UI by going to Operations Center - Manage Jenkins - System Configuration - Master Provisioning - Namespace . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import hudson.* import hudson.util.Secret ; import hudson.util.Scrambler ; import hudson.util.FormValidation ; import jenkins.* import jenkins.model.* import hudson.security.* import com.cloudbees.masterprovisioning.kubernetes.KubernetesMasterProvisioning import com.cloudbees.masterprovisioning.kubernetes.KubernetesClusterEndpoint println === KubernetesMasterProvisioning Configuration - start println == Retrieving main configuration def descriptor = Jenkins . getInstance (). getInjector (). getInstance ( KubernetesMasterProvisioning . DescriptorImpl . class ) def namespace = this . args [ 0 ] def currentKubernetesClusterEndpoint = descriptor . getClusterEndpoints (). get ( 0 ) println = Found current endpoint println = + currentKubernetesClusterEndpoint . toString () def id = currentKubernetesClusterEndpoint . getId () def name = currentKubernetesClusterEndpoint . getName () def url = currentKubernetesClusterEndpoint . getUrl () def credentialsId = currentKubernetesClusterEndpoint . getCredentialsId () println == Setting Namspace to + namespace def updatedKubernetesClusterEndpoint = new KubernetesClusterEndpoint ( id , name , url , credentialsId , namespace ) def clusterEndpoints = new ArrayList KubernetesClusterEndpoint () clusterEndpoints . add ( updatedKubernetesClusterEndpoint ) descriptor . setClusterEndpoints ( clusterEndpoints ) println == Saving Jenkins configuration descriptor . save () println === KubernetesMasterProvisioning Configuration - finish","title":"Create Initial Master"},{"location":"cloudbees/teams-automation/#configure-team-ops-master","text":"Now that we've created the Operations Team Master (Team Ops), we can configure it. The Pipelines we need will require credentials, we describe them below. githubtoken_token : GitHub API Token only, credentials type Secret Text (for the PR pipeline) githubtoken : GitHub username and API Token jenkins-api : Username and API Token for Operations Center. Just like the one we used for Client Jar. We also need to have a Global Pipeline Library defined by the name github . com / joostvdg / jpl - core . This, as the name suggests, should point to https : // github . com / joostvdg / jpl - core . git .","title":"Configure Team Ops Master"},{"location":"cloudbees/teams-automation/#create-gitops-pipeline","text":"In total, we need two repositories and two or three Pipelines. You can either use my CLI Docker Image or roll your own. I will proceed as if you will create your own. CLI Image Pipeline : this will create a CLI Docker Image that is used to talk to Operations Center via the Client Jar (CLI) PR Pipeline : I like the idea of Self-Service, but in order to keep things in check, you might want to provide that via a PullRequest (PR) rather than a direct write to the Master branch. This is also Repository One, as I prefer having each pipeline in their own Repository, but you don't need to. Main Pipeline : will trigger on a commit to the Master branch and create the new team. I'll even throw in a free manage your Team Recipes for free as well.","title":"Create GitOps Pipeline"},{"location":"cloudbees/teams-automation/#create-cli-image-pipeline","text":"In a Kubernetes cluster, you should not build with Docker directly, use an in-cluster builder such as Kaniko or Buildah . You can read more about the why and how elsewhere on this site . Tip If you do not want to create your own, you can re-use my images. There should be one available for every recent version of CloudBees Core that will work with your Operations Center. The images are available in DockerHub at caladreas/cbcore-cli","title":"Create CLI Image Pipeline"},{"location":"cloudbees/teams-automation/#kaniko-configuration","text":"Kaniko uses a Docker Image to build your Docker Image in cluster. It does however need to directly communicate to your Docker Registry. This requires a Kubernetes Secret of type docker - registry . How you can do this and more, you can read on the CloudBees Core Docs .","title":"Kaniko Configuration"},{"location":"cloudbees/teams-automation/#pipeline","text":"Now that you have Kaniko configured, you can use this Pipeline to create your own CLI Images. Caution Make sure you replace the environment variables with values that make sense to you. CJOC_URL internal url in Kubernets, usually http : // cjoc . namespace / cjoc REGISTRY : index.docker.io = DockerHub REPO : docker repository name IMAGE : docker image name Jenkinsfile Jenkins Declarative Pipeline for the CLI Image geberation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 pipeline { agent { kubernetes { //cloud kubernetes label test yaml kind: Pod metadata: name: test spec: containers: - name: curl image: byrnedo/alpine-curl command: - cat tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: jenkins-docker-cfg projected: sources: - secret: name: docker-credentials items: - key: .dockerconfigjson path: .docker/config.json } } environment { CJOC_URL = http://cjoc.cloudbees-core/cjoc CLI_VERSION = REGISTRY = index.docker.io REPO = caladreas IMAGE = cbcore-cli } stages { stage ( Download CLI ) { steps { container ( curl ) { sh curl --version sh echo ${CJOC_URL}/jnlpJars/jenkins-cli.jar sh curl ${CJOC_URL}/jnlpJars/jenkins-cli.jar --output jenkins-cli.jar sh ls -lath } } } stage ( Prepare ) { parallel { stage ( Verify CLI ) { environment { CREDS = credentials ( jenkins-api ) CLI = java -jar jenkins-cli.jar -noKeyAuth -s ${CJOC_URL} -auth } steps { sh echo ${CLI} script { CLI_VERSION = sh returnStdout: true , script: ${CLI} ${CREDS} version } sh echo ${CLI_VERSION} } } stage ( Prepare Dockerfile ) { steps { writeFile encoding: UTF-8 , file: Dockerfile , text: FROM mcr.microsoft.com/java/jre-headless:8u192-zulu-alpine WORKDIR /usr/bin ADD jenkins-cli.jar . RUN pwd RUN ls -lath } } } } stage ( Build with Kaniko ) { environment { PATH = /busybox:/kaniko:$PATH TAG = ${CLI_VERSION} } steps { sh echo image fqn=${REGISTRY}/${REPO}/${IMAGE}:${TAG} container ( name: kaniko , shell: /busybox/sh ) { sh #!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:${TAG} /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:latest } } } } }","title":"Pipeline"},{"location":"cloudbees/teams-automation/#pr-pipeline","text":"Caution The PR Pipeline example builds upon the GitHub API, if you're not using GItHub, you will have to figure out another way to make the PR.","title":"PR Pipeline"},{"location":"cloudbees/teams-automation/#tools-used","text":"yq : commandline tool for processing Yaml files jq commandline tool for pressing Json files Kustomize templating tool for Kubernetes Yaml, as of Kubernetes 1 . 13 , this is part of the Client (note, your server can be older, don't worry!) Hub commandline client for GitHub","title":"Tools Used"},{"location":"cloudbees/teams-automation/#repository-layout","text":"folder: team - master - template with file simple . json folder: namespace - creation with folder: kustomize this contains the Kustomize configuration Simple.json This is a template for the team JSON definition. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { version : 1 , data : { name : NAME , displayName : DISPLAY_NAME , provisioningRecipe : RECIPE , members : [ { id : ADMINS , roles : [ TEAM_ADMIN ] }, { id : MEMBERS , roles : [ TEAM_MEMBER ] }, { id : GUESTS , roles : [ TEAM_GUEST ] } ], icon : { name : ICON , color : HEX_COLOR } } }","title":"Repository Layout"},{"location":"cloudbees/teams-automation/#kustomize-configuration","text":"Kustomize is a tool for template Kubernetes YAML definitions, which is what we need here. However, only for the namespace creation configuration. So if you don't want to do that, you can skip this. The Kustomize configuration has two parts, a folder called team - example with a kustomization . yaml . This will be what we configure to generate a new yaml definition. The main template is in the folder base , where the entrypoint will be again kustomization . yaml . This time, the kustomization . yaml will link to all the template files we need. As posting all these yaml files again is a bit much, I'll link to my example repo. Feel free to fork it instead: cb-team-gitops-template configmap.yaml : the Jenkins Agent ConfigMap namespace.yaml : the new namespace resource-quota.yaml : resource quota's for the namespace role-binding-cjoc.yaml : a role binding for the CJOC ServiceAccount, so it create create the new Master in the new namespace role-binding.yaml : the role binding for the jenkins ServiceAccount, which allows the new Master to create and manage Pods (for PodTemplates) role-cjoc.yaml : the role for CJOC for the ability to create a Master in the new Namspace role.yaml : the role for the jenkins ServiceAccount for the new Master service-account.yaml : the ServiceAccount, jenkins , used by the new Master","title":"Kustomize Configuration"},{"location":"cloudbees/teams-automation/#pipeline_1","text":"The Pipeline will do the following: capture input parameters to be used to customize the Team Master update the Kustomize template to make sure every resource is correct for the new namespace ( teams - name of team ) execute Kustomize to generate a single yaml file that defines the configuration for the new Team Masters' namespace process the simple . json to generate a team . json file for the new Team Master for use with the Jenkins CLI checkout your GIT_REPO that contains your team definitions create a new PR to your GIT_REPO for the new team Jenkinsfile Variables to update: GIT_REPO : the GitHub repository in which the Team Definitions are stored RESET_NAMESPACE : the namespace Operations Center should use as default 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 pipeline { agent { kubernetes { label team-automation yaml kind: Pod spec: containers: - name: hub image: caladreas/hub command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 150m limits: memory: 50Mi cpu: 150m - name: kubectl image: bitnami/kubectl:latest command: [ cat ] tty: true securityContext: runAsUser: 1000 fsGroup: 1000 resources: requests: memory: 50Mi cpu: 100m limits: memory: 150Mi cpu: 200m - name: yq image: mikefarah/yq command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m - name: jq image: colstrom/jq command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m } } libraries { lib ( github.com/joostvdg/jpl-core ) } options { disableConcurrentBuilds () // we don t want more than one at a time checkoutToSubdirectory templates // we need to do two checkouts buildDiscarder logRotator ( artifactDaysToKeepStr: , artifactNumToKeepStr: , daysToKeepStr: 5 , numToKeepStr: 5 ) // always clean up } environment { envGitInfo = RESET_NAMESPACE = jx-production TEAM_BASE_NAME = NAMESPACE_TO_CREATE = DISPLAY_NAME = TEAM_RECIPE = ICON = ICON_COLOR_CODE = ADMINS_ROLE = MEMBERS_ROLE = GUESTS_ROLE = RECORD_LOC = GIT_REPO = } stages { stage ( Team Details ) { input { message Please enter the team details. ok Looks good, proceed parameters { string ( name: Name , defaultValue: hex , description: Please specify a team name ) string ( name: DisplayName , defaultValue: Hex , description: Please specify a team display name ) choice choices: [ joostvdg , basic , java-web ], description: Please select a Team Recipe , name: TeamRecipe choice choices: [ anchor , bear , bowler-hat , briefcase , bug , calculator , calculatorcart , clock , cloud , cloudbees , connect , dollar-bill , dollar-symbol , file , flag , flower-carnation , flower-daisy , help , hexagon , high-heels , jenkins , key , marker , monocle , mustache , office , panther , paw-print , teacup , tiger , truck ], description: Please select an Icon , name: Icon string ( name: IconColorCode , defaultValue: #CCCCCC , description: Please specify a valid html hexcode for the color (https://htmlcolorcodes.com/) ) string ( name: Admins , defaultValue: Catmins , description: Please specify a groupid or userid for the TEAM_ADMIN role ) string ( name: Members , defaultValue: Pirates , description: Please specify a groupid or userid for the TEAM_MEMBER role ) string ( name: Guests , defaultValue: Continental , description: Please specify a groupid or userid for the TEAM_GUEST role ) } } steps { println Name=${Name} println DisplayName=${DisplayName} println TeamRecipe=${TeamRecipe} println Icon=${Icon} println IconColorCode=${IconColorCode} println Admins=${Admins} println Members=${Members} println Guests=${Guests} script { TEAM_BASE_NAME = ${Name} NAMESPACE_TO_CREATE = cb-teams-${Name} DISPLAY_NAME = ${DisplayName} TEAM_RECIPE = ${TeamRecipe} ICON = ${Icon} ICON_COLOR_CODE = ${IconColorCode} ADMINS_ROLE = ${Admins} MEMBERS_ROLE = ${Members} GUESTS_ROLE = ${Guests} RECORD_LOC = templates/teams/${Name} sh mkdir -p ${RECORD_LOC} } } } stage ( Create Team Config ) { environment { BASE = templates/namespace-creation/kustomize NAMESPACE = ${NAMESPACE_TO_CREATE} RECORD_LOC = templates/teams/${TEAM_BASE_NAME} } parallel { stage ( Namespace ) { steps { container ( yq ) { sh yq w -i ${BASE}/base/role-binding.yaml subjects[0].namespace ${NAMESPACE} sh yq w -i ${BASE}/base/namespace.yaml metadata.name ${NAMESPACE} sh yq w -i ${BASE}/team-example/kustomization.yaml namespace ${NAMESPACE} } container ( kubectl ) { sh kubectl kustomize ${BASE}/team-example ${RECORD_LOC}/team.yaml cat ${RECORD_LOC}/team.yaml } } } stage ( Team Master JSON ) { steps { container ( jq ) { sh jq \\ .data.name = ${TEAM_BASE_NAME} |\\ .data.displayName = ${DISPLAY_NAME} |\\ .data.provisioningRecipe = ${TEAM_RECIPE} |\\ .data.icon.name = ${ICON} |\\ .data.icon.color = ${ICON_COLOR_CODE} |\\ .data.members[0].id = ${ADMINS_ROLE} |\\ .data.members[1].id = ${MEMBERS_ROLE} |\\ .data.members[2].id = ${GUESTS_ROLE} \\ templates/team-master-template/simple.json ${RECORD_LOC}/team.json } sh cat ${RECORD_LOC}/team.json } } } } stage ( Create PR ) { when { branch master } environment { RECORD_OLD_LOC = templates/teams/${TEAM_BASE_NAME} RECORD_LOC = teams/${TEAM_BASE_NAME} PR_CHANGE_NAME = add_team_${TEAM_BASE_NAME} } steps { container ( hub ) { dir ( cb-team-gitops ) { script { envGitInfo = git ${GIT_REPO} } sh git checkout -b ${PR_CHANGE_NAME} sh ls -lath ../${RECORD_OLD_LOC} sh cp -R ../${RECORD_OLD_LOC} ./teams sh ls -lath sh ls -lath teams/ gitRemoteConfigByUrl ( envGitInfo . GIT_URL , githubtoken_token ) // must be a API Token ONLY - secret text sh git config --global user.email jenkins@jenkins.io git config --global user.name Jenkins git add ${RECORD_LOC} git status git commit -m add team ${TEAM_BASE_NAME} git push origin ${PR_CHANGE_NAME} // has to be indented like that, else the indents will be in the pr description writeFile encoding: UTF-8 , file: pr-info.md , text: Add ${TEAM_BASE_NAME} \\n This pr is automatically generated via CloudBees.\\\\n \\n The job: ${env.JOB_URL} // TODO: unfortunately, environment {} s credentials have fixed environment variable names // TODO: in this case, they need to be EXACTLY GITHUB_PASSWORD and GITHUB_USER script { withCredentials ([ usernamePassword ( credentialsId: githubtoken , passwordVariable: GITHUB_PASSWORD , usernameVariable: GITHUB_USER )]) { sh set +x hub pull-request --force -F pr-info.md -l ${TEAM_BASE_NAME} --no-edit } } } } } } } }","title":"Pipeline"},{"location":"cloudbees/teams-automation/#main-pipeline","text":"The main Pipeline should be part of a repository. The Repository should look like this: recipes (folder) recipes . json - current complete list of CloudBees Core Team Recipes definition teams (folder) folder per team team . json - CloudBees Core Team definition team . yaml - Kubernetes YAML definition of the namespace and all its resources","title":"Main Pipeline"},{"location":"cloudbees/teams-automation/#process","text":"The pipeline can be a bit hard to grasp, so let me break it down into individual steps. We have the following stages: Create Team - which is broken into sub-stages via the sequential stages feature . * Parse Changelog * Create Namespace * Change OC Namespace * Create Team Master Test CLI Connection Update Team Recipes","title":"Process"},{"location":"cloudbees/teams-automation/#notable-statements","text":"disableConcurrentBuilds We change the namespace of Operation Center to a different value only for the duration of creating this master. This is something that should probably be part of the Team Master creation, but as it is a single configuration option for all that Operation Center does, we need to be careful. By ensuring we only run one build concurrently, we reduce the risk of this blowing up in our face. 1 2 3 options { disableConcurrentBuilds () } when { } The When Directive allows us to creating effective conditions for when a stage should be executed. The snippet below shows the use of a combination of both the branch and changeset built-in filters. changeset looks at the commit being build and validates that there was a change in that file path. 1 when { allOf { branch master ; changeset teams/**/team.* } } post { always { } } The Post Directive allows us to run certain commands after the main pipeline has run depending on the outcome (compared or not to the previous outcome). In this case, we want to make sure we reset the namespace used by Operations Center to the original value. By using post { always {} } , it will ALWAYS run, regardless of the status of the pipeline. So we should be safe. 1 2 3 4 5 6 7 post { always { container ( cli ) { sh ${CLI} ${CREDS} groovy = resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE} } } } stages { stage { parallel { stage() { stages { stage { Oke, you might've noticed this massive indenting depth and probably have some questions. By combining sequential stages with parallel stages we can create a set of stages that will be executed in sequence but can be controlled by a single when {} statement whether or not they get executed. This prevents mistakes being made in the condition and accidentally running one or other but not all the required steps. 1 2 3 4 5 6 stages { stage ( Create Team ) { parallel { stage ( Main ) { stages { stage ( Parse Changelog ) { changetSetData container('jpb') {} Alright, so even if we know a team was added in / teams / team - name , we still don't know the following two things: 1) what is the name of this team, 2) was this team changed or deleted? So we have to process the changelog to be able to answer these questions as well. There are different ways of getting the changelog and parsing it. I've written one you can do on ANY machine, regardless of Jenkins by leveraging Git and my own custom binary ( jpb - Jenkins Pipeline Binary). The code for my binary is at GitHub: github.com/joostvdg/jpb . An alternative approach is described by CloudBees Support here , which leverages Jenkins groovy powers. 1 2 3 4 5 6 7 COMMIT_INFO = ${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT} def changeSetData = sh returnStdout: true , script: git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO} changeSetData = changeSetData . replace ( \\n , \\\\n ) container ( jpb ) { changeSetFolders = sh returnStdout: true , script: /usr/bin/jpb/bin/jpb GitChangeListToFolder ${changeSetData} teams/ changeSetFolders = changeSetFolders . split ( , ) }","title":"Notable Statements"},{"location":"cloudbees/teams-automation/#files","text":"recipes.json The default Team Recipes that ships with CloudBees Core Modern. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { version : 1 , data : [{ name : basic , displayName : Basic , description : The minimalistic setup. , plugins : [ bluesteel-master , cloudbees-folders-plus , cloudbees-jsync-archiver , cloudbees-monitoring , cloudbees-nodes-plus , cloudbees-ssh-slaves , cloudbees-support , cloudbees-workflow-template , credentials-binding , email-ext , git , git-client , github-branch-source , github-organization-folder , infradna-backup , ldap , mailer , operations-center-analytics-reporter , operations-center-cloud , pipeline-model-definition , ssh-credentials , wikitext , workflow-aggregator , workflow-cps-checkpoint ], default : true }, { name : java-web , displayName : Java Web Development , description : The essential tools to build, release and deploy Java Web applications including integration with Maven, Gradle and Node JS. , plugins : [ bluesteel-master , cloudbees-folders-plus , cloudbees-jsync-archiver , cloudbees-monitoring , cloudbees-nodes-plus , cloudbees-ssh-slaves , cloudbees-support , cloudbees-workflow-template , credentials-binding , email-ext , git , git-client , github-branch-source , github-organization-folder , infradna-backup , ldap , mailer , operations-center-analytics-reporter , operations-center-cloud , pipeline-model-definition , ssh-credentials , wikitext , workflow-aggregator , workflow-cps-checkpoint , config-file-provider , cloudbees-aws-cli , cloudbees-cloudfoundry-cli , findbugs , gradle , jira , junit , nodejs , openshift-cli , pipeline-maven , tasks , warnings ], default : false }] } Jenkinsfile This is the pipeline that will process the commit to the repository and, if it detects a new team is created will apply the changes. Variables to overwrite: GIT_REPO : the https url to the Git Repository your GitOps code/configuration is stored RESET_NAMESPACE : the namespace your Operation Center normally operates in CLI : this command depends on the namespace Operation Center is in ( http : // service name . namespace / cjoc ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 pipeline { agent { kubernetes { label jenkins-agent yaml apiVersion: v1 kind: Pod spec: serviceAccountName: jenkins containers: - name: cli image: caladreas/cbcore-cli:2.176.2.3 imagePullPolicy: Always command: - cat tty: true resources: requests: memory: 50Mi cpu: 150m limits: memory: 50Mi cpu: 150m - name: kubectl image: bitnami/kubectl:latest command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 150Mi cpu: 200m - name: yq image: mikefarah/yq command: [ cat ] tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m - name: jpb image: caladreas/jpb command: - cat tty: true resources: requests: memory: 50Mi cpu: 100m limits: memory: 50Mi cpu: 100m securityContext: runAsUser: 1000 fsGroup: 1000 } } options { disableConcurrentBuilds () buildDiscarder logRotator ( artifactDaysToKeepStr: , artifactNumToKeepStr: , daysToKeepStr: 5 , numToKeepStr: 5 ) } environment { RESET_NAMESPACE = cloudbees-core CREDS = credentials ( jenkins-api ) CLI = java -jar /usr/bin/jenkins-cli.jar -noKeyAuth -s http://cjoc.cloudbees-core/cjoc -auth COMMIT_INFO = TEAM = GIT_REPO = } stages { stage ( Create Team ) { when { allOf { branch master ; changeset teams/**/team.* } } parallel { stage ( Main ) { stages { stage ( Parse Changelog ) { steps { // Alternative approach: https://support.cloudbees.com/hc/en-us/articles/217630098-How-to-access-Changelogs-in-a-Pipeline-Job- // However, that runs on the master, JPB runs in an agent! script { scmVars = git ${GIT_REPO} COMMIT_INFO = ${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT} def changeSetData = sh returnStdout: true , script: git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO} changeSetData = changeSetData . replace ( \\n , \\\\n ) container ( jpb ) { changeSetFolders = sh returnStdout: true , script: /usr/bin/jpb/bin/jpb GitChangeListToFolder ${changeSetData} teams/ changeSetFolders = changeSetFolders . split ( , ) } if ( changeSetFolders . length 0 ) { TEAM = changeSetFolders [ 0 ] TEAM = TEAM . trim () // to protect against a team being removed def exists = fileExists teams/${TEAM}/team.yaml if (! exists ) { TEAM = } } else { TEAM = } echo Team that changed: |${TEAM}| } } } stage ( Create Namespace ) { when { expression { return ! TEAM . equals ( ) } } environment { NAMESPACE = cb-teams-${TEAM} RECORD_LOC = teams/${TEAM} } steps { container ( kubectl ) { sh cat ${RECORD_LOC}/team.yaml kubectl apply -f ${RECORD_LOC}/team.yaml } } } stage ( Change OC Namespace ) { when { expression { return ! TEAM . equals ( ) } } environment { NAMESPACE = cb-teams-${TEAM} } steps { container ( cli ) { sh echo ${NAMESPACE} script { def response = sh encoding: UTF-8 , label: create team , returnStatus: true , script: ${CLI} ${CREDS} groovy = resources/bootstrap/configure-oc-namespace.groovy ${NAMESPACE} println Response: ${response} } } } } stage ( Create Team Master ) { when { expression { return ! TEAM . equals ( ) } } environment { TEAM_NAME = ${TEAM} } steps { container ( cli ) { println TEAM_NAME=${TEAM_NAME} sh ls -lath sh ls -lath teams/ script { def response = sh encoding: UTF-8 , label: create team , returnStatus: true , script: ${CLI} ${CREDS} teams ${TEAM_NAME} --put teams/${TEAM_NAME}/team.json println Response: ${response} } } } } } } } } stage ( Test CLI Connection ) { steps { container ( cli ) { script { def response = sh encoding: UTF-8 , label: retrieve version , returnStatus: true , script: ${CLI} ${CREDS} version println Response: ${response} } } } } stage ( Update Team Recipes ) { when { allOf { branch master ; changeset recipes/recipes.json } } steps { container ( cli ) { sh ls -lath sh ls -lath recipes/ script { def response = sh encoding: UTF-8 , label: update team recipe , returnStatus: true , script: ${CLI} ${CREDS} team-creation-recipes --put recipes/recipes.json println Response: ${response} } } } } } post { always { container ( cli ) { sh ${CLI} ${CREDS} groovy = resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE} } } } }","title":"Files"},{"location":"devops/","text":"DevOps Assessment How can I assess an organisation for what to do next. Questions To which extent can your development teams request/create an environment on their own, without going through lengthy approval processes? To which extent can your development teams use pre-configured/ template tool sets (e.g. Jenkins master jobs, master POM etc) which they can extend and/or modify to their needs? To which extent can your developments teams deploy to any environment (including production)? If not, what do they lack: knowledge or passwords to higher environments? Does your system of record provide you tractability from idea to production? How tightly coupled are your key delivery pipeline tools? Is it easy to replace them? Do you have different release management activities based on application blocks? Who is keeping it up-to-date? Maturity Model Resources https://www.devon.nl/continuous-delivery-at-enterprise-level-pitfall-1/ https://www.devon.nl/continuous-delivery-at-enterprise-level-pitfall-2/ https://devops-research.com/ References","title":"DevOps Assessment"},{"location":"devops/#devops-assessment","text":"How can I assess an organisation for what to do next.","title":"DevOps Assessment"},{"location":"devops/#questions","text":"To which extent can your development teams request/create an environment on their own, without going through lengthy approval processes? To which extent can your development teams use pre-configured/ template tool sets (e.g. Jenkins master jobs, master POM etc) which they can extend and/or modify to their needs? To which extent can your developments teams deploy to any environment (including production)? If not, what do they lack: knowledge or passwords to higher environments? Does your system of record provide you tractability from idea to production? How tightly coupled are your key delivery pipeline tools? Is it easy to replace them? Do you have different release management activities based on application blocks? Who is keeping it up-to-date?","title":"Questions"},{"location":"devops/#maturity-model","text":"","title":"Maturity Model"},{"location":"devops/#resources","text":"https://www.devon.nl/continuous-delivery-at-enterprise-level-pitfall-1/ https://www.devon.nl/continuous-delivery-at-enterprise-level-pitfall-2/ https://devops-research.com/","title":"Resources"},{"location":"devops/#references","text":"","title":"References"},{"location":"devops/pipeline-model/","text":"Pipeline Modal Ideas Continuous Integration Continuous Delivery Continuous Deployment Progressive Delivery Continuous, Iterative, Cyclic Small cycle: local dev, CI goal: get it to work Large cycle: CD - CDP - ProgDY goal: deliver value to customers DevOps - single team managing both Small and Large cycle Generic/Shared Concepts Jenkins Tekton Jenkins X Flow Concepts Pipeline Stage Task Pipeline Pipeline Run Entry Gate Exit Gate Other Server Application Resource Release Environment Application Application Deployment Microservice (seems tied to containers?) Microservice Deployment Production Bill-of-Materials Payload Process Approval Version Manifest Components Self-Service Catalog","title":"Pipeline Modal"},{"location":"devops/pipeline-model/#pipeline-modal","text":"","title":"Pipeline Modal"},{"location":"devops/pipeline-model/#ideas","text":"Continuous Integration Continuous Delivery Continuous Deployment Progressive Delivery Continuous, Iterative, Cyclic Small cycle: local dev, CI goal: get it to work Large cycle: CD - CDP - ProgDY goal: deliver value to customers DevOps - single team managing both Small and Large cycle","title":"Ideas"},{"location":"devops/pipeline-model/#genericshared-concepts","text":"","title":"Generic/Shared Concepts"},{"location":"devops/pipeline-model/#jenkins","text":"","title":"Jenkins"},{"location":"devops/pipeline-model/#tekton","text":"","title":"Tekton"},{"location":"devops/pipeline-model/#jenkins-x","text":"","title":"Jenkins X"},{"location":"devops/pipeline-model/#flow","text":"","title":"Flow"},{"location":"devops/pipeline-model/#concepts","text":"Pipeline Stage Task Pipeline Pipeline Run Entry Gate Exit Gate Other Server Application Resource Release Environment Application Application Deployment Microservice (seems tied to containers?) Microservice Deployment Production Bill-of-Materials Payload Process Approval Version Manifest Components Self-Service Catalog","title":"Concepts"},{"location":"devops/progressive-delivery/","text":"Progressive Delivery Resources https://redmonk.com/jgovernor/2018/08/06/towards-progressive-delivery/ https://chrisshort.tumblr.com/post/176701070950/recommended-read-towards-progressive-delivery https://dzone.com/articles/gitops-workflows-for-istio-canary-deployments https://launchdarkly.com/blog/progressive-delivery-a-history-condensed/ https://tech.target.com/infrastructure/2018/06/20/enter-unimatrix.html https://dzone.com/articles/deployments-at-scale-using-kubernetes-and-launchda https://blog.csanchez.org/2019/01/22/progressive-delivery-in-kubernetes-blue-green-and-canary-deployments/ https://blog.csanchez.org/2019/01/24/progressive-delivery-with-jenkins-x/ https://blog.csanchez.org/2019/03/05/progressive-delivery-with-jenkins-x-automatic-canary-deployments/ https://devblogs.microsoft.com/devops/configuring-your-release-pipelines-for-safe-deployments/ https://www.linkedin.com/pulse/counting-down-zero-time-takes-launch-app-target-tom-kadlec-1/","title":"Progressive Delivery"},{"location":"devops/progressive-delivery/#progressive-delivery","text":"","title":"Progressive Delivery"},{"location":"devops/progressive-delivery/#resources","text":"https://redmonk.com/jgovernor/2018/08/06/towards-progressive-delivery/ https://chrisshort.tumblr.com/post/176701070950/recommended-read-towards-progressive-delivery https://dzone.com/articles/gitops-workflows-for-istio-canary-deployments https://launchdarkly.com/blog/progressive-delivery-a-history-condensed/ https://tech.target.com/infrastructure/2018/06/20/enter-unimatrix.html https://dzone.com/articles/deployments-at-scale-using-kubernetes-and-launchda https://blog.csanchez.org/2019/01/22/progressive-delivery-in-kubernetes-blue-green-and-canary-deployments/ https://blog.csanchez.org/2019/01/24/progressive-delivery-with-jenkins-x/ https://blog.csanchez.org/2019/03/05/progressive-delivery-with-jenkins-x-automatic-canary-deployments/ https://devblogs.microsoft.com/devops/configuring-your-release-pipelines-for-safe-deployments/ https://www.linkedin.com/pulse/counting-down-zero-time-takes-launch-app-target-tom-kadlec-1/","title":"Resources"},{"location":"devops/sdm/","text":"Software Development Management Related Concepts Agile scrum safe kanban xp DevOps Lean Systems Thinking Observability SRE EmpathyOps DevSecOps InfoSec Pets vs. Cattle Value Streams Automation Self-service 2-Pizza Teams Spotify Model Feature Flags Development Workflows Trunk Based Development TDD / BDD DDD Testing Automated Exploratory Functional Penetration Static/Dynamic Code Analysis Policies Unit Blacbox ... Development Paradigms Object Oriented Functional Procedural Imperative Declarative Wardley Maps Undifferentiated Heavy Lifting Layers Of Abstraction Models Three-Tier-Model MVC Enterprise Architecture Patterns Evolutionary Architecture Synchronous Asynchronous Or Reactive Streaming CQRS Event Sourcing Premature Optimization Theory of Constraints Segregation Of Duties Feedback Cycles Center Of Excellence Developer Productivity Teams Shared Services Product vs Project Design Thinking Systems Thinking 1-2-Many Evaluation Techniques Blameless Postmortem Retrospective The Five Why's Root Cause Analysis Behavioral Economics Anatomy Of A Toolchain Essential Processes versioning dependency management application tracking release deployment testing point of testing level of testing goal of testing publish Adoption Top Buys Bottom Adopts Workflow Cycles The Small Cycle design build test refactor document publish evaluate Resources https://info.container-solutions.com/info.container-solutions.com/understanding-cloud-native-cs-google-events-thankyou-1 https://unixism.net/2019/08/a-managers-guide-to-kubernetes-adoption/?utm_source=DevOps%27ish utm_campaign=8bd4b37eb5-DEVOPSISH_145 utm_medium=email utm_term=0_eab566bc9f-8bd4b37eb5-46458919 https://www.allthingsdistributed.com/2019/08/modern-applications-at-aws.html https://blog.gitprime.com/individual-contributor-to-manager-julie-zhuo/ https://martinfowler.com/articles/cd4ml.html https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started","title":"Software Development Management"},{"location":"devops/sdm/#software-development-management","text":"","title":"Software Development Management"},{"location":"devops/sdm/#related-concepts","text":"Agile scrum safe kanban xp DevOps Lean Systems Thinking Observability SRE EmpathyOps DevSecOps InfoSec Pets vs. Cattle Value Streams Automation Self-service 2-Pizza Teams Spotify Model Feature Flags Development Workflows Trunk Based Development TDD / BDD DDD Testing Automated Exploratory Functional Penetration Static/Dynamic Code Analysis Policies Unit Blacbox ... Development Paradigms Object Oriented Functional Procedural Imperative Declarative Wardley Maps Undifferentiated Heavy Lifting Layers Of Abstraction Models Three-Tier-Model MVC Enterprise Architecture Patterns Evolutionary Architecture Synchronous Asynchronous Or Reactive Streaming CQRS Event Sourcing Premature Optimization Theory of Constraints Segregation Of Duties Feedback Cycles Center Of Excellence Developer Productivity Teams Shared Services Product vs Project Design Thinking Systems Thinking 1-2-Many Evaluation Techniques Blameless Postmortem Retrospective The Five Why's Root Cause Analysis Behavioral Economics","title":"Related Concepts"},{"location":"devops/sdm/#anatomy-of-a-toolchain","text":"","title":"Anatomy Of A Toolchain"},{"location":"devops/sdm/#essential-processes","text":"versioning dependency management application tracking release deployment testing point of testing level of testing goal of testing publish","title":"Essential Processes"},{"location":"devops/sdm/#adoption","text":"Top Buys Bottom Adopts","title":"Adoption"},{"location":"devops/sdm/#workflow-cycles","text":"","title":"Workflow Cycles"},{"location":"devops/sdm/#the-small-cycle","text":"design build test refactor document publish evaluate","title":"The Small Cycle"},{"location":"devops/sdm/#resources","text":"https://info.container-solutions.com/info.container-solutions.com/understanding-cloud-native-cs-google-events-thankyou-1 https://unixism.net/2019/08/a-managers-guide-to-kubernetes-adoption/?utm_source=DevOps%27ish utm_campaign=8bd4b37eb5-DEVOPSISH_145 utm_medium=email utm_term=0_eab566bc9f-8bd4b37eb5-46458919 https://www.allthingsdistributed.com/2019/08/modern-applications-at-aws.html https://blog.gitprime.com/individual-contributor-to-manager-julie-zhuo/ https://martinfowler.com/articles/cd4ml.html https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started","title":"Resources"},{"location":"docker/build-kit/","text":"Docker Build with Build-Kit Instead of investing in improving docker image building via the Docker Client, Docker created a new API and client library. This library called BuildKit, is completely independent. With Docker 18.09, it is included in the Docker Client allowing anyone to use it as easily as the traditional docker image build . BuildKit is already used by some other tools, such as Buildah and IMG, and allows you to create custom DSL \"Frontends\". As long as the API of BuikdKit is adhered to, the resulting image will be OCI compliant. How To Use It So further remarks below and how to use it. BuildKit In-Depth session Supercharged Docker Build with BuildKit Usable from Docker 18 . 09 HighLights: allows custom DSL for specifying image (BuildKit) to still be used with Docker client/daemon build cache for your own files during build, think Go, Maven, Gradle... much more optimized, builds less, quicker, with more cache in less time support mounts (cache) such as secrets, during build phase 1 2 3 # Set env variable to enable # Or configure docker s json config export DOCKER_BUILDKIT = 1 Example ```dockerfile syntax=docker/dockerfile:experimental 1. BUILD JAR WITH MAVEN FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src ! RUN --mount=type=cache,target=/root/.m2/ mvn clean package -e 2. BUILD NATIVE IMAGE WITH GRAAL FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from=BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath 3. BUILD DOCKER RUNTIME IMAGE FROM alpine:3.8 CMD [\"jpc-graal\"] COPY --from=NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal","title":"BuildKit"},{"location":"docker/build-kit/#docker-build-with-build-kit","text":"Instead of investing in improving docker image building via the Docker Client, Docker created a new API and client library. This library called BuildKit, is completely independent. With Docker 18.09, it is included in the Docker Client allowing anyone to use it as easily as the traditional docker image build . BuildKit is already used by some other tools, such as Buildah and IMG, and allows you to create custom DSL \"Frontends\". As long as the API of BuikdKit is adhered to, the resulting image will be OCI compliant.","title":"Docker Build with Build-Kit"},{"location":"docker/build-kit/#how-to-use-it","text":"So further remarks below and how to use it. BuildKit In-Depth session Supercharged Docker Build with BuildKit Usable from Docker 18 . 09 HighLights: allows custom DSL for specifying image (BuildKit) to still be used with Docker client/daemon build cache for your own files during build, think Go, Maven, Gradle... much more optimized, builds less, quicker, with more cache in less time support mounts (cache) such as secrets, during build phase 1 2 3 # Set env variable to enable # Or configure docker s json config export DOCKER_BUILDKIT = 1","title":"How To Use It"},{"location":"docker/build-kit/#example","text":"```dockerfile","title":"Example"},{"location":"docker/build-kit/#syntaxdockerdockerfileexperimental","text":"","title":"syntax=docker/dockerfile:experimental"},{"location":"docker/build-kit/#1-build-jar-with-maven","text":"FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src ! RUN --mount=type=cache,target=/root/.m2/ mvn clean package -e","title":"1. BUILD JAR WITH MAVEN"},{"location":"docker/build-kit/#2-build-native-image-with-graal","text":"FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from=BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath","title":"2. BUILD NATIVE IMAGE WITH GRAAL"},{"location":"docker/build-kit/#3-build-docker-runtime-image","text":"FROM alpine:3.8 CMD [\"jpc-graal\"] COPY --from=NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal","title":"3. BUILD DOCKER RUNTIME IMAGE"},{"location":"docker/graceful-shutdown/","text":"Gracefully Shutting Down Applications in Docker I'm not sure about you, but I prefer it when my neighbors leave our shared spaces clean and don't take up parking spaces when they don't need them. Imagine you live in an apartment complex with the above-mentioned parking lot. Some tenants go away and never come back. If nothing is done to clean up after them - to reclaim their apartment and parking space - then after some time, more and more apartments are unavailable for no reason, and the parking lot fills with cars which belong to no one. Some tenants did not get a parking lot and are getting frustrated that none are becoming available. When they moved in, they were told that when others leave, they would be next in line. While they're waiting, they have to park outside the complex. Eventually, the entrance gets blocked and no one can enter or leave. The end result is a completely unlivable apartment block with trapped tenants - never to be seen or heard from again. If you agree with me that when a tenant leaves, they should clean the apartment and free the parking spot to make it ready for the next inhabitant; then please read on. We're going to dive into the equivalent of doing this with containers. We will explore running our containers with Docker (run, compose, swarm) and Kubernetes. Even if you use another way to run your containers, this article should provide you with enough insight to get you on your way. The case for graceful shutdown We're in an age where many applications are running in Docker containers across a multitude of clusters. These applications are then confronted with new concerns to tackle such as more moving parts, networking between these parts, remote storage and others. One significant way we defend ourselves against the perils of this distributed nature is to make our applications more robust - able to survive errors. However, even then there is still no guarantee your application is always up and running. So another concern we should tackle is how it responds when it needs to shut down. Where we can differentiate between an unexpected shutdown - we crashed - or an expected shutdown. On top of that, failing instead of trying to recover when something bad happens also adheres to \"fail fast\" - as strongly advocated by Michael Nygard in ReleaseIt. Shutting down can happen for a variety of reasons, in this post we dive into how to deal with an expected shutdown such as it being told to stop by an orchestrator such as Kubernetes. Containers can be purposefully shut down for a variety of reasons, including but not limited too: your application's health check fails your application consumed more resources than allowed the application is scaling down Just as cleaning up when leaving makes you a better tenant, having your application clean up connections, resources Moreover, the more tenants behaving in a good way increases the quality of living for all tenants. In our case, it improves the reliability and consistency of our cluster. Graceful shutdown is not unique to Docker, as it has been part of Linux's best practices for quite some years before Docker's existence. However, applying them to Docker container adds extra dimensions. Start Good So You Can End Well When you sign up for an apartment, you probably have to sign a contract detailing your rights and obligations. The more you state explicitly, the easier it is to deal with bad behaving neighbors. The same is true when running processes; we should make sure that we set the rules, obligations, and expectations from the start. As we say in Dutch: a good beginning is half the work. We will start with how you can run a process in a container with a process that shuts down gracefully. There are many ways to start a process in a container. In this article, we look at processes started by commands defined in a Dockerfile. There are two ways to specify this: CMD : runs a command when the container gets started ENTRYPOINT : provides the location (entrypoint) from where commands get run when the container starts You need at least one ENTRYPOINT or CMD in a Dockerfile for it to be valid. They can be used in collaboration but they can do similar things. For more information on the details of these commands, read Docker's docs on Entrypoint vs. CMD . Docker Shell form example We start with the shell form and see if it can do what we want; begin in such a way, we can stop it nicely. Shell form means we define a shell command without any special format or keywords. Please create Dockerfile with the content that follows. 1 2 FROM ubuntu:18.04 ENTRYPOINT top -b Then build an image and run a container. 1 2 docker image build --tag shell-form . docker run --name shell-form --rm shell-form The above command yields the following output. 1 2 3 4 5 6 7 8 9 top - 16 :34:56 up 1 day, 5 :15, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 541984 free, 302668 used, 1202280 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1579380 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 4624 760 696 S 0 .0 0 .0 0 :00.05 sh 6 root 20 0 36480 2928 2580 R 0 .0 0 .1 0 :00.01 top As you can see, two processes are running, sh and top . Meaning, that killing the process, with ctrl+c for example, terminates the sh process, but not top . This happens because the sh process forked the top process, but the termination will only be send to PID 1 - in this case sh . As sh will not stop the top process for us it will continue running and leave the container alive. To kill this container, open a second terminal and execute the following command. 1 docker rm -f shell-form Shell form doesn't do what we need. Starting a process with shell form will only lead us to the disaster of parking lots filling up unless there's a someone actively cleaning up. Docker exec form example This leads us to the exec form. Hopefully, this gets us somewhere. The exec form is written as an array of parameters: ENTRYPOINT [ top , -b ] To continue in the same line of examples, we will create a Dockerfile, build and run it. 1 2 FROM ubuntu:18.04 ENTRYPOINT [ top , -b ] Then build and run it. 1 2 docker image build --tag exec-form . docker run --name exec-form --rm exec-form This yields the following output. 1 2 3 4 5 6 7 8 top - 18 :12:30 up 1 day, 6 :53, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 535896 free, 307196 used, 1203840 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1574880 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 36480 2940 2584 R 0 .0 0 .1 0 :00.03 top Now we got something we can work with. If something would tell this Container to stop, it will tell our only running process so it is sure to reach the correct one! Gotchas Knowing we can use the exec form for our goal - gracefully shutting down our container - we can move on to the next part of our efforts. For the sake of imparting you with some hard learned lessons, we will explore two gotchas. They're optional, so you can also choose to skip to Make Sure Your Process Listens . Docker exec form with parameters A caveat with the exec form is that it doesn't interpolate parameters. You can try the following: 1 2 3 FROM ubuntu:18.04 ENV PARAM = -b ENTRYPOINT [ top , ${PARAM} ] Then build and run it: 1 2 docker image build --tag exec-param . docker run --name exec-form --rm exec-param This should yield the following: 1 /bin/sh: 1 : [ top: not found This is where Docker created a mix between the two styles. It allows you to create an Entrypoint with a shell command - performing interpolation - but executing it as an exec form. This can be done by prefixing the shell form, with, you guessed it, exec . 1 2 3 FROM ubuntu:18.04 ENV PARAM = -b ENTRYPOINT exec top ${ PARAM } Then build and run it: 1 2 docker image build --tag exec-param . docker run --name exec-form --rm exec-param This will return the exact same as if we would've run ENTRYPOINT [ top , -b ] . Now you can also override the param, by using the environment variable flag. 1 2 docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = help exec-param Resulting in top's help string. The special case of Alpine One of the main best practices for Dockerfiles , is to make them as small as possible. The easiest way to do this is to start with a minimal image. This is where Alpine Linux comes in. We will revisit out shell form example, but replace ubuntu with alpine. Create the following Dockerfile. 1 2 FROM alpine:3.8 ENTRYPOINT top -b Then build and run it. 1 2 docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = help exec-param This yields the following output. 1 2 3 4 5 Mem: 1509068K used, 537864K free, 640K shrd, 126756K buff, 1012436K cached CPU: 0 % usr 0 % sys 0 % nic 100 % idle 0 % io 0 % irq 0 % sirq Load average: 0 .00 0 .00 0 .00 2 /404 5 PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND 1 0 root R 1516 0 % 0 0 % top -b Aside from top 's output looking a bit different, there is only one command. Alpine Linux helps us avoid the problem of shell form altogether! Make Sure Your Process Listens It is excellent if your tenants are all signed up, know their rights and obligations. But you can't contact them when something happens, how will they ever know when to act? Translating that into our process. It starts and can be told to shut down, but does it process listen? Can it interpret the message it gets from Docker or Kubernetes? And if it does, can it relay the message correctly to its Child Processes? In order for your process to gracefully shutdown, it should know when to do so. As such, it should listen not only for itself but also on behalf of its children - yours never do anything wrong though! Some processes do, but many aren't designed to listen or tell their Children . They expect someone else to listen for them and tell them and their children - process managers. In order to listen to these signals , we can call in the help of others. We will look at two options. we let Docker manage the process and its children we use a process manager Let Docker manage it for us If you're not using Docker to run or manage your containers, you should skip to Depend on a process manager . Docker has a build in feature, that it uses a lightweight process manager to help you. So if you're running your images with Docker itself, either directly or via Compose or Swarm, you're fine. You can use the init flag in your run command or your compose file. Please, note that the below examples require a certain minimum version of Docker. run - 1.13+ compose (v 2.2) - 1.13.0+ swarm (v 3.7) - 18.06.0+ With Docker Run 1 docker run --rm -ti --init caladreas/dui With Docker Compose 1 2 3 4 5 version : 2.2 services : web : image : caladreas/java-docker-signal-demo:no-tini init : true With Docker Swarm 1 2 3 4 5 version : 3.7 services : web : image : caladreas/java-docker-signal-demo:no-tini init : true Relying on Docker does create a dependency on how your container runs. It only runs correctly in Docker-related technologies (run, compose, swarm) and only if the proper versions are available. Creating either a different experience for users running your application somewhere else or not able to meet the version requirements. So maybe another solution is to bake a process manager into your image and guarantee its behavior. Depend on a process manager One of our goals for Docker images is to keep them small. We should look for a lightweight process manager. It does not have too many a whole machine worth or processes, just one and perhaps some children. Here we would like to introduce you to Tini , a lightweight process manager designed for this purpose . It is a very successful and widely adopted process manager in the Docker world. So successful, that the before mentioned init flags from Docker are implemented by baking Tini into Docker . Debian example For brevity, the build process is excluded, and for image size, we use Debian slim instead of default Debian. 1 2 3 4 5 6 FROM debian:stable-slim ENV TINI_VERSION v0.18.0 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui , -XX:+UseCGroupMemoryLimitForHeap , -XX:+UnlockExperimentalVMOptions ] COPY --from = build /usr/bin/dui-image/ /usr/bin/dui Alpine example Alpine Linux works wonders for Docker images, so to improve our lives, you can very easily install it if you want. 1 2 3 4 FROM alpine RUN apk add --no-cache tini ENTRYPOINT [ /sbin/tini , -vv , -g , -s , -- ] CMD [ top -b ] How To Be Told What You Want To Hear You've made it this far; your tenets are reachable so you can inform them if they need to act. However, there's another problem lurking around the corner. Do they speak your language? Our process now starts knowing it can be talked to, it has someone who takes care of listening for it and its children. Now we need to make sure it can understand what it hears, it should be able to handle the incoming signals. We have two main ways of doing this. Handle signals as they come : we should make sure our process deal with the signals as they come State the signals we want : we can also tell up front, which signals we want to hear and put the burden of translation on our callers For more details on the subject of Signals and Docker, please read this excellent blog from Grigorii Chudnov . Handle signals as they come Handling process signals depend on your application, programming language or framework. State the signals we want Sometimes your language or framework of choice, doesn't handle signals all that well. It might be very rigid in what it does with specific signals, removing your ability to do the right thing. Of course, not all languages or frameworks are designed with Docker container or Microservices in mind, are yet to catch up to this more dynamic environment. Luckily Docker and Kubernetes allow you to specify what signal too sent to your process. Docker run 1 2 docker run --rm -ti --init --stop-signal = SIGINT \\ caladreas/java-docker-signal-demo Docker compose/swarm Docker's compose file format allows you to specify a stop signal . This is the signal sent when the container is stopped in a normal fashion. Normal in this case, meaning docker stop or when docker itself determines it should stop the container. If you forcefully remove the container, for example with docker rm - f it will directly kill the process, so don't do that. 1 2 3 4 5 6 version : 2.2 services : web : image : caladreas/java-docker-signal-demo stop_signal : SIGINT stop_grace_period : 15s If you run this with docker - compose up and then in a second terminal, stop the container, you will see something like this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 web_1 | HelloWorld! web_1 | Shutdown hook called! web_1 | We re told to stop early... web_1 | java.lang.InterruptedException: sleep interrupted web_1 | at java.base/java.lang.Thread.sleep(Native Method) web_1 | at joostvdg.demo.signal@1.0/com.github.joostvdg.demo.signal.HelloWorld.printHelloWorld(Unknown Source) web_1 | at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) web_1 | at java.base/java.util.concurrent.FutureTask.run(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) web_1 | at java.base/java.lang.Thread.run(Unknown Source) web_1 | [DEBUG tini (1)] Passing signal: Interrupt web_1 | [DEBUG tini (1)] Received SIGCHLD web_1 | [DEBUG tini (1)] Reaped child with pid: 7 web_1 | [INFO tini (1)] Main child exited with signal (with signal Interrupt ) Kubernetes In Kubernetes we can make use of Container Lifecycle Hooks to manage how our container should be stopped. We could, for example, send a SIGINT (interrupt) to tell our application to stop. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : apps/v1 kind : Deployment metadata : name : java-signal-demo namespace : default labels : app : java-signal-demo spec : replicas : 1 template : metadata : labels : app : java-signal-demo spec : containers : - name : main image : caladreas/java-docker-signal-demo lifecycle : preStop : exec : command : [ killall , java , -INT ] terminationGracePeriodSeconds : 60 When you create this as deployment.yml, create and delete it - kubectl apply - f deployment . yml / kubectl delete - f deployment . yml - you will see the same behavior. How To Be Told When You Want To Hear It Our process now will now start knowing it will hear what it wants to hear. But we now have to make sure we hear it when we need to hear it. An intervention is excellent when you can still be saved, but it is a bit useless if you're already dead. Docker You can either configure your health check in your Dockerfile or configure it in your docker-compose.yml for either compose or swarm. Considering only Docker can use the health check in your Dockerfile, it is strongly recommended to have health checks in your application and document how they can be used. Kubernetes In Kubernetes we have the concept of Container Probes . This allows you to configure whether your container is ready (readinessProbe) to be used and if it is still working as expected (livenessProbe). Examples How to actually listen to the signals and determine which one to use will depend on your programming language. There's three examples I have worked out, one for Go (lang) and two for Java: pojo and Spring Boot. Go Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 # build stage FROM golang:latest AS build-env RUN go get -v github.com/docker/docker/client/... RUN go get -v github.com/docker/docker/api/... ADD src/ $GOPATH /flow-proxy-service-lister WORKDIR $GOPATH /flow-proxy-service-lister RUN go build -o main -tags netgo main.go # final stage FROM alpine ENTRYPOINT [ /app/main ] COPY --from = build-env /go/flow-proxy-service-lister/main /app/ RUN chmod +x /app/main Go code for graceful shutdown The following is a way for Go to shutdown a http server when receiving a termination signal. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func main () { c := make ( chan bool ) // make channel for main -- webserver communication go webserver . Start ( 7777 , webserverData , c ) // ignore the missing data stop := make ( chan os . Signal , 1 ) // make a channel that listens to is signals signal . Notify ( stop , syscall . SIGINT , syscall . SIGTERM ) // we listen to some specific syscall signals for i := 1 ; ; i ++ { // this is still infinite t := time . NewTicker ( time . Second * 30 ) // set a timer for the polling select { case - stop : // this means we got a os signal on our channel break // so we can stop case - t . C : // our timer expired, refresh our data continue // and continue with the loop } break } fmt . Println ( Shutting down webserver ) // if we got here, we have to inform the webserver to close shop c - true // we do this by sending a message on the channel if b := - c ; b { // when we get true back, that means the webserver is doing with a graceful shutdown fmt . Println ( Webserver shut down ) // webserver is done } fmt . Println ( Shut down app ) // we can close shop ourselves now } Java plain (Docker Swarm) This application is a Java 9 modular application, which can be found on github, github.com/joostvdg . Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name *.java ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.1.0 LABEL description = Docker image for playing with java applications in a concurrent, parallel and distributed manor. # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui ] ENV DATE_CHANGED = 20180120-1525 COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules Handling code The code first initializes the server which and when started, creates the Shutdown Hook . Java handles certain signals in specific ways, as can be found in this table for linux. For more information, you can read the docs from Oracle . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 public class DockerApp { public static void main ( String [] args ) { ServiceLoader Logger loggers = ServiceLoader . load ( Logger . class ); Logger logger = loggers . findFirst (). isPresent () ? loggers . findFirst (). get () : null ; if ( logger == null ) { System . err . println ( Did not find any loggers, quiting ); System . exit ( 1 ); } logger . start ( LogLevel . INFO ); int pseudoRandom = new Random (). nextInt ( ProtocolConstants . POTENTIAL_SERVER_NAMES . length - 1 ); String serverName = ProtocolConstants . POTENTIAL_SERVER_NAMES [ pseudoRandom ]; int listenPort = ProtocolConstants . EXTERNAL_COMMUNICATION_PORT_A ; String multicastGroup = ProtocolConstants . MULTICAST_GROUP ; DuiServer distributedServer = DuiServerFactory . newDistributedServer ( listenPort , multicastGroup , serverName , logger ); distributedServer . logMembership (); ExecutorService executorService = Executors . newFixedThreadPool ( 1 ); executorService . submit ( distributedServer :: startServer ); long threadId = Thread . currentThread (). getId (); Runtime . getRuntime (). addShutdownHook ( new Thread (() - { System . out . println ( Shutdown hook called! ); logger . log ( LogLevel . WARN , App , ShotdownHook , threadId , Shutting down at request of Docker ); distributedServer . stopServer (); distributedServer . closeServer (); executorService . shutdown (); try { Thread . sleep ( 100 ); executorService . shutdownNow (); logger . stop (); } catch ( InterruptedException e ) { e . printStackTrace (); } })); } } Java Plain (Kubernetes) So far we've utilized the utilities from Docker itself in conjunction with it's native Docker Swarm orchestrator. Unfortunately, when it comes to popularity Kubernetes beats Swarm hands down . So this isn't complete if it doesn't also do graceful shutdown in Kubernetes. In Dockerfile Our original file had to be changed, as Debian's Slim image doesn't actually contain the kill package. And we need a kill package, as we cannot instruct Kubernetes to issue a specific SIGNAL. Instead, we can issue a PreStop exec command , which we can utilise to execute a killall java -INT . The command will be specified in the Kubernetes deployment definition below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name *.java ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.1.0 LABEL description = Docker image for playing with java applications in a concurrent, parallel and distributed manor. # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui ] ENV DATE_CHANGED = 20180120-1525 RUN apt-get update apt-get install --no-install-recommends -y psmisc = 22 .* rm -rf /var/lib/apt/lists/* COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules Kubernetes Deployment So here we have the image's K8s Deployment descriptor. Including the Pod's lifecycle preStop with a exec style command. You should know by now why we prefer that . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion : extensions/v1beta1 kind : Deployment metadata : name : dui-deployment namespace : default labels : k8s-app : dui spec : replicas : 3 template : metadata : labels : k8s-app : dui spec : containers : - name : master image : caladreas/buming ports : - name : http containerPort : 7777 lifecycle : preStop : exec : command : [ killall , java , -INT ] terminationGracePeriodSeconds : 60 Java Spring Boot (1.x) This example is for Spring Boot 1.x, in time we will have an example for 2.x. This example is for the scenario of a Fat Jar with Tomcat as container [^8]. Execute example 1 docker-compose build Execute the following command: 1 docker run --rm -ti --name test spring-boot-graceful Exit the application/container via ctrl + c and you should see the application shutting down gracefully. 1 2 3 2018 -01-30 13 :35:46.327 INFO 7 --- [ Thread-3 ] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@6e5e91e4: startup date [ Tue Jan 30 13 :35:42 GMT 2018 ] ; root of context hierarchy 2018 -01-30 13 :35:46.405 INFO 7 --- [ Thread-3 ] BootGracefulApplication $GracefulShutdown : Tomcat was shutdown gracefully within the allotted time. 2018 -01-30 13 :35:46.408 INFO 7 --- [ Thread-3 ] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans on shutdown Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 FROM maven:3-jdk-8 AS build ENV MAVEN_OPTS = -Dmaven.repo.local = /usr/share/maven/repository ENV WORKDIR = /usr/src/graceful RUN mkdir $WORKDIR WORKDIR $WORKDIR COPY pom.xml $WORKDIR RUN mvn -B -e org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline COPY . $WORKSPACE RUN mvn -B -e clean verify FROM anapsix/alpine-java:8_jdk_unlimited LABEL authors = Joost van der Griendt joostvdg@gmail.com ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- ] ENV DATE_CHANGED = 20180120-1525 COPY --from = build /usr/src/graceful/target/spring-boot-graceful.jar /app.jar CMD [ java , -Xms256M , -Xmx480M , -Djava.security.egd=file:/dev/./urandom , -jar , /app.jar ] Docker compose file 1 2 3 4 5 6 7 version : 3.5 services : web : image : spring-boot-graceful build : . stop_signal : SIGINT Java handling code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 package com.github.joostvdg.demo.springbootgraceful ; import org.springframework.boot.SpringApplication ; import org.springframework.boot.autoconfigure.SpringBootApplication ; import org.apache.catalina.connector.Connector ; import org.apache.tomcat.util.threads.ThreadPoolExecutor ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer ; import org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatConnectorCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory ; import org.springframework.context.ApplicationListener ; import org.springframework.context.annotation.Bean ; import org.springframework.context.event.ContextClosedEvent ; import java.util.concurrent.Executor ; import java.util.concurrent.TimeUnit ; @SpringBootApplication public class SpringBootGracefulApplication { public static void main ( String [] args ) { SpringApplication . run ( SpringBootGracefulApplication . class , args ); } @Bean public GracefulShutdown gracefulShutdown () { return new GracefulShutdown (); } @Bean public EmbeddedServletContainerCustomizer tomcatCustomizer () { return new EmbeddedServletContainerCustomizer () { @Override public void customize ( ConfigurableEmbeddedServletContainer container ) { if ( container instanceof TomcatEmbeddedServletContainerFactory ) { (( TomcatEmbeddedServletContainerFactory ) container ) . addConnectorCustomizers ( gracefulShutdown ()); } } }; } private static class GracefulShutdown implements TomcatConnectorCustomizer , ApplicationListener ContextClosedEvent { private static final Logger log = LoggerFactory . getLogger ( GracefulShutdown . class ); private volatile Connector connector ; @Override public void customize ( Connector connector ) { this . connector = connector ; } @Override public void onApplicationEvent ( ContextClosedEvent event ) { this . connector . pause (); Executor executor = this . connector . getProtocolHandler (). getExecutor (); if ( executor instanceof ThreadPoolExecutor ) { try { ThreadPoolExecutor threadPoolExecutor = ( ThreadPoolExecutor ) executor ; threadPoolExecutor . shutdown (); if (! threadPoolExecutor . awaitTermination ( 30 , TimeUnit . SECONDS )) { log . warn ( Tomcat thread pool did not shut down gracefully within + 30 seconds. Proceeding with forceful shutdown ); } else { log . info ( Tomcat was shutdown gracefully within the allotted time. ); } } catch ( InterruptedException ex ) { Thread . currentThread (). interrupt (); } } } } } Example with Docker Swarm For now there's only an example with docker swarm , in time there will also be a Kubernetes example. Now that you can create Java applications packaged neatly in Docker images that support graceful shutdown, it would be nice to utilize. A good scenario would be a microservices architecture where services can come and go, but are registered in a service registry such as Eureka . Or a membership based protocol where members interact with each other and perhaps shard data. In these cases, of course the interactions are designed to be fault tolerant and discover faulty nodes on their own. But wouldn't it be better that if you knew you're going to quit, you inform the rest? We can reuse the caladreas / buming image and make it a docker swarm stack and run the service on every node. This way, we can easily see members coming and going and reduce the time to detect failure by notifying our peers of our impeding end. Docker swarm cluster Setting up a docker swarm cluster is easy, but has some requirements: virtual box 4.x+ docker-machine 1.12+ docker 17.06+ Warn Make sure this is the first and only virtualbox docker-machine VM being created/running, so that the ip range starts with 192.168.99.100 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 docker-machine create --driver virtualbox dui-1 docker-machine create --driver virtualbox dui-2 docker-machine create --driver virtualbox dui-3 eval $( docker-machine env dui-1 ) IP = 192 .168.99.100 docker swarm init --advertise-addr $IP TOKEN = $( docker swarm join-token -q worker ) eval $( docker-machine env dui-2 ) docker swarm join --token ${ TOKEN } ${ IP } :2377 eval $( docker-machine env dui-3 ) docker swarm join --token ${ TOKEN } ${ IP } :2377 eval $( docker-machine env dui-1 ) docker node ls Docker swarm network and multicast Unfortunately, docker swarm's swarm mode network overlay does not support multicast [ 9][ 10]. Why is this a problem? Well, the application I use to test the graceful shutdown requires this, sorry. Luckily there is a very easy solution for this, its by using Weavenet 's docker network plugin. Don't want to know about it or how you install it? Don't worry, just execute the script below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/usr/bin/env bash echo = Prepare dui-2 eval $( docker-machine env dui-2 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo = Prepare dui-3 eval $( docker-machine env dui-3 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo = Prepare dui-1 eval $( docker-machine env dui-1 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 docker network create --driver = weaveworks/net-plugin:2.1.3 --opt works.weave.multicast = true --attachable dui Docker stack Now to create a service that runs on every node it is the easiest to create a docker stack . Compose file (docker-stack.yml) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 version : 3.5 services : dui : image : caladreas/buming build : . stop_signal : SIGINT networks : - dui deploy : mode : global networks : dui : external : true Create stack 1 docker stack deploy --compose-file docker-stack.yml buming Execute example Now that we have a docker swarm cluster and a stack - which has a service running on every node - we can showcase the power of graceful shutdown in a cluster of dependent services. Confirm the service is running correctly on every node, first lets check our nodes. 1 2 eval $( docker-machine env dui-1 ) docker node ls Which should look like this: 1 2 3 4 ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS f21ilm4thxegn5xbentmss5ur * dui-1 Ready Active Leader y7475bo5uplt2b58d050b4wfd dui-2 Ready Active 6ssxola6y1i6h9p8256pi7bfv dui-3 Ready Active Then check the service. 1 docker service ps buming_dui Which should look like this. 1 2 3 4 ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 3mrpr0jg31x1 buming_dui.6ssxola6y1i6h9p8256pi7bfv dui:latest dui-3 Running Running 17 seconds ago pfubtiy4j7vo buming_dui.f21ilm4thxegn5xbentmss5ur dui:latest dui-1 Running Running 17 seconds ago f4gjnmhoe3y4 buming_dui.y7475bo5uplt2b58d050b4wfd dui:latest dui-2 Running Running 17 seconds ago Now open a second terminal window. In window one, follow the service logs: 1 2 eval $( docker-machine env dui-1 ) docker service logs -f buming_dui In window two, go to a different node and stop the container. 1 2 3 eval $( docker-machine env dui-2 ) docker ps docker stop buming_dui.y7475bo5uplt2b58d050b4wfd.pnoui2x6elrz0tvkjz51njz94 In this case, you will see the other nodes receiving a leave notice and then the node stopping. 1 2 3 4 5 6 buming_dui.0.ryd8szexxku3@dui-3 | [ Server-John D. Carmack ] [ WARN ] [ 14 :19:02.604011 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = 83918f6ad817 , ip = 10.0.0.7 , name = Ken Thompson } buming_dui.0.so5m14sz8ksh@dui-1 | [ Server-Alan Kay ] [ WARN ] [ 14 :19:02.602082 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = 83918f6ad817 , ip = 10.0.0.7 , name = Ken Thompson } buming_dui.0.pnoui2x6elrz@dui-2 | Shutdown hook called! buming_dui.0.pnoui2x6elrz@dui-2 | [ App ] [ WARN ] [ 14 :19:02.598759 ] [ 1 ] [ ShotdownHook ] Shutting down at request of Docker buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.598858 ] [ 12 ] [ Main ] Stopping buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.601008 ] [ 12 ] [ Main ] Closing Further reading Wikipedia page on reboots Microsoft about graceful shutdown Gracefully stopping docker containers What to know about Java and shutdown hooks https://www.weave.works/blog/docker-container-networking-multicast-fast/ https://www.weave.works/docs/net/latest/install/plugin/plugin-how-it-works/ https://www.weave.works/docs/net/latest/install/plugin/plugin-v2/ https://www.auzias.net/en/docker-network-multihost/ https://forums.docker.com/t/cannot-get-zookeeper-to-work-running-in-docker-using-swarm-mode/27109 https://github.com/docker/libnetwork/issues/740","title":"Graceful Shutdown"},{"location":"docker/graceful-shutdown/#gracefully-shutting-down-applications-in-docker","text":"I'm not sure about you, but I prefer it when my neighbors leave our shared spaces clean and don't take up parking spaces when they don't need them. Imagine you live in an apartment complex with the above-mentioned parking lot. Some tenants go away and never come back. If nothing is done to clean up after them - to reclaim their apartment and parking space - then after some time, more and more apartments are unavailable for no reason, and the parking lot fills with cars which belong to no one. Some tenants did not get a parking lot and are getting frustrated that none are becoming available. When they moved in, they were told that when others leave, they would be next in line. While they're waiting, they have to park outside the complex. Eventually, the entrance gets blocked and no one can enter or leave. The end result is a completely unlivable apartment block with trapped tenants - never to be seen or heard from again. If you agree with me that when a tenant leaves, they should clean the apartment and free the parking spot to make it ready for the next inhabitant; then please read on. We're going to dive into the equivalent of doing this with containers. We will explore running our containers with Docker (run, compose, swarm) and Kubernetes. Even if you use another way to run your containers, this article should provide you with enough insight to get you on your way.","title":"Gracefully Shutting Down Applications in Docker"},{"location":"docker/graceful-shutdown/#the-case-for-graceful-shutdown","text":"We're in an age where many applications are running in Docker containers across a multitude of clusters. These applications are then confronted with new concerns to tackle such as more moving parts, networking between these parts, remote storage and others. One significant way we defend ourselves against the perils of this distributed nature is to make our applications more robust - able to survive errors. However, even then there is still no guarantee your application is always up and running. So another concern we should tackle is how it responds when it needs to shut down. Where we can differentiate between an unexpected shutdown - we crashed - or an expected shutdown. On top of that, failing instead of trying to recover when something bad happens also adheres to \"fail fast\" - as strongly advocated by Michael Nygard in ReleaseIt. Shutting down can happen for a variety of reasons, in this post we dive into how to deal with an expected shutdown such as it being told to stop by an orchestrator such as Kubernetes. Containers can be purposefully shut down for a variety of reasons, including but not limited too: your application's health check fails your application consumed more resources than allowed the application is scaling down Just as cleaning up when leaving makes you a better tenant, having your application clean up connections, resources Moreover, the more tenants behaving in a good way increases the quality of living for all tenants. In our case, it improves the reliability and consistency of our cluster. Graceful shutdown is not unique to Docker, as it has been part of Linux's best practices for quite some years before Docker's existence. However, applying them to Docker container adds extra dimensions.","title":"The case for graceful shutdown"},{"location":"docker/graceful-shutdown/#start-good-so-you-can-end-well","text":"When you sign up for an apartment, you probably have to sign a contract detailing your rights and obligations. The more you state explicitly, the easier it is to deal with bad behaving neighbors. The same is true when running processes; we should make sure that we set the rules, obligations, and expectations from the start. As we say in Dutch: a good beginning is half the work. We will start with how you can run a process in a container with a process that shuts down gracefully. There are many ways to start a process in a container. In this article, we look at processes started by commands defined in a Dockerfile. There are two ways to specify this: CMD : runs a command when the container gets started ENTRYPOINT : provides the location (entrypoint) from where commands get run when the container starts You need at least one ENTRYPOINT or CMD in a Dockerfile for it to be valid. They can be used in collaboration but they can do similar things. For more information on the details of these commands, read Docker's docs on Entrypoint vs. CMD .","title":"Start Good So You Can End Well"},{"location":"docker/graceful-shutdown/#docker-shell-form-example","text":"We start with the shell form and see if it can do what we want; begin in such a way, we can stop it nicely. Shell form means we define a shell command without any special format or keywords. Please create Dockerfile with the content that follows. 1 2 FROM ubuntu:18.04 ENTRYPOINT top -b Then build an image and run a container. 1 2 docker image build --tag shell-form . docker run --name shell-form --rm shell-form The above command yields the following output. 1 2 3 4 5 6 7 8 9 top - 16 :34:56 up 1 day, 5 :15, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 541984 free, 302668 used, 1202280 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1579380 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 4624 760 696 S 0 .0 0 .0 0 :00.05 sh 6 root 20 0 36480 2928 2580 R 0 .0 0 .1 0 :00.01 top As you can see, two processes are running, sh and top . Meaning, that killing the process, with ctrl+c for example, terminates the sh process, but not top . This happens because the sh process forked the top process, but the termination will only be send to PID 1 - in this case sh . As sh will not stop the top process for us it will continue running and leave the container alive. To kill this container, open a second terminal and execute the following command. 1 docker rm -f shell-form Shell form doesn't do what we need. Starting a process with shell form will only lead us to the disaster of parking lots filling up unless there's a someone actively cleaning up.","title":"Docker Shell form example"},{"location":"docker/graceful-shutdown/#docker-exec-form-example","text":"This leads us to the exec form. Hopefully, this gets us somewhere. The exec form is written as an array of parameters: ENTRYPOINT [ top , -b ] To continue in the same line of examples, we will create a Dockerfile, build and run it. 1 2 FROM ubuntu:18.04 ENTRYPOINT [ top , -b ] Then build and run it. 1 2 docker image build --tag exec-form . docker run --name exec-form --rm exec-form This yields the following output. 1 2 3 4 5 6 7 8 top - 18 :12:30 up 1 day, 6 :53, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 535896 free, 307196 used, 1203840 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1574880 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 36480 2940 2584 R 0 .0 0 .1 0 :00.03 top Now we got something we can work with. If something would tell this Container to stop, it will tell our only running process so it is sure to reach the correct one!","title":"Docker exec form example"},{"location":"docker/graceful-shutdown/#gotchas","text":"Knowing we can use the exec form for our goal - gracefully shutting down our container - we can move on to the next part of our efforts. For the sake of imparting you with some hard learned lessons, we will explore two gotchas. They're optional, so you can also choose to skip to Make Sure Your Process Listens .","title":"Gotchas"},{"location":"docker/graceful-shutdown/#docker-exec-form-with-parameters","text":"A caveat with the exec form is that it doesn't interpolate parameters. You can try the following: 1 2 3 FROM ubuntu:18.04 ENV PARAM = -b ENTRYPOINT [ top , ${PARAM} ] Then build and run it: 1 2 docker image build --tag exec-param . docker run --name exec-form --rm exec-param This should yield the following: 1 /bin/sh: 1 : [ top: not found This is where Docker created a mix between the two styles. It allows you to create an Entrypoint with a shell command - performing interpolation - but executing it as an exec form. This can be done by prefixing the shell form, with, you guessed it, exec . 1 2 3 FROM ubuntu:18.04 ENV PARAM = -b ENTRYPOINT exec top ${ PARAM } Then build and run it: 1 2 docker image build --tag exec-param . docker run --name exec-form --rm exec-param This will return the exact same as if we would've run ENTRYPOINT [ top , -b ] . Now you can also override the param, by using the environment variable flag. 1 2 docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = help exec-param Resulting in top's help string.","title":"Docker exec form with parameters"},{"location":"docker/graceful-shutdown/#the-special-case-of-alpine","text":"One of the main best practices for Dockerfiles , is to make them as small as possible. The easiest way to do this is to start with a minimal image. This is where Alpine Linux comes in. We will revisit out shell form example, but replace ubuntu with alpine. Create the following Dockerfile. 1 2 FROM alpine:3.8 ENTRYPOINT top -b Then build and run it. 1 2 docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = help exec-param This yields the following output. 1 2 3 4 5 Mem: 1509068K used, 537864K free, 640K shrd, 126756K buff, 1012436K cached CPU: 0 % usr 0 % sys 0 % nic 100 % idle 0 % io 0 % irq 0 % sirq Load average: 0 .00 0 .00 0 .00 2 /404 5 PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND 1 0 root R 1516 0 % 0 0 % top -b Aside from top 's output looking a bit different, there is only one command. Alpine Linux helps us avoid the problem of shell form altogether!","title":"The special case of Alpine"},{"location":"docker/graceful-shutdown/#make-sure-your-process-listens","text":"It is excellent if your tenants are all signed up, know their rights and obligations. But you can't contact them when something happens, how will they ever know when to act? Translating that into our process. It starts and can be told to shut down, but does it process listen? Can it interpret the message it gets from Docker or Kubernetes? And if it does, can it relay the message correctly to its Child Processes? In order for your process to gracefully shutdown, it should know when to do so. As such, it should listen not only for itself but also on behalf of its children - yours never do anything wrong though! Some processes do, but many aren't designed to listen or tell their Children . They expect someone else to listen for them and tell them and their children - process managers. In order to listen to these signals , we can call in the help of others. We will look at two options. we let Docker manage the process and its children we use a process manager","title":"Make Sure Your Process Listens"},{"location":"docker/graceful-shutdown/#let-docker-manage-it-for-us","text":"If you're not using Docker to run or manage your containers, you should skip to Depend on a process manager . Docker has a build in feature, that it uses a lightweight process manager to help you. So if you're running your images with Docker itself, either directly or via Compose or Swarm, you're fine. You can use the init flag in your run command or your compose file. Please, note that the below examples require a certain minimum version of Docker. run - 1.13+ compose (v 2.2) - 1.13.0+ swarm (v 3.7) - 18.06.0+","title":"Let Docker manage it for us"},{"location":"docker/graceful-shutdown/#with-docker-run","text":"1 docker run --rm -ti --init caladreas/dui","title":"With Docker Run"},{"location":"docker/graceful-shutdown/#with-docker-compose","text":"1 2 3 4 5 version : 2.2 services : web : image : caladreas/java-docker-signal-demo:no-tini init : true","title":"With Docker Compose"},{"location":"docker/graceful-shutdown/#with-docker-swarm","text":"1 2 3 4 5 version : 3.7 services : web : image : caladreas/java-docker-signal-demo:no-tini init : true Relying on Docker does create a dependency on how your container runs. It only runs correctly in Docker-related technologies (run, compose, swarm) and only if the proper versions are available. Creating either a different experience for users running your application somewhere else or not able to meet the version requirements. So maybe another solution is to bake a process manager into your image and guarantee its behavior.","title":"With Docker Swarm"},{"location":"docker/graceful-shutdown/#depend-on-a-process-manager","text":"One of our goals for Docker images is to keep them small. We should look for a lightweight process manager. It does not have too many a whole machine worth or processes, just one and perhaps some children. Here we would like to introduce you to Tini , a lightweight process manager designed for this purpose . It is a very successful and widely adopted process manager in the Docker world. So successful, that the before mentioned init flags from Docker are implemented by baking Tini into Docker .","title":"Depend on a process manager"},{"location":"docker/graceful-shutdown/#debian-example","text":"For brevity, the build process is excluded, and for image size, we use Debian slim instead of default Debian. 1 2 3 4 5 6 FROM debian:stable-slim ENV TINI_VERSION v0.18.0 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui , -XX:+UseCGroupMemoryLimitForHeap , -XX:+UnlockExperimentalVMOptions ] COPY --from = build /usr/bin/dui-image/ /usr/bin/dui","title":"Debian example"},{"location":"docker/graceful-shutdown/#alpine-example","text":"Alpine Linux works wonders for Docker images, so to improve our lives, you can very easily install it if you want. 1 2 3 4 FROM alpine RUN apk add --no-cache tini ENTRYPOINT [ /sbin/tini , -vv , -g , -s , -- ] CMD [ top -b ]","title":"Alpine example"},{"location":"docker/graceful-shutdown/#how-to-be-told-what-you-want-to-hear","text":"You've made it this far; your tenets are reachable so you can inform them if they need to act. However, there's another problem lurking around the corner. Do they speak your language? Our process now starts knowing it can be talked to, it has someone who takes care of listening for it and its children. Now we need to make sure it can understand what it hears, it should be able to handle the incoming signals. We have two main ways of doing this. Handle signals as they come : we should make sure our process deal with the signals as they come State the signals we want : we can also tell up front, which signals we want to hear and put the burden of translation on our callers For more details on the subject of Signals and Docker, please read this excellent blog from Grigorii Chudnov .","title":"How To Be Told What You Want To Hear"},{"location":"docker/graceful-shutdown/#handle-signals-as-they-come","text":"Handling process signals depend on your application, programming language or framework.","title":"Handle signals as they come"},{"location":"docker/graceful-shutdown/#state-the-signals-we-want","text":"Sometimes your language or framework of choice, doesn't handle signals all that well. It might be very rigid in what it does with specific signals, removing your ability to do the right thing. Of course, not all languages or frameworks are designed with Docker container or Microservices in mind, are yet to catch up to this more dynamic environment. Luckily Docker and Kubernetes allow you to specify what signal too sent to your process.","title":"State the signals we want"},{"location":"docker/graceful-shutdown/#docker-run","text":"1 2 docker run --rm -ti --init --stop-signal = SIGINT \\ caladreas/java-docker-signal-demo","title":"Docker run"},{"location":"docker/graceful-shutdown/#docker-composeswarm","text":"Docker's compose file format allows you to specify a stop signal . This is the signal sent when the container is stopped in a normal fashion. Normal in this case, meaning docker stop or when docker itself determines it should stop the container. If you forcefully remove the container, for example with docker rm - f it will directly kill the process, so don't do that. 1 2 3 4 5 6 version : 2.2 services : web : image : caladreas/java-docker-signal-demo stop_signal : SIGINT stop_grace_period : 15s If you run this with docker - compose up and then in a second terminal, stop the container, you will see something like this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 web_1 | HelloWorld! web_1 | Shutdown hook called! web_1 | We re told to stop early... web_1 | java.lang.InterruptedException: sleep interrupted web_1 | at java.base/java.lang.Thread.sleep(Native Method) web_1 | at joostvdg.demo.signal@1.0/com.github.joostvdg.demo.signal.HelloWorld.printHelloWorld(Unknown Source) web_1 | at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) web_1 | at java.base/java.util.concurrent.FutureTask.run(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) web_1 | at java.base/java.lang.Thread.run(Unknown Source) web_1 | [DEBUG tini (1)] Passing signal: Interrupt web_1 | [DEBUG tini (1)] Received SIGCHLD web_1 | [DEBUG tini (1)] Reaped child with pid: 7 web_1 | [INFO tini (1)] Main child exited with signal (with signal Interrupt )","title":"Docker compose/swarm"},{"location":"docker/graceful-shutdown/#kubernetes","text":"In Kubernetes we can make use of Container Lifecycle Hooks to manage how our container should be stopped. We could, for example, send a SIGINT (interrupt) to tell our application to stop. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : apps/v1 kind : Deployment metadata : name : java-signal-demo namespace : default labels : app : java-signal-demo spec : replicas : 1 template : metadata : labels : app : java-signal-demo spec : containers : - name : main image : caladreas/java-docker-signal-demo lifecycle : preStop : exec : command : [ killall , java , -INT ] terminationGracePeriodSeconds : 60 When you create this as deployment.yml, create and delete it - kubectl apply - f deployment . yml / kubectl delete - f deployment . yml - you will see the same behavior.","title":"Kubernetes"},{"location":"docker/graceful-shutdown/#how-to-be-told-when-you-want-to-hear-it","text":"Our process now will now start knowing it will hear what it wants to hear. But we now have to make sure we hear it when we need to hear it. An intervention is excellent when you can still be saved, but it is a bit useless if you're already dead.","title":"How To Be Told When You Want To Hear It"},{"location":"docker/graceful-shutdown/#docker","text":"You can either configure your health check in your Dockerfile or configure it in your docker-compose.yml for either compose or swarm. Considering only Docker can use the health check in your Dockerfile, it is strongly recommended to have health checks in your application and document how they can be used.","title":"Docker"},{"location":"docker/graceful-shutdown/#kubernetes_1","text":"In Kubernetes we have the concept of Container Probes . This allows you to configure whether your container is ready (readinessProbe) to be used and if it is still working as expected (livenessProbe).","title":"Kubernetes"},{"location":"docker/graceful-shutdown/#examples","text":"How to actually listen to the signals and determine which one to use will depend on your programming language. There's three examples I have worked out, one for Go (lang) and two for Java: pojo and Spring Boot.","title":"Examples"},{"location":"docker/graceful-shutdown/#go","text":"","title":"Go"},{"location":"docker/graceful-shutdown/#dockerfile","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 # build stage FROM golang:latest AS build-env RUN go get -v github.com/docker/docker/client/... RUN go get -v github.com/docker/docker/api/... ADD src/ $GOPATH /flow-proxy-service-lister WORKDIR $GOPATH /flow-proxy-service-lister RUN go build -o main -tags netgo main.go # final stage FROM alpine ENTRYPOINT [ /app/main ] COPY --from = build-env /go/flow-proxy-service-lister/main /app/ RUN chmod +x /app/main","title":"Dockerfile"},{"location":"docker/graceful-shutdown/#go-code-for-graceful-shutdown","text":"The following is a way for Go to shutdown a http server when receiving a termination signal. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func main () { c := make ( chan bool ) // make channel for main -- webserver communication go webserver . Start ( 7777 , webserverData , c ) // ignore the missing data stop := make ( chan os . Signal , 1 ) // make a channel that listens to is signals signal . Notify ( stop , syscall . SIGINT , syscall . SIGTERM ) // we listen to some specific syscall signals for i := 1 ; ; i ++ { // this is still infinite t := time . NewTicker ( time . Second * 30 ) // set a timer for the polling select { case - stop : // this means we got a os signal on our channel break // so we can stop case - t . C : // our timer expired, refresh our data continue // and continue with the loop } break } fmt . Println ( Shutting down webserver ) // if we got here, we have to inform the webserver to close shop c - true // we do this by sending a message on the channel if b := - c ; b { // when we get true back, that means the webserver is doing with a graceful shutdown fmt . Println ( Webserver shut down ) // webserver is done } fmt . Println ( Shut down app ) // we can close shop ourselves now }","title":"Go code for graceful shutdown"},{"location":"docker/graceful-shutdown/#java-plain-docker-swarm","text":"This application is a Java 9 modular application, which can be found on github, github.com/joostvdg .","title":"Java plain (Docker Swarm)"},{"location":"docker/graceful-shutdown/#dockerfile_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name *.java ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.1.0 LABEL description = Docker image for playing with java applications in a concurrent, parallel and distributed manor. # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui ] ENV DATE_CHANGED = 20180120-1525 COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules","title":"Dockerfile"},{"location":"docker/graceful-shutdown/#handling-code","text":"The code first initializes the server which and when started, creates the Shutdown Hook . Java handles certain signals in specific ways, as can be found in this table for linux. For more information, you can read the docs from Oracle . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 public class DockerApp { public static void main ( String [] args ) { ServiceLoader Logger loggers = ServiceLoader . load ( Logger . class ); Logger logger = loggers . findFirst (). isPresent () ? loggers . findFirst (). get () : null ; if ( logger == null ) { System . err . println ( Did not find any loggers, quiting ); System . exit ( 1 ); } logger . start ( LogLevel . INFO ); int pseudoRandom = new Random (). nextInt ( ProtocolConstants . POTENTIAL_SERVER_NAMES . length - 1 ); String serverName = ProtocolConstants . POTENTIAL_SERVER_NAMES [ pseudoRandom ]; int listenPort = ProtocolConstants . EXTERNAL_COMMUNICATION_PORT_A ; String multicastGroup = ProtocolConstants . MULTICAST_GROUP ; DuiServer distributedServer = DuiServerFactory . newDistributedServer ( listenPort , multicastGroup , serverName , logger ); distributedServer . logMembership (); ExecutorService executorService = Executors . newFixedThreadPool ( 1 ); executorService . submit ( distributedServer :: startServer ); long threadId = Thread . currentThread (). getId (); Runtime . getRuntime (). addShutdownHook ( new Thread (() - { System . out . println ( Shutdown hook called! ); logger . log ( LogLevel . WARN , App , ShotdownHook , threadId , Shutting down at request of Docker ); distributedServer . stopServer (); distributedServer . closeServer (); executorService . shutdown (); try { Thread . sleep ( 100 ); executorService . shutdownNow (); logger . stop (); } catch ( InterruptedException e ) { e . printStackTrace (); } })); } }","title":"Handling code"},{"location":"docker/graceful-shutdown/#java-plain-kubernetes","text":"So far we've utilized the utilities from Docker itself in conjunction with it's native Docker Swarm orchestrator. Unfortunately, when it comes to popularity Kubernetes beats Swarm hands down . So this isn't complete if it doesn't also do graceful shutdown in Kubernetes.","title":"Java Plain (Kubernetes)"},{"location":"docker/graceful-shutdown/#in-dockerfile","text":"Our original file had to be changed, as Debian's Slim image doesn't actually contain the kill package. And we need a kill package, as we cannot instruct Kubernetes to issue a specific SIGNAL. Instead, we can issue a PreStop exec command , which we can utilise to execute a killall java -INT . The command will be specified in the Kubernetes deployment definition below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name *.java ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.1.0 LABEL description = Docker image for playing with java applications in a concurrent, parallel and distributed manor. # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui ] ENV DATE_CHANGED = 20180120-1525 RUN apt-get update apt-get install --no-install-recommends -y psmisc = 22 .* rm -rf /var/lib/apt/lists/* COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules","title":"In Dockerfile"},{"location":"docker/graceful-shutdown/#kubernetes-deployment","text":"So here we have the image's K8s Deployment descriptor. Including the Pod's lifecycle preStop with a exec style command. You should know by now why we prefer that . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion : extensions/v1beta1 kind : Deployment metadata : name : dui-deployment namespace : default labels : k8s-app : dui spec : replicas : 3 template : metadata : labels : k8s-app : dui spec : containers : - name : master image : caladreas/buming ports : - name : http containerPort : 7777 lifecycle : preStop : exec : command : [ killall , java , -INT ] terminationGracePeriodSeconds : 60","title":"Kubernetes Deployment"},{"location":"docker/graceful-shutdown/#java-spring-boot-1x","text":"This example is for Spring Boot 1.x, in time we will have an example for 2.x. This example is for the scenario of a Fat Jar with Tomcat as container [^8].","title":"Java Spring Boot (1.x)"},{"location":"docker/graceful-shutdown/#execute-example","text":"1 docker-compose build Execute the following command: 1 docker run --rm -ti --name test spring-boot-graceful Exit the application/container via ctrl + c and you should see the application shutting down gracefully. 1 2 3 2018 -01-30 13 :35:46.327 INFO 7 --- [ Thread-3 ] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@6e5e91e4: startup date [ Tue Jan 30 13 :35:42 GMT 2018 ] ; root of context hierarchy 2018 -01-30 13 :35:46.405 INFO 7 --- [ Thread-3 ] BootGracefulApplication $GracefulShutdown : Tomcat was shutdown gracefully within the allotted time. 2018 -01-30 13 :35:46.408 INFO 7 --- [ Thread-3 ] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans on shutdown","title":"Execute example"},{"location":"docker/graceful-shutdown/#dockerfile_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 FROM maven:3-jdk-8 AS build ENV MAVEN_OPTS = -Dmaven.repo.local = /usr/share/maven/repository ENV WORKDIR = /usr/src/graceful RUN mkdir $WORKDIR WORKDIR $WORKDIR COPY pom.xml $WORKDIR RUN mvn -B -e org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline COPY . $WORKSPACE RUN mvn -B -e clean verify FROM anapsix/alpine-java:8_jdk_unlimited LABEL authors = Joost van der Griendt joostvdg@gmail.com ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- ] ENV DATE_CHANGED = 20180120-1525 COPY --from = build /usr/src/graceful/target/spring-boot-graceful.jar /app.jar CMD [ java , -Xms256M , -Xmx480M , -Djava.security.egd=file:/dev/./urandom , -jar , /app.jar ]","title":"Dockerfile"},{"location":"docker/graceful-shutdown/#docker-compose-file","text":"1 2 3 4 5 6 7 version : 3.5 services : web : image : spring-boot-graceful build : . stop_signal : SIGINT","title":"Docker compose file"},{"location":"docker/graceful-shutdown/#java-handling-code","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 package com.github.joostvdg.demo.springbootgraceful ; import org.springframework.boot.SpringApplication ; import org.springframework.boot.autoconfigure.SpringBootApplication ; import org.apache.catalina.connector.Connector ; import org.apache.tomcat.util.threads.ThreadPoolExecutor ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer ; import org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatConnectorCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory ; import org.springframework.context.ApplicationListener ; import org.springframework.context.annotation.Bean ; import org.springframework.context.event.ContextClosedEvent ; import java.util.concurrent.Executor ; import java.util.concurrent.TimeUnit ; @SpringBootApplication public class SpringBootGracefulApplication { public static void main ( String [] args ) { SpringApplication . run ( SpringBootGracefulApplication . class , args ); } @Bean public GracefulShutdown gracefulShutdown () { return new GracefulShutdown (); } @Bean public EmbeddedServletContainerCustomizer tomcatCustomizer () { return new EmbeddedServletContainerCustomizer () { @Override public void customize ( ConfigurableEmbeddedServletContainer container ) { if ( container instanceof TomcatEmbeddedServletContainerFactory ) { (( TomcatEmbeddedServletContainerFactory ) container ) . addConnectorCustomizers ( gracefulShutdown ()); } } }; } private static class GracefulShutdown implements TomcatConnectorCustomizer , ApplicationListener ContextClosedEvent { private static final Logger log = LoggerFactory . getLogger ( GracefulShutdown . class ); private volatile Connector connector ; @Override public void customize ( Connector connector ) { this . connector = connector ; } @Override public void onApplicationEvent ( ContextClosedEvent event ) { this . connector . pause (); Executor executor = this . connector . getProtocolHandler (). getExecutor (); if ( executor instanceof ThreadPoolExecutor ) { try { ThreadPoolExecutor threadPoolExecutor = ( ThreadPoolExecutor ) executor ; threadPoolExecutor . shutdown (); if (! threadPoolExecutor . awaitTermination ( 30 , TimeUnit . SECONDS )) { log . warn ( Tomcat thread pool did not shut down gracefully within + 30 seconds. Proceeding with forceful shutdown ); } else { log . info ( Tomcat was shutdown gracefully within the allotted time. ); } } catch ( InterruptedException ex ) { Thread . currentThread (). interrupt (); } } } } }","title":"Java handling code"},{"location":"docker/graceful-shutdown/#example-with-docker-swarm","text":"For now there's only an example with docker swarm , in time there will also be a Kubernetes example. Now that you can create Java applications packaged neatly in Docker images that support graceful shutdown, it would be nice to utilize. A good scenario would be a microservices architecture where services can come and go, but are registered in a service registry such as Eureka . Or a membership based protocol where members interact with each other and perhaps shard data. In these cases, of course the interactions are designed to be fault tolerant and discover faulty nodes on their own. But wouldn't it be better that if you knew you're going to quit, you inform the rest? We can reuse the caladreas / buming image and make it a docker swarm stack and run the service on every node. This way, we can easily see members coming and going and reduce the time to detect failure by notifying our peers of our impeding end.","title":"Example with Docker Swarm"},{"location":"docker/graceful-shutdown/#docker-swarm-cluster","text":"Setting up a docker swarm cluster is easy, but has some requirements: virtual box 4.x+ docker-machine 1.12+ docker 17.06+ Warn Make sure this is the first and only virtualbox docker-machine VM being created/running, so that the ip range starts with 192.168.99.100 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 docker-machine create --driver virtualbox dui-1 docker-machine create --driver virtualbox dui-2 docker-machine create --driver virtualbox dui-3 eval $( docker-machine env dui-1 ) IP = 192 .168.99.100 docker swarm init --advertise-addr $IP TOKEN = $( docker swarm join-token -q worker ) eval $( docker-machine env dui-2 ) docker swarm join --token ${ TOKEN } ${ IP } :2377 eval $( docker-machine env dui-3 ) docker swarm join --token ${ TOKEN } ${ IP } :2377 eval $( docker-machine env dui-1 ) docker node ls","title":"Docker swarm cluster"},{"location":"docker/graceful-shutdown/#docker-swarm-network-and-multicast","text":"Unfortunately, docker swarm's swarm mode network overlay does not support multicast [ 9][ 10]. Why is this a problem? Well, the application I use to test the graceful shutdown requires this, sorry. Luckily there is a very easy solution for this, its by using Weavenet 's docker network plugin. Don't want to know about it or how you install it? Don't worry, just execute the script below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/usr/bin/env bash echo = Prepare dui-2 eval $( docker-machine env dui-2 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo = Prepare dui-3 eval $( docker-machine env dui-3 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo = Prepare dui-1 eval $( docker-machine env dui-1 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 docker network create --driver = weaveworks/net-plugin:2.1.3 --opt works.weave.multicast = true --attachable dui","title":"Docker swarm network and multicast"},{"location":"docker/graceful-shutdown/#docker-stack","text":"Now to create a service that runs on every node it is the easiest to create a docker stack .","title":"Docker stack"},{"location":"docker/graceful-shutdown/#compose-file-docker-stackyml","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 version : 3.5 services : dui : image : caladreas/buming build : . stop_signal : SIGINT networks : - dui deploy : mode : global networks : dui : external : true","title":"Compose file (docker-stack.yml)"},{"location":"docker/graceful-shutdown/#create-stack","text":"1 docker stack deploy --compose-file docker-stack.yml buming","title":"Create stack"},{"location":"docker/graceful-shutdown/#execute-example_1","text":"Now that we have a docker swarm cluster and a stack - which has a service running on every node - we can showcase the power of graceful shutdown in a cluster of dependent services. Confirm the service is running correctly on every node, first lets check our nodes. 1 2 eval $( docker-machine env dui-1 ) docker node ls Which should look like this: 1 2 3 4 ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS f21ilm4thxegn5xbentmss5ur * dui-1 Ready Active Leader y7475bo5uplt2b58d050b4wfd dui-2 Ready Active 6ssxola6y1i6h9p8256pi7bfv dui-3 Ready Active Then check the service. 1 docker service ps buming_dui Which should look like this. 1 2 3 4 ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 3mrpr0jg31x1 buming_dui.6ssxola6y1i6h9p8256pi7bfv dui:latest dui-3 Running Running 17 seconds ago pfubtiy4j7vo buming_dui.f21ilm4thxegn5xbentmss5ur dui:latest dui-1 Running Running 17 seconds ago f4gjnmhoe3y4 buming_dui.y7475bo5uplt2b58d050b4wfd dui:latest dui-2 Running Running 17 seconds ago Now open a second terminal window. In window one, follow the service logs: 1 2 eval $( docker-machine env dui-1 ) docker service logs -f buming_dui In window two, go to a different node and stop the container. 1 2 3 eval $( docker-machine env dui-2 ) docker ps docker stop buming_dui.y7475bo5uplt2b58d050b4wfd.pnoui2x6elrz0tvkjz51njz94 In this case, you will see the other nodes receiving a leave notice and then the node stopping. 1 2 3 4 5 6 buming_dui.0.ryd8szexxku3@dui-3 | [ Server-John D. Carmack ] [ WARN ] [ 14 :19:02.604011 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = 83918f6ad817 , ip = 10.0.0.7 , name = Ken Thompson } buming_dui.0.so5m14sz8ksh@dui-1 | [ Server-Alan Kay ] [ WARN ] [ 14 :19:02.602082 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = 83918f6ad817 , ip = 10.0.0.7 , name = Ken Thompson } buming_dui.0.pnoui2x6elrz@dui-2 | Shutdown hook called! buming_dui.0.pnoui2x6elrz@dui-2 | [ App ] [ WARN ] [ 14 :19:02.598759 ] [ 1 ] [ ShotdownHook ] Shutting down at request of Docker buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.598858 ] [ 12 ] [ Main ] Stopping buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.601008 ] [ 12 ] [ Main ] Closing","title":"Execute example"},{"location":"docker/graceful-shutdown/#further-reading","text":"Wikipedia page on reboots Microsoft about graceful shutdown Gracefully stopping docker containers What to know about Java and shutdown hooks https://www.weave.works/blog/docker-container-networking-multicast-fast/ https://www.weave.works/docs/net/latest/install/plugin/plugin-how-it-works/ https://www.weave.works/docs/net/latest/install/plugin/plugin-v2/ https://www.auzias.net/en/docker-network-multihost/ https://forums.docker.com/t/cannot-get-zookeeper-to-work-running-in-docker-using-swarm-mode/27109 https://github.com/docker/libnetwork/issues/740","title":"Further reading"},{"location":"docker/kubernetes/","text":"Kubernetes Kubernetes terminology Kubernetes model Resources https://github.com/weaveworks/scope https://github.com/hjacobs/kube-ops-view https://coreos.com/tectonic/docs/latest/tutorials/sandbox/install.html https://github.com/kubernetes/dashboard https://blog.alexellis.io/you-need-to-know-kubernetes-and-swarm/ https://kubernetes.io/docs/reference/kubectl/cheatsheet/ https://blog.heptio.com/core-kubernetes-jazz-improv-over-orchestration-a7903ea92ca","title":"Kubernetes"},{"location":"docker/kubernetes/#kubernetes","text":"","title":"Kubernetes"},{"location":"docker/kubernetes/#kubernetes-terminology","text":"","title":"Kubernetes terminology"},{"location":"docker/kubernetes/#kubernetes-model","text":"","title":"Kubernetes model"},{"location":"docker/kubernetes/#resources","text":"https://github.com/weaveworks/scope https://github.com/hjacobs/kube-ops-view https://coreos.com/tectonic/docs/latest/tutorials/sandbox/install.html https://github.com/kubernetes/dashboard https://blog.alexellis.io/you-need-to-know-kubernetes-and-swarm/ https://kubernetes.io/docs/reference/kubectl/cheatsheet/ https://blog.heptio.com/core-kubernetes-jazz-improv-over-orchestration-a7903ea92ca","title":"Resources"},{"location":"docker/multi-stage-builds/","text":"Docker Multi-Stage Builds","title":"Multi-Stage Builds(TBD)"},{"location":"docker/multi-stage-builds/#docker-multi-stage-builds","text":"","title":"Docker Multi-Stage Builds"},{"location":"docker/swarm/","text":"Docker Swarm (mode)","title":"Swarm (mode) (TBD)"},{"location":"docker/swarm/#docker-swarm-mode","text":"","title":"Docker Swarm (mode)"},{"location":"java/","text":"Java Patterns/Anti-patterns Constants Use a class that cannot be instantiated for the use of constants. Using an interface is an anti-pattern because of what an interface implies. 1 2 3 4 5 6 7 8 9 /** * It should also be final, else we can extend this and create a constructor allowing us to instantiate it anyway. */ public final class Constants { private Constants () {} // we should not instantiate this class public static final String HELLO = WORLD ; public static final int AMOUNT_OF_CONSTANTS = 2 ; } Other usefull things Random integer","title":"General"},{"location":"java/#java","text":"","title":"Java"},{"location":"java/#patternsanti-patterns","text":"","title":"Patterns/Anti-patterns"},{"location":"java/#constants","text":"Use a class that cannot be instantiated for the use of constants. Using an interface is an anti-pattern because of what an interface implies. 1 2 3 4 5 6 7 8 9 /** * It should also be final, else we can extend this and create a constructor allowing us to instantiate it anyway. */ public final class Constants { private Constants () {} // we should not instantiate this class public static final String HELLO = WORLD ; public static final int AMOUNT_OF_CONSTANTS = 2 ; }","title":"Constants"},{"location":"java/#other-usefull-things","text":"Random integer","title":"Other usefull things"},{"location":"java/concurrency/","text":"Java Concurrency Terminology Correctness Correctness means that a class conforms to its specification . A good specification defines invariants constraining an object\u2019s state and postconditions describing the effects of its operations. 6 Thread Safe Class a class is thread-safe when it continues to behave correctly when accessed from multiple threads No set of operations performed sequentially or concurrently on instances of a thread-safe class can cause an instance to be in an invalid state. 6 Mutex Every Java object can implicitly act as a lock for purposes of synchronization; these built-in locks are called intrinsic locks or monitor locks . The lock is auto-matically acquired by the executing thread before entering a synchronized block and automatically released when control exits the synchronized block, whether by the normal control path or by throwing an exception out of the block. Intrinsic locks in Java act as mutexes (or mutual exclusion locks ), which means that at most one thread may own the lock. When thread A attempts to acquire a lock held by thread B, A must wait, or block, until B releases it. If B never releases the lock, A waits forever. 6 Reentrant locks When a thread requests a lock that is already held by another thread, the requesting thread blocks. But because intrinsic locks are reentrant , if a thread tries to acquire a lock that it already holds, the request succeeds. Reentrancy means that locks are acquired on a per-thread rather than per-invocation basis. Reentrancy is implemented by associating with each lock an acquisition count and an owning thread . When the count is zero, the lock is considered unheld. When a thread acquires a previously unheld lock, the JVM records the owner and sets the acquisition count to one. If that same thread acquires the lock again, the count is incremented, and when the owning thread exits the synchronized block , the count is decremented. When the count reaches zero, the lock is released. 6 Liveness In concurrent computing, liveness refers to a set of properties of concurrent systems, that require a system to make progress despite the fact that its concurrently executing components (\"processes\") may have to \"take turns\" in critical sections, parts of the program that cannot be simultaneously run by multiple processes. 1 Liveness guarantees are important properties in operating systems and distributed systems. 2 A liveness property cannot be violated in a finite execution of a distributed system because the \"good\" event might only theoretically occur at some time after execution ends. Eventual consistency is an example of a liveness property. 3 All properties can be expressed as the intersection of safety and liveness properties. 4 Volatile fields When a field is declared volatile , the compiler and runtime are put on notice that this variable is shared and that operations on it should not be reordered with other memory operations. Volatile variables are not cached in registers or in caches where they are hidden from other processors, so a read of a volatile variable always returns the most recent write by any thread. 6 You can use volatile variables only when all the following criteria are met: Writes to the variable do not depend on its current value, or you can ensure that only a single thread ever updates the value; The variable does not participate in invariants with other state variables; Locking is not required for any other reason while the variable is being accessed Confinement Confined objects must not escape their intended scope. An object may be confined to a class instance (such as a private class member), a lexical scope (such as a local variable), or a thread (such as an object that is passed from method to method within a thread, but not supposed to be shared across threads). Objects don\u2019t escape on their own, of course\u2014they need help from the developer, who assists by publishing the object beyond its intended scope. 6 Latch Simply put, a CountDownLatch has a counter field, which you can decrement as we require. We can then use it to block a calling thread until it\u2019s been counted down to zero. If we were doing some parallel processing, we could instantiate the CountDownLatch with the same value for the counter as a number of threads we want to work across. Then, we could just call countdown() after each thread finishes, guaranteeing that a dependent thread calling await() will block until the worker threads are finished. 7 Semaphore In computer science, a semaphore is a variable or abstract data type used to control access to a common resource by multiple processes in a concurrent system such as a multiprogramming operating system. A trivial semaphore is a plain variable that is changed (for example, incremented or decremented, or toggled) depending on programmer-defined conditions. The variable is then used as a condition to control access to some system resource. A useful way to think of a semaphore as used in the real-world systems is as a record of how many units of a particular resource are available, coupled with operations to adjust that record safely (i.e. to avoid race conditions) as units are required or become free, and, if necessary, wait until a unit of the resource becomes available. 7 Java Thread pools There are several different types of Thread pools available. FixedThreadPool : A fixed-size thread pool creates threads as tasks are submitted, up to the maximum pool size, and then attempts to keep the pool size constant (adding new threads if a thread dies due to an unexpected Exception ). CachedThreadPool : A cached thread pool has more flexibility to reap idle threads when the current size of the pool exceeds the demand for processing, and to add new threads when demand increases, but places no bounds on the size of the pool. SingleThreadExecutor : A single-threaded executor creates a single worker thread to process tasks, replacing it if it dies unexpectedly. Tasks are guaranteed to be processed sequentially according to the order imposed by the task queue (FIFO, LIFO, priority order). 4 ScheduledThreadPool : A fixed-size thread pool that supports delayed and periodic task execution, similar to Timer. 6 Interrupt Thread provides the interrupt method for interrupting a thread and for querying whether a thread has been interrupted. Each thread has a boolean property that represents its interrupted status; interrupting a thread sets this status. Interruption is a cooperative mechanism. One thread cannot force another to stop what it is doing and do something else; when thread A interrupts thread B, A is merely requesting that B stop what it is doing when it gets to a convenient stopping point\u2014if it feels like it. When your code calls a method that throws InterruptedException , then your method is a blocking method too, and must have a plan for responding to inter- ruption. For library code, there are basically two choices: Propagate the InterruptedException : This is often the most sensible policy if you can get away with it: just propagate the InterruptedException to your caller. This could involve not catching InterruptedException , or catching it and throwing it again after performing some brief activity-specific cleanup. Restore the interrupt : Sometimes you cannot throw InterruptedException , for instance when your code is part of a Runnable . In these situations, you must catch InterruptedException and restore the interrupted status by calling interrupt on the current thread, so that code higher up the call stack can see that an interrupt was issued. 6 Patterns Queue Deque Queue Deque A Deque is a double-ended queue that allows efficient insertion and removal from both the head and the tail. Implementations include ArrayDeque and LinkedBlockingDeque . Just as blocking queues lend themselves to the producer-consumer pattern, deques lend themselves to a related pattern called work stealing. A producer-consumer design has one shared work queue for all consumers; in a work stealing design, every consumer has its own deque. If a consumer exhausts the work in its own deque, it can steal work from the tail of someone else\u2019s deque. Work stealing can be more scalable than a traditional producer-consumer design because workers don\u2019t contend for a shared work queue; most of the time they access only their own deque, reducing contention. When a worker has to access another\u2019s queue, it does so from the tail rather than the head, further reducing contention. 6 Monitor pattern Resources concurrency-patterns-monitor-object Wikipedia article on monitor pattern e-zest blog on monitor pattern java Examples Confinement PersonSet (below) illustrates how confinement and locking can work together to make a class thread-safe even when its component state variables are not. The state of PersonSet is managed by a HashSet , which is not thread-safe. But because mySet is private and not allowed to escape, the HashSet is confined to the PersonSet. The only code paths that can access mySet are addPerson and containsPerson , and each of these acquires the lock on the PersonSet. All its state is guarded by its intrinsic lock, making PersonSet thread-safe. 6 1 2 3 4 5 6 7 8 9 10 11 12 public class PersonSet { @GuardedBy ( this ) private final Set Person mySet = new HashSet Person (); public synchronized void addPerson ( Person p ) { mySet . add ( p ); } public synchronized boolean containsPerson ( Person p ) { return mySet . contains ( p ); } } HTTP Call Counter Unsafe Counter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class UnsafeCounter { private long count = 0 ; public long getCount () { return count ; } public void service () { // do some work try { int pseudoRandom = new Random (). nextInt ( 20 ); Thread . sleep ( pseudoRandom * 100 ); ++ count ; } catch ( InterruptedException e ) { e . printStackTrace (); } } } Safe Counter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class SafeCounter { private final AtomicLong count = new AtomicLong ( 0 ); public long getCount () { return count . get (); } public void service () { try { int pseudoRandom = new Random (). nextInt ( 20 ); Thread . sleep ( pseudoRandom * 100 ); count . incrementAndGet (); } catch ( InterruptedException e ) { e . printStackTrace (); } } } Caller 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 public class Server { public void start ( int port ) throws Exception { HttpServer server = HttpServer . create ( new InetSocketAddress ( port ), 0 ); UnsafeCounter unsafeCounter = new UnsafeCounter (); SafeCounter safeCounter = new SafeCounter (); server . createContext ( /test , new MyTestHandler ( unsafeCounter , safeCounter )); server . createContext ( / , new MyHandler ( unsafeCounter , safeCounter )); Executor executor = Executors . newFixedThreadPool ( 5 ); server . setExecutor ( executor ); // creates a default executor server . start (); } static class MyTestHandler implements HttpHandler { private UnsafeCounter unsafeCounter ; private SafeCounter safeCounter ; public MyTestHandler ( UnsafeCounter unsafeCounter , SafeCounter safeCounter ) { this . unsafeCounter = unsafeCounter ; this . safeCounter = safeCounter ; } @Override public void handle ( HttpExchange t ) throws IOException { safeCounter . service (); unsafeCounter . service (); System . out . println ( Got a request on /test, counts so far: + unsafeCounter . getCount () + :: + safeCounter . getCount ()); String response = This is the response ; t . sendResponseHeaders ( 200 , response . length ()); try ( OutputStream os = t . getResponseBody ()) { os . write ( response . getBytes ()); } } } } Outcome 1 2 3 4 5 6 7 8 Starting server on port 8080 Server started Got a request on /, counts so far:2::1 Got a request on /, counts so far:6::2 Got a request on /, counts so far:6::3 Got a request on /, counts so far:6::4 Got a request on /, counts so far:6::5 Got a request on /, counts so far:6::6 Lamport, L. (1977). \"Proving the Correctness of Multiprocess Programs\". IEEE Transactions on Software Engineering (2): 125\u2013143. doi: 10.1109/TSE.1977.229904 . Lu\u00eds Rodrigues, Christian Cachin; Rachid Guerraoui (2010). Introduction to reliable and secure distributed programming (2. ed.). Berlin: Springer Berlin. pp. 22\u201324. ISBN 978-3-642-15259-7 . Bailis, P.; Ghodsi, A. (2013). \"Eventual Consistency Today: Limitations, Extensions, and Beyond\". Queue. 11 (3): 20. doi: 10.1145/2460276.2462076 . Alpern, B.; Schneider, F. B. (1987). \"Recognizing safety and liveness\". Distributed Computing. 2 (3): 117. doi: 10.1007/BF01782772 . Liveness article Wikipedia Java Concurrency in Practice / Brian Goetz, with Tim Peierls. . . [et al.] Concurrency in Practice Baeldung tutorial on CountDownLatch Wikipedia article on Semaphore","title":"Java Concurrency"},{"location":"java/concurrency/#java-concurrency","text":"","title":"Java Concurrency"},{"location":"java/concurrency/#terminology","text":"Correctness Correctness means that a class conforms to its specification . A good specification defines invariants constraining an object\u2019s state and postconditions describing the effects of its operations. 6 Thread Safe Class a class is thread-safe when it continues to behave correctly when accessed from multiple threads No set of operations performed sequentially or concurrently on instances of a thread-safe class can cause an instance to be in an invalid state. 6 Mutex Every Java object can implicitly act as a lock for purposes of synchronization; these built-in locks are called intrinsic locks or monitor locks . The lock is auto-matically acquired by the executing thread before entering a synchronized block and automatically released when control exits the synchronized block, whether by the normal control path or by throwing an exception out of the block. Intrinsic locks in Java act as mutexes (or mutual exclusion locks ), which means that at most one thread may own the lock. When thread A attempts to acquire a lock held by thread B, A must wait, or block, until B releases it. If B never releases the lock, A waits forever. 6 Reentrant locks When a thread requests a lock that is already held by another thread, the requesting thread blocks. But because intrinsic locks are reentrant , if a thread tries to acquire a lock that it already holds, the request succeeds. Reentrancy means that locks are acquired on a per-thread rather than per-invocation basis. Reentrancy is implemented by associating with each lock an acquisition count and an owning thread . When the count is zero, the lock is considered unheld. When a thread acquires a previously unheld lock, the JVM records the owner and sets the acquisition count to one. If that same thread acquires the lock again, the count is incremented, and when the owning thread exits the synchronized block , the count is decremented. When the count reaches zero, the lock is released. 6 Liveness In concurrent computing, liveness refers to a set of properties of concurrent systems, that require a system to make progress despite the fact that its concurrently executing components (\"processes\") may have to \"take turns\" in critical sections, parts of the program that cannot be simultaneously run by multiple processes. 1 Liveness guarantees are important properties in operating systems and distributed systems. 2 A liveness property cannot be violated in a finite execution of a distributed system because the \"good\" event might only theoretically occur at some time after execution ends. Eventual consistency is an example of a liveness property. 3 All properties can be expressed as the intersection of safety and liveness properties. 4 Volatile fields When a field is declared volatile , the compiler and runtime are put on notice that this variable is shared and that operations on it should not be reordered with other memory operations. Volatile variables are not cached in registers or in caches where they are hidden from other processors, so a read of a volatile variable always returns the most recent write by any thread. 6 You can use volatile variables only when all the following criteria are met: Writes to the variable do not depend on its current value, or you can ensure that only a single thread ever updates the value; The variable does not participate in invariants with other state variables; Locking is not required for any other reason while the variable is being accessed Confinement Confined objects must not escape their intended scope. An object may be confined to a class instance (such as a private class member), a lexical scope (such as a local variable), or a thread (such as an object that is passed from method to method within a thread, but not supposed to be shared across threads). Objects don\u2019t escape on their own, of course\u2014they need help from the developer, who assists by publishing the object beyond its intended scope. 6 Latch Simply put, a CountDownLatch has a counter field, which you can decrement as we require. We can then use it to block a calling thread until it\u2019s been counted down to zero. If we were doing some parallel processing, we could instantiate the CountDownLatch with the same value for the counter as a number of threads we want to work across. Then, we could just call countdown() after each thread finishes, guaranteeing that a dependent thread calling await() will block until the worker threads are finished. 7 Semaphore In computer science, a semaphore is a variable or abstract data type used to control access to a common resource by multiple processes in a concurrent system such as a multiprogramming operating system. A trivial semaphore is a plain variable that is changed (for example, incremented or decremented, or toggled) depending on programmer-defined conditions. The variable is then used as a condition to control access to some system resource. A useful way to think of a semaphore as used in the real-world systems is as a record of how many units of a particular resource are available, coupled with operations to adjust that record safely (i.e. to avoid race conditions) as units are required or become free, and, if necessary, wait until a unit of the resource becomes available. 7 Java Thread pools There are several different types of Thread pools available. FixedThreadPool : A fixed-size thread pool creates threads as tasks are submitted, up to the maximum pool size, and then attempts to keep the pool size constant (adding new threads if a thread dies due to an unexpected Exception ). CachedThreadPool : A cached thread pool has more flexibility to reap idle threads when the current size of the pool exceeds the demand for processing, and to add new threads when demand increases, but places no bounds on the size of the pool. SingleThreadExecutor : A single-threaded executor creates a single worker thread to process tasks, replacing it if it dies unexpectedly. Tasks are guaranteed to be processed sequentially according to the order imposed by the task queue (FIFO, LIFO, priority order). 4 ScheduledThreadPool : A fixed-size thread pool that supports delayed and periodic task execution, similar to Timer. 6 Interrupt Thread provides the interrupt method for interrupting a thread and for querying whether a thread has been interrupted. Each thread has a boolean property that represents its interrupted status; interrupting a thread sets this status. Interruption is a cooperative mechanism. One thread cannot force another to stop what it is doing and do something else; when thread A interrupts thread B, A is merely requesting that B stop what it is doing when it gets to a convenient stopping point\u2014if it feels like it. When your code calls a method that throws InterruptedException , then your method is a blocking method too, and must have a plan for responding to inter- ruption. For library code, there are basically two choices: Propagate the InterruptedException : This is often the most sensible policy if you can get away with it: just propagate the InterruptedException to your caller. This could involve not catching InterruptedException , or catching it and throwing it again after performing some brief activity-specific cleanup. Restore the interrupt : Sometimes you cannot throw InterruptedException , for instance when your code is part of a Runnable . In these situations, you must catch InterruptedException and restore the interrupted status by calling interrupt on the current thread, so that code higher up the call stack can see that an interrupt was issued. 6","title":"Terminology"},{"location":"java/concurrency/#patterns","text":"","title":"Patterns"},{"location":"java/concurrency/#queue-deque","text":"Queue Deque A Deque is a double-ended queue that allows efficient insertion and removal from both the head and the tail. Implementations include ArrayDeque and LinkedBlockingDeque . Just as blocking queues lend themselves to the producer-consumer pattern, deques lend themselves to a related pattern called work stealing. A producer-consumer design has one shared work queue for all consumers; in a work stealing design, every consumer has its own deque. If a consumer exhausts the work in its own deque, it can steal work from the tail of someone else\u2019s deque. Work stealing can be more scalable than a traditional producer-consumer design because workers don\u2019t contend for a shared work queue; most of the time they access only their own deque, reducing contention. When a worker has to access another\u2019s queue, it does so from the tail rather than the head, further reducing contention. 6","title":"Queue &amp; Deque"},{"location":"java/concurrency/#monitor-pattern","text":"","title":"Monitor pattern"},{"location":"java/concurrency/#resources","text":"concurrency-patterns-monitor-object Wikipedia article on monitor pattern e-zest blog on monitor pattern java","title":"Resources"},{"location":"java/concurrency/#examples","text":"","title":"Examples"},{"location":"java/concurrency/#confinement","text":"PersonSet (below) illustrates how confinement and locking can work together to make a class thread-safe even when its component state variables are not. The state of PersonSet is managed by a HashSet , which is not thread-safe. But because mySet is private and not allowed to escape, the HashSet is confined to the PersonSet. The only code paths that can access mySet are addPerson and containsPerson , and each of these acquires the lock on the PersonSet. All its state is guarded by its intrinsic lock, making PersonSet thread-safe. 6 1 2 3 4 5 6 7 8 9 10 11 12 public class PersonSet { @GuardedBy ( this ) private final Set Person mySet = new HashSet Person (); public synchronized void addPerson ( Person p ) { mySet . add ( p ); } public synchronized boolean containsPerson ( Person p ) { return mySet . contains ( p ); } }","title":"Confinement"},{"location":"java/concurrency/#http-call-counter","text":"","title":"HTTP Call Counter"},{"location":"java/concurrency/#unsafe-counter","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class UnsafeCounter { private long count = 0 ; public long getCount () { return count ; } public void service () { // do some work try { int pseudoRandom = new Random (). nextInt ( 20 ); Thread . sleep ( pseudoRandom * 100 ); ++ count ; } catch ( InterruptedException e ) { e . printStackTrace (); } } }","title":"Unsafe Counter"},{"location":"java/concurrency/#safe-counter","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class SafeCounter { private final AtomicLong count = new AtomicLong ( 0 ); public long getCount () { return count . get (); } public void service () { try { int pseudoRandom = new Random (). nextInt ( 20 ); Thread . sleep ( pseudoRandom * 100 ); count . incrementAndGet (); } catch ( InterruptedException e ) { e . printStackTrace (); } } }","title":"Safe Counter"},{"location":"java/concurrency/#caller","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 public class Server { public void start ( int port ) throws Exception { HttpServer server = HttpServer . create ( new InetSocketAddress ( port ), 0 ); UnsafeCounter unsafeCounter = new UnsafeCounter (); SafeCounter safeCounter = new SafeCounter (); server . createContext ( /test , new MyTestHandler ( unsafeCounter , safeCounter )); server . createContext ( / , new MyHandler ( unsafeCounter , safeCounter )); Executor executor = Executors . newFixedThreadPool ( 5 ); server . setExecutor ( executor ); // creates a default executor server . start (); } static class MyTestHandler implements HttpHandler { private UnsafeCounter unsafeCounter ; private SafeCounter safeCounter ; public MyTestHandler ( UnsafeCounter unsafeCounter , SafeCounter safeCounter ) { this . unsafeCounter = unsafeCounter ; this . safeCounter = safeCounter ; } @Override public void handle ( HttpExchange t ) throws IOException { safeCounter . service (); unsafeCounter . service (); System . out . println ( Got a request on /test, counts so far: + unsafeCounter . getCount () + :: + safeCounter . getCount ()); String response = This is the response ; t . sendResponseHeaders ( 200 , response . length ()); try ( OutputStream os = t . getResponseBody ()) { os . write ( response . getBytes ()); } } } }","title":"Caller"},{"location":"java/concurrency/#outcome","text":"1 2 3 4 5 6 7 8 Starting server on port 8080 Server started Got a request on /, counts so far:2::1 Got a request on /, counts so far:6::2 Got a request on /, counts so far:6::3 Got a request on /, counts so far:6::4 Got a request on /, counts so far:6::5 Got a request on /, counts so far:6::6 Lamport, L. (1977). \"Proving the Correctness of Multiprocess Programs\". IEEE Transactions on Software Engineering (2): 125\u2013143. doi: 10.1109/TSE.1977.229904 . Lu\u00eds Rodrigues, Christian Cachin; Rachid Guerraoui (2010). Introduction to reliable and secure distributed programming (2. ed.). Berlin: Springer Berlin. pp. 22\u201324. ISBN 978-3-642-15259-7 . Bailis, P.; Ghodsi, A. (2013). \"Eventual Consistency Today: Limitations, Extensions, and Beyond\". Queue. 11 (3): 20. doi: 10.1145/2460276.2462076 . Alpern, B.; Schneider, F. B. (1987). \"Recognizing safety and liveness\". Distributed Computing. 2 (3): 117. doi: 10.1007/BF01782772 . Liveness article Wikipedia Java Concurrency in Practice / Brian Goetz, with Tim Peierls. . . [et al.] Concurrency in Practice Baeldung tutorial on CountDownLatch Wikipedia article on Semaphore","title":"Outcome"},{"location":"java/ecosystem/","text":"Java Ecosysten","title":"Java Ecosysten"},{"location":"java/ecosystem/#java-ecosysten","text":"","title":"Java Ecosysten"},{"location":"java/java-modules/","text":"Java Modules","title":"Java Modules"},{"location":"java/java-modules/#java-modules","text":"","title":"Java Modules"},{"location":"java/networking/","text":"Java Networking General Remarks Network API works for IPv4 (32-bit adrressing) and IPv6 (128-bit addressing) Java only supports TCP / IP and UDP / IP Java proxy system params socksProxyHost socksProxyPort http.proxySet http.proxyHost http.proxyPort https.proxySet https.proxyHost https.proxyPort ftpProxySet ftpProxyHost ftpProxyPort gopherProxySet gopherProxyHost gopherProxyPort Special IPv4 segments Internal 10. . .* 172.17. . - 172.31. . 192.168. . Local 127. . .* Broadcast 255.255.255.255 Packets sent to this address are received by all nodes on the local network, though they are not routed beyond the local network Special IPv6 segments Local 0:0:0:0:0:0:0:1 (or ::::::1 or ::1)","title":"Java Networking"},{"location":"java/networking/#java-networking","text":"","title":"Java Networking"},{"location":"java/networking/#general-remarks","text":"Network API works for IPv4 (32-bit adrressing) and IPv6 (128-bit addressing) Java only supports TCP / IP and UDP / IP","title":"General Remarks"},{"location":"java/networking/#java-proxy-system-params","text":"socksProxyHost socksProxyPort http.proxySet http.proxyHost http.proxyPort https.proxySet https.proxyHost https.proxyPort ftpProxySet ftpProxyHost ftpProxyPort gopherProxySet gopherProxyHost gopherProxyPort","title":"Java proxy system params"},{"location":"java/networking/#special-ipv4-segments","text":"","title":"Special IPv4 segments"},{"location":"java/networking/#internal","text":"10. . .* 172.17. . - 172.31. . 192.168. .","title":"Internal"},{"location":"java/networking/#local","text":"127. . .*","title":"Local"},{"location":"java/networking/#broadcast","text":"255.255.255.255 Packets sent to this address are received by all nodes on the local network, though they are not routed beyond the local network","title":"Broadcast"},{"location":"java/networking/#special-ipv6-segments","text":"","title":"Special IPv6 segments"},{"location":"java/networking/#local_1","text":"0:0:0:0:0:0:0:1 (or ::::::1 or ::1)","title":"Local"},{"location":"java/streams/","text":"Java Streams Try-with-Resources try with resources can be used with any object that implements the Closeable interface, which includes almost every object you need to dispose. So far, JavaMail Transport objects are the only exceptions I\u2019ve encountered. Those still need to be disposed of explicitly. 1 2 3 4 5 6 7 8 9 public class Main { public static void main ( String [] args ) { try ( OutputStream out = new FileOutputStream ( /tmp/data.txt )) { // work with the output stream... } catch ( IOException ex ) { System . err . println ( ex . getMessage ()); } } }","title":"Java Streams"},{"location":"java/streams/#java-streams","text":"","title":"Java Streams"},{"location":"java/streams/#try-with-resources","text":"try with resources can be used with any object that implements the Closeable interface, which includes almost every object you need to dispose. So far, JavaMail Transport objects are the only exceptions I\u2019ve encountered. Those still need to be disposed of explicitly. 1 2 3 4 5 6 7 8 9 public class Main { public static void main ( String [] args ) { try ( OutputStream out = new FileOutputStream ( /tmp/data.txt )) { // work with the output stream... } catch ( IOException ex ) { System . err . println ( ex . getMessage ()); } } }","title":"Try-with-Resources"},{"location":"java/spring/boot/","text":"","title":"Boot"},{"location":"jenkins/","text":"Jenkins Cloudbees Study Guide Tuning Please read the following articles from Cloudbees: Prepare-Jenkins-for-support tuning-jenkins-gc-responsiveness-and-stability After-moving-a-job-symlinks-for-folders-became-actual-folders How-to-disable-the-weather-column-to-resolve-instance-slowness Accessing-graphs-on-a-Build-History-page-can-cause-Jenkins-to-become-unresponsive AutoBrowser-Feature-Can-Cause-Performance-Issues Disk-Space-Issue-after-upgrading-Branch-API-plugin JVM-Memory-settings-best-practice Pipeline as code The default interaction model with Jenkins, historically, has been very web UI driven, requiring users to manually create jobs, then manually fill in the details through a web browser. This requires additional effort to create and manage jobs to test and build multiple projects, it also keeps the configuration of a job to build/test/deploy separate from the actual code being built/tested/deployed. This prevents users from applying their existing CI/CD best practices to the job configurations themselves. With the introduction of the Pipeline plugin, users now can implement a project\u2019s entire build/test/deploy pipeline in a Jenkinsfile and store that alongside their code, treating their pipeline as another piece of code checked into source control. We will dive into several things that come into play when writing Jenkins pipelines. Kind of Pipeline jobs Info about Pipeline DSL (a groovy DSL) Reuse pipeline DSL scripts Things to keep in mind Do's and Don't Resources Pipeline Steps Pipeline Solution Pipeline as Code Dzone RefCard Type of pipeline jobs Pipeline (inline) Pipeline (from SCM) Multi-Branch Pipeline GitHub Organization BitBucket Team/Project Gitea Organization GitLab Integration Danger When using the stash function keep in mind that the copying goes from where you are now to the master. When you unstash, it will copy the files from the master to where you are building. When your pipeline runs on a node and you stash and then unstash, it will copy the files from the node to the master and then back to the node. This can have a severe penalty on the performance of your pipeline when you are copying over a network. API Jenkins has an extensive API allowing you to retrieve a lot of information from the server. Plugin For this way you of course have to know how to write a plugin. There are some usefull resources to get started: * https://github.com/joostvdg/hello-world-jenkins-pipeline-plugin * https://wiki.jenkins-ci.org/display/JENKINS/Plugin+tutorial * https://jenkins.io/blog/2016/05/25/update-plugin-for-pipeline/ Do's and Don't Aside from the Do's and Don'ts from Cloudbees, there are some we want to share. This changes the requirement for the component identifier property, as a job may only match a single group and a job listing in a group can only match a single. Thus the easiest way to make sure everything will stay unique (template names probably don\u2019t), is to make the component identifier property unique per file - let it use the name of the project. Other Resources Configuration As Code Jenkins CLI - for managing Plugins Jenkinsfile Runner CICD With Jenkins On Docker Compose Jenkins Helm Chart Jenkins Operator Jenkins X CloudBees Jenkins Distribution CloudBees Jenkins X Distribution","title":"Intro"},{"location":"jenkins/#jenkins","text":"Cloudbees Study Guide","title":"Jenkins"},{"location":"jenkins/#tuning","text":"Please read the following articles from Cloudbees: Prepare-Jenkins-for-support tuning-jenkins-gc-responsiveness-and-stability After-moving-a-job-symlinks-for-folders-became-actual-folders How-to-disable-the-weather-column-to-resolve-instance-slowness Accessing-graphs-on-a-Build-History-page-can-cause-Jenkins-to-become-unresponsive AutoBrowser-Feature-Can-Cause-Performance-Issues Disk-Space-Issue-after-upgrading-Branch-API-plugin JVM-Memory-settings-best-practice","title":"Tuning"},{"location":"jenkins/#pipeline-as-code","text":"The default interaction model with Jenkins, historically, has been very web UI driven, requiring users to manually create jobs, then manually fill in the details through a web browser. This requires additional effort to create and manage jobs to test and build multiple projects, it also keeps the configuration of a job to build/test/deploy separate from the actual code being built/tested/deployed. This prevents users from applying their existing CI/CD best practices to the job configurations themselves. With the introduction of the Pipeline plugin, users now can implement a project\u2019s entire build/test/deploy pipeline in a Jenkinsfile and store that alongside their code, treating their pipeline as another piece of code checked into source control. We will dive into several things that come into play when writing Jenkins pipelines. Kind of Pipeline jobs Info about Pipeline DSL (a groovy DSL) Reuse pipeline DSL scripts Things to keep in mind Do's and Don't","title":"Pipeline as code"},{"location":"jenkins/#resources","text":"Pipeline Steps Pipeline Solution Pipeline as Code Dzone RefCard","title":"Resources"},{"location":"jenkins/#type-of-pipeline-jobs","text":"Pipeline (inline) Pipeline (from SCM) Multi-Branch Pipeline GitHub Organization BitBucket Team/Project Gitea Organization GitLab Integration Danger When using the stash function keep in mind that the copying goes from where you are now to the master. When you unstash, it will copy the files from the master to where you are building. When your pipeline runs on a node and you stash and then unstash, it will copy the files from the node to the master and then back to the node. This can have a severe penalty on the performance of your pipeline when you are copying over a network.","title":"Type of pipeline jobs"},{"location":"jenkins/#api","text":"Jenkins has an extensive API allowing you to retrieve a lot of information from the server.","title":"API"},{"location":"jenkins/#plugin","text":"For this way you of course have to know how to write a plugin. There are some usefull resources to get started: * https://github.com/joostvdg/hello-world-jenkins-pipeline-plugin * https://wiki.jenkins-ci.org/display/JENKINS/Plugin+tutorial * https://jenkins.io/blog/2016/05/25/update-plugin-for-pipeline/","title":"Plugin"},{"location":"jenkins/#dos-and-dont","text":"Aside from the Do's and Don'ts from Cloudbees, there are some we want to share. This changes the requirement for the component identifier property, as a job may only match a single group and a job listing in a group can only match a single. Thus the easiest way to make sure everything will stay unique (template names probably don\u2019t), is to make the component identifier property unique per file - let it use the name of the project.","title":"Do's and Don't"},{"location":"jenkins/#other-resources","text":"Configuration As Code Jenkins CLI - for managing Plugins Jenkinsfile Runner CICD With Jenkins On Docker Compose Jenkins Helm Chart Jenkins Operator Jenkins X CloudBees Jenkins Distribution CloudBees Jenkins X Distribution","title":"Other Resources"},{"location":"jenkins/java-gradle/","text":"","title":"Java gradle"},{"location":"jenkins/plugins/","text":"","title":"Plugins"},{"location":"jenkins-jobs/jenkins-jobs-builder/","text":"Jenkins Job Builder The configuration setup of Jenkins Job Builder is composed of two main categories. Basic configuration and job configuration. Job configuration can be further split into several sub categories. Basic Configuration In the basic configuration you will have to specify how the Jenkins Job Builder CLI can connect to the Jenkins instance you want to configure and how it should act. To use such a configuration file, you add --conf to the CLI command. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 localhost . ini [ job_builder ] ignore_cache = True keep_descriptions = False include_path = . : scripts : ~/ git / recursive = False exclude = . * : manual :. / development allow_duplicates = False [ jenkins ] #user = jenkins #password = url = http : // localhost : 8080 / For more information see http://docs.openstack.org/infra/jenkins-job-builder/installation.html . Job Configuration The configuration for configuring the jobs consists of several distinct parts which can all be in the same file or can be distributed in their own respected files. These different parts can also be split into two different categories, those that are strictly linked within the configuration - via template matching - and those that are separate. Separate: * Macro\u2019s * Global defaults * Job configuration defaults * External configuration files Linked: * Templates * Groups * Projects * Job definitions Here\u2019s a schematic representation on how they are linked. Exampe in YAML config: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - job-template : name : {name}-{configComponentId}-ci description : CI Job of {configComponentId} : *config_job_defaults builders : - shell : jenkins-jobs test -r global/:definitions/ -o compiled/ - job-template : name : {name}-{configComponentId}-execute description : Executor Job of {configComponentId} : *config_job_defaults builders : - shell : jenkins-jobs --conf configuration/localhost.ini update definitions/ - job-group : name : {name}-config gitlab-user : jvandergriendt jobs : - {name}-{configComponentId}-ci : - {name}-{configComponentId}-execute : - project : name : RnD-Config jobs : - {name}-config : configComponentId : JenkinsJobDefinitions The above will result in the following jobs: RnD-Config-JenkinsJobDefinitions-ci RnD-Config-JenkinsJobDefinitions-execute Macro\u2019s Macro\u2019s are what the name implies, a group of related commands which can be invoked by the group. In Jenkins Job Builder this means you can define specific configurations for a component type (e.g. builders, paramters, publishes etc). A component has a name and a macro name. In general the component name is plural and the macro name is singular. As can be seen in the examples below. Here\u2019s an example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # The add macro takes a number parameter and will creates a # job which prints Adding followed by the number parameter: - builder : name : add builders : - shell : echo Adding {number} # A specialized macro addtwo reusing the add macro but with # a number parameter hardcoded to two : - builder : name : addtwo builders : - add : number : two # Glue to have Jenkins Job Builder to expand this YAML example: - job : name : testingjob builders : # The specialized macro: - addtwo # Generic macro call with a parameter - add : number : ZERO # Generic macro called without a parameter. Never do this! # See below for the resulting wrong output :( - add To expand the schematic representation, you will get the following. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 - builder : name : test builders : - shell : jenkins-jobs test -r global/:definitions/ -o compiled/ - builder : name : update builders : - shell : jenkins-jobs --conf config.ini update -r global/:definitions/ - job-template : name : {name}-{configComponentId}-ci : *config_job_defaults builders : - test - job-template : name : {name}-{configComponentId}-update : *config_job_defaults builders : - update Global defaults Global defaults are defaults that should be global for the jobs you configure for a certain environment. It is the job counterpart of the basic configuration, usually containing variables for the specific environment. For example, url\u2019s, credential id\u2019s, JDK\u2019s etc. Example: 1 2 3 4 5 6 7 8 global-defaults-localhost.yaml - defaults : name : global flusso-gitlab-url : https://gitlab.flusso.nl nexus-npm-url : http://localhost:8081/nexus/content/repositories/npm-internal default-jdk : JDK 1.8 jenkinsJobsDefinitionJobName : RnD-Config-JenkinsJobDefinitions-ci credentialsId : 4f0dfb96-a7b1-421c-a4ea-b6a154f91b08 Job configuration defaults Job configuration defaults are nothing specific on their own. It refers to using a build in structure from YAML to create basic building blocks to be used by other configuration parts, usually the Templates. Example (definition): 1 2 3 4 5 6 7 8 9 10 - config_job_defaults : config_job_defaults name : config_job_defaults project-type : freestyle disabled : false logrotate : daysToKeep : 7 numToKeep : 5 artifactDaysToKeep : -1 artifactNumToKeep : -1 jdk : {default-jdk} Example (usage): 1 2 3 - job-template : name : {name}-{configComponentId}-ci : *config_job_defaults Templates Templates are used to define job templates. You define the entirety of the job using global defaults, configuration defaults and where useful refer to placeholders to be filled in by the other downstream configuration items. You can configure almost every plugin that is available for Jenkins, these are divided in subdivisions which reflect the Jenkins\u2019 job definition sections. For these subdivision and the available plugins see: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#modules For those plugins that are not supported, you can include the raw XML generated by the plugin. For how to do this, see: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#raw-config Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - job-template : name : {name}-{configComponentId}-ci display-name : {name}-{configComponentId}-ci description : CI Job of {configComponentId} : *config_job_defaults builders : - shell : jenkins-jobs test -r global/:definitions/ -o compiled/ publishers : - archive : artifacts : {filesToArchive_1} fingerprint : true - archive : artifacts : {filesToArchive_2} fingerprint : true - email : notify-every-unstable-build : true send-to-individuals : true Groups Groups are used to group together related components that require the same set of jobs. Where you can also specify a similar set of properties, for example, a different JDK to be used. The name property is mandatory and will be used to match Job definitions. The jobs property is also mandatory and will be used to match Templates for which a Job will be generated per matching Job definition. Example 1 2 3 4 5 6 7 8 9 10 11 12 - job-group : name : {name}-gulp gitlab-user : jvandergriendt artifactId : {gulpComponentId} jobs : - {name}-{gulpComponentId}-ci : - {name}-{gulpComponentId}-version : - {name}-{gulpComponentId}-sonar : - {name}-{gulpComponentId}-publish : - {name}-{gulpComponentId}-deploy-prep : - {name}-{gulpComponentId}-deploy : - {name}-{gulpComponentId}-acceptance : Projects Projects are used to list the actual Job definitions, which via grouping and Templates get generated, and can obviously be used to define jobs for a specific project. The name property is mandatory and will be passed along with a Job definition and is generally used to tie job definitions to Groups. 1 2 3 4 5 6 - project : name : RnD-Maven jobs : - {name}-keep : gulpComponentId : keep-backend displayName : Keep-Backend Job definitions Job definitions are what is all about. Although they are part of the Project configuration item I treat them separately. You list the jobs under a Project and start with the name of the Group it belongs to. After that, you should define at least a name component to be able to differentiate the different jobs you want. As can be seen in the above examples with the gulpComponentId. External configuration files Sometimes you run into the situation you want to use a multi-line configuration for a plugin, or a set of commands. Or, used at in different configurations or templates. Then you run into the situation that it is very difficult to manage in them neatly inside YAML configuration files. For this situation you are able to simply include a text file, via a native YAML construct. See: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#module-jenkins_jobs.local_yaml For example 1 2 3 4 5 6 7 - job : name : test-job-include-raw-1 builders : - shell : !include-raw include-raw001-hello-world.sh - shell : !include-raw include-raw001-vars.sh Usage The information to how you use the tool is very well explained in the documentation. See http://docs.openstack.org/infra/jenkins-job-builder/installation.html#running Automated maintenance If all the jobs you can administer are done via Jenkins Job Builder, you can start to automate the maintenance of these jobs. Simply make jobs that poll/push on the code base where you have your Jenkins Job Builder configuration files. Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 - config_job_defaults : config_job_defaults name : config_job_defaults project-type : freestyle disabled : false logrotate : daysToKeep : 7 numToKeep : 5 artifactDaysToKeep : -1 artifactNumToKeep : -1 jdk : {default-jdk} triggers : - pollscm : H/15 * * * * scm : - git : url : {flusso-gitlab-url}/{gitlab-user}/{componentGitName}.git credentials-id : {credentialsId} publishers : - email : notify-every-unstable-build : true send-to-individuals : true - job-template : name : {name}-{configComponentId}-ci display-name : {name}-{configComponentId}-ci description : CI Job of {configComponentId} : *config_job_defaults builders : - shell : jenkins-jobs test -r global/:definitions/ -o compiled/ publishers : - archive : artifacts : {filesToArchive_1} fingerprint : true - archive : artifacts : {filesToArchive_2} fingerprint : true - email : notify-every-unstable-build : true send-to-individuals : true - job-template : name : {name}-{configComponentId}-x display-name : {name}-{configComponentId}-execute description : Executor Job of {configComponentId}, it will execute the update and delete old command : *config_job_defaults builders : - shell : jenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/ - job-group : name : {name}-config gitlab-user : jvandergriendt jobs : - {name}-{configComponentId}-ci : - {name}-{configComponentId}-x : - project : name : RnD-Config jobs : - {name}-config : configComponentId : JenkinsJobDefinitions componentGitName : jenkins-job-definitions filesToArchive_1 : scripts/*.sh filesToArchive_2 : maven/settings.xml Tips Trick As the documentation is so extensive, it can sometimes be difficult to figure out what would be a good way to deal with some constructs. Component identifier property One important thing to keep in mind is that in order to create a whole set of jobs via the groups and templates it imperative to have a component* identifier property. This way you can define hundreds of jobs in a project, dozens of groups and dozens of templates and generate thousands of unique individual jobs. Scale does not actually matter in this case, if you have more than one job in a project you will need this property. If the jobs that will be generated will not differ the execution will fail. Bulk you can combine multiple files or even entire folder structures together in a single call. For example, if you manage all the jobs of a company or a department and configure them in separate files. For example 1 jenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/","title":"JenkinsJobsBuilder"},{"location":"jenkins-jobs/jenkins-jobs-builder/#jenkins-job-builder","text":"The configuration setup of Jenkins Job Builder is composed of two main categories. Basic configuration and job configuration. Job configuration can be further split into several sub categories.","title":"Jenkins Job Builder"},{"location":"jenkins-jobs/jenkins-jobs-builder/#basic-configuration","text":"In the basic configuration you will have to specify how the Jenkins Job Builder CLI can connect to the Jenkins instance you want to configure and how it should act. To use such a configuration file, you add --conf to the CLI command. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 localhost . ini [ job_builder ] ignore_cache = True keep_descriptions = False include_path = . : scripts : ~/ git / recursive = False exclude = . * : manual :. / development allow_duplicates = False [ jenkins ] #user = jenkins #password = url = http : // localhost : 8080 / For more information see http://docs.openstack.org/infra/jenkins-job-builder/installation.html .","title":"Basic Configuration"},{"location":"jenkins-jobs/jenkins-jobs-builder/#job-configuration","text":"The configuration for configuring the jobs consists of several distinct parts which can all be in the same file or can be distributed in their own respected files. These different parts can also be split into two different categories, those that are strictly linked within the configuration - via template matching - and those that are separate. Separate: * Macro\u2019s * Global defaults * Job configuration defaults * External configuration files Linked: * Templates * Groups * Projects * Job definitions Here\u2019s a schematic representation on how they are linked. Exampe in YAML config: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - job-template : name : {name}-{configComponentId}-ci description : CI Job of {configComponentId} : *config_job_defaults builders : - shell : jenkins-jobs test -r global/:definitions/ -o compiled/ - job-template : name : {name}-{configComponentId}-execute description : Executor Job of {configComponentId} : *config_job_defaults builders : - shell : jenkins-jobs --conf configuration/localhost.ini update definitions/ - job-group : name : {name}-config gitlab-user : jvandergriendt jobs : - {name}-{configComponentId}-ci : - {name}-{configComponentId}-execute : - project : name : RnD-Config jobs : - {name}-config : configComponentId : JenkinsJobDefinitions The above will result in the following jobs: RnD-Config-JenkinsJobDefinitions-ci RnD-Config-JenkinsJobDefinitions-execute","title":"Job Configuration"},{"location":"jenkins-jobs/jenkins-jobs-builder/#macros","text":"Macro\u2019s are what the name implies, a group of related commands which can be invoked by the group. In Jenkins Job Builder this means you can define specific configurations for a component type (e.g. builders, paramters, publishes etc). A component has a name and a macro name. In general the component name is plural and the macro name is singular. As can be seen in the examples below. Here\u2019s an example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # The add macro takes a number parameter and will creates a # job which prints Adding followed by the number parameter: - builder : name : add builders : - shell : echo Adding {number} # A specialized macro addtwo reusing the add macro but with # a number parameter hardcoded to two : - builder : name : addtwo builders : - add : number : two # Glue to have Jenkins Job Builder to expand this YAML example: - job : name : testingjob builders : # The specialized macro: - addtwo # Generic macro call with a parameter - add : number : ZERO # Generic macro called without a parameter. Never do this! # See below for the resulting wrong output :( - add To expand the schematic representation, you will get the following. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 - builder : name : test builders : - shell : jenkins-jobs test -r global/:definitions/ -o compiled/ - builder : name : update builders : - shell : jenkins-jobs --conf config.ini update -r global/:definitions/ - job-template : name : {name}-{configComponentId}-ci : *config_job_defaults builders : - test - job-template : name : {name}-{configComponentId}-update : *config_job_defaults builders : - update","title":"Macro\u2019s"},{"location":"jenkins-jobs/jenkins-jobs-builder/#global-defaults","text":"Global defaults are defaults that should be global for the jobs you configure for a certain environment. It is the job counterpart of the basic configuration, usually containing variables for the specific environment. For example, url\u2019s, credential id\u2019s, JDK\u2019s etc. Example: 1 2 3 4 5 6 7 8 global-defaults-localhost.yaml - defaults : name : global flusso-gitlab-url : https://gitlab.flusso.nl nexus-npm-url : http://localhost:8081/nexus/content/repositories/npm-internal default-jdk : JDK 1.8 jenkinsJobsDefinitionJobName : RnD-Config-JenkinsJobDefinitions-ci credentialsId : 4f0dfb96-a7b1-421c-a4ea-b6a154f91b08","title":"Global defaults"},{"location":"jenkins-jobs/jenkins-jobs-builder/#job-configuration-defaults","text":"Job configuration defaults are nothing specific on their own. It refers to using a build in structure from YAML to create basic building blocks to be used by other configuration parts, usually the Templates. Example (definition): 1 2 3 4 5 6 7 8 9 10 - config_job_defaults : config_job_defaults name : config_job_defaults project-type : freestyle disabled : false logrotate : daysToKeep : 7 numToKeep : 5 artifactDaysToKeep : -1 artifactNumToKeep : -1 jdk : {default-jdk} Example (usage): 1 2 3 - job-template : name : {name}-{configComponentId}-ci : *config_job_defaults","title":"Job configuration defaults"},{"location":"jenkins-jobs/jenkins-jobs-builder/#templates","text":"Templates are used to define job templates. You define the entirety of the job using global defaults, configuration defaults and where useful refer to placeholders to be filled in by the other downstream configuration items. You can configure almost every plugin that is available for Jenkins, these are divided in subdivisions which reflect the Jenkins\u2019 job definition sections. For these subdivision and the available plugins see: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#modules For those plugins that are not supported, you can include the raw XML generated by the plugin. For how to do this, see: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#raw-config Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - job-template : name : {name}-{configComponentId}-ci display-name : {name}-{configComponentId}-ci description : CI Job of {configComponentId} : *config_job_defaults builders : - shell : jenkins-jobs test -r global/:definitions/ -o compiled/ publishers : - archive : artifacts : {filesToArchive_1} fingerprint : true - archive : artifacts : {filesToArchive_2} fingerprint : true - email : notify-every-unstable-build : true send-to-individuals : true","title":"Templates"},{"location":"jenkins-jobs/jenkins-jobs-builder/#groups","text":"Groups are used to group together related components that require the same set of jobs. Where you can also specify a similar set of properties, for example, a different JDK to be used. The name property is mandatory and will be used to match Job definitions. The jobs property is also mandatory and will be used to match Templates for which a Job will be generated per matching Job definition. Example 1 2 3 4 5 6 7 8 9 10 11 12 - job-group : name : {name}-gulp gitlab-user : jvandergriendt artifactId : {gulpComponentId} jobs : - {name}-{gulpComponentId}-ci : - {name}-{gulpComponentId}-version : - {name}-{gulpComponentId}-sonar : - {name}-{gulpComponentId}-publish : - {name}-{gulpComponentId}-deploy-prep : - {name}-{gulpComponentId}-deploy : - {name}-{gulpComponentId}-acceptance :","title":"Groups"},{"location":"jenkins-jobs/jenkins-jobs-builder/#projects","text":"Projects are used to list the actual Job definitions, which via grouping and Templates get generated, and can obviously be used to define jobs for a specific project. The name property is mandatory and will be passed along with a Job definition and is generally used to tie job definitions to Groups. 1 2 3 4 5 6 - project : name : RnD-Maven jobs : - {name}-keep : gulpComponentId : keep-backend displayName : Keep-Backend","title":"Projects"},{"location":"jenkins-jobs/jenkins-jobs-builder/#job-definitions","text":"Job definitions are what is all about. Although they are part of the Project configuration item I treat them separately. You list the jobs under a Project and start with the name of the Group it belongs to. After that, you should define at least a name component to be able to differentiate the different jobs you want. As can be seen in the above examples with the gulpComponentId. External configuration files Sometimes you run into the situation you want to use a multi-line configuration for a plugin, or a set of commands. Or, used at in different configurations or templates. Then you run into the situation that it is very difficult to manage in them neatly inside YAML configuration files. For this situation you are able to simply include a text file, via a native YAML construct. See: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#module-jenkins_jobs.local_yaml For example 1 2 3 4 5 6 7 - job : name : test-job-include-raw-1 builders : - shell : !include-raw include-raw001-hello-world.sh - shell : !include-raw include-raw001-vars.sh","title":"Job definitions"},{"location":"jenkins-jobs/jenkins-jobs-builder/#usage","text":"The information to how you use the tool is very well explained in the documentation. See http://docs.openstack.org/infra/jenkins-job-builder/installation.html#running Automated maintenance If all the jobs you can administer are done via Jenkins Job Builder, you can start to automate the maintenance of these jobs. Simply make jobs that poll/push on the code base where you have your Jenkins Job Builder configuration files. Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 - config_job_defaults : config_job_defaults name : config_job_defaults project-type : freestyle disabled : false logrotate : daysToKeep : 7 numToKeep : 5 artifactDaysToKeep : -1 artifactNumToKeep : -1 jdk : {default-jdk} triggers : - pollscm : H/15 * * * * scm : - git : url : {flusso-gitlab-url}/{gitlab-user}/{componentGitName}.git credentials-id : {credentialsId} publishers : - email : notify-every-unstable-build : true send-to-individuals : true - job-template : name : {name}-{configComponentId}-ci display-name : {name}-{configComponentId}-ci description : CI Job of {configComponentId} : *config_job_defaults builders : - shell : jenkins-jobs test -r global/:definitions/ -o compiled/ publishers : - archive : artifacts : {filesToArchive_1} fingerprint : true - archive : artifacts : {filesToArchive_2} fingerprint : true - email : notify-every-unstable-build : true send-to-individuals : true - job-template : name : {name}-{configComponentId}-x display-name : {name}-{configComponentId}-execute description : Executor Job of {configComponentId}, it will execute the update and delete old command : *config_job_defaults builders : - shell : jenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/ - job-group : name : {name}-config gitlab-user : jvandergriendt jobs : - {name}-{configComponentId}-ci : - {name}-{configComponentId}-x : - project : name : RnD-Config jobs : - {name}-config : configComponentId : JenkinsJobDefinitions componentGitName : jenkins-job-definitions filesToArchive_1 : scripts/*.sh filesToArchive_2 : maven/settings.xml","title":"Usage"},{"location":"jenkins-jobs/jenkins-jobs-builder/#tips-trick","text":"As the documentation is so extensive, it can sometimes be difficult to figure out what would be a good way to deal with some constructs. Component identifier property One important thing to keep in mind is that in order to create a whole set of jobs via the groups and templates it imperative to have a component* identifier property. This way you can define hundreds of jobs in a project, dozens of groups and dozens of templates and generate thousands of unique individual jobs. Scale does not actually matter in this case, if you have more than one job in a project you will need this property. If the jobs that will be generated will not differ the execution will fail. Bulk you can combine multiple files or even entire folder structures together in a single call. For example, if you manage all the jobs of a company or a department and configure them in separate files. For example 1 jenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/","title":"Tips &amp; Trick"},{"location":"jenkins-jobs/jobdsl/","text":"Jenkins Job DSL Jenkins is a wonderful system for managing builds, and people love using its UI to configure jobs. Unfortunately, as the number of jobs grows, maintaining them becomes tedious, and the paradigm of using a UI falls apart. Additionally, the common pattern in this situation is to copy jobs to create new ones, these \"children\" have a habit of diverging from their original \"template\" and consequently it becomes difficult to maintain consistency between these jobs. The Jenkins job-dsl-plugin attempts to solve this problem by allowing jobs to be defined with the absolute minimum necessary in a programmatic form, with the help of templates that are synced with the generated jobs. The goal is for your project to be able to define all the jobs they want to be related to their project, declaring their intent for the jobs, leaving the common stuff up to a template that were defined earlier or hidden behind the DSL. Pipeline with folder example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import hudson.model.* import jenkins.model.* def dslExamplesFolder = DSL-Examples def gitLabCredentialsId = joost-flusso-gitlab-ssh def gitLabUrl = git@gitlab.flusso.nl def gitLabNamespace = keep def gitLabProject = keep-api if (! jenkins . model . Jenkins . instance . getItem ( dslExamplesFolder )) { //folder doesn t exist because item doesn t exist in runtime //Therefore, create the folder. folder ( dslExamplesFolder ) { displayName ( DSL Examples ) description ( Folder for job dsl examples ) } } createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , keep , keep-api ) createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , keep , keep-backend-spring ) createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , keep , keep-frontend ) def createMultibranchPipelineJob ( def gitLabCredentialsId , def gitLabUrl , def folder , def gitNamespace , def project ) { multibranchPipelineJob ( ${folder}/${project}-mb ) { branchSources { git { remote ( ${gitLabUrl}:${gitNamespace}/${project}.git ) credentialsId ( gitLabCredentialsId ) } } orphanedItemStrategy { discardOldItems { numToKeep ( 20 ) } } } } Freestyle maven job 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def project = quidryan/aws-sdk-test def branchApi = new URL ( https://api.github.com/repos/${project}/branches ) def branches = new groovy . json . JsonSlurper (). parse ( branchApi . newReader ()) branches . each { def branchName = it . name def jobName = ${project}-${branchName} . replaceAll ( / , - ) job ( jobName ) { scm { git ( git://github.com/${project}.git , branchName ) } steps { maven ( test -Dproject.name=${project}/${branchName} ) } } } Resources Tutorial Live Playground Main DSL Commands API Viewer Other References Talks and Blogs User Power Movies DZone article Testing DSL Scripts","title":"JobDSL"},{"location":"jenkins-jobs/jobdsl/#jenkins-job-dsl","text":"Jenkins is a wonderful system for managing builds, and people love using its UI to configure jobs. Unfortunately, as the number of jobs grows, maintaining them becomes tedious, and the paradigm of using a UI falls apart. Additionally, the common pattern in this situation is to copy jobs to create new ones, these \"children\" have a habit of diverging from their original \"template\" and consequently it becomes difficult to maintain consistency between these jobs. The Jenkins job-dsl-plugin attempts to solve this problem by allowing jobs to be defined with the absolute minimum necessary in a programmatic form, with the help of templates that are synced with the generated jobs. The goal is for your project to be able to define all the jobs they want to be related to their project, declaring their intent for the jobs, leaving the common stuff up to a template that were defined earlier or hidden behind the DSL.","title":"Jenkins Job DSL"},{"location":"jenkins-jobs/jobdsl/#pipeline-with-folder-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import hudson.model.* import jenkins.model.* def dslExamplesFolder = DSL-Examples def gitLabCredentialsId = joost-flusso-gitlab-ssh def gitLabUrl = git@gitlab.flusso.nl def gitLabNamespace = keep def gitLabProject = keep-api if (! jenkins . model . Jenkins . instance . getItem ( dslExamplesFolder )) { //folder doesn t exist because item doesn t exist in runtime //Therefore, create the folder. folder ( dslExamplesFolder ) { displayName ( DSL Examples ) description ( Folder for job dsl examples ) } } createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , keep , keep-api ) createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , keep , keep-backend-spring ) createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , keep , keep-frontend ) def createMultibranchPipelineJob ( def gitLabCredentialsId , def gitLabUrl , def folder , def gitNamespace , def project ) { multibranchPipelineJob ( ${folder}/${project}-mb ) { branchSources { git { remote ( ${gitLabUrl}:${gitNamespace}/${project}.git ) credentialsId ( gitLabCredentialsId ) } } orphanedItemStrategy { discardOldItems { numToKeep ( 20 ) } } } }","title":"Pipeline with folder example"},{"location":"jenkins-jobs/jobdsl/#freestyle-maven-job","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def project = quidryan/aws-sdk-test def branchApi = new URL ( https://api.github.com/repos/${project}/branches ) def branches = new groovy . json . JsonSlurper (). parse ( branchApi . newReader ()) branches . each { def branchName = it . name def jobName = ${project}-${branchName} . replaceAll ( / , - ) job ( jobName ) { scm { git ( git://github.com/${project}.git , branchName ) } steps { maven ( test -Dproject.name=${project}/${branchName} ) } } }","title":"Freestyle maven job"},{"location":"jenkins-jobs/jobdsl/#resources","text":"Tutorial Live Playground Main DSL Commands API Viewer","title":"Resources"},{"location":"jenkins-jobs/jobdsl/#other-references","text":"Talks and Blogs User Power Movies DZone article Testing DSL Scripts","title":"Other References"},{"location":"jenkins-pipeline/artifactory-integration/","text":"JFrog Jenkins Challenge Visited the stand of JFrog where they had stories about two main products: Artifactory and X-Ray. For both there is a Challenge, an X-Ray Challenge and a Jenkins Artifactory Challenge . Jenkins Challenge The instructions for the Challenge were simply, follow what is stated in their GitHub repository and email a screenshot of the result. The instruction were as follows: Get an Artifactory instance (you can start a free trial on prem or in the cloud) Install Jenkins Install Artifactory Jenkins Plugin Add Artifactory credentials to Jenkins Credentials Create a new pipeline job Use the Artifactory Plugin DSL documentation to complete the following script: With a Scripted Pipeline as starting point: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 node { def rtServer def rtGradle def buildInfo stage ( Preparation ) { git https://github.com/jbaruch/gradle-example.git // create a new Artifactory server using the credentials defined in Jenkins // create a new Gradle build // set the resolver to the Gradle build to resolve from Artifactory // set the deployer to the Gradle build to deploy to Artifactory // declare that your gradle script does not use Artifactory plugin // declare that your gradle script uses Gradle wrapper } stage ( Build ) { //run the artifactoryPublish gradle task and collect the build info } stage ( Publish Build Info ) { //collect the environment variables to build info //publish the build info } } I don't like scripted, so I opted for Declarative with Jenkins in Kubernetes with the Jenkins Kubernetes plugin. Steps I took: get a trial license from the JFrog website install Artifactory and copy in the license when prompted change admin password create local maven repo 'libs-snapshot-local' create remote maven repo 'jcenter' (default remote value is jcenter, so only have to set the name) install Jenkins Artifactory plugin Kubernetes plugin add Artifactory username/password as credential in Jenkins create a gradle application (Spring boot via start.spring.io) which you can find here create a Jenkinsfile Installing Artifactory I installed Artifactory via Helm. JFrog has their own Helm repository - of course, would weird otherwise tbh - and you have to add that first. 1 2 helm repo add jfrog https://charts.jfrog.io helm install --name artifactory stable/artifactory Jenkinsfile This uses the Gradle wrapper - as per instructions in the challenge. So we can use the standard JNLP container, which is default, so agent any will do. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 pipeline { agent any environment { rtServer = rtGradle = buildInfo = artifactoryServerAddress = http://..../artifactory } stages { stage ( Test Container ) { steps { container ( gradle ) { sh which gradle sh uname -a sh gradle -version } } } stage ( Checkout ){ steps { git https://github.com/demomon/gradle-jenkins-challenge.git } } stage ( Preparation ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: artifactoryServerAddress , credentialsId: art-admin // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: jcenter , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: libs-snapshot-local , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( Build ) { steps { script { //run the artifactoryPublish gradle task and collect the build info buildInfo = rtGradle . run buildFile: build.gradle , tasks: clean build artifactoryPublish } } } stage ( Publish Build Info ) { steps { script { //collect the environment variables to build info buildInfo . env . capture = true //publish the build info rtServer . publishBuildInfo buildInfo } } } } } Jenkinsfile without Gradle Wrapper I'd rather not install the Gradle tool if I can just use a pre-build container with it. Unfortunately, to use it correctly with the Artifactory plugin and a Jenkins Kubernetes plugin, we need to do two things. create a Gradle Tool in the Jenkins master because the Artifactory plugin expects a Jenkins Tool object, not a location Manage Jenkins - Global Tool Configuration - Gradle - Add As value supply / usr , the Artifactory build will add / gradle / bin to it automatically set the user of build Pod to id 1000 explicitly else the build will not be allowed to touch files in / home / jenkins / workspace 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 pipeline { agent { kubernetes { label mypod yaml apiVersion: v1 kind: Pod spec: securityContext: runAsUser: 1000 fsGroup: 1000 containers: - name: gradle image: gradle:4.10-jdk-alpine command: [ cat ] tty: true } } environment { rtServer = rtGradle = buildInfo = CONTAINER_GRADLE_TOOL = /usr/bin/gradle } stages { stage ( Test Container ) { steps { container ( gradle ) { sh which gradle sh uname -a sh gradle -version } } } stage ( Checkout ){ steps { // git https://github.com/demomon/gradle-jenkins-challenge.git checkout scm } } stage ( Preparation ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: http://35.204.238.14/artifactory , credentialsId: art-admin // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: jcenter , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: libs-snapshot-local , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( Build ) { //run the artifactoryPublish gradle task and collect the build info steps { script { buildInfo = rtGradle . run buildFile: build.gradle , tasks: clean build artifactoryPublish } } } stage ( Publish Build Info ) { //collect the environment variables to build info //publish the build info steps { script { buildInfo . env . capture = true rtServer . publishBuildInfo buildInfo } } } } }","title":"Artifactory Integration"},{"location":"jenkins-pipeline/artifactory-integration/#jfrog-jenkins-challenge","text":"Visited the stand of JFrog where they had stories about two main products: Artifactory and X-Ray. For both there is a Challenge, an X-Ray Challenge and a Jenkins Artifactory Challenge .","title":"JFrog Jenkins Challenge"},{"location":"jenkins-pipeline/artifactory-integration/#jenkins-challenge","text":"The instructions for the Challenge were simply, follow what is stated in their GitHub repository and email a screenshot of the result. The instruction were as follows: Get an Artifactory instance (you can start a free trial on prem or in the cloud) Install Jenkins Install Artifactory Jenkins Plugin Add Artifactory credentials to Jenkins Credentials Create a new pipeline job Use the Artifactory Plugin DSL documentation to complete the following script: With a Scripted Pipeline as starting point: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 node { def rtServer def rtGradle def buildInfo stage ( Preparation ) { git https://github.com/jbaruch/gradle-example.git // create a new Artifactory server using the credentials defined in Jenkins // create a new Gradle build // set the resolver to the Gradle build to resolve from Artifactory // set the deployer to the Gradle build to deploy to Artifactory // declare that your gradle script does not use Artifactory plugin // declare that your gradle script uses Gradle wrapper } stage ( Build ) { //run the artifactoryPublish gradle task and collect the build info } stage ( Publish Build Info ) { //collect the environment variables to build info //publish the build info } } I don't like scripted, so I opted for Declarative with Jenkins in Kubernetes with the Jenkins Kubernetes plugin. Steps I took: get a trial license from the JFrog website install Artifactory and copy in the license when prompted change admin password create local maven repo 'libs-snapshot-local' create remote maven repo 'jcenter' (default remote value is jcenter, so only have to set the name) install Jenkins Artifactory plugin Kubernetes plugin add Artifactory username/password as credential in Jenkins create a gradle application (Spring boot via start.spring.io) which you can find here create a Jenkinsfile","title":"Jenkins Challenge"},{"location":"jenkins-pipeline/artifactory-integration/#installing-artifactory","text":"I installed Artifactory via Helm. JFrog has their own Helm repository - of course, would weird otherwise tbh - and you have to add that first. 1 2 helm repo add jfrog https://charts.jfrog.io helm install --name artifactory stable/artifactory","title":"Installing Artifactory"},{"location":"jenkins-pipeline/artifactory-integration/#jenkinsfile","text":"This uses the Gradle wrapper - as per instructions in the challenge. So we can use the standard JNLP container, which is default, so agent any will do. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 pipeline { agent any environment { rtServer = rtGradle = buildInfo = artifactoryServerAddress = http://..../artifactory } stages { stage ( Test Container ) { steps { container ( gradle ) { sh which gradle sh uname -a sh gradle -version } } } stage ( Checkout ){ steps { git https://github.com/demomon/gradle-jenkins-challenge.git } } stage ( Preparation ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: artifactoryServerAddress , credentialsId: art-admin // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: jcenter , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: libs-snapshot-local , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( Build ) { steps { script { //run the artifactoryPublish gradle task and collect the build info buildInfo = rtGradle . run buildFile: build.gradle , tasks: clean build artifactoryPublish } } } stage ( Publish Build Info ) { steps { script { //collect the environment variables to build info buildInfo . env . capture = true //publish the build info rtServer . publishBuildInfo buildInfo } } } } }","title":"Jenkinsfile"},{"location":"jenkins-pipeline/artifactory-integration/#jenkinsfile-without-gradle-wrapper","text":"I'd rather not install the Gradle tool if I can just use a pre-build container with it. Unfortunately, to use it correctly with the Artifactory plugin and a Jenkins Kubernetes plugin, we need to do two things. create a Gradle Tool in the Jenkins master because the Artifactory plugin expects a Jenkins Tool object, not a location Manage Jenkins - Global Tool Configuration - Gradle - Add As value supply / usr , the Artifactory build will add / gradle / bin to it automatically set the user of build Pod to id 1000 explicitly else the build will not be allowed to touch files in / home / jenkins / workspace 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 pipeline { agent { kubernetes { label mypod yaml apiVersion: v1 kind: Pod spec: securityContext: runAsUser: 1000 fsGroup: 1000 containers: - name: gradle image: gradle:4.10-jdk-alpine command: [ cat ] tty: true } } environment { rtServer = rtGradle = buildInfo = CONTAINER_GRADLE_TOOL = /usr/bin/gradle } stages { stage ( Test Container ) { steps { container ( gradle ) { sh which gradle sh uname -a sh gradle -version } } } stage ( Checkout ){ steps { // git https://github.com/demomon/gradle-jenkins-challenge.git checkout scm } } stage ( Preparation ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: http://35.204.238.14/artifactory , credentialsId: art-admin // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: jcenter , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: libs-snapshot-local , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( Build ) { //run the artifactoryPublish gradle task and collect the build info steps { script { buildInfo = rtGradle . run buildFile: build.gradle , tasks: clean build artifactoryPublish } } } stage ( Publish Build Info ) { //collect the environment variables to build info //publish the build info steps { script { buildInfo . env . capture = true rtServer . publishBuildInfo buildInfo } } } } }","title":"Jenkinsfile without Gradle Wrapper"},{"location":"jenkins-pipeline/core-concepts/","text":"Core Concepts Below are some core concepts to understand before building pipelines in Jenkins. Pipeline as Code Step Master vs Nodes Checkout Workspace Stage Sandbox and Script Security Java vs. Groovy Env (object) Stash archive Credentials Tools Build Environment Pipeline Syntax Page Terminology The terminology used in this page is based upon the terms used by Cloudbees as related to Jenkins. If in doubt, please consult the Jenkins Glossary . Pipeline as Code Step A single task; fundamentally steps tell Jenkins what to do inside of a Pipeline or Project. Consider the following piece of pipeline code: 1 2 3 4 5 6 7 8 9 10 11 node { timestamps { stage ( My FIrst Stage ) { if ( isUnix ()) { sh echo this is Unix! } else { bat echo this is windows } } } } The only execution that happens (almost) exclusively on the node (or build slave) are the isUnix() , sh and bat shell commands. Those specific tasks are the steps in pipeline code. Master vs Nodes There are many things to keep in mind about Pipelines in Jenkins. By far the most important are those related to the distinction between Masters and Nodes. Aside from the points below, the key thing to keep in mind: Nodes (build slaves) are designed to executes task, Masters are not. Except for the steps themselves, all of the Pipeline logic, the Groovy conditionals, loops, etc execute on the master . Whether simple or complex! Even inside a node block ! Steps may use executors to do work where appropriate, but each step has a small on-master overhead too. Pipeline code is written as Groovy but the execution model is radically transformed at compile-time to Continuation Passing Style (CPS). This transformation provides valuable safety and durability guarantees for Pipelines, but it comes with trade-offs: Steps can invoke Java and execute fast and efficiently, but Groovy is much slower to run than normal. Groovy logic requires far more memory, because an object-based syntax/block tree is kept in memory. Pipelines persist the program and its state frequently to be able to survive failure of the master. Source: Sam van Oort , Cloudbees Engineer Node A machine which is part of the Jenkins environment and capable of executing Pipelines or Projects. Both the Master and Agents are considered to be Nodes. Master The central, coordinating process which stores configuration, loads plugins, and renders the various user interfaces for Jenkins. What to do? So, if Pipeline code can cause big loads on Master, what should we do than? Try to limit the use of logic in your groovy code Avoid blocking or I/O calls unless explicitly done on a slave via a Step If you need heavy processing, and there isn't a Step, create either a plugin Shared Library Or use a CLI tool via a platform independent language, such as Java or Go Tip If need to do any I/O, use a plugin or anything related to a workspace, you need a node. If you only need to interact with variables, for example for an input form, do this outside of a node block. See Pipeline Input for how that works. Workspace A disposable directory on the file system of a Node where work can be done by a Pipeline or Project. Workspaces are typically left in place after a Build or Pipeline run completes unless specific Workspace cleanup policies have been put in place on the Jenkins Master. The key part of the glossary entry there is disposable directory . There are absolutely no guarantees about Workspaces in pipeline jobs. That said, what you should take care of: always clean your workspace before you start, you don't know the state of the folder you get always clean your workspace after you finish, this way you're less likely to run into problems in subsequent builds a workspace is a temporary folder on a single node's filesystem: so every time you use node {} you have a new workspace after your build is finish or leaving the node otherwise, your workspace should be considered gone: need something from? stash or archive it! Checkout There are several ways to do a checkout in the Jenkins pipeline code. In the groovy DSL you can use the Checkout dsl command, svn shorthand or the git shorthand. 1 2 3 4 5 node { stage ( scm ) { git https://github.com/joostvdg/jishi } } Danger If you use a pipeline from SCM, multi-branch pipeline or a derived job type, beware! Only the Jenkinsfile gets checked out. You still need to checkout the rest of your files yourself! Tip However, when using pipeline from SCM, multi-branch pipeline or a derived job type. You can use a shorthand: checkout scm . This checks out the scm defined in your job (where the Jenkinsfile came from). 1 2 3 4 5 node { stage ( scm ) { checkout scm } } Stage Stage is a step for defining a conceptually distinct subset of the entire Pipeline, for example: \"Build\", \"Test\", and \"Deploy\", which is used by many plugins to visualize or present Jenkins Pipeline status/progress. The stage \"step\" has a primary function and a secondary function. Its primary function is to define the visual boundaries between logically separable parts of the pipeline. For example, you can define SCM, Build, QA, Deploy as stages to tell you where the build currently is or where it failed. The secondary function is to provided a scope for variables. Just like most programming languages, code blocks are a more than just syntactic sugar, they also limit the scope of variables. 1 2 3 4 5 6 7 8 9 10 node { stage ( SCM ) { def myVar = abc checkout scm } stage ( Build ) { sh mvn clean install echo myVar # will fail because the variable doesn t exist here } } Stages in classic view Stages in Blue Ocean view Sandbox and Script Security In Jenkins some plugins - such as the pipeline plugin - allow you to write groovy code that gets executed on the master. This means you could run code on the master that accesses the host machine with the same rights as Jenkins. As is unsafe, Jenkins has some guards against this in the form the sandbox mode and the script security . When you create a pipeline job, you get a inline code editor by default. If you're an administrator you get the option to turn the \"sandbox\" mode of. If you use a pipeline from SCM or any of the higher abstraction pipeline job types (Multibranch Pipeline, BitBucket Team) you are always running in sandbox mode. When you're in sandbox mode, your script will run past the script security. This uses a whitelisting technique to block dangerous or undesired methods, but is does so in a very restrictive manner. It could be you're doing something that is safe but still gets blocked. An administrator can then go to the script approval page (under Jenkins Administration) and approve your script. For more details, please consult Script Security plugin page. Example error 1 2 3 4 5 6 7 8 org . jenkinsci . plugins . scriptsecurity . sandbox . RejectedAccessException : unclassified staticMethod org . tmatesoft . svn . core . internal . io . dav . DAVRepositoryFactory create org . tmatesoft . svn . core . SVNURL at org . jenkinsci . plugins . scriptsecurity . sandbox . groovy . SandboxInterceptor . onStaticCall ( SandboxInterceptor . java : 138 ) at org . kohsuke . groovy . sandbox . impl . Checker$2 . call ( Checker . java : 180 ) at org . kohsuke . groovy . sandbox . impl . Checker . checkedStaticCall ( Checker . java : 177 ) at org . kohsuke . groovy . sandbox . impl . Checker . checkedCall ( Checker . java : 91 ) at com . cloudbees . groovy . cps . sandbox . SandboxInvoker . methodCall ( SandboxInvoker . java : 16 ) at WorkflowScript . run ( WorkflowScript : 12 ) at ___cps . transform___ ( Native Method ) Tip There are three ways to deal with these errors. go to manage jenkins script approval and approve the script use a Shared Library use a CLI tool/script via a shell command to do what you need to do Java vs. Groovy The pipeline code has to be written in groovy and therefor can also use java code. Two big difference to note: the usage of double quoted string (gstring, interpreted) and single quoted strings (literal) def abc = xyz # is a literal echo $abc # prints $ abc echo $abc # prints xyz no use of ; Unfortunately, due to the way the Pipeline code is processed, many of the groovy features don't work or don't work as expected. Things like the lambda's and for-each loops don't work well and are best avoided. In these situations, it is best to keep to the standard syntax of Java. For more information on how the groovy is being processed, it is best to read the technical-design . Env (object) The env object is an object that is available to use in any pipeline script. The env object allows you to store objects and variables to be used anywhere during the script. So things can be shared between nodes, the master and nodes and code blocks. Why would you want to use it? As in general, global variables are a bad practice. But if you need to have variables to be available through the execution on different machines (master, nodes) it is good to use this. Also the env object contains context variables, such as BRANCH_NAME, JOB_NAME and so one. For a complete overview, view the pipeline syntax page. Don't use the env object in functions, always feed them the parameters directly. Only use it in the \"pipeline flow\" and use it for the parameters of the methods. 1 2 3 4 5 6 7 8 node { stage ( SCM ) { checkout scm } stage ( Echo ){ echo Branch=$env.BRANCH_NAME // will print Branch=master } } Stash archive If you need to store files for keeping for later, there are two options available stash and archive . Both should be avoided as they cause heavy I/O traffic, usually between the Node and Master. For more specific information, please consult the Pipeline Syntax Page. Stash Stash allows you to copy files from the current workspace to a temp folder in the workspace in the master. If you're currently on a different machine it will copy them one by one over the network, keep this in mind. The files can only be retrieved during the pipeline execution and you can do so via the unstash command. 1 2 3 4 5 6 7 8 9 10 11 node ( Machine1 ) { stage ( A ) { // generate some files stash excludes: secret.txt , includes: *.txt , name: abc } } node ( Machine2 ) { stage ( B ) { unstash abc } } Saves a set of files for use later in the same build, generally on another node/workspace. Stashed files are not otherwise available and are generally discarded at the end of the build. Note that the stash and unstash steps are designed for use with small files. For large data transfers, use the External Workspace Manager plugin, or use an external repository manager such as Nexus or Artifactory. Archive archiveArtifacts Archives build output artifacts for later use. As of Jenkins 2.x, you may use the more configurable archiveArtifacts. With archive you can store a file semi-permanently in your job. Semi as the files will be overridden by the latest build. The files you archive will be stored in the Job folder on the master. One usecase is to save a log file from a build tool. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 node { stage ( A ) { try { // do some build } finally { // This step should not normally be used in your script. Consult the inline help for details. archive excludes: useless.log , includes: *.log // Use this instead, but only for permanent files, or external logfiles archiveArtifacts allowEmptyArchive: true , artifacts: *.log , excludes: useless.log , fingerprint: true , onlyIfSuccessful: true } } } Credentials In many pipelines you will have to deal with external systems, requiring credentials. Jenkins has the Credentials API which you can also utilize in the pipeline. You can use do this via the Credentials and Credentials Binding plugins, the first is the core plugin the second provides the integration for the pipeline. The best way to generate the required code snippet, is to go to the pipeline syntax page, select withCredentials and configure what you need. 1 2 3 4 5 6 7 node { stage ( someRemoteCall ) { withCredentials ([ usernameColonPassword ( credentialsId: someCredentialsId , variable: USRPASS )]) { sh curl -u $env.USRPASS $URL } } } For more examples, please consult Cloudbees' Injecting-Secrets-into-Jenkins-Build-Jobs blog post. Tools Build Environment Jenkins would not be Jenkins without the direct support for the build tools, such as JDK's, SDK's, Maven, Ant what have you not. So, how do you use them in the pipeline? Unfortunately, this is a bit more cumbersome than it is in a freestyle (or legacy ) job. You have to do two things: retrieve the tool's location via the tool DSL method set the environment variables to suit the tool 1 2 3 4 5 6 7 8 9 10 11 12 13 14 node { stage ( Maven ) { String jdk = tool name: jdk_8 , type: jdk String maven = tool name: maven_3.5.0 , type: maven withEnv ([ JAVA_HOME=$jdk , PATH+MAVEN=${jdk}/bin:${maven}/bin ]) { sh mvn clean install } // or in one go withEnv ([ JAVA_HOME=${ tool jdk_8 } , PATH+MAVEN=${tool maven_3.5.0 }/bin:${env.JAVA_HOME}/bin ]) { sh mvn clean install } } } Pipeline Syntax Page Soooo, do I always have to figure out how to write these code snippets? No, don't worry. You don't have to. At every pipeline job type there is a link called \"Pipeline Syntax\". This gives you a page with a drop down menu, from where you can select all the available steps. Once you select a step, you can use the UI to setup the step and then use the generate button to give you the correct syntax.","title":"Core Concepts"},{"location":"jenkins-pipeline/core-concepts/#core-concepts","text":"Below are some core concepts to understand before building pipelines in Jenkins. Pipeline as Code Step Master vs Nodes Checkout Workspace Stage Sandbox and Script Security Java vs. Groovy Env (object) Stash archive Credentials Tools Build Environment Pipeline Syntax Page","title":"Core Concepts"},{"location":"jenkins-pipeline/core-concepts/#terminology","text":"The terminology used in this page is based upon the terms used by Cloudbees as related to Jenkins. If in doubt, please consult the Jenkins Glossary .","title":"Terminology"},{"location":"jenkins-pipeline/core-concepts/#pipeline-as-code","text":"","title":"Pipeline as Code"},{"location":"jenkins-pipeline/core-concepts/#step","text":"A single task; fundamentally steps tell Jenkins what to do inside of a Pipeline or Project. Consider the following piece of pipeline code: 1 2 3 4 5 6 7 8 9 10 11 node { timestamps { stage ( My FIrst Stage ) { if ( isUnix ()) { sh echo this is Unix! } else { bat echo this is windows } } } } The only execution that happens (almost) exclusively on the node (or build slave) are the isUnix() , sh and bat shell commands. Those specific tasks are the steps in pipeline code.","title":"Step"},{"location":"jenkins-pipeline/core-concepts/#master-vs-nodes","text":"There are many things to keep in mind about Pipelines in Jenkins. By far the most important are those related to the distinction between Masters and Nodes. Aside from the points below, the key thing to keep in mind: Nodes (build slaves) are designed to executes task, Masters are not. Except for the steps themselves, all of the Pipeline logic, the Groovy conditionals, loops, etc execute on the master . Whether simple or complex! Even inside a node block ! Steps may use executors to do work where appropriate, but each step has a small on-master overhead too. Pipeline code is written as Groovy but the execution model is radically transformed at compile-time to Continuation Passing Style (CPS). This transformation provides valuable safety and durability guarantees for Pipelines, but it comes with trade-offs: Steps can invoke Java and execute fast and efficiently, but Groovy is much slower to run than normal. Groovy logic requires far more memory, because an object-based syntax/block tree is kept in memory. Pipelines persist the program and its state frequently to be able to survive failure of the master. Source: Sam van Oort , Cloudbees Engineer","title":"Master vs Nodes"},{"location":"jenkins-pipeline/core-concepts/#node","text":"A machine which is part of the Jenkins environment and capable of executing Pipelines or Projects. Both the Master and Agents are considered to be Nodes.","title":"Node"},{"location":"jenkins-pipeline/core-concepts/#master","text":"The central, coordinating process which stores configuration, loads plugins, and renders the various user interfaces for Jenkins.","title":"Master"},{"location":"jenkins-pipeline/core-concepts/#what-to-do","text":"So, if Pipeline code can cause big loads on Master, what should we do than? Try to limit the use of logic in your groovy code Avoid blocking or I/O calls unless explicitly done on a slave via a Step If you need heavy processing, and there isn't a Step, create either a plugin Shared Library Or use a CLI tool via a platform independent language, such as Java or Go Tip If need to do any I/O, use a plugin or anything related to a workspace, you need a node. If you only need to interact with variables, for example for an input form, do this outside of a node block. See Pipeline Input for how that works.","title":"What to do?"},{"location":"jenkins-pipeline/core-concepts/#workspace","text":"A disposable directory on the file system of a Node where work can be done by a Pipeline or Project. Workspaces are typically left in place after a Build or Pipeline run completes unless specific Workspace cleanup policies have been put in place on the Jenkins Master. The key part of the glossary entry there is disposable directory . There are absolutely no guarantees about Workspaces in pipeline jobs. That said, what you should take care of: always clean your workspace before you start, you don't know the state of the folder you get always clean your workspace after you finish, this way you're less likely to run into problems in subsequent builds a workspace is a temporary folder on a single node's filesystem: so every time you use node {} you have a new workspace after your build is finish or leaving the node otherwise, your workspace should be considered gone: need something from? stash or archive it!","title":"Workspace"},{"location":"jenkins-pipeline/core-concepts/#checkout","text":"There are several ways to do a checkout in the Jenkins pipeline code. In the groovy DSL you can use the Checkout dsl command, svn shorthand or the git shorthand. 1 2 3 4 5 node { stage ( scm ) { git https://github.com/joostvdg/jishi } } Danger If you use a pipeline from SCM, multi-branch pipeline or a derived job type, beware! Only the Jenkinsfile gets checked out. You still need to checkout the rest of your files yourself! Tip However, when using pipeline from SCM, multi-branch pipeline or a derived job type. You can use a shorthand: checkout scm . This checks out the scm defined in your job (where the Jenkinsfile came from). 1 2 3 4 5 node { stage ( scm ) { checkout scm } }","title":"Checkout"},{"location":"jenkins-pipeline/core-concepts/#stage","text":"Stage is a step for defining a conceptually distinct subset of the entire Pipeline, for example: \"Build\", \"Test\", and \"Deploy\", which is used by many plugins to visualize or present Jenkins Pipeline status/progress. The stage \"step\" has a primary function and a secondary function. Its primary function is to define the visual boundaries between logically separable parts of the pipeline. For example, you can define SCM, Build, QA, Deploy as stages to tell you where the build currently is or where it failed. The secondary function is to provided a scope for variables. Just like most programming languages, code blocks are a more than just syntactic sugar, they also limit the scope of variables. 1 2 3 4 5 6 7 8 9 10 node { stage ( SCM ) { def myVar = abc checkout scm } stage ( Build ) { sh mvn clean install echo myVar # will fail because the variable doesn t exist here } }","title":"Stage"},{"location":"jenkins-pipeline/core-concepts/#stages-in-classic-view","text":"","title":"Stages in classic view"},{"location":"jenkins-pipeline/core-concepts/#stages-in-blue-ocean-view","text":"","title":"Stages in Blue Ocean view"},{"location":"jenkins-pipeline/core-concepts/#sandbox-and-script-security","text":"In Jenkins some plugins - such as the pipeline plugin - allow you to write groovy code that gets executed on the master. This means you could run code on the master that accesses the host machine with the same rights as Jenkins. As is unsafe, Jenkins has some guards against this in the form the sandbox mode and the script security . When you create a pipeline job, you get a inline code editor by default. If you're an administrator you get the option to turn the \"sandbox\" mode of. If you use a pipeline from SCM or any of the higher abstraction pipeline job types (Multibranch Pipeline, BitBucket Team) you are always running in sandbox mode. When you're in sandbox mode, your script will run past the script security. This uses a whitelisting technique to block dangerous or undesired methods, but is does so in a very restrictive manner. It could be you're doing something that is safe but still gets blocked. An administrator can then go to the script approval page (under Jenkins Administration) and approve your script. For more details, please consult Script Security plugin page.","title":"Sandbox and Script Security"},{"location":"jenkins-pipeline/core-concepts/#example-error","text":"1 2 3 4 5 6 7 8 org . jenkinsci . plugins . scriptsecurity . sandbox . RejectedAccessException : unclassified staticMethod org . tmatesoft . svn . core . internal . io . dav . DAVRepositoryFactory create org . tmatesoft . svn . core . SVNURL at org . jenkinsci . plugins . scriptsecurity . sandbox . groovy . SandboxInterceptor . onStaticCall ( SandboxInterceptor . java : 138 ) at org . kohsuke . groovy . sandbox . impl . Checker$2 . call ( Checker . java : 180 ) at org . kohsuke . groovy . sandbox . impl . Checker . checkedStaticCall ( Checker . java : 177 ) at org . kohsuke . groovy . sandbox . impl . Checker . checkedCall ( Checker . java : 91 ) at com . cloudbees . groovy . cps . sandbox . SandboxInvoker . methodCall ( SandboxInvoker . java : 16 ) at WorkflowScript . run ( WorkflowScript : 12 ) at ___cps . transform___ ( Native Method ) Tip There are three ways to deal with these errors. go to manage jenkins script approval and approve the script use a Shared Library use a CLI tool/script via a shell command to do what you need to do","title":"Example error"},{"location":"jenkins-pipeline/core-concepts/#java-vs-groovy","text":"The pipeline code has to be written in groovy and therefor can also use java code. Two big difference to note: the usage of double quoted string (gstring, interpreted) and single quoted strings (literal) def abc = xyz # is a literal echo $abc # prints $ abc echo $abc # prints xyz no use of ; Unfortunately, due to the way the Pipeline code is processed, many of the groovy features don't work or don't work as expected. Things like the lambda's and for-each loops don't work well and are best avoided. In these situations, it is best to keep to the standard syntax of Java. For more information on how the groovy is being processed, it is best to read the technical-design .","title":"Java vs. Groovy"},{"location":"jenkins-pipeline/core-concepts/#env-object","text":"The env object is an object that is available to use in any pipeline script. The env object allows you to store objects and variables to be used anywhere during the script. So things can be shared between nodes, the master and nodes and code blocks. Why would you want to use it? As in general, global variables are a bad practice. But if you need to have variables to be available through the execution on different machines (master, nodes) it is good to use this. Also the env object contains context variables, such as BRANCH_NAME, JOB_NAME and so one. For a complete overview, view the pipeline syntax page. Don't use the env object in functions, always feed them the parameters directly. Only use it in the \"pipeline flow\" and use it for the parameters of the methods. 1 2 3 4 5 6 7 8 node { stage ( SCM ) { checkout scm } stage ( Echo ){ echo Branch=$env.BRANCH_NAME // will print Branch=master } }","title":"Env (object)"},{"location":"jenkins-pipeline/core-concepts/#stash-archive","text":"If you need to store files for keeping for later, there are two options available stash and archive . Both should be avoided as they cause heavy I/O traffic, usually between the Node and Master. For more specific information, please consult the Pipeline Syntax Page.","title":"Stash &amp; archive"},{"location":"jenkins-pipeline/core-concepts/#stash","text":"Stash allows you to copy files from the current workspace to a temp folder in the workspace in the master. If you're currently on a different machine it will copy them one by one over the network, keep this in mind. The files can only be retrieved during the pipeline execution and you can do so via the unstash command. 1 2 3 4 5 6 7 8 9 10 11 node ( Machine1 ) { stage ( A ) { // generate some files stash excludes: secret.txt , includes: *.txt , name: abc } } node ( Machine2 ) { stage ( B ) { unstash abc } } Saves a set of files for use later in the same build, generally on another node/workspace. Stashed files are not otherwise available and are generally discarded at the end of the build. Note that the stash and unstash steps are designed for use with small files. For large data transfers, use the External Workspace Manager plugin, or use an external repository manager such as Nexus or Artifactory.","title":"Stash"},{"location":"jenkins-pipeline/core-concepts/#archive-archiveartifacts","text":"Archives build output artifacts for later use. As of Jenkins 2.x, you may use the more configurable archiveArtifacts. With archive you can store a file semi-permanently in your job. Semi as the files will be overridden by the latest build. The files you archive will be stored in the Job folder on the master. One usecase is to save a log file from a build tool. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 node { stage ( A ) { try { // do some build } finally { // This step should not normally be used in your script. Consult the inline help for details. archive excludes: useless.log , includes: *.log // Use this instead, but only for permanent files, or external logfiles archiveArtifacts allowEmptyArchive: true , artifacts: *.log , excludes: useless.log , fingerprint: true , onlyIfSuccessful: true } } }","title":"Archive &amp; archiveArtifacts"},{"location":"jenkins-pipeline/core-concepts/#credentials","text":"In many pipelines you will have to deal with external systems, requiring credentials. Jenkins has the Credentials API which you can also utilize in the pipeline. You can use do this via the Credentials and Credentials Binding plugins, the first is the core plugin the second provides the integration for the pipeline. The best way to generate the required code snippet, is to go to the pipeline syntax page, select withCredentials and configure what you need. 1 2 3 4 5 6 7 node { stage ( someRemoteCall ) { withCredentials ([ usernameColonPassword ( credentialsId: someCredentialsId , variable: USRPASS )]) { sh curl -u $env.USRPASS $URL } } } For more examples, please consult Cloudbees' Injecting-Secrets-into-Jenkins-Build-Jobs blog post.","title":"Credentials"},{"location":"jenkins-pipeline/core-concepts/#tools-build-environment","text":"Jenkins would not be Jenkins without the direct support for the build tools, such as JDK's, SDK's, Maven, Ant what have you not. So, how do you use them in the pipeline? Unfortunately, this is a bit more cumbersome than it is in a freestyle (or legacy ) job. You have to do two things: retrieve the tool's location via the tool DSL method set the environment variables to suit the tool 1 2 3 4 5 6 7 8 9 10 11 12 13 14 node { stage ( Maven ) { String jdk = tool name: jdk_8 , type: jdk String maven = tool name: maven_3.5.0 , type: maven withEnv ([ JAVA_HOME=$jdk , PATH+MAVEN=${jdk}/bin:${maven}/bin ]) { sh mvn clean install } // or in one go withEnv ([ JAVA_HOME=${ tool jdk_8 } , PATH+MAVEN=${tool maven_3.5.0 }/bin:${env.JAVA_HOME}/bin ]) { sh mvn clean install } } }","title":"Tools &amp; Build Environment"},{"location":"jenkins-pipeline/core-concepts/#pipeline-syntax-page","text":"Soooo, do I always have to figure out how to write these code snippets? No, don't worry. You don't have to. At every pipeline job type there is a link called \"Pipeline Syntax\". This gives you a page with a drop down menu, from where you can select all the available steps. Once you select a step, you can use the UI to setup the step and then use the generate button to give you the correct syntax.","title":"Pipeline Syntax Page"},{"location":"jenkins-pipeline/declarative-pipeline/","text":"Declarative Pipeline Declarative Pipeline is a relatively recent addition to Jenkins Pipeline [1] which presents a more simplified and opinionated syntax on top of the Pipeline sub-systems. All valid Declarative Pipelines must be enclosed within a pipeline block, for example: 1 2 3 pipeline { /* insert Declarative Pipeline here */ } Hello World Example 1 2 3 4 5 6 7 8 9 10 pipeline { agent { docker python:3.5.1 } stages { stage ( build ) { steps { sh python --version } } } } MKDocs Build Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 pipeline { agent none options { timeout ( time: 10 , unit: MINUTES ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: 5 )) } stages { stage ( Prepare ){ agent { label docker } steps { parallel ( Clean: { deleteDir () }, NotifySlack: { slackSend channel: cicd , color: #FFFF00 , message: STARTED: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } ) } } stage ( Checkout ){ agent { label docker } steps { git credentialsId: 355df378-e726-4abd-90fa-e723c5c21ad5 , url: git@gitlab.flusso.nl:CICD/ci-cd-docs.git script { env . GIT_COMMIT_HASH = sh returnStdout: true , script: git rev-parse --verify HEAD } } } stage ( Build Docs ) { agent { docker { image caladreas/mkdocs-docker-build-container label docker } } steps { sh mkdocs build } } stage ( Prepare Docker Image ){ agent { label docker } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true , script: docker run --rm -i lukasmartinelli/hadolint Dockerfile if ( lintResult . trim () == ) { println Lint finished with no errors } else { println Error found in Lint println ${lintResult} currentBuild . result = UNSTABLE } } }, // end test dockerfile BuildImage: { sh chmod +x build.sh sh ./build.sh } ) } post { success { sh chmod +x push.sh sh ./push.sh } } } stage ( Update Docker Container ) { agent { label docker } steps { sh chmod +x container-update.sh sh ./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH} } } } post { success { slackSend channel: cicd , color: #00FF00 , message: SUCCESSFUL: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } failure { slackSend channel: cicd , color: #FF0000 , message: FAILED: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } } } Resources Syntax Reference Getting started Notifications","title":"Pipeline (declarative)"},{"location":"jenkins-pipeline/declarative-pipeline/#declarative-pipeline","text":"Declarative Pipeline is a relatively recent addition to Jenkins Pipeline [1] which presents a more simplified and opinionated syntax on top of the Pipeline sub-systems. All valid Declarative Pipelines must be enclosed within a pipeline block, for example: 1 2 3 pipeline { /* insert Declarative Pipeline here */ }","title":"Declarative Pipeline"},{"location":"jenkins-pipeline/declarative-pipeline/#hello-world-example","text":"1 2 3 4 5 6 7 8 9 10 pipeline { agent { docker python:3.5.1 } stages { stage ( build ) { steps { sh python --version } } } }","title":"Hello World Example"},{"location":"jenkins-pipeline/declarative-pipeline/#mkdocs-build-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 pipeline { agent none options { timeout ( time: 10 , unit: MINUTES ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: 5 )) } stages { stage ( Prepare ){ agent { label docker } steps { parallel ( Clean: { deleteDir () }, NotifySlack: { slackSend channel: cicd , color: #FFFF00 , message: STARTED: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } ) } } stage ( Checkout ){ agent { label docker } steps { git credentialsId: 355df378-e726-4abd-90fa-e723c5c21ad5 , url: git@gitlab.flusso.nl:CICD/ci-cd-docs.git script { env . GIT_COMMIT_HASH = sh returnStdout: true , script: git rev-parse --verify HEAD } } } stage ( Build Docs ) { agent { docker { image caladreas/mkdocs-docker-build-container label docker } } steps { sh mkdocs build } } stage ( Prepare Docker Image ){ agent { label docker } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true , script: docker run --rm -i lukasmartinelli/hadolint Dockerfile if ( lintResult . trim () == ) { println Lint finished with no errors } else { println Error found in Lint println ${lintResult} currentBuild . result = UNSTABLE } } }, // end test dockerfile BuildImage: { sh chmod +x build.sh sh ./build.sh } ) } post { success { sh chmod +x push.sh sh ./push.sh } } } stage ( Update Docker Container ) { agent { label docker } steps { sh chmod +x container-update.sh sh ./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH} } } } post { success { slackSend channel: cicd , color: #00FF00 , message: SUCCESSFUL: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } failure { slackSend channel: cicd , color: #FF0000 , message: FAILED: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } } }","title":"MKDocs Build Example"},{"location":"jenkins-pipeline/declarative-pipeline/#resources","text":"Syntax Reference Getting started Notifications","title":"Resources"},{"location":"jenkins-pipeline/global-shared-library/","text":"Global Shared Library https://jenkins.io/doc/book/pipeline/shared-libraries/ When you're making pipelines on Jenkins you will run into the situation that you will want to stay DRY . To share pipeline code there are several ways. SCM: Have a pipeline dsl script in a SCM and load it from there Plugin: A Jenkins plugin that you can call via the pipeline dsl Global Workflow Library: There is a global library for pipeline dsl scripts in the Jekins master Preferred solution Please read the documentation to get a basic idea. Danger When using a Global Library you will always have to import something from this library. This doesn't make sense when you online use functions (via the vars folder). In this case, you have to import nothing, which you do via: \"_\" 1 2 @Library ( FlussoGlobal ) import nl.flusso.Utilities 1 @Library ( FlussoGlobal ) _ Library Directory structure The directory structure of a shared library repository is as follows: 1 2 3 4 5 6 7 8 9 10 11 12 ( root ) +- src # Groovy source files | +- org | +- foo | +- Bar . groovy # for org . foo . Bar class +- vars | +- foo . groovy # for global foo variable / function | +- foo . txt # help for foo variable / function +- resources # resource files ( external libraries only ) | +- org | +- foo | +- bar . json # static helper data for org . foo . Bar The src directory should look like standard Java source directory structure. This directory is added to the classpath when executing Pipelines. The vars directory hosts scripts that define global variables accessible from Pipeline scripts. The basename of each * . groovy file should be a Groovy (~ Java) identifier, conventionally camelCased . The matching * . txt , if present, can contain documentation, processed through the system\u2019s configured markup formatter (so may really be HTML, Markdown, etc., though the txt extension is required). The Groovy source files in these directories get the same \u201cCPS transformation\u201d as your Pipeline scripts. A resources directory allows the libraryResource step to be used from an external library to load associated non-Groovy files. Currently this feature is not supported for internal libraries. Other directories under the root are reserved for future enhancements. Configure libraries in Jenkins The a Jenkins Master you can configure the Global Pipeline Libraries. You can find this in: Manage Jenkins - Configure System - Global Pipeline Libraries You can configure multiple libraries, where the there is a preference for Git repositories. You can select a default version (for example: the master branch), and either allow or disallow overrides to this. To be able to use a different version, you would use the @ in case of Git. 1 @Library ( FlussoGlobal @my - feature - branch ) HelloWorld Example Create Git repository (see below for structure) Configure this Git repository as an \"Global Pipeline Libraries\" entry Name: FlussoGlobal Default Version: master Modern SCM: git Project repository: :CICD/jenkins-pipeline-library.git Create the resources you want in the git repository Use the library in a pipeline Util Class (class) Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/usr/bin/groovy # /src/ nl /flusso/ Utilities . groovy package nl . flusso import java.io.Serializable class Utilities implements Serializable { def steps Utilities ( steps ) { this . steps = steps } def sayHello ( String name ) { steps . sh echo $name } } 1 2 3 4 5 6 7 8 9 @Library ( FlussoGlobal ) import nl.flusso.Utilities def utils = new Utilities ( steps ) node { String name = Joost utils . sayHello ( name ) } Util method (var) Example 1 2 3 4 5 6 #!/usr/bin/groovy # /vars/ sayHello . groovy def call ( name ) { // you can call any valid step functions from your code, just like you can from Pipeline scripts echo Hello world, ${name} } 1 2 3 4 5 @Library ( FlussoGlobal ) _ node { String name = Joost sayHello name } Combining libraries Lets say you want to want to have a core library and multiple specific libraries that utilize these. There are several to do this, we will show two. Import both One way is to explicitly import both libraries in the Jenkinsfile. 1 @Library ([ github.com/joostvdg/jenkins-pipeline-lib , github.com/joostvdg/jenkins-pipeline-go ]) _ Con: you have to import all the required libraries yourself Pro: you can specify the versions of each Implicit Import + Explicit Import You can also configure the core (in this case jenkins-pipeline-lib) as \"loaded implicitly\". This will make anything from this library available by default. Be careful with the naming of the vars though! The resulting Jenkinsfile would then be. 1 @Library ( github.com/joostvdg/jenkins-pipeline-go ) _ Resources implement-reusable-function-call","title":"Pipeline Libraries"},{"location":"jenkins-pipeline/global-shared-library/#global-shared-library","text":"https://jenkins.io/doc/book/pipeline/shared-libraries/ When you're making pipelines on Jenkins you will run into the situation that you will want to stay DRY . To share pipeline code there are several ways. SCM: Have a pipeline dsl script in a SCM and load it from there Plugin: A Jenkins plugin that you can call via the pipeline dsl Global Workflow Library: There is a global library for pipeline dsl scripts in the Jekins master Preferred solution Please read the documentation to get a basic idea. Danger When using a Global Library you will always have to import something from this library. This doesn't make sense when you online use functions (via the vars folder). In this case, you have to import nothing, which you do via: \"_\" 1 2 @Library ( FlussoGlobal ) import nl.flusso.Utilities 1 @Library ( FlussoGlobal ) _","title":"Global Shared Library"},{"location":"jenkins-pipeline/global-shared-library/#library-directory-structure","text":"The directory structure of a shared library repository is as follows: 1 2 3 4 5 6 7 8 9 10 11 12 ( root ) +- src # Groovy source files | +- org | +- foo | +- Bar . groovy # for org . foo . Bar class +- vars | +- foo . groovy # for global foo variable / function | +- foo . txt # help for foo variable / function +- resources # resource files ( external libraries only ) | +- org | +- foo | +- bar . json # static helper data for org . foo . Bar The src directory should look like standard Java source directory structure. This directory is added to the classpath when executing Pipelines. The vars directory hosts scripts that define global variables accessible from Pipeline scripts. The basename of each * . groovy file should be a Groovy (~ Java) identifier, conventionally camelCased . The matching * . txt , if present, can contain documentation, processed through the system\u2019s configured markup formatter (so may really be HTML, Markdown, etc., though the txt extension is required). The Groovy source files in these directories get the same \u201cCPS transformation\u201d as your Pipeline scripts. A resources directory allows the libraryResource step to be used from an external library to load associated non-Groovy files. Currently this feature is not supported for internal libraries. Other directories under the root are reserved for future enhancements.","title":"Library Directory structure"},{"location":"jenkins-pipeline/global-shared-library/#configure-libraries-in-jenkins","text":"The a Jenkins Master you can configure the Global Pipeline Libraries. You can find this in: Manage Jenkins - Configure System - Global Pipeline Libraries You can configure multiple libraries, where the there is a preference for Git repositories. You can select a default version (for example: the master branch), and either allow or disallow overrides to this. To be able to use a different version, you would use the @ in case of Git. 1 @Library ( FlussoGlobal @my - feature - branch )","title":"Configure libraries in Jenkins"},{"location":"jenkins-pipeline/global-shared-library/#helloworld-example","text":"Create Git repository (see below for structure) Configure this Git repository as an \"Global Pipeline Libraries\" entry Name: FlussoGlobal Default Version: master Modern SCM: git Project repository: :CICD/jenkins-pipeline-library.git Create the resources you want in the git repository Use the library in a pipeline","title":"HelloWorld Example"},{"location":"jenkins-pipeline/global-shared-library/#util-class-class-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/usr/bin/groovy # /src/ nl /flusso/ Utilities . groovy package nl . flusso import java.io.Serializable class Utilities implements Serializable { def steps Utilities ( steps ) { this . steps = steps } def sayHello ( String name ) { steps . sh echo $name } } 1 2 3 4 5 6 7 8 9 @Library ( FlussoGlobal ) import nl.flusso.Utilities def utils = new Utilities ( steps ) node { String name = Joost utils . sayHello ( name ) }","title":"Util Class (class) Example"},{"location":"jenkins-pipeline/global-shared-library/#util-method-var-example","text":"1 2 3 4 5 6 #!/usr/bin/groovy # /vars/ sayHello . groovy def call ( name ) { // you can call any valid step functions from your code, just like you can from Pipeline scripts echo Hello world, ${name} } 1 2 3 4 5 @Library ( FlussoGlobal ) _ node { String name = Joost sayHello name }","title":"Util method (var) Example"},{"location":"jenkins-pipeline/global-shared-library/#combining-libraries","text":"Lets say you want to want to have a core library and multiple specific libraries that utilize these. There are several to do this, we will show two.","title":"Combining libraries"},{"location":"jenkins-pipeline/global-shared-library/#import-both","text":"One way is to explicitly import both libraries in the Jenkinsfile. 1 @Library ([ github.com/joostvdg/jenkins-pipeline-lib , github.com/joostvdg/jenkins-pipeline-go ]) _ Con: you have to import all the required libraries yourself Pro: you can specify the versions of each","title":"Import both"},{"location":"jenkins-pipeline/global-shared-library/#implicit-import-explicit-import","text":"You can also configure the core (in this case jenkins-pipeline-lib) as \"loaded implicitly\". This will make anything from this library available by default. Be careful with the naming of the vars though! The resulting Jenkinsfile would then be. 1 @Library ( github.com/joostvdg/jenkins-pipeline-go ) _","title":"Implicit Import + Explicit Import"},{"location":"jenkins-pipeline/global-shared-library/#resources","text":"implement-reusable-function-call","title":"Resources"},{"location":"jenkins-pipeline/groovy-pipeline/","text":"Jenkins Pipelines Warning This style of pipeline definition is deprecated. When possible, please use the declarative version. Jenkins Pipeline is a suite of plugins which supports implementing and integrating continuous delivery pipelines into Jenkins. Pipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines \"as code\" via the Pipeline DSL. There are two ways to create pipelines in Jenkins. Either via the Groovy DSL or via the Declarative pipeline . For more information about the declarative pipeline, read the next page . Hello World Example 1 2 3 4 5 6 7 8 9 10 11 node { timestamps { stage ( My FIrst Stage ) { if ( isUnix ()) { sh echo this is Unix! } else { bat echo this is windows } } } } Resources Getting started Best practices Best practices for scaling Possible Steps","title":"Groovy DSL Pipeline"},{"location":"jenkins-pipeline/groovy-pipeline/#jenkins-pipelines","text":"Warning This style of pipeline definition is deprecated. When possible, please use the declarative version. Jenkins Pipeline is a suite of plugins which supports implementing and integrating continuous delivery pipelines into Jenkins. Pipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines \"as code\" via the Pipeline DSL. There are two ways to create pipelines in Jenkins. Either via the Groovy DSL or via the Declarative pipeline . For more information about the declarative pipeline, read the next page .","title":"Jenkins Pipelines"},{"location":"jenkins-pipeline/groovy-pipeline/#hello-world-example","text":"1 2 3 4 5 6 7 8 9 10 11 node { timestamps { stage ( My FIrst Stage ) { if ( isUnix ()) { sh echo this is Unix! } else { bat echo this is windows } } } }","title":"Hello World Example"},{"location":"jenkins-pipeline/groovy-pipeline/#resources","text":"Getting started Best practices Best practices for scaling Possible Steps","title":"Resources"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/","text":"IDE Integration for Jenkins Pipeline DSL Supported IDE's Currently only Jetbrain's Intelli J's IDEA is supported . This via a Groovy DSL file (.gdsl). Configure Intelli J IDEA Go to a Jenkins Pipeline job and open the Pipeline Syntax page. On the page in the left hand menu, you will see a link to download a Jenkins Master specific Groovy DSL file. Download this and save it into your project's workspace. It will have to be part of your classpath, the easiest way to do this is to add the file as pipeline.gdsl in a/the src folder. For more information, you can read Steffen Gerbert 's blog. Remarks from Kohsuke Kawaguchi More effort in this space will be taken by Cloudbees. But the priority is low compared to other initiatives. Integration of Pipeline Library If you're using the Global Shared Libraries for sharing generic pipeline building blocks, it would be nice to have this awareness in your editor as well. One of the ways to do this, is to checkout the source code of this library and make sure it is compiled. In your editor (assuming Intelli J IDEA) you can then add the compiled classes as dependency (type: classes). This way, at least every class defined in your library is usable as a normal dependency would be. Final configuration Intelli J IDEA","title":"DSL IDE Integration"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#ide-integration-for-jenkins-pipeline-dsl","text":"","title":"IDE Integration for Jenkins Pipeline DSL"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#supported-ides","text":"Currently only Jetbrain's Intelli J's IDEA is supported . This via a Groovy DSL file (.gdsl).","title":"Supported IDE's"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#configure-intelli-j-idea","text":"Go to a Jenkins Pipeline job and open the Pipeline Syntax page. On the page in the left hand menu, you will see a link to download a Jenkins Master specific Groovy DSL file. Download this and save it into your project's workspace. It will have to be part of your classpath, the easiest way to do this is to add the file as pipeline.gdsl in a/the src folder. For more information, you can read Steffen Gerbert 's blog.","title":"Configure Intelli J IDEA"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#remarks-from-kohsuke-kawaguchi","text":"More effort in this space will be taken by Cloudbees. But the priority is low compared to other initiatives.","title":"Remarks from Kohsuke Kawaguchi"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#integration-of-pipeline-library","text":"If you're using the Global Shared Libraries for sharing generic pipeline building blocks, it would be nice to have this awareness in your editor as well. One of the ways to do this, is to checkout the source code of this library and make sure it is compiled. In your editor (assuming Intelli J IDEA) you can then add the compiled classes as dependency (type: classes). This way, at least every class defined in your library is usable as a normal dependency would be.","title":"Integration of Pipeline Library"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#final-configuration-intelli-j-idea","text":"","title":"Final configuration Intelli J IDEA"},{"location":"jenkins-pipeline/input/","text":"Jenkins Pipeline - Input The Jenkins Pipeline has a plugin for dealing with external input. Generally it is used to gather user input (values or approval), but it also has a REST API for this. General Info The Pipeline Input Step allows you to The plugin allows you to capture input in a variety of ways, but there are some gotcha's. If you have a single parameter, it will be returned as a single value If you have multiple parameters, it will be returned as a map The choices for the Choice parameter should be a single line, where values are separated with /n Don't use input within a node {}, as this will block an executor slot .. Examples Single Parameter 1 2 3 4 5 def hello = input id: CustomId , message: Want to continue? , ok: Yes , parameters: [ string ( defaultValue: world , description: , name: hello )] node { println echo $hello } Multiple Parameters 1 2 3 4 5 6 7 def userInput = input id: CustomId , message: Want to continue? , ok: Yes , parameters: [ string ( defaultValue: world , description: , name: hello ), string ( defaultValue: , description: , name: token )] node { def hello = userInput [ hello ] def token = userInput [ token ] println hello=$hello, token=$token } Timeout on Input 1 2 3 4 5 6 def userInput timeout ( time: 10 , unit: SECONDS ) { println Waiting for input userInput = input id: CustomId , message: Want to continue? , ok: Yes , parameters: [ string ( defaultValue: world , description: , name: hello ), string ( defaultValue: , description: , name: token )] } REST API There's a rest API for sending the input to a waiting input step. The format of the url: {JenkinsURL}/ {JenkinsURL}/ {JobURL}/ {Build#}/input/ {Build#}/input/ {InputID}/submit. There are some things to keep in mind: If Jenkins has CSRF protection enabled, you need a Crumb (see below) for the requests Requests are send via POST For supplying values you need to have a JSON with the parameters with as json param You need to supply the proceed value: the value of the ok button, as proceed param You will have to fill in the input id , so it is best to configure a unique input id for the input steps you want to connect to from outside Examples 1 2 3 4 5 6 { parameter : [ { name : hello , value : joost }, { name : token , value : not a token } ] } 1 2 # single parameter curl --user $USER : $PASS -X POST -H Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33 -d json = { parameter : { name : hello , value : joost }} -d proceed = Yes https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit 1 2 # Multiple Parameters curl --user $USER : $PASS -X POST -H Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33 -d json = { parameter : [{ name : hello , value : joost },{ name : token , value : not a token }]} -d proceed = Yes https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit Crumb (secured Jenkins) If Jenkins is secured against CSRF (via Global Security: Prevent Cross Site Request Forgery exploits), any API call requires a Crumb. You can read more about it here . To get a valid crumb you have to send a crumb request as authenticated user. JSON: https://ci.flusso.nl/jenkins/crumbIssuer/api/json XML (parsed): https://ci.flusso.nl/jenkins/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,%22:%22,//crumb )","title":"Input"},{"location":"jenkins-pipeline/input/#jenkins-pipeline-input","text":"The Jenkins Pipeline has a plugin for dealing with external input. Generally it is used to gather user input (values or approval), but it also has a REST API for this.","title":"Jenkins Pipeline - Input"},{"location":"jenkins-pipeline/input/#general-info","text":"The Pipeline Input Step allows you to The plugin allows you to capture input in a variety of ways, but there are some gotcha's. If you have a single parameter, it will be returned as a single value If you have multiple parameters, it will be returned as a map The choices for the Choice parameter should be a single line, where values are separated with /n Don't use input within a node {}, as this will block an executor slot ..","title":"General Info"},{"location":"jenkins-pipeline/input/#examples","text":"","title":"Examples"},{"location":"jenkins-pipeline/input/#single-parameter","text":"1 2 3 4 5 def hello = input id: CustomId , message: Want to continue? , ok: Yes , parameters: [ string ( defaultValue: world , description: , name: hello )] node { println echo $hello }","title":"Single Parameter"},{"location":"jenkins-pipeline/input/#multiple-parameters","text":"1 2 3 4 5 6 7 def userInput = input id: CustomId , message: Want to continue? , ok: Yes , parameters: [ string ( defaultValue: world , description: , name: hello ), string ( defaultValue: , description: , name: token )] node { def hello = userInput [ hello ] def token = userInput [ token ] println hello=$hello, token=$token }","title":"Multiple Parameters"},{"location":"jenkins-pipeline/input/#timeout-on-input","text":"1 2 3 4 5 6 def userInput timeout ( time: 10 , unit: SECONDS ) { println Waiting for input userInput = input id: CustomId , message: Want to continue? , ok: Yes , parameters: [ string ( defaultValue: world , description: , name: hello ), string ( defaultValue: , description: , name: token )] }","title":"Timeout on Input"},{"location":"jenkins-pipeline/input/#rest-api","text":"There's a rest API for sending the input to a waiting input step. The format of the url: {JenkinsURL}/ {JenkinsURL}/ {JobURL}/ {Build#}/input/ {Build#}/input/ {InputID}/submit. There are some things to keep in mind: If Jenkins has CSRF protection enabled, you need a Crumb (see below) for the requests Requests are send via POST For supplying values you need to have a JSON with the parameters with as json param You need to supply the proceed value: the value of the ok button, as proceed param You will have to fill in the input id , so it is best to configure a unique input id for the input steps you want to connect to from outside","title":"REST API"},{"location":"jenkins-pipeline/input/#examples_1","text":"1 2 3 4 5 6 { parameter : [ { name : hello , value : joost }, { name : token , value : not a token } ] } 1 2 # single parameter curl --user $USER : $PASS -X POST -H Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33 -d json = { parameter : { name : hello , value : joost }} -d proceed = Yes https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit 1 2 # Multiple Parameters curl --user $USER : $PASS -X POST -H Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33 -d json = { parameter : [{ name : hello , value : joost },{ name : token , value : not a token }]} -d proceed = Yes https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit","title":"Examples"},{"location":"jenkins-pipeline/input/#crumb-secured-jenkins","text":"If Jenkins is secured against CSRF (via Global Security: Prevent Cross Site Request Forgery exploits), any API call requires a Crumb. You can read more about it here . To get a valid crumb you have to send a crumb request as authenticated user. JSON: https://ci.flusso.nl/jenkins/crumbIssuer/api/json XML (parsed): https://ci.flusso.nl/jenkins/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,%22:%22,//crumb )","title":"Crumb (secured Jenkins)"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/","text":"Parallel Pipeline Building applications can be fun, but it can also cause a lot of wait time 3 . There's many ways to speed up builds, do less tests, get bigger and better hardware, or run some tasks in parallel. Jenkins Pipelines can do parallel stages for a while, even in the Declarative format 1 . Although, doing parallel pipelines Jenkins didn't become awesome until Sequential Stages 2 . We will dive into the magic of Sequential Stages , but first, let's start with building in parallel. Parallel Stages This is a very basic example, we have an application we want to build in Java 11 - latest LTS - and the latest version of Java - now Java 13. As both are running in their own containers, each can leverage its own resources - provided the underlying VM has them available. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 pipeline { agent { kubernetes { label jx-maven-lib yaml apiVersion: v1 kind: Pod spec: containers: - name: maven11 image: maven:3-jdk-11 command: [ cat ] tty: true - name: maven13 image: maven:3-jdk-13 command: [ cat ] tty: true } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/jx-maven-lib.git } } stage ( Run Tests ) { parallel { stage ( Java 11 ) { steps { container ( maven11 ) { sh mvn -V -e -C verify } } } stage ( Java 13 ) { steps { container ( maven13 ) { sh mvn -V -e -C -Pjava13 verify } } } } } } } Visualization This will then look like this: Sequential Before we dive into all the benefits of Sequential Stages , let's look at how the syntax changes. With Parallel Stages we can execute some steps in parallel, but with regards to visualizing individual steps, it is very poor. Sequentual Stages allows us to add Stages in sequence within a Parallel step. Wich is why I usually call them Parallel Sequential stages. In summmary, the syntax now becomes: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 stages { stage ( Checkout ) { stage ( Run Tests ) { parallel { stage ( Java 11 ) { stages { stage ( Build ) { steps {} } } } } } } } The full example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 pipeline { agent { kubernetes { label jx-maven-lib yaml apiVersion: v1 kind: Pod spec: containers: - name: maven11 image: maven:3-jdk-11 command: [ cat ] tty: true - name: maven13 image: maven:3-jdk-13 command: [ cat ] tty: true } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/jx-maven-lib.git } } stage ( Run Tests ) { parallel { stage ( Java 11 ) { stages { stage ( Build ) { steps { container ( maven11 ) { sh mvn -V -e -C package } } } stage ( Test ) { steps { container ( maven11 ) { sh mvn -V -e -C test } } } } } stage ( Java 13 ) { stages { stage ( Build ) { steps { container ( maven13 ) { sh mvn -V -e -C -Pjava13 package } } } stage ( Test ) { steps { container ( maven13 ) { sh mvn -V -e -C -Pjava13 test } } } } } } } } } Visualization The first thing we notice is that we get more indenting and more { } . But, we can also visualize independent stages within the parallel \"streams\". If you're thinking, but can't I now do much more with stage individual stages? You're absolutely right, we'll dive into that next. Sequential With Separate Pods The biggest downside of the previous examples, is that the Kubernetes Pod is always there, including all the containers we need. But what if some parallel tasks take much longer than others? It would be great if the other containers would be removed as soon if we're done with them. With Sequential stages we can achieve this. We first set agent none , to make sure we don't have a Pod running from start to finish. This comes with a price though, now every stage will need to have its own agent defined. Luckily, combining Parallel and Sequential stages, we can give each parallel \"stream\" an agent - a Pod - and have each sequential stage use this. In summary, we do this: Pipeline: no agent parallel: Build stream java 11: agent maven11 stream java 13: agent maven13 parallel: Test stream java 11: agent maven11 Functional Tests API Contract Tests Performance Tests stream java 13: agent maven13 Functional Tests API Contract Tests Performance Tests Deploy Another benefit, is that each stage can leverage every Declarative Directive 4 , such as when { } 5 . In this example, we've used when { branch master } to avoid executing steps when we're not on branch master . To extend this even further, we can now leverage both the dynamic Pod allocation and the When Directive. When combined with beforeAgent true , we won't even spin up the Pod, avoiding unnecessary resource consumption and waiting. 1 2 3 4 5 6 7 8 stage ( Deploy ) { agent { ... } when { branch master beforeAgent true } steps { echo hello } } The complete example now looks like this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 pipeline { agent none stages { stage ( Build ) { parallel { stage ( Java 11 ) { agent { kubernetes { label jxmavenlib-jdk11_build containerTemplate { name maven11 image maven:3-jdk-11 ttyEnabled true command cat } } } steps { container ( maven11 ) { sh mvn -v } } } stage ( Java 13 ) { agent { kubernetes { label jxmavenlib-jdk13-build containerTemplate { name maven13 image maven:3-jdk-13 ttyEnabled true command cat } } } steps { container ( maven13 ) { sh mvn -v } } } } } stage ( Test ) { parallel { stage ( Java 11 ) { agent { kubernetes { label jxmavenlib-jdk11-test containerTemplate { name maven image maven:3-jdk-11 ttyEnabled true command cat } } } stages { stage ( Functional Tests ) { steps { echo Hello } } stage ( API Contract Tests ) { steps { echo Hello } } stage ( Performance Tests ) { when { branch master } steps { echo Hello } } } } stage ( Java 13 ) { agent { kubernetes { label jxmavenlib-jdk13-test containerTemplate { name mavenjdk11 image maven:3-jdk-13 ttyEnabled true command cat } } } stages { stage ( Functional Tests ) { steps { echo Hello } } stage ( API Contract Tests ) { steps { echo Hello } } stage ( Performance Tests ) { when { branch master } steps { echo Hello } } } } } } stage ( Deploy ) { agent { kubernetes { label jxmavenlib-deployment containerTemplate { name pl_deployment image cloudbees/docker-java-with-docker-client ttyEnabled true command cat } } } when { branch master beforeAgent true } steps { echo hello } } } } Visualization When visualized, we can clearly spot which stages were skipped due to the when {} Directives. Sequential With Optional Reusable Pods We're not done yet. One more step to take. While dynamically allocating Pods with containers sounds great, it comes at a cost. If you do something in the build phase and you need the data in latest stages you've now lost it - new Pod = new workspace. There's a few things you can do, but none of them pretty. You can use stash 6 and unstash [\u02c67] but these can be very costly in terms of I/O performance and time. Alternatively, you can either look for externalizing your workspace or keep the Pod around for reuse. Important If you can run more than one build at the same time - concurrent builds - you run the risk of have builds claim Pods another build has done work in. To avoid builds re-using Pods from other runs, you can disable concurrent builds. 1 2 3 options { disableConcurrentBuilds () } Alternatively, you can encode the build number into the name: 1 2 3 4 5 6 agent { kubernetes { label jxmavenlib-jdk11-b${BUILD_NUMBER} yaml ... } } Pod Reuse To reuse a Pod, we have to override some default values in the PodTemplate 10 . idleMinutes : Allows the Pod to remain active for reuse until the configured number of minutes has passed since the last step was executed on it. The configuration below means the Pod can be idle for about 5 minutes before it gets deleted. Additionally, we changed the label to not include the phase name - else we cannot get the same Pod. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 agent { kubernetes { idleMinutes 5 label jxmavenlib-jdk11 yaml spec: containers: - name: maven11 image: maven:3-jdk-11 command: [ cat ] tty: true } } Volume For Externalizing Workspace There are various ways to externalize your workspace, you can leverage NFS 8 or Jenkins workspace related plugins 9 . One way we'll look at here is to leverage a PersistedVolume in Kubernetes - which could be of any kind, incl NFS 8 . We add a volume of type persistentVolumeClaim and point to an existing one by claimName . 1 2 3 4 5 6 7 volumeMounts : - name : build-cache mountPath : /tmp/cache volumes : - name : build-cache persistentVolumeClaim : claimName : azure-managed-disk azure-managed-disk-pvc.yaml 1 2 3 4 5 6 7 8 9 10 11 apiVersion : v1 kind : PersistentVolumeClaim metadata : name : azure-managed-disk spec : accessModes : - ReadWriteOnce storageClassName : managed-premium resources : requests : storage : 5Gi Full Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 pipeline { agent none options { disableConcurrentBuilds () } stages { stage ( Build ) { parallel { stage ( Java 11 ) { agent { kubernetes { idleMinutes 5 label jxmavenlib-jdk11-b${BUILD_NUMBER} yaml spec: containers: - name: maven11 image: maven:3-jdk-11 command: [ cat ] tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2/repository - name: build-cache mountPath: /tmp/cache volumes: - name: maven-cache hostPath: path: /tmp type: Directory - name: build-cache persistentVolumeClaim: claimName: azure-managed-disk } } steps { git https://github.com/joostvdg/jx-maven-lib.git container ( maven11 ) { sh mvn -V -e -C verify sh cp -R target/ /tmp/cache/ sh ls -lath /tmp/cache/ } } } stage ( Java 13 ) { agent { kubernetes { idleMinutes 5 label jxmavenlib-jdk13-b${BUILD_NUMBER} yaml spec: containers: - name: maven13 image: maven:3-jdk-13 command: [ cat ] tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2/repository volumes: - name: maven-cache hostPath: path: /tmp type: Directory } } steps { git https://github.com/joostvdg/jx-maven-lib.git container ( maven13 ) { sh mvn -V -e -C -Pjava13 verify } } } } } stage ( Test ) { parallel { stage ( Java 11 ) { agent { kubernetes { idleMinutes 5 label jxmavenlib-jdk11-b${BUILD_NUMBER} yaml spec: containers: - name: maven11 image: maven:3-jdk-11 command: [ cat ] tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2/repository - name: build-cache mountPath: /tmp/cache volumes: - name: maven-cache hostPath: path: /tmp type: Directory - name: build-cache persistentVolumeClaim: claimName: azure-managed-disk } } stages { stage ( Functional Tests ) { steps { container ( maven11 ) { sh ls -lath /tmp/cache sh cp -R /tmp/cache/ . sh ls -lath } } } stage ( API Contract Tests ) { steps { echo Hello } } stage ( Performance Tests ) { when { branch master } steps { echo Hello } } } } stage ( Java 13 ) { agent { kubernetes { idleMinutes 5 label jxmavenlib-jdk13-b${BUILD_NUMBER} yaml spec: containers: - name: maven13 image: maven:3-jdk-13 command: [ cat ] tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2/repository volumes: - name: maven-cache hostPath: path: /tmp type: Directory } } stages { stage ( Functional Tests ) { steps { container ( maven13 ) { sh ls -lath } } } stage ( API Contract Tests ) { steps { echo Hello } } stage ( Performance Tests ) { when { branch master } steps { echo Hello } } } } } } stage ( Deploy ) { agent { kubernetes { label jxmavenlib-deployment containerTemplate { name pl_deployment image cloudbees/docker-java-with-docker-client ttyEnabled true command cat } } } when { branch master beforeAgent true } steps { echo hello } } } } References Jenkins Pipeline - Parallel Stages Jenkins Pipeline - Introducing Sequential Stages XKCD - Code's Compiling Jenkins Pipeline Syntax Jenkins Declarative Pipeline - When Directive Jenkins Pipeline - Step Stash Jenkins Pipeline - Step Unstash Kubernetes NFS Storage Provisioner Jenkins External Workspace Manager Plugin Jenkins Kubernetes Plugin - template values explained","title":"Parallel Pipelines"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#parallel-pipeline","text":"Building applications can be fun, but it can also cause a lot of wait time 3 . There's many ways to speed up builds, do less tests, get bigger and better hardware, or run some tasks in parallel. Jenkins Pipelines can do parallel stages for a while, even in the Declarative format 1 . Although, doing parallel pipelines Jenkins didn't become awesome until Sequential Stages 2 . We will dive into the magic of Sequential Stages , but first, let's start with building in parallel.","title":"Parallel Pipeline"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#parallel-stages","text":"This is a very basic example, we have an application we want to build in Java 11 - latest LTS - and the latest version of Java - now Java 13. As both are running in their own containers, each can leverage its own resources - provided the underlying VM has them available. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 pipeline { agent { kubernetes { label jx-maven-lib yaml apiVersion: v1 kind: Pod spec: containers: - name: maven11 image: maven:3-jdk-11 command: [ cat ] tty: true - name: maven13 image: maven:3-jdk-13 command: [ cat ] tty: true } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/jx-maven-lib.git } } stage ( Run Tests ) { parallel { stage ( Java 11 ) { steps { container ( maven11 ) { sh mvn -V -e -C verify } } } stage ( Java 13 ) { steps { container ( maven13 ) { sh mvn -V -e -C -Pjava13 verify } } } } } } }","title":"Parallel Stages"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#visualization","text":"This will then look like this:","title":"Visualization"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#sequential","text":"Before we dive into all the benefits of Sequential Stages , let's look at how the syntax changes. With Parallel Stages we can execute some steps in parallel, but with regards to visualizing individual steps, it is very poor. Sequentual Stages allows us to add Stages in sequence within a Parallel step. Wich is why I usually call them Parallel Sequential stages. In summmary, the syntax now becomes: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 stages { stage ( Checkout ) { stage ( Run Tests ) { parallel { stage ( Java 11 ) { stages { stage ( Build ) { steps {} } } } } } } } The full example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 pipeline { agent { kubernetes { label jx-maven-lib yaml apiVersion: v1 kind: Pod spec: containers: - name: maven11 image: maven:3-jdk-11 command: [ cat ] tty: true - name: maven13 image: maven:3-jdk-13 command: [ cat ] tty: true } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/jx-maven-lib.git } } stage ( Run Tests ) { parallel { stage ( Java 11 ) { stages { stage ( Build ) { steps { container ( maven11 ) { sh mvn -V -e -C package } } } stage ( Test ) { steps { container ( maven11 ) { sh mvn -V -e -C test } } } } } stage ( Java 13 ) { stages { stage ( Build ) { steps { container ( maven13 ) { sh mvn -V -e -C -Pjava13 package } } } stage ( Test ) { steps { container ( maven13 ) { sh mvn -V -e -C -Pjava13 test } } } } } } } } }","title":"Sequential"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#visualization_1","text":"The first thing we notice is that we get more indenting and more { } . But, we can also visualize independent stages within the parallel \"streams\". If you're thinking, but can't I now do much more with stage individual stages? You're absolutely right, we'll dive into that next.","title":"Visualization"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#sequential-with-separate-pods","text":"The biggest downside of the previous examples, is that the Kubernetes Pod is always there, including all the containers we need. But what if some parallel tasks take much longer than others? It would be great if the other containers would be removed as soon if we're done with them. With Sequential stages we can achieve this. We first set agent none , to make sure we don't have a Pod running from start to finish. This comes with a price though, now every stage will need to have its own agent defined. Luckily, combining Parallel and Sequential stages, we can give each parallel \"stream\" an agent - a Pod - and have each sequential stage use this. In summary, we do this: Pipeline: no agent parallel: Build stream java 11: agent maven11 stream java 13: agent maven13 parallel: Test stream java 11: agent maven11 Functional Tests API Contract Tests Performance Tests stream java 13: agent maven13 Functional Tests API Contract Tests Performance Tests Deploy Another benefit, is that each stage can leverage every Declarative Directive 4 , such as when { } 5 . In this example, we've used when { branch master } to avoid executing steps when we're not on branch master . To extend this even further, we can now leverage both the dynamic Pod allocation and the When Directive. When combined with beforeAgent true , we won't even spin up the Pod, avoiding unnecessary resource consumption and waiting. 1 2 3 4 5 6 7 8 stage ( Deploy ) { agent { ... } when { branch master beforeAgent true } steps { echo hello } } The complete example now looks like this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 pipeline { agent none stages { stage ( Build ) { parallel { stage ( Java 11 ) { agent { kubernetes { label jxmavenlib-jdk11_build containerTemplate { name maven11 image maven:3-jdk-11 ttyEnabled true command cat } } } steps { container ( maven11 ) { sh mvn -v } } } stage ( Java 13 ) { agent { kubernetes { label jxmavenlib-jdk13-build containerTemplate { name maven13 image maven:3-jdk-13 ttyEnabled true command cat } } } steps { container ( maven13 ) { sh mvn -v } } } } } stage ( Test ) { parallel { stage ( Java 11 ) { agent { kubernetes { label jxmavenlib-jdk11-test containerTemplate { name maven image maven:3-jdk-11 ttyEnabled true command cat } } } stages { stage ( Functional Tests ) { steps { echo Hello } } stage ( API Contract Tests ) { steps { echo Hello } } stage ( Performance Tests ) { when { branch master } steps { echo Hello } } } } stage ( Java 13 ) { agent { kubernetes { label jxmavenlib-jdk13-test containerTemplate { name mavenjdk11 image maven:3-jdk-13 ttyEnabled true command cat } } } stages { stage ( Functional Tests ) { steps { echo Hello } } stage ( API Contract Tests ) { steps { echo Hello } } stage ( Performance Tests ) { when { branch master } steps { echo Hello } } } } } } stage ( Deploy ) { agent { kubernetes { label jxmavenlib-deployment containerTemplate { name pl_deployment image cloudbees/docker-java-with-docker-client ttyEnabled true command cat } } } when { branch master beforeAgent true } steps { echo hello } } } }","title":"Sequential With Separate Pods"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#visualization_2","text":"When visualized, we can clearly spot which stages were skipped due to the when {} Directives.","title":"Visualization"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#sequential-with-optional-reusable-pods","text":"We're not done yet. One more step to take. While dynamically allocating Pods with containers sounds great, it comes at a cost. If you do something in the build phase and you need the data in latest stages you've now lost it - new Pod = new workspace. There's a few things you can do, but none of them pretty. You can use stash 6 and unstash [\u02c67] but these can be very costly in terms of I/O performance and time. Alternatively, you can either look for externalizing your workspace or keep the Pod around for reuse. Important If you can run more than one build at the same time - concurrent builds - you run the risk of have builds claim Pods another build has done work in. To avoid builds re-using Pods from other runs, you can disable concurrent builds. 1 2 3 options { disableConcurrentBuilds () } Alternatively, you can encode the build number into the name: 1 2 3 4 5 6 agent { kubernetes { label jxmavenlib-jdk11-b${BUILD_NUMBER} yaml ... } }","title":"Sequential With Optional &amp; Reusable Pods"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#pod-reuse","text":"To reuse a Pod, we have to override some default values in the PodTemplate 10 . idleMinutes : Allows the Pod to remain active for reuse until the configured number of minutes has passed since the last step was executed on it. The configuration below means the Pod can be idle for about 5 minutes before it gets deleted. Additionally, we changed the label to not include the phase name - else we cannot get the same Pod. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 agent { kubernetes { idleMinutes 5 label jxmavenlib-jdk11 yaml spec: containers: - name: maven11 image: maven:3-jdk-11 command: [ cat ] tty: true } }","title":"Pod Reuse"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#volume-for-externalizing-workspace","text":"There are various ways to externalize your workspace, you can leverage NFS 8 or Jenkins workspace related plugins 9 . One way we'll look at here is to leverage a PersistedVolume in Kubernetes - which could be of any kind, incl NFS 8 . We add a volume of type persistentVolumeClaim and point to an existing one by claimName . 1 2 3 4 5 6 7 volumeMounts : - name : build-cache mountPath : /tmp/cache volumes : - name : build-cache persistentVolumeClaim : claimName : azure-managed-disk azure-managed-disk-pvc.yaml 1 2 3 4 5 6 7 8 9 10 11 apiVersion : v1 kind : PersistentVolumeClaim metadata : name : azure-managed-disk spec : accessModes : - ReadWriteOnce storageClassName : managed-premium resources : requests : storage : 5Gi","title":"Volume For Externalizing Workspace"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#full-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 pipeline { agent none options { disableConcurrentBuilds () } stages { stage ( Build ) { parallel { stage ( Java 11 ) { agent { kubernetes { idleMinutes 5 label jxmavenlib-jdk11-b${BUILD_NUMBER} yaml spec: containers: - name: maven11 image: maven:3-jdk-11 command: [ cat ] tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2/repository - name: build-cache mountPath: /tmp/cache volumes: - name: maven-cache hostPath: path: /tmp type: Directory - name: build-cache persistentVolumeClaim: claimName: azure-managed-disk } } steps { git https://github.com/joostvdg/jx-maven-lib.git container ( maven11 ) { sh mvn -V -e -C verify sh cp -R target/ /tmp/cache/ sh ls -lath /tmp/cache/ } } } stage ( Java 13 ) { agent { kubernetes { idleMinutes 5 label jxmavenlib-jdk13-b${BUILD_NUMBER} yaml spec: containers: - name: maven13 image: maven:3-jdk-13 command: [ cat ] tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2/repository volumes: - name: maven-cache hostPath: path: /tmp type: Directory } } steps { git https://github.com/joostvdg/jx-maven-lib.git container ( maven13 ) { sh mvn -V -e -C -Pjava13 verify } } } } } stage ( Test ) { parallel { stage ( Java 11 ) { agent { kubernetes { idleMinutes 5 label jxmavenlib-jdk11-b${BUILD_NUMBER} yaml spec: containers: - name: maven11 image: maven:3-jdk-11 command: [ cat ] tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2/repository - name: build-cache mountPath: /tmp/cache volumes: - name: maven-cache hostPath: path: /tmp type: Directory - name: build-cache persistentVolumeClaim: claimName: azure-managed-disk } } stages { stage ( Functional Tests ) { steps { container ( maven11 ) { sh ls -lath /tmp/cache sh cp -R /tmp/cache/ . sh ls -lath } } } stage ( API Contract Tests ) { steps { echo Hello } } stage ( Performance Tests ) { when { branch master } steps { echo Hello } } } } stage ( Java 13 ) { agent { kubernetes { idleMinutes 5 label jxmavenlib-jdk13-b${BUILD_NUMBER} yaml spec: containers: - name: maven13 image: maven:3-jdk-13 command: [ cat ] tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2/repository volumes: - name: maven-cache hostPath: path: /tmp type: Directory } } stages { stage ( Functional Tests ) { steps { container ( maven13 ) { sh ls -lath } } } stage ( API Contract Tests ) { steps { echo Hello } } stage ( Performance Tests ) { when { branch master } steps { echo Hello } } } } } } stage ( Deploy ) { agent { kubernetes { label jxmavenlib-deployment containerTemplate { name pl_deployment image cloudbees/docker-java-with-docker-client ttyEnabled true command cat } } } when { branch master beforeAgent true } steps { echo hello } } } }","title":"Full Example"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#references","text":"Jenkins Pipeline - Parallel Stages Jenkins Pipeline - Introducing Sequential Stages XKCD - Code's Compiling Jenkins Pipeline Syntax Jenkins Declarative Pipeline - When Directive Jenkins Pipeline - Step Stash Jenkins Pipeline - Step Unstash Kubernetes NFS Storage Provisioner Jenkins External Workspace Manager Plugin Jenkins Kubernetes Plugin - template values explained","title":"References"},{"location":"jenkins-pipeline/job-types/","text":"Jenkins Pipeline Job Types Jenkins Pipelines are written as a Jenkinsfile and can have either a Declarative 1 or Scripted 2 format. Whenever possible for as much as possible, always write your pipelines in Declarative format. How they are configured and run is a classic case of it depends ... . This page goes into the job types available and when to use which and why. Caution This piece is full of personal opinions. While based on years of experience with writing Jenkins Pipelines and helping customers and clients with reviewing theirs, they're still just an opinion. Job Types We classify the job types as follows: Something that isn't a pipeline job and you should never ever use (I'm looking at you Freestyle ) Pipeline Multibranch Pipeline Organization Job (named after the most commonly used GitHub Organization Job) Pipeline A Pipeline job 9 is the most fundamental building block available. You can create a New Item in Jenkins and choose the type Pipeline . Here you can choose two different configurations, either you write the Pipeline inline or from SCM . Inline means that you write the Pipeline directly in the Job configuration itself. Where from SCM let's check out a Jenkinsfile from an SCM (such as GitHub). Aside from testing Snippets, you should never use this Job type. It is very limited in what it offers beyond running a Pipeline. Tip I use this Job type for testing sections of a more extensive Pipeline or new features. Once it works, I will commit it to an SCM source and use one of the other Job types. Multibranch Pipeline Now, this is starting to look like something we can use! The Multibranch Pipeline 3 allows you to map a Pipeline to an SCM Repository. Where a Pipeline Job maps to a specific Branch (or Branch scheme) of a Repository, a Multibranch Pipeline Job targets the entire repository. This means that it scans the repository for its Branches. For each branch that meets the criteria you set, it creates a Pipeline Job targeting this specific branch. By default, the only criteria configured is the existence of a file called Jenkinsfile . However, you can add other criteria such as a Branch Naming Scheme - either based on Regex or Wildcards. Benefits Beyond Pipeline Job Aside from not having to manage Job configurations for branches, as Multibranch handles both creation and deletion, this job type has additional benefits. It automatically populates environment variables such as BRANCH_NAME and many other related to the Git Commit. This allows you to take full leverage of the When 10 Directive. 1 2 3 4 5 6 7 8 9 10 pipeline { stages { stage ( Only Run If Master Branch ) { when { branch master } steps { echo This only runs iff we are in the master branch } } } } To leverage the Multibranch Pipeline well, I recommend going through this excellent end-to-end tutorial 4 . Branch Sources The Multibranch Pipeline Job lets you configure to wich SCM Repository it should map to. This can be merely pointing to a Git or SVN (please don't) repository, but more importantly, you can map it to specific Branch Sources . What are Branch Sources ? These are specific Git providers that 1) give Jenkins more information, and 2) have their own unique handler plugins providing better integration with the Git provider. In most cases, this means you can also build Pull Requests and deal with Tags in a way that leverages Jenkins Pipeline Directives such as the already mentioned When 10 directive. As of this writing - August 2019 - there are four such Branch Source plugins available; GitHub 5 (most mature), GitLab 8 (latest addition), Bitbucket 6 , and Gitea 7 . Organization Job I name these types of jobs after the first one that worked very well: GitHub Organization Job. This plugin is replaced by GitHub Branch Source , now the common name for the plugins providing this kind of integration. The goal of Organization Pipeline Job is to scan an entire namespace of Git repositories for branches containing Jenkinsfile . In the case of GitHub, this is called an Organization , in the case of Bitbucket cloud, this is a Team and so forth. Creating several benefits, the essential being, you create a single job within Jenkins and manage the Pipeline for every application in that Organization/Team/Whatever. Moreover, don't worry, there are ample settings to configure if you want to limit the job to specific branches or repositories. Benefits reduced maintenance : a single job maintained in Jenkins takes care of all applications in an Organization/Team/... including creation and deletion of the repositories and branches! reduced complexity : due to the configuration working as a standardization, there are no special snowflakes, there's just one way a job gets configured, and that includes the naming visible coupling : a Pipeline is coupled to a repository, the job clearly shows this connection with links and icons manages status reporting : although this does depend on the specific integration, in general, every Pipeline automatically reports the build status of each commit to your SCM without requiring configuration Models What people often fail to realize, is that the mentioned Pipeline Job Types are not on their own. They are part of a single model, where they build on top of each other. The Pipeline job being the smallest building block, then Multibranch , and last but not least, Organization job extends Multibranch . I've created some models to try and visualize this. Abstract At the very abstract level, we see this babushka doll type of layering. Basic Starting from the outer layer, we have a GitHub Organization job. In essence, it is a folder that contains Multbranch Pipeline jobs. A Multibranch Pipeline job is also a folder, and it contains Pipeline jobs. Mapping This layering is created by mapping the different types of jobs to corresponding resources in the SCM you are talking to. In the example of a GitHub Organization job, we're talking to a GitHub Organization. Jenkins scans each repository in that Organization, and per repository, scan each branch. For each branch that meets the criteria - by default the presence of a Jenkinsfile - it creates the Multibranch Pipeline job for that repository. The Multibranch Pipeline job creates a Pipeline job for each branch that met the criteria. So as summary: GitHub Organization Job - GitHub Organization / Bitbucket Team / ... Multibranch Pipeline Job - Repository Pipeline Job - Branch References Jenkins Declarative Pipeline Fundamentals Jenkins Scripted Pipeline Fundamentals Jenkins Pipeline Multibranch Plugin Tutorial On Creating a Multibranch Pipeline GitHub Branch Source Plugin Bitbucket Branch Source Plugin Integrate Jenkins And Gitea GitLab Branch Source Plugin Create Pipeline Job Via UI Pipeline Syntax - When Directive","title":"Job Types"},{"location":"jenkins-pipeline/job-types/#jenkins-pipeline-job-types","text":"Jenkins Pipelines are written as a Jenkinsfile and can have either a Declarative 1 or Scripted 2 format. Whenever possible for as much as possible, always write your pipelines in Declarative format. How they are configured and run is a classic case of it depends ... . This page goes into the job types available and when to use which and why. Caution This piece is full of personal opinions. While based on years of experience with writing Jenkins Pipelines and helping customers and clients with reviewing theirs, they're still just an opinion.","title":"Jenkins Pipeline Job Types"},{"location":"jenkins-pipeline/job-types/#job-types","text":"We classify the job types as follows: Something that isn't a pipeline job and you should never ever use (I'm looking at you Freestyle ) Pipeline Multibranch Pipeline Organization Job (named after the most commonly used GitHub Organization Job)","title":"Job Types"},{"location":"jenkins-pipeline/job-types/#pipeline","text":"A Pipeline job 9 is the most fundamental building block available. You can create a New Item in Jenkins and choose the type Pipeline . Here you can choose two different configurations, either you write the Pipeline inline or from SCM . Inline means that you write the Pipeline directly in the Job configuration itself. Where from SCM let's check out a Jenkinsfile from an SCM (such as GitHub). Aside from testing Snippets, you should never use this Job type. It is very limited in what it offers beyond running a Pipeline. Tip I use this Job type for testing sections of a more extensive Pipeline or new features. Once it works, I will commit it to an SCM source and use one of the other Job types.","title":"Pipeline"},{"location":"jenkins-pipeline/job-types/#multibranch-pipeline","text":"Now, this is starting to look like something we can use! The Multibranch Pipeline 3 allows you to map a Pipeline to an SCM Repository. Where a Pipeline Job maps to a specific Branch (or Branch scheme) of a Repository, a Multibranch Pipeline Job targets the entire repository. This means that it scans the repository for its Branches. For each branch that meets the criteria you set, it creates a Pipeline Job targeting this specific branch. By default, the only criteria configured is the existence of a file called Jenkinsfile . However, you can add other criteria such as a Branch Naming Scheme - either based on Regex or Wildcards.","title":"Multibranch Pipeline"},{"location":"jenkins-pipeline/job-types/#benefits-beyond-pipeline-job","text":"Aside from not having to manage Job configurations for branches, as Multibranch handles both creation and deletion, this job type has additional benefits. It automatically populates environment variables such as BRANCH_NAME and many other related to the Git Commit. This allows you to take full leverage of the When 10 Directive. 1 2 3 4 5 6 7 8 9 10 pipeline { stages { stage ( Only Run If Master Branch ) { when { branch master } steps { echo This only runs iff we are in the master branch } } } } To leverage the Multibranch Pipeline well, I recommend going through this excellent end-to-end tutorial 4 .","title":"Benefits Beyond Pipeline Job"},{"location":"jenkins-pipeline/job-types/#branch-sources","text":"The Multibranch Pipeline Job lets you configure to wich SCM Repository it should map to. This can be merely pointing to a Git or SVN (please don't) repository, but more importantly, you can map it to specific Branch Sources . What are Branch Sources ? These are specific Git providers that 1) give Jenkins more information, and 2) have their own unique handler plugins providing better integration with the Git provider. In most cases, this means you can also build Pull Requests and deal with Tags in a way that leverages Jenkins Pipeline Directives such as the already mentioned When 10 directive. As of this writing - August 2019 - there are four such Branch Source plugins available; GitHub 5 (most mature), GitLab 8 (latest addition), Bitbucket 6 , and Gitea 7 .","title":"Branch Sources"},{"location":"jenkins-pipeline/job-types/#organization-job","text":"I name these types of jobs after the first one that worked very well: GitHub Organization Job. This plugin is replaced by GitHub Branch Source , now the common name for the plugins providing this kind of integration. The goal of Organization Pipeline Job is to scan an entire namespace of Git repositories for branches containing Jenkinsfile . In the case of GitHub, this is called an Organization , in the case of Bitbucket cloud, this is a Team and so forth. Creating several benefits, the essential being, you create a single job within Jenkins and manage the Pipeline for every application in that Organization/Team/Whatever. Moreover, don't worry, there are ample settings to configure if you want to limit the job to specific branches or repositories.","title":"Organization Job"},{"location":"jenkins-pipeline/job-types/#benefits","text":"reduced maintenance : a single job maintained in Jenkins takes care of all applications in an Organization/Team/... including creation and deletion of the repositories and branches! reduced complexity : due to the configuration working as a standardization, there are no special snowflakes, there's just one way a job gets configured, and that includes the naming visible coupling : a Pipeline is coupled to a repository, the job clearly shows this connection with links and icons manages status reporting : although this does depend on the specific integration, in general, every Pipeline automatically reports the build status of each commit to your SCM without requiring configuration","title":"Benefits"},{"location":"jenkins-pipeline/job-types/#models","text":"What people often fail to realize, is that the mentioned Pipeline Job Types are not on their own. They are part of a single model, where they build on top of each other. The Pipeline job being the smallest building block, then Multibranch , and last but not least, Organization job extends Multibranch . I've created some models to try and visualize this.","title":"Models"},{"location":"jenkins-pipeline/job-types/#abstract","text":"At the very abstract level, we see this babushka doll type of layering.","title":"Abstract"},{"location":"jenkins-pipeline/job-types/#basic","text":"Starting from the outer layer, we have a GitHub Organization job. In essence, it is a folder that contains Multbranch Pipeline jobs. A Multibranch Pipeline job is also a folder, and it contains Pipeline jobs.","title":"Basic"},{"location":"jenkins-pipeline/job-types/#mapping","text":"This layering is created by mapping the different types of jobs to corresponding resources in the SCM you are talking to. In the example of a GitHub Organization job, we're talking to a GitHub Organization. Jenkins scans each repository in that Organization, and per repository, scan each branch. For each branch that meets the criteria - by default the presence of a Jenkinsfile - it creates the Multibranch Pipeline job for that repository. The Multibranch Pipeline job creates a Pipeline job for each branch that met the criteria. So as summary: GitHub Organization Job - GitHub Organization / Bitbucket Team / ... Multibranch Pipeline Job - Repository Pipeline Job - Branch","title":"Mapping"},{"location":"jenkins-pipeline/job-types/#references","text":"Jenkins Declarative Pipeline Fundamentals Jenkins Scripted Pipeline Fundamentals Jenkins Pipeline Multibranch Plugin Tutorial On Creating a Multibranch Pipeline GitHub Branch Source Plugin Bitbucket Branch Source Plugin Integrate Jenkins And Gitea GitLab Branch Source Plugin Create Pipeline Job Via UI Pipeline Syntax - When Directive","title":"References"},{"location":"jenkins-pipeline/kaniko-pipelines/","text":"Kaniko Pipelines Kaniko 1 is one of the recommended tools for building Docker images within Kubernetes, especially when you build them as part of a Jenkins Pipeline. I've written about why you should use Kaniko (or similar) tools, the rest assumes you want to use Kaniko within your pipeline. Quote Kaniko is a tool to build container images from a Dockerfile, inside a container or Kubernetes cluster. kaniko doesn't depend on a Docker daemon and executes each command within a Dockerfile completely in userspace. This enables building container images in environments that can't easily or securely run a Docker daemon, such as a standard Kubernetes cluster. For more examples for leveraging Kaniko when using Jenkins in Kubernetes, you can look at the documentation from CloudBees Core 2 . Pipeline Example Note The Kaniko logger uses ANSI Colors, which can be represented via the Jenkins ANSI Color Plugin . If you have the plugin installed, you can do something like the snipped below to render the colors. 1 2 3 4 5 6 7 container ( name: kaniko , shell: /busybox/sh ) { ansiColor ( xterm ) { sh #!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=${REGISTRY}/${REPOSITORY}/${IMAGE} } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 pipeline { agent { kubernetes { label kaniko yaml kind: Pod metadata: name: kaniko spec: containers: - name: golang image: golang:1.12 command: - cat tty: true - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /kaniko/.docker volumes: - name: jenkins-docker-cfg projected: sources: - secret: name: registry-credentials items: - key: .dockerconfigjson path: config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { environment { PATH = /busybox: $PATH REGISTRY = index.docker.io // Configure your own registry REPOSITORY = caladreas IMAGE = cat } steps { container ( name: kaniko , shell: /busybox/sh ) { sh #!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=${REGISTRY}/${REPOSITORY}/${IMAGE} } } } } } Configuration Kaniko relies on a docker secret for directly communicating to a Docker Registry. This can be supplied in various ways, but the most common is to create a Kubernetes Secret of type docker - registry . 1 2 3 4 kubectl create secret docker-registry registry-credentials \\ --docker-username = username \\ --docker-password = password \\ --docker-email = email-address We can then mount it in the pod via Volumes (PodSpec level) and volumeMounts (Container level). 1 2 3 4 5 6 7 8 9 volumes : - name : jenkins-docker-cfg projected : sources : - secret : name : docker-credentials items : - key : .dockerconfigjson path : config.json Azure ACR Of course, there always have to be difference between the Public Cloud Providers (AWS, Azure, Alibaba, GCP). In the case of Kaniko, its Azure that does things differently. Assuming you want to leverage Azure Container Registry (ACR), you're in Azure after all, you will have to do a few things differently. Create ACR You can use the Azure CLI 5 or an Configuration-As-Code Tool such as Terraform 6 . Azure CLI First, create a resource group. 1 az group create --name myResourceGroup --location eastus And then create the ACR. 1 az acr create --resource-group myResourceGroup --name myContainerRegistry007 --sku Basic Terraform We leverage the azurerm backend of terraform 9 10 . 1 2 3 4 5 6 7 8 9 10 11 12 resource azurerm_resource_group acr { name = ${var.resource_group_name}-acr location = ${var.location} } resource azurerm_container_registry acr { name = ${var.container_registry_name} resource_group_name = ${azurerm_resource_group.acr.name} location = ${azurerm_resource_group.k8s.location} sku = Premium admin_enabled = false } Configure Access to ACR Now that we have an ACR, we need to be able to pull and images from and to the registry. This requires access credentials, which we can create in several ways, we'll explore via ServicePrinciple. Via ServicePrinciple Credentials The commands below are taken from the Azure Container Registry documentation about authentication 7 . First, lets setup some values that are not derived from something. 1 2 3 EMAIL = me@example.com SERVICE_PRINCIPAL_NAME = acr-service-principal ACR_NAME = myacrinstance Second, we fetch the basic information about the registry we have. We need this information for the other commands. 1 2 ACR_LOGIN_SERVER = $( az acr show --name $ACR_NAME --query loginServer --output tsv ) ACR_REGISTRY_ID = $( az acr show --name $ACR_NAME --query id --output tsv ) Now we can create a ServicePrinciple with just the rights we need 8 . In the case of Kaniko, we need Push and Pull rights, which are both captured in the role acrpush . 1 2 SP_PASSWD = $( az ad sp create-for-rbac --name http:// $SERVICE_PRINCIPAL_NAME --role acrpush --scopes $ACR_REGISTRY_ID --query password --output tsv ) CLIENT_ID = $( az ad sp show --id http:// $SERVICE_PRINCIPAL_NAME --query appId --output tsv ) 1 kubectl create secret docker-registry registry-credentials --docker-server ${ ACR_LOGIN_SERVER } --docker-username ${ CLIENT_ID } --docker-password ${ SP_PASSWD } --docker-email ${ EMAIL } References Kaniko GitHub CloudBees Guide On Using Kaniko With CloudBees Core Sail CI On Kaniko With Azure Container Registry Create Azure Container Registry With Azure CLI Azure CLI Terraform Azure Container Registry Authentication Documentation Azure Container Registry Roles and Permissions Terraform AzureRM Backend Create AKS Cluster Via Terraform Azure Container Registry Credentials Management","title":"Build Docker Image Kaniko"},{"location":"jenkins-pipeline/kaniko-pipelines/#kaniko-pipelines","text":"Kaniko 1 is one of the recommended tools for building Docker images within Kubernetes, especially when you build them as part of a Jenkins Pipeline. I've written about why you should use Kaniko (or similar) tools, the rest assumes you want to use Kaniko within your pipeline. Quote Kaniko is a tool to build container images from a Dockerfile, inside a container or Kubernetes cluster. kaniko doesn't depend on a Docker daemon and executes each command within a Dockerfile completely in userspace. This enables building container images in environments that can't easily or securely run a Docker daemon, such as a standard Kubernetes cluster. For more examples for leveraging Kaniko when using Jenkins in Kubernetes, you can look at the documentation from CloudBees Core 2 .","title":"Kaniko Pipelines"},{"location":"jenkins-pipeline/kaniko-pipelines/#pipeline-example","text":"Note The Kaniko logger uses ANSI Colors, which can be represented via the Jenkins ANSI Color Plugin . If you have the plugin installed, you can do something like the snipped below to render the colors. 1 2 3 4 5 6 7 container ( name: kaniko , shell: /busybox/sh ) { ansiColor ( xterm ) { sh #!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=${REGISTRY}/${REPOSITORY}/${IMAGE} } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 pipeline { agent { kubernetes { label kaniko yaml kind: Pod metadata: name: kaniko spec: containers: - name: golang image: golang:1.12 command: - cat tty: true - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /kaniko/.docker volumes: - name: jenkins-docker-cfg projected: sources: - secret: name: registry-credentials items: - key: .dockerconfigjson path: config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { environment { PATH = /busybox: $PATH REGISTRY = index.docker.io // Configure your own registry REPOSITORY = caladreas IMAGE = cat } steps { container ( name: kaniko , shell: /busybox/sh ) { sh #!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=${REGISTRY}/${REPOSITORY}/${IMAGE} } } } } }","title":"Pipeline Example"},{"location":"jenkins-pipeline/kaniko-pipelines/#configuration","text":"Kaniko relies on a docker secret for directly communicating to a Docker Registry. This can be supplied in various ways, but the most common is to create a Kubernetes Secret of type docker - registry . 1 2 3 4 kubectl create secret docker-registry registry-credentials \\ --docker-username = username \\ --docker-password = password \\ --docker-email = email-address We can then mount it in the pod via Volumes (PodSpec level) and volumeMounts (Container level). 1 2 3 4 5 6 7 8 9 volumes : - name : jenkins-docker-cfg projected : sources : - secret : name : docker-credentials items : - key : .dockerconfigjson path : config.json","title":"Configuration"},{"location":"jenkins-pipeline/kaniko-pipelines/#azure-acr","text":"Of course, there always have to be difference between the Public Cloud Providers (AWS, Azure, Alibaba, GCP). In the case of Kaniko, its Azure that does things differently. Assuming you want to leverage Azure Container Registry (ACR), you're in Azure after all, you will have to do a few things differently.","title":"Azure &amp; ACR"},{"location":"jenkins-pipeline/kaniko-pipelines/#create-acr","text":"You can use the Azure CLI 5 or an Configuration-As-Code Tool such as Terraform 6 .","title":"Create ACR"},{"location":"jenkins-pipeline/kaniko-pipelines/#azure-cli","text":"First, create a resource group. 1 az group create --name myResourceGroup --location eastus And then create the ACR. 1 az acr create --resource-group myResourceGroup --name myContainerRegistry007 --sku Basic","title":"Azure CLI"},{"location":"jenkins-pipeline/kaniko-pipelines/#terraform","text":"We leverage the azurerm backend of terraform 9 10 . 1 2 3 4 5 6 7 8 9 10 11 12 resource azurerm_resource_group acr { name = ${var.resource_group_name}-acr location = ${var.location} } resource azurerm_container_registry acr { name = ${var.container_registry_name} resource_group_name = ${azurerm_resource_group.acr.name} location = ${azurerm_resource_group.k8s.location} sku = Premium admin_enabled = false }","title":"Terraform"},{"location":"jenkins-pipeline/kaniko-pipelines/#configure-access-to-acr","text":"Now that we have an ACR, we need to be able to pull and images from and to the registry. This requires access credentials, which we can create in several ways, we'll explore via ServicePrinciple.","title":"Configure Access to ACR"},{"location":"jenkins-pipeline/kaniko-pipelines/#via-serviceprinciple-credentials","text":"The commands below are taken from the Azure Container Registry documentation about authentication 7 . First, lets setup some values that are not derived from something. 1 2 3 EMAIL = me@example.com SERVICE_PRINCIPAL_NAME = acr-service-principal ACR_NAME = myacrinstance Second, we fetch the basic information about the registry we have. We need this information for the other commands. 1 2 ACR_LOGIN_SERVER = $( az acr show --name $ACR_NAME --query loginServer --output tsv ) ACR_REGISTRY_ID = $( az acr show --name $ACR_NAME --query id --output tsv ) Now we can create a ServicePrinciple with just the rights we need 8 . In the case of Kaniko, we need Push and Pull rights, which are both captured in the role acrpush . 1 2 SP_PASSWD = $( az ad sp create-for-rbac --name http:// $SERVICE_PRINCIPAL_NAME --role acrpush --scopes $ACR_REGISTRY_ID --query password --output tsv ) CLIENT_ID = $( az ad sp show --id http:// $SERVICE_PRINCIPAL_NAME --query appId --output tsv ) 1 kubectl create secret docker-registry registry-credentials --docker-server ${ ACR_LOGIN_SERVER } --docker-username ${ CLIENT_ID } --docker-password ${ SP_PASSWD } --docker-email ${ EMAIL }","title":"Via ServicePrinciple Credentials"},{"location":"jenkins-pipeline/kaniko-pipelines/#references","text":"Kaniko GitHub CloudBees Guide On Using Kaniko With CloudBees Core Sail CI On Kaniko With Azure Container Registry Create Azure Container Registry With Azure CLI Azure CLI Terraform Azure Container Registry Authentication Documentation Azure Container Registry Roles and Permissions Terraform AzureRM Backend Create AKS Cluster Via Terraform Azure Container Registry Credentials Management","title":"References"},{"location":"jenkins-pipeline/podtemplate-dind/","text":"Docker-in-Docker With PodTemplates First of all, doing Docker-In-Docker is a controversial practice to begin with 1 , creating just as many problems as it solves. Second, if you're in Kubernetes, one should not use Docker directly . Ok, all caveats aside, there can be legitimate reasons for wanting to run Docker containers directly even in Kubernetes. Scenario: Pipeline Framework Based On Docker I created the solution in this article for a client. This client has created a internal framework around Jenkins Pipeline 3 . This pipeline processes the requirements off the pipeline run and spins up the appropriate containers in parallel. Running parallel container can also be done with PodTemplates via the Kubernetes Plugin , which was deemed the way forward. However, as one can expect, you do not rewrite a framework used by dozens of applications unless you're reasonably sure you can get everything to run as it should. In order to bridge this period of running the pipelines as is, while evaluating Jenkins on Kubernetes, we got to work on achieving Docker-in-Docker with Jenkins Kubernetes Plugin 4 . Goal The goal of the exercise, is to prove we can do a single git clone, and spin up multiple containers re-using the original namespace. Docker Socket There's many ways to spin up Docker containers, and I would say the most common one is know as Docker-in-Docker or dind . I find this misleading, because most of the time the second container doesn't run inside the original container, but parallel to it. This is often achieved by mounting the docker socket as a volume - / var / run / docker . sock . I call this: Docker - On - Docker . Because I want to stay close to the term Docker-in-Docker, but also signify it is different. What Docker-In-Docker should be, is that we host a docker daemon in a docker container, which then hosts the new containers in itself. Before we go into the differences, let's discuss why it matters. Remember, the goal is to be able to re-use the git clone in the original container. The default container with Jenkins Kubernetes Plugin is the jnpl , which acts as an ephemeral agent. It mounts a emptyDir {} as temporary volume as its workspace, which means this folder only exists within the Pod (practically speaking). Docker-On-Docker Classic Docker-On-Docker, we have a VM which has a Docker Daemon. We have a Docker client container which mounts the docker socket. The docker client now pretends to talk to a local Docker Daemon, but this redirects to the VM's daemon instead. When we do a docker run , the daemon will spawn a new container next to us. While this is nice, and stays clear from virtulization inception, this new container - let's say, a maven container - cannot access our workspace. Any volume flag we give to our container with - v { origin }:{ target } maps a Host directory to our new container, but not the workspace volume (purple) in our Pod. This is because the new container cannot access any volume inside the Pod. For this, we need true Docker-In-Docker. Jenkinsfile Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 podTemplate ( yaml: apiVersion: v1 kind: Pod spec: containers: - name: docker image: docker:1.11 command: [ cat ] tty: true volumeMounts: - name: dockersock mountPath: /var/run/docker.sock volumes: - name: dockersock hostPath: path: /var/run/docker.sock ) { def image = jenkins/jnlp-slave node ( POD_LABEL ) { stage ( Build Docker image ) { git https://github.com/jenkinsci/docker-jnlp-slave.git container ( docker ) { sh docker build -t ${image} . } } } } Docker-In-Docker True Docker-In-Docker means spinning up another container inside an existing container - as illustrated below. This has a big benefit. The container now has access to all the volumes inside the Pod. We can now do a git clone and use docker build on this workspace. In addition, we can also spin up a new container and give it the workspace as a volume and have it build with it. Pod Configuration The examples below contain some specific configuration elements required to make the magic happen. So let's explore each configuration item to see what it does and why we set it. Client Container The Docker Client has to know where the Docker Daemon is. We do this by setting the environment variable DOCKER_HOST 2 to tcp : // localhost : 2375 . If we put the Daemon container within the same Pod, we can use localhost and then the default port of the daemon, being 2375 . 1 2 3 env : - name : DOCKER_HOST value : tcp://localhost:2375 The client container will directly terminate unless we give it something to do. If we put it to sleep for a significant amount of time, it should be there to execute our every command! 1 command : [ sleep , 99d ] Daemon Container As of Docker 18.09 the Docker Daemon container can use TLS, as of 19.03 it is configured by default. 2 So either you work around this or get the certificates sorted. The easiest way around it, is to set the environment variable DOCKER_TLS_CERTDIR to , which will disable TLS. Caution Disabling TLS is at your OWN risk. Read the docs carefully for what this means 2 . 1 2 3 env : - name : DOCKER_TLS_CERTDIR value : In order for the Docker Daemon to create containers, it needs the privileged flag set to true. This should be another warning to you, to be careful of what you do with it! 1 2 securityContext : privileged : true Last but not least, we would not want to store all the Docker Daemon data in the pods. We might as well leverage local storage or a some specific volume. The volume and volumeMounts configuration of the Daemon container ensures we can leverage the Docker build and image caches. 1 2 3 4 5 6 7 8 volumeMounts : - name : cache mountPath : /var/lib/docker volumes : - name : cache hostPath : path : /tmp type : Directory Docker Build Now that we can directly use Docker to build, we can also leverage buildkit 7 . Because we have the workspace available to us, we can directly start our docker build. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 pipeline { options { disableConcurrentBuilds () } agent { kubernetes { label docker-in-docker-maven yaml apiVersion: v1 kind: Pod spec: containers: - name: docker-client image: docker:19.03.1 command: [ sleep , 99d ] env: - name: DOCKER_HOST value: tcp://localhost:2375 - name: docker-daemon image: docker:19.03.1-dind env: - name: DOCKER_TLS_CERTDIR value: securityContext: privileged: true volumeMounts: - name: cache mountPath: /var/lib/docker volumes: - name: cache hostPath: path: /tmp type: Directory } } stages { stage ( Checkout ) { steps { git https://github.com/jenkinsci/docker-jnlp-slave.git } } stage ( Docker Build ) { steps { container ( docker-client ) { sh docker version DOCKER_BUILDKIT=1 docker build --progress plain -t testing . } } } } } Docker Run With Workspace In this example we're going to spin up another container to run our build with maven. This means it needs our workspace - see below - but it will also download a lot of Maven depencencies. We want to make sure we can leverage a local Maven Repository in order to speed up builds. We can do so by mounting a volume - or a hostPath to our Docker Client container. 1 2 3 volumeMounts : - name : cache mountPath : /tmp/repository We can then mount this into our new container via the Docker volume flag ( - v ). 1 - v / tmp / repository : / root / . m2 / repository Here we mount the workspace from our Jenkins Build. Notice the double quotes - in the pipeline below - this means we interpolate our Jenkins Environment variables. 1 -v ${ WORKSPACE } :/usr/src/mymaven The - w flag means work directory , it makes sure our container works in the directory container our workspace. 1 - w / usr / src / mymaven 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 pipeline { options { disableConcurrentBuilds () } agent { kubernetes { label docker-in-docker-maven yaml apiVersion: v1 kind: Pod spec: containers: - name: docker-client image: docker:19.03.1 command: [ sleep , 99d ] env: - name: DOCKER_HOST value: tcp://localhost:2375 volumeMounts: - name: cache mountPath: /tmp/repository - name: docker-daemon image: docker:19.03.1-dind env: - name: DOCKER_TLS_CERTDIR value: securityContext: privileged: true volumeMounts: - name: cache mountPath: /var/lib/docker volumes: - name: cache hostPath: path: /tmp type: Directory } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/jx-maven-lib.git } } stage ( Build ) { steps { container ( docker-client ) { sh docker run -v ${WORKSPACE}:/usr/src/mymaven -v /tmp/repository:/root/.m2/repository -w /usr/src/mymaven maven:3-jdk-11-slim mvn clean verify } } } } } References Using Docker-in-Docker - Jerome Petazzo Docker images on Dockerhub Jenkins Pipeline Jenkins Kubernetes Plugin Jenkins Kubernetes Plugin - Docker In Docker Example Jenkins Kubernetes Plugin - Docker On Docker Example Docker Build Enchancements - Buildkit","title":"PodTemplate With Docker-In-Docker"},{"location":"jenkins-pipeline/podtemplate-dind/#docker-in-docker-with-podtemplates","text":"First of all, doing Docker-In-Docker is a controversial practice to begin with 1 , creating just as many problems as it solves. Second, if you're in Kubernetes, one should not use Docker directly . Ok, all caveats aside, there can be legitimate reasons for wanting to run Docker containers directly even in Kubernetes.","title":"Docker-in-Docker With PodTemplates"},{"location":"jenkins-pipeline/podtemplate-dind/#scenario-pipeline-framework-based-on-docker","text":"I created the solution in this article for a client. This client has created a internal framework around Jenkins Pipeline 3 . This pipeline processes the requirements off the pipeline run and spins up the appropriate containers in parallel. Running parallel container can also be done with PodTemplates via the Kubernetes Plugin , which was deemed the way forward. However, as one can expect, you do not rewrite a framework used by dozens of applications unless you're reasonably sure you can get everything to run as it should. In order to bridge this period of running the pipelines as is, while evaluating Jenkins on Kubernetes, we got to work on achieving Docker-in-Docker with Jenkins Kubernetes Plugin 4 .","title":"Scenario: Pipeline Framework Based On Docker"},{"location":"jenkins-pipeline/podtemplate-dind/#goal","text":"The goal of the exercise, is to prove we can do a single git clone, and spin up multiple containers re-using the original namespace.","title":"Goal"},{"location":"jenkins-pipeline/podtemplate-dind/#docker-socket","text":"There's many ways to spin up Docker containers, and I would say the most common one is know as Docker-in-Docker or dind . I find this misleading, because most of the time the second container doesn't run inside the original container, but parallel to it. This is often achieved by mounting the docker socket as a volume - / var / run / docker . sock . I call this: Docker - On - Docker . Because I want to stay close to the term Docker-in-Docker, but also signify it is different. What Docker-In-Docker should be, is that we host a docker daemon in a docker container, which then hosts the new containers in itself. Before we go into the differences, let's discuss why it matters. Remember, the goal is to be able to re-use the git clone in the original container. The default container with Jenkins Kubernetes Plugin is the jnpl , which acts as an ephemeral agent. It mounts a emptyDir {} as temporary volume as its workspace, which means this folder only exists within the Pod (practically speaking).","title":"Docker Socket"},{"location":"jenkins-pipeline/podtemplate-dind/#docker-on-docker","text":"Classic Docker-On-Docker, we have a VM which has a Docker Daemon. We have a Docker client container which mounts the docker socket. The docker client now pretends to talk to a local Docker Daemon, but this redirects to the VM's daemon instead. When we do a docker run , the daemon will spawn a new container next to us. While this is nice, and stays clear from virtulization inception, this new container - let's say, a maven container - cannot access our workspace. Any volume flag we give to our container with - v { origin }:{ target } maps a Host directory to our new container, but not the workspace volume (purple) in our Pod. This is because the new container cannot access any volume inside the Pod. For this, we need true Docker-In-Docker.","title":"Docker-On-Docker"},{"location":"jenkins-pipeline/podtemplate-dind/#jenkinsfile-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 podTemplate ( yaml: apiVersion: v1 kind: Pod spec: containers: - name: docker image: docker:1.11 command: [ cat ] tty: true volumeMounts: - name: dockersock mountPath: /var/run/docker.sock volumes: - name: dockersock hostPath: path: /var/run/docker.sock ) { def image = jenkins/jnlp-slave node ( POD_LABEL ) { stage ( Build Docker image ) { git https://github.com/jenkinsci/docker-jnlp-slave.git container ( docker ) { sh docker build -t ${image} . } } } }","title":"Jenkinsfile Example"},{"location":"jenkins-pipeline/podtemplate-dind/#docker-in-docker","text":"True Docker-In-Docker means spinning up another container inside an existing container - as illustrated below. This has a big benefit. The container now has access to all the volumes inside the Pod. We can now do a git clone and use docker build on this workspace. In addition, we can also spin up a new container and give it the workspace as a volume and have it build with it.","title":"Docker-In-Docker"},{"location":"jenkins-pipeline/podtemplate-dind/#pod-configuration","text":"The examples below contain some specific configuration elements required to make the magic happen. So let's explore each configuration item to see what it does and why we set it.","title":"Pod Configuration"},{"location":"jenkins-pipeline/podtemplate-dind/#client-container","text":"The Docker Client has to know where the Docker Daemon is. We do this by setting the environment variable DOCKER_HOST 2 to tcp : // localhost : 2375 . If we put the Daemon container within the same Pod, we can use localhost and then the default port of the daemon, being 2375 . 1 2 3 env : - name : DOCKER_HOST value : tcp://localhost:2375 The client container will directly terminate unless we give it something to do. If we put it to sleep for a significant amount of time, it should be there to execute our every command! 1 command : [ sleep , 99d ]","title":"Client Container"},{"location":"jenkins-pipeline/podtemplate-dind/#daemon-container","text":"As of Docker 18.09 the Docker Daemon container can use TLS, as of 19.03 it is configured by default. 2 So either you work around this or get the certificates sorted. The easiest way around it, is to set the environment variable DOCKER_TLS_CERTDIR to , which will disable TLS. Caution Disabling TLS is at your OWN risk. Read the docs carefully for what this means 2 . 1 2 3 env : - name : DOCKER_TLS_CERTDIR value : In order for the Docker Daemon to create containers, it needs the privileged flag set to true. This should be another warning to you, to be careful of what you do with it! 1 2 securityContext : privileged : true Last but not least, we would not want to store all the Docker Daemon data in the pods. We might as well leverage local storage or a some specific volume. The volume and volumeMounts configuration of the Daemon container ensures we can leverage the Docker build and image caches. 1 2 3 4 5 6 7 8 volumeMounts : - name : cache mountPath : /var/lib/docker volumes : - name : cache hostPath : path : /tmp type : Directory Docker Build Now that we can directly use Docker to build, we can also leverage buildkit 7 . Because we have the workspace available to us, we can directly start our docker build. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 pipeline { options { disableConcurrentBuilds () } agent { kubernetes { label docker-in-docker-maven yaml apiVersion: v1 kind: Pod spec: containers: - name: docker-client image: docker:19.03.1 command: [ sleep , 99d ] env: - name: DOCKER_HOST value: tcp://localhost:2375 - name: docker-daemon image: docker:19.03.1-dind env: - name: DOCKER_TLS_CERTDIR value: securityContext: privileged: true volumeMounts: - name: cache mountPath: /var/lib/docker volumes: - name: cache hostPath: path: /tmp type: Directory } } stages { stage ( Checkout ) { steps { git https://github.com/jenkinsci/docker-jnlp-slave.git } } stage ( Docker Build ) { steps { container ( docker-client ) { sh docker version DOCKER_BUILDKIT=1 docker build --progress plain -t testing . } } } } } Docker Run With Workspace In this example we're going to spin up another container to run our build with maven. This means it needs our workspace - see below - but it will also download a lot of Maven depencencies. We want to make sure we can leverage a local Maven Repository in order to speed up builds. We can do so by mounting a volume - or a hostPath to our Docker Client container. 1 2 3 volumeMounts : - name : cache mountPath : /tmp/repository We can then mount this into our new container via the Docker volume flag ( - v ). 1 - v / tmp / repository : / root / . m2 / repository Here we mount the workspace from our Jenkins Build. Notice the double quotes - in the pipeline below - this means we interpolate our Jenkins Environment variables. 1 -v ${ WORKSPACE } :/usr/src/mymaven The - w flag means work directory , it makes sure our container works in the directory container our workspace. 1 - w / usr / src / mymaven 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 pipeline { options { disableConcurrentBuilds () } agent { kubernetes { label docker-in-docker-maven yaml apiVersion: v1 kind: Pod spec: containers: - name: docker-client image: docker:19.03.1 command: [ sleep , 99d ] env: - name: DOCKER_HOST value: tcp://localhost:2375 volumeMounts: - name: cache mountPath: /tmp/repository - name: docker-daemon image: docker:19.03.1-dind env: - name: DOCKER_TLS_CERTDIR value: securityContext: privileged: true volumeMounts: - name: cache mountPath: /var/lib/docker volumes: - name: cache hostPath: path: /tmp type: Directory } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/jx-maven-lib.git } } stage ( Build ) { steps { container ( docker-client ) { sh docker run -v ${WORKSPACE}:/usr/src/mymaven -v /tmp/repository:/root/.m2/repository -w /usr/src/mymaven maven:3-jdk-11-slim mvn clean verify } } } } }","title":"Daemon Container"},{"location":"jenkins-pipeline/podtemplate-dind/#references","text":"Using Docker-in-Docker - Jerome Petazzo Docker images on Dockerhub Jenkins Pipeline Jenkins Kubernetes Plugin Jenkins Kubernetes Plugin - Docker In Docker Example Jenkins Kubernetes Plugin - Docker On Docker Example Docker Build Enchancements - Buildkit","title":"References"},{"location":"jenkins-pipeline-examples/","text":"Jenkins Pipeline Examples Please mind that these examples all assume the following: you have Jenkins 2.32+ you have a recent set of pipeline plugins Jenkins Pipeline Model Jenkins Blue Ocean Jenkins Pipeline Maven Build timeout plugin Credentials Binding Credentials Pipeline Multi-Branch SonarQube Timestamper Pipeline Supporting APIs Pipeline Shared Groovy Libraries","title":"Index"},{"location":"jenkins-pipeline-examples/#jenkins-pipeline-examples","text":"Please mind that these examples all assume the following: you have Jenkins 2.32+ you have a recent set of pipeline plugins Jenkins Pipeline Model Jenkins Blue Ocean Jenkins Pipeline Maven Build timeout plugin Credentials Binding Credentials Pipeline Multi-Branch SonarQube Timestamper Pipeline Supporting APIs Pipeline Shared Groovy Libraries","title":"Jenkins Pipeline Examples"},{"location":"jenkins-pipeline-examples/docker-alternatives/","text":"Pipelines With Docker Alternatives Building pipelines with Jenkins on Docker has been common for a while. But accessing the docker socket has always been a bit tricky. The easiest solution is directly mounting the docker socket of the host into your build container. However, this is a big security risk and something that is undesirable at best and, pretty dangerous at worst. When you're using an orchestrator such as Kubernetes - which is where Jenkins is currently best put to work - the problem gets worse. Not only do you open up security holes, you also mess up the schedular's insight into available resources, and mess with it's ability to keep your cluster in a correct state. In some sense, using docker directly in a Kubernetes cluster, is likened to running with scissors . Potential Alternatives So, directly running docker containers via a docker engine inside your cluster is out. Let's look at some alternatives. Kubernetes Pod and External Node : the simple way out, use a cloud (such as EC2 Cloud) to provision a classic VM with docker engine as an agent and build there JIB : tool from Google, only works for Java applications and is supported directly from Gradle and Maven - Link Kaniko : tool from Google, works as 1-on-1 replacement for docker image build (except Windows Containers ) - Link IMG : tool from Jess Frazelle to avoid building docker images with a root user involved Link Kubernetes Pod and External Node One of the most used cloud environments is AWS, so I created this solution with AWS's Amazon EC2 Plugin . Warning Unfortunately, you cannot combine the Kubernetes plugin with external nodes (none pod container nodes) in a Declarative pipeline. So you have to use Scripted . This can be done with various different cloud providers such as Digital Ocean, Google, AWS or Azure. This guide will use AWS, as it has the most mature Jenkins Cloud plugin. Prerequisites AWS Account with rights to create AMI's and run EC2 instances Packer Jenkins with Amazon EC2 Plugin installed Steps create AMI with Packer install and configure Amazon EC2 plugin create a test pipeline Create AMI with Packer Packer needs to be able to access EC2 API's and be able to spin up an EC2 instance and create an AMI out of it. AWS setup for Packer You need to configure two things: account details for Packer to use security group where your EC2 instances will be running with this security group needs to open port 22 both Packer and Jenkins will use this for their connection 1 2 3 export AWS_DEFAULT_REGION = eu-west-1 export AWS_ACCESS_KEY_ID = XXX export AWS_SECRET_ACCESS_KEY = XXX 1 2 3 4 5 6 7 aws ec2 --profile myAwsProfile create-security-group \\ --description For building Docker images \\ --group-name docker { GroupId : sg-08079f78cXXXXXXX } Export the security group ID. 1 2 export SG_ID = sg-08079f78cXXXXXXX echo $SG_ID Enable port 22 1 2 3 4 5 6 7 aws ec2 \\ --profile myAwsProfile \\ authorize-security-group-ingress \\ --group-name docker \\ --protocol tcp \\ --port 22 \\ --cidr 0 .0.0.0/0 Packer AMI definition Here's an example definition for Packer for a Ubuntu 18.04 LTS base image with JDK 8 (required by Jenkins) and Docker. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { builders : [{ type : amazon-ebs , region : eu-west-1 , source_ami_filter : { filters : { virtualization-type : hvm , name : *ubuntu-bionic-18.04-amd64-server-* , root-device-type : ebs }, owners : [ 679593333241 ], most_recent : true }, instance_type : t2.micro , ssh_username : ubuntu , ami_name : docker , force_deregister : true }], provisioners : [{ type : shell , inline : [ sleep 15 , sudo apt-get clean , sudo apt-get update , sudo apt-get install -y apt-transport-https ca-certificates nfs-common , curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - , sudo add-apt-repository \\ deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\ , sudo add-apt-repository -y ppa:openjdk-r/ppa , sudo apt-get update , sudo apt-get install -y docker-ce , sudo usermod -aG docker ubuntu , sudo apt-get install -y openjdk-8-jdk , java -version , docker version ] }] } Build the new AMI with packer. 1 2 packer build docker-ami.json export AMI = ami-0212ab37f84e418f4 EC2 Key Pair Create EC2 key pair, this will be used by Jenkins to connect to the instances via ssh. 1 2 3 4 aws ec2 --profile myAwsProfile create-key-pair \\ --key-name jenkinsec2 \\ | jq -r .KeyMaterial \\ jenkins-ec2-proton.pem EC2 Cloud Configuration In a Jenkins' master main configuration, you add a new cloud . In this case, we will use a ec2 - cloud so we can instantiate our EC2 VM's with docker. use EC2 credentials for initial connection use key ( .pem ) for VM connection (jenkins agent) configure the following: AMI: ami-0212ab37f84e418f4 availability zone: eu-west-1a VPC SubnetID: subnet-aa54XXXX Remote user: ubuntu labels: docker ubuntu linux SecurityGroup Name: (the id) sg-08079f78cXXXXXXX public ip = true connect via public ip = true Pipeline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @Library ( jenkins-pipeline-library@master ) _ def scmVars def label = jenkins-slave-${UUID.randomUUID().toString()} podTemplate ( label: label , yaml: apiVersion: v1 kind: Pod spec: containers: - name: kubectl image: vfarcic/kubectl command: [ cat ] tty: true ) { node ( label ) { node ( docker ) { stage ( SCM Prepare ) { scmVars = checkout scm } stage ( Lint ) { dockerfileLint () } stage ( Build Docker ) { sh docker image build -t demo:rc-1 . } stage ( Tag Push Docker ) { IMAGE = ${DOCKER_IMAGE_NAME} TAG = ${DOCKER_IMAGE_TAG} FULL_NAME = ${FULL_IMAGE_NAME} withCredentials ([ usernamePassword ( credentialsId: dockerhub , usernameVariable: USER , passwordVariable: PASS )]) { sh docker login -u $USER -p $PASS } sh docker image tag ${IMAGE}:${TAG} ${FULL_NAME} sh docker image push ${FULL_NAME} } } // end node docker stage ( Prepare Pod ) { // have to checkout on our kubernetes pod aswell checkout scm } stage ( Check version ) { container ( kubectl ) { sh kubectl version } } } // end node random label } // end pod def Maven JIB If you use Java with either Gradle or Maven , you can use JIB to create docker image without requiring a docker client or docker engine. JIB will communicate with DockerHub and Google's container registry for sending image layers back and forth. The last layer will be created by the JIB plugin itself, adding your self-executing Jar to the layer and a Entrypoint with the correct flags. For more information about the runtime options, see either jib-maven-plugin documentation or jib-gradle-plugin . Prerequisites Java project build with Gradle or Maven Java project that can start it self, such as Spring Boot or Thorntail (previously Wildfly Swarm, from the JBoss family) able to build either gradle or maven applications The project used can be found at github.com/demomon/maven-spring-boot-demo . Steps configure the plugin for either Gradle or Maven build using an official docker image via the kubernetes pod template Pipeline Using a bit more elaborate pipeline example here. Using SonarCloud for static code analysis and then JIB to create a docker image. We could then use that image either directly in anything that runs docker or in the same cluster via a Helm Chart. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def scmVars def tag pipeline { options { buildDiscarder logRotator ( artifactDaysToKeepStr: 5 , artifactNumToKeepStr: 5 , daysToKeepStr: 5 , numToKeepStr: 5 ) } libraries { lib ( core@master ) lib ( maven@master ) } agent { kubernetes { label mypod defaultContainer jnlp yaml apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:3-jdk-11-slim command: - cat tty: true } } stages { stage ( Test versions ) { steps { container ( maven ) { sh uname -a sh mvn -version } } } stage ( Checkout ) { steps { script { scmVars = checkout scm } gitRemoteConfigByUrl ( scmVars . GIT_URL , githubtoken ) sh git config --global user.email jenkins@jenkins.io git config --global user.name Jenkins } } stage ( Build ) { steps { container ( maven ) { sh mvn clean verify -B -e } } } stage ( Version Analysis ) { parallel { stage ( Version Bump ) { when { branch master } environment { NEW_VERSION = gitNextSemverTagMaven ( pom.xml ) } steps { script { tag = ${NEW_VERSION} } container ( maven ) { sh mvn versions:set -DnewVersion=${NEW_VERSION} } gitTag ( v${NEW_VERSION} ) } } stage ( Sonar Analysis ) { when { branch master } environment { SONAR_HOST = https://sonarcloud.io KEY = spring-maven-demo ORG = demomon SONAR_TOKEN = credentials ( sonarcloud ) } steps { container ( maven ) { sh mvn sonar:sonar \\ -Dsonar.projectKey=${KEY} \\ -Dsonar.organization=${ORG} \\ -Dsonar.host.url=${SONAR_HOST} \\ -Dsonar.login=${SONAR_TOKEN} } } } } } stage ( Publish Artifact ) { when { branch master } environment { DHUB = credentials ( dockerhub ) } steps { container ( maven ) { // we should never come here if the tests have not run, as we run verify before sh mvn clean compile -B -e jib:build -Djib.to.auth.username=${DHUB_USR} -Djib.to.auth.password=${DHUB_PSW} -DskipTests } } } } post { always { cleanWs () } } } Kaniko Google loves Kubernetes and Google prefers people building docker images in Kubernetes without Docker. As JIB is only available for Java projects, there's needs to be an alternative for any other usecase/programming language. The answer to that is Kaniko , a specialized Docker image to create Docker images. Kaniko isn't the most secure way to create docker images, it barely beats mounting a Docker Socket, or might even be worse if you ask others (such as Jess Frazelle ). That said, it is gaining some traction being used by JenkinsX and having an example in Jenkins' Kubernetes Plugin . Info When building more than one image inside the kaniko container, make sure to use the --cleanup flag. So it cleans its temporary cache data before building the next image, as discussed in this google group . Prerequisites Steps Create docker registry secret Configure pod container template Configure stage Create docker registry secret This is an example for DockerHub inside the build namespace. 1 2 3 4 5 kubectl create secret docker-registry -n build regcred \\ --docker-server = index.docker.io \\ --docker-username = myDockerHubAccount \\ --docker-password = myDockerHubPassword \\ --docker-email = myDockerHub@Email.com Example Ppeline Warning Although multi-stage Dockerfile 's are supported, it did fail in my case. So I created a second Dockerfile which is only for running the application ( Dockerfile . run ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 pipeline { agent { kubernetes { //cloud kubernetes label kaniko yaml kind: Pod metadata: name: kaniko spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root - name: go-build-cache mountPath: /root/.cache/go-build - name: img-build-cache mountPath: /root/.local volumes: - name: go-build-cache emptyDir: {} - name: img-build-cache emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { environment { PATH = /busybox:$PATH } steps { container ( name: kaniko , shell: /busybox/sh ) { sh #!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=index.docker.io/caladreas/cat } } } } } IMG img is the brainchild of Jess Frazelle, a prominent figure in the container space. The goal is to be the safest and best way to build OCI compliant images, as outlined in her blog building container images securely on kubernetes . Not working (for me) yet It does not seem to work for me on AWS's EKS. To many little details with relation to runc, file permissions and other configuration. For those who want to give it a spin, here are some resources to take a look at. https://blog.jessfraz.com/post/building-container-images-securely-on-kubernetes/ https://github.com/genuinetools/img https://github.com/opencontainers/runc https://git.j3ss.co/genuinetools/img/+/d05b3e4e10cd0e3c074ffb03dc22d7bb6cde1e78 Pipeline Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 pipeline { agent { kubernetes { label img yaml kind: Pod metadata: name: img annotations: container.apparmor.security.beta.kubernetes.io/img: unconfined spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: img workingDir: /home/jenkins image: caladreas/img:0.5.1 imagePullPolicy: Always securityContext: rawProc: true privileged: true command: - cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: temp emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { steps { container ( img ) { sh mkdir cache sh img build -s ./cache -f Dockerfile.run -t caladreas/cat . } } } } }","title":"Docker Alternatives"},{"location":"jenkins-pipeline-examples/docker-alternatives/#pipelines-with-docker-alternatives","text":"Building pipelines with Jenkins on Docker has been common for a while. But accessing the docker socket has always been a bit tricky. The easiest solution is directly mounting the docker socket of the host into your build container. However, this is a big security risk and something that is undesirable at best and, pretty dangerous at worst. When you're using an orchestrator such as Kubernetes - which is where Jenkins is currently best put to work - the problem gets worse. Not only do you open up security holes, you also mess up the schedular's insight into available resources, and mess with it's ability to keep your cluster in a correct state. In some sense, using docker directly in a Kubernetes cluster, is likened to running with scissors .","title":"Pipelines With Docker Alternatives"},{"location":"jenkins-pipeline-examples/docker-alternatives/#potential-alternatives","text":"So, directly running docker containers via a docker engine inside your cluster is out. Let's look at some alternatives. Kubernetes Pod and External Node : the simple way out, use a cloud (such as EC2 Cloud) to provision a classic VM with docker engine as an agent and build there JIB : tool from Google, only works for Java applications and is supported directly from Gradle and Maven - Link Kaniko : tool from Google, works as 1-on-1 replacement for docker image build (except Windows Containers ) - Link IMG : tool from Jess Frazelle to avoid building docker images with a root user involved Link","title":"Potential Alternatives"},{"location":"jenkins-pipeline-examples/docker-alternatives/#kubernetes-pod-and-external-node","text":"One of the most used cloud environments is AWS, so I created this solution with AWS's Amazon EC2 Plugin . Warning Unfortunately, you cannot combine the Kubernetes plugin with external nodes (none pod container nodes) in a Declarative pipeline. So you have to use Scripted . This can be done with various different cloud providers such as Digital Ocean, Google, AWS or Azure. This guide will use AWS, as it has the most mature Jenkins Cloud plugin.","title":"Kubernetes Pod and External Node"},{"location":"jenkins-pipeline-examples/docker-alternatives/#prerequisites","text":"AWS Account with rights to create AMI's and run EC2 instances Packer Jenkins with Amazon EC2 Plugin installed","title":"Prerequisites"},{"location":"jenkins-pipeline-examples/docker-alternatives/#steps","text":"create AMI with Packer install and configure Amazon EC2 plugin create a test pipeline","title":"Steps"},{"location":"jenkins-pipeline-examples/docker-alternatives/#create-ami-with-packer","text":"Packer needs to be able to access EC2 API's and be able to spin up an EC2 instance and create an AMI out of it.","title":"Create AMI with Packer"},{"location":"jenkins-pipeline-examples/docker-alternatives/#aws-setup-for-packer","text":"You need to configure two things: account details for Packer to use security group where your EC2 instances will be running with this security group needs to open port 22 both Packer and Jenkins will use this for their connection 1 2 3 export AWS_DEFAULT_REGION = eu-west-1 export AWS_ACCESS_KEY_ID = XXX export AWS_SECRET_ACCESS_KEY = XXX 1 2 3 4 5 6 7 aws ec2 --profile myAwsProfile create-security-group \\ --description For building Docker images \\ --group-name docker { GroupId : sg-08079f78cXXXXXXX } Export the security group ID. 1 2 export SG_ID = sg-08079f78cXXXXXXX echo $SG_ID","title":"AWS setup for Packer"},{"location":"jenkins-pipeline-examples/docker-alternatives/#enable-port-22","text":"1 2 3 4 5 6 7 aws ec2 \\ --profile myAwsProfile \\ authorize-security-group-ingress \\ --group-name docker \\ --protocol tcp \\ --port 22 \\ --cidr 0 .0.0.0/0","title":"Enable port 22"},{"location":"jenkins-pipeline-examples/docker-alternatives/#packer-ami-definition","text":"Here's an example definition for Packer for a Ubuntu 18.04 LTS base image with JDK 8 (required by Jenkins) and Docker. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { builders : [{ type : amazon-ebs , region : eu-west-1 , source_ami_filter : { filters : { virtualization-type : hvm , name : *ubuntu-bionic-18.04-amd64-server-* , root-device-type : ebs }, owners : [ 679593333241 ], most_recent : true }, instance_type : t2.micro , ssh_username : ubuntu , ami_name : docker , force_deregister : true }], provisioners : [{ type : shell , inline : [ sleep 15 , sudo apt-get clean , sudo apt-get update , sudo apt-get install -y apt-transport-https ca-certificates nfs-common , curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - , sudo add-apt-repository \\ deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\ , sudo add-apt-repository -y ppa:openjdk-r/ppa , sudo apt-get update , sudo apt-get install -y docker-ce , sudo usermod -aG docker ubuntu , sudo apt-get install -y openjdk-8-jdk , java -version , docker version ] }] } Build the new AMI with packer. 1 2 packer build docker-ami.json export AMI = ami-0212ab37f84e418f4","title":"Packer AMI definition"},{"location":"jenkins-pipeline-examples/docker-alternatives/#ec2-key-pair","text":"Create EC2 key pair, this will be used by Jenkins to connect to the instances via ssh. 1 2 3 4 aws ec2 --profile myAwsProfile create-key-pair \\ --key-name jenkinsec2 \\ | jq -r .KeyMaterial \\ jenkins-ec2-proton.pem","title":"EC2 Key Pair"},{"location":"jenkins-pipeline-examples/docker-alternatives/#ec2-cloud-configuration","text":"In a Jenkins' master main configuration, you add a new cloud . In this case, we will use a ec2 - cloud so we can instantiate our EC2 VM's with docker. use EC2 credentials for initial connection use key ( .pem ) for VM connection (jenkins agent) configure the following: AMI: ami-0212ab37f84e418f4 availability zone: eu-west-1a VPC SubnetID: subnet-aa54XXXX Remote user: ubuntu labels: docker ubuntu linux SecurityGroup Name: (the id) sg-08079f78cXXXXXXX public ip = true connect via public ip = true","title":"EC2 Cloud Configuration"},{"location":"jenkins-pipeline-examples/docker-alternatives/#pipeline","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @Library ( jenkins-pipeline-library@master ) _ def scmVars def label = jenkins-slave-${UUID.randomUUID().toString()} podTemplate ( label: label , yaml: apiVersion: v1 kind: Pod spec: containers: - name: kubectl image: vfarcic/kubectl command: [ cat ] tty: true ) { node ( label ) { node ( docker ) { stage ( SCM Prepare ) { scmVars = checkout scm } stage ( Lint ) { dockerfileLint () } stage ( Build Docker ) { sh docker image build -t demo:rc-1 . } stage ( Tag Push Docker ) { IMAGE = ${DOCKER_IMAGE_NAME} TAG = ${DOCKER_IMAGE_TAG} FULL_NAME = ${FULL_IMAGE_NAME} withCredentials ([ usernamePassword ( credentialsId: dockerhub , usernameVariable: USER , passwordVariable: PASS )]) { sh docker login -u $USER -p $PASS } sh docker image tag ${IMAGE}:${TAG} ${FULL_NAME} sh docker image push ${FULL_NAME} } } // end node docker stage ( Prepare Pod ) { // have to checkout on our kubernetes pod aswell checkout scm } stage ( Check version ) { container ( kubectl ) { sh kubectl version } } } // end node random label } // end pod def","title":"Pipeline"},{"location":"jenkins-pipeline-examples/docker-alternatives/#maven-jib","text":"If you use Java with either Gradle or Maven , you can use JIB to create docker image without requiring a docker client or docker engine. JIB will communicate with DockerHub and Google's container registry for sending image layers back and forth. The last layer will be created by the JIB plugin itself, adding your self-executing Jar to the layer and a Entrypoint with the correct flags. For more information about the runtime options, see either jib-maven-plugin documentation or jib-gradle-plugin .","title":"Maven JIB"},{"location":"jenkins-pipeline-examples/docker-alternatives/#prerequisites_1","text":"Java project build with Gradle or Maven Java project that can start it self, such as Spring Boot or Thorntail (previously Wildfly Swarm, from the JBoss family) able to build either gradle or maven applications The project used can be found at github.com/demomon/maven-spring-boot-demo .","title":"Prerequisites"},{"location":"jenkins-pipeline-examples/docker-alternatives/#steps_1","text":"configure the plugin for either Gradle or Maven build using an official docker image via the kubernetes pod template","title":"Steps"},{"location":"jenkins-pipeline-examples/docker-alternatives/#pipeline_1","text":"Using a bit more elaborate pipeline example here. Using SonarCloud for static code analysis and then JIB to create a docker image. We could then use that image either directly in anything that runs docker or in the same cluster via a Helm Chart. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def scmVars def tag pipeline { options { buildDiscarder logRotator ( artifactDaysToKeepStr: 5 , artifactNumToKeepStr: 5 , daysToKeepStr: 5 , numToKeepStr: 5 ) } libraries { lib ( core@master ) lib ( maven@master ) } agent { kubernetes { label mypod defaultContainer jnlp yaml apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:3-jdk-11-slim command: - cat tty: true } } stages { stage ( Test versions ) { steps { container ( maven ) { sh uname -a sh mvn -version } } } stage ( Checkout ) { steps { script { scmVars = checkout scm } gitRemoteConfigByUrl ( scmVars . GIT_URL , githubtoken ) sh git config --global user.email jenkins@jenkins.io git config --global user.name Jenkins } } stage ( Build ) { steps { container ( maven ) { sh mvn clean verify -B -e } } } stage ( Version Analysis ) { parallel { stage ( Version Bump ) { when { branch master } environment { NEW_VERSION = gitNextSemverTagMaven ( pom.xml ) } steps { script { tag = ${NEW_VERSION} } container ( maven ) { sh mvn versions:set -DnewVersion=${NEW_VERSION} } gitTag ( v${NEW_VERSION} ) } } stage ( Sonar Analysis ) { when { branch master } environment { SONAR_HOST = https://sonarcloud.io KEY = spring-maven-demo ORG = demomon SONAR_TOKEN = credentials ( sonarcloud ) } steps { container ( maven ) { sh mvn sonar:sonar \\ -Dsonar.projectKey=${KEY} \\ -Dsonar.organization=${ORG} \\ -Dsonar.host.url=${SONAR_HOST} \\ -Dsonar.login=${SONAR_TOKEN} } } } } } stage ( Publish Artifact ) { when { branch master } environment { DHUB = credentials ( dockerhub ) } steps { container ( maven ) { // we should never come here if the tests have not run, as we run verify before sh mvn clean compile -B -e jib:build -Djib.to.auth.username=${DHUB_USR} -Djib.to.auth.password=${DHUB_PSW} -DskipTests } } } } post { always { cleanWs () } } }","title":"Pipeline"},{"location":"jenkins-pipeline-examples/docker-alternatives/#kaniko","text":"Google loves Kubernetes and Google prefers people building docker images in Kubernetes without Docker. As JIB is only available for Java projects, there's needs to be an alternative for any other usecase/programming language. The answer to that is Kaniko , a specialized Docker image to create Docker images. Kaniko isn't the most secure way to create docker images, it barely beats mounting a Docker Socket, or might even be worse if you ask others (such as Jess Frazelle ). That said, it is gaining some traction being used by JenkinsX and having an example in Jenkins' Kubernetes Plugin . Info When building more than one image inside the kaniko container, make sure to use the --cleanup flag. So it cleans its temporary cache data before building the next image, as discussed in this google group .","title":"Kaniko"},{"location":"jenkins-pipeline-examples/docker-alternatives/#prerequisites_2","text":"","title":"Prerequisites"},{"location":"jenkins-pipeline-examples/docker-alternatives/#steps_2","text":"Create docker registry secret Configure pod container template Configure stage","title":"Steps"},{"location":"jenkins-pipeline-examples/docker-alternatives/#create-docker-registry-secret","text":"This is an example for DockerHub inside the build namespace. 1 2 3 4 5 kubectl create secret docker-registry -n build regcred \\ --docker-server = index.docker.io \\ --docker-username = myDockerHubAccount \\ --docker-password = myDockerHubPassword \\ --docker-email = myDockerHub@Email.com","title":"Create docker registry secret"},{"location":"jenkins-pipeline-examples/docker-alternatives/#example-ppeline","text":"Warning Although multi-stage Dockerfile 's are supported, it did fail in my case. So I created a second Dockerfile which is only for running the application ( Dockerfile . run ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 pipeline { agent { kubernetes { //cloud kubernetes label kaniko yaml kind: Pod metadata: name: kaniko spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root - name: go-build-cache mountPath: /root/.cache/go-build - name: img-build-cache mountPath: /root/.local volumes: - name: go-build-cache emptyDir: {} - name: img-build-cache emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { environment { PATH = /busybox:$PATH } steps { container ( name: kaniko , shell: /busybox/sh ) { sh #!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=index.docker.io/caladreas/cat } } } } }","title":"Example Ppeline"},{"location":"jenkins-pipeline-examples/docker-alternatives/#img","text":"img is the brainchild of Jess Frazelle, a prominent figure in the container space. The goal is to be the safest and best way to build OCI compliant images, as outlined in her blog building container images securely on kubernetes .","title":"IMG"},{"location":"jenkins-pipeline-examples/docker-alternatives/#not-working-for-me-yet","text":"It does not seem to work for me on AWS's EKS. To many little details with relation to runc, file permissions and other configuration. For those who want to give it a spin, here are some resources to take a look at. https://blog.jessfraz.com/post/building-container-images-securely-on-kubernetes/ https://github.com/genuinetools/img https://github.com/opencontainers/runc https://git.j3ss.co/genuinetools/img/+/d05b3e4e10cd0e3c074ffb03dc22d7bb6cde1e78","title":"Not working (for me) yet"},{"location":"jenkins-pipeline-examples/docker-alternatives/#pipeline-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 pipeline { agent { kubernetes { label img yaml kind: Pod metadata: name: img annotations: container.apparmor.security.beta.kubernetes.io/img: unconfined spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: img workingDir: /home/jenkins image: caladreas/img:0.5.1 imagePullPolicy: Always securityContext: rawProc: true privileged: true command: - cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: temp emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { steps { container ( img ) { sh mkdir cache sh img build -s ./cache -f Dockerfile.run -t caladreas/cat . } } } } }","title":"Pipeline Example"},{"location":"jenkins-pipeline-examples/docker-declarative/","text":"Docker Declarative Examples 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 pipeline { agent none options { timeout ( time: 10 , unit: MINUTES ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: 5 )) } stages { stage ( Prepare ){ agent { label docker } steps { parallel ( Clean: { deleteDir () }, NotifySlack: { slackSend channel: cicd , color: #FFFF00 , message: STARTED: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } ) } } stage ( Checkout ){ agent { label docker } steps { git credentialsId: 355df378-e726-4abd-90fa-e723c5c21ad5 , url: git@gitlab.flusso.nl:CICD/ci-cd-docs.git script { env . GIT_COMMIT_HASH = sh returnStdout: true , script: git rev-parse --verify HEAD } } } stage ( Build Docs ) { agent { docker { image caladreas/mkdocs-docker-build-container label docker } } steps { sh mkdocs build } } stage ( Prepare Docker Image ){ agent { label docker } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true , script: docker run --rm -i lukasmartinelli/hadolint Dockerfile if ( lintResult . trim () == ) { println Lint finished with no errors } else { println Error found in Lint println ${lintResult} currentBuild . result = UNSTABLE } } }, // end test dockerfile BuildImage: { sh chmod +x build.sh sh ./build.sh } ) } post { success { sh chmod +x push.sh sh ./push.sh } } } stage ( Update Docker Container ) { agent { label docker } steps { sh chmod +x container-update.sh sh ./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH} } } } post { success { slackSend channel: cicd , color: #00FF00 , message: SUCCESSFUL: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } failure { slackSend channel: cicd , color: #FF0000 , message: FAILED: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } } }","title":"Docker Declarative"},{"location":"jenkins-pipeline-examples/docker-declarative/#docker-declarative-examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 pipeline { agent none options { timeout ( time: 10 , unit: MINUTES ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: 5 )) } stages { stage ( Prepare ){ agent { label docker } steps { parallel ( Clean: { deleteDir () }, NotifySlack: { slackSend channel: cicd , color: #FFFF00 , message: STARTED: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } ) } } stage ( Checkout ){ agent { label docker } steps { git credentialsId: 355df378-e726-4abd-90fa-e723c5c21ad5 , url: git@gitlab.flusso.nl:CICD/ci-cd-docs.git script { env . GIT_COMMIT_HASH = sh returnStdout: true , script: git rev-parse --verify HEAD } } } stage ( Build Docs ) { agent { docker { image caladreas/mkdocs-docker-build-container label docker } } steps { sh mkdocs build } } stage ( Prepare Docker Image ){ agent { label docker } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true , script: docker run --rm -i lukasmartinelli/hadolint Dockerfile if ( lintResult . trim () == ) { println Lint finished with no errors } else { println Error found in Lint println ${lintResult} currentBuild . result = UNSTABLE } } }, // end test dockerfile BuildImage: { sh chmod +x build.sh sh ./build.sh } ) } post { success { sh chmod +x push.sh sh ./push.sh } } } stage ( Update Docker Container ) { agent { label docker } steps { sh chmod +x container-update.sh sh ./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH} } } } post { success { slackSend channel: cicd , color: #00FF00 , message: SUCCESSFUL: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } failure { slackSend channel: cicd , color: #FF0000 , message: FAILED: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } } }","title":"Docker Declarative Examples"},{"location":"jenkins-pipeline-examples/maven-declarative/","text":"Maven Declarative Examples Basics We have to wrap the entire script in pipeline { } , for it to be marked a declarative script. As we will be using different agents for different stages, we select none as the default. For house keeping, we add the options {} block, where we configure the following: timeout: make sure this jobs succeeds in 10 minutes, else just cancel it timestamps(): to make sure we have timestamps in our logs buildDiscarder(): this will make sure we will only keep the latest 5 builds 1 2 3 4 5 6 7 8 pipeline { agent none options { timeout ( time: 10 , unit: MINUTES ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: 5 )) } } Checkout There are several ways to checkout the code. Let's assume our code is somewhere in a git repository. Full Checkout command The main command for checking out is the Checkout command. It will look like this. 1 2 3 4 5 6 7 8 9 10 11 12 stage ( SCM ) { checkout ([ $class : GitSCM , branches: [[ name: */master ]], doGenerateSubmoduleConfigurations: false , extensions: [], submoduleCfg: [], userRemoteConfigs: [[ credentialsId: MyCredentialsId , url: https://github.com/joostvdg/keep-watching ]] ]) } Git shorthand Thats a lot of configuration for a simple checkout. So what if I'm just using the master branch of a publicly accessible repository (as is the case with GitHub)? 1 2 3 stage ( SCM ) { git https://github.com/joostvdg/keep-watching } Or with a different branch and credentials: 1 2 3 stage ( SCM ) { git credentialsId: MyCredentialsId , url: https://github.com/joostvdg/keep-watching } That's much better, but we can do even better. SCM shorthand If you're starting this pipeline job via a SCM, you've already configured the SCM. So assuming you've configured a pipeline job with 'Jenkinsfile from SCM' or an abstraction job - such as Multibranch-Pipeline, GitHub Organization or BitBucket Team/Project - you can do this. 1 2 3 stage ( SCM ) { checkout scm } The checkout scm line will use the checkout command we've used in the first example together with the object scm . This scm object, will contain the SCM configuration of the Job and will be reused for checking out. Warning A pipeline job from SCM or abstraction, will only checkout your Jenkinsfile. You will always need to checkout the rest of your code if you want to build it. For that, just use checkout scm Different Agent per Stage As you could see on the top, we've set agent to none. So for every stage we now need to tell it which agent to use - without it, the stage will fail. Agent any If you don't care what node it comes on, you specify any. 1 2 3 4 5 6 stage ( Checkout ) { agent any steps { git https://github.com/joostvdg/keep-watching } } Agent via Label If you want build on a node with a specific label - here docker - you do so with agent { label LABEL } . 1 2 3 4 5 6 stage ( Checkout ) { agent { label docker } steps { git https://github.com/joostvdg/keep-watching } } Docker Container as Agent Many developers are using docker for their CI/CD builds. So being able to use docker containers as build agents is a requirement these days. 1 2 3 4 5 6 7 8 9 10 11 12 stage ( Maven Build ) { agent { docker { image maven:3-alpine label docker args -v /home/joost/.m2:/root/.m2 } } steps { sh mvn -B clean package } } Cache Maven repo When you're using a docker build container, it will be clean every time. So if you want to avoid downloading the maven dependencies every build, you have to cache them. One way to do this, is to map a volume into the container so the container will use that folder instead. 1 2 3 4 5 6 7 8 9 10 11 12 stage ( Maven Build ) { agent { docker { image maven:3-alpine label docker args -v /home/joost/.m2:/root/.m2 } } steps { sh mvn -B clean package } } Post stage/build The declarative pipeline allows for Post actions, on both stage and complete build level. For both types there are different post hooks you can use, such as success, failure. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 stage ( Maven Build ) { agent { docker { image maven:3-alpine label docker args -v /home/joost/.m2:/root/.m2 } } steps { sh mvn -B clean package } post { success { junit target/surefire-reports/**/*.xml } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 stages { stage ( Example ) { steps { echo Hello World } } } post { always { echo This will always run } success { echo SUCCESS! } failure { echo We Failed } unstable { echo We re unstable } changed { echo Status Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result] } } Entire example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 pipeline { agent none options { timeout ( time: 10 , unit: MINUTES ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: 5 )) } stages { stage ( Example ) { steps { echo Hello World } } stage ( Checkout ) { agent { label docker } steps { git https://github.com/joostvdg/keep-watching } } stage ( Maven Build ) { agent { docker { image maven:3-alpine label docker args -v /home/joost/.m2:/root/.m2 } } steps { sh mvn -B clean package } post { success { junit target/surefire-reports/**/*.xml } } } stage ( Docker Build ) { agent { label docker } steps { sh docker build --tag=keep-watching-be . } } } post { always { echo This will always run } success { echo SUCCESS! } failure { echo We Failed } unstable { echo We re unstable } changed { echo Status Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result] } } }","title":"Maven Declarative"},{"location":"jenkins-pipeline-examples/maven-declarative/#maven-declarative-examples","text":"","title":"Maven Declarative Examples"},{"location":"jenkins-pipeline-examples/maven-declarative/#basics","text":"We have to wrap the entire script in pipeline { } , for it to be marked a declarative script. As we will be using different agents for different stages, we select none as the default. For house keeping, we add the options {} block, where we configure the following: timeout: make sure this jobs succeeds in 10 minutes, else just cancel it timestamps(): to make sure we have timestamps in our logs buildDiscarder(): this will make sure we will only keep the latest 5 builds 1 2 3 4 5 6 7 8 pipeline { agent none options { timeout ( time: 10 , unit: MINUTES ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: 5 )) } }","title":"Basics"},{"location":"jenkins-pipeline-examples/maven-declarative/#checkout","text":"There are several ways to checkout the code. Let's assume our code is somewhere in a git repository.","title":"Checkout"},{"location":"jenkins-pipeline-examples/maven-declarative/#full-checkout-command","text":"The main command for checking out is the Checkout command. It will look like this. 1 2 3 4 5 6 7 8 9 10 11 12 stage ( SCM ) { checkout ([ $class : GitSCM , branches: [[ name: */master ]], doGenerateSubmoduleConfigurations: false , extensions: [], submoduleCfg: [], userRemoteConfigs: [[ credentialsId: MyCredentialsId , url: https://github.com/joostvdg/keep-watching ]] ]) }","title":"Full Checkout command"},{"location":"jenkins-pipeline-examples/maven-declarative/#git-shorthand","text":"Thats a lot of configuration for a simple checkout. So what if I'm just using the master branch of a publicly accessible repository (as is the case with GitHub)? 1 2 3 stage ( SCM ) { git https://github.com/joostvdg/keep-watching } Or with a different branch and credentials: 1 2 3 stage ( SCM ) { git credentialsId: MyCredentialsId , url: https://github.com/joostvdg/keep-watching } That's much better, but we can do even better.","title":"Git shorthand"},{"location":"jenkins-pipeline-examples/maven-declarative/#scm-shorthand","text":"If you're starting this pipeline job via a SCM, you've already configured the SCM. So assuming you've configured a pipeline job with 'Jenkinsfile from SCM' or an abstraction job - such as Multibranch-Pipeline, GitHub Organization or BitBucket Team/Project - you can do this. 1 2 3 stage ( SCM ) { checkout scm } The checkout scm line will use the checkout command we've used in the first example together with the object scm . This scm object, will contain the SCM configuration of the Job and will be reused for checking out. Warning A pipeline job from SCM or abstraction, will only checkout your Jenkinsfile. You will always need to checkout the rest of your code if you want to build it. For that, just use checkout scm","title":"SCM shorthand"},{"location":"jenkins-pipeline-examples/maven-declarative/#different-agent-per-stage","text":"As you could see on the top, we've set agent to none. So for every stage we now need to tell it which agent to use - without it, the stage will fail.","title":"Different Agent per Stage"},{"location":"jenkins-pipeline-examples/maven-declarative/#agent-any","text":"If you don't care what node it comes on, you specify any. 1 2 3 4 5 6 stage ( Checkout ) { agent any steps { git https://github.com/joostvdg/keep-watching } }","title":"Agent any"},{"location":"jenkins-pipeline-examples/maven-declarative/#agent-via-label","text":"If you want build on a node with a specific label - here docker - you do so with agent { label LABEL } . 1 2 3 4 5 6 stage ( Checkout ) { agent { label docker } steps { git https://github.com/joostvdg/keep-watching } }","title":"Agent via Label"},{"location":"jenkins-pipeline-examples/maven-declarative/#docker-container-as-agent","text":"Many developers are using docker for their CI/CD builds. So being able to use docker containers as build agents is a requirement these days. 1 2 3 4 5 6 7 8 9 10 11 12 stage ( Maven Build ) { agent { docker { image maven:3-alpine label docker args -v /home/joost/.m2:/root/.m2 } } steps { sh mvn -B clean package } }","title":"Docker Container as Agent"},{"location":"jenkins-pipeline-examples/maven-declarative/#cache-maven-repo","text":"When you're using a docker build container, it will be clean every time. So if you want to avoid downloading the maven dependencies every build, you have to cache them. One way to do this, is to map a volume into the container so the container will use that folder instead. 1 2 3 4 5 6 7 8 9 10 11 12 stage ( Maven Build ) { agent { docker { image maven:3-alpine label docker args -v /home/joost/.m2:/root/.m2 } } steps { sh mvn -B clean package } }","title":"Cache Maven repo"},{"location":"jenkins-pipeline-examples/maven-declarative/#post-stagebuild","text":"The declarative pipeline allows for Post actions, on both stage and complete build level. For both types there are different post hooks you can use, such as success, failure. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 stage ( Maven Build ) { agent { docker { image maven:3-alpine label docker args -v /home/joost/.m2:/root/.m2 } } steps { sh mvn -B clean package } post { success { junit target/surefire-reports/**/*.xml } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 stages { stage ( Example ) { steps { echo Hello World } } } post { always { echo This will always run } success { echo SUCCESS! } failure { echo We Failed } unstable { echo We re unstable } changed { echo Status Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result] } }","title":"Post stage/build"},{"location":"jenkins-pipeline-examples/maven-declarative/#entire-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 pipeline { agent none options { timeout ( time: 10 , unit: MINUTES ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: 5 )) } stages { stage ( Example ) { steps { echo Hello World } } stage ( Checkout ) { agent { label docker } steps { git https://github.com/joostvdg/keep-watching } } stage ( Maven Build ) { agent { docker { image maven:3-alpine label docker args -v /home/joost/.m2:/root/.m2 } } steps { sh mvn -B clean package } post { success { junit target/surefire-reports/**/*.xml } } } stage ( Docker Build ) { agent { label docker } steps { sh docker build --tag=keep-watching-be . } } } post { always { echo This will always run } success { echo SUCCESS! } failure { echo We Failed } unstable { echo We re unstable } changed { echo Status Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result] } } }","title":"Entire example"},{"location":"jenkins-pipeline-examples/maven-groovy-dsl/","text":"Maven Groovy DSL Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 node { timestamps { timeout ( time: 15 , unit: MINUTES ) { deleteDir () stage SCM //git branch: master , credentialsId: flusso-gitlab , url: https://gitlab.flusso.nl/keep/keep-backend-spring.git checkout scm env . JAVA_HOME = ${tool JDK 8 Latest } env . PATH = ${env.JAVA_HOME}/bin:${env.PATH} sh java -version try { def gradleHome = tool name: Gradle Latest , type: hudson.plugins.gradle.GradleInstallation stage Build sh ${gradleHome}/bin/gradle clean build javadoc step ([ $class : CheckStylePublisher , canComputeNew: false , defaultEncoding: , healthy: , pattern: build/reports/checkstyle/main.xml , unHealthy: ]) step ([ $class : JUnitResultArchiver , testResults: build/test-results/*.xml ]) step ([ $class : JavadocArchiver , javadocDir: build/docs/javadoc ]) stage SonarQube sh ${gradleHome}/bin/gradle sonarqube -Dsonar.host.url=http://sonarqube5-instance:9000 stash workspace } catch ( err ) { archive build/**/*.html echo Caught: ${err} currentBuild . result = FAILURE } } } } node ( docker ) { timestamps { timeout ( time: 15 , unit: MINUTES ) { deleteDir () unstash workspace stage Build Docker image sh ./build.sh def image = docker . image ( keep-backend-spring-img ) stage Push Docker image try { sh docker tag keep-backend-spring-img nexus.docker:18443/flusso/keep-backend-spring-img:latest sh docker push nexus.docker:18443/flusso/keep-backend-spring-img:latest } catch ( err ) { archive build/**/*.html echo Caught: ${err} currentBuild . result = FAILURE } } } }","title":"Maven Groovy DSL Example"},{"location":"jenkins-pipeline-examples/maven-groovy-dsl/#maven-groovy-dsl-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 node { timestamps { timeout ( time: 15 , unit: MINUTES ) { deleteDir () stage SCM //git branch: master , credentialsId: flusso-gitlab , url: https://gitlab.flusso.nl/keep/keep-backend-spring.git checkout scm env . JAVA_HOME = ${tool JDK 8 Latest } env . PATH = ${env.JAVA_HOME}/bin:${env.PATH} sh java -version try { def gradleHome = tool name: Gradle Latest , type: hudson.plugins.gradle.GradleInstallation stage Build sh ${gradleHome}/bin/gradle clean build javadoc step ([ $class : CheckStylePublisher , canComputeNew: false , defaultEncoding: , healthy: , pattern: build/reports/checkstyle/main.xml , unHealthy: ]) step ([ $class : JUnitResultArchiver , testResults: build/test-results/*.xml ]) step ([ $class : JavadocArchiver , javadocDir: build/docs/javadoc ]) stage SonarQube sh ${gradleHome}/bin/gradle sonarqube -Dsonar.host.url=http://sonarqube5-instance:9000 stash workspace } catch ( err ) { archive build/**/*.html echo Caught: ${err} currentBuild . result = FAILURE } } } } node ( docker ) { timestamps { timeout ( time: 15 , unit: MINUTES ) { deleteDir () unstash workspace stage Build Docker image sh ./build.sh def image = docker . image ( keep-backend-spring-img ) stage Push Docker image try { sh docker tag keep-backend-spring-img nexus.docker:18443/flusso/keep-backend-spring-img:latest sh docker push nexus.docker:18443/flusso/keep-backend-spring-img:latest } catch ( err ) { archive build/**/*.html echo Caught: ${err} currentBuild . result = FAILURE } } } }","title":"Maven Groovy DSL Example"},{"location":"jenkinsx/aks-boot-core/","text":"Jenkins X On AKS With JX Boot CloudBees Core The goal of the guide is the following: manage CloudBees Core on Modern via Jenkins X in its own environment/namespace. To make it more interesting, we add more variables in the mix in the form of \"requirements\". cluster must NOT run on GKE, Jenkins X works pretty well there and doesn't teach us much every exposed service MUST use TLS, no excuses we do not want to create a certificate for every service that uses TLS as much as possible must be Configuration-as-Code In conclusion: We use Terraform to manage the Kubernetes Cluster on AKS JX Boot to manage Jenkins X We use Google CloudDNS to manage the DNS this enables us to validate an entire subdomain via Let's Encrypt in one go Note Unfortunately, these are already quite a lot of requirements. The Vault integration on anywhere but GKE is not stable. So we cheat and use local storage for credentials, meaning we need to use jx boot every time to upgrade the cluster. We will come back to this! Create AKS Cluster Either create a cluster via AKS Terraform (recommended) or via AKS CLI . Install Jenkins X Boot Config Make a fork of the jenkins-x-boot-config repository and clone it. 1 GH_USER = 1 2 git clone https://github.com/ ${ GH_USER } /jenkins-x-boot-config.git cd jenkins-x-boot-config Changes to make: provider from gke to aks set domain set clustername set external dns (see below) set repository value for each environments (not dev) as below 1 2 - key : staging repository : environment-jx-aks-staging External DNS Using Google CloudDNS: login to the GCP account you want to use enable CloudDNS API by going to it create a CloudDNS zone for your subdomain if the main domain is example . com - aks . example . com once created, you get NS entries, copy these (usualy in the form ns - cloud - X { 1 - 4 } . googledomains . com in your Domain's DNS configuration, map your subdomain to these NS entries create a service account that can use CloudDNS API add the Google Project to which the Service Account belongs to: jx - requirements . yaml and values . yaml export the json configuration file rename the file to credentials . json create secret a secret in Kubernetes kubectl create secret generic external - dns - gcp - sa --from-file=credentials.json fix external dns values template - systems / external - dns / values . tmpl . yaml add project: {{ .Requirements.cluster.project }} to external - dns . google Important You have to create the secret external - dns - gcp - sa in every namespace you set up TLS via the dns01 challenge. jx-requirements.yaml We're omitting the default values as much as possible, such as the dev and production environments. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 cluster : environmentGitOwner : YOUR GITHUB ACCOUNT gitKind : github gitName : github gitServer : https://github.com namespace : jx project : your-google-project provider : aks environments : - ingress : domain : staging.aks.example.com externalDNS : true namespaceSubDomain : tls : email : YOUR EMAIL ADDRESS enabled : true production : true key : staging repository : environment-jx-aks-staging gitops : true ingress : domain : aks.example.com externalDNS : true namespaceSubDomain : -jx. tls : email : YOUR EMAIL ADDRESS enabled : true production : true kaniko : true secretStorage : local values.yaml 1 2 cluster : projectID : your-google-project TLS Config Update the jx - requirements . yaml , make sure ingress configuration is correct: 1 2 3 4 5 6 7 8 ingress : domain : aks.example.com externalDNS : true namespaceSubDomain : -jx. tls : email : admin@example.com enabled : true production : true If all is done correctly with the CloudDNS configuration, the external dns will contain all the entries of the jx services (such as hook, chartmuseum) and certmanager will be able to verify the domain with Let's Encrypt. Docker Registry Config values.yaml 1 2 jenkins-x-platform : dockerRegistry : myacr.azurecr.io This was not enough, added it to the values template: env / jenkins - x - platform / values . tmpl . yaml 1 dockerRegistry : myacr.azurecr.io TLS For Application In Environment create issuer create certificate Note This implies you need to run jx boot at least once before working on your environment configuration! Easiest way I found, was to copy the yaml from the issuer and certificate in the jx namespace. You then remove the unnecesary elements, those generated by Kubernetes itself (such as creation date, status, etc). You have to change the domain name and hosts values, as they should now point to the subdomain corresponding to this environment (unless its production). Once the files are good, you add them to your environment. You do so, by adding them to the templates folder - env / templates . 1 2 3 4 5 6 7 8 9 10 11 12 13 . \u251c\u2500\u2500 Jenkinsfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 env \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 requirements.yaml \u2502 \u251c\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 certificate.yaml \u2502 \u2502 \u2514\u2500\u2500 issuer.yaml \u2502 \u2514\u2500\u2500 values.yaml \u2514\u2500\u2500 jenkins-x.yml 1 kubectl -n jx get issuer letsencrypt-prod -o yaml 1 kubectl -n jx get certificate tls- unique to your cluster -p -o yaml issuer.yaml The end result should look like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : letsencrypt-prod spec : acme : email : admin@example.com privateKeySecretRef : name : letsencrypt-prod server : https://acme-v02.api.letsencrypt.org/directory solvers : - dns01 : clouddns : project : your-google-project serviceAccountSecretRef : key : credentials.json name : external-dns-gcp-sa selector : dnsNames : - *.staging.aks.example.com - staging.aks.example.com certificate.yaml 1 2 3 4 5 6 7 8 9 10 11 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : tls-staging-aks-example-com-p spec : commonName : *.staging.aks.example.com dnsNames : - *.staging.aks.example.com issuerRef : name : letsencrypt-prod secretName : tls-staging-aks-example-com-p Install CloudBees Core In order to install CloudBees Core with TLS, we need the following: TLS configuration for the environment Core is landing in (see above on how) add CloudBees Core as a requirement to the env / requirements . yaml add configuration for CloudBees Core to the env / values . yaml requirements.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 dependencies : - name : exposecontroller version : 2.3.89 repository : http://chartmuseum.jenkins-x.io alias : expose - name : exposecontroller version : 2.3.89 repository : http://chartmuseum.jenkins-x.io alias : cleanup - name : cloudbees-core version : 2.176.203 repository : https://charts.cloudbees.com/public/cloudbees alias : cbcore values.yaml Important The value you've set for the alias in the requirements, is your entrypoint for the configuration in the values . yaml ! Also, take care to change the following values to reflect your environment! * OperationsCenter.HostName * OperationsCenter.Ingress.tls.Host * OperationsCenter.Ingress.tls.SecretName 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cbcore : OperationsCenter : CSRF : ProxyCompatibility : true HostName : cbcore.staging.aks.example.com Ingress : Annotations : kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : true nginx.ingress.kubernetes.io/app-root : https://$best_http_host/cjoc/teams-check/ nginx.ingress.kubernetes.io/proxy-body-size : 50m nginx.ingress.kubernetes.io/proxy-request-buffering : off nginx.ingress.kubernetes.io/ssl-redirect : true tls : Enable : true Host : cbcore.staging.aks.example.com SecretName : tls-staging-aks-example-com-p ServiceType : ClusterIP nginx-ingress : Enabled : false Resources https://cloud.google.com/iam/docs/creating-managing-service-account-keys#iam-service-account-keys-create-console https://medium.com/google-cloud/kubernetes-w-lets-encrypt-cloud-dns-c888b2ff8c0e https://support.google.com/domains/answer/3290309?hl=en-GB ref_topic=9018335 https://thorsten-hans.com/how-to-use-private-azure-container-registry-with-kubernetes https://cloud.google.com/dns/docs/migrating","title":"JX Boot & CloudBees Core"},{"location":"jenkinsx/aks-boot-core/#jenkins-x-on-aks-with-jx-boot-cloudbees-core","text":"The goal of the guide is the following: manage CloudBees Core on Modern via Jenkins X in its own environment/namespace. To make it more interesting, we add more variables in the mix in the form of \"requirements\". cluster must NOT run on GKE, Jenkins X works pretty well there and doesn't teach us much every exposed service MUST use TLS, no excuses we do not want to create a certificate for every service that uses TLS as much as possible must be Configuration-as-Code In conclusion: We use Terraform to manage the Kubernetes Cluster on AKS JX Boot to manage Jenkins X We use Google CloudDNS to manage the DNS this enables us to validate an entire subdomain via Let's Encrypt in one go Note Unfortunately, these are already quite a lot of requirements. The Vault integration on anywhere but GKE is not stable. So we cheat and use local storage for credentials, meaning we need to use jx boot every time to upgrade the cluster. We will come back to this!","title":"Jenkins X On AKS With JX Boot &amp; CloudBees Core"},{"location":"jenkinsx/aks-boot-core/#create-aks-cluster","text":"Either create a cluster via AKS Terraform (recommended) or via AKS CLI .","title":"Create AKS Cluster"},{"location":"jenkinsx/aks-boot-core/#install-jenkins-x","text":"","title":"Install Jenkins X"},{"location":"jenkinsx/aks-boot-core/#boot-config","text":"Make a fork of the jenkins-x-boot-config repository and clone it. 1 GH_USER = 1 2 git clone https://github.com/ ${ GH_USER } /jenkins-x-boot-config.git cd jenkins-x-boot-config Changes to make: provider from gke to aks set domain set clustername set external dns (see below) set repository value for each environments (not dev) as below 1 2 - key : staging repository : environment-jx-aks-staging","title":"Boot Config"},{"location":"jenkinsx/aks-boot-core/#external-dns","text":"Using Google CloudDNS: login to the GCP account you want to use enable CloudDNS API by going to it create a CloudDNS zone for your subdomain if the main domain is example . com - aks . example . com once created, you get NS entries, copy these (usualy in the form ns - cloud - X { 1 - 4 } . googledomains . com in your Domain's DNS configuration, map your subdomain to these NS entries create a service account that can use CloudDNS API add the Google Project to which the Service Account belongs to: jx - requirements . yaml and values . yaml export the json configuration file rename the file to credentials . json create secret a secret in Kubernetes kubectl create secret generic external - dns - gcp - sa --from-file=credentials.json fix external dns values template - systems / external - dns / values . tmpl . yaml add project: {{ .Requirements.cluster.project }} to external - dns . google Important You have to create the secret external - dns - gcp - sa in every namespace you set up TLS via the dns01 challenge. jx-requirements.yaml We're omitting the default values as much as possible, such as the dev and production environments. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 cluster : environmentGitOwner : YOUR GITHUB ACCOUNT gitKind : github gitName : github gitServer : https://github.com namespace : jx project : your-google-project provider : aks environments : - ingress : domain : staging.aks.example.com externalDNS : true namespaceSubDomain : tls : email : YOUR EMAIL ADDRESS enabled : true production : true key : staging repository : environment-jx-aks-staging gitops : true ingress : domain : aks.example.com externalDNS : true namespaceSubDomain : -jx. tls : email : YOUR EMAIL ADDRESS enabled : true production : true kaniko : true secretStorage : local values.yaml 1 2 cluster : projectID : your-google-project","title":"External DNS"},{"location":"jenkinsx/aks-boot-core/#tls-config","text":"Update the jx - requirements . yaml , make sure ingress configuration is correct: 1 2 3 4 5 6 7 8 ingress : domain : aks.example.com externalDNS : true namespaceSubDomain : -jx. tls : email : admin@example.com enabled : true production : true If all is done correctly with the CloudDNS configuration, the external dns will contain all the entries of the jx services (such as hook, chartmuseum) and certmanager will be able to verify the domain with Let's Encrypt.","title":"TLS Config"},{"location":"jenkinsx/aks-boot-core/#docker-registry-config","text":"values.yaml 1 2 jenkins-x-platform : dockerRegistry : myacr.azurecr.io This was not enough, added it to the values template: env / jenkins - x - platform / values . tmpl . yaml 1 dockerRegistry : myacr.azurecr.io","title":"Docker Registry Config"},{"location":"jenkinsx/aks-boot-core/#tls-for-application-in-environment","text":"create issuer create certificate Note This implies you need to run jx boot at least once before working on your environment configuration! Easiest way I found, was to copy the yaml from the issuer and certificate in the jx namespace. You then remove the unnecesary elements, those generated by Kubernetes itself (such as creation date, status, etc). You have to change the domain name and hosts values, as they should now point to the subdomain corresponding to this environment (unless its production). Once the files are good, you add them to your environment. You do so, by adding them to the templates folder - env / templates . 1 2 3 4 5 6 7 8 9 10 11 12 13 . \u251c\u2500\u2500 Jenkinsfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 env \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 requirements.yaml \u2502 \u251c\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 certificate.yaml \u2502 \u2502 \u2514\u2500\u2500 issuer.yaml \u2502 \u2514\u2500\u2500 values.yaml \u2514\u2500\u2500 jenkins-x.yml 1 kubectl -n jx get issuer letsencrypt-prod -o yaml 1 kubectl -n jx get certificate tls- unique to your cluster -p -o yaml issuer.yaml The end result should look like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : letsencrypt-prod spec : acme : email : admin@example.com privateKeySecretRef : name : letsencrypt-prod server : https://acme-v02.api.letsencrypt.org/directory solvers : - dns01 : clouddns : project : your-google-project serviceAccountSecretRef : key : credentials.json name : external-dns-gcp-sa selector : dnsNames : - *.staging.aks.example.com - staging.aks.example.com certificate.yaml 1 2 3 4 5 6 7 8 9 10 11 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : tls-staging-aks-example-com-p spec : commonName : *.staging.aks.example.com dnsNames : - *.staging.aks.example.com issuerRef : name : letsencrypt-prod secretName : tls-staging-aks-example-com-p","title":"TLS For Application In Environment"},{"location":"jenkinsx/aks-boot-core/#install-cloudbees-core","text":"In order to install CloudBees Core with TLS, we need the following: TLS configuration for the environment Core is landing in (see above on how) add CloudBees Core as a requirement to the env / requirements . yaml add configuration for CloudBees Core to the env / values . yaml","title":"Install CloudBees Core"},{"location":"jenkinsx/aks-boot-core/#requirementsyaml","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 dependencies : - name : exposecontroller version : 2.3.89 repository : http://chartmuseum.jenkins-x.io alias : expose - name : exposecontroller version : 2.3.89 repository : http://chartmuseum.jenkins-x.io alias : cleanup - name : cloudbees-core version : 2.176.203 repository : https://charts.cloudbees.com/public/cloudbees alias : cbcore","title":"requirements.yaml"},{"location":"jenkinsx/aks-boot-core/#valuesyaml","text":"Important The value you've set for the alias in the requirements, is your entrypoint for the configuration in the values . yaml ! Also, take care to change the following values to reflect your environment! * OperationsCenter.HostName * OperationsCenter.Ingress.tls.Host * OperationsCenter.Ingress.tls.SecretName 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cbcore : OperationsCenter : CSRF : ProxyCompatibility : true HostName : cbcore.staging.aks.example.com Ingress : Annotations : kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : true nginx.ingress.kubernetes.io/app-root : https://$best_http_host/cjoc/teams-check/ nginx.ingress.kubernetes.io/proxy-body-size : 50m nginx.ingress.kubernetes.io/proxy-request-buffering : off nginx.ingress.kubernetes.io/ssl-redirect : true tls : Enable : true Host : cbcore.staging.aks.example.com SecretName : tls-staging-aks-example-com-p ServiceType : ClusterIP nginx-ingress : Enabled : false","title":"values.yaml"},{"location":"jenkinsx/aks-boot-core/#resources","text":"https://cloud.google.com/iam/docs/creating-managing-service-account-keys#iam-service-account-keys-create-console https://medium.com/google-cloud/kubernetes-w-lets-encrypt-cloud-dns-c888b2ff8c0e https://support.google.com/domains/answer/3290309?hl=en-GB ref_topic=9018335 https://thorsten-hans.com/how-to-use-private-azure-container-registry-with-kubernetes https://cloud.google.com/dns/docs/migrating","title":"Resources"},{"location":"jenkinsx/buildpack/","text":"Build Packs There are multiple ways to create your own buildpack for Jenkins X. Start from a working example : either create a quickstart project or import your existing application. Make the build and promotions work and then create a new buildpack by making the same changes (parameterized where applicable) to a copy of the buildpack you started from. Start from a working example We're going to build a buildpack for the following application: Micronaut framework build with Gradle with a Redis datastore with a TLS certificate for the ingress (https) Create Micronaut application create application via Micronaut CLI add a controller enable default healthendpoint import application with Jenkins X update helm chart: change healtcheck endpoint update helm chart: add dependency on Redis update values: set redis to not use a password 1 mn create-app example.micronaut.complete --features = kotlin,spek,tracing-jaeger,redis-lettuce 1 jx import Secrets 1 2 helm repo add soluto https://charts.soluto.io helm repo update 1 helm upgrade --install kamus soluto/kamus","title":"BuildPack"},{"location":"jenkinsx/buildpack/#build-packs","text":"There are multiple ways to create your own buildpack for Jenkins X. Start from a working example : either create a quickstart project or import your existing application. Make the build and promotions work and then create a new buildpack by making the same changes (parameterized where applicable) to a copy of the buildpack you started from.","title":"Build Packs"},{"location":"jenkinsx/buildpack/#start-from-a-working-example","text":"We're going to build a buildpack for the following application: Micronaut framework build with Gradle with a Redis datastore with a TLS certificate for the ingress (https)","title":"Start from a working example"},{"location":"jenkinsx/buildpack/#create-micronaut-application","text":"create application via Micronaut CLI add a controller enable default healthendpoint import application with Jenkins X update helm chart: change healtcheck endpoint update helm chart: add dependency on Redis update values: set redis to not use a password 1 mn create-app example.micronaut.complete --features = kotlin,spek,tracing-jaeger,redis-lettuce 1 jx import","title":"Create Micronaut application"},{"location":"jenkinsx/buildpack/#secrets","text":"1 2 helm repo add soluto https://charts.soluto.io helm repo update 1 helm upgrade --install kamus soluto/kamus","title":"Secrets"},{"location":"jenkinsx/custom-domain/","text":"Custom Domain At Creation Time After Creation Changing Domain","title":"Custom Domain"},{"location":"jenkinsx/custom-domain/#custom-domain","text":"","title":"Custom Domain"},{"location":"jenkinsx/custom-domain/#at-creation-time","text":"","title":"At Creation Time"},{"location":"jenkinsx/custom-domain/#after-creation","text":"","title":"After Creation"},{"location":"jenkinsx/custom-domain/#changing-domain","text":"","title":"Changing Domain"},{"location":"jenkinsx/hello-world/","text":"Hello World Demo Create GKE + Jenkins X cluster 1 2 3 4 5 6 7 8 9 10 11 12 export JX_CLUSTER_NAME = joostvdg export JX_ENV_PREFIX = joostvdg export JX_ADMIN_PSS = vXDzpiaVAthneXJR355J7PBT export JX_DOMAIN = jx.kearos.net export JX_GIT_USER = joostvdg export JX_API_TOKEN = 61edcbf6507d31b3f2fe811baa82aa6de33db001 export JX_ORG = demomon export JX_GCE_PROJECT = ps-dev-201405 export JX_K8S_REGION = europe-west4 export JX_K8S_ZONE = europe-west4-a export GKE_NODE_LOCATIONS = ${ REGION } -a, ${ REGION } -b export JX_K8S_VERSION = Get supported K8S versions Zonal 1 gcloud container get-server-config --zone ${ JX_K8S_ZONE } Regional 1 gcloud container get-server-config --region ${ JX_K8S_REGION } 1 export JX_K8S_VERSION = 1 .11.7-gke.4 Create regional cluster w/ Domain Currently only possible if you create a regional cluster first and then install jx. 1 2 3 4 5 6 7 8 9 10 gcloud container clusters create ${ JX_CLUSTER_NAME } \\ --region ${ JX_K8S_REGION } --node-locations ${ GKE_NODE_LOCATIONS } \\ --cluster-version ${ JX_K8S_VERSION } \\ --enable-pod-security-policy \\ --enable-network-policy \\ --num-nodes 2 --machine-type n1-standard-2 \\ --min-nodes 2 --max-nodes 3 \\ --enable-autoupgrade \\ --enable-autoscaling \\ --labels = owner = jvandergriendt,purpose = practice 1 jx install Create zonal cluster w/ Domain 1 2 3 4 5 6 7 8 9 10 11 12 jx create cluster gke \\ --cluster-name = ${ JX_CLUSTER_NAME } \\ --default-admin-password = ${ JX_ADMIN_PSS } \\ --domain = ${ JX_DOMAIN } \\ --kubernetes-version = ${ JX_K8S_VERSION } \\ --machine-type = n1-standard-2 \\ --max-num-nodes = 3 \\ --min-num-nodes = 2 \\ --project-id = ${ JX_GCE_PROJECT } \\ --zone = ${ JX_K8S_ZONE } \\ --kaniko = true \\ --skip-login Reinstall 1 2 3 4 5 6 7 jx install \\ --default-environment-prefix = $JX_ENV_PREFIX \\ --git-api-token = $JX_API_TOKEN \\ --git-username = $JX_GIT_USER \\ --environment-git-owner = $JX_GIT_USER \\ --default-admin-password = ${ JX_ADMIN_PSS } \\ --domain = ${ JX_DOMAIN } Configure Domain Once the cluster is up and the Jenkins X basics are installed, jx will prompt us about missing an ingress controller. 1 ? No existing ingress controller found in the kube - system namespace , shall we install one ? Yes Reply yes, and in a little while, you will see the following message: 1 2 You can now configure your wildcard DNS jx.kearos.net to point to 35 .204.0.182 nginx ingress controller installed and configured We can now go to our Domain configuration and set *.jx. ${ DomainName } to the ip listed. If you're using Google Domains by any chance, you create an A class record for * . jx with ip 35 . 204 . 0 . 182 . Unfortunately, that's not enough, as the ingress resources created by jx after will have a different IP address. So we have to add a second IP address to your Class A record. Still assuming GKE, you can retrieve the second IP address as follows: 1 2 INGRESS_IP = $( kubectl get ing chartmuseum -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) echo $INGRESS_IP To test the domain, you can do the following: 1 CM_URL = $( kubectl get ing chartmuseum -o jsonpath = {.spec.rules[0].host} ) Curl 1 curl $CM_URL Httpie 1 http $CM_URL Configure TLS If we have a proper domain configured and working, we can also enable TLS. As we've not done so at the start, we will have to update the Ingress configuration. For all the options for updating the Ingress configuration, use the command below. 1 jx upgrade ingress --help To configure TLS for our ingresses, we need TLS certificates. Jenkins X does this via Let's Encrypt, which in Kubernetes is easily done via Certmanager . The command we will issue will ask us if we want to install CertManager , and then delete all existing ingress resources and recreate them with the certificate. Unfortunately, when in Batch mode ( - b ) it does not install CertManager nor is there an option to force it in this case. When asked if we want to delete and recreate the existing ingress rules, say yes ( y ). Select the expose type , which should be Ingress (route is for OpenShift). Confirm your domain - do not change it, as this upgrade does NOT change your domain configuration everywhere and you will end up with a broken system Say yes to cluster wide TLS If you're certain your Domain works, select the production LetEncrypt configuration, else choose staging for tests Confirm your email address and the summary Agree with installing CertManager Do Not agree with updating the webhooks (see below) 1 jx upgrade ingress --cluster --verbose Warning There's currently a bug with changing the webhooks via this command; see issue #3115 It somehow can only select a different GitHub user than the current one, which makes no sense for an UPDATE. So we must update the webhooks manually ourselves! Manually update webhooks Due to issue #3115 we need to manually update our webhooks for the environment repositories. If you're not sure where your environment repositories are, you can retrieve them with the command below: 1 js get env Open each environment repository, go to the settings tabs (top right), open the webhooks menu (on the left), and edit the webhook. Simply change the http : // to https : // and save. Warning If you've selected the staging configuration for Let's Encrypt, you have set the SSL configuration to Disable ( not recommended ) . Create options -- buildpack = : The name of the build pack to use for the Team --vault --helm3=false: Use helm3 to install Jenkins X which does not use Tiller --kaniko=false -- urltemplate = : For ingress ; exposers can set the urltemplate to expose Addons 1 2 3 4 5 6 7 8 9 10 11 12 13 14 create addon ambassador Create an ambassador addon create addon anchore Create the Anchore addon for verifying container images create addon cloudbees Create the CloudBees app for Kubernetes ( a web console for working with CI/CD, Environments and GitOps ) create addon flagger Create the Flagger addon for Canary deployments create addon gitea Create a Gitea addon for hosting Git repositories create addon istio Create the Istio addon for service mesh create addon knative-build Create the knative build addon create addon kubeless Create a kubeless addon for hosting Git repositories create addon owasp-zap Create the OWASP Zed Attack Proxy addon for dynamic security checks against running apps create addon pipeline-events Create the pipeline events addon create addon prometheus Creates a prometheus addon create addon prow Create a Prow addon create addon sso Create a SSO addon for Single Sign-On create addon vault-operator Create an vault-operator addon for Hashicorp Vault Upgrade 1 jx upgrade --help 1 2 3 4 5 6 7 8 upgrade addons Upgrades any Addons added to Jenkins X if there are any new releases available upgrade apps Upgrades any Apps to the latest release upgrade binaries Upgrades the command line binaries ( like helm or eksctl ) - if there are new versions available upgrade cli Upgrades the command line applications - if there are new versions available upgrade cluster Upgrades the Kubernetes master to the specified version upgrade extensions Upgrades the Jenkins X extensions available to this Jenkins X install if there are new versions available upgrade ingress Upgrades Ingress rules upgrade platform Upgrades the Jenkins X platform if there is a new release available Go lang example 1 jx create quickstartjc select golang - http Promote 1 2 APP = jx-go-demo-5 VERSION = 0 .0.2 1 jx promote ${ APP } --version $VERSION --env production -b 1 jx get apps Spring Boot Example 1 jx create spring -d web -d actuator Serverless 1 2 3 4 5 6 jx create terraform gke \\ --vault = true \\ --cluster = ${ JX_CLUSTER_NAME } = gke \\ --gke-project-id = ${ JX_GCE_PROJECT } \\ --prow \\ --skip-login Demo - Show JX Stuff GitOps 1 2 3 4 5 6 Get environments: jx get environments Watch pipeline activity via: jx get activity -f golang-http -w Browse the pipeline log via: jx get build logs demomon/golang-http/master Open the Jenkins console via jx console You can list the pipelines via: jx get pipelines When the pipeline is complete: jx get applications Build It explain build packs jx create quickstart show Jenkinsfile open application jx get applications create new branch git checkout - b wip change main.go commit git - a - m better message push git push remote wip open PR page explain tide add cat picture: / meow test it add comment / test this open logs jx logs - k open PR environment approve the change / approve or add approved label open logs for next step promote: jx promote myapp --version 1.2.3 --env production promote: jx promote ${ APP } --version ${ VERSION } --env production","title":"HelloWorld"},{"location":"jenkinsx/hello-world/#hello-world-demo","text":"","title":"Hello World Demo"},{"location":"jenkinsx/hello-world/#create-gke-jenkins-x-cluster","text":"1 2 3 4 5 6 7 8 9 10 11 12 export JX_CLUSTER_NAME = joostvdg export JX_ENV_PREFIX = joostvdg export JX_ADMIN_PSS = vXDzpiaVAthneXJR355J7PBT export JX_DOMAIN = jx.kearos.net export JX_GIT_USER = joostvdg export JX_API_TOKEN = 61edcbf6507d31b3f2fe811baa82aa6de33db001 export JX_ORG = demomon export JX_GCE_PROJECT = ps-dev-201405 export JX_K8S_REGION = europe-west4 export JX_K8S_ZONE = europe-west4-a export GKE_NODE_LOCATIONS = ${ REGION } -a, ${ REGION } -b export JX_K8S_VERSION =","title":"Create GKE + Jenkins X cluster"},{"location":"jenkinsx/hello-world/#get-supported-k8s-versions","text":"Zonal 1 gcloud container get-server-config --zone ${ JX_K8S_ZONE } Regional 1 gcloud container get-server-config --region ${ JX_K8S_REGION } 1 export JX_K8S_VERSION = 1 .11.7-gke.4","title":"Get supported K8S versions"},{"location":"jenkinsx/hello-world/#create-regional-cluster-w-domain","text":"Currently only possible if you create a regional cluster first and then install jx. 1 2 3 4 5 6 7 8 9 10 gcloud container clusters create ${ JX_CLUSTER_NAME } \\ --region ${ JX_K8S_REGION } --node-locations ${ GKE_NODE_LOCATIONS } \\ --cluster-version ${ JX_K8S_VERSION } \\ --enable-pod-security-policy \\ --enable-network-policy \\ --num-nodes 2 --machine-type n1-standard-2 \\ --min-nodes 2 --max-nodes 3 \\ --enable-autoupgrade \\ --enable-autoscaling \\ --labels = owner = jvandergriendt,purpose = practice 1 jx install","title":"Create regional cluster w/ Domain"},{"location":"jenkinsx/hello-world/#create-zonal-cluster-w-domain","text":"1 2 3 4 5 6 7 8 9 10 11 12 jx create cluster gke \\ --cluster-name = ${ JX_CLUSTER_NAME } \\ --default-admin-password = ${ JX_ADMIN_PSS } \\ --domain = ${ JX_DOMAIN } \\ --kubernetes-version = ${ JX_K8S_VERSION } \\ --machine-type = n1-standard-2 \\ --max-num-nodes = 3 \\ --min-num-nodes = 2 \\ --project-id = ${ JX_GCE_PROJECT } \\ --zone = ${ JX_K8S_ZONE } \\ --kaniko = true \\ --skip-login","title":"Create zonal cluster w/ Domain"},{"location":"jenkinsx/hello-world/#reinstall","text":"1 2 3 4 5 6 7 jx install \\ --default-environment-prefix = $JX_ENV_PREFIX \\ --git-api-token = $JX_API_TOKEN \\ --git-username = $JX_GIT_USER \\ --environment-git-owner = $JX_GIT_USER \\ --default-admin-password = ${ JX_ADMIN_PSS } \\ --domain = ${ JX_DOMAIN }","title":"Reinstall"},{"location":"jenkinsx/hello-world/#configure-domain","text":"Once the cluster is up and the Jenkins X basics are installed, jx will prompt us about missing an ingress controller. 1 ? No existing ingress controller found in the kube - system namespace , shall we install one ? Yes Reply yes, and in a little while, you will see the following message: 1 2 You can now configure your wildcard DNS jx.kearos.net to point to 35 .204.0.182 nginx ingress controller installed and configured We can now go to our Domain configuration and set *.jx. ${ DomainName } to the ip listed. If you're using Google Domains by any chance, you create an A class record for * . jx with ip 35 . 204 . 0 . 182 . Unfortunately, that's not enough, as the ingress resources created by jx after will have a different IP address. So we have to add a second IP address to your Class A record. Still assuming GKE, you can retrieve the second IP address as follows: 1 2 INGRESS_IP = $( kubectl get ing chartmuseum -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) echo $INGRESS_IP To test the domain, you can do the following: 1 CM_URL = $( kubectl get ing chartmuseum -o jsonpath = {.spec.rules[0].host} ) Curl 1 curl $CM_URL Httpie 1 http $CM_URL","title":"Configure Domain"},{"location":"jenkinsx/hello-world/#configure-tls","text":"If we have a proper domain configured and working, we can also enable TLS. As we've not done so at the start, we will have to update the Ingress configuration. For all the options for updating the Ingress configuration, use the command below. 1 jx upgrade ingress --help To configure TLS for our ingresses, we need TLS certificates. Jenkins X does this via Let's Encrypt, which in Kubernetes is easily done via Certmanager . The command we will issue will ask us if we want to install CertManager , and then delete all existing ingress resources and recreate them with the certificate. Unfortunately, when in Batch mode ( - b ) it does not install CertManager nor is there an option to force it in this case. When asked if we want to delete and recreate the existing ingress rules, say yes ( y ). Select the expose type , which should be Ingress (route is for OpenShift). Confirm your domain - do not change it, as this upgrade does NOT change your domain configuration everywhere and you will end up with a broken system Say yes to cluster wide TLS If you're certain your Domain works, select the production LetEncrypt configuration, else choose staging for tests Confirm your email address and the summary Agree with installing CertManager Do Not agree with updating the webhooks (see below) 1 jx upgrade ingress --cluster --verbose Warning There's currently a bug with changing the webhooks via this command; see issue #3115 It somehow can only select a different GitHub user than the current one, which makes no sense for an UPDATE. So we must update the webhooks manually ourselves!","title":"Configure TLS"},{"location":"jenkinsx/hello-world/#manually-update-webhooks","text":"Due to issue #3115 we need to manually update our webhooks for the environment repositories. If you're not sure where your environment repositories are, you can retrieve them with the command below: 1 js get env Open each environment repository, go to the settings tabs (top right), open the webhooks menu (on the left), and edit the webhook. Simply change the http : // to https : // and save. Warning If you've selected the staging configuration for Let's Encrypt, you have set the SSL configuration to Disable ( not recommended ) .","title":"Manually update webhooks"},{"location":"jenkinsx/hello-world/#create-options","text":"-- buildpack = : The name of the build pack to use for the Team --vault --helm3=false: Use helm3 to install Jenkins X which does not use Tiller --kaniko=false -- urltemplate = : For ingress ; exposers can set the urltemplate to expose","title":"Create options"},{"location":"jenkinsx/hello-world/#addons","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 create addon ambassador Create an ambassador addon create addon anchore Create the Anchore addon for verifying container images create addon cloudbees Create the CloudBees app for Kubernetes ( a web console for working with CI/CD, Environments and GitOps ) create addon flagger Create the Flagger addon for Canary deployments create addon gitea Create a Gitea addon for hosting Git repositories create addon istio Create the Istio addon for service mesh create addon knative-build Create the knative build addon create addon kubeless Create a kubeless addon for hosting Git repositories create addon owasp-zap Create the OWASP Zed Attack Proxy addon for dynamic security checks against running apps create addon pipeline-events Create the pipeline events addon create addon prometheus Creates a prometheus addon create addon prow Create a Prow addon create addon sso Create a SSO addon for Single Sign-On create addon vault-operator Create an vault-operator addon for Hashicorp Vault","title":"Addons"},{"location":"jenkinsx/hello-world/#upgrade","text":"1 jx upgrade --help 1 2 3 4 5 6 7 8 upgrade addons Upgrades any Addons added to Jenkins X if there are any new releases available upgrade apps Upgrades any Apps to the latest release upgrade binaries Upgrades the command line binaries ( like helm or eksctl ) - if there are new versions available upgrade cli Upgrades the command line applications - if there are new versions available upgrade cluster Upgrades the Kubernetes master to the specified version upgrade extensions Upgrades the Jenkins X extensions available to this Jenkins X install if there are new versions available upgrade ingress Upgrades Ingress rules upgrade platform Upgrades the Jenkins X platform if there is a new release available","title":"Upgrade"},{"location":"jenkinsx/hello-world/#go-lang-example","text":"1 jx create quickstartjc select golang - http","title":"Go lang example"},{"location":"jenkinsx/hello-world/#promote","text":"1 2 APP = jx-go-demo-5 VERSION = 0 .0.2 1 jx promote ${ APP } --version $VERSION --env production -b 1 jx get apps","title":"Promote"},{"location":"jenkinsx/hello-world/#spring-boot-example","text":"1 jx create spring -d web -d actuator","title":"Spring Boot Example"},{"location":"jenkinsx/hello-world/#serverless","text":"1 2 3 4 5 6 jx create terraform gke \\ --vault = true \\ --cluster = ${ JX_CLUSTER_NAME } = gke \\ --gke-project-id = ${ JX_GCE_PROJECT } \\ --prow \\ --skip-login","title":"Serverless"},{"location":"jenkinsx/hello-world/#demo-show-jx-stuff","text":"","title":"Demo - Show JX Stuff"},{"location":"jenkinsx/hello-world/#gitops","text":"1 2 3 4 5 6 Get environments: jx get environments Watch pipeline activity via: jx get activity -f golang-http -w Browse the pipeline log via: jx get build logs demomon/golang-http/master Open the Jenkins console via jx console You can list the pipelines via: jx get pipelines When the pipeline is complete: jx get applications","title":"GitOps"},{"location":"jenkinsx/hello-world/#build-it","text":"explain build packs jx create quickstart show Jenkinsfile open application jx get applications create new branch git checkout - b wip change main.go commit git - a - m better message push git push remote wip open PR page explain tide add cat picture: / meow test it add comment / test this open logs jx logs - k open PR environment approve the change / approve or add approved label open logs for next step promote: jx promote myapp --version 1.2.3 --env production promote: jx promote ${ APP } --version ${ VERSION } --env production","title":"Build It"},{"location":"jenkinsx/hybrid/","text":"Jenkins X Hybrid TLS Jenkins X Hybrid TLS is a configuration of Jenkins X using both Static Jenkins and Jenkins X Serverless with Tekton within the same cluster. As the TLS suffix hints at, it also uses TLS for both installations to make sure all the services and your applications are accessible via https with a valid certificate. Pre-requisites GCP account with active subscription with an active project with which you are authenticated gcloud CLI Jenkins X CLI jx httpie or curl Steps create JX cluster in GKE with static Jenkins without Nexus create Go (lang) quickstart configure TLS install Serverless Jenkins X in the same cluster create Spring Boot Quickstart configure TLS for Serverless namespaces only re-install Jenkins X with Nexus Static Prepare Variables 1 2 3 4 5 CLUSTER_NAME = #name of your cluster PROJECT = #name of your GCP project REGION = #GCP region to install cluster in GITHUB_USER = #your GitHub Username GITHUB_TOKEN = #GitHub apitoken myvalues.yaml We're going to use a demo application based on Go, so we don't need Nexus. To configure Jenkins X to skip Nexus' installation, create the file myvalues . yaml with the following contents: 1 2 3 4 nexus : enabled : false docker-registry : enabled : true Install JX Make sure you execute this command where you have the myvalues . yaml file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 jx create cluster gke \\ --cluster-name ${ CLUSTER_NAME } \\ --project-id ${ PROJECT } \\ --region ${ REGION } \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --git-username ${ GITHUB_USER } \\ --git-provider-kind github \\ --git-api-token ${ GITHUB_TOKEN } \\ --batch-mode Go Quickstart 1 2 3 4 5 6 jx create quickstart \\ -l go --org ${ GITHUB_USER } \\ --project-name jx-static-go \\ --import-mode = Jenkinsfile \\ --deploy-kind default \\ -b Watch activity You can either go to Jenkins and watch the job there: jx console or watch in your console via jx get activity . 1 jx get activity -f jx-static-go -w Once the build completes, you should see something like the line below, you can test the application. 1 Promoted 28m5s 1m41s Succeeded Application is at: http://jx-static-go.jx-staging.34.90.105.15.nip.io Test application To confirm the application is running in the staging environment: 1 jx get applications Which should show something like this: 1 2 APPLICATION STAGING PODS URL jx-static-go 0 .0.1 1 /1 http://jx-static-go.jx-staging. ${ LIB_IP } .nip.io 1 LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) 1 http jx-static-go.jx-staging. ${ LB_IP } .nip.io Which should show the following: 1 2 3 4 5 6 7 8 HTTP/1.1 200 OK Connection: keep-alive Content-Length: 43 Content-Type: text/plain ; charset = utf-8 Date: Thu, 13 Jun 2019 12 :17:39 GMT Server: nginx/1.15.8 Hello from: Jenkins X golang http example Configure TLS Make sure you have two things: the address of your LoadBalancer (see below how to retrieve this) a Domain name with a quick and easy DNS configuration (incl. wildcard support) Retrieve LoadBalancer address 1 LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) Configure DNS Go to your Domain provider of choice, if you don't have one, consider Google Domains for 12 Euro per year. They might no be the cheapest, but the service is great and works quick - changes like we're about to do, take a few minutes to be effectuated. Configure the following wildcards to direct to your LoadBalancer's IP address: * . jx - type A * . jx - staging - type A * . jx - production - type A * . serverless - type A (for the serverless section) Upgrade Ingress To configure TLS inside Jenkins X, we make use of Let's Encrypt and cert-manager . To get Jenkins X to configure TLS, we use the jx upgrade ingress command. 1 DOMAIN = #your domain name 1 2 3 jx upgrade ingress \\ --cluster true \\ --domain $DOMAIN Info To be sure, the Domain name above should the base hostname only. Any resource within your JX installation will automatically get the following domain name: { name } . { namespace } . { DOMAIN } . For example, if your domain is example . com Jenkins will become jenkins . jx . example . com . Test applications Confirm your application now has a https protocol. 1 jx get applications 1 http https://jx-static-go.jx-staging. ${ DOMAIN } Serverless Prepare The values for INGRESS_NS and INGRESS_DEP are the default based on the static install created above. If your ingress controller namespace and/or deployment have different names, replace the values. For the LB_IP , we're also assuming default names and namespaces. 1 2 3 4 5 6 7 8 PROVIDER = gke LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) DOMAIN_SUFFIX = #your domain name DOMAIN = serverless. ${ DOMAIN_SUFFIX } INGRESS_NS = kube-system INGRESS_DEP = jxing-nginx-ingress-controller INSTALL_NS = cdx PROJECT = #your GCP project Info We're going to use the cdx namespace, this will create namespaces such as cdx and cdx - staging . In order to avoid having to register every environment in at our DNS provider, we will use an additional domain prefix serverless . Making the domain serverless . { DOMAIN } and the JX components { name } . cdx . serverless . { DOMAIN } . Install Serverless JX 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b Spring Boot Quickstart Create quickstart 1 2 3 4 jx create spring -d web -d actuator \\ --group com.example \\ --artifact jx-spring-boot-demo \\ -b 1 cd jx-spring-boot-demo Add controller Assuming you kept the group the same, you should find a folder src / main / java / com / example / jxspringbootdemo containing a file, DemoApplication . java . We're going to have to add two files to the same folder: Greeting . java GreetingController . java Greeting 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package com.example.jxspringbootdemo ; public class Greeting { private final long id ; private final String content ; public Greeting ( long id , String content ) { this . id = id ; this . content = content ; } public long getId () { return id ; } public String getContent () { return content ; } } GreetingController 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package com.example.jxspringbootdemo ; import java.util.concurrent.atomic.AtomicLong ; import org.springframework.web.bind.annotation.RequestMapping ; import org.springframework.web.bind.annotation.RequestParam ; import org.springframework.web.bind.annotation.RestController ; @RestController public class GreetingController { private static final String template = Hello, %s! ; private final AtomicLong counter = new AtomicLong (); @RequestMapping ( /greeting ) public Greeting greeting ( @RequestParam ( value = name , defaultValue = World ) String name ) { return new Greeting ( counter . incrementAndGet (), String . format ( template , name )); } } Test application 1 jx get activity -f jx-cdx-spring-boot-demo-1 -w Re-Install with Nexus myvalues.yaml Our application didn't work because now we have an application that depends on a Maven repository. We have to \"re-install\" Jenkins X, to have it install Nexus for us in the cdx namespace. 1 2 3 4 nexus : enabled : true docker-registry : enabled : true Install Make sure you execute this command where you have the myvalues . yaml file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain serverless. $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b Test Application To trigger a new build, make a change - for example to the README . md and push it. 1 jx get activity -f jx-cdx-spring-boot-demo-1 -w 1 http jx-cdx-spring-boot-demo-1.cdx-staging.serverless. ${ DOMAIN } /greeting Configure TLS 1 jx upgrade ingress --domain $DOMAIN --namespaces cdx,cdx-staging Re-test application 1 ORG = #the GitHub user or organisation your application is in 1 2 3 4 jx update webhooks --repo = jx-cdx-spring-boot-demo-1 --org = ${ ORG } jx get applications jx get activity -f jx-cdx-spring-boot-demo-1 -w http https://jx-cdx-spring-boot-demo-1.cdx-staging.serverless. ${ DOMAIN } /greeting","title":"Hybrid"},{"location":"jenkinsx/hybrid/#jenkins-x-hybrid-tls","text":"Jenkins X Hybrid TLS is a configuration of Jenkins X using both Static Jenkins and Jenkins X Serverless with Tekton within the same cluster. As the TLS suffix hints at, it also uses TLS for both installations to make sure all the services and your applications are accessible via https with a valid certificate.","title":"Jenkins X Hybrid TLS"},{"location":"jenkinsx/hybrid/#pre-requisites","text":"GCP account with active subscription with an active project with which you are authenticated gcloud CLI Jenkins X CLI jx httpie or curl","title":"Pre-requisites"},{"location":"jenkinsx/hybrid/#steps","text":"create JX cluster in GKE with static Jenkins without Nexus create Go (lang) quickstart configure TLS install Serverless Jenkins X in the same cluster create Spring Boot Quickstart configure TLS for Serverless namespaces only re-install Jenkins X with Nexus","title":"Steps"},{"location":"jenkinsx/hybrid/#static","text":"","title":"Static"},{"location":"jenkinsx/hybrid/#prepare","text":"","title":"Prepare"},{"location":"jenkinsx/hybrid/#variables","text":"1 2 3 4 5 CLUSTER_NAME = #name of your cluster PROJECT = #name of your GCP project REGION = #GCP region to install cluster in GITHUB_USER = #your GitHub Username GITHUB_TOKEN = #GitHub apitoken","title":"Variables"},{"location":"jenkinsx/hybrid/#myvaluesyaml","text":"We're going to use a demo application based on Go, so we don't need Nexus. To configure Jenkins X to skip Nexus' installation, create the file myvalues . yaml with the following contents: 1 2 3 4 nexus : enabled : false docker-registry : enabled : true","title":"myvalues.yaml"},{"location":"jenkinsx/hybrid/#install-jx","text":"Make sure you execute this command where you have the myvalues . yaml file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 jx create cluster gke \\ --cluster-name ${ CLUSTER_NAME } \\ --project-id ${ PROJECT } \\ --region ${ REGION } \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --git-username ${ GITHUB_USER } \\ --git-provider-kind github \\ --git-api-token ${ GITHUB_TOKEN } \\ --batch-mode","title":"Install JX"},{"location":"jenkinsx/hybrid/#go-quickstart","text":"1 2 3 4 5 6 jx create quickstart \\ -l go --org ${ GITHUB_USER } \\ --project-name jx-static-go \\ --import-mode = Jenkinsfile \\ --deploy-kind default \\ -b","title":"Go Quickstart"},{"location":"jenkinsx/hybrid/#watch-activity","text":"You can either go to Jenkins and watch the job there: jx console or watch in your console via jx get activity . 1 jx get activity -f jx-static-go -w Once the build completes, you should see something like the line below, you can test the application. 1 Promoted 28m5s 1m41s Succeeded Application is at: http://jx-static-go.jx-staging.34.90.105.15.nip.io","title":"Watch activity"},{"location":"jenkinsx/hybrid/#test-application","text":"To confirm the application is running in the staging environment: 1 jx get applications Which should show something like this: 1 2 APPLICATION STAGING PODS URL jx-static-go 0 .0.1 1 /1 http://jx-static-go.jx-staging. ${ LIB_IP } .nip.io 1 LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) 1 http jx-static-go.jx-staging. ${ LB_IP } .nip.io Which should show the following: 1 2 3 4 5 6 7 8 HTTP/1.1 200 OK Connection: keep-alive Content-Length: 43 Content-Type: text/plain ; charset = utf-8 Date: Thu, 13 Jun 2019 12 :17:39 GMT Server: nginx/1.15.8 Hello from: Jenkins X golang http example","title":"Test application"},{"location":"jenkinsx/hybrid/#configure-tls","text":"Make sure you have two things: the address of your LoadBalancer (see below how to retrieve this) a Domain name with a quick and easy DNS configuration (incl. wildcard support)","title":"Configure TLS"},{"location":"jenkinsx/hybrid/#retrieve-loadbalancer-address","text":"1 LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = {.status.loadBalancer.ingress[0].ip} )","title":"Retrieve LoadBalancer address"},{"location":"jenkinsx/hybrid/#configure-dns","text":"Go to your Domain provider of choice, if you don't have one, consider Google Domains for 12 Euro per year. They might no be the cheapest, but the service is great and works quick - changes like we're about to do, take a few minutes to be effectuated. Configure the following wildcards to direct to your LoadBalancer's IP address: * . jx - type A * . jx - staging - type A * . jx - production - type A * . serverless - type A (for the serverless section)","title":"Configure DNS"},{"location":"jenkinsx/hybrid/#upgrade-ingress","text":"To configure TLS inside Jenkins X, we make use of Let's Encrypt and cert-manager . To get Jenkins X to configure TLS, we use the jx upgrade ingress command. 1 DOMAIN = #your domain name 1 2 3 jx upgrade ingress \\ --cluster true \\ --domain $DOMAIN Info To be sure, the Domain name above should the base hostname only. Any resource within your JX installation will automatically get the following domain name: { name } . { namespace } . { DOMAIN } . For example, if your domain is example . com Jenkins will become jenkins . jx . example . com .","title":"Upgrade Ingress"},{"location":"jenkinsx/hybrid/#test-applications","text":"Confirm your application now has a https protocol. 1 jx get applications 1 http https://jx-static-go.jx-staging. ${ DOMAIN }","title":"Test applications"},{"location":"jenkinsx/hybrid/#serverless","text":"","title":"Serverless"},{"location":"jenkinsx/hybrid/#prepare_1","text":"The values for INGRESS_NS and INGRESS_DEP are the default based on the static install created above. If your ingress controller namespace and/or deployment have different names, replace the values. For the LB_IP , we're also assuming default names and namespaces. 1 2 3 4 5 6 7 8 PROVIDER = gke LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) DOMAIN_SUFFIX = #your domain name DOMAIN = serverless. ${ DOMAIN_SUFFIX } INGRESS_NS = kube-system INGRESS_DEP = jxing-nginx-ingress-controller INSTALL_NS = cdx PROJECT = #your GCP project Info We're going to use the cdx namespace, this will create namespaces such as cdx and cdx - staging . In order to avoid having to register every environment in at our DNS provider, we will use an additional domain prefix serverless . Making the domain serverless . { DOMAIN } and the JX components { name } . cdx . serverless . { DOMAIN } .","title":"Prepare"},{"location":"jenkinsx/hybrid/#install-serverless-jx","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b","title":"Install Serverless JX"},{"location":"jenkinsx/hybrid/#spring-boot-quickstart","text":"","title":"Spring Boot Quickstart"},{"location":"jenkinsx/hybrid/#create-quickstart","text":"1 2 3 4 jx create spring -d web -d actuator \\ --group com.example \\ --artifact jx-spring-boot-demo \\ -b 1 cd jx-spring-boot-demo","title":"Create quickstart"},{"location":"jenkinsx/hybrid/#add-controller","text":"Assuming you kept the group the same, you should find a folder src / main / java / com / example / jxspringbootdemo containing a file, DemoApplication . java . We're going to have to add two files to the same folder: Greeting . java GreetingController . java","title":"Add controller"},{"location":"jenkinsx/hybrid/#greeting","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package com.example.jxspringbootdemo ; public class Greeting { private final long id ; private final String content ; public Greeting ( long id , String content ) { this . id = id ; this . content = content ; } public long getId () { return id ; } public String getContent () { return content ; } }","title":"Greeting"},{"location":"jenkinsx/hybrid/#greetingcontroller","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package com.example.jxspringbootdemo ; import java.util.concurrent.atomic.AtomicLong ; import org.springframework.web.bind.annotation.RequestMapping ; import org.springframework.web.bind.annotation.RequestParam ; import org.springframework.web.bind.annotation.RestController ; @RestController public class GreetingController { private static final String template = Hello, %s! ; private final AtomicLong counter = new AtomicLong (); @RequestMapping ( /greeting ) public Greeting greeting ( @RequestParam ( value = name , defaultValue = World ) String name ) { return new Greeting ( counter . incrementAndGet (), String . format ( template , name )); } }","title":"GreetingController"},{"location":"jenkinsx/hybrid/#test-application_1","text":"1 jx get activity -f jx-cdx-spring-boot-demo-1 -w","title":"Test application"},{"location":"jenkinsx/hybrid/#re-install-with-nexus","text":"","title":"Re-Install with Nexus"},{"location":"jenkinsx/hybrid/#myvaluesyaml_1","text":"Our application didn't work because now we have an application that depends on a Maven repository. We have to \"re-install\" Jenkins X, to have it install Nexus for us in the cdx namespace. 1 2 3 4 nexus : enabled : true docker-registry : enabled : true","title":"myvalues.yaml"},{"location":"jenkinsx/hybrid/#install","text":"Make sure you execute this command where you have the myvalues . yaml file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain serverless. $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b","title":"Install"},{"location":"jenkinsx/hybrid/#test-application_2","text":"To trigger a new build, make a change - for example to the README . md and push it. 1 jx get activity -f jx-cdx-spring-boot-demo-1 -w 1 http jx-cdx-spring-boot-demo-1.cdx-staging.serverless. ${ DOMAIN } /greeting","title":"Test Application"},{"location":"jenkinsx/hybrid/#configure-tls_1","text":"1 jx upgrade ingress --domain $DOMAIN --namespaces cdx,cdx-staging","title":"Configure TLS"},{"location":"jenkinsx/hybrid/#re-test-application","text":"1 ORG = #the GitHub user or organisation your application is in 1 2 3 4 jx update webhooks --repo = jx-cdx-spring-boot-demo-1 --org = ${ ORG } jx get applications jx get activity -f jx-cdx-spring-boot-demo-1 -w http https://jx-cdx-spring-boot-demo-1.cdx-staging.serverless. ${ DOMAIN } /greeting","title":"Re-test application"},{"location":"jenkinsx/intro/","text":"Introduction to Jenkins X","title":"Introduction to Jenkins X"},{"location":"jenkinsx/intro/#introduction-to-jenkins-x","text":"","title":"Introduction to Jenkins X"},{"location":"jenkinsx/kubernetes-days/","text":"Jenkins X - Kubernetes Days Prepare 1 asciinema rec first.cast 1 asciinema play -i 2 first.cast - i - play back with max of 2 seconds of idleness - s - play back with double speed Process Create cluster Create quickstart Gitops Promotion Pr jx boot Commands 1 asciinema rec jx-k8s-days-00-logo.cast 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ______ __ ______ __ __ _______ .__ __. ___ .___________. __ ____ ____ _______ / || | / __ \\ | | | | | \\ | \\ | | / \\ | || | \\ \\ / / | ____ | | ,---- | | | | | | | | | | | .--. |______| \\| | / ^ \\ `---| |----`| | \\ \\/ / | |__ | | | | | | | | | | | | | | | |______| . ` | / /_\\ \\ | | | | \\ / | __| | `----.| `----.| `-- | | ` -- | | -- | | |\\ | / _____ \\ | | | | \\ / | |____ \\______||_______| \\______/ \\______/ |_______/ |__| \\__| /__/ \\__\\ |__| |__| \\__/ |_______| ______ __ ___ ______ _______ / || | / // || \\ | ,---- | | / / | ,---- | .--. | | | | | / / | | | | | | | `----.| | / / | `----.| -- | \\______||__| /__/ \\______||_______/ ____ __ ____ __ .___________. __ __ __ _______ .__ __. __ ___ __ .__ __. _______. ___ ___ \\ \\ / \\ / / | | | || | | | | | | ____|| \\ | | | |/ / | | | \\ | | / | \\ \\ / / \\ \\/ \\/ / | | `---| |----`| |__| | | | | |__ | \\| | | / | | | \\| | | ( ---- ` \\ V / \\ / | | | | | __ | .--. | | | __ | | . ` | | | | | . ` | \\ \\ \\ / \\ / | | | | | | | | | ` -- | | | ____ | | \\ | | . \\ | | | | \\ | .---- ) | / . \\ \\_ _/ \\_ _/ | __ | | __ | | __ | | __ | \\_ _____/ | _______ || __ | \\_ _ | | __ | \\_ _ \\ | __ | | __ | \\_ _ | | _______/ /__/ \\_ _ \\ 1 asciinema play jx-k8s-days-00-logo.cast Create Cluster 1 2 export NAMESPACE = cd export PROJECT = 1 asciinema rec jx-k8s-days-01-create.cast 1 2 3 4 5 jx create cluster gke -n jx-rocks -p $PROJECT -r us-east1 \\ -m n1-standard-4 --min-num-nodes 1 --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks --git-provider-kind github \\ --namespace $NAMESPACE --prow --tekton 1 asciinema play -i 1 -s 4 jx-k8s-days-01-create.cast Create QuickStart Go 1 asciinema rec jx-k8s-days-02-quickstart.cast 1 export APP_NAME = jx-k8s-days-go-02 1 jx create quickstart --filter golang-http --project-name ${ APP_NAME } --batch-mode 1 ls -l ${ APP_NAME } 1 jx get activity -f ${ APP_NAME } -w 1 jx get pipelines 1 jx get applications -e staging 1 http http:// ${ APP_NAME } .cd-staging.35.185.41.106.nip.io 1 jx get build logs -f ${ APP_NAME } # Cancel with ctrl+c 1 2 cd ${ APP_NAME } vim main.go 1 jx get activity -f ${ APP_NAME } -w 1 jx get applications -e staging 1 http http:// ${ APP_NAME } .cd-staging.35.185.41.106.nip.io 1 asciinema play -i 2 -s 2 jx-k8s-days-02-quickstart.cast Import Project 1 2 3 4 5 6 7 8 9 10 11 12 13 __ .___ ___. .______ ______ .______ .___________. __________ ___ __ _______.___________..__ __. _______ | | | \\/ | | _ \\ / __ \\ | _ \\ | | | ____ \\ \\ / / | | / | || \\ | | / _____ | | | | \\ / | | | _ ) | | | | | | | _ ) | ` --- | | ---- ` | | __ \\ V / | | | ( ---- ` --- | | ---- ` | \\| | | | __ | | | | \\/ | | | ___/ | | | | | / | | | __ | | | \\ \\ | | | . ` | | | | _ | | | | | | | | | | ` -- | | |\\ \\----. | | | |____ / . \\ | | .----) | | | | |\\ | | |__| | |__| |__| |__| | _| \\______/ | _| `._____| |__| |_______/__/ \\__\\ |__| |_______/ |__| |__| \\__| \\______| ___ .______ .______ __ __ ______ ___ .___________. __ ______ .__ __. / \\ | _ \\ | _ \\ | | | | / | / \\ | || | / __ \\ | \\ | | / ^ \\ | |_) | | |_) | | | | | | ,---- / ^ \\ ` --- | | ---- ` | | | | | | | \\| | / /_ \\ \\ | ___/ | ___/ | | | | | | / /_ \\ \\ | | | | | | | | | . ` | / _____ \\ | | | | | ` ----. | | | ` ----./ _____ \\ | | | | | ` -- | | | \\ | /__/ \\_ _ \\ | _ | | _ | | _______ || __ | \\_ _____/__/ \\_ _ \\ | __ | | __ | \\_ _____/ | __ | \\_ _ | 1 asciinema rec jx-k8s-days-03-import.cast 1 git clone https://github.com/joostvdg/go-demo-6.git 1 2 3 4 5 6 7 cd go-demo-6 git checkout orig git merge -s ours master --no-edit git checkout master git merge orig rm -rf charts ls -lath 1 2 3 git push jx import --batch-mode 1 jx get activities --filter go-demo-6 --watch 1 jx get applications 1 kubectl --namespace cd-staging logs -l app = jx-go-demo-6 1 2 3 4 5 6 echo dependencies: - name: mongodb alias: go-demo-6-db version: 5.3.0 repository: https://kubernetes-charts.storage.googleapis.com condition: db.enabled charts/go-demo-6/requirements.yaml 1 cat charts/go-demo-6/requirements.yaml 1 vim charts/go-demo-6/templates/deployment.yaml 1 2 3 env: - name: DB value: {{ template fullname . }} -db 1 vim charts/go-demo-6/values.yaml 1 probePath: /demo/hello?health = true 1 2 3 4 git status git add charts/ git commit -m add db dependency git push 1 jx get activities --filter go-demo-6 --watch 1 jx get applications 1 http http://go-demo-6.cd-staging.35.185.41.106.nip.io/demo/hello 1 jx delete application 1 rm -rf go-demo-6 1 jx get applications 1 asciinema play -i 2 -s 2 jx-k8s-days-03-import.cast Preview Environments 1 APP_NAME = jx-k8s-days-go-01 1 asciinema rec jx-k8s-days-04-preview.cast 1 jx get applications 1 cd ${ APP_NAME } 1 git checkout -b my-new-pr-3 1 vim main.go 1 2 3 4 git status git add main.go git commit -m change message git push --set-upstream origin my-new-pr-3 1 2 3 4 jx create pullrequest \\ --title My PR \\ --body This is the text that describes the PR \\ --batch-mode 1 open pr link 1 jx get previews 1 http .. 1 2 add /lgtm to pr merge pr 1 jx get activity --filter jx-k8s-day-go-01 --watch 1 jx get applications 1 2 3 git checkout master git pull cd .. 1 2 3 jx get previews jx gc previews jx get previews 1 asciinema play -i 2 -s 2 jx-k8s-days-04-preview.cast Env 1 jx create environment Reel 1 2 3 4 5 6 7 8 for VARIABLE in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 do asciinema play jx-k8s-days-00-logo.cast asciinema play -i 2 -s 3 jx-k8s-days-01-create.cast asciinema play -i 2 -s 1 jx-k8s-days-02-quickstart.cast asciinema play -i 2 -s 1 jx-k8s-days-03-import.cast asciinema play -i 2 -s 2 jx-k8s-days-04-preview.cast done","title":"Example For Console Reel"},{"location":"jenkinsx/kubernetes-days/#jenkins-x-kubernetes-days","text":"","title":"Jenkins X - Kubernetes Days"},{"location":"jenkinsx/kubernetes-days/#prepare","text":"1 asciinema rec first.cast 1 asciinema play -i 2 first.cast - i - play back with max of 2 seconds of idleness - s - play back with double speed","title":"Prepare"},{"location":"jenkinsx/kubernetes-days/#process","text":"Create cluster Create quickstart Gitops Promotion Pr jx boot","title":"Process"},{"location":"jenkinsx/kubernetes-days/#commands","text":"1 asciinema rec jx-k8s-days-00-logo.cast 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ______ __ ______ __ __ _______ .__ __. ___ .___________. __ ____ ____ _______ / || | / __ \\ | | | | | \\ | \\ | | / \\ | || | \\ \\ / / | ____ | | ,---- | | | | | | | | | | | .--. |______| \\| | / ^ \\ `---| |----`| | \\ \\/ / | |__ | | | | | | | | | | | | | | | |______| . ` | / /_\\ \\ | | | | \\ / | __| | `----.| `----.| `-- | | ` -- | | -- | | |\\ | / _____ \\ | | | | \\ / | |____ \\______||_______| \\______/ \\______/ |_______/ |__| \\__| /__/ \\__\\ |__| |__| \\__/ |_______| ______ __ ___ ______ _______ / || | / // || \\ | ,---- | | / / | ,---- | .--. | | | | | / / | | | | | | | `----.| | / / | `----.| -- | \\______||__| /__/ \\______||_______/ ____ __ ____ __ .___________. __ __ __ _______ .__ __. __ ___ __ .__ __. _______. ___ ___ \\ \\ / \\ / / | | | || | | | | | | ____|| \\ | | | |/ / | | | \\ | | / | \\ \\ / / \\ \\/ \\/ / | | `---| |----`| |__| | | | | |__ | \\| | | / | | | \\| | | ( ---- ` \\ V / \\ / | | | | | __ | .--. | | | __ | | . ` | | | | | . ` | \\ \\ \\ / \\ / | | | | | | | | | ` -- | | | ____ | | \\ | | . \\ | | | | \\ | .---- ) | / . \\ \\_ _/ \\_ _/ | __ | | __ | | __ | | __ | \\_ _____/ | _______ || __ | \\_ _ | | __ | \\_ _ \\ | __ | | __ | \\_ _ | | _______/ /__/ \\_ _ \\ 1 asciinema play jx-k8s-days-00-logo.cast","title":"Commands"},{"location":"jenkinsx/kubernetes-days/#create-cluster","text":"1 2 export NAMESPACE = cd export PROJECT = 1 asciinema rec jx-k8s-days-01-create.cast 1 2 3 4 5 jx create cluster gke -n jx-rocks -p $PROJECT -r us-east1 \\ -m n1-standard-4 --min-num-nodes 1 --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks --git-provider-kind github \\ --namespace $NAMESPACE --prow --tekton 1 asciinema play -i 1 -s 4 jx-k8s-days-01-create.cast","title":"Create Cluster"},{"location":"jenkinsx/kubernetes-days/#create-quickstart-go","text":"1 asciinema rec jx-k8s-days-02-quickstart.cast 1 export APP_NAME = jx-k8s-days-go-02 1 jx create quickstart --filter golang-http --project-name ${ APP_NAME } --batch-mode 1 ls -l ${ APP_NAME } 1 jx get activity -f ${ APP_NAME } -w 1 jx get pipelines 1 jx get applications -e staging 1 http http:// ${ APP_NAME } .cd-staging.35.185.41.106.nip.io 1 jx get build logs -f ${ APP_NAME } # Cancel with ctrl+c 1 2 cd ${ APP_NAME } vim main.go 1 jx get activity -f ${ APP_NAME } -w 1 jx get applications -e staging 1 http http:// ${ APP_NAME } .cd-staging.35.185.41.106.nip.io 1 asciinema play -i 2 -s 2 jx-k8s-days-02-quickstart.cast","title":"Create QuickStart Go"},{"location":"jenkinsx/kubernetes-days/#import-project","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 __ .___ ___. .______ ______ .______ .___________. __________ ___ __ _______.___________..__ __. _______ | | | \\/ | | _ \\ / __ \\ | _ \\ | | | ____ \\ \\ / / | | / | || \\ | | / _____ | | | | \\ / | | | _ ) | | | | | | | _ ) | ` --- | | ---- ` | | __ \\ V / | | | ( ---- ` --- | | ---- ` | \\| | | | __ | | | | \\/ | | | ___/ | | | | | / | | | __ | | | \\ \\ | | | . ` | | | | _ | | | | | | | | | | ` -- | | |\\ \\----. | | | |____ / . \\ | | .----) | | | | |\\ | | |__| | |__| |__| |__| | _| \\______/ | _| `._____| |__| |_______/__/ \\__\\ |__| |_______/ |__| |__| \\__| \\______| ___ .______ .______ __ __ ______ ___ .___________. __ ______ .__ __. / \\ | _ \\ | _ \\ | | | | / | / \\ | || | / __ \\ | \\ | | / ^ \\ | |_) | | |_) | | | | | | ,---- / ^ \\ ` --- | | ---- ` | | | | | | | \\| | / /_ \\ \\ | ___/ | ___/ | | | | | | / /_ \\ \\ | | | | | | | | | . ` | / _____ \\ | | | | | ` ----. | | | ` ----./ _____ \\ | | | | | ` -- | | | \\ | /__/ \\_ _ \\ | _ | | _ | | _______ || __ | \\_ _____/__/ \\_ _ \\ | __ | | __ | \\_ _____/ | __ | \\_ _ | 1 asciinema rec jx-k8s-days-03-import.cast 1 git clone https://github.com/joostvdg/go-demo-6.git 1 2 3 4 5 6 7 cd go-demo-6 git checkout orig git merge -s ours master --no-edit git checkout master git merge orig rm -rf charts ls -lath 1 2 3 git push jx import --batch-mode 1 jx get activities --filter go-demo-6 --watch 1 jx get applications 1 kubectl --namespace cd-staging logs -l app = jx-go-demo-6 1 2 3 4 5 6 echo dependencies: - name: mongodb alias: go-demo-6-db version: 5.3.0 repository: https://kubernetes-charts.storage.googleapis.com condition: db.enabled charts/go-demo-6/requirements.yaml 1 cat charts/go-demo-6/requirements.yaml 1 vim charts/go-demo-6/templates/deployment.yaml 1 2 3 env: - name: DB value: {{ template fullname . }} -db 1 vim charts/go-demo-6/values.yaml 1 probePath: /demo/hello?health = true 1 2 3 4 git status git add charts/ git commit -m add db dependency git push 1 jx get activities --filter go-demo-6 --watch 1 jx get applications 1 http http://go-demo-6.cd-staging.35.185.41.106.nip.io/demo/hello 1 jx delete application 1 rm -rf go-demo-6 1 jx get applications 1 asciinema play -i 2 -s 2 jx-k8s-days-03-import.cast","title":"Import Project"},{"location":"jenkinsx/kubernetes-days/#preview-environments","text":"1 APP_NAME = jx-k8s-days-go-01 1 asciinema rec jx-k8s-days-04-preview.cast 1 jx get applications 1 cd ${ APP_NAME } 1 git checkout -b my-new-pr-3 1 vim main.go 1 2 3 4 git status git add main.go git commit -m change message git push --set-upstream origin my-new-pr-3 1 2 3 4 jx create pullrequest \\ --title My PR \\ --body This is the text that describes the PR \\ --batch-mode 1 open pr link 1 jx get previews 1 http .. 1 2 add /lgtm to pr merge pr 1 jx get activity --filter jx-k8s-day-go-01 --watch 1 jx get applications 1 2 3 git checkout master git pull cd .. 1 2 3 jx get previews jx gc previews jx get previews 1 asciinema play -i 2 -s 2 jx-k8s-days-04-preview.cast","title":"Preview Environments"},{"location":"jenkinsx/kubernetes-days/#env","text":"1 jx create environment","title":"Env"},{"location":"jenkinsx/kubernetes-days/#reel","text":"1 2 3 4 5 6 7 8 for VARIABLE in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 do asciinema play jx-k8s-days-00-logo.cast asciinema play -i 2 -s 3 jx-k8s-days-01-create.cast asciinema play -i 2 -s 1 jx-k8s-days-02-quickstart.cast asciinema play -i 2 -s 1 jx-k8s-days-03-import.cast asciinema play -i 2 -s 2 jx-k8s-days-04-preview.cast done","title":"Reel"},{"location":"jenkinsx/maven/","text":"Jenkins X + Maven + Nexus The goal of this article it to demonstrate how Jenkins X works with Maven and Sonatype Nexus . Unless you configure otherwise, Jenkins X comes with a Nexus instance pre-configure out-of-the-box. Create Jenkins X Cluster Static See: Example Here's an example for creating a standard Jenkins X installation in Google Cloud with GKE. This example uses Google Cloud and it's CLI, gcloud . Where: CLUSTER_NAME : the name of your GKE cluster PROJECT : the project ID of your Google Project/account ( gcloud config list ) REGION : the region in Google Cloud where you want to run this cluster, if you don't know, use us - east1 1 2 3 4 5 6 7 8 9 10 11 12 jx create cluster gke \\ --cluster-name ${ CLUSTER_NAME } \\ --project-id ${ PROJECT } \\ --region ${ REGION } \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --skip-login \\ --batch-mode Serverless See: Nexus Open To open Nexus' Web UI, you can use jx open to see it's URL. By default, the URL will be nexus . jx . domain , for example http://nexus.jx. ${ LB_IP } .nip.io . I recommend using a proper Domain name and use TLS via Let's Encrypt. Jenkins X has built in support for this, via the jx upgrade ingress command. This is food for another article though. Credentials The username will be admin , the password depends on you. If you specified the --default-admin-password , it will be that. If you didn't specify the password, you can find it in a Kubernetes secret. 1 kubectl get secret nexus -o yaml Which should look like this: 1 2 3 4 apiVersion : v1 data : password : YWRtaW4= kind : Secret To retrieve the password, we have to decode the value of password with Base64. On Mac or Linux, this should be as easy as the command below. 1 echo YWRtaW4= | base64 -D Use Log in as Administrator and you get two views. Either browse, which allows you to discover and inspect packages. Or, Administrate (the Gear Icon) which allows you to manage the repositories. For more information, read the Nexus 3 documentation . Use Nexus with Maven in Jenkins X Maven Library Steps create new Jenkins X buildpack create new maven application import application into Jenkins X (with the Build Pack) double check job in Jenkins double check webhook in GitHub build the application in Jenkins verify package in Nexus Maven Application Steps create new maven application add repository for local dev add dependency on library build locally import application into Jenkins X build application in Jenkins How the magic works build image let's dig to see whats in it kubernetes secret with settings.xml maven repo maven distribution management maven mirror How would you do this yourself Options adjust Jenkins X's solution bridge Jenkins X's solution to your existing repo's create something yourself Adjust Jenkins X solution ? Bridge to existing Only to external Library buildpack https://github.com/jenkins-x-buildpacks/jenkins-x-classic/blob/master/packs/maven/pipeline.yaml Create new application 1 mvn archetype:generate -DarchetypeGroupId = org.apache.maven.archetypes -DarchetypeArtifactId = maven-archetype-quickstart -DarchetypeVersion = 1 .4 Import JX 1 jx import --pack maven-lib -b Do we need it? Seems to work without it as well. Perhaps its inside the build image? Edit secret secret: jenkins-maven-settings add labels: jenkins.io/credentials-type: secretFile https://jenkinsci.github.io/kubernetes-credentials-provider-plugin/examples/ https://jenkinsci.github.io/kubernetes-credentials-provider-plugin/ Pom xml config In order to publish stuff, we need to make sure we have our distribution config setup. 1 2 3 4 5 6 7 8 9 10 11 12 profiles profile id jx-nexus /id distributionManagement repository id nexus /id name nexus /name url ${altReleaseDeploymentRepository} /url /repository /distributionManagement /profile /profiles Pipeline example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 pipeline { agent { label jenkins-maven-java11 } stages { stage ( Test ) { environment { SETTINGS = credentials ( another-test-file2 ) } steps { sh echo ${SETTINGS} sh cat ${SETTINGS} container ( maven ) { sh mvn clean javadoc:aggregate verify -C -e sh mvn deploy --show-version --errors --activate-profiles jx-nexus --strict-checksums --settings ${SETTINGS} } } } } } Config example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : Secret metadata : # this is the jenkins id. name : another-test-file2 labels : # so we know what type it is. jenkins.io/credentials-type : secretFile annotations : # description - can not be a label as spaces are not allowed jenkins.io/credentials-description : secret file credential from Kubernetes type : Opaque stringData : filename : mySecret.txt data : # base64 encoded bytes data : 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 Create App create new java application with maven or gradle add dependency add repo: https://nexus.jx.kearos.net/repository/maven-public/ Know Issues Jenkins X doesn't have a kubernetes buildpack for Maven libraries, so I'm not sure how to import that directly which is why, for now, we create a new build pack first Jenkins X cannot import more than one application into static Jenkins within the same folder requires GitHub issue + PR","title":"Maven"},{"location":"jenkinsx/maven/#jenkins-x-maven-nexus","text":"The goal of this article it to demonstrate how Jenkins X works with Maven and Sonatype Nexus . Unless you configure otherwise, Jenkins X comes with a Nexus instance pre-configure out-of-the-box.","title":"Jenkins X + Maven + Nexus"},{"location":"jenkinsx/maven/#create-jenkins-x-cluster","text":"","title":"Create Jenkins X Cluster"},{"location":"jenkinsx/maven/#static","text":"See:","title":"Static"},{"location":"jenkinsx/maven/#example","text":"Here's an example for creating a standard Jenkins X installation in Google Cloud with GKE. This example uses Google Cloud and it's CLI, gcloud . Where: CLUSTER_NAME : the name of your GKE cluster PROJECT : the project ID of your Google Project/account ( gcloud config list ) REGION : the region in Google Cloud where you want to run this cluster, if you don't know, use us - east1 1 2 3 4 5 6 7 8 9 10 11 12 jx create cluster gke \\ --cluster-name ${ CLUSTER_NAME } \\ --project-id ${ PROJECT } \\ --region ${ REGION } \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --skip-login \\ --batch-mode","title":"Example"},{"location":"jenkinsx/maven/#serverless","text":"See:","title":"Serverless"},{"location":"jenkinsx/maven/#nexus","text":"","title":"Nexus"},{"location":"jenkinsx/maven/#open","text":"To open Nexus' Web UI, you can use jx open to see it's URL. By default, the URL will be nexus . jx . domain , for example http://nexus.jx. ${ LB_IP } .nip.io . I recommend using a proper Domain name and use TLS via Let's Encrypt. Jenkins X has built in support for this, via the jx upgrade ingress command. This is food for another article though.","title":"Open"},{"location":"jenkinsx/maven/#credentials","text":"The username will be admin , the password depends on you. If you specified the --default-admin-password , it will be that. If you didn't specify the password, you can find it in a Kubernetes secret. 1 kubectl get secret nexus -o yaml Which should look like this: 1 2 3 4 apiVersion : v1 data : password : YWRtaW4= kind : Secret To retrieve the password, we have to decode the value of password with Base64. On Mac or Linux, this should be as easy as the command below. 1 echo YWRtaW4= | base64 -D","title":"Credentials"},{"location":"jenkinsx/maven/#use","text":"Log in as Administrator and you get two views. Either browse, which allows you to discover and inspect packages. Or, Administrate (the Gear Icon) which allows you to manage the repositories. For more information, read the Nexus 3 documentation .","title":"Use"},{"location":"jenkinsx/maven/#use-nexus-with-maven-in-jenkins-x","text":"","title":"Use Nexus with Maven in Jenkins X"},{"location":"jenkinsx/maven/#maven-library","text":"","title":"Maven Library"},{"location":"jenkinsx/maven/#steps","text":"create new Jenkins X buildpack create new maven application import application into Jenkins X (with the Build Pack) double check job in Jenkins double check webhook in GitHub build the application in Jenkins verify package in Nexus","title":"Steps"},{"location":"jenkinsx/maven/#maven-application","text":"","title":"Maven Application"},{"location":"jenkinsx/maven/#steps_1","text":"create new maven application add repository for local dev add dependency on library build locally import application into Jenkins X build application in Jenkins","title":"Steps"},{"location":"jenkinsx/maven/#how-the-magic-works","text":"build image let's dig to see whats in it kubernetes secret with settings.xml maven repo maven distribution management maven mirror","title":"How the magic works"},{"location":"jenkinsx/maven/#how-would-you-do-this-yourself","text":"","title":"How would you do this yourself"},{"location":"jenkinsx/maven/#options","text":"adjust Jenkins X's solution bridge Jenkins X's solution to your existing repo's create something yourself","title":"Options"},{"location":"jenkinsx/maven/#adjust-jenkins-x-solution","text":"?","title":"Adjust Jenkins X solution"},{"location":"jenkinsx/maven/#bridge-to-existing","text":"","title":"Bridge to existing"},{"location":"jenkinsx/maven/#only-to-external","text":"","title":"Only to external"},{"location":"jenkinsx/maven/#library","text":"buildpack https://github.com/jenkins-x-buildpacks/jenkins-x-classic/blob/master/packs/maven/pipeline.yaml","title":"Library"},{"location":"jenkinsx/maven/#create-new-application","text":"1 mvn archetype:generate -DarchetypeGroupId = org.apache.maven.archetypes -DarchetypeArtifactId = maven-archetype-quickstart -DarchetypeVersion = 1 .4","title":"Create new application"},{"location":"jenkinsx/maven/#import-jx","text":"1 jx import --pack maven-lib -b","title":"Import JX"},{"location":"jenkinsx/maven/#do-we-need-it","text":"Seems to work without it as well. Perhaps its inside the build image?","title":"Do we need it?"},{"location":"jenkinsx/maven/#edit-secret","text":"secret: jenkins-maven-settings add labels: jenkins.io/credentials-type: secretFile https://jenkinsci.github.io/kubernetes-credentials-provider-plugin/examples/ https://jenkinsci.github.io/kubernetes-credentials-provider-plugin/","title":"Edit secret"},{"location":"jenkinsx/maven/#pom-xml-config","text":"In order to publish stuff, we need to make sure we have our distribution config setup. 1 2 3 4 5 6 7 8 9 10 11 12 profiles profile id jx-nexus /id distributionManagement repository id nexus /id name nexus /name url ${altReleaseDeploymentRepository} /url /repository /distributionManagement /profile /profiles","title":"Pom xml config"},{"location":"jenkinsx/maven/#pipeline-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 pipeline { agent { label jenkins-maven-java11 } stages { stage ( Test ) { environment { SETTINGS = credentials ( another-test-file2 ) } steps { sh echo ${SETTINGS} sh cat ${SETTINGS} container ( maven ) { sh mvn clean javadoc:aggregate verify -C -e sh mvn deploy --show-version --errors --activate-profiles jx-nexus --strict-checksums --settings ${SETTINGS} } } } } }","title":"Pipeline example"},{"location":"jenkinsx/maven/#config-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : Secret metadata : # this is the jenkins id. name : another-test-file2 labels : # so we know what type it is. jenkins.io/credentials-type : secretFile annotations : # description - can not be a label as spaces are not allowed jenkins.io/credentials-description : secret file credential from Kubernetes type : Opaque stringData : filename : mySecret.txt data : # base64 encoded bytes data : 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","title":"Config example"},{"location":"jenkinsx/maven/#create-app","text":"create new java application with maven or gradle add dependency add repo: https://nexus.jx.kearos.net/repository/maven-public/","title":"Create App"},{"location":"jenkinsx/maven/#know-issues","text":"Jenkins X doesn't have a kubernetes buildpack for Maven libraries, so I'm not sure how to import that directly which is why, for now, we create a new build pack first Jenkins X cannot import more than one application into static Jenkins within the same folder requires GitHub issue + PR","title":"Know Issues"},{"location":"jenkinsx/serverless/","text":"Jenkins X Serverless What Tekton Jenkins X Serverless Jenkins X Pipelines Commands Create Cluster 1 2 3 4 5 6 7 8 9 10 11 jx create cluster gke \\ --cluster-name jx-rocks \\ --project-id $PROJECT \\ --region us-east1 \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --batch-mode Install JX Where Project, is Gcloud Project ID. Requires docker - registry gcr . io , else it doesn't work. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b Notes jx create cluster gke cannot use kubernetes server version flag it somehow sets an empty --machineType flag instead jx install --tekton --prow requires --dockerRegistry to be set jx install --tekton --prow can be install multiple times in the same cluster to differentiate, set different namespace (which becomes part of the domain) might need to update webhooks incase env's already existed two jx serverless installs, and now jx get build logs doesn't work error : no Tekton pipelines have been triggered which match the current filter","title":"Jenkins X Serverless"},{"location":"jenkinsx/serverless/#jenkins-x-serverless","text":"","title":"Jenkins X Serverless"},{"location":"jenkinsx/serverless/#what","text":"Tekton Jenkins X Serverless Jenkins X Pipelines","title":"What"},{"location":"jenkinsx/serverless/#commands","text":"","title":"Commands"},{"location":"jenkinsx/serverless/#create-cluster","text":"1 2 3 4 5 6 7 8 9 10 11 jx create cluster gke \\ --cluster-name jx-rocks \\ --project-id $PROJECT \\ --region us-east1 \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --batch-mode","title":"Create Cluster"},{"location":"jenkinsx/serverless/#install-jx","text":"Where Project, is Gcloud Project ID. Requires docker - registry gcr . io , else it doesn't work. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b","title":"Install JX"},{"location":"jenkinsx/serverless/#notes","text":"jx create cluster gke cannot use kubernetes server version flag it somehow sets an empty --machineType flag instead jx install --tekton --prow requires --dockerRegistry to be set jx install --tekton --prow can be install multiple times in the same cluster to differentiate, set different namespace (which becomes part of the domain) might need to update webhooks incase env's already existed two jx serverless installs, and now jx get build logs doesn't work error : no Tekton pipelines have been triggered which match the current filter","title":"Notes"},{"location":"jenkinsx/workshop/","text":"Jenkins X Workshop Create Cluster 1 2 3 4 5 6 7 8 PROJECT = NAME = ws-feb ZONE = europe-west4 MACHINE = n1-standard-2 MIN_NODES = 3 MAX_NODES = 5 PASS = admin PREFIX = ws Warning You might want to do this: (not sure why) 1 2 3 echo nexus: enabled: false | tee myvalues.yaml 1 2 3 4 jx create cluster gke -n $NAME -p $PROJECT -z $ZONE -m $MACHINE \\ --min-num-nodes $MIN_NODES --max-num-nodes $MAX_NODES \\ --default-admin-password = $PASS \\ --default-environment-prefix $NAME Alternatively Info Domain will get .jx as a prefix anyway. 1 2 3 4 5 6 JX_CLUSTER_NAME = joostvdg JX_DOMAIN = kearos.net JX_GIT_USER = joostvdg JX_ORG = joostvdg JX_K8S_REGION = europe-west4 JX_NAME = jx-joostvdg 1 2 3 JX_API_TOKEN = JX_ADMIN_PSS = JX_GCE_PROJECT = 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jx create cluster gke \\ -n ${ JX_NAME } \\ --exposer = Ingress \\ --preemptible = false \\ --cluster-name = ${ JX_CLUSTER_NAME } \\ --default-admin-password = ${ JX_ADMIN_PSS } \\ --domain = ${ JX_DOMAIN } \\ --machine-type = n1-standard-2 \\ --max-num-nodes = 3 \\ --min-num-nodes = 2 \\ --project-id = ${ JX_GCE_PROJECT } \\ --default-environment-prefix ${ JX_NAME } \\ --zone = ${ JX_ZONE } \\ --http = false \\ --tls-acme = true \\ --skip-login 1 2 3 --prow \\ --no-tiller = true \\ --vault = true \\ Issues - b doesn't work with --vault as the config is empty --vault= true doesn't work with https (cert-manager) because sync . go : 64 ] Not syncing ingress jx / cm - acme - http - solver - stx47 as it does not contain necessary annotations ** also, it seems its TLS config isn't correct for some reason does TLS with cert-manager actually work? now it doesn't actually install cert - manager ? Whats up with that. Install Certmanager Via JX Updates the entire ingress configuration, installs cert-mananger, certificates, replaces ingress definitions, updates webhooks, and allows you to set a different domain name. 1 jx upgrade ingress --cluster Manually This does not create certificates nor does it update the ingress defintions. 1 2 3 kubectl create namespace cert-manager kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.6/deploy/manifests/00-crds.yaml helm install --name cert-manager --namespace cert-manager stable/cert-manager Demo App Post creation 1 2 3 4 5 6 7 8 9 Creating GitHub webhook for joostvdg/cmg for url https://hook.jx.jx.kearos.net/hook Watch pipeline activity via: jx get activity -f cmg -w Browse the pipeline log via: jx get build logs joostvdg/cmg/master Open the Jenkins console via jx console You can list the pipelines via: jx get pipelines When the pipeline is complete: jx get applications For more help on available commands see: https://jenkins-x.io/developing/browsing/ Promote 1 jx promote ${ APP } --version $VERSION --env production -b Compliance 1 jx compliance run 1 jx compliance status 1 jx compliance logs -f 1 jx compliance delete Chartmuseum auth faillure uses kubernetes secret relies on kubernetes-secret (Jenkins) plugin can have trouble with special charactes to fix, update the kubernetes secret (used by chartmuseum and the pipeline) Workshop responses send to Alyssa Juni address for sending Responses Extra requirements billing needs to be enabled, else you cannot create a cluster of that size we need to test more with windows ** without admin ** different python versions","title":"Jenkins X Workshop"},{"location":"jenkinsx/workshop/#jenkins-x-workshop","text":"","title":"Jenkins X Workshop"},{"location":"jenkinsx/workshop/#create-cluster","text":"1 2 3 4 5 6 7 8 PROJECT = NAME = ws-feb ZONE = europe-west4 MACHINE = n1-standard-2 MIN_NODES = 3 MAX_NODES = 5 PASS = admin PREFIX = ws Warning You might want to do this: (not sure why) 1 2 3 echo nexus: enabled: false | tee myvalues.yaml 1 2 3 4 jx create cluster gke -n $NAME -p $PROJECT -z $ZONE -m $MACHINE \\ --min-num-nodes $MIN_NODES --max-num-nodes $MAX_NODES \\ --default-admin-password = $PASS \\ --default-environment-prefix $NAME","title":"Create Cluster"},{"location":"jenkinsx/workshop/#alternatively","text":"Info Domain will get .jx as a prefix anyway. 1 2 3 4 5 6 JX_CLUSTER_NAME = joostvdg JX_DOMAIN = kearos.net JX_GIT_USER = joostvdg JX_ORG = joostvdg JX_K8S_REGION = europe-west4 JX_NAME = jx-joostvdg 1 2 3 JX_API_TOKEN = JX_ADMIN_PSS = JX_GCE_PROJECT = 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jx create cluster gke \\ -n ${ JX_NAME } \\ --exposer = Ingress \\ --preemptible = false \\ --cluster-name = ${ JX_CLUSTER_NAME } \\ --default-admin-password = ${ JX_ADMIN_PSS } \\ --domain = ${ JX_DOMAIN } \\ --machine-type = n1-standard-2 \\ --max-num-nodes = 3 \\ --min-num-nodes = 2 \\ --project-id = ${ JX_GCE_PROJECT } \\ --default-environment-prefix ${ JX_NAME } \\ --zone = ${ JX_ZONE } \\ --http = false \\ --tls-acme = true \\ --skip-login 1 2 3 --prow \\ --no-tiller = true \\ --vault = true \\","title":"Alternatively"},{"location":"jenkinsx/workshop/#issues","text":"- b doesn't work with --vault as the config is empty --vault= true doesn't work with https (cert-manager) because sync . go : 64 ] Not syncing ingress jx / cm - acme - http - solver - stx47 as it does not contain necessary annotations ** also, it seems its TLS config isn't correct for some reason does TLS with cert-manager actually work? now it doesn't actually install cert - manager ? Whats up with that.","title":"Issues"},{"location":"jenkinsx/workshop/#install-certmanager","text":"","title":"Install Certmanager"},{"location":"jenkinsx/workshop/#via-jx","text":"Updates the entire ingress configuration, installs cert-mananger, certificates, replaces ingress definitions, updates webhooks, and allows you to set a different domain name. 1 jx upgrade ingress --cluster","title":"Via JX"},{"location":"jenkinsx/workshop/#manually","text":"This does not create certificates nor does it update the ingress defintions. 1 2 3 kubectl create namespace cert-manager kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.6/deploy/manifests/00-crds.yaml helm install --name cert-manager --namespace cert-manager stable/cert-manager","title":"Manually"},{"location":"jenkinsx/workshop/#demo-app","text":"","title":"Demo App"},{"location":"jenkinsx/workshop/#post-creation","text":"1 2 3 4 5 6 7 8 9 Creating GitHub webhook for joostvdg/cmg for url https://hook.jx.jx.kearos.net/hook Watch pipeline activity via: jx get activity -f cmg -w Browse the pipeline log via: jx get build logs joostvdg/cmg/master Open the Jenkins console via jx console You can list the pipelines via: jx get pipelines When the pipeline is complete: jx get applications For more help on available commands see: https://jenkins-x.io/developing/browsing/","title":"Post creation"},{"location":"jenkinsx/workshop/#promote","text":"1 jx promote ${ APP } --version $VERSION --env production -b","title":"Promote"},{"location":"jenkinsx/workshop/#compliance","text":"1 jx compliance run 1 jx compliance status 1 jx compliance logs -f 1 jx compliance delete","title":"Compliance"},{"location":"jenkinsx/workshop/#chartmuseum-auth-faillure","text":"uses kubernetes secret relies on kubernetes-secret (Jenkins) plugin can have trouble with special charactes to fix, update the kubernetes secret (used by chartmuseum and the pipeline)","title":"Chartmuseum auth faillure"},{"location":"jenkinsx/workshop/#workshop-responses","text":"send to Alyssa Juni address for sending","title":"Workshop responses"},{"location":"jenkinsx/workshop/#responses","text":"","title":"Responses"},{"location":"jenkinsx/workshop/#extra-requirements","text":"billing needs to be enabled, else you cannot create a cluster of that size we need to test more with windows ** without admin ** different python versions","title":"Extra requirements"},{"location":"kubernetes/","text":"Kubernetes Some resources on how to start with Kubernetes. Workshops My Own Jerome Petazzoni Viktor Farcic Play With Kubernetes Katacoda VMWare Slide Decks My Own Viktor Farcic Books DevOps Toolkit 2.3 - Viktor Farcic DevOps Toolkit 2.4 - Viktor Farcic DevOps Toolkit 2.5 - Viktor Farcic Kubernetes Up And Running - Joe Beda, Brendan Burns, Kelsey Hightower The Kubernetes Book - Nigel Poulton Articles Article Looking At Kubernetes' Reconciliation In-depth Look At Pods https://medium.com/@vikram.fugro/container-networking-interface-aka-cni-bdfe23f865cf https://itnext.io/benchmark-results-of-kubernetes-network-plugins-cni-over-10gbit-s-network-36475925a560 ? Linux basics Namespaces CGroups https://jvns.ca/blog/2016/10/10/what-even-is-a-container/ https://www.youtube.com/watch?v=sK5i-N34im8 https://www.ianlewis.org/en/what-are-kubernetes-pods-anyway Networking https://itnext.io/kubernetes-networking-behind-the-scenes-39a1ab1792bb https://github.com/nleiva/kubernetes-networking-links IP Tables CIDR Explanation video Packets Frames introduction Traefik on AWS Metrics https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-66936addedae Secrets Hashicorp Vault - Kelsey Hightower https://medium.com/qubit-engineering/kubernetes-up-integrated-secrets-configuration-5a15b9f5a6c6 Security RBAC 11 ways not to get hacked on Kubernetes https://www.youtube.com/channel/UCiqnRXPAAk6iv2m47odUFzw Tools to use Microscanner Dex - OpenID Connect solution Sonubuoy Helm Learn LUA book , lua's required for Helm 3.0 ChartMuseum Skaffold KSynch Traefik Istio Falco Prometheus Grafana Jenkins Kaniko Prow Tarmak Kube-Lego Knative Rook Stern - aggregate log rendering tool Linkerd 2","title":"Introduction"},{"location":"kubernetes/#kubernetes","text":"Some resources on how to start with Kubernetes.","title":"Kubernetes"},{"location":"kubernetes/#workshops","text":"My Own Jerome Petazzoni Viktor Farcic Play With Kubernetes Katacoda VMWare","title":"Workshops"},{"location":"kubernetes/#slide-decks","text":"My Own Viktor Farcic","title":"Slide Decks"},{"location":"kubernetes/#books","text":"DevOps Toolkit 2.3 - Viktor Farcic DevOps Toolkit 2.4 - Viktor Farcic DevOps Toolkit 2.5 - Viktor Farcic Kubernetes Up And Running - Joe Beda, Brendan Burns, Kelsey Hightower The Kubernetes Book - Nigel Poulton","title":"Books"},{"location":"kubernetes/#articles","text":"Article Looking At Kubernetes' Reconciliation In-depth Look At Pods https://medium.com/@vikram.fugro/container-networking-interface-aka-cni-bdfe23f865cf https://itnext.io/benchmark-results-of-kubernetes-network-plugins-cni-over-10gbit-s-network-36475925a560 ?","title":"Articles"},{"location":"kubernetes/#linux-basics","text":"","title":"Linux basics"},{"location":"kubernetes/#namespaces-cgroups","text":"https://jvns.ca/blog/2016/10/10/what-even-is-a-container/ https://www.youtube.com/watch?v=sK5i-N34im8 https://www.ianlewis.org/en/what-are-kubernetes-pods-anyway","title":"Namespaces &amp; CGroups"},{"location":"kubernetes/#networking","text":"https://itnext.io/kubernetes-networking-behind-the-scenes-39a1ab1792bb https://github.com/nleiva/kubernetes-networking-links IP Tables CIDR Explanation video Packets Frames introduction Traefik on AWS","title":"Networking"},{"location":"kubernetes/#metrics","text":"https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-66936addedae","title":"Metrics"},{"location":"kubernetes/#secrets","text":"Hashicorp Vault - Kelsey Hightower https://medium.com/qubit-engineering/kubernetes-up-integrated-secrets-configuration-5a15b9f5a6c6","title":"Secrets"},{"location":"kubernetes/#security","text":"RBAC 11 ways not to get hacked on Kubernetes https://www.youtube.com/channel/UCiqnRXPAAk6iv2m47odUFzw","title":"Security"},{"location":"kubernetes/#tools-to-use","text":"Microscanner Dex - OpenID Connect solution Sonubuoy Helm Learn LUA book , lua's required for Helm 3.0 ChartMuseum Skaffold KSynch Traefik Istio Falco Prometheus Grafana Jenkins Kaniko Prow Tarmak Kube-Lego Knative Rook Stern - aggregate log rendering tool Linkerd 2","title":"Tools to use"},{"location":"kubernetes/cka-exam-prep/","text":"CKA Exam Prep https://github.com/dgkanatsios/CKAD-exercises https://github.com/kelseyhightower/kubernetes-the-hard-way https://github.com/walidshaari/Kubernetes-Certified-Administrator https://github.com/kubernetes/community/blob/master/contributors/devel/e2e-tests.md https://www.cncf.io/certification/cka/ https://oscon2018.container.training https://github.com/ahmetb/kubernetes-network-policy-recipes https://github.com/ramitsurana/awesome-kubernetes https://sysdig.com/blog/kubernetes-security-guide/ https://severalnines.com/blog/installing-kubernetes-cluster-minions-centos7-manage-pods-services https://docs.google.com/presentation/d/1Gp-2blk5WExI_QR59EUZdwfO2BWLJqa626mK2ej-huo/edit#slide=id.g27a78b354c_0_0 https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html Some basic commands 1 kubectl -n kube-public get secrets Test network policy For some common recipes, look at Ahmet's recipe repository . Warning Make sure you have CNI enabled and you have a network plugin that enforces the policies. Note You can check current existing policies like this: kubectl get netpol --all-namespaces Example Ingress Policy 1 2 3 4 5 6 7 8 9 10 11 kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : name : dui-network-policy namespace : dui spec : podSelector : matchLabels : app : dui distribution : server ingress : [] Run test pod Apply above network policy, and then test in the same dui namespace, and in the default namespace. Note Use alpine : 3.6 because telnet was dropped starting 3.7. 1 2 3 kubectl -n dui get pods -l app = dui -o wide kubectl run --rm -i -t --image = alpine:3.6 -n dui test -- sh telnet 10 .32.0.7 8888 This should now fail - timeout - due the packages being dropped. Egress 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : dui-network-policy-egress namespace : dui spec : podSelector : matchLabels : app : dui policyTypes : - Egress egress : - ports : - port : 7777 protocol : TCP - to : - podSelector : matchLabels : app : dui Warning This should in theory, block our test pod from reading this. As it doesn't have the label app = dui . But it seems it is working just fine. Allow DNS If it should also be able to do DNS calls, we have to enable port 53. 1 2 3 4 5 6 7 8 9 - ports : - port : 53 protocol : UDP - port : 53 protocol : TCP - port : 7777 protocol : TCP - to : - namespaceSelector : {} Create a test pod with curl 1 2 3 kubectl run --rm -i -t --image = alpine:3.6 -n dui test -- sh apk --no-cache add curl curl 10 .32.0.11:7777/servers Run minikube cluster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 ###################### # Create The Cluster # ###################### # Make sure that your minikube version is v0.25 or higher # WARNING!!! # Some users experienced problems starting the cluster with minikuber v0.26 and v0.27. # A few of the reported issues are https://github.com/kubernetes/minikube/issues/2707 and https://github.com/kubernetes/minikube/issues/2703 # If you are experiencing problems creating a cluster, please consider downgrading to minikube v0.25. minikube start \\ --vm-driver virtualbox \\ --cpus 4 \\ --memory 12228 \\ --network-plugin = cni \\ --extra-config = kubelet.network-plugin = cni ############################### # Install Ingress and Storage # ############################### minikube addons enable ingress minikube addons enable storage-provisioner minikube addons enable default-storageclass ################## # Install Tiller # ################## kubectl create \\ -f https://raw.githubusercontent.com/vfarcic/k8s-specs/master/helm/tiller-rbac.yml \\ --record --save-config helm init --service-account tiller kubectl -n kube-system \\ rollout status deploy tiller-deploy ################## # Get Cluster IP # ################## export LB_IP = $( minikube ip ) ####################### # Install ChartMuseum # ####################### CM_ADDR = cm. $LB_IP .nip.io echo $CM_ADDR CM_ADDR_ESC = $( echo $CM_ADDR \\ | sed -e s@\\.@\\\\\\.@g ) echo $CM_ADDR_ESC helm install stable/chartmuseum \\ --namespace charts \\ --name cm \\ --values helm/chartmuseum-values.yml \\ --set ingress.hosts. $CM_ADDR_ESC ={ / } \\ --set env.secret.BASIC_AUTH_USER = admin \\ --set env.secret.BASIC_AUTH_PASS = admin kubectl -n charts \\ rollout status deploy \\ cm-chartmuseum # http http://$CM_ADDR/health # It should return `{ healthy :true} ###################### # Install Weave Net ## ###################### kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n ) kubectl -n kube-system rollout status daemonset weave-net Weave Net On minikube To run Weave Net on minikube, after upgrading minikube, you need to overwrite the default CNI config shipped with minikube: mkdir -p ~/.minikube/files/etc/cni/net.d/ touch ~/.minikube/files/etc/cni.net.d/k8s.conf and then to start minikube with CNI enabled: minikube start --network-plugin=cni --extra-config=kubelet.network-plugin=cni. Afterwards, you can install Weave Net. 1 kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n ) Install stern Stern - aggregate log rendering tool via brew 1 brew install stern Binary release 1 2 3 sudo curl -L -o /usr/local/bin/stern \\ https://github.com/wercker/stern/releases/download/1.6.0/stern_linux_amd64 sudo chmod +x /usr/local/bin/stern Sysdig Install Sysdig Run Sysdig for Kubernetes collect API server address collect client cert + key https://www.digitalocean.com/community/tutorials/how-to-monitor-your-ubuntu-16-04-system-with-sysdig 1 2 3 4 certificate-authority: /home/joostvdg/.minikube/ca.crt server: https://192.168.99.100:8443 client-certificate: /home/joostvdg/.minikube/client.crt client-key: /home/joostvdg/.minikube/client.key 1 2 3 sysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key sysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key syslog.severity.str = info CSysdig 1 sudo csysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key From Udemy Course","title":"CKA Exam prep"},{"location":"kubernetes/cka-exam-prep/#cka-exam-prep","text":"https://github.com/dgkanatsios/CKAD-exercises https://github.com/kelseyhightower/kubernetes-the-hard-way https://github.com/walidshaari/Kubernetes-Certified-Administrator https://github.com/kubernetes/community/blob/master/contributors/devel/e2e-tests.md https://www.cncf.io/certification/cka/ https://oscon2018.container.training https://github.com/ahmetb/kubernetes-network-policy-recipes https://github.com/ramitsurana/awesome-kubernetes https://sysdig.com/blog/kubernetes-security-guide/ https://severalnines.com/blog/installing-kubernetes-cluster-minions-centos7-manage-pods-services https://docs.google.com/presentation/d/1Gp-2blk5WExI_QR59EUZdwfO2BWLJqa626mK2ej-huo/edit#slide=id.g27a78b354c_0_0 https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html","title":"CKA Exam Prep"},{"location":"kubernetes/cka-exam-prep/#some-basic-commands","text":"1 kubectl -n kube-public get secrets","title":"Some basic commands"},{"location":"kubernetes/cka-exam-prep/#test-network-policy","text":"For some common recipes, look at Ahmet's recipe repository . Warning Make sure you have CNI enabled and you have a network plugin that enforces the policies. Note You can check current existing policies like this: kubectl get netpol --all-namespaces","title":"Test network policy"},{"location":"kubernetes/cka-exam-prep/#example-ingress-policy","text":"1 2 3 4 5 6 7 8 9 10 11 kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : name : dui-network-policy namespace : dui spec : podSelector : matchLabels : app : dui distribution : server ingress : []","title":"Example Ingress Policy"},{"location":"kubernetes/cka-exam-prep/#run-test-pod","text":"Apply above network policy, and then test in the same dui namespace, and in the default namespace. Note Use alpine : 3.6 because telnet was dropped starting 3.7. 1 2 3 kubectl -n dui get pods -l app = dui -o wide kubectl run --rm -i -t --image = alpine:3.6 -n dui test -- sh telnet 10 .32.0.7 8888 This should now fail - timeout - due the packages being dropped.","title":"Run test pod"},{"location":"kubernetes/cka-exam-prep/#egress","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : dui-network-policy-egress namespace : dui spec : podSelector : matchLabels : app : dui policyTypes : - Egress egress : - ports : - port : 7777 protocol : TCP - to : - podSelector : matchLabels : app : dui Warning This should in theory, block our test pod from reading this. As it doesn't have the label app = dui . But it seems it is working just fine.","title":"Egress"},{"location":"kubernetes/cka-exam-prep/#allow-dns","text":"If it should also be able to do DNS calls, we have to enable port 53. 1 2 3 4 5 6 7 8 9 - ports : - port : 53 protocol : UDP - port : 53 protocol : TCP - port : 7777 protocol : TCP - to : - namespaceSelector : {}","title":"Allow DNS"},{"location":"kubernetes/cka-exam-prep/#create-a-test-pod-with-curl","text":"1 2 3 kubectl run --rm -i -t --image = alpine:3.6 -n dui test -- sh apk --no-cache add curl curl 10 .32.0.11:7777/servers","title":"Create a test pod with curl"},{"location":"kubernetes/cka-exam-prep/#run-minikube-cluster","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 ###################### # Create The Cluster # ###################### # Make sure that your minikube version is v0.25 or higher # WARNING!!! # Some users experienced problems starting the cluster with minikuber v0.26 and v0.27. # A few of the reported issues are https://github.com/kubernetes/minikube/issues/2707 and https://github.com/kubernetes/minikube/issues/2703 # If you are experiencing problems creating a cluster, please consider downgrading to minikube v0.25. minikube start \\ --vm-driver virtualbox \\ --cpus 4 \\ --memory 12228 \\ --network-plugin = cni \\ --extra-config = kubelet.network-plugin = cni ############################### # Install Ingress and Storage # ############################### minikube addons enable ingress minikube addons enable storage-provisioner minikube addons enable default-storageclass ################## # Install Tiller # ################## kubectl create \\ -f https://raw.githubusercontent.com/vfarcic/k8s-specs/master/helm/tiller-rbac.yml \\ --record --save-config helm init --service-account tiller kubectl -n kube-system \\ rollout status deploy tiller-deploy ################## # Get Cluster IP # ################## export LB_IP = $( minikube ip ) ####################### # Install ChartMuseum # ####################### CM_ADDR = cm. $LB_IP .nip.io echo $CM_ADDR CM_ADDR_ESC = $( echo $CM_ADDR \\ | sed -e s@\\.@\\\\\\.@g ) echo $CM_ADDR_ESC helm install stable/chartmuseum \\ --namespace charts \\ --name cm \\ --values helm/chartmuseum-values.yml \\ --set ingress.hosts. $CM_ADDR_ESC ={ / } \\ --set env.secret.BASIC_AUTH_USER = admin \\ --set env.secret.BASIC_AUTH_PASS = admin kubectl -n charts \\ rollout status deploy \\ cm-chartmuseum # http http://$CM_ADDR/health # It should return `{ healthy :true} ###################### # Install Weave Net ## ###################### kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n ) kubectl -n kube-system rollout status daemonset weave-net","title":"Run minikube cluster"},{"location":"kubernetes/cka-exam-prep/#weave-net","text":"","title":"Weave Net"},{"location":"kubernetes/cka-exam-prep/#on-minikube","text":"To run Weave Net on minikube, after upgrading minikube, you need to overwrite the default CNI config shipped with minikube: mkdir -p ~/.minikube/files/etc/cni/net.d/ touch ~/.minikube/files/etc/cni.net.d/k8s.conf and then to start minikube with CNI enabled: minikube start --network-plugin=cni --extra-config=kubelet.network-plugin=cni. Afterwards, you can install Weave Net. 1 kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n )","title":"On minikube"},{"location":"kubernetes/cka-exam-prep/#install-stern","text":"Stern - aggregate log rendering tool","title":"Install stern"},{"location":"kubernetes/cka-exam-prep/#via-brew","text":"1 brew install stern","title":"via brew"},{"location":"kubernetes/cka-exam-prep/#binary-release","text":"1 2 3 sudo curl -L -o /usr/local/bin/stern \\ https://github.com/wercker/stern/releases/download/1.6.0/stern_linux_amd64 sudo chmod +x /usr/local/bin/stern","title":"Binary release"},{"location":"kubernetes/cka-exam-prep/#sysdig","text":"","title":"Sysdig"},{"location":"kubernetes/cka-exam-prep/#install-sysdig","text":"","title":"Install Sysdig"},{"location":"kubernetes/cka-exam-prep/#run-sysdig-for-kubernetes","text":"collect API server address collect client cert + key https://www.digitalocean.com/community/tutorials/how-to-monitor-your-ubuntu-16-04-system-with-sysdig 1 2 3 4 certificate-authority: /home/joostvdg/.minikube/ca.crt server: https://192.168.99.100:8443 client-certificate: /home/joostvdg/.minikube/client.crt client-key: /home/joostvdg/.minikube/client.key 1 2 3 sysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key sysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key syslog.severity.str = info","title":"Run Sysdig for Kubernetes"},{"location":"kubernetes/cka-exam-prep/#csysdig","text":"1 sudo csysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key","title":"CSysdig"},{"location":"kubernetes/cka-exam-prep/#from-udemy-course","text":"","title":"From Udemy Course"},{"location":"kubernetes/cka-exam/","text":"Certified Kubernetes Administrator Exam","title":"Certified Kubernetes Administrator Exam"},{"location":"kubernetes/cka-exam/#certified-kubernetes-administrator-exam","text":"","title":"Certified Kubernetes Administrator Exam"},{"location":"kubernetes/dev-platform/","text":"Kubernetes As Developer Platform Resources https://medium.com/@jpcontad/a-year-of-running-kubernetes-as-a-product-7eed1204eecd","title":"Kubernetes As Developer Platform"},{"location":"kubernetes/dev-platform/#kubernetes-as-developer-platform","text":"","title":"Kubernetes As Developer Platform"},{"location":"kubernetes/dev-platform/#resources","text":"https://medium.com/@jpcontad/a-year-of-running-kubernetes-as-a-product-7eed1204eecd","title":"Resources"},{"location":"kubernetes/observability/","text":"Kubernetes Observability Monitoring Metrics Server Helm chart: https://github.com/helm/charts/tree/master/stable/metrics-server Home: https://github.com/kubernetes-incubator/metrics-server 1 2 3 4 5 6 7 8 helm install stable/metrics-server \\ --name metrics-server \\ --version 2 .0.3 \\ --namespace metrics kubectl -n metrics \\ rollout status \\ deployment metrics-server Prometheus Alert Manager Prometheus Helm Values 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 server : ingress : enabled : true annotations : ingress . kubernetes . io / ssl - redirect : false nginx . ingress . kubernetes . io / ssl - redirect : false resources : limits : cpu : 100 m memory : 1000 Mi requests : cpu : 10 m memory : 500 Mi alertmanager : ingress : enabled : true annotations : ingress . kubernetes . io / ssl - redirect : false nginx . ingress . kubernetes . io / ssl - redirect : false resources : limits : cpu : 10 m memory : 20 Mi requests : cpu : 5 m memory : 10 Mi kubeStateMetrics : resources : limits : cpu : 10 m memory : 50 Mi requests : cpu : 5 m memory : 25 Mi nodeExporter : resources : limits : cpu : 10 m memory : 20 Mi requests : cpu : 5 m memory : 10 Mi pushgateway : resources : limits : cpu : 10 m memory : 20 Mi requests : cpu : 5 m memory : 10 Mi Grafana Application Metrics Resources https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-b190cc97f0f6 https://brancz.com/2018/01/05/prometheus-vs-heapster-vs-kubernetes-metrics-apis/ https://rancher.com/blog/2018/2018-06-26-measuring-metrics-that-matter-in-kubernetes-clusters/","title":""},{"location":"kubernetes/observability/#kubernetes-observability","text":"","title":"Kubernetes Observability"},{"location":"kubernetes/observability/#monitoring","text":"","title":"Monitoring"},{"location":"kubernetes/observability/#metrics-server","text":"Helm chart: https://github.com/helm/charts/tree/master/stable/metrics-server Home: https://github.com/kubernetes-incubator/metrics-server 1 2 3 4 5 6 7 8 helm install stable/metrics-server \\ --name metrics-server \\ --version 2 .0.3 \\ --namespace metrics kubectl -n metrics \\ rollout status \\ deployment metrics-server","title":"Metrics Server"},{"location":"kubernetes/observability/#prometheus-alert-manager","text":"","title":"Prometheus &amp; Alert Manager"},{"location":"kubernetes/observability/#prometheus-helm-values","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 server : ingress : enabled : true annotations : ingress . kubernetes . io / ssl - redirect : false nginx . ingress . kubernetes . io / ssl - redirect : false resources : limits : cpu : 100 m memory : 1000 Mi requests : cpu : 10 m memory : 500 Mi alertmanager : ingress : enabled : true annotations : ingress . kubernetes . io / ssl - redirect : false nginx . ingress . kubernetes . io / ssl - redirect : false resources : limits : cpu : 10 m memory : 20 Mi requests : cpu : 5 m memory : 10 Mi kubeStateMetrics : resources : limits : cpu : 10 m memory : 50 Mi requests : cpu : 5 m memory : 25 Mi nodeExporter : resources : limits : cpu : 10 m memory : 20 Mi requests : cpu : 5 m memory : 10 Mi pushgateway : resources : limits : cpu : 10 m memory : 20 Mi requests : cpu : 5 m memory : 10 Mi","title":"Prometheus Helm Values"},{"location":"kubernetes/observability/#grafana","text":"","title":"Grafana"},{"location":"kubernetes/observability/#application-metrics","text":"","title":"Application Metrics"},{"location":"kubernetes/observability/#resources","text":"https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-b190cc97f0f6 https://brancz.com/2018/01/05/prometheus-vs-heapster-vs-kubernetes-metrics-apis/ https://rancher.com/blog/2018/2018-06-26-measuring-metrics-that-matter-in-kubernetes-clusters/","title":"Resources"},{"location":"kubernetes/tools/","text":"Kubernetes Tools Helm We use Helm as a package manager to more easily install other tools on Kubernetes. There's several repositories with a large number of mature charts - the name of the Helm packages. One being Helm/Stable another being Helm Hub . Install MacOS/Homebrew 1 brew install kubernetes-helm Windows/Chocolatey 1 hoco install kubernetes-helm Ubuntu/Snap 1 sudo snap install helm --classic Sccop 1 scoop install helm GoFish 1 gofish install helm Usage 1 helm install stable/jenkins Kubecontext Kubectx is a utility to manage and switch between Kubernetes ( kubectl ) contexts and namespaces (via kubens , see below). Install MacOS/Homebrew 1 brew install kubectx Ubuntu 1 sudo apt install kubectx Usage Kubectx 1 2 3 4 5 6 7 8 9 10 11 12 kubectx minikube Switched to context minikube . $ kubectx - Switched to context oregon . $ kubectx - Switched to context minikube . $ kubectx dublin = gke_ahmetb_europe-west1-b_dublin Context dublin set. Aliased gke_ahmetb_europe-west1-b_dublin as dublin . Kubens Kubens (part of Kubectx) helps you manage your current Kubernetes namespace. 1 2 3 4 5 6 7 $ kubens kube-system Context test set. Active namespace is kube-system . $ kubens - Context test set. Active namespace is default . Kuard Kuard is a small demo application to show your cluster works. Also exposes some info you might want to see. 1 2 kubectl run --restart = Never --image = gcr.io/kuar-demo/kuard-amd64:blue kuard kubectl port-forward kuard 8080 :8080 Open your browser to http://localhost:8080 . Stern Stern allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging. 1 brew install stern Usage Imagine a build in Jenkins using more than one container in the Pod. You want to tail the logs of all containers... you can with stern. 1 stern maven- Kube Capacity Kube Capacity is a simple CLI that provides an overview of the resource requests, limits, and utilization in a Kubernetes cluster. 1 2 brew tap robscott/tap brew install robscott/tap/kube-capacity 1 kube-capacity 1 2 3 4 NODE CPU REQUESTS CPU LIMITS MEMORY REQUESTS MEMORY LIMITS * 560m ( 28 % ) 130m ( 7 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) example-node-1 220m ( 22 % ) 10m ( 1 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) example-node-2 340m ( 34 % ) 120m ( 12 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) 1 kube-capacity --pods 1 2 3 4 5 6 7 8 9 10 NODE NAMESPACE POD CPU REQUESTS CPU LIMITS MEMORY REQUESTS MEMORY LIMITS * * * 560m ( 28 % ) 780m ( 38 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) example-node-1 * * 220m ( 22 % ) 320m ( 32 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) example-node-1 kube-system metrics-server-lwc6z 100m ( 10 % ) 200m ( 20 % ) 100Mi ( 3 % ) 200Mi ( 7 % ) example-node-1 kube-system coredns-7b5bcb98f8 120m ( 12 % ) 120m ( 12 % ) 92Mi ( 3 % ) 160Mi ( 5 % ) example-node-2 * * 340m ( 34 % ) 460m ( 46 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) example-node-2 kube-system kube-proxy-3ki7 200m ( 20 % ) 280m ( 28 % ) 210Mi ( 7 % ) 210Mi ( 7 % ) example-node-2 tiller tiller-deploy 140m ( 14 % ) 180m ( 18 % ) 170Mi ( 5 % ) 200Mi ( 7 % ) 1 kube-capacity --util 1 2 3 4 NODE CPU REQUESTS CPU LIMITS CPU UTIL MEMORY REQUESTS MEMORY LIMITS MEMORY UTIL * 560m ( 28 % ) 130m ( 7 % ) 40m ( 2 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) 470Mi ( 8 % ) example-node-1 220m ( 22 % ) 10m ( 1 % ) 10m ( 1 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) 210Mi ( 7 % ) example-node-2 340m ( 34 % ) 120m ( 12 % ) 30m ( 3 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) 260Mi ( 9 % ) 1 kube-capacity --pods --util Velero Velero RBAC Lookup RBAC Lookup Install bash 1 brew install reactiveops/tap/rbac-lookup Krew 1 kubectl krew install rbac-lookup Lookup user 1 rbac-lookup jvandergriendt -owide Lookup GKE user 1 rbac-lookup jvandergriendt --gke K9S K9S is a tool that gives you a console UI on your kubernetes cluster/namespace. Install 1 brew tap derailed/k9s brew install k9s Use By default is looks at a single namespace, and allows you to view elements of the pods running. 1 k9s -n cje K9S K9S is a tool that gives you a console UI on your kubernetes cluster/namespace. Install 1 brew tap derailed/k9s brew install k9s Use By default is looks at a single namespace, and allows you to view elements of the pods running. 1 k9s -n cje Dive A tool for exploring a docker image, layer contents, and discovering ways to shrink your Docker image size. Dive is a tool for analyzing Docker images. Install Debian based 1 2 wget https://github.com/wagoodman/dive/releases/download/v0.7.1/dive_0.7.1_linux_amd64.deb sudo apt install ./dive_0.7.1_linux_amd64.deb RHEL based 1 2 curl -OL https://github.com/wagoodman/dive/releases/download/v0.7.1/dive_0.7.1_linux_amd64.rpm rpm -i dive_0.7.1_linux_amd64.rpm Homebrew 1 2 brew tap wagoodman/dive brew install dive Windows 1 go get github.com/wagoodman/dive Use Existing image 1 dive your-image-tag To be build image 1 dive build -t some-tag . For CI builds 1 CI = true dive your-image Kiali https://www.kiali.io/ Telepresence https://www.telepresence.io/","title":"Tools"},{"location":"kubernetes/tools/#kubernetes-tools","text":"","title":"Kubernetes Tools"},{"location":"kubernetes/tools/#helm","text":"We use Helm as a package manager to more easily install other tools on Kubernetes. There's several repositories with a large number of mature charts - the name of the Helm packages. One being Helm/Stable another being Helm Hub .","title":"Helm"},{"location":"kubernetes/tools/#install","text":"MacOS/Homebrew 1 brew install kubernetes-helm Windows/Chocolatey 1 hoco install kubernetes-helm Ubuntu/Snap 1 sudo snap install helm --classic Sccop 1 scoop install helm GoFish 1 gofish install helm","title":"Install"},{"location":"kubernetes/tools/#usage","text":"1 helm install stable/jenkins","title":"Usage"},{"location":"kubernetes/tools/#kubecontext","text":"Kubectx is a utility to manage and switch between Kubernetes ( kubectl ) contexts and namespaces (via kubens , see below).","title":"Kubecontext"},{"location":"kubernetes/tools/#install_1","text":"MacOS/Homebrew 1 brew install kubectx Ubuntu 1 sudo apt install kubectx","title":"Install"},{"location":"kubernetes/tools/#usage_1","text":"","title":"Usage"},{"location":"kubernetes/tools/#kubectx","text":"1 2 3 4 5 6 7 8 9 10 11 12 kubectx minikube Switched to context minikube . $ kubectx - Switched to context oregon . $ kubectx - Switched to context minikube . $ kubectx dublin = gke_ahmetb_europe-west1-b_dublin Context dublin set. Aliased gke_ahmetb_europe-west1-b_dublin as dublin .","title":"Kubectx"},{"location":"kubernetes/tools/#kubens","text":"Kubens (part of Kubectx) helps you manage your current Kubernetes namespace. 1 2 3 4 5 6 7 $ kubens kube-system Context test set. Active namespace is kube-system . $ kubens - Context test set. Active namespace is default .","title":"Kubens"},{"location":"kubernetes/tools/#kuard","text":"Kuard is a small demo application to show your cluster works. Also exposes some info you might want to see. 1 2 kubectl run --restart = Never --image = gcr.io/kuar-demo/kuard-amd64:blue kuard kubectl port-forward kuard 8080 :8080 Open your browser to http://localhost:8080 .","title":"Kuard"},{"location":"kubernetes/tools/#stern","text":"Stern allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging. 1 brew install stern","title":"Stern"},{"location":"kubernetes/tools/#usage_2","text":"Imagine a build in Jenkins using more than one container in the Pod. You want to tail the logs of all containers... you can with stern. 1 stern maven-","title":"Usage"},{"location":"kubernetes/tools/#kube-capacity","text":"Kube Capacity is a simple CLI that provides an overview of the resource requests, limits, and utilization in a Kubernetes cluster. 1 2 brew tap robscott/tap brew install robscott/tap/kube-capacity 1 kube-capacity 1 2 3 4 NODE CPU REQUESTS CPU LIMITS MEMORY REQUESTS MEMORY LIMITS * 560m ( 28 % ) 130m ( 7 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) example-node-1 220m ( 22 % ) 10m ( 1 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) example-node-2 340m ( 34 % ) 120m ( 12 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) 1 kube-capacity --pods 1 2 3 4 5 6 7 8 9 10 NODE NAMESPACE POD CPU REQUESTS CPU LIMITS MEMORY REQUESTS MEMORY LIMITS * * * 560m ( 28 % ) 780m ( 38 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) example-node-1 * * 220m ( 22 % ) 320m ( 32 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) example-node-1 kube-system metrics-server-lwc6z 100m ( 10 % ) 200m ( 20 % ) 100Mi ( 3 % ) 200Mi ( 7 % ) example-node-1 kube-system coredns-7b5bcb98f8 120m ( 12 % ) 120m ( 12 % ) 92Mi ( 3 % ) 160Mi ( 5 % ) example-node-2 * * 340m ( 34 % ) 460m ( 46 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) example-node-2 kube-system kube-proxy-3ki7 200m ( 20 % ) 280m ( 28 % ) 210Mi ( 7 % ) 210Mi ( 7 % ) example-node-2 tiller tiller-deploy 140m ( 14 % ) 180m ( 18 % ) 170Mi ( 5 % ) 200Mi ( 7 % ) 1 kube-capacity --util 1 2 3 4 NODE CPU REQUESTS CPU LIMITS CPU UTIL MEMORY REQUESTS MEMORY LIMITS MEMORY UTIL * 560m ( 28 % ) 130m ( 7 % ) 40m ( 2 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) 470Mi ( 8 % ) example-node-1 220m ( 22 % ) 10m ( 1 % ) 10m ( 1 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) 210Mi ( 7 % ) example-node-2 340m ( 34 % ) 120m ( 12 % ) 30m ( 3 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) 260Mi ( 9 % ) 1 kube-capacity --pods --util","title":"Kube Capacity"},{"location":"kubernetes/tools/#velero","text":"Velero","title":"Velero"},{"location":"kubernetes/tools/#rbac-lookup","text":"RBAC Lookup","title":"RBAC Lookup"},{"location":"kubernetes/tools/#install_2","text":"bash 1 brew install reactiveops/tap/rbac-lookup Krew 1 kubectl krew install rbac-lookup","title":"Install"},{"location":"kubernetes/tools/#lookup-user","text":"1 rbac-lookup jvandergriendt -owide","title":"Lookup user"},{"location":"kubernetes/tools/#lookup-gke-user","text":"1 rbac-lookup jvandergriendt --gke","title":"Lookup GKE user"},{"location":"kubernetes/tools/#k9s","text":"K9S is a tool that gives you a console UI on your kubernetes cluster/namespace.","title":"K9S"},{"location":"kubernetes/tools/#install_3","text":"1 brew tap derailed/k9s brew install k9s","title":"Install"},{"location":"kubernetes/tools/#use","text":"By default is looks at a single namespace, and allows you to view elements of the pods running. 1 k9s -n cje","title":"Use"},{"location":"kubernetes/tools/#k9s_1","text":"K9S is a tool that gives you a console UI on your kubernetes cluster/namespace.","title":"K9S"},{"location":"kubernetes/tools/#install_4","text":"1 brew tap derailed/k9s brew install k9s","title":"Install"},{"location":"kubernetes/tools/#use_1","text":"By default is looks at a single namespace, and allows you to view elements of the pods running. 1 k9s -n cje","title":"Use"},{"location":"kubernetes/tools/#dive","text":"A tool for exploring a docker image, layer contents, and discovering ways to shrink your Docker image size. Dive is a tool for analyzing Docker images.","title":"Dive"},{"location":"kubernetes/tools/#install_5","text":"Debian based 1 2 wget https://github.com/wagoodman/dive/releases/download/v0.7.1/dive_0.7.1_linux_amd64.deb sudo apt install ./dive_0.7.1_linux_amd64.deb RHEL based 1 2 curl -OL https://github.com/wagoodman/dive/releases/download/v0.7.1/dive_0.7.1_linux_amd64.rpm rpm -i dive_0.7.1_linux_amd64.rpm Homebrew 1 2 brew tap wagoodman/dive brew install dive Windows 1 go get github.com/wagoodman/dive","title":"Install"},{"location":"kubernetes/tools/#use_2","text":"Existing image 1 dive your-image-tag To be build image 1 dive build -t some-tag . For CI builds 1 CI = true dive your-image","title":"Use"},{"location":"kubernetes/tools/#kiali","text":"https://www.kiali.io/","title":"Kiali"},{"location":"kubernetes/tools/#telepresence","text":"https://www.telepresence.io/","title":"Telepresence"},{"location":"kubernetes/cicd/cdp-jenkins-helm/","text":"CD Pipeline with Jenkins Helm Prerequisites Kubernetes 1.9.x+ cluster Valid domain names Jenkins 2.x+ with pipeline plugins below Helm/Tiller Tools Jenkins 2.x Install Helm For more information, checkout the github page . Helm's current version (as of October 2018) - version 2 - consists of two parts. One is a local client - Helm - which you should install on your own machine, see here for how. The other is a server component part - Tiller - that should be installed in your Kubernetes cluster. Install Tiller 1 kubectl create serviceaccount tiller --namespace kube-system create rbac config: rbac-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : v1 kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : tiller-role-binding roleRef : kind : ClusterRole name : cluster-admin apiGroup : rbac.authorization.k8s.io subjects : - kind : ServiceAccount name : tiller namespace : kube-system 1 2 kubectl apply -f rbac-config.yaml helm init --service-account tiller install nginx helm chart 1 helm install stable/nginx-ingress Jenkins Plugins Warnings Plugin: https://github.com/jenkinsci/warnings-plugin/blob/master/doc/Documentation.md Anchore: https://jenkins.io/blog/2018/06/20/anchore-image-scanning/ Anchore https://github.com/anchore/anchore-engine https://github.com/helm/charts/tree/master/stable/anchore-engine https://wiki.jenkins.io/display/JENKINS/Anchore+Container+Image+Scanner+Plugin","title":"CD Pipeline with Jenkins & Helm"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#cd-pipeline-with-jenkins-helm","text":"","title":"CD Pipeline with Jenkins &amp; Helm"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#prerequisites","text":"Kubernetes 1.9.x+ cluster Valid domain names Jenkins 2.x+ with pipeline plugins below Helm/Tiller","title":"Prerequisites"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#tools","text":"Jenkins 2.x","title":"Tools"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#install-helm","text":"For more information, checkout the github page . Helm's current version (as of October 2018) - version 2 - consists of two parts. One is a local client - Helm - which you should install on your own machine, see here for how. The other is a server component part - Tiller - that should be installed in your Kubernetes cluster.","title":"Install Helm"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#install-tiller","text":"1 kubectl create serviceaccount tiller --namespace kube-system create rbac config: rbac-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : v1 kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : tiller-role-binding roleRef : kind : ClusterRole name : cluster-admin apiGroup : rbac.authorization.k8s.io subjects : - kind : ServiceAccount name : tiller namespace : kube-system 1 2 kubectl apply -f rbac-config.yaml helm init --service-account tiller","title":"Install Tiller"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#install-nginx-helm-chart","text":"1 helm install stable/nginx-ingress","title":"install nginx helm chart"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#jenkins-plugins","text":"Warnings Plugin: https://github.com/jenkinsci/warnings-plugin/blob/master/doc/Documentation.md Anchore: https://jenkins.io/blog/2018/06/20/anchore-image-scanning/","title":"Jenkins Plugins"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#anchore","text":"https://github.com/anchore/anchore-engine https://github.com/helm/charts/tree/master/stable/anchore-engine https://wiki.jenkins.io/display/JENKINS/Anchore+Container+Image+Scanner+Plugin","title":"Anchore"},{"location":"kubernetes/distributions/","text":"Kubernetes What is kubernetes Kubernetes Objects Kubernetes tutorials Kubernetes Guides Linux basics Namespaces CGroups https://jvns.ca/blog/2016/10/10/what-even-is-a-container/ https://www.youtube.com/watch?v=sK5i-N34im8 https://www.ianlewis.org/en/what-are-kubernetes-pods-anyway Networking https://itnext.io/kubernetes-networking-behind-the-scenes-39a1ab1792bb https://github.com/nleiva/kubernetes-networking-links IP Tables CIDR Explanation video Packets Frames introduction Ingress Traefik on AWS Metrics https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-66936addedae Secrets Hashicorp Vault - Kelsey Hightower https://medium.com/qubit-engineering/kubernetes-up-integrated-secrets-configuration-5a15b9f5a6c6 Security RBAC 11 ways not to get hacked on Kubernetes https://www.youtube.com/channel/UCiqnRXPAAk6iv2m47odUFzw Tools to use Microscanner Dex - OpenID Connect solution Sonubuoy Helm Learn LUA book , lua's required for Helm 3.0 ChartMuseum Skaffold KSynch Traefik Istio Falco Prometheus Grafana Jenkins Kaniko Prow Tarmak Kube-Lego Knative Rook Stern - aggregate log rendering tool Linkerd 2","title":"Kubernetes"},{"location":"kubernetes/distributions/#kubernetes","text":"","title":"Kubernetes"},{"location":"kubernetes/distributions/#what-is-kubernetes","text":"","title":"What is kubernetes"},{"location":"kubernetes/distributions/#kubernetes-objects","text":"","title":"Kubernetes Objects"},{"location":"kubernetes/distributions/#kubernetes-tutorials","text":"","title":"Kubernetes tutorials"},{"location":"kubernetes/distributions/#kubernetes-guides","text":"","title":"Kubernetes Guides"},{"location":"kubernetes/distributions/#linux-basics","text":"","title":"Linux basics"},{"location":"kubernetes/distributions/#namespaces-cgroups","text":"https://jvns.ca/blog/2016/10/10/what-even-is-a-container/ https://www.youtube.com/watch?v=sK5i-N34im8 https://www.ianlewis.org/en/what-are-kubernetes-pods-anyway","title":"Namespaces &amp; CGroups"},{"location":"kubernetes/distributions/#networking","text":"https://itnext.io/kubernetes-networking-behind-the-scenes-39a1ab1792bb https://github.com/nleiva/kubernetes-networking-links IP Tables CIDR Explanation video Packets Frames introduction","title":"Networking"},{"location":"kubernetes/distributions/#ingress","text":"Traefik on AWS","title":"Ingress"},{"location":"kubernetes/distributions/#metrics","text":"https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-66936addedae","title":"Metrics"},{"location":"kubernetes/distributions/#secrets","text":"Hashicorp Vault - Kelsey Hightower https://medium.com/qubit-engineering/kubernetes-up-integrated-secrets-configuration-5a15b9f5a6c6","title":"Secrets"},{"location":"kubernetes/distributions/#security","text":"RBAC 11 ways not to get hacked on Kubernetes https://www.youtube.com/channel/UCiqnRXPAAk6iv2m47odUFzw","title":"Security"},{"location":"kubernetes/distributions/#tools-to-use","text":"Microscanner Dex - OpenID Connect solution Sonubuoy Helm Learn LUA book , lua's required for Helm 3.0 ChartMuseum Skaffold KSynch Traefik Istio Falco Prometheus Grafana Jenkins Kaniko Prow Tarmak Kube-Lego Knative Rook Stern - aggregate log rendering tool Linkerd 2","title":"Tools to use"},{"location":"kubernetes/distributions/aks-cli/","text":"Azure CLI Configure AZ CLI 1 az login 1 az account show --query {subscriptionId:id, tenantId:tenantId} 1 export SUBSCRIPTION_ID = 1 2 SUBSCRIPTION_ID = ... az account set --subscription = ${ SUBSCRIPTION_ID } 1 az ad sp create-for-rbac --role = Owner --scopes = /subscriptions/ ${ SUBSCRIPTION_ID } Should be owner Should be owner, else it cannot create a LoadBalancer via the nginx - ingress . See: https://weidongzhou.wordpress.com/2018/06/27/could-not-get-external-ip-for-load-balancer-on-azure-aks/ And: https://github.com/Azure/AKS/issues/427 Configure Kubecontext 1 az aks get-credentials --resource-group cbcore --name cbcore Configure Cluster Autoscaler 1 az extension add --name aks-preview 1 2 3 4 az feature register --name VMSSPreview --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService/VMSSPreview )].{Name:name,State:properties.state} az provider register --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService )].{Name:name,State:properties.state} Configure multi-node pool 1 2 az feature register --name MultiAgentpoolPreview --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService/MultiAgentpoolPreview )].{Name:name,State:properties.state} 1 az provider register --namespace Microsoft.ContainerService Create AKS cluster via CLI Resources https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/ Get available versions 1 az aks get-versions --location westeurope Create initial cluster Prepare variables 1 2 3 4 5 6 7 RESOURCE_GROUP_NAME = CLUSTER_NAME = LOCATION = eastus NODE_POOL_MASTERS = masters NODE_POOL_BUILDS = builds VM_SIZE_MASTERS_NP = Standard_DS2_v2 VM_SIZE_BUILDS_NP = ? Create Resource Group 1 az group create --name ${ RESOURCE_GROUP_NAME } --location ${ LOCATION } Create AKS Cluster 1 2 3 4 5 6 7 8 9 10 az aks create \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --name ${ CLUSTER_NAME } \\ --enable-vmss \\ --node-count 1 \\ --nodepool-name ${ NODE_POOL_MASTERS } \\ --node-vm-size ${ VM_SIZE_MASTERS_NP } \\ --enable-cluster-autoscaler \\ --enable-vmss \\ --generate-ssh-keys PodSecurityPolicy 1 2 --enable-cluster-autoscaler : Enable cluster autoscaler, default value is false. If specified, please make sure the kubernetes version is larger than 1 .10.6. Networking 1 2 3 4 5 6 7 --network-plugin : The Kubernetes network plugin to use. Specify azure for advanced networking configurations. Defaults to kubenet . --network-policy : ( PREVIEW ) The Kubernetes network policy to use. Using together with azure network plugin. Specify azure for Azure network policy manager and calico for calico network policy controller. Defaults to ( network policy disabled ) . Retrieve credentials 1 az aks get-credentials --resource-group ${ RESOURCE_GROUP_NAME } --name ${ CLUSTER_NAME } Add second node pool 1 2 3 4 5 az aks nodepool add \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --cluster-name ${ CLUSTER_NAME } \\ --name ${ NODE_POOL_BUILDS } \\ --node-count 3","title":"AKS CLI"},{"location":"kubernetes/distributions/aks-cli/#azure-cli","text":"","title":"Azure CLI"},{"location":"kubernetes/distributions/aks-cli/#configure-az-cli","text":"1 az login 1 az account show --query {subscriptionId:id, tenantId:tenantId} 1 export SUBSCRIPTION_ID = 1 2 SUBSCRIPTION_ID = ... az account set --subscription = ${ SUBSCRIPTION_ID } 1 az ad sp create-for-rbac --role = Owner --scopes = /subscriptions/ ${ SUBSCRIPTION_ID }","title":"Configure AZ CLI"},{"location":"kubernetes/distributions/aks-cli/#should-be-owner","text":"Should be owner, else it cannot create a LoadBalancer via the nginx - ingress . See: https://weidongzhou.wordpress.com/2018/06/27/could-not-get-external-ip-for-load-balancer-on-azure-aks/ And: https://github.com/Azure/AKS/issues/427","title":"Should be owner"},{"location":"kubernetes/distributions/aks-cli/#configure-kubecontext","text":"1 az aks get-credentials --resource-group cbcore --name cbcore","title":"Configure Kubecontext"},{"location":"kubernetes/distributions/aks-cli/#configure-cluster-autoscaler","text":"1 az extension add --name aks-preview 1 2 3 4 az feature register --name VMSSPreview --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService/VMSSPreview )].{Name:name,State:properties.state} az provider register --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService )].{Name:name,State:properties.state}","title":"Configure Cluster Autoscaler"},{"location":"kubernetes/distributions/aks-cli/#configure-multi-node-pool","text":"1 2 az feature register --name MultiAgentpoolPreview --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService/MultiAgentpoolPreview )].{Name:name,State:properties.state} 1 az provider register --namespace Microsoft.ContainerService","title":"Configure multi-node pool"},{"location":"kubernetes/distributions/aks-cli/#create-aks-cluster-via-cli","text":"","title":"Create AKS cluster via CLI"},{"location":"kubernetes/distributions/aks-cli/#resources","text":"https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/","title":"Resources"},{"location":"kubernetes/distributions/aks-cli/#get-available-versions","text":"1 az aks get-versions --location westeurope","title":"Get available versions"},{"location":"kubernetes/distributions/aks-cli/#create-initial-cluster","text":"","title":"Create initial cluster"},{"location":"kubernetes/distributions/aks-cli/#prepare-variables","text":"1 2 3 4 5 6 7 RESOURCE_GROUP_NAME = CLUSTER_NAME = LOCATION = eastus NODE_POOL_MASTERS = masters NODE_POOL_BUILDS = builds VM_SIZE_MASTERS_NP = Standard_DS2_v2 VM_SIZE_BUILDS_NP = ?","title":"Prepare variables"},{"location":"kubernetes/distributions/aks-cli/#create-resource-group","text":"1 az group create --name ${ RESOURCE_GROUP_NAME } --location ${ LOCATION }","title":"Create Resource Group"},{"location":"kubernetes/distributions/aks-cli/#create-aks-cluster","text":"1 2 3 4 5 6 7 8 9 10 az aks create \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --name ${ CLUSTER_NAME } \\ --enable-vmss \\ --node-count 1 \\ --nodepool-name ${ NODE_POOL_MASTERS } \\ --node-vm-size ${ VM_SIZE_MASTERS_NP } \\ --enable-cluster-autoscaler \\ --enable-vmss \\ --generate-ssh-keys","title":"Create AKS Cluster"},{"location":"kubernetes/distributions/aks-cli/#podsecuritypolicy","text":"1 2 --enable-cluster-autoscaler : Enable cluster autoscaler, default value is false. If specified, please make sure the kubernetes version is larger than 1 .10.6.","title":"PodSecurityPolicy"},{"location":"kubernetes/distributions/aks-cli/#networking","text":"1 2 3 4 5 6 7 --network-plugin : The Kubernetes network plugin to use. Specify azure for advanced networking configurations. Defaults to kubenet . --network-policy : ( PREVIEW ) The Kubernetes network policy to use. Using together with azure network plugin. Specify azure for Azure network policy manager and calico for calico network policy controller. Defaults to ( network policy disabled ) .","title":"Networking"},{"location":"kubernetes/distributions/aks-cli/#retrieve-credentials","text":"1 az aks get-credentials --resource-group ${ RESOURCE_GROUP_NAME } --name ${ CLUSTER_NAME }","title":"Retrieve credentials"},{"location":"kubernetes/distributions/aks-cli/#add-second-node-pool","text":"1 2 3 4 5 az aks nodepool add \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --cluster-name ${ CLUSTER_NAME } \\ --name ${ NODE_POOL_BUILDS } \\ --node-count 3","title":"Add second node pool"},{"location":"kubernetes/distributions/aks-terraform/","text":"AKS Terraform Resources https://docs.microsoft.com/en-us/azure/terraform/terraform-create-k8s-cluster-with-tf-and-aks https://www.terraform.io/docs/providers/azurerm/r/kubernetes_cluster.html Pre-Requisites Create Service Principle It comes from this guide . 1 az account show -- query {subscriptionId:id, tenantId:tenantId} 1 az ad sp create-for-rbac --role = Owner --scopes = /subscriptions/ ${ SUBSCRIPTION_ID } Retrieve current Kubernetes Versions 1 az aks get-versions --location westeurope --output table Terraform Config 1 2 3 4 5 ARM_SUBSCRIPTION_ID = ARM_CLIENT_ID = ARM_CLIENT_SECRET = ARM_TENANT_ID = ARM_ENVIRONMENT = Create storage account for TF State 1 2 3 4 LOCATION = westeurope RESOURCE_GROUP_NAME = joostvdg-cb-ext-storage STORAGE_ACCOUNT_NAME = joostvdgcbtfstate CONTAINER_NAME = tfstate List locations 1 2 3 az account list-locations \\ --query [].{Region:name} \\ --out table Create resource group 1 2 3 az group create \\ --name ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION } Create storage account 1 2 3 4 5 6 az storage account create \\ --name ${ STORAGE_ACCOUNT_NAME } \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION } \\ --sku Standard_ZRS \\ --kind StorageV2 Retrieve storage account login Apparently, no CLI commands available? Use the Azure Blog on AKS via Terraform for how via the UI. 1 STORAGE_ACCOUNT_KEY = Create TF Storage 1 az storage container create -n ${ CONTAINER_NAME } --account-name ${ STORAGE_ACCOUNT_NAME } --account-key ${ STORAGE_ACCOUNT_KEY } Init Terraform backend 1 2 3 4 terraform init -backend-config = storage_account_name= ${ STORAGE_ACCOUNT_NAME } \\ -backend-config = container_name= ${ CONTAINER_NAME } \\ -backend-config = access_key= ${ STORAGE_ACCOUNT_KEY } \\ -backend-config = key=codelab.microsoft.tfstate Expose temp variables These are from your Service Principle we created earlier. Where client_id = appId, and client_secret the password. 1 2 export TF_VAR_client_id = your-client-id export TF_VAR_client_secret = your-client-secret Rollout Set variables 1 source ../export-variables.sh Validate 1 terraform validate Plan 1 terraform plan -out out.plan Apply the plan 1 terraform apply out.plan Get kubectl config 1 2 AKS_RESOURCE_GROUP = joostvdg-cbcore AKS_CLUSTER_NAME = acctestaks1 1 az aks get-credentials --resource-group ${ AKS_RESOURCE_GROUP } --name ${ AKS_CLUSTER_NAME } Enable Preview Features Currently having cluster autoscalers requires enabling of a Preview Feature in Azure. The same holds true for enabling multiple node pools, which I think is a best practice for using Kubernetes. Enable Multi Node Pool Enable Cluster Autoscaler - via VMScaleSets Terraform Code Important When using Terraform for AKS and you want to use Multiple Node Pools and/or the Cluster Autoscaler, you need to use the minimum of 1 . 32 . 0 of the azurerm provider. main.tf 1 2 3 4 5 6 7 8 provider azurerm { # whilst the `version` attribute is optional, we recommend pinning to a given version of the Provider version = ~ 1.32.0 } terraform { backend azurerm {} } k8s.tf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 resource azurerm_kubernetes_cluster k8s { name = acctestaks1 location = ${azurerm_resource_group.k8s.location} resource_group_name = ${azurerm_resource_group.k8s.name} dns_prefix = jvdg kubernetes_version = ${var.kubernetes_version} agent_pool_profile { name = default vm_size = Standard_D2s_v3 os_type = Linux os_disk_size_gb = 30 enable_auto_scaling = true count = 2 min_count = 2 max_count = 3 type = VirtualMachineScaleSets node_taints = [ mytaint = true :NoSchedule ] } agent_pool_profile { name = pool1 vm_size = Standard_D2s_v3 os_type = Linux os_disk_size_gb = 30 enable_auto_scaling = true min_count = 1 max_count = 3 type = VirtualMachineScaleSets } agent_pool_profile { name = pool2 vm_size = Standard_D4s_v3 os_type = Linux os_disk_size_gb = 30 enable_auto_scaling = true min_count = 1 max_count = 3 type = VirtualMachineScaleSets } role_based_access_control { enabled = true } service_principal { client_id = ${var.client_id} client_secret = ${var.client_secret} } tags = { Environment = Development CreatedBy = Joostvdg } } output client_certificate { value = ${azurerm_kubernetes_cluster.k8s.kube_config.0.client_certificate} } output kube_config { value = ${azurerm_kubernetes_cluster.k8s.kube_config_raw} } variables.tf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 variable client_id {} variable client_secret {} variable kubernetes_version { default = 1.14.6 } variable agent_count { default = 3 } variable ssh_public_key { default = ~/.ssh/id_rsa.pub } variable dns_prefix { default = jvdg } variable cluster_name { default = cbcore } variable resource_group_name { default = joostvdg-cbcore } variable container_registry_name { default = joostvdgacr } variable location { default = westeurope } acr.tf 1 2 3 4 5 6 7 8 9 10 11 12 resource azurerm_resource_group ecr { name = ${var.resource_group_name}-acr location = ${var.location} } resource azurerm_container_registry acr { name = ${var.container_registry_name} resource_group_name = ${azurerm_resource_group.ecr.name} location = ${azurerm_resource_group.k8s.location} sku = Premium admin_enabled = false } resource-group.tf 1 2 3 4 resource azurerm_resource_group k8s { name = ${var.resource_group_name} location = ${var.location} }","title":"AKS Terraform"},{"location":"kubernetes/distributions/aks-terraform/#aks-terraform","text":"","title":"AKS Terraform"},{"location":"kubernetes/distributions/aks-terraform/#resources","text":"https://docs.microsoft.com/en-us/azure/terraform/terraform-create-k8s-cluster-with-tf-and-aks https://www.terraform.io/docs/providers/azurerm/r/kubernetes_cluster.html","title":"Resources"},{"location":"kubernetes/distributions/aks-terraform/#pre-requisites","text":"","title":"Pre-Requisites"},{"location":"kubernetes/distributions/aks-terraform/#create-service-principle","text":"It comes from this guide . 1 az account show -- query {subscriptionId:id, tenantId:tenantId} 1 az ad sp create-for-rbac --role = Owner --scopes = /subscriptions/ ${ SUBSCRIPTION_ID }","title":"Create Service Principle"},{"location":"kubernetes/distributions/aks-terraform/#retrieve-current-kubernetes-versions","text":"1 az aks get-versions --location westeurope --output table","title":"Retrieve current Kubernetes Versions"},{"location":"kubernetes/distributions/aks-terraform/#terraform-config","text":"1 2 3 4 5 ARM_SUBSCRIPTION_ID = ARM_CLIENT_ID = ARM_CLIENT_SECRET = ARM_TENANT_ID = ARM_ENVIRONMENT =","title":"Terraform Config"},{"location":"kubernetes/distributions/aks-terraform/#create-storage-account-for-tf-state","text":"1 2 3 4 LOCATION = westeurope RESOURCE_GROUP_NAME = joostvdg-cb-ext-storage STORAGE_ACCOUNT_NAME = joostvdgcbtfstate CONTAINER_NAME = tfstate","title":"Create storage account for TF State"},{"location":"kubernetes/distributions/aks-terraform/#list-locations","text":"1 2 3 az account list-locations \\ --query [].{Region:name} \\ --out table","title":"List locations"},{"location":"kubernetes/distributions/aks-terraform/#create-resource-group","text":"1 2 3 az group create \\ --name ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION }","title":"Create resource group"},{"location":"kubernetes/distributions/aks-terraform/#create-storage-account","text":"1 2 3 4 5 6 az storage account create \\ --name ${ STORAGE_ACCOUNT_NAME } \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION } \\ --sku Standard_ZRS \\ --kind StorageV2","title":"Create storage account"},{"location":"kubernetes/distributions/aks-terraform/#retrieve-storage-account-login","text":"Apparently, no CLI commands available? Use the Azure Blog on AKS via Terraform for how via the UI. 1 STORAGE_ACCOUNT_KEY =","title":"Retrieve storage account login"},{"location":"kubernetes/distributions/aks-terraform/#create-tf-storage","text":"1 az storage container create -n ${ CONTAINER_NAME } --account-name ${ STORAGE_ACCOUNT_NAME } --account-key ${ STORAGE_ACCOUNT_KEY }","title":"Create TF Storage"},{"location":"kubernetes/distributions/aks-terraform/#init-terraform-backend","text":"1 2 3 4 terraform init -backend-config = storage_account_name= ${ STORAGE_ACCOUNT_NAME } \\ -backend-config = container_name= ${ CONTAINER_NAME } \\ -backend-config = access_key= ${ STORAGE_ACCOUNT_KEY } \\ -backend-config = key=codelab.microsoft.tfstate","title":"Init Terraform backend"},{"location":"kubernetes/distributions/aks-terraform/#expose-temp-variables","text":"These are from your Service Principle we created earlier. Where client_id = appId, and client_secret the password. 1 2 export TF_VAR_client_id = your-client-id export TF_VAR_client_secret = your-client-secret","title":"Expose temp variables"},{"location":"kubernetes/distributions/aks-terraform/#rollout","text":"","title":"Rollout"},{"location":"kubernetes/distributions/aks-terraform/#set-variables","text":"1 source ../export-variables.sh","title":"Set variables"},{"location":"kubernetes/distributions/aks-terraform/#validate","text":"1 terraform validate","title":"Validate"},{"location":"kubernetes/distributions/aks-terraform/#plan","text":"1 terraform plan -out out.plan","title":"Plan"},{"location":"kubernetes/distributions/aks-terraform/#apply-the-plan","text":"1 terraform apply out.plan","title":"Apply the plan"},{"location":"kubernetes/distributions/aks-terraform/#get-kubectl-config","text":"1 2 AKS_RESOURCE_GROUP = joostvdg-cbcore AKS_CLUSTER_NAME = acctestaks1 1 az aks get-credentials --resource-group ${ AKS_RESOURCE_GROUP } --name ${ AKS_CLUSTER_NAME }","title":"Get kubectl config"},{"location":"kubernetes/distributions/aks-terraform/#enable-preview-features","text":"Currently having cluster autoscalers requires enabling of a Preview Feature in Azure. The same holds true for enabling multiple node pools, which I think is a best practice for using Kubernetes. Enable Multi Node Pool Enable Cluster Autoscaler - via VMScaleSets","title":"Enable Preview Features"},{"location":"kubernetes/distributions/aks-terraform/#terraform-code","text":"Important When using Terraform for AKS and you want to use Multiple Node Pools and/or the Cluster Autoscaler, you need to use the minimum of 1 . 32 . 0 of the azurerm provider. main.tf 1 2 3 4 5 6 7 8 provider azurerm { # whilst the `version` attribute is optional, we recommend pinning to a given version of the Provider version = ~ 1.32.0 } terraform { backend azurerm {} } k8s.tf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 resource azurerm_kubernetes_cluster k8s { name = acctestaks1 location = ${azurerm_resource_group.k8s.location} resource_group_name = ${azurerm_resource_group.k8s.name} dns_prefix = jvdg kubernetes_version = ${var.kubernetes_version} agent_pool_profile { name = default vm_size = Standard_D2s_v3 os_type = Linux os_disk_size_gb = 30 enable_auto_scaling = true count = 2 min_count = 2 max_count = 3 type = VirtualMachineScaleSets node_taints = [ mytaint = true :NoSchedule ] } agent_pool_profile { name = pool1 vm_size = Standard_D2s_v3 os_type = Linux os_disk_size_gb = 30 enable_auto_scaling = true min_count = 1 max_count = 3 type = VirtualMachineScaleSets } agent_pool_profile { name = pool2 vm_size = Standard_D4s_v3 os_type = Linux os_disk_size_gb = 30 enable_auto_scaling = true min_count = 1 max_count = 3 type = VirtualMachineScaleSets } role_based_access_control { enabled = true } service_principal { client_id = ${var.client_id} client_secret = ${var.client_secret} } tags = { Environment = Development CreatedBy = Joostvdg } } output client_certificate { value = ${azurerm_kubernetes_cluster.k8s.kube_config.0.client_certificate} } output kube_config { value = ${azurerm_kubernetes_cluster.k8s.kube_config_raw} } variables.tf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 variable client_id {} variable client_secret {} variable kubernetes_version { default = 1.14.6 } variable agent_count { default = 3 } variable ssh_public_key { default = ~/.ssh/id_rsa.pub } variable dns_prefix { default = jvdg } variable cluster_name { default = cbcore } variable resource_group_name { default = joostvdg-cbcore } variable container_registry_name { default = joostvdgacr } variable location { default = westeurope } acr.tf 1 2 3 4 5 6 7 8 9 10 11 12 resource azurerm_resource_group ecr { name = ${var.resource_group_name}-acr location = ${var.location} } resource azurerm_container_registry acr { name = ${var.container_registry_name} resource_group_name = ${azurerm_resource_group.ecr.name} location = ${azurerm_resource_group.k8s.location} sku = Premium admin_enabled = false } resource-group.tf 1 2 3 4 resource azurerm_resource_group k8s { name = ${var.resource_group_name} location = ${var.location} }","title":"Terraform Code"},{"location":"kubernetes/distributions/aks/","text":"Azure Kubernetes Service Resources https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/ https://www.cloudbees.com/blog/securing-jenkins-role-based-access-control-and-azure-active-directory https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/aks-install/# https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/kubernetes-helm-install/#_additional_information_creating_a_tls_certificate Configure AZ CLI 1 az login 1 az account show --query {subscriptionId:id, tenantId:tenantId} 1 export SUBSCRIPTION_ID = 1 2 SUBSCRIPTION_ID = ... az account set --subscription = ${ SUBSCRIPTION_ID } 1 az ad sp create-for-rbac --role = Owner --scopes = /subscriptions/ ${ SUBSCRIPTION_ID } Should be owner Should be owner, else it cannot create a LoadBalancer via the nginx - ingress . See: https://weidongzhou.wordpress.com/2018/06/27/could-not-get-external-ip-for-load-balancer-on-azure-aks/ And: https://github.com/Azure/AKS/issues/427 Terraform Config 1 2 3 4 5 ARM_SUBSCRIPTION_ID ARM_CLIENT_ID ARM_CLIENT_SECRET ARM_TENANT_ID ARM_ENVIRONMENT Create storage account for TF State 1 2 3 4 LOCATION = westeurope RESOURCE_GROUP_NAME = joostvdg-cb-ext-storage STORAGE_ACCOUNT_NAME = joostvdgcbtfstate CONTAINER_NAME = tfstate List locations 1 2 3 az account list-locations \\ --query [].{Region:name} \\ --out table Create resource group 1 2 3 az group create \\ --name ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION } Create storage account 1 2 3 4 5 6 az storage account create \\ --name ${ STORAGE_ACCOUNT_NAME } \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION } \\ --sku Standard_ZRS \\ --kind StorageV2 Retrieve storage account login Apparently, no CLI commands available? Use the Azure Blog on AKS via Terraform for how via the UI. 1 STORAGE_ACCOUNT_KEY = Create TF Storage 1 az storage container create -n ${ CONTAINER_NAME } --account-name ${ STORAGE_ACCOUNT_NAME } --account-key ${ STORAGE_ACCOUNT_KEY } Init Terraform backend 1 2 3 4 terraform init -backend-config = storage_account_name= ${ STORAGE_ACCOUNT_NAME } \\ -backend-config = container_name= ${ CONTAINER_NAME } \\ -backend-config = access_key= ${ STORAGE_ACCOUNT_KEY } \\ -backend-config = key=codelab.microsoft.tfstate Expose temp variables These are from your Service Principle we created earlier. Where client_id = appId, and client_secret the password. 1 2 export TF_VAR_client_id = your-client-id export TF_VAR_client_secret = your-client-secret Rollout Set variables 1 source ../export-variables.sh Validate 1 terraform validate Plan 1 terraform plan -out out.plan Apply the plan 1 terraform apply out.plan Configure Kubecontext 1 az aks get-credentials --resource-group cbcore --name cbcore Configure Cluster Autoscaler 1 az extension add --name aks-preview 1 2 3 4 az feature register --name VMSSPreview --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService/VMSSPreview )].{Name:name,State:properties.state} az provider register --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService )].{Name:name,State:properties.state} Configure multi-node pool 1 2 az feature register --name MultiAgentpoolPreview --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService/MultiAgentpoolPreview )].{Name:name,State:properties.state} 1 az provider register --namespace Microsoft.ContainerService Create AKS cluster via CLI Resources https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/ Get available versions 1 az aks get-versions --location westeurope Create initial cluster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Create a resource group in East US az group create --name myResourceGroup --location eastus # Create a basic single-node AKS cluster az aks create \\ --resource-group myResourceGroup \\ --name myAKSCluster \\ --enable-vmss \\ --node-count 1 \\ --nodepool-name masters \\ --node-vm-size Standard_DS2_v2 \\ --enable-cluster-autoscaler \\ --enable-vmss \\ --generate-ssh-keys PodSecurityPolicy 1 2 --enable-cluster-autoscaler : Enable cluster autoscaler, default value is false. If specified, please make sure the kubernetes version is larger than 1 .10.6. Networking 1 2 3 4 5 6 7 --network-plugin : The Kubernetes network plugin to use. Specify azure for advanced networking configurations. Defaults to kubenet . --network-policy : ( PREVIEW ) The Kubernetes network policy to use. Using together with azure network plugin. Specify azure for Azure network policy manager and calico for calico network policy controller. Defaults to ( network policy disabled ) . Retrieve credentials 1 az aks get-credentials --resource-group myResourceGroup --name myAKSCluster Add second node pool 1 2 3 4 5 az aks nodepool add \\ --resource-group myResourceGroup \\ --cluster-name myAKSCluster \\ --name mynodepool \\ --node-count 3","title":"Azure Kubernetes Service"},{"location":"kubernetes/distributions/aks/#azure-kubernetes-service","text":"","title":"Azure Kubernetes Service"},{"location":"kubernetes/distributions/aks/#resources","text":"https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/ https://www.cloudbees.com/blog/securing-jenkins-role-based-access-control-and-azure-active-directory https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/aks-install/# https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/kubernetes-helm-install/#_additional_information_creating_a_tls_certificate","title":"Resources"},{"location":"kubernetes/distributions/aks/#configure-az-cli","text":"1 az login 1 az account show --query {subscriptionId:id, tenantId:tenantId} 1 export SUBSCRIPTION_ID = 1 2 SUBSCRIPTION_ID = ... az account set --subscription = ${ SUBSCRIPTION_ID } 1 az ad sp create-for-rbac --role = Owner --scopes = /subscriptions/ ${ SUBSCRIPTION_ID }","title":"Configure AZ CLI"},{"location":"kubernetes/distributions/aks/#should-be-owner","text":"Should be owner, else it cannot create a LoadBalancer via the nginx - ingress . See: https://weidongzhou.wordpress.com/2018/06/27/could-not-get-external-ip-for-load-balancer-on-azure-aks/ And: https://github.com/Azure/AKS/issues/427","title":"Should be owner"},{"location":"kubernetes/distributions/aks/#terraform-config","text":"1 2 3 4 5 ARM_SUBSCRIPTION_ID ARM_CLIENT_ID ARM_CLIENT_SECRET ARM_TENANT_ID ARM_ENVIRONMENT","title":"Terraform Config"},{"location":"kubernetes/distributions/aks/#create-storage-account-for-tf-state","text":"1 2 3 4 LOCATION = westeurope RESOURCE_GROUP_NAME = joostvdg-cb-ext-storage STORAGE_ACCOUNT_NAME = joostvdgcbtfstate CONTAINER_NAME = tfstate","title":"Create storage account for TF State"},{"location":"kubernetes/distributions/aks/#list-locations","text":"1 2 3 az account list-locations \\ --query [].{Region:name} \\ --out table","title":"List locations"},{"location":"kubernetes/distributions/aks/#create-resource-group","text":"1 2 3 az group create \\ --name ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION }","title":"Create resource group"},{"location":"kubernetes/distributions/aks/#create-storage-account","text":"1 2 3 4 5 6 az storage account create \\ --name ${ STORAGE_ACCOUNT_NAME } \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION } \\ --sku Standard_ZRS \\ --kind StorageV2","title":"Create storage account"},{"location":"kubernetes/distributions/aks/#retrieve-storage-account-login","text":"Apparently, no CLI commands available? Use the Azure Blog on AKS via Terraform for how via the UI. 1 STORAGE_ACCOUNT_KEY =","title":"Retrieve storage account login"},{"location":"kubernetes/distributions/aks/#create-tf-storage","text":"1 az storage container create -n ${ CONTAINER_NAME } --account-name ${ STORAGE_ACCOUNT_NAME } --account-key ${ STORAGE_ACCOUNT_KEY }","title":"Create TF Storage"},{"location":"kubernetes/distributions/aks/#init-terraform-backend","text":"1 2 3 4 terraform init -backend-config = storage_account_name= ${ STORAGE_ACCOUNT_NAME } \\ -backend-config = container_name= ${ CONTAINER_NAME } \\ -backend-config = access_key= ${ STORAGE_ACCOUNT_KEY } \\ -backend-config = key=codelab.microsoft.tfstate","title":"Init Terraform backend"},{"location":"kubernetes/distributions/aks/#expose-temp-variables","text":"These are from your Service Principle we created earlier. Where client_id = appId, and client_secret the password. 1 2 export TF_VAR_client_id = your-client-id export TF_VAR_client_secret = your-client-secret","title":"Expose temp variables"},{"location":"kubernetes/distributions/aks/#rollout","text":"","title":"Rollout"},{"location":"kubernetes/distributions/aks/#set-variables","text":"1 source ../export-variables.sh","title":"Set variables"},{"location":"kubernetes/distributions/aks/#validate","text":"1 terraform validate","title":"Validate"},{"location":"kubernetes/distributions/aks/#plan","text":"1 terraform plan -out out.plan","title":"Plan"},{"location":"kubernetes/distributions/aks/#apply-the-plan","text":"1 terraform apply out.plan","title":"Apply the plan"},{"location":"kubernetes/distributions/aks/#configure-kubecontext","text":"1 az aks get-credentials --resource-group cbcore --name cbcore","title":"Configure Kubecontext"},{"location":"kubernetes/distributions/aks/#configure-cluster-autoscaler","text":"1 az extension add --name aks-preview 1 2 3 4 az feature register --name VMSSPreview --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService/VMSSPreview )].{Name:name,State:properties.state} az provider register --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService )].{Name:name,State:properties.state}","title":"Configure Cluster Autoscaler"},{"location":"kubernetes/distributions/aks/#configure-multi-node-pool","text":"1 2 az feature register --name MultiAgentpoolPreview --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService/MultiAgentpoolPreview )].{Name:name,State:properties.state} 1 az provider register --namespace Microsoft.ContainerService","title":"Configure multi-node pool"},{"location":"kubernetes/distributions/aks/#create-aks-cluster-via-cli","text":"","title":"Create AKS cluster via CLI"},{"location":"kubernetes/distributions/aks/#resources_1","text":"https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/","title":"Resources"},{"location":"kubernetes/distributions/aks/#get-available-versions","text":"1 az aks get-versions --location westeurope","title":"Get available versions"},{"location":"kubernetes/distributions/aks/#create-initial-cluster","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Create a resource group in East US az group create --name myResourceGroup --location eastus # Create a basic single-node AKS cluster az aks create \\ --resource-group myResourceGroup \\ --name myAKSCluster \\ --enable-vmss \\ --node-count 1 \\ --nodepool-name masters \\ --node-vm-size Standard_DS2_v2 \\ --enable-cluster-autoscaler \\ --enable-vmss \\ --generate-ssh-keys","title":"Create initial cluster"},{"location":"kubernetes/distributions/aks/#podsecuritypolicy","text":"1 2 --enable-cluster-autoscaler : Enable cluster autoscaler, default value is false. If specified, please make sure the kubernetes version is larger than 1 .10.6.","title":"PodSecurityPolicy"},{"location":"kubernetes/distributions/aks/#networking","text":"1 2 3 4 5 6 7 --network-plugin : The Kubernetes network plugin to use. Specify azure for advanced networking configurations. Defaults to kubenet . --network-policy : ( PREVIEW ) The Kubernetes network policy to use. Using together with azure network plugin. Specify azure for Azure network policy manager and calico for calico network policy controller. Defaults to ( network policy disabled ) .","title":"Networking"},{"location":"kubernetes/distributions/aks/#retrieve-credentials","text":"1 az aks get-credentials --resource-group myResourceGroup --name myAKSCluster","title":"Retrieve credentials"},{"location":"kubernetes/distributions/aks/#add-second-node-pool","text":"1 2 3 4 5 az aks nodepool add \\ --resource-group myResourceGroup \\ --cluster-name myAKSCluster \\ --name mynodepool \\ --node-count 3","title":"Add second node pool"},{"location":"kubernetes/distributions/eks-eksctl/","text":"AWS EKS via eksctl EKS Access Configuration Some reference configuration, this is assuming you need temporary access tokens based on a assume role while having a MFA device configured. I seemed to have to create a new token every X minutes. If you don't run into this, ignore the configuration below and go straight to creating the cluster. EKS Keys Config 1 2 3 4 [ cloudbees-eks ] aws_access_key_id = ASI... aws_secret_access_key = NMAX... aws_session_token = FQoGZXIvYXdzEJr//////////wE..................... // one long ass token Generate Temporary Access Tokens With MFA 1 2 3 4 5 keys =( $( aws sts assume-role --profile default --role-arn arn:aws:iam:: ROLE_ARN :role/ ROLE_NAME \\ --role-session-name MyEKSCTLSession \\ --serial-number arn:aws:iam:: MFA_ARN :mfa/ USER \\ --token-code REPLACE_THIS_WITH_MFA_TOKEN \\ --query Credentials.[AccessKeyId,SecretAccessKey,SessionToken] --output text ) ) Cluster Create 1 2 3 4 5 EKS_CLUSTER_NAME = mycluster AWS_PROFILE = cloudbees-eks AWS_REGION = us-east-1 AWS_SSH_KEY_LOCATION = ~/.ssh/id_rsa.pub EKS_NUM_NODES = 4 1 2 3 4 5 6 7 8 9 10 11 eksctl create cluster \\ --asg-access \\ --auto-kubeconfig \\ --full-ecr-access \\ --name ${ EKS_CLUSTER_NAME } \\ --profile ${ AWS_PROFILE } \\ --region ${ AWS_REGION } \\ --set-kubeconfig-context \\ --ssh-public-key ${ AWS_SSH_KEY_LOCATION } \\ --nodes = ${ EKS_NUM_NODES } \\ --verbose 4 1 alias eks = kubectl --kubeconfig=~/.kube/eksctl/clusters/mycluster Encrypted Network With Weavenet If you want your network to be encrypted, you can use Weavenet. Warning The price of the encrypted network is high. So you're probably better off with a Network Policy. 1 2 WEAVENET_PASS = vjStsrzC4q7xDnb1wZkYacnk IPALLOC_RANGE = 10 .10.0.0/24 1 2 3 echo ${ WEAVENET_PASS } weave-passwd eks create secret -n kube-system generic weave-passwd --from-file = weave-passwd eks apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n ) password-secret=weave-passwd env.IPALLOC_RANGE= ${ IPALLOC_RANGE } Helm Tiller 1 alias helmks = helm --kubeconfig=~/.kube/eksctl/clusters/mycluster 1 2 3 eks create serviceaccount --namespace kube-system tiller eks create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller helmks init --service-account tiller --upgrade Nginx Nginx Ingress Docs, How to install on AWS CloudBees AWS Docs 1 2 3 4 eks apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml eks apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/aws/service-l4.yaml eks apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/aws/patch-configmap-l4.yaml eks patch service ingress-nginx -p { spec :{ externalTrafficPolicy : Local }} -n ingress-nginx Certmanager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 helmks install --name cert-manager --namespace default stable/cert-manager echo apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: letsencrypt-prd spec: acme: # The ACME server URL server: https://acme-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: yourname@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-prd # Enable the HTTP-01 challenge provider http01: {} cluster-issuer.yml eks apply -f cluster-issuer.yml Storage class Create a gp2 storage class and set it as default. 1 2 3 4 5 6 7 8 9 10 11 echo kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gp2 provisioner: kubernetes.io/aws-ebs parameters: type: gp2 encrypted: \\ true\\ gp2-storage.yaml eks patch storageclass gp2 -p { metadata : { annotations :{ storageclass.kubernetes.io/is-default-class : true }}} eks patch storageclass default -p { metadata : { annotations :{ storageclass.kubernetes.io/is-default-class : false }}} Confirm Confirm the storage class is create and set as default. 1 kubectl get sc Expected result. 1 2 NAME PROVISIONER AGE gp2 ( default ) kubernetes.io/aws-ebs 59","title":"AWS EKS"},{"location":"kubernetes/distributions/eks-eksctl/#aws-eks-via-eksctl","text":"","title":"AWS EKS via eksctl"},{"location":"kubernetes/distributions/eks-eksctl/#eks-access-configuration","text":"Some reference configuration, this is assuming you need temporary access tokens based on a assume role while having a MFA device configured. I seemed to have to create a new token every X minutes. If you don't run into this, ignore the configuration below and go straight to creating the cluster.","title":"EKS Access Configuration"},{"location":"kubernetes/distributions/eks-eksctl/#eks-keys-config","text":"1 2 3 4 [ cloudbees-eks ] aws_access_key_id = ASI... aws_secret_access_key = NMAX... aws_session_token = FQoGZXIvYXdzEJr//////////wE..................... // one long ass token","title":"EKS Keys Config"},{"location":"kubernetes/distributions/eks-eksctl/#generate-temporary-access-tokens-with-mfa","text":"1 2 3 4 5 keys =( $( aws sts assume-role --profile default --role-arn arn:aws:iam:: ROLE_ARN :role/ ROLE_NAME \\ --role-session-name MyEKSCTLSession \\ --serial-number arn:aws:iam:: MFA_ARN :mfa/ USER \\ --token-code REPLACE_THIS_WITH_MFA_TOKEN \\ --query Credentials.[AccessKeyId,SecretAccessKey,SessionToken] --output text ) )","title":"Generate Temporary Access Tokens With MFA"},{"location":"kubernetes/distributions/eks-eksctl/#cluster-create","text":"1 2 3 4 5 EKS_CLUSTER_NAME = mycluster AWS_PROFILE = cloudbees-eks AWS_REGION = us-east-1 AWS_SSH_KEY_LOCATION = ~/.ssh/id_rsa.pub EKS_NUM_NODES = 4 1 2 3 4 5 6 7 8 9 10 11 eksctl create cluster \\ --asg-access \\ --auto-kubeconfig \\ --full-ecr-access \\ --name ${ EKS_CLUSTER_NAME } \\ --profile ${ AWS_PROFILE } \\ --region ${ AWS_REGION } \\ --set-kubeconfig-context \\ --ssh-public-key ${ AWS_SSH_KEY_LOCATION } \\ --nodes = ${ EKS_NUM_NODES } \\ --verbose 4 1 alias eks = kubectl --kubeconfig=~/.kube/eksctl/clusters/mycluster","title":"Cluster Create"},{"location":"kubernetes/distributions/eks-eksctl/#encrypted-network-with-weavenet","text":"If you want your network to be encrypted, you can use Weavenet. Warning The price of the encrypted network is high. So you're probably better off with a Network Policy. 1 2 WEAVENET_PASS = vjStsrzC4q7xDnb1wZkYacnk IPALLOC_RANGE = 10 .10.0.0/24 1 2 3 echo ${ WEAVENET_PASS } weave-passwd eks create secret -n kube-system generic weave-passwd --from-file = weave-passwd eks apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n ) password-secret=weave-passwd env.IPALLOC_RANGE= ${ IPALLOC_RANGE }","title":"Encrypted Network With Weavenet"},{"location":"kubernetes/distributions/eks-eksctl/#helm-tiller","text":"1 alias helmks = helm --kubeconfig=~/.kube/eksctl/clusters/mycluster 1 2 3 eks create serviceaccount --namespace kube-system tiller eks create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller helmks init --service-account tiller --upgrade","title":"Helm &amp; Tiller"},{"location":"kubernetes/distributions/eks-eksctl/#nginx","text":"Nginx Ingress Docs, How to install on AWS CloudBees AWS Docs 1 2 3 4 eks apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml eks apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/aws/service-l4.yaml eks apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/aws/patch-configmap-l4.yaml eks patch service ingress-nginx -p { spec :{ externalTrafficPolicy : Local }} -n ingress-nginx","title":"Nginx"},{"location":"kubernetes/distributions/eks-eksctl/#certmanager","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 helmks install --name cert-manager --namespace default stable/cert-manager echo apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: letsencrypt-prd spec: acme: # The ACME server URL server: https://acme-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: yourname@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-prd # Enable the HTTP-01 challenge provider http01: {} cluster-issuer.yml eks apply -f cluster-issuer.yml","title":"Certmanager"},{"location":"kubernetes/distributions/eks-eksctl/#storage-class","text":"Create a gp2 storage class and set it as default. 1 2 3 4 5 6 7 8 9 10 11 echo kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gp2 provisioner: kubernetes.io/aws-ebs parameters: type: gp2 encrypted: \\ true\\ gp2-storage.yaml eks patch storageclass gp2 -p { metadata : { annotations :{ storageclass.kubernetes.io/is-default-class : true }}} eks patch storageclass default -p { metadata : { annotations :{ storageclass.kubernetes.io/is-default-class : false }}}","title":"Storage class"},{"location":"kubernetes/distributions/eks-eksctl/#confirm","text":"Confirm the storage class is create and set as default. 1 kubectl get sc Expected result. 1 2 NAME PROVISIONER AGE gp2 ( default ) kubernetes.io/aws-ebs 59","title":"Confirm"},{"location":"kubernetes/distributions/install-gke/","text":"GKE with Helm Env Variables 1 2 3 4 5 6 CLUSTER_NAME = MyGKECluster REGION = europe-west4 NODE_LOCATIONS = ${ REGION } -a, ${ REGION } -b ZONE = europe-west4-a K8S_VERSION = 1 .11.5-gke.4 PROJECT_ID = Get Kubernetes versions 1 gcloud container get-server-config --region $REGION Create Cluster 1 2 3 4 5 6 7 8 9 10 gcloud container clusters create ${ CLUSTER_NAME } \\ --region ${ REGION } --node-locations ${ NODE_LOCATIONS } \\ --cluster-version ${ K8S_VERSION } \\ --num-nodes 2 --machine-type n1-standard-2 \\ --addons = HorizontalPodAutoscaling \\ --min-nodes 2 --max-nodes 3 \\ --enable-autoupgrade \\ --enable-autoscaling \\ --enable-network-policy \\ --labels = purpose = practice Post Install 1 2 3 kubectl create clusterrolebinding cluster-admin-binding \\ --clusterrole cluster-admin \\ --user $( gcloud config get-value account ) Delete Cluster 1 gcloud container clusters delete $CLUSTER_NAME --region $REGION Configure kubeconfig 1 gcloud container clusters get-credentials ${ CLUSTER_NAME } --region ${ REGION } Install Cluster Tools Helm We use Helm as a package manager to more easily install other tools on Kubernetes. There's several repositories with a large number of mature charts - the name of the Helm packages. One being Helm/Stable another being Helm Hub . Create service account 1 kubectl create serviceaccount --namespace kube-system tiller Warning Tiller is deemed not safe for production, at least not in its default configuration. Either enable its TLS configuration and take other measures (such as namespace limitation) or use alternative solutions. Such as Kustomize, Pulumi, Jenkins X or raw Yaml. Create cluster role binding 1 kubectl create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller helm init 1 helm init --service-account tiller Test version 1 helm version Warning Currently, nginx ingress controller has an issue with Helm 2.14. So if you 2.14, either downgrade to 2.13.1 or install the Ingress Controller via an alternative solution (such as Kustomize). Ingress Controller 1 2 3 4 helm install --namespace ingress-nginx --name nginx-ingress stable/nginx-ingress \\ --set controller.service.externalTrafficPolicy = Local \\ --set controller.replicaCount = 3 \\ --set rbac.create = true Get LoadBalancer IP 1 2 export LB_IP = $( kubectl get svc -n ingress-nginx nginx-ingress-controller -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) echo $LB_IP Warning Now is the time to configure your DNS to use whatever LB_IP 's value is. Cert Manager Cert Manager is the recommended approach for managing TLS certificates in Kubernetes. If you do not want to manage certificates yourself, please use this. The certificates it uses are real and valid certificates, provided by Let's Encrypt . Install CRD's 1 kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8/deploy/manifests/00-crds.yaml Create Namespace 1 kubectl create namespace cert-manager Label namespace 1 kubectl label namespace cert-manager certmanager.k8s.io/disable-validation = true Add Helm Repo 1 2 helm repo add jetstack https://charts.jetstack.io helm repo update Install 1 2 3 4 5 helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.8.0 \\ jetstack/cert-manager","title":"GKE"},{"location":"kubernetes/distributions/install-gke/#gke-with-helm","text":"","title":"GKE with Helm"},{"location":"kubernetes/distributions/install-gke/#env-variables","text":"1 2 3 4 5 6 CLUSTER_NAME = MyGKECluster REGION = europe-west4 NODE_LOCATIONS = ${ REGION } -a, ${ REGION } -b ZONE = europe-west4-a K8S_VERSION = 1 .11.5-gke.4 PROJECT_ID =","title":"Env Variables"},{"location":"kubernetes/distributions/install-gke/#get-kubernetes-versions","text":"1 gcloud container get-server-config --region $REGION","title":"Get Kubernetes versions"},{"location":"kubernetes/distributions/install-gke/#create-cluster","text":"1 2 3 4 5 6 7 8 9 10 gcloud container clusters create ${ CLUSTER_NAME } \\ --region ${ REGION } --node-locations ${ NODE_LOCATIONS } \\ --cluster-version ${ K8S_VERSION } \\ --num-nodes 2 --machine-type n1-standard-2 \\ --addons = HorizontalPodAutoscaling \\ --min-nodes 2 --max-nodes 3 \\ --enable-autoupgrade \\ --enable-autoscaling \\ --enable-network-policy \\ --labels = purpose = practice","title":"Create Cluster"},{"location":"kubernetes/distributions/install-gke/#post-install","text":"1 2 3 kubectl create clusterrolebinding cluster-admin-binding \\ --clusterrole cluster-admin \\ --user $( gcloud config get-value account )","title":"Post Install"},{"location":"kubernetes/distributions/install-gke/#delete-cluster","text":"1 gcloud container clusters delete $CLUSTER_NAME --region $REGION","title":"Delete Cluster"},{"location":"kubernetes/distributions/install-gke/#configure-kubeconfig","text":"1 gcloud container clusters get-credentials ${ CLUSTER_NAME } --region ${ REGION }","title":"Configure kubeconfig"},{"location":"kubernetes/distributions/install-gke/#install-cluster-tools","text":"","title":"Install Cluster Tools"},{"location":"kubernetes/distributions/install-gke/#helm","text":"We use Helm as a package manager to more easily install other tools on Kubernetes. There's several repositories with a large number of mature charts - the name of the Helm packages. One being Helm/Stable another being Helm Hub .","title":"Helm"},{"location":"kubernetes/distributions/install-gke/#create-service-account","text":"1 kubectl create serviceaccount --namespace kube-system tiller Warning Tiller is deemed not safe for production, at least not in its default configuration. Either enable its TLS configuration and take other measures (such as namespace limitation) or use alternative solutions. Such as Kustomize, Pulumi, Jenkins X or raw Yaml.","title":"Create service account"},{"location":"kubernetes/distributions/install-gke/#create-cluster-role-binding","text":"1 kubectl create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller","title":"Create cluster role binding"},{"location":"kubernetes/distributions/install-gke/#helm-init","text":"1 helm init --service-account tiller","title":"helm init"},{"location":"kubernetes/distributions/install-gke/#test-version","text":"1 helm version Warning Currently, nginx ingress controller has an issue with Helm 2.14. So if you 2.14, either downgrade to 2.13.1 or install the Ingress Controller via an alternative solution (such as Kustomize).","title":"Test version"},{"location":"kubernetes/distributions/install-gke/#ingress-controller","text":"1 2 3 4 helm install --namespace ingress-nginx --name nginx-ingress stable/nginx-ingress \\ --set controller.service.externalTrafficPolicy = Local \\ --set controller.replicaCount = 3 \\ --set rbac.create = true","title":"Ingress Controller"},{"location":"kubernetes/distributions/install-gke/#get-loadbalancer-ip","text":"1 2 export LB_IP = $( kubectl get svc -n ingress-nginx nginx-ingress-controller -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) echo $LB_IP Warning Now is the time to configure your DNS to use whatever LB_IP 's value is.","title":"Get LoadBalancer IP"},{"location":"kubernetes/distributions/install-gke/#cert-manager","text":"Cert Manager is the recommended approach for managing TLS certificates in Kubernetes. If you do not want to manage certificates yourself, please use this. The certificates it uses are real and valid certificates, provided by Let's Encrypt .","title":"Cert Manager"},{"location":"kubernetes/distributions/install-gke/#install-crds","text":"1 kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8/deploy/manifests/00-crds.yaml","title":"Install CRD's"},{"location":"kubernetes/distributions/install-gke/#create-namespace","text":"1 kubectl create namespace cert-manager","title":"Create Namespace"},{"location":"kubernetes/distributions/install-gke/#label-namespace","text":"1 kubectl label namespace cert-manager certmanager.k8s.io/disable-validation = true","title":"Label namespace"},{"location":"kubernetes/distributions/install-gke/#add-helm-repo","text":"1 2 helm repo add jetstack https://charts.jetstack.io helm repo update","title":"Add Helm Repo"},{"location":"kubernetes/distributions/install-gke/#install","text":"1 2 3 4 5 helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.8.0 \\ jetstack/cert-manager","title":"Install"},{"location":"kubernetes/khw-gce/","text":"Kubernetes the Hard Way - GCE This assumes OSX and GCE. Goal The goal is to setup up HA Kubernetes cluster on GCE from it's most basic parts. That means we will install and configure the basic components ourselves, such as the API server and Kubelets. Setup As to limit the scope to doing the setup of the Kubernetes cluster ourselves, we will make it static. That means we will create and configure the network and compute resources to be fit for 3 Control Plane VM's and 3 worker VM's. We will not be able to recover a failing node or accomidate additional resources. Resources in GCE Public IP address, as front-end for the three API servers 3 VM's for the Control Plance 3 VM's as workers VPC Network Routes: from POD CIDR blocks to the host VM (for workers) Firewall configuration: allow health checks, dns, internal communication and connection to API server Kubernetes Resources Control Plane etcd : stores cluster state kube-api server : entry point for interacting with the cluster by exposing the api kube-scheduler : makes sure pods get scheduled kube-controller-manager : aggregate of required controllers Node Controller : Responsible for noticing and responding when nodes go down. Replication Controller : Responsible for maintaining the correct number of pods for every replication controller object in the system. Endpoints Controller : Populates the Endpoints object (that is, joins Services Pods). Service Account Token Controller : Create default accounts and API access tokens for new namespaces. Worker nodes kubelet : An agent that runs on each node in the cluster. It makes sure that containers are running in a pod. kube-proxy : kube-proxy enables the Kubernetes service abstraction by maintaining network rules on the host and performing connection forwarding A container runtime: this can be Docker , rkt or as in our case containerd Network https://blog.csnet.me/k8s-thw/part1/ https://github.com/kelseyhightower/kubernetes-the-hard-way We will be using the network components - with Weave-Net and CoreDNS - as described in the csnet blog. But we will use the CIDR blocks as stated in the Kelsey Hightower's Kubernetes the Hard Way ( KHW ). Kelsey's KHW Range Use 10.240.0.10/24 LAN (GCE VMS) 10.200.0.0/16 k8s Pod network 10.32.0.0/24 k8s Service network 10.32.0.1 k8s API server 10.32.0.10 k8s dns API Server: https://127.0.0.1:6443 service-cluster-ip-range=10.32.0.0/24 cluster-cidr=10.200.0.0/1 CSNETs Range Use 10.32.2.0/24 LAN (csnet.me) 10.16.0.0/16 k8s Pod network 10.10.0.0/22 k8s Service network 10.10.0.1 k8s API server 10.10.0.10 k8s dns API Server: https://10.32.2.97:6443 service-cluster-ip-range=10.10.0.0/22 cluster-cidr=10.16.0.0/16 Install tools On the machine doing the installation, we will need some tools installed. We will use the following tools: kubectl : for communicating with the API server cfssl : for creating the certificates and sign them helm : for installing additional tools later stern : for viewing logs of multiple pods at once (for example, all kube-dns pods) terraform : for managing our resources in GCE 1 2 3 4 5 brew install kubernetes-cli brew install cfssl brew install kubernetes-helm brew install stern brew install terraform Check versions 1 2 3 4 5 kubectl version -c -o yaml cfssl version helm version -c --short stern --version terraform version Terraform remote storage The help with problems of local storage and potential loss of data when local OS problems occur, we will use an S3 bucket as Terraform state storage. create s3 bucket configure Terraform to use this as remote state storage see how to this here read more about this, in Terraform's docs 1 2 3 export AWS_ACCESS_KEY_ID = anaccesskey export AWS_SECRET_ACCESS_KEY = asecretkey export AWS_DEFAULT_REGION = eu-central-1 1 2 3 4 5 6 7 8 terraform { backend s3 { bucket = euros-terraform-state key = terraform.tfstate region = eu-central-1 encrypt = true } } GKE Service Account Create a new GKE service account, and export it's json credentials file for use with Terraform. See GKE Tutorial page for how you can do this.","title":"Installer Preparation"},{"location":"kubernetes/khw-gce/#kubernetes-the-hard-way-gce","text":"This assumes OSX and GCE.","title":"Kubernetes the Hard Way - GCE"},{"location":"kubernetes/khw-gce/#goal","text":"The goal is to setup up HA Kubernetes cluster on GCE from it's most basic parts. That means we will install and configure the basic components ourselves, such as the API server and Kubelets.","title":"Goal"},{"location":"kubernetes/khw-gce/#setup","text":"As to limit the scope to doing the setup of the Kubernetes cluster ourselves, we will make it static. That means we will create and configure the network and compute resources to be fit for 3 Control Plane VM's and 3 worker VM's. We will not be able to recover a failing node or accomidate additional resources.","title":"Setup"},{"location":"kubernetes/khw-gce/#resources-in-gce","text":"Public IP address, as front-end for the three API servers 3 VM's for the Control Plance 3 VM's as workers VPC Network Routes: from POD CIDR blocks to the host VM (for workers) Firewall configuration: allow health checks, dns, internal communication and connection to API server","title":"Resources in GCE"},{"location":"kubernetes/khw-gce/#kubernetes-resources","text":"","title":"Kubernetes Resources"},{"location":"kubernetes/khw-gce/#control-plane","text":"etcd : stores cluster state kube-api server : entry point for interacting with the cluster by exposing the api kube-scheduler : makes sure pods get scheduled kube-controller-manager : aggregate of required controllers Node Controller : Responsible for noticing and responding when nodes go down. Replication Controller : Responsible for maintaining the correct number of pods for every replication controller object in the system. Endpoints Controller : Populates the Endpoints object (that is, joins Services Pods). Service Account Token Controller : Create default accounts and API access tokens for new namespaces.","title":"Control Plane"},{"location":"kubernetes/khw-gce/#worker-nodes","text":"kubelet : An agent that runs on each node in the cluster. It makes sure that containers are running in a pod. kube-proxy : kube-proxy enables the Kubernetes service abstraction by maintaining network rules on the host and performing connection forwarding A container runtime: this can be Docker , rkt or as in our case containerd","title":"Worker nodes"},{"location":"kubernetes/khw-gce/#network","text":"https://blog.csnet.me/k8s-thw/part1/ https://github.com/kelseyhightower/kubernetes-the-hard-way We will be using the network components - with Weave-Net and CoreDNS - as described in the csnet blog. But we will use the CIDR blocks as stated in the Kelsey Hightower's Kubernetes the Hard Way ( KHW ).","title":"Network"},{"location":"kubernetes/khw-gce/#kelseys-khw","text":"Range Use 10.240.0.10/24 LAN (GCE VMS) 10.200.0.0/16 k8s Pod network 10.32.0.0/24 k8s Service network 10.32.0.1 k8s API server 10.32.0.10 k8s dns API Server: https://127.0.0.1:6443 service-cluster-ip-range=10.32.0.0/24 cluster-cidr=10.200.0.0/1","title":"Kelsey's KHW"},{"location":"kubernetes/khw-gce/#csnets","text":"Range Use 10.32.2.0/24 LAN (csnet.me) 10.16.0.0/16 k8s Pod network 10.10.0.0/22 k8s Service network 10.10.0.1 k8s API server 10.10.0.10 k8s dns API Server: https://10.32.2.97:6443 service-cluster-ip-range=10.10.0.0/22 cluster-cidr=10.16.0.0/16","title":"CSNETs"},{"location":"kubernetes/khw-gce/#install-tools","text":"On the machine doing the installation, we will need some tools installed. We will use the following tools: kubectl : for communicating with the API server cfssl : for creating the certificates and sign them helm : for installing additional tools later stern : for viewing logs of multiple pods at once (for example, all kube-dns pods) terraform : for managing our resources in GCE 1 2 3 4 5 brew install kubernetes-cli brew install cfssl brew install kubernetes-helm brew install stern brew install terraform","title":"Install tools"},{"location":"kubernetes/khw-gce/#check-versions","text":"1 2 3 4 5 kubectl version -c -o yaml cfssl version helm version -c --short stern --version terraform version","title":"Check versions"},{"location":"kubernetes/khw-gce/#terraform-remote-storage","text":"The help with problems of local storage and potential loss of data when local OS problems occur, we will use an S3 bucket as Terraform state storage. create s3 bucket configure Terraform to use this as remote state storage see how to this here read more about this, in Terraform's docs 1 2 3 export AWS_ACCESS_KEY_ID = anaccesskey export AWS_SECRET_ACCESS_KEY = asecretkey export AWS_DEFAULT_REGION = eu-central-1 1 2 3 4 5 6 7 8 terraform { backend s3 { bucket = euros-terraform-state key = terraform.tfstate region = eu-central-1 encrypt = true } }","title":"Terraform remote storage"},{"location":"kubernetes/khw-gce/#gke-service-account","text":"Create a new GKE service account, and export it's json credentials file for use with Terraform. See GKE Tutorial page for how you can do this.","title":"GKE Service Account"},{"location":"kubernetes/khw-gce/certificates/","text":"Certificates Note Before we can continue here, we need to have our nodes up and running with their external ip addresses and our fixed public ip address. This is because some certificates require these external ip addresses! 1 2 gcloud compute instances list gcloud compute addresses list --filter = name=( kubernetes-the-hard-way ) We need to create a whole lot of certificates, listed below, with the help of cfssl . A tool from CDN provider CloudFlare. Required certificates CA (or Certificate Authority): will be the root certificate of our trust chain result: ca . pem ca - key . pem Admin : the admin of our cluster (you!) result: admin - key . pem admin . pem Kubelet : the certificates of the kubelet processes on the worker nodes result: 1 2 3 4 5 worker - 0 . pem worker - 1 - key . pem worker - 1 . pem worker - 2 - key . pem worker - 2 . pem Controller Manager result: kube - controller - manager - key . pem kube - controller - manager . pem Scheduler result: kube - scheduler - key . pem kube - scheduler . pem API Server result kubernetes - key . pem kubernetes . pem Service Account : ??? result: service - account - key . pem service - account . pem Certificate example Because we will use the cfssl tool from CloudFlare, we will define our certificate signing request (CSR's) in json. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { CN : service-accounts , key : { algo : rsa , size : 2048 }, names : [ { C : NL , L : Utrecht , O : Kubernetes , OU : Kubernetes The Hard Way , ST : Utrecht } ] } Install scripts Make sure you're in k8s - the - hard - way / scripts 1 ./certs.sh","title":"Prepare Certificates"},{"location":"kubernetes/khw-gce/certificates/#certificates","text":"Note Before we can continue here, we need to have our nodes up and running with their external ip addresses and our fixed public ip address. This is because some certificates require these external ip addresses! 1 2 gcloud compute instances list gcloud compute addresses list --filter = name=( kubernetes-the-hard-way ) We need to create a whole lot of certificates, listed below, with the help of cfssl . A tool from CDN provider CloudFlare.","title":"Certificates"},{"location":"kubernetes/khw-gce/certificates/#required-certificates","text":"CA (or Certificate Authority): will be the root certificate of our trust chain result: ca . pem ca - key . pem Admin : the admin of our cluster (you!) result: admin - key . pem admin . pem Kubelet : the certificates of the kubelet processes on the worker nodes result: 1 2 3 4 5 worker - 0 . pem worker - 1 - key . pem worker - 1 . pem worker - 2 - key . pem worker - 2 . pem Controller Manager result: kube - controller - manager - key . pem kube - controller - manager . pem Scheduler result: kube - scheduler - key . pem kube - scheduler . pem API Server result kubernetes - key . pem kubernetes . pem Service Account : ??? result: service - account - key . pem service - account . pem","title":"Required certificates"},{"location":"kubernetes/khw-gce/certificates/#certificate-example","text":"Because we will use the cfssl tool from CloudFlare, we will define our certificate signing request (CSR's) in json. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { CN : service-accounts , key : { algo : rsa , size : 2048 }, names : [ { C : NL , L : Utrecht , O : Kubernetes , OU : Kubernetes The Hard Way , ST : Utrecht } ] }","title":"Certificate example"},{"location":"kubernetes/khw-gce/certificates/#install-scripts","text":"Make sure you're in k8s - the - hard - way / scripts 1 ./certs.sh","title":"Install scripts"},{"location":"kubernetes/khw-gce/controller/","text":"Controller Config We have to configure the following: move certificates to the correct location move encryption configuration to / var / lib / kubernetes download and install binaries kubectl kube-apiserver kube-scheduler kube-controller-manager configure API server systemd service configure Controller Manager systemd service configure Scheduler systemd service kubernetes configuration yaml kind : KubeSchedulerConfiguration create nginx reverse proxy to enable GCE's health checks to reach each API Server instance configure RBAC configuration in the API server via ClusterRole and ClusterRoleBinding Install We have an installer script, controller - local . sh , which should be executed on each controller VM. To do so, use the controller . sh script to upload this file to the VM's. 1 ./controller.sh","title":"Controller Config"},{"location":"kubernetes/khw-gce/controller/#controller-config","text":"We have to configure the following: move certificates to the correct location move encryption configuration to / var / lib / kubernetes download and install binaries kubectl kube-apiserver kube-scheduler kube-controller-manager configure API server systemd service configure Controller Manager systemd service configure Scheduler systemd service kubernetes configuration yaml kind : KubeSchedulerConfiguration create nginx reverse proxy to enable GCE's health checks to reach each API Server instance configure RBAC configuration in the API server via ClusterRole and ClusterRoleBinding","title":"Controller Config"},{"location":"kubernetes/khw-gce/controller/#install","text":"We have an installer script, controller - local . sh , which should be executed on each controller VM. To do so, use the controller . sh script to upload this file to the VM's. 1 ./controller.sh","title":"Install"},{"location":"kubernetes/khw-gce/debug/","text":"Debug Kubernetes components not healthy Check for healthy status On a control plane node, check etcd . 1 2 3 4 5 sudo ETCDCTL_API = 3 etcdctl member list \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem 1 2 3 3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379 f98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379 ffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379 On a control plan node, check control plane components. 1 kubectl get componentstatuses --kubeconfig admin.kubeconfig Should look like this: 1 2 3 4 5 6 NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy { health : true } etcd-0 Healthy { health : true } etcd-1 Healthy { health : true } On a control plane node, check API server status (via nginx reverse proxy). 1 curl -H Host: kubernetes.default.svc.cluster.local -i http://127.0.0.1/healthz 1 2 3 4 5 6 7 8 HTTP/1.1 200 OK Server: nginx/1.14.0 ( Ubuntu ) Date: Mon, 14 May 2018 13 :45:39 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 2 Connection: keep-alive ok On an external system, you can check if the API server is working and reachable via routing. 1 curl --cacert ca.pem https:// ${ KUBERNETES_PUBLIC_ADDRESS } :6443/version Assuming that GCE is used. 1 2 3 KUBERNETES_PUBLIC_ADDRESS = $( gcloud compute addresses describe kubernetes-the-hard-way \\ --region $( gcloud config get-value compute/region ) \\ --format value(address) ) Check for errors 1 journalctl Or for specific components. 1 journalctl -u kube-scheduler Weave-Net pods Blocked Sometimes when installing weave-net as the CNI plugin, the pods are blocked. 1 2 3 4 NAME READY STATUS RESTARTS AGE weave-net-fwvsr 0 /2 Blocked 0 3m weave-net-v9z9n 0 /2 Blocked 0 3m weave-net-zfghq 0 /2 Blocked 0 3m Usually this means something went wrong with the CNI configuration. Ideally, Weave-Net will generate this when installed, but sometimes this doesn't happen. This is easily found when checking the journalctl on the worker nodes ( journalctl - u kubelet ). There are three things to be done before installing weave-net again. Ensure ip4 forwarding is enabled 1 2 sysctl net.ipv4.ip_forward = 1 sysctl -p /etc/sysctl.conf See Kubernetes Docs for GCE routing or Michael Champagne 's blog on KHW. Ensure all weave-net resources are gone I've noticed that when this problem occurs, deleting the weave-net resources with kubectl delete - f weaveNet resource leaves the pods. The pods are terminated (they never started) but are not removed. To remove them, use the line below, as explained on stackoverflow . 1 kubectl delete pod NAME --grace-period = 0 --force Restart Kubelet I'm not sure if this is 100% required, but I've had better luck with restarting the kubelet before reinstalling weave-net. So, login to each worker node, gcloud compute ssh worker -? and issue the following commands. 1 2 sudo systemctl daemon-reload sudo systemctl restart kubelet DNS on GCE not working It seemed something has changed in GCE after Kelsey Hightower's Kubernetes The Hardway was written/updated. This means that if you follow through the documentation, you will run into this: 1 2 3 4 kubectl exec -ti $POD_NAME -- nslookup kubernetes ;; connection timed out ; no servers could be reached command terminated with exit code 1 The cure seems to be to add additional resolve . conf file configuration to the kubelet's systemd service definition. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cat EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description = Kubernetes Kubelet Documentation = https://github.com/kubernetes/kubernetes After = containerd.service Requires = containerd.service [Service] ExecStart = /usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --container-runtime=remote \\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\ --resolv-conf=/run/systemd/resolve/resolv.conf \\ --image-pull-progress-deadline=2m \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --network-plugin=cni \\ --register-node=true \\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF In addition, one should also use at least busybox 1.28 to do the dns check. For more information, read this issue .","title":"Debug"},{"location":"kubernetes/khw-gce/debug/#debug","text":"","title":"Debug"},{"location":"kubernetes/khw-gce/debug/#kubernetes-components-not-healthy","text":"","title":"Kubernetes components not healthy"},{"location":"kubernetes/khw-gce/debug/#check-for-healthy-status","text":"On a control plane node, check etcd . 1 2 3 4 5 sudo ETCDCTL_API = 3 etcdctl member list \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem 1 2 3 3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379 f98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379 ffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379 On a control plan node, check control plane components. 1 kubectl get componentstatuses --kubeconfig admin.kubeconfig Should look like this: 1 2 3 4 5 6 NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy { health : true } etcd-0 Healthy { health : true } etcd-1 Healthy { health : true } On a control plane node, check API server status (via nginx reverse proxy). 1 curl -H Host: kubernetes.default.svc.cluster.local -i http://127.0.0.1/healthz 1 2 3 4 5 6 7 8 HTTP/1.1 200 OK Server: nginx/1.14.0 ( Ubuntu ) Date: Mon, 14 May 2018 13 :45:39 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 2 Connection: keep-alive ok On an external system, you can check if the API server is working and reachable via routing. 1 curl --cacert ca.pem https:// ${ KUBERNETES_PUBLIC_ADDRESS } :6443/version Assuming that GCE is used. 1 2 3 KUBERNETES_PUBLIC_ADDRESS = $( gcloud compute addresses describe kubernetes-the-hard-way \\ --region $( gcloud config get-value compute/region ) \\ --format value(address) )","title":"Check for healthy status"},{"location":"kubernetes/khw-gce/debug/#check-for-errors","text":"1 journalctl Or for specific components. 1 journalctl -u kube-scheduler","title":"Check for errors"},{"location":"kubernetes/khw-gce/debug/#weave-net-pods-blocked","text":"Sometimes when installing weave-net as the CNI plugin, the pods are blocked. 1 2 3 4 NAME READY STATUS RESTARTS AGE weave-net-fwvsr 0 /2 Blocked 0 3m weave-net-v9z9n 0 /2 Blocked 0 3m weave-net-zfghq 0 /2 Blocked 0 3m Usually this means something went wrong with the CNI configuration. Ideally, Weave-Net will generate this when installed, but sometimes this doesn't happen. This is easily found when checking the journalctl on the worker nodes ( journalctl - u kubelet ). There are three things to be done before installing weave-net again.","title":"Weave-Net pods Blocked"},{"location":"kubernetes/khw-gce/debug/#ensure-ip4-forwarding-is-enabled","text":"1 2 sysctl net.ipv4.ip_forward = 1 sysctl -p /etc/sysctl.conf See Kubernetes Docs for GCE routing or Michael Champagne 's blog on KHW.","title":"Ensure ip4 forwarding is enabled"},{"location":"kubernetes/khw-gce/debug/#ensure-all-weave-net-resources-are-gone","text":"I've noticed that when this problem occurs, deleting the weave-net resources with kubectl delete - f weaveNet resource leaves the pods. The pods are terminated (they never started) but are not removed. To remove them, use the line below, as explained on stackoverflow . 1 kubectl delete pod NAME --grace-period = 0 --force","title":"Ensure all weave-net resources are gone"},{"location":"kubernetes/khw-gce/debug/#restart-kubelet","text":"I'm not sure if this is 100% required, but I've had better luck with restarting the kubelet before reinstalling weave-net. So, login to each worker node, gcloud compute ssh worker -? and issue the following commands. 1 2 sudo systemctl daemon-reload sudo systemctl restart kubelet","title":"Restart Kubelet"},{"location":"kubernetes/khw-gce/debug/#dns-on-gce-not-working","text":"It seemed something has changed in GCE after Kelsey Hightower's Kubernetes The Hardway was written/updated. This means that if you follow through the documentation, you will run into this: 1 2 3 4 kubectl exec -ti $POD_NAME -- nslookup kubernetes ;; connection timed out ; no servers could be reached command terminated with exit code 1 The cure seems to be to add additional resolve . conf file configuration to the kubelet's systemd service definition. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cat EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description = Kubernetes Kubelet Documentation = https://github.com/kubernetes/kubernetes After = containerd.service Requires = containerd.service [Service] ExecStart = /usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --container-runtime=remote \\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\ --resolv-conf=/run/systemd/resolve/resolv.conf \\ --image-pull-progress-deadline=2m \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --network-plugin=cni \\ --register-node=true \\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF In addition, one should also use at least busybox 1.28 to do the dns check. For more information, read this issue .","title":"DNS on GCE not working"},{"location":"kubernetes/khw-gce/encryption/","text":"Encryption Kubernetes stores a variety of data including cluster state, application configurations, and secrets. Kubernetes supports the ability to encrypt cluster data at rest. In order to use this ability to encrypt data at rest, each member of the control plane has to know the encryption key. So we will have to create one. Encryption configuration We have to create a encryption key first. For the sake of embedding it into a yaml file, we will have to encode it to base64 . 1 ENCRYPTION_KEY = $( head -c 32 /dev/urandom | base64 ) 1 Install scripts Make sure you're in k8s - the - hard - way / scripts 1 ./encryption.sh","title":"Encryption configuration"},{"location":"kubernetes/khw-gce/encryption/#encryption","text":"Kubernetes stores a variety of data including cluster state, application configurations, and secrets. Kubernetes supports the ability to encrypt cluster data at rest. In order to use this ability to encrypt data at rest, each member of the control plane has to know the encryption key. So we will have to create one.","title":"Encryption"},{"location":"kubernetes/khw-gce/encryption/#encryption-configuration","text":"We have to create a encryption key first. For the sake of embedding it into a yaml file, we will have to encode it to base64 . 1 ENCRYPTION_KEY = $( head -c 32 /dev/urandom | base64 ) 1","title":"Encryption configuration"},{"location":"kubernetes/khw-gce/encryption/#install-scripts","text":"Make sure you're in k8s - the - hard - way / scripts 1 ./encryption.sh","title":"Install scripts"},{"location":"kubernetes/khw-gce/etcd/","text":"ETCD Kubernetes components are stateless and store cluster state in etcd. In this lab you will bootstrap a three node etcd cluster and configure it for high availability and secure remote access. The bare minimum is to have a single etcd instance running. But for production purposes it is best to run etcd in HA mode. This means we need to have three instances running that know eachother. Again, this is not a production ready setup, as the static nature prevents automatic recovery if a node fails. Steps to take download install etcd binary prepare required certificates create systemd service definition reload systemd configuration, enable start the service Install script Make sure that the local install script is on every server, you can use the etcd . sh script for this. Then, make sure you're connect to all three controller VM's at the same time, for example via tmux or iterm. For iterm: use ctrl + shift + d to open three horizontal windows use ctrl + shift + i to write output to all three windows at once login to each controller gcloud compute ssh controller -? . / etcd - local . sh Verification 1 2 3 4 5 sudo ETCDCTL_API = 3 etcdctl member list \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem Expected Output 1 2 3 3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379 f98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379 ffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379","title":"ETCD configuration"},{"location":"kubernetes/khw-gce/etcd/#etcd","text":"Kubernetes components are stateless and store cluster state in etcd. In this lab you will bootstrap a three node etcd cluster and configure it for high availability and secure remote access. The bare minimum is to have a single etcd instance running. But for production purposes it is best to run etcd in HA mode. This means we need to have three instances running that know eachother. Again, this is not a production ready setup, as the static nature prevents automatic recovery if a node fails.","title":"ETCD"},{"location":"kubernetes/khw-gce/etcd/#steps-to-take","text":"download install etcd binary prepare required certificates create systemd service definition reload systemd configuration, enable start the service","title":"Steps to take"},{"location":"kubernetes/khw-gce/etcd/#install-script","text":"Make sure that the local install script is on every server, you can use the etcd . sh script for this. Then, make sure you're connect to all three controller VM's at the same time, for example via tmux or iterm. For iterm: use ctrl + shift + d to open three horizontal windows use ctrl + shift + i to write output to all three windows at once login to each controller gcloud compute ssh controller -? . / etcd - local . sh","title":"Install script"},{"location":"kubernetes/khw-gce/etcd/#verification","text":"1 2 3 4 5 sudo ETCDCTL_API = 3 etcdctl member list \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem","title":"Verification"},{"location":"kubernetes/khw-gce/etcd/#expected-output","text":"1 2 3 3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379 f98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379 ffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379","title":"Expected Output"},{"location":"kubernetes/khw-gce/kubeconfigs/","text":"Kubeconfigs Now that we have certificates we have to make sure we have configurations that the Kubernetes parts can actually use - certificates themselves are not enough. This is where we will use kubernetes configuration files, or kubeconfigs . We will have to create the following kubeconfigs : controller manager kubelet kube-proxy kube-scheduler admin user Create Test kubeconfig file Here's an example script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate = kube-controller-manager.pem \\ --client-key = kube-controller-manager-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-controller-manager \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig = kube-controller-manager.kubeconfig The steps we execute in order are the following: create a kubeconfig entry for our kubernetes - the - hard - way cluster and export this into a .kubeconfig file add credentials to this config file, in the form of our kubernetes component's certificate set the default config of this config file to namespace default and user to the component we're configuring test the configuration file by using it Install scripts Make sure you're in k8s - the - hard - way / scripts 1 ./kube-configs.sh","title":"Prepare Kubeconfigs"},{"location":"kubernetes/khw-gce/kubeconfigs/#kubeconfigs","text":"Now that we have certificates we have to make sure we have configurations that the Kubernetes parts can actually use - certificates themselves are not enough. This is where we will use kubernetes configuration files, or kubeconfigs . We will have to create the following kubeconfigs : controller manager kubelet kube-proxy kube-scheduler admin user","title":"Kubeconfigs"},{"location":"kubernetes/khw-gce/kubeconfigs/#create-test-kubeconfig-file","text":"Here's an example script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate = kube-controller-manager.pem \\ --client-key = kube-controller-manager-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-controller-manager \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig = kube-controller-manager.kubeconfig The steps we execute in order are the following: create a kubeconfig entry for our kubernetes - the - hard - way cluster and export this into a .kubeconfig file add credentials to this config file, in the form of our kubernetes component's certificate set the default config of this config file to namespace default and user to the component we're configuring test the configuration file by using it","title":"Create &amp; Test kubeconfig file"},{"location":"kubernetes/khw-gce/kubeconfigs/#install-scripts","text":"Make sure you're in k8s - the - hard - way / scripts 1 ./kube-configs.sh","title":"Install scripts"},{"location":"kubernetes/khw-gce/network/","text":"Networking First, configure external access so we can run kubectl commands from our own machine. Confirm the you can now call the following: 1 kubectl get nodes -o wide Configure WeaveNet 1 kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n ) env.IPALLOC_RANGE=10.200.0.0/16 Confirm WeaveNet works 1 kubectl get pod --namespace = kube-system -l name = weave-net It should look like this: 1 2 3 4 NAME READY STATUS RESTARTS AGE weave-net-fwvsr 2 /2 Running 1 4h weave-net-v9z9n 2 /2 Running 1 4h weave-net-zfghq 2 /2 Running 1 4h Configure CoreDNS Before installing CoreDNS , please confirm networking is in order. 1 kubectl get nodes -o wide Warning If nodes are not Ready , something is wrong and needs to be fixed before you continue. 1 kubectl apply -f ../configs/core-dns-config.yaml Confirm CoreDNS pods 1 kubectl get pod --all-namespaces -l k8s-app = coredns -o wide Confirm DNS works 1 kubectl run busybox --image = busybox:1.28 --command -- sleep 3600 1 POD_NAME = $( kubectl get pods -l run = busybox -o jsonpath = {.items[0].metadata.name} ) 1 kubectl exec -ti $POD_NAME -- nslookup kubernetes Note It should look like this: 1 2 3 4 5 Server: 10 .10.0.10 Address 1 : 10 .10.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1 : 10 .10.0.1 kubernetes.default.svc.cluster.local","title":"Network config"},{"location":"kubernetes/khw-gce/network/#networking","text":"First, configure external access so we can run kubectl commands from our own machine. Confirm the you can now call the following: 1 kubectl get nodes -o wide","title":"Networking"},{"location":"kubernetes/khw-gce/network/#configure-weavenet","text":"1 kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n ) env.IPALLOC_RANGE=10.200.0.0/16","title":"Configure WeaveNet"},{"location":"kubernetes/khw-gce/network/#confirm-weavenet-works","text":"1 kubectl get pod --namespace = kube-system -l name = weave-net It should look like this: 1 2 3 4 NAME READY STATUS RESTARTS AGE weave-net-fwvsr 2 /2 Running 1 4h weave-net-v9z9n 2 /2 Running 1 4h weave-net-zfghq 2 /2 Running 1 4h","title":"Confirm WeaveNet works"},{"location":"kubernetes/khw-gce/network/#configure-coredns","text":"Before installing CoreDNS , please confirm networking is in order. 1 kubectl get nodes -o wide Warning If nodes are not Ready , something is wrong and needs to be fixed before you continue. 1 kubectl apply -f ../configs/core-dns-config.yaml","title":"Configure CoreDNS"},{"location":"kubernetes/khw-gce/network/#confirm-coredns-pods","text":"1 kubectl get pod --all-namespaces -l k8s-app = coredns -o wide","title":"Confirm CoreDNS pods"},{"location":"kubernetes/khw-gce/network/#confirm-dns-works","text":"1 kubectl run busybox --image = busybox:1.28 --command -- sleep 3600 1 POD_NAME = $( kubectl get pods -l run = busybox -o jsonpath = {.items[0].metadata.name} ) 1 kubectl exec -ti $POD_NAME -- nslookup kubernetes Note It should look like this: 1 2 3 4 5 Server: 10 .10.0.10 Address 1 : 10 .10.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1 : 10 .10.0.1 kubernetes.default.svc.cluster.local","title":"Confirm DNS works"},{"location":"kubernetes/khw-gce/remote-access/","text":"Remote Access 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 KUBERNETES_PUBLIC_ADDRESS = $( gcloud compute addresses describe kubernetes-the-hard-way \\ --region $( gcloud config get-value compute/region ) \\ --format value(address) ) echo KUBERNETES_PUBLIC_ADDRESS= ${ KUBERNETES_PUBLIC_ADDRESS } kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https:// ${ KUBERNETES_PUBLIC_ADDRESS } :6443 kubectl config set-credentials admin \\ --client-certificate = admin.pem \\ --client-key = admin-key.pem kubectl config set-context kubernetes-the-hard-way \\ --cluster = kubernetes-the-hard-way \\ --user = admin kubectl config use-context kubernetes-the-hard-way Confirm 1 kubectl get nodes -o wide","title":"Remote access"},{"location":"kubernetes/khw-gce/remote-access/#remote-access","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 KUBERNETES_PUBLIC_ADDRESS = $( gcloud compute addresses describe kubernetes-the-hard-way \\ --region $( gcloud config get-value compute/region ) \\ --format value(address) ) echo KUBERNETES_PUBLIC_ADDRESS= ${ KUBERNETES_PUBLIC_ADDRESS } kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https:// ${ KUBERNETES_PUBLIC_ADDRESS } :6443 kubectl config set-credentials admin \\ --client-certificate = admin.pem \\ --client-key = admin-key.pem kubectl config set-context kubernetes-the-hard-way \\ --cluster = kubernetes-the-hard-way \\ --user = admin kubectl config use-context kubernetes-the-hard-way","title":"Remote Access"},{"location":"kubernetes/khw-gce/remote-access/#confirm","text":"1 kubectl get nodes -o wide","title":"Confirm"},{"location":"kubernetes/khw-gce/terraform-compute/","text":"Compute resources Create network VPC with Firewall rules 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 provider google { credentials = ${file( ${var.credentials_file_path} )} project = ${var.project_name} region = ${var.region} } resource google_compute_network khw { name = kubernetes-the-hard-way auto_create_subnetworks = false } resource google_compute_subnetwork khw-kubernetes { name = kubernetes ip_cidr_range = 10.240.0.0/24 region = ${var.region} network = ${google_compute_network.khw.self_link} } resource google_compute_firewall khw-allow-internal { name = kubernetes-the-hard-way-allow-internal network = ${google_compute_network.khw.name} source_ranges = [ 10.240.0.0/24 , 10.200.0.0/16 ] allow { protocol = tcp } allow { protocol = udp } allow { protocol = icmp } } resource google_compute_firewall khw-allow-external { name = kubernetes-the-hard-way-allow-external network = ${google_compute_network.khw.name} allow { protocol = icmp } allow { protocol = tcp ports = [ 22 , 6443 ] } source_ranges = [ 0.0.0.0/0 ] } resource google_compute_firewall khw-allow-dns { name = kubernetes-the-hard-way-allow-dns network = ${google_compute_network.khw.name} source_ranges = [ 0.0.0.0 ] allow { protocol = tcp ports = [ 53 , 443 ] } allow { protocol = udp ports = [ 53 ] } } resource google_compute_firewall khw-allow-health-check { name = kubernetes-the-hard-way-allow-health-check network = ${google_compute_network.khw.name} allow { protocol = tcp } source_ranges = [ 209.85.152.0/22 , 209.85.204.0/22 , 35.191.0.0/16 ] } Confirm network 1 gcloud compute firewall-rules list --filter = network:kubernetes-the-hard-way Should look like: 1 2 3 NAME NETWORK DIRECTION PRIORITY ALLOW DENY kubernetes-the-hard-way-allow-external kubernetes-the-hard-way INGRESS 1000 icmp,tcp:22,tcp:6443 kubernetes-the-hard-way-allow-internal kubernetes-the-hard-way INGRESS 1000 icmp,udp,tcp Public IP 1 2 3 resource google_compute_address khw-lb-public-ip { name = kubernetes-the-hard-way } Confirm: 1 gcloud compute addresses list --filter = name=( kubernetes-the-hard-way ) Output: 1 2 NAME REGION ADDRESS STATUS kubernetes-the-hard-way europe-west4 35 .204.134.219 RESERVED VM Definitions with Terraform modules We're going to need to create 6 VM's. 3 Controller nodes and 3 worker nodes. Within each of the two categories, all the three VM's will be the same. So it would be a waste to define them more than once. This can be achieved via Terraform's Module system (read more here . Define a module For the sake of naming convention, we'll put all of our modules in a modules subfolder. We'll start with the controller module, but you can do the same for the worker. 1 mkdir -p modules/controller 1 2 3 4 ls -lath drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 . drwxr-xr-x 20 joostvdg staff 640B Aug 22 14 :47 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 modules 1 2 3 4 ls -lath modules drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 . drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :03 controller Inside modules / controller we create two files, main . tf and variables . tf . We have to create an additional variables file, as the module cannot use the main folder's variables. Then, in our main folder we'll create a tf file for using these modules, called nodes . tf . As stated above, we pass along any variable from our main variables . tf to the module. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 module controller { source = modules/controller machine_type = ${var.machine_type_controllers} num = ${var.num_controllers} zone = ${var.region_default_zone} subnet = ${var.subnet_name} } module worker { source = modules/worker machine_type = ${var.machine_type_workers} num = ${var.num_workers} zone = ${var.region_default_zone} network = ${google_compute_network.khw.name} subnet = ${var.subnet_name} } Controller config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 data google_compute_image khw-ubuntu { family = ubuntu-1804-lts project = ubuntu-os-cloud } resource google_compute_instance khw-controller { count = ${var.num} name = controller-${count.index} machine_type = ${var.machine_type} zone = ${var.zone} can_ip_forward = true tags = [ kubernetes-the-hard-way , controller ] boot_disk { initialize_params { image = ${data.google_compute_image.khw-ubuntu.self_link} size = 200 // in GB } } network_interface { subnetwork = ${var.subnet} address = 10.240.0.1${count.index} access_config { // Ephemeral External IP } } # compute-rw,storage-ro,service-management,service-control,logging-write,monitoring service_account { scopes = [ compute-rw , storage-ro , service-management , service-control , logging-write , monitoring , ] } } Variables 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 variable num { description = The number of controller VMs } variable machine_type { description = The type of VM for controllers } variable zone { description = The zone to create the controllers in } variable subnet { description = The subnet to create the nic in } Worker config Extra config for the worker are the routes, to aid the pods going out of the node. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 data google_compute_image khw-ubuntu { family = ubuntu-1804-lts project = ubuntu-os-cloud } resource google_compute_instance khw-worker { count = ${var.num} name = worker-${count.index} machine_type = ${var.machine_type} zone = ${var.zone} can_ip_forward = true tags = [ kubernetes-the-hard-way , worker ] metadata { pod-cidr = 10.200.${count.index}.0/24 } boot_disk { initialize_params { image = ${data.google_compute_image.khw-ubuntu.self_link} size = 200 // in GB } } network_interface { subnetwork = ${var.subnet} address = 10.240.0.2${count.index} access_config { // Ephemeral External IP } } service_account { scopes = [ compute-rw , storage-ro , service-management , service-control , logging-write , monitoring , ] } } resource google_compute_route khw-worker-route { count = ${var.num} name = kubernetes-route-10-200-${count.index}-0-24 network = ${var.network} next_hop_ip = 10.240.0.2${count.index} dest_range = 10.200.${count.index}.0/24 } Variables 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 variable num { description = The number of controller VMs } variable machine_type { description = The type of VM for controllers } variable zone { description = The zone to create the controllers in } variable network { description = The network to use for routes } variable subnet { description = The subnet to create the nic in } Health check Because we will have three controllers, we have to make sure that GKE forwards Kubernetes API requests to each of them via our public IP address. We do this via a http health check, wich involves a forwarding rule and a target pool. Target pool being the group of controller VM's for which the forwarding rule is active. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 resource google_compute_target_pool khw-hc-target-pool { name = instance-pool # TODO: fixed set for now, maybe we can make this dynamic some day instances = [ ${var.region_default_zone}/controller-0 , ${var.region_default_zone}/controller-1 , ${var.region_default_zone}/controller-2 , ] health_checks = [ ${google_compute_http_health_check.khw-health-check.name} , ] } resource google_compute_http_health_check khw-health-check { name = kubernetes request_path = /healthz description = The health check for Kubernetes API server host = ${var.kubernetes-cluster-dns} } resource google_compute_forwarding_rule khw-hc-forward { name = kubernetes-forwarding-rule target = ${google_compute_target_pool.khw-hc-target-pool.self_link} region = ${var.region} port_range = 6443 ip_address = ${google_compute_address.khw-lb-public-ip.self_link} } Apply Terraform state In the end, our configuration should consist out of several .tf files and look something like this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ls -lath drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 . drwxr-xr-x 20 joostvdg staff 640B Aug 22 14 :47 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 modules -rw-r--r-- 1 joostvdg staff 1 .5K Aug 26 12 :50 variables.tf -rw-r--r-- 1 joostvdg staff 1 .3K Aug 17 16 :03 firewall.tf -rw-r--r-- 1 joostvdg staff 4 .4K Aug 17 12 :06 worker-config.md -rw-r--r-- 1 joostvdg staff 1 .6K Aug 17 09 :35 healthcheck.tf -rw-r--r-- 1 joostvdg staff 517B Aug 16 17 :09 nodes.tf -rw-r--r-- 1 joostvdg staff 92B Aug 16 13 :52 publicip.tf -rw-r--r-- 1 joostvdg staff 365B Aug 7 22 :07 vpc.tf -rw-r--r-- 1 joostvdg staff 189B Aug 7 16 :51 base.tf drwxr-xr-x 5 joostvdg staff 160B Aug 7 21 :52 .terraform -rw-r--r-- 1 joostvdg staff 0B Aug 7 18 :28 terraform.tfstate We're now going to plan and then apply our Terraform configuration to create the resources in GCE. 1 terraform plan 1 terraform apply","title":"Create GCE resources"},{"location":"kubernetes/khw-gce/terraform-compute/#compute-resources","text":"","title":"Compute resources"},{"location":"kubernetes/khw-gce/terraform-compute/#create-network","text":"","title":"Create network"},{"location":"kubernetes/khw-gce/terraform-compute/#vpc-with-firewall-rules","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 provider google { credentials = ${file( ${var.credentials_file_path} )} project = ${var.project_name} region = ${var.region} } resource google_compute_network khw { name = kubernetes-the-hard-way auto_create_subnetworks = false } resource google_compute_subnetwork khw-kubernetes { name = kubernetes ip_cidr_range = 10.240.0.0/24 region = ${var.region} network = ${google_compute_network.khw.self_link} } resource google_compute_firewall khw-allow-internal { name = kubernetes-the-hard-way-allow-internal network = ${google_compute_network.khw.name} source_ranges = [ 10.240.0.0/24 , 10.200.0.0/16 ] allow { protocol = tcp } allow { protocol = udp } allow { protocol = icmp } } resource google_compute_firewall khw-allow-external { name = kubernetes-the-hard-way-allow-external network = ${google_compute_network.khw.name} allow { protocol = icmp } allow { protocol = tcp ports = [ 22 , 6443 ] } source_ranges = [ 0.0.0.0/0 ] } resource google_compute_firewall khw-allow-dns { name = kubernetes-the-hard-way-allow-dns network = ${google_compute_network.khw.name} source_ranges = [ 0.0.0.0 ] allow { protocol = tcp ports = [ 53 , 443 ] } allow { protocol = udp ports = [ 53 ] } } resource google_compute_firewall khw-allow-health-check { name = kubernetes-the-hard-way-allow-health-check network = ${google_compute_network.khw.name} allow { protocol = tcp } source_ranges = [ 209.85.152.0/22 , 209.85.204.0/22 , 35.191.0.0/16 ] }","title":"VPC with Firewall rules"},{"location":"kubernetes/khw-gce/terraform-compute/#confirm-network","text":"1 gcloud compute firewall-rules list --filter = network:kubernetes-the-hard-way Should look like: 1 2 3 NAME NETWORK DIRECTION PRIORITY ALLOW DENY kubernetes-the-hard-way-allow-external kubernetes-the-hard-way INGRESS 1000 icmp,tcp:22,tcp:6443 kubernetes-the-hard-way-allow-internal kubernetes-the-hard-way INGRESS 1000 icmp,udp,tcp","title":"Confirm network"},{"location":"kubernetes/khw-gce/terraform-compute/#public-ip","text":"1 2 3 resource google_compute_address khw-lb-public-ip { name = kubernetes-the-hard-way } Confirm: 1 gcloud compute addresses list --filter = name=( kubernetes-the-hard-way ) Output: 1 2 NAME REGION ADDRESS STATUS kubernetes-the-hard-way europe-west4 35 .204.134.219 RESERVED","title":"Public IP"},{"location":"kubernetes/khw-gce/terraform-compute/#vm-definitions-with-terraform-modules","text":"We're going to need to create 6 VM's. 3 Controller nodes and 3 worker nodes. Within each of the two categories, all the three VM's will be the same. So it would be a waste to define them more than once. This can be achieved via Terraform's Module system (read more here .","title":"VM Definitions with Terraform modules"},{"location":"kubernetes/khw-gce/terraform-compute/#define-a-module","text":"For the sake of naming convention, we'll put all of our modules in a modules subfolder. We'll start with the controller module, but you can do the same for the worker. 1 mkdir -p modules/controller 1 2 3 4 ls -lath drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 . drwxr-xr-x 20 joostvdg staff 640B Aug 22 14 :47 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 modules 1 2 3 4 ls -lath modules drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 . drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :03 controller Inside modules / controller we create two files, main . tf and variables . tf . We have to create an additional variables file, as the module cannot use the main folder's variables. Then, in our main folder we'll create a tf file for using these modules, called nodes . tf . As stated above, we pass along any variable from our main variables . tf to the module. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 module controller { source = modules/controller machine_type = ${var.machine_type_controllers} num = ${var.num_controllers} zone = ${var.region_default_zone} subnet = ${var.subnet_name} } module worker { source = modules/worker machine_type = ${var.machine_type_workers} num = ${var.num_workers} zone = ${var.region_default_zone} network = ${google_compute_network.khw.name} subnet = ${var.subnet_name} }","title":"Define a module"},{"location":"kubernetes/khw-gce/terraform-compute/#controller-config","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 data google_compute_image khw-ubuntu { family = ubuntu-1804-lts project = ubuntu-os-cloud } resource google_compute_instance khw-controller { count = ${var.num} name = controller-${count.index} machine_type = ${var.machine_type} zone = ${var.zone} can_ip_forward = true tags = [ kubernetes-the-hard-way , controller ] boot_disk { initialize_params { image = ${data.google_compute_image.khw-ubuntu.self_link} size = 200 // in GB } } network_interface { subnetwork = ${var.subnet} address = 10.240.0.1${count.index} access_config { // Ephemeral External IP } } # compute-rw,storage-ro,service-management,service-control,logging-write,monitoring service_account { scopes = [ compute-rw , storage-ro , service-management , service-control , logging-write , monitoring , ] } }","title":"Controller config"},{"location":"kubernetes/khw-gce/terraform-compute/#variables","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 variable num { description = The number of controller VMs } variable machine_type { description = The type of VM for controllers } variable zone { description = The zone to create the controllers in } variable subnet { description = The subnet to create the nic in }","title":"Variables"},{"location":"kubernetes/khw-gce/terraform-compute/#worker-config","text":"Extra config for the worker are the routes, to aid the pods going out of the node. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 data google_compute_image khw-ubuntu { family = ubuntu-1804-lts project = ubuntu-os-cloud } resource google_compute_instance khw-worker { count = ${var.num} name = worker-${count.index} machine_type = ${var.machine_type} zone = ${var.zone} can_ip_forward = true tags = [ kubernetes-the-hard-way , worker ] metadata { pod-cidr = 10.200.${count.index}.0/24 } boot_disk { initialize_params { image = ${data.google_compute_image.khw-ubuntu.self_link} size = 200 // in GB } } network_interface { subnetwork = ${var.subnet} address = 10.240.0.2${count.index} access_config { // Ephemeral External IP } } service_account { scopes = [ compute-rw , storage-ro , service-management , service-control , logging-write , monitoring , ] } } resource google_compute_route khw-worker-route { count = ${var.num} name = kubernetes-route-10-200-${count.index}-0-24 network = ${var.network} next_hop_ip = 10.240.0.2${count.index} dest_range = 10.200.${count.index}.0/24 }","title":"Worker config"},{"location":"kubernetes/khw-gce/terraform-compute/#variables_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 variable num { description = The number of controller VMs } variable machine_type { description = The type of VM for controllers } variable zone { description = The zone to create the controllers in } variable network { description = The network to use for routes } variable subnet { description = The subnet to create the nic in }","title":"Variables"},{"location":"kubernetes/khw-gce/terraform-compute/#health-check","text":"Because we will have three controllers, we have to make sure that GKE forwards Kubernetes API requests to each of them via our public IP address. We do this via a http health check, wich involves a forwarding rule and a target pool. Target pool being the group of controller VM's for which the forwarding rule is active. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 resource google_compute_target_pool khw-hc-target-pool { name = instance-pool # TODO: fixed set for now, maybe we can make this dynamic some day instances = [ ${var.region_default_zone}/controller-0 , ${var.region_default_zone}/controller-1 , ${var.region_default_zone}/controller-2 , ] health_checks = [ ${google_compute_http_health_check.khw-health-check.name} , ] } resource google_compute_http_health_check khw-health-check { name = kubernetes request_path = /healthz description = The health check for Kubernetes API server host = ${var.kubernetes-cluster-dns} } resource google_compute_forwarding_rule khw-hc-forward { name = kubernetes-forwarding-rule target = ${google_compute_target_pool.khw-hc-target-pool.self_link} region = ${var.region} port_range = 6443 ip_address = ${google_compute_address.khw-lb-public-ip.self_link} }","title":"Health check"},{"location":"kubernetes/khw-gce/terraform-compute/#apply-terraform-state","text":"In the end, our configuration should consist out of several .tf files and look something like this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ls -lath drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 . drwxr-xr-x 20 joostvdg staff 640B Aug 22 14 :47 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 modules -rw-r--r-- 1 joostvdg staff 1 .5K Aug 26 12 :50 variables.tf -rw-r--r-- 1 joostvdg staff 1 .3K Aug 17 16 :03 firewall.tf -rw-r--r-- 1 joostvdg staff 4 .4K Aug 17 12 :06 worker-config.md -rw-r--r-- 1 joostvdg staff 1 .6K Aug 17 09 :35 healthcheck.tf -rw-r--r-- 1 joostvdg staff 517B Aug 16 17 :09 nodes.tf -rw-r--r-- 1 joostvdg staff 92B Aug 16 13 :52 publicip.tf -rw-r--r-- 1 joostvdg staff 365B Aug 7 22 :07 vpc.tf -rw-r--r-- 1 joostvdg staff 189B Aug 7 16 :51 base.tf drwxr-xr-x 5 joostvdg staff 160B Aug 7 21 :52 .terraform -rw-r--r-- 1 joostvdg staff 0B Aug 7 18 :28 terraform.tfstate We're now going to plan and then apply our Terraform configuration to create the resources in GCE. 1 terraform plan 1 terraform apply","title":"Apply Terraform state"},{"location":"kubernetes/khw-gce/worker/","text":"Worker Installation Install base components Download 1 2 3 4 5 6 7 8 9 wget -q --show-progress --https-only --timestamping \\ https://github.com/kubernetes-incubator/cri-tools/releases/download/v1.0.0-beta.0/crictl-v1.0.0-beta.0-linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-the-hard-way/runsc \\ https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \\ https://github.com/containernetworking/plugins/releases/download/v0.7.1/cni-plugins-amd64-v0.7.1.tgz \\ https://github.com/containerd/containerd/releases/download/v1.1.2/containerd-1.1.2.linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-proxy \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubelet Prepare landing folders 1 2 3 4 5 6 7 8 sudo mkdir -p \\ /etc/cni/net.d \\ /opt/cni/bin \\ /var/lib/kubelet \\ /var/lib/kube-proxy \\ /var/lib/kubernetes \\ /var/run/kubernetes \\ /etc/containerd/ Unpack to folders 1 2 3 4 5 6 chmod +x kubectl kube-proxy kubelet runc.amd64 runsc sudo mv runc.amd64 runc sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/ sudo tar -xvf crictl-v1.0.0-beta.0-linux-amd64.tar.gz -C /usr/local/bin/ sudo tar -xvf cni-plugins-amd64-v0.7.1.tgz -C /opt/cni/bin/ sudo tar -xvf containerd-1.1.2.linux-amd64.tar.gz -C / List variables 1 2 3 4 5 POD_CIDR = $( curl -s -H Metadata-Flavor: Google \\ http://metadata.google.internal/computeMetadata/v1/instance/attributes/pod-cidr ) echo HOSTNAME = $HOSTNAME echo POD_CIDR = $POD_CIDR Configure ContainerD Runtime configuration file 1 2 3 4 5 6 7 8 9 10 11 12 13 cat EOF | sudo tee /etc/containerd/config.toml [plugins] [plugins.cri.containerd] snapshotter = overlayfs [plugins.cri.containerd.default_runtime] runtime_type = io.containerd.runtime.v1.linux runtime_engine = /usr/local/bin/runc runtime_root = [plugins.cri.containerd.untrusted_workload_runtime] runtime_type = io.containerd.runtime.v1.linux runtime_engine = /usr/local/bin/runsc runtime_root = /run/containerd/runsc EOF SystemD service configuration file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 cat EOF | sudo tee /etc/systemd/system/containerd.service [Unit] Description = containerd container runtime Documentation = https://containerd.io After = network.target [Service] ExecStartPre = /sbin/modprobe overlay ExecStart = /bin/containerd Restart = always RestartSec = 5 Delegate = yes KillMode = process OOMScoreAdjust = -999 LimitNOFILE = 1048576 LimitNPROC = infinity LimitCORE = infinity [Install] WantedBy = multi-user.target EOF Configure CNI Warning We do not need to configure cni as we will setup Weave and it will do the necessary setup automagically. Configure Kubelet Move certificates to correct places 1 2 3 sudo mv ${ HOSTNAME } -key.pem ${ HOSTNAME } .pem /var/lib/kubelet/ sudo mv ${ HOSTNAME } .kubeconfig /var/lib/kubelet/kubeconfig sudo mv ca.pem /var/lib/kubernetes/ Create k8s yaml configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cat EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml kind : KubeletConfiguration apiVersion : kubelet.config.k8s.io/v1beta1 authentication : anonymous : enabled : false webhook : enabled : true x509 : clientCAFile : /var/lib/kubernetes/ca.pem authorization : mode : Webhook clusterDomain : cluster.local clusterDNS : - 10.32.0.10 podCIDR : ${POD_CIDR} runtimeRequestTimeout : 15m tlsCertFile : /var/lib/kubelet/${HOSTNAME}.pem tlsPrivateKeyFile : /var/lib/kubelet/${HOSTNAME}-key.pem EOF SystemD service configuration file Warning One thing I see missing from your kubelet configuration is --non-masquerade-cidr flag. Kubelet needs to be run with this option for traffic to outside clusterIP range. Refer here - kubenet 1 Kubelet should also be run with the ` --non-masquerade-cidr= clusterCidr ` argument to ensure traffic to IPs outside this range will use IP masquerade. Not sure, if this is the cause, but looks like this is a requirement and is missing from the Kubelet config. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cat EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description = Kubernetes Kubelet Documentation = https://github.com/kubernetes/kubernetes After = containerd.service Requires = containerd.service [Service] ExecStart = /usr/local/bin/kubelet \\\\ --allow-privileged=true \\\\ --config=/var/lib/kubelet/kubelet-config.yaml \\\\ --container-runtime=remote \\\\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\ --image-pull-progress-deadline=2m \\\\ --kubeconfig=/var/lib/kubelet/kubeconfig \\\\ --network-plugin=cni \\\\ --register-node=true \\\\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF Kube-Proxy Move kubeconfig 1 sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig Create k8s yaml config 1 2 3 4 5 6 7 8 cat EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml kind : KubeProxyConfiguration apiVersion : kubeproxy.config.k8s.io/v1alpha1 clientConnection : kubeconfig : /var/lib/kube-proxy/kubeconfig mode : iptables clusterCIDR : 10.200.0.0/16 EOF Create SystemD service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat EOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description = Kubernetes Kube Proxy Documentation = https://github.com/kubernetes/kubernetes [Service] ExecStart = /usr/local/bin/kube-proxy \\\\ --config=/var/lib/kube-proxy/kube-proxy-config.yaml \\\\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF Configure and start the SystemD services 1 2 3 sudo systemctl daemon-reload sudo systemctl enable containerd kubelet kube-proxy sudo systemctl start containerd kubelet kube-proxy Validate Note Run this from a machine outside the cluster, with access to the admin kubeconfig. 1 gcloud compute ssh controller-0 --command kubectl get nodes --kubeconfig admin.kubeconfig Note As we didn't configure networking yet, the nodes should be shown as NotReady status.","title":"Worker Config"},{"location":"kubernetes/khw-gce/worker/#worker-installation","text":"","title":"Worker Installation"},{"location":"kubernetes/khw-gce/worker/#install-base-components","text":"","title":"Install base components"},{"location":"kubernetes/khw-gce/worker/#download","text":"1 2 3 4 5 6 7 8 9 wget -q --show-progress --https-only --timestamping \\ https://github.com/kubernetes-incubator/cri-tools/releases/download/v1.0.0-beta.0/crictl-v1.0.0-beta.0-linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-the-hard-way/runsc \\ https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \\ https://github.com/containernetworking/plugins/releases/download/v0.7.1/cni-plugins-amd64-v0.7.1.tgz \\ https://github.com/containerd/containerd/releases/download/v1.1.2/containerd-1.1.2.linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-proxy \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubelet","title":"Download"},{"location":"kubernetes/khw-gce/worker/#prepare-landing-folders","text":"1 2 3 4 5 6 7 8 sudo mkdir -p \\ /etc/cni/net.d \\ /opt/cni/bin \\ /var/lib/kubelet \\ /var/lib/kube-proxy \\ /var/lib/kubernetes \\ /var/run/kubernetes \\ /etc/containerd/","title":"Prepare landing folders"},{"location":"kubernetes/khw-gce/worker/#unpack-to-folders","text":"1 2 3 4 5 6 chmod +x kubectl kube-proxy kubelet runc.amd64 runsc sudo mv runc.amd64 runc sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/ sudo tar -xvf crictl-v1.0.0-beta.0-linux-amd64.tar.gz -C /usr/local/bin/ sudo tar -xvf cni-plugins-amd64-v0.7.1.tgz -C /opt/cni/bin/ sudo tar -xvf containerd-1.1.2.linux-amd64.tar.gz -C /","title":"Unpack to folders"},{"location":"kubernetes/khw-gce/worker/#list-variables","text":"1 2 3 4 5 POD_CIDR = $( curl -s -H Metadata-Flavor: Google \\ http://metadata.google.internal/computeMetadata/v1/instance/attributes/pod-cidr ) echo HOSTNAME = $HOSTNAME echo POD_CIDR = $POD_CIDR","title":"List variables"},{"location":"kubernetes/khw-gce/worker/#configure-containerd","text":"","title":"Configure ContainerD"},{"location":"kubernetes/khw-gce/worker/#runtime-configuration-file","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 cat EOF | sudo tee /etc/containerd/config.toml [plugins] [plugins.cri.containerd] snapshotter = overlayfs [plugins.cri.containerd.default_runtime] runtime_type = io.containerd.runtime.v1.linux runtime_engine = /usr/local/bin/runc runtime_root = [plugins.cri.containerd.untrusted_workload_runtime] runtime_type = io.containerd.runtime.v1.linux runtime_engine = /usr/local/bin/runsc runtime_root = /run/containerd/runsc EOF","title":"Runtime configuration file"},{"location":"kubernetes/khw-gce/worker/#systemd-service-configuration-file","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 cat EOF | sudo tee /etc/systemd/system/containerd.service [Unit] Description = containerd container runtime Documentation = https://containerd.io After = network.target [Service] ExecStartPre = /sbin/modprobe overlay ExecStart = /bin/containerd Restart = always RestartSec = 5 Delegate = yes KillMode = process OOMScoreAdjust = -999 LimitNOFILE = 1048576 LimitNPROC = infinity LimitCORE = infinity [Install] WantedBy = multi-user.target EOF","title":"SystemD service configuration file"},{"location":"kubernetes/khw-gce/worker/#configure-cni","text":"Warning We do not need to configure cni as we will setup Weave and it will do the necessary setup automagically.","title":"Configure CNI"},{"location":"kubernetes/khw-gce/worker/#configure-kubelet","text":"","title":"Configure Kubelet"},{"location":"kubernetes/khw-gce/worker/#move-certificates-to-correct-places","text":"1 2 3 sudo mv ${ HOSTNAME } -key.pem ${ HOSTNAME } .pem /var/lib/kubelet/ sudo mv ${ HOSTNAME } .kubeconfig /var/lib/kubelet/kubeconfig sudo mv ca.pem /var/lib/kubernetes/","title":"Move certificates to correct places"},{"location":"kubernetes/khw-gce/worker/#create-k8s-yaml-configuration","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cat EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml kind : KubeletConfiguration apiVersion : kubelet.config.k8s.io/v1beta1 authentication : anonymous : enabled : false webhook : enabled : true x509 : clientCAFile : /var/lib/kubernetes/ca.pem authorization : mode : Webhook clusterDomain : cluster.local clusterDNS : - 10.32.0.10 podCIDR : ${POD_CIDR} runtimeRequestTimeout : 15m tlsCertFile : /var/lib/kubelet/${HOSTNAME}.pem tlsPrivateKeyFile : /var/lib/kubelet/${HOSTNAME}-key.pem EOF","title":"Create k8s yaml configuration"},{"location":"kubernetes/khw-gce/worker/#systemd-service-configuration-file_1","text":"Warning One thing I see missing from your kubelet configuration is --non-masquerade-cidr flag. Kubelet needs to be run with this option for traffic to outside clusterIP range. Refer here - kubenet 1 Kubelet should also be run with the ` --non-masquerade-cidr= clusterCidr ` argument to ensure traffic to IPs outside this range will use IP masquerade. Not sure, if this is the cause, but looks like this is a requirement and is missing from the Kubelet config. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cat EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description = Kubernetes Kubelet Documentation = https://github.com/kubernetes/kubernetes After = containerd.service Requires = containerd.service [Service] ExecStart = /usr/local/bin/kubelet \\\\ --allow-privileged=true \\\\ --config=/var/lib/kubelet/kubelet-config.yaml \\\\ --container-runtime=remote \\\\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\ --image-pull-progress-deadline=2m \\\\ --kubeconfig=/var/lib/kubelet/kubeconfig \\\\ --network-plugin=cni \\\\ --register-node=true \\\\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF","title":"SystemD service configuration file"},{"location":"kubernetes/khw-gce/worker/#kube-proxy","text":"","title":"Kube-Proxy"},{"location":"kubernetes/khw-gce/worker/#move-kubeconfig","text":"1 sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig","title":"Move kubeconfig"},{"location":"kubernetes/khw-gce/worker/#create-k8s-yaml-config","text":"1 2 3 4 5 6 7 8 cat EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml kind : KubeProxyConfiguration apiVersion : kubeproxy.config.k8s.io/v1alpha1 clientConnection : kubeconfig : /var/lib/kube-proxy/kubeconfig mode : iptables clusterCIDR : 10.200.0.0/16 EOF","title":"Create k8s yaml config"},{"location":"kubernetes/khw-gce/worker/#create-systemd-service","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat EOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description = Kubernetes Kube Proxy Documentation = https://github.com/kubernetes/kubernetes [Service] ExecStart = /usr/local/bin/kube-proxy \\\\ --config=/var/lib/kube-proxy/kube-proxy-config.yaml \\\\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF","title":"Create SystemD service"},{"location":"kubernetes/khw-gce/worker/#configure-and-start-the-systemd-services","text":"1 2 3 sudo systemctl daemon-reload sudo systemctl enable containerd kubelet kube-proxy sudo systemctl start containerd kubelet kube-proxy","title":"Configure and start the SystemD services"},{"location":"kubernetes/khw-gce/worker/#validate","text":"Note Run this from a machine outside the cluster, with access to the admin kubeconfig. 1 gcloud compute ssh controller-0 --command kubectl get nodes --kubeconfig admin.kubeconfig Note As we didn't configure networking yet, the nodes should be shown as NotReady status.","title":"Validate"},{"location":"linux/","text":"Linux","title":"Linux"},{"location":"linux/#linux","text":"","title":"Linux"},{"location":"linux/iptables/","text":"iptables","title":"iptables"},{"location":"linux/iptables/#iptables","text":"","title":"iptables"},{"location":"linux/networking/","text":"Networking","title":"Networking"},{"location":"linux/networking/#networking","text":"","title":"Networking"},{"location":"linux/systemd/","text":"SystemD Increasingly, Linux distributions are adopting or planning to adopt the systemd init system. This powerful suite of software can manage many aspects of your server, from services to mounted devices and system states. 1 Concepts Unit In systemd , a unit refers to any resource that the system knows how to operate on and manage. This is the primary object that the systemd tools know how to deal with. These resources are defined using configuration files called unit files . 1 Path A path unit defines a filesystem path that systmed can monitor for changes. Another unit must exist that will be be activated when certain activity is detected at the path location. Path activity is determined through inotify events . My idea, you can use this for those services that should trigger on file uploads or backup dumps. Although I wonder if the Unit's main service knows which path was triggered? If it does, than it's easy, else you still need a \"file walker\". Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 [Unit] Description = Timezone Helper Service After = network.target StartLimitIntervalSec = 0 [Service] Type = simple Restart = always RestartSec = 3 User = joostvdg ExecStart = /usr/bin/timezone_helper_service [Install] WantedBy = multi-user.target Resources https://www.linuxjournal.com/content/linux-filesystem-events-inotify References Introduction to systemd from Digital Ocean","title":"SystemD"},{"location":"linux/systemd/#systemd","text":"Increasingly, Linux distributions are adopting or planning to adopt the systemd init system. This powerful suite of software can manage many aspects of your server, from services to mounted devices and system states. 1","title":"SystemD"},{"location":"linux/systemd/#concepts","text":"","title":"Concepts"},{"location":"linux/systemd/#unit","text":"In systemd , a unit refers to any resource that the system knows how to operate on and manage. This is the primary object that the systemd tools know how to deal with. These resources are defined using configuration files called unit files . 1","title":"Unit"},{"location":"linux/systemd/#path","text":"A path unit defines a filesystem path that systmed can monitor for changes. Another unit must exist that will be be activated when certain activity is detected at the path location. Path activity is determined through inotify events . My idea, you can use this for those services that should trigger on file uploads or backup dumps. Although I wonder if the Unit's main service knows which path was triggered? If it does, than it's easy, else you still need a \"file walker\".","title":"Path"},{"location":"linux/systemd/#example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 [Unit] Description = Timezone Helper Service After = network.target StartLimitIntervalSec = 0 [Service] Type = simple Restart = always RestartSec = 3 User = joostvdg ExecStart = /usr/bin/timezone_helper_service [Install] WantedBy = multi-user.target","title":"Example"},{"location":"linux/systemd/#resources","text":"https://www.linuxjournal.com/content/linux-filesystem-events-inotify","title":"Resources"},{"location":"linux/systemd/#references","text":"Introduction to systemd from Digital Ocean","title":"References"},{"location":"other/mkdocs/","text":"MKDocs This website is build using the following: MKDocs a python tool for building static websites from MarkDown files MK Material expansion/theme of MK Docs that makes it a responsive website with Google's Material theme Add information to the docs MKDocs can be a bit daunting to use, especially when extended with MKDocs Material and PyMdown Extensions . There are two parts to the site: 1) the markdown files, they're in docs / and 2) the site listing (mkdocs.yml) and automation scripts, these can be found in docs - scripts / . Extends current page To extend a current page, simply write the MarkDown as you're used to. For the specific extensions offered by PyMX and Material, checkout the following pages: MKDocs Material Getting Started Guide MKDocs Extensions PyMdown Extensions Usage Guide Add a new page In the docs - scripts / mkdocs . yml you will find the site structure under the yml item of pages . 1 2 3 4 5 6 pages : - Home : index . md - Other Root Page : some - page . md - Root with children : - ChildOne : root2 / child1 . md - ChildTwo : root2 / child2 . md Things to know All .md files that are listed in the pages will be translated to an HTML file and dubbed {OriginalFileName}.html Naming a file index.md will allow you to refer to it by path without the file name we can refer to root2 simply by site / root2 and can omit the index. 1 2 - Root : index . md - Root2 : root2 / index . html Configuration Of This Website 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 # Theme # Configuration theme : feature : tabs : true name : material language : en logo : icon : public palette : primary : orange accent : red font : text : Roboto code : Roboto Mono plugins : - search - minify : minify_html : true extra : social : - type : github link : https://github.com/joostvdg - type : twitter link : https://twitter.com/joost_vdg - type : linkedin link : https://linkedin.com/in/joostvdg # Extensions markdown_extensions : - admonition - codehilite : linenums : true guess_lang : true - footnotes - meta - toc : permalink : true - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.details - pymdownx.critic - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde Build the site locally As it is a Python tool, you can easily build it with Python (2.7 is recommended). The requirements are captured in a pip install scripts: docs - scripts / install . sh where the dependencies are in Pip's requirements.txt . Once that is done, you can do the following: 1 mkdocs build --clean Which will generate the site into docs - scripts / site where you can simply open the index.html with a browser - it is a static site. For docker, you can use the * . sh scripts, or simply run . sh to kick of the entire build. Dependencies You can use pip to manage the dependencies required for building the site. 1 pip install -r requirements.txt Requirements.txt 1 2 3 4 5 6 7 8 mkdocs = 1 .0.4 mkdocs-bootswatch = 0 .4.0 python-jenkins = 0 .4.10 mkdocs-material = 4 .4.0 mkdocs-minify-plugin = 0 .1.0 pygments = 2 .4.2 pymdown-extensions = 6 .0.0 Markdown = 3 .0.1 Host It With Docker Dockerfile 1 2 3 4 5 6 7 8 9 10 FROM nginx:mainline LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 1.0.0 LABEL description = Mr J s knowledge base RUN apt-get update apt-get install --no-install-recommends -y curl = 7 .* rm -rf /var/lib/apt/lists/* HEALTHCHECK CMD curl --fail http://localhost:80/docs/ || exit 1 COPY site/ /usr/share/nginx/html/docs RUN ls -lath /usr/share/nginx/html/docs Build 1 2 3 4 5 #!/usr/bin/env bash TAGNAME = joostvdg-github-io-image echo # Building new image with tag: $TAGNAME docker build --tag = $TAGNAME . Run 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #!/usr/bin/env bash IMAGE = joostvdg-github-io-image NAME = joostvdg-github-io-instance RUNNING = ` docker ps | grep -c $NAME ` if [ $RUNNING -gt 0 ] then echo Stopping $NAME docker stop $NAME fi EXISTING = ` docker ps -a | grep -c $NAME ` if [ $EXISTING -gt 0 ] then echo Removing $NAME docker rm $NAME fi echo Create new instance $NAME based on $IMAGE docker run --name $NAME -d -p 8088 :80 $IMAGE echo Tail the logs of the new instance docker logs $NAME # IP=$(docker inspect --format {{.NetworkSettings.Networks.bridge.IPAddress}} $NAME) # echo IP address of the container: $IP echo http://127.0.0.1.nip.io:8088/docs/ Jenkins build Declarative format 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 pipeline { agent none options { timeout(time: 10, unit: MINUTES ) timestamps() buildDiscarder(logRotator(numToKeepStr: 5 )) } stages { stage( Prepare ){ agent { label docker } steps { deleteDir() } } stage( Checkout ) { agent { label docker } steps { checkout scm script { env.GIT_COMMIT_HASH = sh returnStdout: true, script: git rev-parse --verify HEAD } } } stage( Build Docs ) { agent { docker { image caladreas/mkdocs-docker-build-container label docker } } steps { sh cd docs-scripts mkdocs build } } stage( Prepare Docker Image ) { agent { label docker } environment { DOCKER_CRED = credentials( ldap ) } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true, script: cd docs-scripts docker run --rm -i lukasmartinelli/hadolint Dockerfile if (lintResult.trim() == ) { println Lint finished with no errors } else { println Error found in Lint println ${lintResult} currentBuild.result = UNSTABLE } } }, // end test dockerfile BuildImage: { sh chmod +x docs-scripts/build.sh sh cd docs-scripts ./build.sh } , login: { sh docker login -u ${DOCKER_CRED_USR} -p ${DOCKER_CRED_PSW} registry } ) } post { success { sh chmod +x push.sh sh ./push.sh } } } } }","title":"MKDocs (This Static Website)"},{"location":"other/mkdocs/#mkdocs","text":"This website is build using the following: MKDocs a python tool for building static websites from MarkDown files MK Material expansion/theme of MK Docs that makes it a responsive website with Google's Material theme","title":"MKDocs"},{"location":"other/mkdocs/#add-information-to-the-docs","text":"MKDocs can be a bit daunting to use, especially when extended with MKDocs Material and PyMdown Extensions . There are two parts to the site: 1) the markdown files, they're in docs / and 2) the site listing (mkdocs.yml) and automation scripts, these can be found in docs - scripts / .","title":"Add information to the docs"},{"location":"other/mkdocs/#extends-current-page","text":"To extend a current page, simply write the MarkDown as you're used to. For the specific extensions offered by PyMX and Material, checkout the following pages: MKDocs Material Getting Started Guide MKDocs Extensions PyMdown Extensions Usage Guide","title":"Extends current page"},{"location":"other/mkdocs/#add-a-new-page","text":"In the docs - scripts / mkdocs . yml you will find the site structure under the yml item of pages . 1 2 3 4 5 6 pages : - Home : index . md - Other Root Page : some - page . md - Root with children : - ChildOne : root2 / child1 . md - ChildTwo : root2 / child2 . md","title":"Add a new page"},{"location":"other/mkdocs/#things-to-know","text":"All .md files that are listed in the pages will be translated to an HTML file and dubbed {OriginalFileName}.html Naming a file index.md will allow you to refer to it by path without the file name we can refer to root2 simply by site / root2 and can omit the index. 1 2 - Root : index . md - Root2 : root2 / index . html","title":"Things to know"},{"location":"other/mkdocs/#configuration-of-this-website","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 # Theme # Configuration theme : feature : tabs : true name : material language : en logo : icon : public palette : primary : orange accent : red font : text : Roboto code : Roboto Mono plugins : - search - minify : minify_html : true extra : social : - type : github link : https://github.com/joostvdg - type : twitter link : https://twitter.com/joost_vdg - type : linkedin link : https://linkedin.com/in/joostvdg # Extensions markdown_extensions : - admonition - codehilite : linenums : true guess_lang : true - footnotes - meta - toc : permalink : true - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.details - pymdownx.critic - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde","title":"Configuration Of This Website"},{"location":"other/mkdocs/#build-the-site-locally","text":"As it is a Python tool, you can easily build it with Python (2.7 is recommended). The requirements are captured in a pip install scripts: docs - scripts / install . sh where the dependencies are in Pip's requirements.txt . Once that is done, you can do the following: 1 mkdocs build --clean Which will generate the site into docs - scripts / site where you can simply open the index.html with a browser - it is a static site. For docker, you can use the * . sh scripts, or simply run . sh to kick of the entire build.","title":"Build the site locally"},{"location":"other/mkdocs/#dependencies","text":"You can use pip to manage the dependencies required for building the site. 1 pip install -r requirements.txt","title":"Dependencies"},{"location":"other/mkdocs/#requirementstxt","text":"1 2 3 4 5 6 7 8 mkdocs = 1 .0.4 mkdocs-bootswatch = 0 .4.0 python-jenkins = 0 .4.10 mkdocs-material = 4 .4.0 mkdocs-minify-plugin = 0 .1.0 pygments = 2 .4.2 pymdown-extensions = 6 .0.0 Markdown = 3 .0.1","title":"Requirements.txt"},{"location":"other/mkdocs/#host-it-with-docker","text":"","title":"Host It With Docker"},{"location":"other/mkdocs/#dockerfile","text":"1 2 3 4 5 6 7 8 9 10 FROM nginx:mainline LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 1.0.0 LABEL description = Mr J s knowledge base RUN apt-get update apt-get install --no-install-recommends -y curl = 7 .* rm -rf /var/lib/apt/lists/* HEALTHCHECK CMD curl --fail http://localhost:80/docs/ || exit 1 COPY site/ /usr/share/nginx/html/docs RUN ls -lath /usr/share/nginx/html/docs","title":"Dockerfile"},{"location":"other/mkdocs/#build","text":"1 2 3 4 5 #!/usr/bin/env bash TAGNAME = joostvdg-github-io-image echo # Building new image with tag: $TAGNAME docker build --tag = $TAGNAME .","title":"Build"},{"location":"other/mkdocs/#run","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #!/usr/bin/env bash IMAGE = joostvdg-github-io-image NAME = joostvdg-github-io-instance RUNNING = ` docker ps | grep -c $NAME ` if [ $RUNNING -gt 0 ] then echo Stopping $NAME docker stop $NAME fi EXISTING = ` docker ps -a | grep -c $NAME ` if [ $EXISTING -gt 0 ] then echo Removing $NAME docker rm $NAME fi echo Create new instance $NAME based on $IMAGE docker run --name $NAME -d -p 8088 :80 $IMAGE echo Tail the logs of the new instance docker logs $NAME # IP=$(docker inspect --format {{.NetworkSettings.Networks.bridge.IPAddress}} $NAME) # echo IP address of the container: $IP echo http://127.0.0.1.nip.io:8088/docs/","title":"Run"},{"location":"other/mkdocs/#jenkins-build","text":"","title":"Jenkins build"},{"location":"other/mkdocs/#declarative-format","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 pipeline { agent none options { timeout(time: 10, unit: MINUTES ) timestamps() buildDiscarder(logRotator(numToKeepStr: 5 )) } stages { stage( Prepare ){ agent { label docker } steps { deleteDir() } } stage( Checkout ) { agent { label docker } steps { checkout scm script { env.GIT_COMMIT_HASH = sh returnStdout: true, script: git rev-parse --verify HEAD } } } stage( Build Docs ) { agent { docker { image caladreas/mkdocs-docker-build-container label docker } } steps { sh cd docs-scripts mkdocs build } } stage( Prepare Docker Image ) { agent { label docker } environment { DOCKER_CRED = credentials( ldap ) } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true, script: cd docs-scripts docker run --rm -i lukasmartinelli/hadolint Dockerfile if (lintResult.trim() == ) { println Lint finished with no errors } else { println Error found in Lint println ${lintResult} currentBuild.result = UNSTABLE } } }, // end test dockerfile BuildImage: { sh chmod +x docs-scripts/build.sh sh cd docs-scripts ./build.sh } , login: { sh docker login -u ${DOCKER_CRED_USR} -p ${DOCKER_CRED_PSW} registry } ) } post { success { sh chmod +x push.sh sh ./push.sh } } } } }","title":"Declarative format"},{"location":"other/shell/","text":"Shell End Result OSX Iterm2 + Solarized + OZSH + Font Awesome https://gist.github.com/kevin-smets/8568070 Using Powerline Font with VS Code Terminal https://medium.com/@hippojs.guo/vs-code-fix-fonts-in-terminal-761cc821ef41 My Config - 9K 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ZSH_THEME = powerlevel9k/powerlevel9k #ZSH_THEME= powerlevel10k/powerlevel10k POWERLEVEL9K_MODE = awesome-patched P9KGT_BACKGROUND = dark P9KGT_COLORS = light POWERLEVEL9K_LEFT_PROMPT_ELEMENTS =( dir vcs ) POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS =( status command_execution_time kubecontext battery ram time ) POWERLEVEL9K_STATUS_ICON_BEFORE_CONTENT = true POWERLEVEL9K_PROMPT_ON_NEWLINE = true POWERLEVEL9K_RPROMPT_ON_NEWLINE = true POWERLEVEL9K_PROMPT_ADD_NEWLINE = true","title":"Shell"},{"location":"other/shell/#shell","text":"","title":"Shell"},{"location":"other/shell/#end-result","text":"","title":"End Result"},{"location":"other/shell/#osx","text":"","title":"OSX"},{"location":"other/shell/#iterm2-solarized-ozsh-font-awesome","text":"https://gist.github.com/kevin-smets/8568070","title":"Iterm2 + Solarized + OZSH + Font Awesome"},{"location":"other/shell/#using-powerline-font-with-vs-code-terminal","text":"https://medium.com/@hippojs.guo/vs-code-fix-fonts-in-terminal-761cc821ef41","title":"Using Powerline Font with VS Code Terminal"},{"location":"other/shell/#my-config-9k","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ZSH_THEME = powerlevel9k/powerlevel9k #ZSH_THEME= powerlevel10k/powerlevel10k POWERLEVEL9K_MODE = awesome-patched P9KGT_BACKGROUND = dark P9KGT_COLORS = light POWERLEVEL9K_LEFT_PROMPT_ELEMENTS =( dir vcs ) POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS =( status command_execution_time kubecontext battery ram time ) POWERLEVEL9K_STATUS_ICON_BEFORE_CONTENT = true POWERLEVEL9K_PROMPT_ON_NEWLINE = true POWERLEVEL9K_RPROMPT_ON_NEWLINE = true POWERLEVEL9K_PROMPT_ADD_NEWLINE = true","title":"My Config - 9K"},{"location":"other/vim/","text":"VIM End Result Install Vundle 1 git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim Install plugins 1 vim ~/.vimrc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 filetype off filetype plugin indent on syntax on set rtp += ~/.vim/bundle/Vundle.vim call vundle#begin () Plugin gmarik/Vundle.vim Plugin reedes/vim-thematic Plugin airblade/vim-gitgutter Plugin vim-airline/vim-airline Plugin vim-airline/vim-airline-themes Plugin itchyny/lightline.vim Plugin nathanaelkane/vim-indent-guides Plugin scrooloose/nerdtree Plugin editorconfig/editorconfig-vim Plugin mhinz/vim-signify call vundle#end () filetype plugin indent on Open VIM, and install the plugins: 1 :installPlugins","title":"VIM"},{"location":"other/vim/#vim","text":"","title":"VIM"},{"location":"other/vim/#end-result","text":"","title":"End Result"},{"location":"other/vim/#install-vundle","text":"1 git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim","title":"Install Vundle"},{"location":"other/vim/#install-plugins","text":"1 vim ~/.vimrc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 filetype off filetype plugin indent on syntax on set rtp += ~/.vim/bundle/Vundle.vim call vundle#begin () Plugin gmarik/Vundle.vim Plugin reedes/vim-thematic Plugin airblade/vim-gitgutter Plugin vim-airline/vim-airline Plugin vim-airline/vim-airline-themes Plugin itchyny/lightline.vim Plugin nathanaelkane/vim-indent-guides Plugin scrooloose/nerdtree Plugin editorconfig/editorconfig-vim Plugin mhinz/vim-signify call vundle#end () filetype plugin indent on Open VIM, and install the plugins: 1 :installPlugins","title":"Install plugins"},{"location":"productivity/","text":"Developer Productivity Commoditization \"The big change has been in the hardware/software cost ratio. The buyer of a $2 million machine in 1960 felt that he could afford $250,000 more ofr a customized payroll program, one that slipped easily and nondisruptively into the computer-hostile social environment. Buyers of %50,000 office machines today cannot conceivably afford customized payroll programs; so they adapt their paryoll procedures to the packages available.\" - 2 F. Brooks - No Silver Bullet Where should productivity be sought If you're looking to increase productivity, it would be best to answer some fundamental questions first. What should we be productive in? What is productivity? How do you measure productivity? The first step is to determine, what you should be productive in. If you're building software for example, it is in finding out what to build. \"The hardest single part of building a software system is deciding precisely what to build.\" - F. Brooks 2 That is actually already one step to far, as you would need a reason to build a software system. So the first step for any individual or organization (start up, or otherwise) is to find out what people want that you can offer. \"The fundamental activity of a startup is to turn ideas into products, measure how customers respond, and then learn whether to pivot or persevere. All successful startup processes should be geared to accelerate that feedback loop.\" - The Lean Startup 3 How do you measure productivity Grow v.s. Build The Balancing act between centralized and decentralized On Multitasking Deep Work Attention Residue Why is it so hard to do my work Learning from Lean/Toyota Open Space Floor Plans http://rstb.royalsocietypublishing.org/content/373/1753/20170239 Conway's Law \"organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations.\" - M. Conway 1 Undifferentiated Heavy Lifting \"Work that needs to get done, but having it done doesn't bring our customers any direct benefit.\" - Dave Hahn Agile https://www.infoq.com/articles/agile-agile-blah-blah/ https://www.infoq.com/articles/death-agile-beyond Further reading Others https://www.youtube.com/watch?v=UTKIT6STSVM https://en.wikipedia.org/wiki/Complex_adaptive_system https://jobs.netflix.com/culture http://blackswanfarming.com/cost-of-delay/ https://www.rundeck.com/blog/tickets_make_operations_unnecessarily_miserable https://www.digitalocean.com/community/tutorials/what-is-immutable-infrastructure https://hbr.org/2015/12/what-the-research-tells-us-about-team-creativity-and-innovation https://www.thoughtworks.com/insights/blog/continuous-improvement-safe-environment https://qualitysafety.bmj.com/content/13/suppl_2/ii22 https://www.plutora.com/wp-content/uploads/dlm_uploads/2018/03/StateOfDevOpsTools_v14.pdf https://medium.com/@ATavgen/never-fail-twice-608147cb49b https://blogs.dropbox.com/dropbox/2018/07/study-high-performing-teams/?_tk=social oqa=183tl01liov linkId=100000003064606 http://psycnet.apa.org/record/1979-28632-001 https://pdfs.semanticscholar.org/a85d/432f44e43d61753bb8a121c246127b562a39.pdf https://medium.com/@dr_eprice/laziness-does-not-exist-3af27e312d01 https://en.wikipedia.org/wiki/Mindset#Fixed_and_growth http://www.reinventingorganizationswiki.com/Teal_Organizations https://www.mckinsey.com/business-functions/organization/our-insights/the-irrational-side-of-change-management https://www.barrypopik.com/index.php/new_york_city/entry/how_do_you_eat_an_elephant https://kadavy.net/blog/posts/mind-management-intro/ https://en.wikipedia.org/wiki/Planning_fallacy https://stories.lemonade.com/lemonade-proves-trust-pays-off-big-time-fdcf587af5a1 https://www.venturi-group.com/developer-to-cto/ https://dzone.com/articles/an-introduction-to-devops-principles https://www.thoughtworks.com/insights/blog/evolving-thoughtworks-internal-it-solve-broader-cross-cutting-problems https://www.thoughtworks.com/insights/blog/platform-tech-strategy-three-layers https://www.thoughtworks.com/insights/blog/why-it-departments-must-reinvent-themselves-part-1 https://en.wikipedia.org/wiki/Peter_principle https://hackernoon.com/why-all-engineers-must-understand-management-the-view-from-both-ladders-cc749ae14905 https://medium.freecodecamp.org/cognitive-bias-and-why-performance-management-is-so-hard-8852a1b874cd https://en.wikipedia.org/wiki/Horn_effect https://en.wikipedia.org/wiki/Halo_effect http://serendipstudio.org/bb/neuro/neuro02/web2/hhochman.html https://betterhumans.coach.me/how-to-be-a-better-manager-by-understanding-the-difference-between-market-norms-and-social-norms-3082d97d440f https://skillsmatter.com/skillscasts/10466-deep-dive-on-kubernetes-networking https://purplegriffon.com/blog/is-itil-agile-enough https://launchdarkly.com/blog/progressive-delivery-a-history-condensed/ http://www.collaborativefund.com/blog/real-world-vs-book-knowledge/ https://blog.codeship.com/using-jx-create-gitops-managed-jenkins-x-installation-cloudbees-codeship-terraform-google-kubernetes-engine/ https://martinfowler.com/articles/serverless.html https://www.quora.com/Some-people-including-the-creator-of-C-claim-that-there-is-a-huge-decline-of-quality-among-software-developers-What-seems-to-be-the-main-cause https://medium.com/netflix-techblog/full-cycle-developers-at-netflix-a08c31f83249 https://queue.acm.org/detail.cfm?id=3182626 http://www.safetydifferently.com/why-do-things-go-right/ https://www.quora.com/What-is-better-to-become-an-specialist-or-a-generalist-in-software-development-Does-the-full-stack-term-still-makes-sense https://cengizhan.com/3-pillars-of-observability-8e6cb5434206 https://www.thoughtworks.com/perspectives/edition1-agile-article https://blog.mexia.com.au/a-pace-layered-integration-architecture Presentations https://speakerdeck.com/tylertreat/the-future-of-ops https://www.slideshare.net/rheinwein/the-container-shame-spiral Articles https://dzone.com/articles/a-praise-for-self-service-in-it-value-streams http://blog.christianposta.com/microservices/application-safety-and-correctness-cannot-be-offloaded-to-istio-or-any-service-mesh/ https://www.gatesnotes.com/Books/Capitalism-Without-Capital?WT.mc_id=08_16_2018_06_CapitalismWithoutCapital_BG-LI_ WT.tsrc=BGLI linkId=55623312 https://uxdesign.cc/stop-delivering-software-with-agile-it-doesn-t-work-edccea3ab5d3 Concept of Shared Services and beyond Introduction to Observability by Weave Net Article on the state of Systems Languages Article on SILO's Blog on Twitter's Engineering Efficiency Why Companies should have a Heroku platform for their developers Multitasking is bad for your health Microsoft research on Developer's perception of productivity Developer Productivity Struggles You cannot measure productivity The Productivity Paradox There is no Productivity Paradox: it lags behind investments Economist: solving the paradox The Myth Of Developer Productivity Effectiveness vs. Efficiency Lean Manufactoring Theory of Constraints Thoughtworks: demystifying Conway's Law John Allspaw: a mature role for automation Research from DORA Mik Kersten - Cambrian Explosion of DevOps Tools Mik Kersten - End of Manufacturing Line Analogy Mik Kersten - Mining the ground thruth of Enterprise Toolchains Framework for putting Mental Models to practice Books The Goal The Phoenix Project Continuous Delivery The Lean Startup The Lean Enterprise DevOps Handbook Thinking Fast and Slow Sapiens Project to Product Debugging Teams The Trusted Advisor Crossing the Chasm Papers https://www.researchgate.net/publication/200085969_Using_task_context_to_improve_programmer_productivity/link/540dce6b0cf2df04e75676d0/download https://storage.googleapis.com/pub-tools-public-publication-data/pdf/3d102e42ad79a345ebd6464047ac9a6cd10670f4.pdf On Writing https://www.proofreadingservices.com/pages/very References Conway's law in wikipedia No Silver Bullet - F. Brooks The Lean Startup Principles","title":"General"},{"location":"productivity/#developer-productivity","text":"","title":"Developer Productivity"},{"location":"productivity/#commoditization","text":"\"The big change has been in the hardware/software cost ratio. The buyer of a $2 million machine in 1960 felt that he could afford $250,000 more ofr a customized payroll program, one that slipped easily and nondisruptively into the computer-hostile social environment. Buyers of %50,000 office machines today cannot conceivably afford customized payroll programs; so they adapt their paryoll procedures to the packages available.\" - 2 F. Brooks - No Silver Bullet","title":"Commoditization"},{"location":"productivity/#where-should-productivity-be-sought","text":"If you're looking to increase productivity, it would be best to answer some fundamental questions first. What should we be productive in? What is productivity? How do you measure productivity? The first step is to determine, what you should be productive in. If you're building software for example, it is in finding out what to build. \"The hardest single part of building a software system is deciding precisely what to build.\" - F. Brooks 2 That is actually already one step to far, as you would need a reason to build a software system. So the first step for any individual or organization (start up, or otherwise) is to find out what people want that you can offer. \"The fundamental activity of a startup is to turn ideas into products, measure how customers respond, and then learn whether to pivot or persevere. All successful startup processes should be geared to accelerate that feedback loop.\" - The Lean Startup 3","title":"Where should productivity be sought"},{"location":"productivity/#how-do-you-measure-productivity","text":"","title":"How do you measure productivity"},{"location":"productivity/#grow-vs-build","text":"","title":"Grow v.s. Build"},{"location":"productivity/#the-balancing-act-between-centralized-and-decentralized","text":"","title":"The Balancing act between centralized and decentralized"},{"location":"productivity/#on-multitasking","text":"Deep Work Attention Residue Why is it so hard to do my work","title":"On Multitasking"},{"location":"productivity/#learning-from-leantoyota","text":"","title":"Learning from Lean/Toyota"},{"location":"productivity/#open-space-floor-plans","text":"http://rstb.royalsocietypublishing.org/content/373/1753/20170239","title":"Open Space Floor Plans"},{"location":"productivity/#conways-law","text":"\"organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations.\" - M. Conway 1","title":"Conway's Law"},{"location":"productivity/#undifferentiated-heavy-lifting","text":"\"Work that needs to get done, but having it done doesn't bring our customers any direct benefit.\" - Dave Hahn","title":"Undifferentiated Heavy Lifting"},{"location":"productivity/#agile","text":"https://www.infoq.com/articles/agile-agile-blah-blah/ https://www.infoq.com/articles/death-agile-beyond","title":"Agile"},{"location":"productivity/#further-reading","text":"","title":"Further reading"},{"location":"productivity/#others","text":"https://www.youtube.com/watch?v=UTKIT6STSVM https://en.wikipedia.org/wiki/Complex_adaptive_system https://jobs.netflix.com/culture http://blackswanfarming.com/cost-of-delay/ https://www.rundeck.com/blog/tickets_make_operations_unnecessarily_miserable https://www.digitalocean.com/community/tutorials/what-is-immutable-infrastructure https://hbr.org/2015/12/what-the-research-tells-us-about-team-creativity-and-innovation https://www.thoughtworks.com/insights/blog/continuous-improvement-safe-environment https://qualitysafety.bmj.com/content/13/suppl_2/ii22 https://www.plutora.com/wp-content/uploads/dlm_uploads/2018/03/StateOfDevOpsTools_v14.pdf https://medium.com/@ATavgen/never-fail-twice-608147cb49b https://blogs.dropbox.com/dropbox/2018/07/study-high-performing-teams/?_tk=social oqa=183tl01liov linkId=100000003064606 http://psycnet.apa.org/record/1979-28632-001 https://pdfs.semanticscholar.org/a85d/432f44e43d61753bb8a121c246127b562a39.pdf https://medium.com/@dr_eprice/laziness-does-not-exist-3af27e312d01 https://en.wikipedia.org/wiki/Mindset#Fixed_and_growth http://www.reinventingorganizationswiki.com/Teal_Organizations https://www.mckinsey.com/business-functions/organization/our-insights/the-irrational-side-of-change-management https://www.barrypopik.com/index.php/new_york_city/entry/how_do_you_eat_an_elephant https://kadavy.net/blog/posts/mind-management-intro/ https://en.wikipedia.org/wiki/Planning_fallacy https://stories.lemonade.com/lemonade-proves-trust-pays-off-big-time-fdcf587af5a1 https://www.venturi-group.com/developer-to-cto/ https://dzone.com/articles/an-introduction-to-devops-principles https://www.thoughtworks.com/insights/blog/evolving-thoughtworks-internal-it-solve-broader-cross-cutting-problems https://www.thoughtworks.com/insights/blog/platform-tech-strategy-three-layers https://www.thoughtworks.com/insights/blog/why-it-departments-must-reinvent-themselves-part-1 https://en.wikipedia.org/wiki/Peter_principle https://hackernoon.com/why-all-engineers-must-understand-management-the-view-from-both-ladders-cc749ae14905 https://medium.freecodecamp.org/cognitive-bias-and-why-performance-management-is-so-hard-8852a1b874cd https://en.wikipedia.org/wiki/Horn_effect https://en.wikipedia.org/wiki/Halo_effect http://serendipstudio.org/bb/neuro/neuro02/web2/hhochman.html https://betterhumans.coach.me/how-to-be-a-better-manager-by-understanding-the-difference-between-market-norms-and-social-norms-3082d97d440f https://skillsmatter.com/skillscasts/10466-deep-dive-on-kubernetes-networking https://purplegriffon.com/blog/is-itil-agile-enough https://launchdarkly.com/blog/progressive-delivery-a-history-condensed/ http://www.collaborativefund.com/blog/real-world-vs-book-knowledge/ https://blog.codeship.com/using-jx-create-gitops-managed-jenkins-x-installation-cloudbees-codeship-terraform-google-kubernetes-engine/ https://martinfowler.com/articles/serverless.html https://www.quora.com/Some-people-including-the-creator-of-C-claim-that-there-is-a-huge-decline-of-quality-among-software-developers-What-seems-to-be-the-main-cause https://medium.com/netflix-techblog/full-cycle-developers-at-netflix-a08c31f83249 https://queue.acm.org/detail.cfm?id=3182626 http://www.safetydifferently.com/why-do-things-go-right/ https://www.quora.com/What-is-better-to-become-an-specialist-or-a-generalist-in-software-development-Does-the-full-stack-term-still-makes-sense https://cengizhan.com/3-pillars-of-observability-8e6cb5434206 https://www.thoughtworks.com/perspectives/edition1-agile-article https://blog.mexia.com.au/a-pace-layered-integration-architecture","title":"Others"},{"location":"productivity/#presentations","text":"https://speakerdeck.com/tylertreat/the-future-of-ops https://www.slideshare.net/rheinwein/the-container-shame-spiral","title":"Presentations"},{"location":"productivity/#articles","text":"https://dzone.com/articles/a-praise-for-self-service-in-it-value-streams http://blog.christianposta.com/microservices/application-safety-and-correctness-cannot-be-offloaded-to-istio-or-any-service-mesh/ https://www.gatesnotes.com/Books/Capitalism-Without-Capital?WT.mc_id=08_16_2018_06_CapitalismWithoutCapital_BG-LI_ WT.tsrc=BGLI linkId=55623312 https://uxdesign.cc/stop-delivering-software-with-agile-it-doesn-t-work-edccea3ab5d3 Concept of Shared Services and beyond Introduction to Observability by Weave Net Article on the state of Systems Languages Article on SILO's Blog on Twitter's Engineering Efficiency Why Companies should have a Heroku platform for their developers Multitasking is bad for your health Microsoft research on Developer's perception of productivity Developer Productivity Struggles You cannot measure productivity The Productivity Paradox There is no Productivity Paradox: it lags behind investments Economist: solving the paradox The Myth Of Developer Productivity Effectiveness vs. Efficiency Lean Manufactoring Theory of Constraints Thoughtworks: demystifying Conway's Law John Allspaw: a mature role for automation Research from DORA Mik Kersten - Cambrian Explosion of DevOps Tools Mik Kersten - End of Manufacturing Line Analogy Mik Kersten - Mining the ground thruth of Enterprise Toolchains Framework for putting Mental Models to practice","title":"Articles"},{"location":"productivity/#books","text":"The Goal The Phoenix Project Continuous Delivery The Lean Startup The Lean Enterprise DevOps Handbook Thinking Fast and Slow Sapiens Project to Product Debugging Teams The Trusted Advisor Crossing the Chasm","title":"Books"},{"location":"productivity/#papers","text":"https://www.researchgate.net/publication/200085969_Using_task_context_to_improve_programmer_productivity/link/540dce6b0cf2df04e75676d0/download https://storage.googleapis.com/pub-tools-public-publication-data/pdf/3d102e42ad79a345ebd6464047ac9a6cd10670f4.pdf","title":"Papers"},{"location":"productivity/#on-writing","text":"https://www.proofreadingservices.com/pages/very","title":"On Writing"},{"location":"productivity/#references","text":"Conway's law in wikipedia No Silver Bullet - F. Brooks The Lean Startup Principles","title":"References"},{"location":"productivity/incidents/","text":"Incidents incidents five why's blameless postmortems identify causes (not culprits) assume good will take your time dangers of automation observability human bias human factors percententize work how much percent of work should be what?m figure out how to track, make it easy/automated identify real vs. desired, figure out how to get (closer) to desired \"Just Culture\" (as in, justice, it is just) bad apple theory = debunked bad apple theory = remove the small percentage of bad apples and the problem goes away Notes most incidents happen near updates/upgrades/deployments make deployments a non-event small increments, high frequency, automated, tested it is not systems, it not humans, but humans within systems human will make mistakes processes are (almost) always part of the problem automation needs to include sanity checks (ranges of sane values) References Below is a significant collection of references to resources that tackle the different parts of incident management. They can explain it better than I ever can, so use them to better your own understanding just as I have. Books Foundations of Safety Science Code Complete Talks Ironies Of Automation Google SRE: Postmortems and Retrospectives John Allspaw: Blameless Post Mortems Papers Patient Safety and the \"Just Culture\" - Marx D How do systems manage their adaptive capacity to successfully handle disruptions - M Branlat D Woods Ironies Of Automation - Lisanne Bainbridge Managing The Development Of Large Software Systems - Winston Royce Articles Weathering The Unexpected - Kripa Krishnan John Allspaw: a mature role for automation John Allspaw: Resillience Engineering: Part I John Allspaw: getting the messy details is critical John Allspaw: Ask Me Anything John Allspaw: Blameless PostMortems And A Just Culture Etsy's Postmortem Proces Etsy's Winning Secret: Don't Play The Blame Game Blameless Portmortems at Etsy Google SRE - Postmortem Culture: Learning from Failure HoneyComb.io - Incident Review GitHub Outage Incident Analysis Google Postmortem AWS Postmortem (S3 outage) GitHub Page Listing Public Post Mortems Charity Majors: I Test In Prod The Network Is Reliable: an informal survey of real-world communications failures Charity Majors: Shipping Software Should Not Be Scary Circle CI: A brief history of devops part I: waterfall Circle CI: A brief history of devops part II: agile development Circle CI: A brief history of devops part III: automated testing and continuous integration Circle CI: A brief history of devops part IV: continuous delivery and deployment","title":"Incidents"},{"location":"productivity/incidents/#incidents","text":"incidents five why's blameless postmortems identify causes (not culprits) assume good will take your time dangers of automation observability human bias human factors percententize work how much percent of work should be what?m figure out how to track, make it easy/automated identify real vs. desired, figure out how to get (closer) to desired \"Just Culture\" (as in, justice, it is just) bad apple theory = debunked bad apple theory = remove the small percentage of bad apples and the problem goes away","title":"Incidents"},{"location":"productivity/incidents/#notes","text":"most incidents happen near updates/upgrades/deployments make deployments a non-event small increments, high frequency, automated, tested it is not systems, it not humans, but humans within systems human will make mistakes processes are (almost) always part of the problem automation needs to include sanity checks (ranges of sane values)","title":"Notes"},{"location":"productivity/incidents/#references","text":"Below is a significant collection of references to resources that tackle the different parts of incident management. They can explain it better than I ever can, so use them to better your own understanding just as I have.","title":"References"},{"location":"productivity/incidents/#books","text":"Foundations of Safety Science Code Complete","title":"Books"},{"location":"productivity/incidents/#talks","text":"Ironies Of Automation Google SRE: Postmortems and Retrospectives John Allspaw: Blameless Post Mortems","title":"Talks"},{"location":"productivity/incidents/#papers","text":"Patient Safety and the \"Just Culture\" - Marx D How do systems manage their adaptive capacity to successfully handle disruptions - M Branlat D Woods Ironies Of Automation - Lisanne Bainbridge Managing The Development Of Large Software Systems - Winston Royce","title":"Papers"},{"location":"productivity/incidents/#articles","text":"Weathering The Unexpected - Kripa Krishnan John Allspaw: a mature role for automation John Allspaw: Resillience Engineering: Part I John Allspaw: getting the messy details is critical John Allspaw: Ask Me Anything John Allspaw: Blameless PostMortems And A Just Culture Etsy's Postmortem Proces Etsy's Winning Secret: Don't Play The Blame Game Blameless Portmortems at Etsy Google SRE - Postmortem Culture: Learning from Failure HoneyComb.io - Incident Review GitHub Outage Incident Analysis Google Postmortem AWS Postmortem (S3 outage) GitHub Page Listing Public Post Mortems Charity Majors: I Test In Prod The Network Is Reliable: an informal survey of real-world communications failures Charity Majors: Shipping Software Should Not Be Scary Circle CI: A brief history of devops part I: waterfall Circle CI: A brief history of devops part II: agile development Circle CI: A brief history of devops part III: automated testing and continuous integration Circle CI: A brief history of devops part IV: continuous delivery and deployment","title":"Articles"},{"location":"productivity/intellij/","text":"Intelli J Beneficial OS changes Linux Increase inotify watches \\","title":"Intelli J"},{"location":"productivity/intellij/#intelli-j","text":"","title":"Intelli J"},{"location":"productivity/intellij/#beneficial-os-changes","text":"","title":"Beneficial OS changes"},{"location":"productivity/intellij/#linux","text":"Increase inotify watches \\","title":"Linux"},{"location":"productivity/paradigms/","text":"Paradigms Product centered Resources","title":"Paradigms"},{"location":"productivity/paradigms/#paradigms","text":"","title":"Paradigms"},{"location":"productivity/paradigms/#product-centered","text":"","title":"Product centered"},{"location":"productivity/paradigms/#resources","text":"","title":"Resources"},{"location":"productivity/remote/","text":"Working Remote Resources https://open.nytimes.com/how-to-grow-as-an-engineer-working-remotely-3baff8211f3e","title":"Working Remote"},{"location":"productivity/remote/#working-remote","text":"","title":"Working Remote"},{"location":"productivity/remote/#resources","text":"https://open.nytimes.com/how-to-grow-as-an-engineer-working-remotely-3baff8211f3e","title":"Resources"},{"location":"productivity/studies/","text":"Developer Productivity Studies","title":"Developer Productivity Studies"},{"location":"productivity/studies/#developer-productivity-studies","text":"","title":"Developer Productivity Studies"},{"location":"productivity/team-development/","text":"Team Development Resources Article about Tuckman's theory of group development Wiki of Tuckman's theory of group development How google thinks about Team Effectiveness Google Re:Work tutorial on Team Effectiveness https://charity.wtf/2018/12/02/software-sprawl-the-golden-path-and-scaling-teams-with-agency/","title":"Team Development"},{"location":"productivity/team-development/#team-development","text":"","title":"Team Development"},{"location":"productivity/team-development/#resources","text":"Article about Tuckman's theory of group development Wiki of Tuckman's theory of group development How google thinks about Team Effectiveness Google Re:Work tutorial on Team Effectiveness https://charity.wtf/2018/12/02/software-sprawl-the-golden-path-and-scaling-teams-with-agency/","title":"Resources"},{"location":"productivity/tools/","text":"Developer Productivity Tools","title":"Developer Productivity Tools"},{"location":"productivity/tools/#developer-productivity-tools","text":"","title":"Developer Productivity Tools"},{"location":"swe/API/","text":"API's https://docs.microsoft.com/en-us/azure/architecture/best-practices/api-design","title":"API's"},{"location":"swe/API/#apis","text":"https://docs.microsoft.com/en-us/azure/architecture/best-practices/api-design","title":"API's"},{"location":"swe/algorithms/","text":"","title":"Algorithms"},{"location":"swe/ddd/","text":"Domain Driven Design","title":"Domain Driven Design"},{"location":"swe/ddd/#domain-driven-design","text":"","title":"Domain Driven Design"},{"location":"swe/distributed/","text":"Distributed Computing Distributed Computing fundamentals Time and Event ordering See: Lamport timestamp Distributed Applications Topics to take into account logging structured pulled into central log service Java: SLF4J + LogBack? Go: logrus tracing sampling based metrics prometheus including alert definitions network connection stability services discovery loadbalancing circuit brakers backpressure shallow queues connection pools dynamic/randomized backoff procedures network connection performance 3-step handshake binary over http standard protocols thin wrapper for UI: GraphQL thick wrapper for UI: JSON over HTTP (restful) Service to Service: gRPC / twirp Designing Distributed Systems - Brandon Burns Sidecar pattern 1 docker run -d my-app-image After you run that image, you will receive the identifier for that specific container. It will look something like: cccf82b85000... If you don\u2019t have it, you can always look it up using the docker ps command, which will show all currently running containers. Assuming you have stashed that value in an environment variable named APP_ID, you can then run the topz container in the same PID namespace using: 1 docker run --pid = container: ${ APP_ID } \\ -p 8080 :8080 brendanburns/topz:db0fa58 /server --address = 0 .0.0.0:8080 Resources Coursera course Article on synchronization in a distributed system http://label-schema.org/ https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236 https://eng.lyft.com/announcing-envoy-c-l7-proxy-and-communication-bus-92520b6c8191 https://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc https://cse.buffalo.edu/~demirbas/publications/cloudConsensus.pdf http://www.read.seas.harvard.edu/~kohler/class/08w-dsi/chandra07paxos.pdf https://medium.com/source-code/understanding-the-memcached-source-code-slab-i-9199de613762","title":"Distributed Computing"},{"location":"swe/distributed/#distributed-computing","text":"","title":"Distributed Computing"},{"location":"swe/distributed/#distributed-computing-fundamentals","text":"","title":"Distributed Computing fundamentals"},{"location":"swe/distributed/#time-and-event-ordering","text":"See: Lamport timestamp","title":"Time and Event ordering"},{"location":"swe/distributed/#distributed-applications","text":"","title":"Distributed Applications"},{"location":"swe/distributed/#topics-to-take-into-account","text":"logging structured pulled into central log service Java: SLF4J + LogBack? Go: logrus tracing sampling based metrics prometheus including alert definitions network connection stability services discovery loadbalancing circuit brakers backpressure shallow queues connection pools dynamic/randomized backoff procedures network connection performance 3-step handshake binary over http standard protocols thin wrapper for UI: GraphQL thick wrapper for UI: JSON over HTTP (restful) Service to Service: gRPC / twirp","title":"Topics to take into account"},{"location":"swe/distributed/#designing-distributed-systems-brandon-burns","text":"","title":"Designing Distributed Systems - Brandon Burns"},{"location":"swe/distributed/#sidecar-pattern","text":"1 docker run -d my-app-image After you run that image, you will receive the identifier for that specific container. It will look something like: cccf82b85000... If you don\u2019t have it, you can always look it up using the docker ps command, which will show all currently running containers. Assuming you have stashed that value in an environment variable named APP_ID, you can then run the topz container in the same PID namespace using: 1 docker run --pid = container: ${ APP_ID } \\ -p 8080 :8080 brendanburns/topz:db0fa58 /server --address = 0 .0.0.0:8080","title":"Sidecar pattern"},{"location":"swe/distributed/#resources","text":"Coursera course Article on synchronization in a distributed system http://label-schema.org/ https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236 https://eng.lyft.com/announcing-envoy-c-l7-proxy-and-communication-bus-92520b6c8191 https://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc https://cse.buffalo.edu/~demirbas/publications/cloudConsensus.pdf http://www.read.seas.harvard.edu/~kohler/class/08w-dsi/chandra07paxos.pdf https://medium.com/source-code/understanding-the-memcached-source-code-slab-i-9199de613762","title":"Resources"},{"location":"swe/http-caching/","text":"HTTP Caching https://medium.freecodecamp.org/http-caching-in-depth-part-1-a853c6af99db","title":"HTTP Caching"},{"location":"swe/http-caching/#http-caching","text":"https://medium.freecodecamp.org/http-caching-in-depth-part-1-a853c6af99db","title":"HTTP Caching"},{"location":"swe/important-concepts/","text":"Software Engineering Concepts Resource management When there are finite resources in your system, manage them explicitly. Be that memory, CPU, amount of connections to a database or incoming http connections. Back Pressure Back pressure When one component is struggling to keep-up, the system as a whole needs to respond in a sensible way. It is unacceptable for the component under stress to fail catastrophically or to drop messages in an uncontrolled fashion. Since it can\u2019t cope and it can\u2019t fail it should communicate the fact that it is under stress to upstream components and so get them to reduce the load. This back-pressure is an important feedback mechanism that allows systems to gracefully respond to load rather than collapse under it. The back-pressure may cascade all the way up to the user, at which point responsiveness may degrade, but this mechanism will ensure that the system is resilient under load, and will provide information that may allow the system itself to apply other resources to help distribute the load, see Elasticity. 1 Further reading: DZone article Spotify Engineering Memoization Memoization In computing, memoization or memoisation is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again. Memoization has also been used in other contexts (and for purposes other than speed gains), such as in simple mutually recursive descent parsing. Important Theories Theory of constraints Law of demeter Conway's law Little's law Commoditization Amdahl's Law Web Technologies HTTP Caching https://medium.freecodecamp.org/http-caching-in-depth-part-1-a853c6af99db Reactive Manifesto Wikipedia article on Memoization","title":"Important Concepts"},{"location":"swe/important-concepts/#software-engineering-concepts","text":"","title":"Software Engineering Concepts"},{"location":"swe/important-concepts/#resource-management","text":"When there are finite resources in your system, manage them explicitly. Be that memory, CPU, amount of connections to a database or incoming http connections.","title":"Resource management"},{"location":"swe/important-concepts/#back-pressure","text":"Back pressure When one component is struggling to keep-up, the system as a whole needs to respond in a sensible way. It is unacceptable for the component under stress to fail catastrophically or to drop messages in an uncontrolled fashion. Since it can\u2019t cope and it can\u2019t fail it should communicate the fact that it is under stress to upstream components and so get them to reduce the load. This back-pressure is an important feedback mechanism that allows systems to gracefully respond to load rather than collapse under it. The back-pressure may cascade all the way up to the user, at which point responsiveness may degrade, but this mechanism will ensure that the system is resilient under load, and will provide information that may allow the system itself to apply other resources to help distribute the load, see Elasticity. 1 Further reading: DZone article Spotify Engineering","title":"Back Pressure"},{"location":"swe/important-concepts/#memoization","text":"Memoization In computing, memoization or memoisation is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again. Memoization has also been used in other contexts (and for purposes other than speed gains), such as in simple mutually recursive descent parsing.","title":"Memoization"},{"location":"swe/important-concepts/#important-theories","text":"Theory of constraints Law of demeter Conway's law Little's law Commoditization Amdahl's Law","title":"Important Theories"},{"location":"swe/important-concepts/#web-technologies","text":"","title":"Web Technologies"},{"location":"swe/important-concepts/#http-caching","text":"https://medium.freecodecamp.org/http-caching-in-depth-part-1-a853c6af99db Reactive Manifesto Wikipedia article on Memoization","title":"HTTP Caching"},{"location":"swe/microservices/","text":"Microservices","title":"Microservices"},{"location":"swe/microservices/#microservices","text":"","title":"Microservices"},{"location":"swe/naming/","text":"On Naming There are only two hard things in Computer Science: cache invalidation and naming things. -- Phil Karlton There are 2 hard problems in computer science: cache invalidation, naming things, and off-by-1 errors. - ? Naming things is hard, but very important. Naming things correctly conveys meaning, helps us understand and reduces cognitive load . Talks Peter Hilton: How To Name Things: The Hardest Problem In Programming Jimmy Bogard: Domain Driven Design: The Good Parts Slides Peter Hilto: Naming Guidelines For Professional Programmers Books Domain Driven Design - Eric Evans Clean Code Collection - Robert C Martin Other Martin Fowler - Two Hard Things Martin Fowler - Ubiquitous Language Industry Specific Dictionaries Clean Coders Peter Hilton - Why Naming Things Is Hard","title":"Naming"},{"location":"swe/naming/#on-naming","text":"There are only two hard things in Computer Science: cache invalidation and naming things. -- Phil Karlton There are 2 hard problems in computer science: cache invalidation, naming things, and off-by-1 errors. - ? Naming things is hard, but very important. Naming things correctly conveys meaning, helps us understand and reduces cognitive load .","title":"On Naming"},{"location":"swe/naming/#talks","text":"Peter Hilton: How To Name Things: The Hardest Problem In Programming Jimmy Bogard: Domain Driven Design: The Good Parts","title":"Talks"},{"location":"swe/naming/#slides","text":"Peter Hilto: Naming Guidelines For Professional Programmers","title":"Slides"},{"location":"swe/naming/#books","text":"Domain Driven Design - Eric Evans Clean Code Collection - Robert C Martin","title":"Books"},{"location":"swe/naming/#other","text":"Martin Fowler - Two Hard Things Martin Fowler - Ubiquitous Language Industry Specific Dictionaries Clean Coders Peter Hilton - Why Naming Things Is Hard","title":"Other"},{"location":"swe/observability/","text":"Observability Resources https://www.vividcortex.com/blog/monitoring-isnt-observability https://medium.com/@copyconstruct/monitoring-and-observability-8417d1952e1c https://codeascraft.com/2011/02/15/measure-anything-measure-everything/","title":"Observability"},{"location":"swe/observability/#observability","text":"","title":"Observability"},{"location":"swe/observability/#resources","text":"https://www.vividcortex.com/blog/monitoring-isnt-observability https://medium.com/@copyconstruct/monitoring-and-observability-8417d1952e1c https://codeascraft.com/2011/02/15/measure-anything-measure-everything/","title":"Resources"},{"location":"swe/others/","text":"Software Engineering Concepts Resource management When there are finite resources in your system, manage them explicitly. Be that memory, CPU, amount of connections to a database or incoming http connections. Back Pressure Back pressure When one component is struggling to keep-up, the system as a whole needs to respond in a sensible way. It is unacceptable for the component under stress to fail catastrophically or to drop messages in an uncontrolled fashion. Since it can\u2019t cope and it can\u2019t fail it should communicate the fact that it is under stress to upstream components and so get them to reduce the load. This back-pressure is an important feedback mechanism that allows systems to gracefully respond to load rather than collapse under it. The back-pressure may cascade all the way up to the user, at which point responsiveness may degrade, but this mechanism will ensure that the system is resilient under load, and will provide information that may allow the system itself to apply other resources to help distribute the load, see Elasticity. 1 Further reading: DZone article Spotify Engineering Memoization Memoization In computing, memoization or memoisation is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again. Memoization has also been used in other contexts (and for purposes other than speed gains), such as in simple mutually recursive descent parsing. Important Theories Theory of constraints Law of demeter Conway's law Little's law Commoditization Amdahl's Law Reactive Manifesto Wikipedia article on Memoization","title":"Software Engineering Concepts"},{"location":"swe/others/#software-engineering-concepts","text":"","title":"Software Engineering Concepts"},{"location":"swe/others/#resource-management","text":"When there are finite resources in your system, manage them explicitly. Be that memory, CPU, amount of connections to a database or incoming http connections.","title":"Resource management"},{"location":"swe/others/#back-pressure","text":"Back pressure When one component is struggling to keep-up, the system as a whole needs to respond in a sensible way. It is unacceptable for the component under stress to fail catastrophically or to drop messages in an uncontrolled fashion. Since it can\u2019t cope and it can\u2019t fail it should communicate the fact that it is under stress to upstream components and so get them to reduce the load. This back-pressure is an important feedback mechanism that allows systems to gracefully respond to load rather than collapse under it. The back-pressure may cascade all the way up to the user, at which point responsiveness may degrade, but this mechanism will ensure that the system is resilient under load, and will provide information that may allow the system itself to apply other resources to help distribute the load, see Elasticity. 1 Further reading: DZone article Spotify Engineering","title":"Back Pressure"},{"location":"swe/others/#memoization","text":"Memoization In computing, memoization or memoisation is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again. Memoization has also been used in other contexts (and for purposes other than speed gains), such as in simple mutually recursive descent parsing.","title":"Memoization"},{"location":"swe/others/#important-theories","text":"Theory of constraints Law of demeter Conway's law Little's law Commoditization Amdahl's Law Reactive Manifesto Wikipedia article on Memoization","title":"Important Theories"}]}