{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Joost van der Griendt's Software Development Docs \u00b6 This is a collection of knowledge that I have gathered over the years. I find it helps me learn better if I write it down, and often use the docs at different customers as reference. Call me J \u00b6 My full name is Joost van der Griendt , which unfortunately cannot be pronounced well in English. In order to facilitate non-Dutch speakers, you can refer to me as J (Jay). I've worked as a Java developer in the past, but currently I'm employed as a Consultant at CloudBees . My day-to-day work involves helping clients with CI/CD, Kubernetes and Software Development Management . Or, in simple words, how to make it easy and less painful to get software to customers/clients that they want to pay for at scale. In my spare time I keep my development skills active by developing in Go and Java mostly. But I'm also a big fan of automating the creation and management of CI/CD (self-service) platforms. I'm a big fan of Open Source Software and when it makes sense, Free & Free software. Which is also why this site is completely open, and open source as well. Info Curious how this site is build? Read my explanation here Tracker \u00b6 Your browser will tell you there's a tracker. I'm curious to understand if people are reading my docs and if so, which pages. Feel free to block the tracker (Google Analytics), most browsers are able to do so. Main Topics \u00b6 CI/CD (Continuous Integration / Continous Delivery) Jenkins Jenkins X CloudBees Products (my current employer as of 2018) Containers (Docker, Kubernetes, ...) SWE : Software Engineering in all its facets (building, maintaining, social aspects, psychology, etc.) Other Docs \u00b6 Breakdown of a Spring Boot + ReactJS Application Continuous Integration \u00b6 A good definition can be found here: http://www.martinfowler.com/articles/continuousIntegration.html Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\" Continuous Delivery \u00b6 Continuous Delivery/deployment is the next step in getting yr software changes at the desired server in order to let your clients take a look at it. This article provides a good example of it: http://www.martinfowler.com/articles/continuousIntegration.html To do Continuous Integration you need multiple environments, one to run commit tests, one or more to run secondary tests. Since you are moving executables between these environments multiple times a day, you'll want to do this automatically. So it's important to have scripts that will allow you to deploy the application into any environment easily.","title":"Me"},{"location":"#joost-van-der-griendts-software-development-docs","text":"This is a collection of knowledge that I have gathered over the years. I find it helps me learn better if I write it down, and often use the docs at different customers as reference.","title":"Joost van der Griendt's Software Development Docs"},{"location":"#call-me-j","text":"My full name is Joost van der Griendt , which unfortunately cannot be pronounced well in English. In order to facilitate non-Dutch speakers, you can refer to me as J (Jay). I've worked as a Java developer in the past, but currently I'm employed as a Consultant at CloudBees . My day-to-day work involves helping clients with CI/CD, Kubernetes and Software Development Management . Or, in simple words, how to make it easy and less painful to get software to customers/clients that they want to pay for at scale. In my spare time I keep my development skills active by developing in Go and Java mostly. But I'm also a big fan of automating the creation and management of CI/CD (self-service) platforms. I'm a big fan of Open Source Software and when it makes sense, Free & Free software. Which is also why this site is completely open, and open source as well. Info Curious how this site is build? Read my explanation here","title":"Call me J"},{"location":"#tracker","text":"Your browser will tell you there's a tracker. I'm curious to understand if people are reading my docs and if so, which pages. Feel free to block the tracker (Google Analytics), most browsers are able to do so.","title":"Tracker"},{"location":"#main-topics","text":"CI/CD (Continuous Integration / Continous Delivery) Jenkins Jenkins X CloudBees Products (my current employer as of 2018) Containers (Docker, Kubernetes, ...) SWE : Software Engineering in all its facets (building, maintaining, social aspects, psychology, etc.)","title":"Main Topics"},{"location":"#other-docs","text":"Breakdown of a Spring Boot + ReactJS Application","title":"Other Docs"},{"location":"#continuous-integration","text":"A good definition can be found here: http://www.martinfowler.com/articles/continuousIntegration.html Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\"","title":"Continuous Integration"},{"location":"#continuous-delivery","text":"Continuous Delivery/deployment is the next step in getting yr software changes at the desired server in order to let your clients take a look at it. This article provides a good example of it: http://www.martinfowler.com/articles/continuousIntegration.html To do Continuous Integration you need multiple environments, one to run commit tests, one or more to run secondary tests. Since you are moving executables between these environments multiple times a day, you'll want to do this automatically. So it's important to have scripts that will allow you to deploy the application into any environment easily.","title":"Continuous Delivery"},{"location":"blogs/docker-alternatives/","text":"Pipelines With Docker Alternatives \u00b6 Building pipelines with Jenkins on Docker has been common for a while. But accessing the docker socket has always been a bit tricky. The easiest solution is directly mounting the docker socket of the host into your build container. However, this is a big security risk and something that is undesirable at best and, pretty dangerous at worst. When you're using an orchestrator such as Kubernetes - which is where Jenkins is currently best put to work - the problem gets worse. Not only do you open up security holes, you also mess up the schedular's insight into available resources, and mess with it's ability to keep your cluster in a correct state. In some sense, using docker directly in a Kubernetes cluster, is likened to running with scissors . Potential Alternatives \u00b6 So, directly running docker containers via a docker engine inside your cluster is out. Let's look at some alternatives. Kubernetes Pod and External Node : the simple way out, use a cloud (such as EC2 Cloud) to provision a classic VM with docker engine as an agent and build there JIB : tool from Google, only works for Java applications and is supported directly from Gradle and Maven - Link Kaniko : tool from Google, works as 1-on-1 replacement for docker image build (except Windows Containers ) - Link IMG : tool from Jess Frazelle to avoid building docker images with a root user involved Link Kubernetes Pod and External Node \u00b6 One of the most used cloud environments is AWS, so I created this solution with AWS's Amazon EC2 Plugin . Warning Unfortunately, you cannot combine the Kubernetes plugin with external nodes (none pod container nodes) in a Declarative pipeline. So you have to use Scripted . This can be done with various different cloud providers such as Digital Ocean, Google, AWS or Azure. This guide will use AWS, as it has the most mature Jenkins Cloud plugin. Prerequisites \u00b6 AWS Account with rights to create AMI's and run EC2 instances Packer Jenkins with Amazon EC2 Plugin installed Steps \u00b6 create AMI with Packer install and configure Amazon EC2 plugin create a test pipeline Create AMI with Packer \u00b6 Packer needs to be able to access EC2 API's and be able to spin up an EC2 instance and create an AMI out of it. AWS setup for Packer \u00b6 You need to configure two things: account details for Packer to use security group where your EC2 instances will be running with this security group needs to open port 22 both Packer and Jenkins will use this for their connection export AWS_DEFAULT_REGION = eu-west-1 export AWS_ACCESS_KEY_ID = XXX export AWS_SECRET_ACCESS_KEY = XXX aws ec2 --profile myAwsProfile create-security-group \\ --description \"For building Docker images\" \\ --group-name docker { \"GroupId\" : \"sg-08079f78cXXXXXXX\" } Export the security group ID. export SG_ID = sg-08079f78cXXXXXXX echo $SG_ID Enable port 22 \u00b6 aws ec2 \\ --profile myAwsProfile \\ authorize-security-group-ingress \\ --group-name docker \\ --protocol tcp \\ --port 22 \\ --cidr 0 .0.0.0/0 Packer AMI definition \u00b6 Here's an example definition for Packer for a Ubuntu 18.04 LTS base image with JDK 8 (required by Jenkins) and Docker. { \"builders\" : [{ \"type\" : \"amazon-ebs\" , \"region\" : \"eu-west-1\" , \"source_ami_filter\" : { \"filters\" : { \"virtualization-type\" : \"hvm\" , \"name\" : \"*ubuntu-bionic-18.04-amd64-server-*\" , \"root-device-type\" : \"ebs\" }, \"owners\" : [ \"679593333241\" ], \"most_recent\" : true }, \"instance_type\" : \"t2.micro\" , \"ssh_username\" : \"ubuntu\" , \"ami_name\" : \"docker\" , \"force_deregister\" : true }], \"provisioners\" : [{ \"type\" : \"shell\" , \"inline\" : [ \"sleep 15\" , \"sudo apt-get clean\" , \"sudo apt-get update\" , \"sudo apt-get install -y apt-transport-https ca-certificates nfs-common\" , \"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\" , \"sudo add-apt-repository \\\"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\\"\" , \"sudo add-apt-repository -y ppa:openjdk-r/ppa\" , \"sudo apt-get update\" , \"sudo apt-get install -y docker-ce\" , \"sudo usermod -aG docker ubuntu\" , \"sudo apt-get install -y openjdk-8-jdk\" , \"java -version\" , \"docker version\" ] }] } Build the new AMI with packer. packer build docker-ami.json export AMI = ami-0212ab37f84e418f4 EC2 Key Pair \u00b6 Create EC2 key pair, this will be used by Jenkins to connect to the instances via ssh. aws ec2 --profile myAwsProfile create-key-pair \\ --key-name jenkinsec2 \\ | jq -r '.KeyMaterial' \\ >jenkins-ec2-proton.pem EC2 Cloud Configuration \u00b6 In a Jenkins' master main configuration, you add a new cloud . In this case, we will use a ec2-cloud so we can instantiate our EC2 VM's with docker. use EC2 credentials for initial connection use key ( .pem ) for VM connection (jenkins <> agent) configure the following: AMI: ami-0212ab37f84e418f4 availability zone: eu-west-1a VPC SubnetID: subnet-aa54XXXX Remote user: ubuntu labels: docker ubuntu linux SecurityGroup Name: (the id) sg-08079f78cXXXXXXX public ip = true connect via public ip = true Pipeline \u00b6 @Library ( 'jenkins-pipeline-library@master' ) _ def scmVars def label = \"jenkins-slave-${UUID.randomUUID().toString()}\" podTemplate ( label: label , yaml: \"\"\" apiVersion: v1 kind: Pod spec: containers: - name: kubectl image: vfarcic/kubectl command: [\"cat\"] tty: true \"\"\" ) { node ( label ) { node ( \"docker\" ) { stage ( 'SCM & Prepare' ) { scmVars = checkout scm } stage ( 'Lint' ) { dockerfileLint () } stage ( 'Build Docker' ) { sh \"docker image build -t demo:rc-1 .\" } stage ( 'Tag & Push Docker' ) { IMAGE = \"${DOCKER_IMAGE_NAME}\" TAG = \"${DOCKER_IMAGE_TAG}\" FULL_NAME = \"${FULL_IMAGE_NAME}\" withCredentials ([ usernamePassword ( credentialsId: \"dockerhub\" , usernameVariable: \"USER\" , passwordVariable: \"PASS\" )]) { sh \"docker login -u $USER -p $PASS\" } sh \"docker image tag ${IMAGE}:${TAG} ${FULL_NAME}\" sh \"docker image push ${FULL_NAME}\" } } // end node docker stage ( 'Prepare Pod' ) { // have to checkout on our kubernetes pod aswell checkout scm } stage ( 'Check version' ) { container ( 'kubectl' ) { sh 'kubectl version' } } } // end node random label } // end pod def Maven JIB \u00b6 If you use Java with either Gradle or Maven , you can use JIB to create docker image without requiring a docker client or docker engine. JIB will communicate with DockerHub and Google's container registry for sending image layers back and forth. The last layer will be created by the JIB plugin itself, adding your self-executing Jar to the layer and a Entrypoint with the correct flags. For more information about the runtime options, see either jib-maven-plugin documentation or jib-gradle-plugin . Prerequisites \u00b6 Java project build with Gradle or Maven Java project that can start it self, such as Spring Boot or Thorntail (previously Wildfly Swarm, from the JBoss family) able to build either gradle or maven applications The project used can be found at github.com/demomon/maven-spring-boot-demo . Steps \u00b6 configure the plugin for either Gradle or Maven build using an official docker image via the kubernetes pod template Pipeline \u00b6 Using a bit more elaborate pipeline example here. Using SonarCloud for static code analysis and then JIB to create a docker image. We could then use that image either directly in anything that runs docker or in the same cluster via a Helm Chart. def scmVars def tag pipeline { options { buildDiscarder logRotator ( artifactDaysToKeepStr: '5' , artifactNumToKeepStr: '5' , daysToKeepStr: '5' , numToKeepStr: '5' ) } libraries { lib ( 'core@master' ) lib ( 'maven@master' ) } agent { kubernetes { label 'mypod' defaultContainer 'jnlp' yaml \"\"\" apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:3-jdk-11-slim command: - cat tty: true \"\"\" } } stages { stage ( 'Test versions' ) { steps { container ( 'maven' ) { sh 'uname -a' sh 'mvn -version' } } } stage ( 'Checkout' ) { steps { script { scmVars = checkout scm } gitRemoteConfigByUrl ( scmVars . GIT_URL , 'githubtoken' ) sh ''' git config --global user.email \"jenkins@jenkins.io\" git config --global user.name \"Jenkins\" ''' } } stage ( 'Build' ) { steps { container ( 'maven' ) { sh 'mvn clean verify -B -e' } } } stage ( 'Version & Analysis' ) { parallel { stage ( 'Version Bump' ) { when { branch 'master' } environment { NEW_VERSION = gitNextSemverTagMaven ( 'pom.xml' ) } steps { script { tag = \"${NEW_VERSION}\" } container ( 'maven' ) { sh 'mvn versions:set -DnewVersion=${NEW_VERSION}' } gitTag ( \"v${NEW_VERSION}\" ) } } stage ( 'Sonar Analysis' ) { when { branch 'master' } environment { SONAR_HOST = 'https://sonarcloud.io' KEY = 'spring-maven-demo' ORG = 'demomon' SONAR_TOKEN = credentials ( 'sonarcloud' ) } steps { container ( 'maven' ) { sh '''mvn sonar:sonar \\ -Dsonar.projectKey=${KEY} \\ -Dsonar.organization=${ORG} \\ -Dsonar.host.url=${SONAR_HOST} \\ -Dsonar.login=${SONAR_TOKEN} ''' } } } } } stage ( 'Publish Artifact' ) { when { branch 'master' } environment { DHUB = credentials ( 'dockerhub' ) } steps { container ( 'maven' ) { // we should never come here if the tests have not run, as we run verify before sh 'mvn clean compile -B -e jib:build -Djib.to.auth.username=${DHUB_USR} -Djib.to.auth.password=${DHUB_PSW} -DskipTests' } } } } post { always { cleanWs () } } } Kaniko \u00b6 Google loves Kubernetes and Google prefers people building docker images in Kubernetes without Docker. As JIB is only available for Java projects, there's needs to be an alternative for any other usecase/programming language. The answer to that is Kaniko , a specialized Docker image to create Docker images. Kaniko isn't the most secure way to create docker images, it barely beats mounting a Docker Socket, or might even be worse if you ask others (such as Jess Frazelle ). That said, it is gaining some traction being used by JenkinsX and having an example in Jenkins' Kubernetes Plugin . Info When building more than one image inside the kaniko container, make sure to use the --cleanup flag. So it cleans its temporary cache data before building the next image, as discussed in this google group . Prerequisites \u00b6 Steps \u00b6 Create docker registry secret Configure pod container template Configure stage Create docker registry secret \u00b6 This is an example for DockerHub inside the build namespace. kubectl create secret docker-registry -n build regcred \\ --docker-server = index.docker.io \\ --docker-username = myDockerHubAccount \\ --docker-password = myDockerHubPassword \\ --docker-email = myDockerHub@Email.com Example Ppeline \u00b6 Warning Although multi-stage Dockerfile 's are supported, it did fail in my case. So I created a second Dockerfile which is only for running the application ( Dockerfile.run ). pipeline { agent { kubernetes { //cloud 'kubernetes' label 'kaniko' yaml \"\"\" kind: Pod metadata: name: kaniko spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root - name: go-build-cache mountPath: /root/.cache/go-build - name: img-build-cache mountPath: /root/.local volumes: - name: go-build-cache emptyDir: {} - name: img-build-cache emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/cat.git' } } stage ( 'Build' ) { steps { container ( 'golang' ) { sh './build-go-bin.sh' } } } stage ( 'Make Image' ) { environment { PATH = \"/busybox:$PATH\" } steps { container ( name: 'kaniko' , shell: '/busybox/sh' ) { sh '''#!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=index.docker.io/caladreas/cat ''' } } } } } IMG \u00b6 img is the brainchild of Jess Frazelle, a prominent figure in the container space. The goal is to be the safest and best way to build OCI compliant images, as outlined in her blog building container images securely on kubernetes . Not working (for me) yet \u00b6 It does not seem to work for me on AWS's EKS. To many little details with relation to runc, file permissions and other configuration. For those who want to give it a spin, here are some resources to take a look at. https://blog.jessfraz.com/post/building-container-images-securely-on-kubernetes/ https://github.com/genuinetools/img https://github.com/opencontainers/runc https://git.j3ss.co/genuinetools/img/+/d05b3e4e10cd0e3c074ffb03dc22d7bb6cde1e78 Pipeline Example \u00b6 pipeline { agent { kubernetes { label 'img' yaml \"\"\" kind: Pod metadata: name: img annotations: container.apparmor.security.beta.kubernetes.io/img: unconfined spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: img workingDir: /home/jenkins image: caladreas/img:0.5.1 imagePullPolicy: Always securityContext: rawProc: true privileged: true command: - cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: temp emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/cat.git' } } stage ( 'Build' ) { steps { container ( 'golang' ) { sh './build-go-bin.sh' } } } stage ( 'Make Image' ) { steps { container ( 'img' ) { sh 'mkdir cache' sh 'img build -s ./cache -f Dockerfile.run -t caladreas/cat .' } } } } }","title":"Pipelines with Docker Alternatives"},{"location":"blogs/docker-alternatives/#pipelines-with-docker-alternatives","text":"Building pipelines with Jenkins on Docker has been common for a while. But accessing the docker socket has always been a bit tricky. The easiest solution is directly mounting the docker socket of the host into your build container. However, this is a big security risk and something that is undesirable at best and, pretty dangerous at worst. When you're using an orchestrator such as Kubernetes - which is where Jenkins is currently best put to work - the problem gets worse. Not only do you open up security holes, you also mess up the schedular's insight into available resources, and mess with it's ability to keep your cluster in a correct state. In some sense, using docker directly in a Kubernetes cluster, is likened to running with scissors .","title":"Pipelines With Docker Alternatives"},{"location":"blogs/docker-alternatives/#potential-alternatives","text":"So, directly running docker containers via a docker engine inside your cluster is out. Let's look at some alternatives. Kubernetes Pod and External Node : the simple way out, use a cloud (such as EC2 Cloud) to provision a classic VM with docker engine as an agent and build there JIB : tool from Google, only works for Java applications and is supported directly from Gradle and Maven - Link Kaniko : tool from Google, works as 1-on-1 replacement for docker image build (except Windows Containers ) - Link IMG : tool from Jess Frazelle to avoid building docker images with a root user involved Link","title":"Potential Alternatives"},{"location":"blogs/docker-alternatives/#kubernetes-pod-and-external-node","text":"One of the most used cloud environments is AWS, so I created this solution with AWS's Amazon EC2 Plugin . Warning Unfortunately, you cannot combine the Kubernetes plugin with external nodes (none pod container nodes) in a Declarative pipeline. So you have to use Scripted . This can be done with various different cloud providers such as Digital Ocean, Google, AWS or Azure. This guide will use AWS, as it has the most mature Jenkins Cloud plugin.","title":"Kubernetes Pod and External Node"},{"location":"blogs/docker-alternatives/#prerequisites","text":"AWS Account with rights to create AMI's and run EC2 instances Packer Jenkins with Amazon EC2 Plugin installed","title":"Prerequisites"},{"location":"blogs/docker-alternatives/#steps","text":"create AMI with Packer install and configure Amazon EC2 plugin create a test pipeline","title":"Steps"},{"location":"blogs/docker-alternatives/#create-ami-with-packer","text":"Packer needs to be able to access EC2 API's and be able to spin up an EC2 instance and create an AMI out of it.","title":"Create AMI with Packer"},{"location":"blogs/docker-alternatives/#aws-setup-for-packer","text":"You need to configure two things: account details for Packer to use security group where your EC2 instances will be running with this security group needs to open port 22 both Packer and Jenkins will use this for their connection export AWS_DEFAULT_REGION = eu-west-1 export AWS_ACCESS_KEY_ID = XXX export AWS_SECRET_ACCESS_KEY = XXX aws ec2 --profile myAwsProfile create-security-group \\ --description \"For building Docker images\" \\ --group-name docker { \"GroupId\" : \"sg-08079f78cXXXXXXX\" } Export the security group ID. export SG_ID = sg-08079f78cXXXXXXX echo $SG_ID","title":"AWS setup for Packer"},{"location":"blogs/docker-alternatives/#enable-port-22","text":"aws ec2 \\ --profile myAwsProfile \\ authorize-security-group-ingress \\ --group-name docker \\ --protocol tcp \\ --port 22 \\ --cidr 0 .0.0.0/0","title":"Enable port 22"},{"location":"blogs/docker-alternatives/#packer-ami-definition","text":"Here's an example definition for Packer for a Ubuntu 18.04 LTS base image with JDK 8 (required by Jenkins) and Docker. { \"builders\" : [{ \"type\" : \"amazon-ebs\" , \"region\" : \"eu-west-1\" , \"source_ami_filter\" : { \"filters\" : { \"virtualization-type\" : \"hvm\" , \"name\" : \"*ubuntu-bionic-18.04-amd64-server-*\" , \"root-device-type\" : \"ebs\" }, \"owners\" : [ \"679593333241\" ], \"most_recent\" : true }, \"instance_type\" : \"t2.micro\" , \"ssh_username\" : \"ubuntu\" , \"ami_name\" : \"docker\" , \"force_deregister\" : true }], \"provisioners\" : [{ \"type\" : \"shell\" , \"inline\" : [ \"sleep 15\" , \"sudo apt-get clean\" , \"sudo apt-get update\" , \"sudo apt-get install -y apt-transport-https ca-certificates nfs-common\" , \"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\" , \"sudo add-apt-repository \\\"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\\"\" , \"sudo add-apt-repository -y ppa:openjdk-r/ppa\" , \"sudo apt-get update\" , \"sudo apt-get install -y docker-ce\" , \"sudo usermod -aG docker ubuntu\" , \"sudo apt-get install -y openjdk-8-jdk\" , \"java -version\" , \"docker version\" ] }] } Build the new AMI with packer. packer build docker-ami.json export AMI = ami-0212ab37f84e418f4","title":"Packer AMI definition"},{"location":"blogs/docker-alternatives/#ec2-key-pair","text":"Create EC2 key pair, this will be used by Jenkins to connect to the instances via ssh. aws ec2 --profile myAwsProfile create-key-pair \\ --key-name jenkinsec2 \\ | jq -r '.KeyMaterial' \\ >jenkins-ec2-proton.pem","title":"EC2 Key Pair"},{"location":"blogs/docker-alternatives/#ec2-cloud-configuration","text":"In a Jenkins' master main configuration, you add a new cloud . In this case, we will use a ec2-cloud so we can instantiate our EC2 VM's with docker. use EC2 credentials for initial connection use key ( .pem ) for VM connection (jenkins <> agent) configure the following: AMI: ami-0212ab37f84e418f4 availability zone: eu-west-1a VPC SubnetID: subnet-aa54XXXX Remote user: ubuntu labels: docker ubuntu linux SecurityGroup Name: (the id) sg-08079f78cXXXXXXX public ip = true connect via public ip = true","title":"EC2 Cloud Configuration"},{"location":"blogs/docker-alternatives/#pipeline","text":"@Library ( 'jenkins-pipeline-library@master' ) _ def scmVars def label = \"jenkins-slave-${UUID.randomUUID().toString()}\" podTemplate ( label: label , yaml: \"\"\" apiVersion: v1 kind: Pod spec: containers: - name: kubectl image: vfarcic/kubectl command: [\"cat\"] tty: true \"\"\" ) { node ( label ) { node ( \"docker\" ) { stage ( 'SCM & Prepare' ) { scmVars = checkout scm } stage ( 'Lint' ) { dockerfileLint () } stage ( 'Build Docker' ) { sh \"docker image build -t demo:rc-1 .\" } stage ( 'Tag & Push Docker' ) { IMAGE = \"${DOCKER_IMAGE_NAME}\" TAG = \"${DOCKER_IMAGE_TAG}\" FULL_NAME = \"${FULL_IMAGE_NAME}\" withCredentials ([ usernamePassword ( credentialsId: \"dockerhub\" , usernameVariable: \"USER\" , passwordVariable: \"PASS\" )]) { sh \"docker login -u $USER -p $PASS\" } sh \"docker image tag ${IMAGE}:${TAG} ${FULL_NAME}\" sh \"docker image push ${FULL_NAME}\" } } // end node docker stage ( 'Prepare Pod' ) { // have to checkout on our kubernetes pod aswell checkout scm } stage ( 'Check version' ) { container ( 'kubectl' ) { sh 'kubectl version' } } } // end node random label } // end pod def","title":"Pipeline"},{"location":"blogs/docker-alternatives/#maven-jib","text":"If you use Java with either Gradle or Maven , you can use JIB to create docker image without requiring a docker client or docker engine. JIB will communicate with DockerHub and Google's container registry for sending image layers back and forth. The last layer will be created by the JIB plugin itself, adding your self-executing Jar to the layer and a Entrypoint with the correct flags. For more information about the runtime options, see either jib-maven-plugin documentation or jib-gradle-plugin .","title":"Maven JIB"},{"location":"blogs/docker-alternatives/#prerequisites_1","text":"Java project build with Gradle or Maven Java project that can start it self, such as Spring Boot or Thorntail (previously Wildfly Swarm, from the JBoss family) able to build either gradle or maven applications The project used can be found at github.com/demomon/maven-spring-boot-demo .","title":"Prerequisites"},{"location":"blogs/docker-alternatives/#steps_1","text":"configure the plugin for either Gradle or Maven build using an official docker image via the kubernetes pod template","title":"Steps"},{"location":"blogs/docker-alternatives/#pipeline_1","text":"Using a bit more elaborate pipeline example here. Using SonarCloud for static code analysis and then JIB to create a docker image. We could then use that image either directly in anything that runs docker or in the same cluster via a Helm Chart. def scmVars def tag pipeline { options { buildDiscarder logRotator ( artifactDaysToKeepStr: '5' , artifactNumToKeepStr: '5' , daysToKeepStr: '5' , numToKeepStr: '5' ) } libraries { lib ( 'core@master' ) lib ( 'maven@master' ) } agent { kubernetes { label 'mypod' defaultContainer 'jnlp' yaml \"\"\" apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:3-jdk-11-slim command: - cat tty: true \"\"\" } } stages { stage ( 'Test versions' ) { steps { container ( 'maven' ) { sh 'uname -a' sh 'mvn -version' } } } stage ( 'Checkout' ) { steps { script { scmVars = checkout scm } gitRemoteConfigByUrl ( scmVars . GIT_URL , 'githubtoken' ) sh ''' git config --global user.email \"jenkins@jenkins.io\" git config --global user.name \"Jenkins\" ''' } } stage ( 'Build' ) { steps { container ( 'maven' ) { sh 'mvn clean verify -B -e' } } } stage ( 'Version & Analysis' ) { parallel { stage ( 'Version Bump' ) { when { branch 'master' } environment { NEW_VERSION = gitNextSemverTagMaven ( 'pom.xml' ) } steps { script { tag = \"${NEW_VERSION}\" } container ( 'maven' ) { sh 'mvn versions:set -DnewVersion=${NEW_VERSION}' } gitTag ( \"v${NEW_VERSION}\" ) } } stage ( 'Sonar Analysis' ) { when { branch 'master' } environment { SONAR_HOST = 'https://sonarcloud.io' KEY = 'spring-maven-demo' ORG = 'demomon' SONAR_TOKEN = credentials ( 'sonarcloud' ) } steps { container ( 'maven' ) { sh '''mvn sonar:sonar \\ -Dsonar.projectKey=${KEY} \\ -Dsonar.organization=${ORG} \\ -Dsonar.host.url=${SONAR_HOST} \\ -Dsonar.login=${SONAR_TOKEN} ''' } } } } } stage ( 'Publish Artifact' ) { when { branch 'master' } environment { DHUB = credentials ( 'dockerhub' ) } steps { container ( 'maven' ) { // we should never come here if the tests have not run, as we run verify before sh 'mvn clean compile -B -e jib:build -Djib.to.auth.username=${DHUB_USR} -Djib.to.auth.password=${DHUB_PSW} -DskipTests' } } } } post { always { cleanWs () } } }","title":"Pipeline"},{"location":"blogs/docker-alternatives/#kaniko","text":"Google loves Kubernetes and Google prefers people building docker images in Kubernetes without Docker. As JIB is only available for Java projects, there's needs to be an alternative for any other usecase/programming language. The answer to that is Kaniko , a specialized Docker image to create Docker images. Kaniko isn't the most secure way to create docker images, it barely beats mounting a Docker Socket, or might even be worse if you ask others (such as Jess Frazelle ). That said, it is gaining some traction being used by JenkinsX and having an example in Jenkins' Kubernetes Plugin . Info When building more than one image inside the kaniko container, make sure to use the --cleanup flag. So it cleans its temporary cache data before building the next image, as discussed in this google group .","title":"Kaniko"},{"location":"blogs/docker-alternatives/#prerequisites_2","text":"","title":"Prerequisites"},{"location":"blogs/docker-alternatives/#steps_2","text":"Create docker registry secret Configure pod container template Configure stage","title":"Steps"},{"location":"blogs/docker-alternatives/#create-docker-registry-secret","text":"This is an example for DockerHub inside the build namespace. kubectl create secret docker-registry -n build regcred \\ --docker-server = index.docker.io \\ --docker-username = myDockerHubAccount \\ --docker-password = myDockerHubPassword \\ --docker-email = myDockerHub@Email.com","title":"Create docker registry secret"},{"location":"blogs/docker-alternatives/#example-ppeline","text":"Warning Although multi-stage Dockerfile 's are supported, it did fail in my case. So I created a second Dockerfile which is only for running the application ( Dockerfile.run ). pipeline { agent { kubernetes { //cloud 'kubernetes' label 'kaniko' yaml \"\"\" kind: Pod metadata: name: kaniko spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root - name: go-build-cache mountPath: /root/.cache/go-build - name: img-build-cache mountPath: /root/.local volumes: - name: go-build-cache emptyDir: {} - name: img-build-cache emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/cat.git' } } stage ( 'Build' ) { steps { container ( 'golang' ) { sh './build-go-bin.sh' } } } stage ( 'Make Image' ) { environment { PATH = \"/busybox:$PATH\" } steps { container ( name: 'kaniko' , shell: '/busybox/sh' ) { sh '''#!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=index.docker.io/caladreas/cat ''' } } } } }","title":"Example Ppeline"},{"location":"blogs/docker-alternatives/#img","text":"img is the brainchild of Jess Frazelle, a prominent figure in the container space. The goal is to be the safest and best way to build OCI compliant images, as outlined in her blog building container images securely on kubernetes .","title":"IMG"},{"location":"blogs/docker-alternatives/#not-working-for-me-yet","text":"It does not seem to work for me on AWS's EKS. To many little details with relation to runc, file permissions and other configuration. For those who want to give it a spin, here are some resources to take a look at. https://blog.jessfraz.com/post/building-container-images-securely-on-kubernetes/ https://github.com/genuinetools/img https://github.com/opencontainers/runc https://git.j3ss.co/genuinetools/img/+/d05b3e4e10cd0e3c074ffb03dc22d7bb6cde1e78","title":"Not working (for me) yet"},{"location":"blogs/docker-alternatives/#pipeline-example","text":"pipeline { agent { kubernetes { label 'img' yaml \"\"\" kind: Pod metadata: name: img annotations: container.apparmor.security.beta.kubernetes.io/img: unconfined spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: img workingDir: /home/jenkins image: caladreas/img:0.5.1 imagePullPolicy: Always securityContext: rawProc: true privileged: true command: - cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: temp emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/cat.git' } } stage ( 'Build' ) { steps { container ( 'golang' ) { sh './build-go-bin.sh' } } } stage ( 'Make Image' ) { steps { container ( 'img' ) { sh 'mkdir cache' sh 'img build -s ./cache -f Dockerfile.run -t caladreas/cat .' } } } } }","title":"Pipeline Example"},{"location":"blogs/dockercon-eu-2018/","text":"DockerCon EU 2018 - Recap \u00b6 Generally outline \u00b6 From my perspective, there were a few red threads throughout the conference. Security begins at the Developer \u00b6 A shift left of security, bringing the responsibility of knowing your dependencies and their quality firmly to the developer. Most of this tooling is still aimed at enterprises though, being part of paid solutions mostly. At least that which was shown at the conference. docker-assemble, that can build in an image from a Maven pom.xml and will include meta-data of all your dependencies (transitive included) JFrog X-Ray Docker EE tooling such as Docker Trusted Registry (DTR) Broader Automation \u00b6 More focus on the entire lifecycle of a system and not just an application. It seems people are starting to understand that doing CI/CD and Infrastructure As Code is not a single event for a single application. There is likely to be a few applications belonging together making a whole system which will land on more than one type of infrastructure and possibly more types of clusters. What we see is tools looking at either a broader scope, a higher level abstraction or more developer focussed (more love for the Dev in DevOps) to allow for easier integration with multiple platforms. For example, Pulumi will enable you to create any type of infrastructure - like Hashicorp's Terraform - but then in programming languages, you're used to (TypeScript, Python, Go). Pulumi Docker App CNAB Build-Kit Containerization Influences Everything \u00b6 Containerization has left deep and easy to spot imprints in our industry from startups building entirely on top of containers to programming languages changing their ways to stay relevant. There are new monitoring kings in the world, DataDog, Sysdig, Honeycomb.io and so on. They live and breathe containers and are not afraid of being thrown around different public clouds, networks and what not. In contrast to traditional monitoring tools, which are often bolting on container support and struggle with the dynamic nature of containerized clusters. Another extraordinary influence is that on the Java language. Declared dead a million times over and deemed obsolete in the container era due to its massive footprint in image size and runtime size. Both are being addressed, and we see a lot of work done on reducing footprint and static linking (JLink, Graal). The most significant influence might be on the software behemoth that has rejuvenated itself. Microsoft has sworn allegiance to open source, Linux and containers. Windows 2019 server can run container workloads natively and work as nodes alongside a Docker EE cluster - which can include Kubernetes workloads. The next step would be support for Kubernetes integration, and as in the case of Java, smaller container footprint. Java & Docker Windows Container & Windows Server Support Observability tools Kubernetes offerings everywhere... Docker Build with Build-Kit \u00b6 Instead of investing in improving docker image building via the Docker Client, Docker created a new API and client library. This library called BuildKit, is completely independent. With Docker 18.09, it is included in the Docker Client allowing anyone to use it as easily as the traditional docker image build . BuildKit is already used by some other tools, such as Buildah and IMG, and allows you to create custom DSL \"Frontends\". As long as the API of BuikdKit is adhered to, the resulting image will be OCI compliant. So further remarks below and how to use it. BuildKit In-Depth session Supercharged Docker Build with BuildKit Usable from Docker 18.09 HighLights: allows custom DSL for specifying image (BuildKit) to still be used with Docker client/daemon build cache for your own files during build, think Go, Maven, Gradle... much more optimized, builds less, quicker, with more cache in less time support mounts (cache) such as secrets, during build phase # Set env variable to enable # Or configure docker's json config export DOCKER_BUILDKIT = 1 # syntax=docker/dockerfile:experimental ####################################### ## 1. BUILD JAR WITH MAVEN FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src ! RUN --mount = type = cache,target = /root/.m2/ mvn clean package -e ####################################### ## 2. BUILD NATIVE IMAGE WITH GRAAL FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from = BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath ####################################### ## 3. BUILD DOCKER RUNTIME IMAGE FROM alpine:3.8 CMD [ \"jpc-graal\" ] COPY --from = NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal ####################################### Secure your Kubernetes \u00b6 https://www.openpolicyagent.org + admission controller Network Policies Service Accounts Broader Cloud Automation \u00b6 Two of the broader cloud automation initiatives that impressed me at DockerCon were Pulumi and CNAB. Where Pulumi is an attempt to provide a more developer-friendly alternative to Terraform, CNAB is an attempt to create an environment agnostic installer specification. Meaning, you could create a CNAB installer which uses Pulumi to install all required infrastructure, applications and other resources. CNAB: cloud native application bundle \u00b6 Bundle.json invocation image (oci) = installer https://cnab.io docker app implements it helm support https://github.com/deislabs Install an implementation \u00b6 There are currently two implementations - that I found. Duffle from DeisLabs - open source from Azure - and Docker App - From Docker Inc.. Duffle \u00b6 create a new repo clone the repo init a duffle bundle copy duffle bundle data to our repo folder git clone git@github.com:demomon/cnab-duffle-demo-1.git duffle create cnab-duffle-demo-2 mv cnab-duffle-demo-2/cnab cnab-duffle-demo-1/ mv cnab-duffle-demo-2/duffle.json cnab-duffle-demo-1/ edit our install file ( cnab/run ) build our duffle bundle ( duffle build . ) We can now inspect our bundle with duffle. duffle show cnab-duffle-demo-1:0.1.0 -r -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA256 { \"name\" : \"cnab-duffle-demo-1\" , \"version\" : \"0.1.0\" , \"description\" : \"A short description of your bundle\" , \"keywords\" : [ \"cnab-duffle-demo-1\" , \"cnab\" , \"demo\" , \"joostvdg\" ] , \"maintainers\" : [ { \"name\" : \"Joost van der Griendt\" , \"email\" : \"joostvdg@gmail.com\" , \"url\" : \"https://joostvdg.github.io\" } ] , \"invocationImages\" : [ { \"imageType\" : \"docker\" , \"image\" : \"deislabs/cnab-duffle-demo-1-cnab:2965aad7406e1b651a98fffe1194fcaaec5e623c\" } ] , \"images\" : null, \"parameters\" : null, \"credentials\" : null } -----BEGIN PGP SIGNATURE----- wsDcBAEBCAAQBQJcL3L8CRDgq4YEOipZ/QAAjDYMAJI5o66SteUP2o4HsbVk+Viw 3Fd874vSVmpPKcmN3tRCEDWGdMdvqQiirDpa//ghx4y5oFTahK2ihQ35GbJLlq8S v9/CK6CKGJtp5g38s2LZrKIvESzEF2lTXwHB03PG8PJ37iWiYkHkxvMpyzded3Rs 4d+VgUnip0Cre7DemaUaz5+fTQjs88WNTIhqPg47YvgqFXV0s1y7yN3RTLr3ohQ1 9mkw87nWfOD+ULpbCUaq9FhNZ+v4dK5IZcWlkyv+yrtguyBBiA3MC54ueVBAdFCl 2OhxXgZjbBHPfQPV1mPqCQudOsWjK/+gqyNb6KTzKrAnyrumVQli/C/8BVk/SRC/ GS2o4EdTS2lfREc2Gl0/VTmMkqzFZZhWd7pwt/iMjl0bICFehSU0N6OqN1d+v6Sq vWIZ5ppxt1NnCzp05Y+NRfVZOxBc2xjYTquFwIa/+qGPrmXBKamw/irjmCOndvx+ l1tf/g0UVSQI2R2/19svl7dlMkYpDdlth1YGgZi/Hg == = 8Xwi -----END PGP SIGNATURE----- Demo \u00b6 I've created a demo on GitHub: github.com/demomon/cnab-duffle-demo-1 Its goal is to install CloudBees Core and its prerequisites in a (GKE) Kubernetes cluster. It wraps a Go (lang) binary that will execute the commands, for which you can find the source code on GitHub . Components \u00b6 A CNAB bundle has some components by default, for this demo we needed the following: duffle.json : Duffle configuration file Dockerfile : the CNAB installer runtime run (script): the installer script kgi (binary): the binary executable from my k8s-gitops-installer code, that we will leverage for the installation Dockerfile \u00b6 The installer tool ( kgi ) requires Helm and Kubectl , so we a Docker image that has those. As we might end up packaging the entire image as part of the full CNAB package, it should also be based on Alpine (or similar minimal Linux). There seems to be one very well maintained and widely used (according to GitHub and Dockerhub stats): dtzar/helm-kubect . So no need to roll our own. FROM dtzar/helm-kubectl:2.12.1 COPY Dockerfile /cnab/Dockerfile COPY app /cnab/app COPY kgi /usr/bin RUN ls -lath /cnab/app/ RUN kgi --help CMD [ \"/cnab/app/run\" ] duffle.json \u00b6 The only thing we need to add beyond the auto-generated file, is the credentials section. \"credentials\" : { \"kubeconfig\" : { \"path\" : \"/cnab/app/kube-config\" } } kgi \u00b6 I pre-build a binary suitable for Linux that works in Alpine and included it in the CNAB folder. run script \u00b6 First thing we need to make sure, is to configure the Kubeconfig location. export KUBECONFIG = \"/cnab/app/kube-config\" This should match what we defined in the duffle.json configuration - as you might expect - to make sure it gets bound in the right location. The kubectl command now knows which file to use. For the rest, we can do what we want, but convention tells us we need at least support status , install and uninstall . I'm lazy and only implemented install at the moment. In the install action, we will use the kgi executable to install CloudBees Core and it's pre-requisites. action = $CNAB_ACTION case $action in install ) echo \"[Install]\" kgi validate kubectl ;; esac For the rest, I recommend you look at the sources. Run the demo \u00b6 First, we have to build the bundle. duffle build . Once the build succeeded, we can create a credentials configuration. This will be a separate configuration file managed by Duffle. This configuration config must then be used with any installation that requires it - which makes it reusable as well. We have to populate it with the credential. In this case a path to a kube-config file. If you do not have one that you can export - e.g. based on GCloud - you can create a new user/certificate with a script. This is taken from gravitational , which were nice enough to create a script for doing so. You can find the script on GitHub (get-kubeconfig.sh) . Once you have that, store the end result at a decent place and configure it as your credential. duffle creds generate demo1 cnab-duffle-demo-1 With the above command we can create a credential config object based on the build bundle cnab-duffle-demo-1 . The credential object will be demo-1 , which we can now use for installing. duffle install demo1 cnab-duffle-demo-1:1.0.0 -c demo1 Further reading \u00b6 Howto guide on creating a Duffle Bundle Howto on handling credentials Wordpress with Kubernetes and AWS demo by Bitnami Example bundles Pulumi \u00b6 Just pointing to the documentation and GitHub repository is a bit boring and most of all, lazy. So I've gone through the trouble to create a demo project that shows what Pulumi can do. One of the things to keep in mind is that Pulumi keeps its state in a remote location by default - Pulumi's SaaS service. Much like using an S3 bucket to store your Terraform state. The goal of the demo is to show what you can do with Pulumi. My personal opinion is that Pulumi is excellent for managing entire systems as code. From public cloud infrastructure to public open source resources right down to your applications and their configurations. The system that I've written with Pulumi is a CI/CD system running in a GKE cluster. The system contains Jenkins, Artifactory and a custom LDAP server. To make it interesting, we'll use different aspects of Pulumi to create the system. We will create the GKE cluster via Pulumi's wrapper for the gcloud cli - in a similar way as JenkinsX does -, install Jenkins and Artifactory via Helm and install the LDAP server via Pulumi's Kubernetes resource classes. Steps taken \u00b6 For more info Pulumi.io install: brew install pulumi clone demo: git clone https://github.com/demomon/pulumi-demo-1 init stack: pulumi stack init demomon-pulumi-demo-1 connect to GitHub set kubernetes config pulumi config set kubernetes:context gke_ps-dev-201405_europe-west4_joostvdg-reg-dec18-1 pulumi config set isMinikube false install npm resources: npm install (I've used the TypeScript demo's) pulumi config set username administrator pulumi config set password 3OvlgaockdnTsYRU5JAcgM1o --secret pulumi preview incase pulumi loses your stack: pulumi stack select demomon-pulumi-demo-1 pulumi destroy Based on the following demo's: https://pulumi.io/quickstart/kubernetes/tutorial-exposed-deployment.html https://github.com/pulumi/examples/tree/master/kubernetes-ts-jenkins Artifactory via Helm \u00b6 To install the helm charts of Artifactory using Helm, we will first need to add JFrog's repository. helm repo add jfrog https://charts.jfrog.io helm repo update GKE Cluster \u00b6 Below is the code for the cluster. import * as gcp from \"@pulumi/gcp\" ; import * as k8s from \"@pulumi/kubernetes\" ; import * as pulumi from \"@pulumi/pulumi\" ; import { nodeCount , nodeMachineType , password , username } from \"./gke-config\" ; export const k8sCluster = new gcp . container . Cluster ( \"gke-cluster\" , { name : \"joostvdg-dec-2018-pulumi\" , initialNodeCount : nodeCount , nodeVersion : \"latest\" , minMasterVersion : \"latest\" , nodeConfig : { machineType : nodeMachineType , oauthScopes : [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" ], }, }); GKE Config \u00b6 As you could see, we import variables from a configuration file gke-config . import { Config } from \"@pulumi/pulumi\" ; const config = new Config (); export const nodeCount = config . getNumber ( \"nodeCount\" ) || 3 ; export const nodeMachineType = config . get ( \"nodeMachineType\" ) || \"n1-standard-2\" ; // username is the admin username for the cluster. export const username = config . get ( \"username\" ) || \"admin\" ; // password is the password for the admin user in the cluster. export const password = config . require ( \"password\" ); Kubeconfig \u00b6 As you probably will want to install other things (Helm charts, services) inside this cluster, we need to make sure we get the kubeconfig file of our cluster - which is yet to be created. Below is the code - courtesy of Pulumi's GKE Example - that generates and exports the Kubernetes Client Configuration. This client configuration can then be used by Helm charts or other Kubernetes services. Pulumi will then also understand it depends on the cluster and create/update it first before moving on to the others. // Manufacture a GKE-style Kubeconfig. Note that this is slightly \"different\" because of the way GKE requires // gcloud to be in the picture for cluster authentication (rather than using the client cert/key directly). export const k8sConfig = pulumi . all ([ k8sCluster . name , k8sCluster . endpoint , k8sCluster . masterAuth ]). apply (([ name , endpoint , auth ]) => { const context = ` ${ gcp . config . project } _ ${ gcp . config . zone } _ ${ name } ` ; return `apiVersion: v1 clusters: - cluster: certificate-authority-data: ${ auth . clusterCaCertificate } server: https:// ${ endpoint } name: ${ context } contexts: - context: cluster: ${ context } user: ${ context } name: ${ context } current-context: ${ context } kind: Config preferences: {} users: - name: ${ context } user: auth-provider: config: cmd-args: config config-helper --format=json cmd-path: gcloud expiry-key: '{.credential.token_expiry}' token-key: '{.credential.access_token}' name: gcp ` ; }); // Export a Kubernetes provider instance that uses our cluster from above. export const k8sProvider = new k8s . Provider ( \"gkeK8s\" , { kubeconfig : k8sConfig , }); Pulumi GCP Config \u00b6 https://github.com/pulumi/examples/blob/master/gcp-ts-gke/README.md export GCP_PROJECT = ... export GCP_ZONE = europe-west4-a export CLUSTER_PASSWORD = ... export GCP_SA_NAME = ... Make sure you have a Google SA (Service Account) by that name first, as you can read here . For me it worked best to NOT set any environment variables mentioned. They invairably caused authentication or authorization issues. Just make sure the SA account and it's credential file (see below) are authorized and the gcloud cli works. gcloud iam service-accounts keys create gcp-credentials.json \\ --iam-account ${ GCP_SA_NAME } @ ${ GCP_PROJECT } .iam.gserviceaccount.com gcloud auth activate-service-account --key-file gcp-credentials.json gcloud auth application-default login pulumi config set gcp:project ${ GCP_PROJECT } pulumi config set gcp:zone ${ GCP_ZONE } pulumi config set password --secret ${ CLUSTER_PASSWORD } Post Cluster Creation \u00b6 gcloud container clusters get-credentials joostvdg-dec-2018-pulumi kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $( gcloud config get-value account ) Install failed \u00b6 Failed to install kubernetes:rbac.authorization.k8s.io:Role artifactory-artifactory . Probably due to missing rights, so probably have to execute the admin binding before the helm charts. error: Plan apply failed: roles.rbac.authorization.k8s.io \"artifactory-artifactory\" is forbidden: attempt to grant extra privileges: ... Helm Charts \u00b6 Using Pulumi to install a Helm Chart feels a bit like adding layers of wrapping upon wrapping. The power of Pulumi becomes visible when using more than one related service on the same cluster - for example a SDLC Tool Chain. This example application installs two helm charts, Jenkins and Artifactory, on a GKE cluster that is also created and managed by Pulumi. Below is an example of installing a Helm chart of Jenkins, where we provide the Kubernetes config from the GKE cluster as Provider. This way, Pulumi knows it must install the helm chart in that GKE cluster and not in the current Kubeconfig. import { k8sProvider , k8sConfig } from \"./gke-cluster\" ; const jenkins = new k8s . helm . v2 . Chart ( \"jenkins\" , { repo : \"stable\" , version : \"0.25.1\" , chart : \"jenkins\" , }, { providers : { kubernetes : k8sProvider } } ); Deployment & Service \u00b6 First, make sure you have an interface for the configuration arguments. export interface LdapArgs { readonly name : string , readonly imageName : string , readonly imageTag : string } Then, create a exportable Pulumi resource class that can be reused. export class LdapInstallation extends pulumi . ComponentResource { public readonly deployment : k8s.apps.v1.Deployment ; public readonly service : k8s.core.v1.Service ; // constructor } Inside the constructor placehold we will create a constructor method. It will do all the configuration we need to do for this resource, in this case a Kubernetes Service and Deployment. constructor ( args : LdapArgs ) { super ( \"k8stypes:service:LdapInstallation\" , args . name , {}); const labels = { app : args.name }; const name = args . name } First Kubernetes resource to create is a container specification for the Deployment. const container : k8stypes.core.v1.Container = { name , image : args.imageName + \":\" + args . imageTag , resources : { requests : { cpu : \"100m\" , memory : \"200Mi\" }, limits : { cpu : \"100m\" , memory : \"200Mi\" }, }, ports : [{ name : \"ldap\" , containerPort : 1389 , }, ] }; As the configuration arguments can be any TypeScript type, you can allow people to override entire segments (such as Resources). Which you would do as follows: resources : args.resources || { requests : { cpu : \"100m\" , memory : \"200Mi\" }, limits : { cpu : \"100m\" , memory : \"200Mi\" }, }, The Deployment and Service construction are quite similar. this . deployment = new k8s . apps . v1 . Deployment ( args . name , { spec : { selector : { matchLabels : labels }, replicas : 1 , template : { metadata : { labels : labels }, spec : { containers : [ container ] }, }, }, },{ provider : cluster.k8sProvider }); this . service = new k8s . core . v1 . Service ( args . name , { metadata : { labels : this.deployment.metadata.apply ( meta => meta . labels ), }, spec : { ports : [{ name : \"ldap\" , port : 389 , targetPort : \"ldap\" , protocol : \"TCP\" }, ], selector : this.deployment.spec.apply ( spec => spec . template . metadata . labels ), type : \"ClusterIP\" , }, }, { provider : cluster.k8sProvider }); JFrog Jenkins Challenge \u00b6 Visited the stand of JFrog where they had stories about two main products: Artifactory and X-Ray. For both there is a Challenge, an X-Ray Challenge and a Jenkins & Artifactory Challenge . Jenkins Challenge \u00b6 The instructions for the Challenge were simply, follow what is stated in their GitHub repository and email a screenshot of the result. The instruction were as follows: Get an Artifactory instance (you can start a free trial on prem or in the cloud) Install Jenkins Install Artifactory Jenkins Plugin Add Artifactory credentials to Jenkins Credentials Create a new pipeline job Use the Artifactory Plugin DSL documentation to complete the following script: With a Scripted Pipeline as starting point: node { def rtServer def rtGradle def buildInfo stage ( 'Preparation' ) { git 'https://github.com/jbaruch/gradle-example.git' // create a new Artifactory server using the credentials defined in Jenkins // create a new Gradle build // set the resolver to the Gradle build to resolve from Artifactory // set the deployer to the Gradle build to deploy to Artifactory // declare that your gradle script does not use Artifactory plugin // declare that your gradle script uses Gradle wrapper } stage ( 'Build' ) { //run the artifactoryPublish gradle task and collect the build info } stage ( 'Publish Build Info' ) { //collect the environment variables to build info //publish the build info } } I don't like scripted, so I opted for Declarative with Jenkins in Kubernetes with the Jenkins Kubernetes plugin. Steps I took: get a trial license from the JFrog website install Artifactory and copy in the license when prompted change admin password create local maven repo 'libs-snapshot-local' create remote maven repo 'jcenter' (default remote value is jcenter, so only have to set the name) install Jenkins Artifactory plugin Kubernetes plugin add Artifactory username/password as credential in Jenkins create a gradle application (Spring boot via start.spring.io) which you can find here create a Jenkinsfile Installing Artifactory \u00b6 I installed Artifactory via Helm. JFrog has their own Helm repository - of course, would weird otherwise tbh - and you have to add that first. helm repo add jfrog https://charts.jfrog.io helm install --name artifactory stable/artifactory Jenkinsfile \u00b6 This uses the Gradle wrapper - as per instructions in the challenge. So we can use the standard JNLP container, which is default, so agent any will do. pipeline { agent any environment { rtServer = '' rtGradle = '' buildInfo = '' artifactoryServerAddress = 'http://..../artifactory' } stages { stage ( 'Test Container' ) { steps { container ( 'gradle' ) { sh 'which gradle' sh 'uname -a' sh 'gradle -version' } } } stage ( 'Checkout' ){ steps { git 'https://github.com/demomon/gradle-jenkins-challenge.git' } } stage ( 'Preparation' ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: artifactoryServerAddress , credentialsId: 'art-admin' // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: 'jcenter' , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: 'libs-snapshot-local' , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( 'Build' ) { steps { script { //run the artifactoryPublish gradle task and collect the build info buildInfo = rtGradle . run buildFile: 'build.gradle' , tasks: 'clean build artifactoryPublish' } } } stage ( 'Publish Build Info' ) { steps { script { //collect the environment variables to build info buildInfo . env . capture = true //publish the build info rtServer . publishBuildInfo buildInfo } } } } } Jenkinsfile without Gradle Wrapper \u00b6 I'd rather not install the Gradle tool if I can just use a pre-build container with it. Unfortunately, to use it correctly with the Artifactory plugin and a Jenkins Kubernetes plugin, we need to do two things. create a Gradle Tool in the Jenkins master because the Artifactory plugin expects a Jenkins Tool object, not a location Manage Jenkins -> Global Tool Configuration -> Gradle -> Add As value supply /usr , the Artifactory build will add /gradle/bin to it automatically set the user of build Pod to id 1000 explicitly else the build will not be allowed to touch files in /home/jenkins/workspace pipeline { agent { kubernetes { label 'mypod' yaml \"\"\"apiVersion: v1 kind: Pod spec: securityContext: runAsUser: 1000 fsGroup: 1000 containers: - name: gradle image: gradle:4.10-jdk-alpine command: ['cat'] tty: true \"\"\" } } environment { rtServer = '' rtGradle = '' buildInfo = '' CONTAINER_GRADLE_TOOL = '/usr/bin/gradle' } stages { stage ( 'Test Container' ) { steps { container ( 'gradle' ) { sh 'which gradle' sh 'uname -a' sh 'gradle -version' } } } stage ( 'Checkout' ){ steps { // git 'https://github.com/demomon/gradle-jenkins-challenge.git' checkout scm } } stage ( 'Preparation' ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: 'http://35.204.238.14/artifactory' , credentialsId: 'art-admin' // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: 'jcenter' , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: 'libs-snapshot-local' , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( 'Build' ) { //run the artifactoryPublish gradle task and collect the build info steps { script { buildInfo = rtGradle . run buildFile: 'build.gradle' , tasks: 'clean build artifactoryPublish' } } } stage ( 'Publish Build Info' ) { //collect the environment variables to build info //publish the build info steps { script { buildInfo . env . capture = true rtServer . publishBuildInfo buildInfo } } } } } Docker security & standards \u00b6 security takes place in every layer/lifecycle phase for scaling, security needs to be part of developer's day-to-day as everything is code, anything part of the sdlc should be secure and auditable use an admission controller network policies automate your security processes expand your security automation by adding learnings Docker & Java & CICD \u00b6 telepresence Distroless (google mini os) OpenJ9 Portala (for jdk 12) wagoodman/dive use jre for the runtime instead of jdk buildkit can use mounttarget for local caches add labels with Metadata (depency trees) grafeas & kritis FindSecBugs org.owasp:dependency-check-maven arminc/clair-scanner jlink = in limbo Docker & Windows \u00b6 specific base images for different use cases Docker capabilities heavily depend on Windows Server version Other \u00b6 Docker pipeline \u00b6 Dind + privileged mount socket windows & linux Windows build agent provisioning with docker EE & Jenkins Docker swarm update_config Idea: build a dynamic ci/cd platform with kubernetes \u00b6 jenkins evergreen + jcasc kubernetes plugin gitops pipeline AKS + virtual kubelet + ACI Jenkins + EC2 Pluging + ECS/Fargate jenkins agent as ecs task (fargate agent) docker on windows, only on ECS Apply Diplomacy to Code Review \u00b6 apply diplomacy to code review always positive remove human resistantance with inclusive language improvement focused persist, kindly Citizens Bank journey \u00b6 started with swarm, grew towards kubernetes (ucp) elk stack, centralised operations cluster Docker EE - Assemble \u00b6 Docker EE now has a binary called docker-assemble . This allows you to build a Docker image directly from something like a pom.xml, much like JIB. Other \u00b6","title":"DockerCon EU 2018"},{"location":"blogs/dockercon-eu-2018/#dockercon-eu-2018-recap","text":"","title":"DockerCon EU 2018 - Recap"},{"location":"blogs/dockercon-eu-2018/#generally-outline","text":"From my perspective, there were a few red threads throughout the conference.","title":"Generally outline"},{"location":"blogs/dockercon-eu-2018/#security-begins-at-the-developer","text":"A shift left of security, bringing the responsibility of knowing your dependencies and their quality firmly to the developer. Most of this tooling is still aimed at enterprises though, being part of paid solutions mostly. At least that which was shown at the conference. docker-assemble, that can build in an image from a Maven pom.xml and will include meta-data of all your dependencies (transitive included) JFrog X-Ray Docker EE tooling such as Docker Trusted Registry (DTR)","title":"Security begins at the Developer"},{"location":"blogs/dockercon-eu-2018/#broader-automation","text":"More focus on the entire lifecycle of a system and not just an application. It seems people are starting to understand that doing CI/CD and Infrastructure As Code is not a single event for a single application. There is likely to be a few applications belonging together making a whole system which will land on more than one type of infrastructure and possibly more types of clusters. What we see is tools looking at either a broader scope, a higher level abstraction or more developer focussed (more love for the Dev in DevOps) to allow for easier integration with multiple platforms. For example, Pulumi will enable you to create any type of infrastructure - like Hashicorp's Terraform - but then in programming languages, you're used to (TypeScript, Python, Go). Pulumi Docker App CNAB Build-Kit","title":"Broader Automation"},{"location":"blogs/dockercon-eu-2018/#containerization-influences-everything","text":"Containerization has left deep and easy to spot imprints in our industry from startups building entirely on top of containers to programming languages changing their ways to stay relevant. There are new monitoring kings in the world, DataDog, Sysdig, Honeycomb.io and so on. They live and breathe containers and are not afraid of being thrown around different public clouds, networks and what not. In contrast to traditional monitoring tools, which are often bolting on container support and struggle with the dynamic nature of containerized clusters. Another extraordinary influence is that on the Java language. Declared dead a million times over and deemed obsolete in the container era due to its massive footprint in image size and runtime size. Both are being addressed, and we see a lot of work done on reducing footprint and static linking (JLink, Graal). The most significant influence might be on the software behemoth that has rejuvenated itself. Microsoft has sworn allegiance to open source, Linux and containers. Windows 2019 server can run container workloads natively and work as nodes alongside a Docker EE cluster - which can include Kubernetes workloads. The next step would be support for Kubernetes integration, and as in the case of Java, smaller container footprint. Java & Docker Windows Container & Windows Server Support Observability tools Kubernetes offerings everywhere...","title":"Containerization Influences Everything"},{"location":"blogs/dockercon-eu-2018/#docker-build-with-build-kit","text":"Instead of investing in improving docker image building via the Docker Client, Docker created a new API and client library. This library called BuildKit, is completely independent. With Docker 18.09, it is included in the Docker Client allowing anyone to use it as easily as the traditional docker image build . BuildKit is already used by some other tools, such as Buildah and IMG, and allows you to create custom DSL \"Frontends\". As long as the API of BuikdKit is adhered to, the resulting image will be OCI compliant. So further remarks below and how to use it. BuildKit In-Depth session Supercharged Docker Build with BuildKit Usable from Docker 18.09 HighLights: allows custom DSL for specifying image (BuildKit) to still be used with Docker client/daemon build cache for your own files during build, think Go, Maven, Gradle... much more optimized, builds less, quicker, with more cache in less time support mounts (cache) such as secrets, during build phase # Set env variable to enable # Or configure docker's json config export DOCKER_BUILDKIT = 1 # syntax=docker/dockerfile:experimental ####################################### ## 1. BUILD JAR WITH MAVEN FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src ! RUN --mount = type = cache,target = /root/.m2/ mvn clean package -e ####################################### ## 2. BUILD NATIVE IMAGE WITH GRAAL FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from = BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath ####################################### ## 3. BUILD DOCKER RUNTIME IMAGE FROM alpine:3.8 CMD [ \"jpc-graal\" ] COPY --from = NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal #######################################","title":"Docker Build with Build-Kit"},{"location":"blogs/dockercon-eu-2018/#secure-your-kubernetes","text":"https://www.openpolicyagent.org + admission controller Network Policies Service Accounts","title":"Secure your Kubernetes"},{"location":"blogs/dockercon-eu-2018/#broader-cloud-automation","text":"Two of the broader cloud automation initiatives that impressed me at DockerCon were Pulumi and CNAB. Where Pulumi is an attempt to provide a more developer-friendly alternative to Terraform, CNAB is an attempt to create an environment agnostic installer specification. Meaning, you could create a CNAB installer which uses Pulumi to install all required infrastructure, applications and other resources.","title":"Broader Cloud Automation"},{"location":"blogs/dockercon-eu-2018/#cnab-cloud-native-application-bundle","text":"Bundle.json invocation image (oci) = installer https://cnab.io docker app implements it helm support https://github.com/deislabs","title":"CNAB: cloud native application bundle"},{"location":"blogs/dockercon-eu-2018/#install-an-implementation","text":"There are currently two implementations - that I found. Duffle from DeisLabs - open source from Azure - and Docker App - From Docker Inc..","title":"Install an implementation"},{"location":"blogs/dockercon-eu-2018/#duffle","text":"create a new repo clone the repo init a duffle bundle copy duffle bundle data to our repo folder git clone git@github.com:demomon/cnab-duffle-demo-1.git duffle create cnab-duffle-demo-2 mv cnab-duffle-demo-2/cnab cnab-duffle-demo-1/ mv cnab-duffle-demo-2/duffle.json cnab-duffle-demo-1/ edit our install file ( cnab/run ) build our duffle bundle ( duffle build . ) We can now inspect our bundle with duffle. duffle show cnab-duffle-demo-1:0.1.0 -r -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA256 { \"name\" : \"cnab-duffle-demo-1\" , \"version\" : \"0.1.0\" , \"description\" : \"A short description of your bundle\" , \"keywords\" : [ \"cnab-duffle-demo-1\" , \"cnab\" , \"demo\" , \"joostvdg\" ] , \"maintainers\" : [ { \"name\" : \"Joost van der Griendt\" , \"email\" : \"joostvdg@gmail.com\" , \"url\" : \"https://joostvdg.github.io\" } ] , \"invocationImages\" : [ { \"imageType\" : \"docker\" , \"image\" : \"deislabs/cnab-duffle-demo-1-cnab:2965aad7406e1b651a98fffe1194fcaaec5e623c\" } ] , \"images\" : null, \"parameters\" : null, \"credentials\" : null } -----BEGIN PGP SIGNATURE----- wsDcBAEBCAAQBQJcL3L8CRDgq4YEOipZ/QAAjDYMAJI5o66SteUP2o4HsbVk+Viw 3Fd874vSVmpPKcmN3tRCEDWGdMdvqQiirDpa//ghx4y5oFTahK2ihQ35GbJLlq8S v9/CK6CKGJtp5g38s2LZrKIvESzEF2lTXwHB03PG8PJ37iWiYkHkxvMpyzded3Rs 4d+VgUnip0Cre7DemaUaz5+fTQjs88WNTIhqPg47YvgqFXV0s1y7yN3RTLr3ohQ1 9mkw87nWfOD+ULpbCUaq9FhNZ+v4dK5IZcWlkyv+yrtguyBBiA3MC54ueVBAdFCl 2OhxXgZjbBHPfQPV1mPqCQudOsWjK/+gqyNb6KTzKrAnyrumVQli/C/8BVk/SRC/ GS2o4EdTS2lfREc2Gl0/VTmMkqzFZZhWd7pwt/iMjl0bICFehSU0N6OqN1d+v6Sq vWIZ5ppxt1NnCzp05Y+NRfVZOxBc2xjYTquFwIa/+qGPrmXBKamw/irjmCOndvx+ l1tf/g0UVSQI2R2/19svl7dlMkYpDdlth1YGgZi/Hg == = 8Xwi -----END PGP SIGNATURE-----","title":"Duffle"},{"location":"blogs/dockercon-eu-2018/#demo","text":"I've created a demo on GitHub: github.com/demomon/cnab-duffle-demo-1 Its goal is to install CloudBees Core and its prerequisites in a (GKE) Kubernetes cluster. It wraps a Go (lang) binary that will execute the commands, for which you can find the source code on GitHub .","title":"Demo"},{"location":"blogs/dockercon-eu-2018/#components","text":"A CNAB bundle has some components by default, for this demo we needed the following: duffle.json : Duffle configuration file Dockerfile : the CNAB installer runtime run (script): the installer script kgi (binary): the binary executable from my k8s-gitops-installer code, that we will leverage for the installation","title":"Components"},{"location":"blogs/dockercon-eu-2018/#dockerfile","text":"The installer tool ( kgi ) requires Helm and Kubectl , so we a Docker image that has those. As we might end up packaging the entire image as part of the full CNAB package, it should also be based on Alpine (or similar minimal Linux). There seems to be one very well maintained and widely used (according to GitHub and Dockerhub stats): dtzar/helm-kubect . So no need to roll our own. FROM dtzar/helm-kubectl:2.12.1 COPY Dockerfile /cnab/Dockerfile COPY app /cnab/app COPY kgi /usr/bin RUN ls -lath /cnab/app/ RUN kgi --help CMD [ \"/cnab/app/run\" ]","title":"Dockerfile"},{"location":"blogs/dockercon-eu-2018/#dufflejson","text":"The only thing we need to add beyond the auto-generated file, is the credentials section. \"credentials\" : { \"kubeconfig\" : { \"path\" : \"/cnab/app/kube-config\" } }","title":"duffle.json"},{"location":"blogs/dockercon-eu-2018/#kgi","text":"I pre-build a binary suitable for Linux that works in Alpine and included it in the CNAB folder.","title":"kgi"},{"location":"blogs/dockercon-eu-2018/#run-script","text":"First thing we need to make sure, is to configure the Kubeconfig location. export KUBECONFIG = \"/cnab/app/kube-config\" This should match what we defined in the duffle.json configuration - as you might expect - to make sure it gets bound in the right location. The kubectl command now knows which file to use. For the rest, we can do what we want, but convention tells us we need at least support status , install and uninstall . I'm lazy and only implemented install at the moment. In the install action, we will use the kgi executable to install CloudBees Core and it's pre-requisites. action = $CNAB_ACTION case $action in install ) echo \"[Install]\" kgi validate kubectl ;; esac For the rest, I recommend you look at the sources.","title":"run script"},{"location":"blogs/dockercon-eu-2018/#run-the-demo","text":"First, we have to build the bundle. duffle build . Once the build succeeded, we can create a credentials configuration. This will be a separate configuration file managed by Duffle. This configuration config must then be used with any installation that requires it - which makes it reusable as well. We have to populate it with the credential. In this case a path to a kube-config file. If you do not have one that you can export - e.g. based on GCloud - you can create a new user/certificate with a script. This is taken from gravitational , which were nice enough to create a script for doing so. You can find the script on GitHub (get-kubeconfig.sh) . Once you have that, store the end result at a decent place and configure it as your credential. duffle creds generate demo1 cnab-duffle-demo-1 With the above command we can create a credential config object based on the build bundle cnab-duffle-demo-1 . The credential object will be demo-1 , which we can now use for installing. duffle install demo1 cnab-duffle-demo-1:1.0.0 -c demo1","title":"Run the demo"},{"location":"blogs/dockercon-eu-2018/#further-reading","text":"Howto guide on creating a Duffle Bundle Howto on handling credentials Wordpress with Kubernetes and AWS demo by Bitnami Example bundles","title":"Further reading"},{"location":"blogs/dockercon-eu-2018/#pulumi","text":"Just pointing to the documentation and GitHub repository is a bit boring and most of all, lazy. So I've gone through the trouble to create a demo project that shows what Pulumi can do. One of the things to keep in mind is that Pulumi keeps its state in a remote location by default - Pulumi's SaaS service. Much like using an S3 bucket to store your Terraform state. The goal of the demo is to show what you can do with Pulumi. My personal opinion is that Pulumi is excellent for managing entire systems as code. From public cloud infrastructure to public open source resources right down to your applications and their configurations. The system that I've written with Pulumi is a CI/CD system running in a GKE cluster. The system contains Jenkins, Artifactory and a custom LDAP server. To make it interesting, we'll use different aspects of Pulumi to create the system. We will create the GKE cluster via Pulumi's wrapper for the gcloud cli - in a similar way as JenkinsX does -, install Jenkins and Artifactory via Helm and install the LDAP server via Pulumi's Kubernetes resource classes.","title":"Pulumi"},{"location":"blogs/dockercon-eu-2018/#steps-taken","text":"For more info Pulumi.io install: brew install pulumi clone demo: git clone https://github.com/demomon/pulumi-demo-1 init stack: pulumi stack init demomon-pulumi-demo-1 connect to GitHub set kubernetes config pulumi config set kubernetes:context gke_ps-dev-201405_europe-west4_joostvdg-reg-dec18-1 pulumi config set isMinikube false install npm resources: npm install (I've used the TypeScript demo's) pulumi config set username administrator pulumi config set password 3OvlgaockdnTsYRU5JAcgM1o --secret pulumi preview incase pulumi loses your stack: pulumi stack select demomon-pulumi-demo-1 pulumi destroy Based on the following demo's: https://pulumi.io/quickstart/kubernetes/tutorial-exposed-deployment.html https://github.com/pulumi/examples/tree/master/kubernetes-ts-jenkins","title":"Steps taken"},{"location":"blogs/dockercon-eu-2018/#artifactory-via-helm","text":"To install the helm charts of Artifactory using Helm, we will first need to add JFrog's repository. helm repo add jfrog https://charts.jfrog.io helm repo update","title":"Artifactory via Helm"},{"location":"blogs/dockercon-eu-2018/#gke-cluster","text":"Below is the code for the cluster. import * as gcp from \"@pulumi/gcp\" ; import * as k8s from \"@pulumi/kubernetes\" ; import * as pulumi from \"@pulumi/pulumi\" ; import { nodeCount , nodeMachineType , password , username } from \"./gke-config\" ; export const k8sCluster = new gcp . container . Cluster ( \"gke-cluster\" , { name : \"joostvdg-dec-2018-pulumi\" , initialNodeCount : nodeCount , nodeVersion : \"latest\" , minMasterVersion : \"latest\" , nodeConfig : { machineType : nodeMachineType , oauthScopes : [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" ], }, });","title":"GKE Cluster"},{"location":"blogs/dockercon-eu-2018/#gke-config","text":"As you could see, we import variables from a configuration file gke-config . import { Config } from \"@pulumi/pulumi\" ; const config = new Config (); export const nodeCount = config . getNumber ( \"nodeCount\" ) || 3 ; export const nodeMachineType = config . get ( \"nodeMachineType\" ) || \"n1-standard-2\" ; // username is the admin username for the cluster. export const username = config . get ( \"username\" ) || \"admin\" ; // password is the password for the admin user in the cluster. export const password = config . require ( \"password\" );","title":"GKE Config"},{"location":"blogs/dockercon-eu-2018/#kubeconfig","text":"As you probably will want to install other things (Helm charts, services) inside this cluster, we need to make sure we get the kubeconfig file of our cluster - which is yet to be created. Below is the code - courtesy of Pulumi's GKE Example - that generates and exports the Kubernetes Client Configuration. This client configuration can then be used by Helm charts or other Kubernetes services. Pulumi will then also understand it depends on the cluster and create/update it first before moving on to the others. // Manufacture a GKE-style Kubeconfig. Note that this is slightly \"different\" because of the way GKE requires // gcloud to be in the picture for cluster authentication (rather than using the client cert/key directly). export const k8sConfig = pulumi . all ([ k8sCluster . name , k8sCluster . endpoint , k8sCluster . masterAuth ]). apply (([ name , endpoint , auth ]) => { const context = ` ${ gcp . config . project } _ ${ gcp . config . zone } _ ${ name } ` ; return `apiVersion: v1 clusters: - cluster: certificate-authority-data: ${ auth . clusterCaCertificate } server: https:// ${ endpoint } name: ${ context } contexts: - context: cluster: ${ context } user: ${ context } name: ${ context } current-context: ${ context } kind: Config preferences: {} users: - name: ${ context } user: auth-provider: config: cmd-args: config config-helper --format=json cmd-path: gcloud expiry-key: '{.credential.token_expiry}' token-key: '{.credential.access_token}' name: gcp ` ; }); // Export a Kubernetes provider instance that uses our cluster from above. export const k8sProvider = new k8s . Provider ( \"gkeK8s\" , { kubeconfig : k8sConfig , });","title":"Kubeconfig"},{"location":"blogs/dockercon-eu-2018/#pulumi-gcp-config","text":"https://github.com/pulumi/examples/blob/master/gcp-ts-gke/README.md export GCP_PROJECT = ... export GCP_ZONE = europe-west4-a export CLUSTER_PASSWORD = ... export GCP_SA_NAME = ... Make sure you have a Google SA (Service Account) by that name first, as you can read here . For me it worked best to NOT set any environment variables mentioned. They invairably caused authentication or authorization issues. Just make sure the SA account and it's credential file (see below) are authorized and the gcloud cli works. gcloud iam service-accounts keys create gcp-credentials.json \\ --iam-account ${ GCP_SA_NAME } @ ${ GCP_PROJECT } .iam.gserviceaccount.com gcloud auth activate-service-account --key-file gcp-credentials.json gcloud auth application-default login pulumi config set gcp:project ${ GCP_PROJECT } pulumi config set gcp:zone ${ GCP_ZONE } pulumi config set password --secret ${ CLUSTER_PASSWORD }","title":"Pulumi GCP Config"},{"location":"blogs/dockercon-eu-2018/#post-cluster-creation","text":"gcloud container clusters get-credentials joostvdg-dec-2018-pulumi kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $( gcloud config get-value account )","title":"Post Cluster Creation"},{"location":"blogs/dockercon-eu-2018/#install-failed","text":"Failed to install kubernetes:rbac.authorization.k8s.io:Role artifactory-artifactory . Probably due to missing rights, so probably have to execute the admin binding before the helm charts. error: Plan apply failed: roles.rbac.authorization.k8s.io \"artifactory-artifactory\" is forbidden: attempt to grant extra privileges: ...","title":"Install failed"},{"location":"blogs/dockercon-eu-2018/#helm-charts","text":"Using Pulumi to install a Helm Chart feels a bit like adding layers of wrapping upon wrapping. The power of Pulumi becomes visible when using more than one related service on the same cluster - for example a SDLC Tool Chain. This example application installs two helm charts, Jenkins and Artifactory, on a GKE cluster that is also created and managed by Pulumi. Below is an example of installing a Helm chart of Jenkins, where we provide the Kubernetes config from the GKE cluster as Provider. This way, Pulumi knows it must install the helm chart in that GKE cluster and not in the current Kubeconfig. import { k8sProvider , k8sConfig } from \"./gke-cluster\" ; const jenkins = new k8s . helm . v2 . Chart ( \"jenkins\" , { repo : \"stable\" , version : \"0.25.1\" , chart : \"jenkins\" , }, { providers : { kubernetes : k8sProvider } } );","title":"Helm Charts"},{"location":"blogs/dockercon-eu-2018/#deployment-service","text":"First, make sure you have an interface for the configuration arguments. export interface LdapArgs { readonly name : string , readonly imageName : string , readonly imageTag : string } Then, create a exportable Pulumi resource class that can be reused. export class LdapInstallation extends pulumi . ComponentResource { public readonly deployment : k8s.apps.v1.Deployment ; public readonly service : k8s.core.v1.Service ; // constructor } Inside the constructor placehold we will create a constructor method. It will do all the configuration we need to do for this resource, in this case a Kubernetes Service and Deployment. constructor ( args : LdapArgs ) { super ( \"k8stypes:service:LdapInstallation\" , args . name , {}); const labels = { app : args.name }; const name = args . name } First Kubernetes resource to create is a container specification for the Deployment. const container : k8stypes.core.v1.Container = { name , image : args.imageName + \":\" + args . imageTag , resources : { requests : { cpu : \"100m\" , memory : \"200Mi\" }, limits : { cpu : \"100m\" , memory : \"200Mi\" }, }, ports : [{ name : \"ldap\" , containerPort : 1389 , }, ] }; As the configuration arguments can be any TypeScript type, you can allow people to override entire segments (such as Resources). Which you would do as follows: resources : args.resources || { requests : { cpu : \"100m\" , memory : \"200Mi\" }, limits : { cpu : \"100m\" , memory : \"200Mi\" }, }, The Deployment and Service construction are quite similar. this . deployment = new k8s . apps . v1 . Deployment ( args . name , { spec : { selector : { matchLabels : labels }, replicas : 1 , template : { metadata : { labels : labels }, spec : { containers : [ container ] }, }, }, },{ provider : cluster.k8sProvider }); this . service = new k8s . core . v1 . Service ( args . name , { metadata : { labels : this.deployment.metadata.apply ( meta => meta . labels ), }, spec : { ports : [{ name : \"ldap\" , port : 389 , targetPort : \"ldap\" , protocol : \"TCP\" }, ], selector : this.deployment.spec.apply ( spec => spec . template . metadata . labels ), type : \"ClusterIP\" , }, }, { provider : cluster.k8sProvider });","title":"Deployment &amp; Service"},{"location":"blogs/dockercon-eu-2018/#jfrog-jenkins-challenge","text":"Visited the stand of JFrog where they had stories about two main products: Artifactory and X-Ray. For both there is a Challenge, an X-Ray Challenge and a Jenkins & Artifactory Challenge .","title":"JFrog Jenkins Challenge"},{"location":"blogs/dockercon-eu-2018/#jenkins-challenge","text":"The instructions for the Challenge were simply, follow what is stated in their GitHub repository and email a screenshot of the result. The instruction were as follows: Get an Artifactory instance (you can start a free trial on prem or in the cloud) Install Jenkins Install Artifactory Jenkins Plugin Add Artifactory credentials to Jenkins Credentials Create a new pipeline job Use the Artifactory Plugin DSL documentation to complete the following script: With a Scripted Pipeline as starting point: node { def rtServer def rtGradle def buildInfo stage ( 'Preparation' ) { git 'https://github.com/jbaruch/gradle-example.git' // create a new Artifactory server using the credentials defined in Jenkins // create a new Gradle build // set the resolver to the Gradle build to resolve from Artifactory // set the deployer to the Gradle build to deploy to Artifactory // declare that your gradle script does not use Artifactory plugin // declare that your gradle script uses Gradle wrapper } stage ( 'Build' ) { //run the artifactoryPublish gradle task and collect the build info } stage ( 'Publish Build Info' ) { //collect the environment variables to build info //publish the build info } } I don't like scripted, so I opted for Declarative with Jenkins in Kubernetes with the Jenkins Kubernetes plugin. Steps I took: get a trial license from the JFrog website install Artifactory and copy in the license when prompted change admin password create local maven repo 'libs-snapshot-local' create remote maven repo 'jcenter' (default remote value is jcenter, so only have to set the name) install Jenkins Artifactory plugin Kubernetes plugin add Artifactory username/password as credential in Jenkins create a gradle application (Spring boot via start.spring.io) which you can find here create a Jenkinsfile","title":"Jenkins Challenge"},{"location":"blogs/dockercon-eu-2018/#installing-artifactory","text":"I installed Artifactory via Helm. JFrog has their own Helm repository - of course, would weird otherwise tbh - and you have to add that first. helm repo add jfrog https://charts.jfrog.io helm install --name artifactory stable/artifactory","title":"Installing Artifactory"},{"location":"blogs/dockercon-eu-2018/#jenkinsfile","text":"This uses the Gradle wrapper - as per instructions in the challenge. So we can use the standard JNLP container, which is default, so agent any will do. pipeline { agent any environment { rtServer = '' rtGradle = '' buildInfo = '' artifactoryServerAddress = 'http://..../artifactory' } stages { stage ( 'Test Container' ) { steps { container ( 'gradle' ) { sh 'which gradle' sh 'uname -a' sh 'gradle -version' } } } stage ( 'Checkout' ){ steps { git 'https://github.com/demomon/gradle-jenkins-challenge.git' } } stage ( 'Preparation' ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: artifactoryServerAddress , credentialsId: 'art-admin' // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: 'jcenter' , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: 'libs-snapshot-local' , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( 'Build' ) { steps { script { //run the artifactoryPublish gradle task and collect the build info buildInfo = rtGradle . run buildFile: 'build.gradle' , tasks: 'clean build artifactoryPublish' } } } stage ( 'Publish Build Info' ) { steps { script { //collect the environment variables to build info buildInfo . env . capture = true //publish the build info rtServer . publishBuildInfo buildInfo } } } } }","title":"Jenkinsfile"},{"location":"blogs/dockercon-eu-2018/#jenkinsfile-without-gradle-wrapper","text":"I'd rather not install the Gradle tool if I can just use a pre-build container with it. Unfortunately, to use it correctly with the Artifactory plugin and a Jenkins Kubernetes plugin, we need to do two things. create a Gradle Tool in the Jenkins master because the Artifactory plugin expects a Jenkins Tool object, not a location Manage Jenkins -> Global Tool Configuration -> Gradle -> Add As value supply /usr , the Artifactory build will add /gradle/bin to it automatically set the user of build Pod to id 1000 explicitly else the build will not be allowed to touch files in /home/jenkins/workspace pipeline { agent { kubernetes { label 'mypod' yaml \"\"\"apiVersion: v1 kind: Pod spec: securityContext: runAsUser: 1000 fsGroup: 1000 containers: - name: gradle image: gradle:4.10-jdk-alpine command: ['cat'] tty: true \"\"\" } } environment { rtServer = '' rtGradle = '' buildInfo = '' CONTAINER_GRADLE_TOOL = '/usr/bin/gradle' } stages { stage ( 'Test Container' ) { steps { container ( 'gradle' ) { sh 'which gradle' sh 'uname -a' sh 'gradle -version' } } } stage ( 'Checkout' ){ steps { // git 'https://github.com/demomon/gradle-jenkins-challenge.git' checkout scm } } stage ( 'Preparation' ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: 'http://35.204.238.14/artifactory' , credentialsId: 'art-admin' // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: 'jcenter' , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: 'libs-snapshot-local' , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( 'Build' ) { //run the artifactoryPublish gradle task and collect the build info steps { script { buildInfo = rtGradle . run buildFile: 'build.gradle' , tasks: 'clean build artifactoryPublish' } } } stage ( 'Publish Build Info' ) { //collect the environment variables to build info //publish the build info steps { script { buildInfo . env . capture = true rtServer . publishBuildInfo buildInfo } } } } }","title":"Jenkinsfile without Gradle Wrapper"},{"location":"blogs/dockercon-eu-2018/#docker-security-standards","text":"security takes place in every layer/lifecycle phase for scaling, security needs to be part of developer's day-to-day as everything is code, anything part of the sdlc should be secure and auditable use an admission controller network policies automate your security processes expand your security automation by adding learnings","title":"Docker security &amp; standards"},{"location":"blogs/dockercon-eu-2018/#docker-java-cicd","text":"telepresence Distroless (google mini os) OpenJ9 Portala (for jdk 12) wagoodman/dive use jre for the runtime instead of jdk buildkit can use mounttarget for local caches add labels with Metadata (depency trees) grafeas & kritis FindSecBugs org.owasp:dependency-check-maven arminc/clair-scanner jlink = in limbo","title":"Docker &amp; Java &amp; CICD"},{"location":"blogs/dockercon-eu-2018/#docker-windows","text":"specific base images for different use cases Docker capabilities heavily depend on Windows Server version","title":"Docker &amp; Windows"},{"location":"blogs/dockercon-eu-2018/#other","text":"","title":"Other"},{"location":"blogs/dockercon-eu-2018/#docker-pipeline","text":"Dind + privileged mount socket windows & linux Windows build agent provisioning with docker EE & Jenkins Docker swarm update_config","title":"Docker pipeline"},{"location":"blogs/dockercon-eu-2018/#idea-build-a-dynamic-cicd-platform-with-kubernetes","text":"jenkins evergreen + jcasc kubernetes plugin gitops pipeline AKS + virtual kubelet + ACI Jenkins + EC2 Pluging + ECS/Fargate jenkins agent as ecs task (fargate agent) docker on windows, only on ECS","title":"Idea: build a dynamic ci/cd platform with kubernetes"},{"location":"blogs/dockercon-eu-2018/#apply-diplomacy-to-code-review","text":"apply diplomacy to code review always positive remove human resistantance with inclusive language improvement focused persist, kindly","title":"Apply Diplomacy to Code Review"},{"location":"blogs/dockercon-eu-2018/#citizens-bank-journey","text":"started with swarm, grew towards kubernetes (ucp) elk stack, centralised operations cluster","title":"Citizens Bank journey"},{"location":"blogs/dockercon-eu-2018/#docker-ee-assemble","text":"Docker EE now has a binary called docker-assemble . This allows you to build a Docker image directly from something like a pom.xml, much like JIB.","title":"Docker EE - Assemble"},{"location":"blogs/dockercon-eu-2018/#other_1","text":"","title":"Other"},{"location":"blogs/gitops-pipeline/","text":"GitOps Pipeline with Jenkins on Kubernetes \u00b6 Missing pieces \u00b6 secrets via Vault service mesh infrastructure as code (around the clusters) multiple clusters Pulumi Rancher","title":"GitOps Pipeline with Jenkins on Kubernetes"},{"location":"blogs/gitops-pipeline/#gitops-pipeline-with-jenkins-on-kubernetes","text":"","title":"GitOps Pipeline with Jenkins on Kubernetes"},{"location":"blogs/gitops-pipeline/#missing-pieces","text":"secrets via Vault service mesh infrastructure as code (around the clusters) multiple clusters Pulumi Rancher","title":"Missing pieces"},{"location":"blogs/graceful-shutdown/","text":"Gracefully Shutting Down Applications in Docker \u00b6 I'm not sure about you, but I prefer it when my neighbors leave our shared spaces clean and don't take up parking spaces when they don't need them. Imagine you live in an apartment complex with the above-mentioned parking lot. Some tenants go away and never come back. If nothing is done to clean up after them - to reclaim their apartment and parking space - then after some time, more and more apartments are unavailable for no reason, and the parking lot fills with cars which belong to no one. Some tenants did not get a parking lot and are getting frustrated that none are becoming available. When they moved in, they were told that when others leave, they would be next in line. While they're waiting, they have to park outside the complex. Eventually, the entrance gets blocked and no one can enter or leave. The end result is a completely unlivable apartment block with trapped tenants - never to be seen or heard from again. If you agree with me that when a tenant leaves, they should clean the apartment and free the parking spot to make it ready for the next inhabitant; then please read on. We're going to dive into the equivalent of doing this with containers. We will explore running our containers with Docker (run, compose, swarm) and Kubernetes. Even if you use another way to run your containers, this article should provide you with enough insight to get you on your way. The case for graceful shutdown \u00b6 We're in an age where many applications are running in Docker containers across a multitude of clusters. These applications are then confronted with new concerns to tackle such as more moving parts, networking between these parts, remote storage and others. One significant way we defend ourselves against the perils of this distributed nature is to make our applications more robust - able to survive errors. However, even then there is still no guarantee your application is always up and running. So another concern we should tackle is how it responds when it needs to shut down. Where we can differentiate between an unexpected shutdown - we crashed - or an expected shutdown. On top of that, failing instead of trying to recover when something bad happens also adheres to \"fail fast\" - as strongly advocated by Michael Nygard in ReleaseIt. Shutting down can happen for a variety of reasons, in this post we dive into how to deal with an expected shutdown such as it being told to stop by an orchestrator such as Kubernetes. Containers can be purposefully shut down for a variety of reasons, including but not limited too: your application's health check fails your application consumed more resources than allowed the application is scaling down Just as cleaning up when leaving makes you a better tenant, having your application clean up connections, resources Moreover, the more tenants behaving in a good way increases the quality of living for all tenants. In our case, it improves the reliability and consistency of our cluster. Graceful shutdown is not unique to Docker, as it has been part of Linux's best practices for quite some years before Docker's existence. However, applying them to Docker container adds extra dimensions. Start Good So You Can End Well \u00b6 When you sign up for an apartment, you probably have to sign a contract detailing your rights and obligations. The more you state explicitly, the easier it is to deal with bad behaving neighbors. The same is true when running processes; we should make sure that we set the rules, obligations, and expectations from the start. As we say in Dutch: a good beginning is half the work. We will start with how you can run a process in a container with a process that shuts down gracefully. There are many ways to start a process in a container. In this article, we look at processes started by commands defined in a Dockerfile. There are two ways to specify this: CMD : runs a command when the container gets started ENTRYPOINT : provides the location (entrypoint) from where commands get run when the container starts You need at least one ENTRYPOINT or CMD in a Dockerfile for it to be valid. They can be used in collaboration but they can do similar things. For more information on the details of these commands, read Docker's docs on Entrypoint vs. CMD . Docker Shell form example \u00b6 We start with the shell form and see if it can do what we want; begin in such a way, we can stop it nicely. Shell form means we define a shell command without any special format or keywords. Please create Dockerfile with the content that follows. FROM ubuntu:18.04 ENTRYPOINT top -b Then build an image and run a container. docker image build --tag shell-form . docker run --name shell-form --rm shell-form The above command yields the following output. top - 16 :34:56 up 1 day, 5 :15, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 541984 free, 302668 used, 1202280 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1579380 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 4624 760 696 S 0 .0 0 .0 0 :00.05 sh 6 root 20 0 36480 2928 2580 R 0 .0 0 .1 0 :00.01 top As you can see, two processes are running, sh and top . Meaning, that killing the process, with ctrl+c for example, terminates the sh process, but not top . This happens because the sh process forked the top process, but the termination will only be send to PID 1 - in this case sh . As sh will not stop the top process for us it will continue running and leave the container alive. To kill this container, open a second terminal and execute the following command. docker rm -f shell-form Shell form doesn't do what we need. Starting a process with shell form will only lead us to the disaster of parking lots filling up unless there's a someone actively cleaning up. Docker exec form example \u00b6 This leads us to the exec form. Hopefully, this gets us somewhere. The exec form is written as an array of parameters: ENTRYPOINT [\"top\", \"-b\"] To continue in the same line of examples, we will create a Dockerfile, build and run it. FROM ubuntu:18.04 ENTRYPOINT [ \"top\" , \"-b\" ] Then build and run it. docker image build --tag exec-form . docker run --name exec-form --rm exec-form This yields the following output. top - 18 :12:30 up 1 day, 6 :53, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 535896 free, 307196 used, 1203840 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1574880 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 36480 2940 2584 R 0 .0 0 .1 0 :00.03 top Now we got something we can work with. If something would tell this Container to stop, it will tell our only running process so it is sure to reach the correct one! Gotchas \u00b6 Knowing we can use the exec form for our goal - gracefully shutting down our container - we can move on to the next part of our efforts. For the sake of imparting you with some hard learned lessons, we will explore two gotchas. They're optional, so you can also choose to skip to Make Sure Your Process Listens . Docker exec form with parameters \u00b6 A caveat with the exec form is that it doesn't interpolate parameters. You can try the following: FROM ubuntu:18.04 ENV PARAM = \"-b\" ENTRYPOINT [ \"top\" , \"${PARAM}\" ] Then build and run it: docker image build --tag exec-param . docker run --name exec-form --rm exec-param This should yield the following: /bin/sh: 1 : [ top: not found This is where Docker created a mix between the two styles. It allows you to create an Entrypoint with a shell command - performing interpolation - but executing it as an exec form. This can be done by prefixing the shell form, with, you guessed it, exec . FROM ubuntu:18.04 ENV PARAM = \"-b\" ENTRYPOINT exec \"top\" \" ${ PARAM } \" Then build and run it: docker image build --tag exec-param . docker run --name exec-form --rm exec-param This will return the exact same as if we would've run ENTRYPOINT [\"top\", \"-b\"] . Now you can also override the param, by using the environment variable flag. docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = \"help\" exec-param Resulting in top's help string. The special case of Alpine \u00b6 One of the main best practices for Dockerfiles , is to make them as small as possible. The easiest way to do this is to start with a minimal image. This is where Alpine Linux comes in. We will revisit out shell form example, but replace ubuntu with alpine. Create the following Dockerfile. FROM alpine:3.8 ENTRYPOINT top -b Then build and run it. docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = \"help\" exec-param This yields the following output. Mem: 1509068K used, 537864K free, 640K shrd, 126756K buff, 1012436K cached CPU: 0 % usr 0 % sys 0 % nic 100 % idle 0 % io 0 % irq 0 % sirq Load average: 0 .00 0 .00 0 .00 2 /404 5 PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND 1 0 root R 1516 0 % 0 0 % top -b Aside from top 's output looking a bit different, there is only one command. Alpine Linux helps us avoid the problem of shell form altogether! Make Sure Your Process Listens \u00b6 It is excellent if your tenants are all signed up, know their rights and obligations. But you can't contact them when something happens, how will they ever know when to act? Translating that into our process. It starts and can be told to shut down, but does it process listen? Can it interpret the message it gets from Docker or Kubernetes? And if it does, can it relay the message correctly to its Child Processes? In order for your process to gracefully shutdown, it should know when to do so. As such, it should listen not only for itself but also on behalf of its children - yours never do anything wrong though! Some processes do, but many aren't designed to listen or tell their Children . They expect someone else to listen for them and tell them and their children - process managers. In order to listen to these signals , we can call in the help of others. We will look at two options. we let Docker manage the process and its children we use a process manager Let Docker manage it for us \u00b6 If you're not using Docker to run or manage your containers, you should skip to Depend on a process manager . Docker has a build in feature, that it uses a lightweight process manager to help you. So if you're running your images with Docker itself, either directly or via Compose or Swarm, you're fine. You can use the init flag in your run command or your compose file. Please, note that the below examples require a certain minimum version of Docker. run - 1.13+ compose (v 2.2) - 1.13.0+ swarm (v 3.7) - 18.06.0+ With Docker Run \u00b6 docker run --rm -ti --init caladreas/dui With Docker Compose \u00b6 version : '2.2' services : web : image : caladreas/java-docker-signal-demo:no-tini init : true With Docker Swarm \u00b6 version : '3.7' services : web : image : caladreas/java-docker-signal-demo:no-tini init : true Relying on Docker does create a dependency on how your container runs. It only runs correctly in Docker-related technologies (run, compose, swarm) and only if the proper versions are available. Creating either a different experience for users running your application somewhere else or not able to meet the version requirements. So maybe another solution is to bake a process manager into your image and guarantee its behavior. Depend on a process manager \u00b6 One of our goals for Docker images is to keep them small. We should look for a lightweight process manager. It does not have too many a whole machine worth or processes, just one and perhaps some children. Here we would like to introduce you to Tini , a lightweight process manager designed for this purpose . It is a very successful and widely adopted process manager in the Docker world. So successful, that the before mentioned init flags from Docker are implemented by baking Tini into Docker . Debian example \u00b6 For brevity, the build process is excluded, and for image size, we use Debian slim instead of default Debian. FROM debian:stable-slim ENV TINI_VERSION v0.18.0 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ \"/tini\" , \"-vv\" , \"-g\" , \"--\" , \"/usr/bin/dui/bin/dui\" , \"-XX:+UseCGroupMemoryLimitForHeap\" , \"-XX:+UnlockExperimentalVMOptions\" ] COPY --from = build /usr/bin/dui-image/ /usr/bin/dui Alpine example \u00b6 Alpine Linux works wonders for Docker images, so to improve our lives, you can very easily install it if you want. FROM alpine RUN apk add --no-cache tini ENTRYPOINT [ \"/sbin/tini\" , \"-vv\" , \"-g\" , \"-s\" , \"--\" ] CMD [ \"top -b\" ] How To Be Told What You Want To Hear \u00b6 You've made it this far; your tenets are reachable so you can inform them if they need to act. However, there's another problem lurking around the corner. Do they speak your language? Our process now starts knowing it can be talked to, it has someone who takes care of listening for it and its children. Now we need to make sure it can understand what it hears, it should be able to handle the incoming signals. We have two main ways of doing this. Handle signals as they come : we should make sure our process deal with the signals as they come State the signals we want : we can also tell up front, which signals we want to hear and put the burden of translation on our callers For more details on the subject of Signals and Docker, please read this excellent blog from Grigorii Chudnov . Handle signals as they come \u00b6 Handling process signals depend on your application, programming language or framework. State the signals we want \u00b6 Sometimes your language or framework of choice, doesn't handle signals all that well. It might be very rigid in what it does with specific signals, removing your ability to do the right thing. Of course, not all languages or frameworks are designed with Docker container or Microservices in mind, are yet to catch up to this more dynamic environment. Luckily Docker and Kubernetes allow you to specify what signal too sent to your process. Docker run \u00b6 docker run --rm -ti --init --stop-signal = SIGINT \\ caladreas/java-docker-signal-demo Docker compose/swarm \u00b6 Docker's compose file format allows you to specify a stop signal . This is the signal sent when the container is stopped in a normal fashion. Normal in this case, meaning docker stop or when docker itself determines it should stop the container. If you forcefully remove the container, for example with docker rm -f it will directly kill the process, so don't do that. version : '2.2' services : web : image : caladreas/java-docker-signal-demo stop_signal : SIGINT stop_grace_period : 15s If you run this with docker-compose up and then in a second terminal, stop the container, you will see something like this. web_1 | HelloWorld! web_1 | Shutdown hook called! web_1 | We 're told to stop early... web_1 | java.lang.InterruptedException: sleep interrupted web_1 | at java.base/java.lang.Thread.sleep(Native Method) web_1 | at joostvdg.demo.signal@1.0/com.github.joostvdg.demo.signal.HelloWorld.printHelloWorld(Unknown Source) web_1 | at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) web_1 | at java.base/java.util.concurrent.FutureTask.run(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) web_1 | at java.base/java.lang.Thread.run(Unknown Source) web_1 | [DEBUG tini (1)] Passing signal: ' Interrupt ' web_1 | [DEBUG tini (1)] Received SIGCHLD web_1 | [DEBUG tini (1)] Reaped child with pid: ' 7 ' web_1 | [INFO tini (1)] Main child exited with signal (with signal ' Interrupt ' ) Kubernetes \u00b6 In Kubernetes we can make use of Container Lifecycle Hooks to manage how our container should be stopped. We could, for example, send a SIGINT (interrupt) to tell our application to stop. apiVersion : apps/v1 kind : Deployment metadata : name : java-signal-demo namespace : default labels : app : java-signal-demo spec : replicas : 1 template : metadata : labels : app : java-signal-demo spec : containers : - name : main image : caladreas/java-docker-signal-demo lifecycle : preStop : exec : command : [ \"killall\" , \"java\" , \"-INT\" ] terminationGracePeriodSeconds : 60 When you create this as deployment.yml, create and delete it - kubectl apply -f deployment.yml / kubectl delete -f deployment.yml - you will see the same behavior. How To Be Told When You Want To Hear It \u00b6 Our process now will now start knowing it will hear what it wants to hear. But we now have to make sure we hear it when we need to hear it. An intervention is excellent when you can still be saved, but it is a bit useless if you're already dead. Docker \u00b6 You can either configure your health check in your Dockerfile or configure it in your docker-compose.yml for either compose or swarm. Considering only Docker can use the health check in your Dockerfile, it is strongly recommended to have health checks in your application and document how they can be used. Kubernetes \u00b6 In Kubernetes we have the concept of Container Probes . This allows you to configure whether your container is ready (readinessProbe) to be used and if it is still working as expected (livenessProbe). Examples \u00b6 How to actually listen to the signals and determine which one to use will depend on your programming language. There's three examples I have worked out, one for Go (lang) and two for Java: pojo and Spring Boot. Go \u00b6 Dockerfile \u00b6 # build stage FROM golang:latest AS build-env RUN go get -v github.com/docker/docker/client/... RUN go get -v github.com/docker/docker/api/... ADD src/ $GOPATH /flow-proxy-service-lister WORKDIR $GOPATH /flow-proxy-service-lister RUN go build -o main -tags netgo main.go # final stage FROM alpine ENTRYPOINT [ \"/app/main\" ] COPY --from = build-env /go/flow-proxy-service-lister/main /app/ RUN chmod +x /app/main Go code for graceful shutdown \u00b6 The following is a way for Go to shutdown a http server when receiving a termination signal. func main () { c := make ( chan bool ) // make channel for main <--> webserver communication go webserver . Start ( \"7777\" , webserverData , c ) // ignore the missing data stop := make ( chan os . Signal , 1 ) // make a channel that listens to is signals signal . Notify ( stop , syscall . SIGINT , syscall . SIGTERM ) // we listen to some specific syscall signals for i := 1 ; ; i ++ { // this is still infinite t := time . NewTicker ( time . Second * 30 ) // set a timer for the polling select { case <- stop : // this means we got a os signal on our channel break // so we can stop case <- t . C : // our timer expired, refresh our data continue // and continue with the loop } break } fmt . Println ( \"Shutting down webserver\" ) // if we got here, we have to inform the webserver to close shop c <- true // we do this by sending a message on the channel if b := <- c ; b { // when we get true back, that means the webserver is doing with a graceful shutdown fmt . Println ( \"Webserver shut down\" ) // webserver is done } fmt . Println ( \"Shut down app\" ) // we can close shop ourselves now } Java plain (Docker Swarm) \u00b6 This application is a Java 9 modular application, which can be found on github, github.com/joostvdg . Dockerfile \u00b6 FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name \"*.java\" ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" LABEL version = \"0.1.0\" LABEL description = \"Docker image for playing with java applications in a concurrent, parallel and distributed manor.\" # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ \"/tini\" , \"-vv\" , \"-g\" , \"--\" , \"/usr/bin/dui/bin/dui\" ] ENV DATE_CHANGED = \"20180120-1525\" COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules Handling code \u00b6 The code first initializes the server which and when started, creates the Shutdown Hook . Java handles certain signals in specific ways, as can be found in this table for linux. For more information, you can read the docs from Oracle . public class DockerApp { public static void main ( String [] args ) { ServiceLoader < Logger > loggers = ServiceLoader . load ( Logger . class ); Logger logger = loggers . findFirst (). isPresent () ? loggers . findFirst (). get () : null ; if ( logger == null ) { System . err . println ( \"Did not find any loggers, quiting\" ); System . exit ( 1 ); } logger . start ( LogLevel . INFO ); int pseudoRandom = new Random (). nextInt ( ProtocolConstants . POTENTIAL_SERVER_NAMES . length - 1 ); String serverName = ProtocolConstants . POTENTIAL_SERVER_NAMES [ pseudoRandom ] ; int listenPort = ProtocolConstants . EXTERNAL_COMMUNICATION_PORT_A ; String multicastGroup = ProtocolConstants . MULTICAST_GROUP ; DuiServer distributedServer = DuiServerFactory . newDistributedServer ( listenPort , multicastGroup , serverName , logger ); distributedServer . logMembership (); ExecutorService executorService = Executors . newFixedThreadPool ( 1 ); executorService . submit ( distributedServer :: startServer ); long threadId = Thread . currentThread (). getId (); Runtime . getRuntime (). addShutdownHook ( new Thread (() -> { System . out . println ( \"Shutdown hook called!\" ); logger . log ( LogLevel . WARN , \"App\" , \"ShotdownHook\" , threadId , \"Shutting down at request of Docker\" ); distributedServer . stopServer (); distributedServer . closeServer (); executorService . shutdown (); try { Thread . sleep ( 100 ); executorService . shutdownNow (); logger . stop (); } catch ( InterruptedException e ) { e . printStackTrace (); } })); } } Java Plain (Kubernetes) \u00b6 So far we've utilized the utilities from Docker itself in conjunction with it's native Docker Swarm orchestrator. Unfortunately, when it comes to popularity Kubernetes beats Swarm hands down . So this isn't complete if it doesn't also do graceful shutdown in Kubernetes. In Dockerfile \u00b6 Our original file had to be changed, as Debian's Slim image doesn't actually contain the kill package. And we need a kill package, as we cannot instruct Kubernetes to issue a specific SIGNAL. Instead, we can issue a PreStop exec command , which we can utilise to execute a killall java -INT . The command will be specified in the Kubernetes deployment definition below. FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name \"*.java\" ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" LABEL version = \"0.1.0\" LABEL description = \"Docker image for playing with java applications in a concurrent, parallel and distributed manor.\" # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ \"/tini\" , \"-vv\" , \"-g\" , \"--\" , \"/usr/bin/dui/bin/dui\" ] ENV DATE_CHANGED = \"20180120-1525\" RUN apt-get update && apt-get install --no-install-recommends -y psmisc = 22 .* && rm -rf /var/lib/apt/lists/* COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules Kubernetes Deployment \u00b6 So here we have the image's K8s Deployment descriptor. Including the Pod's lifecycle preStop with a exec style command. You should know by now why we prefer that . apiVersion : extensions/v1beta1 kind : Deployment metadata : name : dui-deployment namespace : default labels : k8s-app : dui spec : replicas : 3 template : metadata : labels : k8s-app : dui spec : containers : - name : master image : caladreas/buming ports : - name : http containerPort : 7777 lifecycle : preStop : exec : command : [ \"killall\" , \"java\" , \"-INT\" ] terminationGracePeriodSeconds : 60 Java Spring Boot (1.x) \u00b6 This example is for Spring Boot 1.x, in time we will have an example for 2.x. This example is for the scenario of a Fat Jar with Tomcat as container [^8]. Execute example \u00b6 docker-compose build Execute the following command: docker run --rm -ti --name test spring-boot-graceful Exit the application/container via ctrl+c and you should see the application shutting down gracefully. 2018 -01-30 13 :35:46.327 INFO 7 --- [ Thread-3 ] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@6e5e91e4: startup date [ Tue Jan 30 13 :35:42 GMT 2018 ] ; root of context hierarchy 2018 -01-30 13 :35:46.405 INFO 7 --- [ Thread-3 ] BootGracefulApplication $GracefulShutdown : Tomcat was shutdown gracefully within the allotted time. 2018 -01-30 13 :35:46.408 INFO 7 --- [ Thread-3 ] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans on shutdown Dockerfile \u00b6 FROM maven:3-jdk-8 AS build ENV MAVEN_OPTS = -Dmaven.repo.local = /usr/share/maven/repository ENV WORKDIR = /usr/src/graceful RUN mkdir $WORKDIR WORKDIR $WORKDIR COPY pom.xml $WORKDIR RUN mvn -B -e org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline COPY . $WORKSPACE RUN mvn -B -e clean verify FROM anapsix/alpine-java:8_jdk_unlimited LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ \"/tini\" , \"-vv\" , \"-g\" , \"--\" ] ENV DATE_CHANGED = \"20180120-1525\" COPY --from = build /usr/src/graceful/target/spring-boot-graceful.jar /app.jar CMD [ \"java\" , \"-Xms256M\" , \"-Xmx480M\" , \"-Djava.security.egd=file:/dev/./urandom\" , \"-jar\" , \"/app.jar\" ] Docker compose file \u00b6 version : \"3.5\" services : web : image : spring-boot-graceful build : . stop_signal : SIGINT Java handling code \u00b6 package com.github.joostvdg.demo.springbootgraceful ; import org.springframework.boot.SpringApplication ; import org.springframework.boot.autoconfigure.SpringBootApplication ; import org.apache.catalina.connector.Connector ; import org.apache.tomcat.util.threads.ThreadPoolExecutor ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer ; import org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatConnectorCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory ; import org.springframework.context.ApplicationListener ; import org.springframework.context.annotation.Bean ; import org.springframework.context.event.ContextClosedEvent ; import java.util.concurrent.Executor ; import java.util.concurrent.TimeUnit ; @SpringBootApplication public class SpringBootGracefulApplication { public static void main ( String [] args ) { SpringApplication . run ( SpringBootGracefulApplication . class , args ); } @Bean public GracefulShutdown gracefulShutdown () { return new GracefulShutdown (); } @Bean public EmbeddedServletContainerCustomizer tomcatCustomizer () { return new EmbeddedServletContainerCustomizer () { @Override public void customize ( ConfigurableEmbeddedServletContainer container ) { if ( container instanceof TomcatEmbeddedServletContainerFactory ) { (( TomcatEmbeddedServletContainerFactory ) container ) . addConnectorCustomizers ( gracefulShutdown ()); } } }; } private static class GracefulShutdown implements TomcatConnectorCustomizer , ApplicationListener < ContextClosedEvent > { private static final Logger log = LoggerFactory . getLogger ( GracefulShutdown . class ); private volatile Connector connector ; @Override public void customize ( Connector connector ) { this . connector = connector ; } @Override public void onApplicationEvent ( ContextClosedEvent event ) { this . connector . pause (); Executor executor = this . connector . getProtocolHandler (). getExecutor (); if ( executor instanceof ThreadPoolExecutor ) { try { ThreadPoolExecutor threadPoolExecutor = ( ThreadPoolExecutor ) executor ; threadPoolExecutor . shutdown (); if ( ! threadPoolExecutor . awaitTermination ( 30 , TimeUnit . SECONDS )) { log . warn ( \"Tomcat thread pool did not shut down gracefully within \" + \"30 seconds. Proceeding with forceful shutdown\" ); } else { log . info ( \"Tomcat was shutdown gracefully within the allotted time.\" ); } } catch ( InterruptedException ex ) { Thread . currentThread (). interrupt (); } } } } } Example with Docker Swarm \u00b6 For now there's only an example with docker swarm , in time there will also be a Kubernetes example. Now that you can create Java applications packaged neatly in Docker images that support graceful shutdown, it would be nice to utilize. A good scenario would be a microservices architecture where services can come and go, but are registered in a service registry such as Eureka . Or a membership based protocol where members interact with each other and perhaps shard data. In these cases, of course the interactions are designed to be fault tolerant and discover faulty nodes on their own. But wouldn't it be better that if you knew you're going to quit, you inform the rest? We can reuse the caladreas/buming image and make it a docker swarm stack and run the service on every node. This way, we can easily see members coming and going and reduce the time to detect failure by notifying our peers of our impeding end. Docker swarm cluster \u00b6 Setting up a docker swarm cluster is easy, but has some requirements: virtual box 4.x+ docker-machine 1.12+ docker 17.06+ Warn Make sure this is the first and only virtualbox docker-machine VM being created/running, so that the ip range starts with 192.168.99.100 docker-machine create --driver virtualbox dui-1 docker-machine create --driver virtualbox dui-2 docker-machine create --driver virtualbox dui-3 eval \" $( docker-machine env dui-1 ) \" IP = 192 .168.99.100 docker swarm init --advertise-addr $IP TOKEN = $( docker swarm join-token -q worker ) eval \" $( docker-machine env dui-2 ) \" docker swarm join --token ${ TOKEN } ${ IP } :2377 eval \" $( docker-machine env dui-3 ) \" docker swarm join --token ${ TOKEN } ${ IP } :2377 eval \" $( docker-machine env dui-1 ) \" docker node ls Docker swarm network and multicast \u00b6 Unfortunately, docker swarm's swarm mode network overlay does not support multicast [ 9][ 10]. Why is this a problem? Well, the application I use to test the graceful shutdown requires this, sorry. Luckily there is a very easy solution for this, its by using Weavenet 's docker network plugin. Don't want to know about it or how you install it? Don't worry, just execute the script below. #!/usr/bin/env bash echo \"=> Prepare dui-2\" eval \" $( docker-machine env dui-2 ) \" docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo \"=> Prepare dui-3\" eval \" $( docker-machine env dui-3 ) \" docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo \"=> Prepare dui-1\" eval \" $( docker-machine env dui-1 ) \" docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 docker network create --driver = weaveworks/net-plugin:2.1.3 --opt works.weave.multicast = true --attachable dui Docker stack \u00b6 Now to create a service that runs on every node it is the easiest to create a docker stack . Compose file (docker-stack.yml) \u00b6 version : \"3.5\" services : dui : image : caladreas/buming build : . stop_signal : SIGINT networks : - dui deploy : mode : global networks : dui : external : true Create stack \u00b6 docker stack deploy --compose-file docker-stack.yml buming Execute example \u00b6 Now that we have a docker swarm cluster and a stack - which has a service running on every node - we can showcase the power of graceful shutdown in a cluster of dependent services. Confirm the service is running correctly on every node, first lets check our nodes. eval \" $( docker-machine env dui-1 ) \" docker node ls Which should look like this: ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS f21ilm4thxegn5xbentmss5ur * dui-1 Ready Active Leader y7475bo5uplt2b58d050b4wfd dui-2 Ready Active 6ssxola6y1i6h9p8256pi7bfv dui-3 Ready Active Then check the service. docker service ps buming_dui Which should look like this. ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 3mrpr0jg31x1 buming_dui.6ssxola6y1i6h9p8256pi7bfv dui:latest dui-3 Running Running 17 seconds ago pfubtiy4j7vo buming_dui.f21ilm4thxegn5xbentmss5ur dui:latest dui-1 Running Running 17 seconds ago f4gjnmhoe3y4 buming_dui.y7475bo5uplt2b58d050b4wfd dui:latest dui-2 Running Running 17 seconds ago Now open a second terminal window. In window one, follow the service logs: eval \" $( docker-machine env dui-1 ) \" docker service logs -f buming_dui In window two, go to a different node and stop the container. eval \" $( docker-machine env dui-2 ) \" docker ps docker stop buming_dui.y7475bo5uplt2b58d050b4wfd.pnoui2x6elrz0tvkjz51njz94 In this case, you will see the other nodes receiving a leave notice and then the node stopping. buming_dui.0.ryd8szexxku3@dui-3 | [ Server-John D. Carmack ] [ WARN ] [ 14 :19:02.604011 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = '83918f6ad817' , ip = '10.0.0.7' , name = 'Ken Thompson' } buming_dui.0.so5m14sz8ksh@dui-1 | [ Server-Alan Kay ] [ WARN ] [ 14 :19:02.602082 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = '83918f6ad817' , ip = '10.0.0.7' , name = 'Ken Thompson' } buming_dui.0.pnoui2x6elrz@dui-2 | Shutdown hook called! buming_dui.0.pnoui2x6elrz@dui-2 | [ App ] [ WARN ] [ 14 :19:02.598759 ] [ 1 ] [ ShotdownHook ] Shutting down at request of Docker buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.598858 ] [ 12 ] [ Main ] Stopping buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.601008 ] [ 12 ] [ Main ] Closing Further reading \u00b6 Wikipedia page on reboots Microsoft about graceful shutdown Gracefully stopping docker containers What to know about Java and shutdown hooks https://www.weave.works/blog/docker-container-networking-multicast-fast/ https://www.weave.works/docs/net/latest/install/plugin/plugin-how-it-works/ https://www.weave.works/docs/net/latest/install/plugin/plugin-v2/ https://www.auzias.net/en/docker-network-multihost/ https://forums.docker.com/t/cannot-get-zookeeper-to-work-running-in-docker-using-swarm-mode/27109 https://github.com/docker/libnetwork/issues/740","title":"Docker Graceful Shutdown"},{"location":"blogs/graceful-shutdown/#gracefully-shutting-down-applications-in-docker","text":"I'm not sure about you, but I prefer it when my neighbors leave our shared spaces clean and don't take up parking spaces when they don't need them. Imagine you live in an apartment complex with the above-mentioned parking lot. Some tenants go away and never come back. If nothing is done to clean up after them - to reclaim their apartment and parking space - then after some time, more and more apartments are unavailable for no reason, and the parking lot fills with cars which belong to no one. Some tenants did not get a parking lot and are getting frustrated that none are becoming available. When they moved in, they were told that when others leave, they would be next in line. While they're waiting, they have to park outside the complex. Eventually, the entrance gets blocked and no one can enter or leave. The end result is a completely unlivable apartment block with trapped tenants - never to be seen or heard from again. If you agree with me that when a tenant leaves, they should clean the apartment and free the parking spot to make it ready for the next inhabitant; then please read on. We're going to dive into the equivalent of doing this with containers. We will explore running our containers with Docker (run, compose, swarm) and Kubernetes. Even if you use another way to run your containers, this article should provide you with enough insight to get you on your way.","title":"Gracefully Shutting Down Applications in Docker"},{"location":"blogs/graceful-shutdown/#the-case-for-graceful-shutdown","text":"We're in an age where many applications are running in Docker containers across a multitude of clusters. These applications are then confronted with new concerns to tackle such as more moving parts, networking between these parts, remote storage and others. One significant way we defend ourselves against the perils of this distributed nature is to make our applications more robust - able to survive errors. However, even then there is still no guarantee your application is always up and running. So another concern we should tackle is how it responds when it needs to shut down. Where we can differentiate between an unexpected shutdown - we crashed - or an expected shutdown. On top of that, failing instead of trying to recover when something bad happens also adheres to \"fail fast\" - as strongly advocated by Michael Nygard in ReleaseIt. Shutting down can happen for a variety of reasons, in this post we dive into how to deal with an expected shutdown such as it being told to stop by an orchestrator such as Kubernetes. Containers can be purposefully shut down for a variety of reasons, including but not limited too: your application's health check fails your application consumed more resources than allowed the application is scaling down Just as cleaning up when leaving makes you a better tenant, having your application clean up connections, resources Moreover, the more tenants behaving in a good way increases the quality of living for all tenants. In our case, it improves the reliability and consistency of our cluster. Graceful shutdown is not unique to Docker, as it has been part of Linux's best practices for quite some years before Docker's existence. However, applying them to Docker container adds extra dimensions.","title":"The case for graceful shutdown"},{"location":"blogs/graceful-shutdown/#start-good-so-you-can-end-well","text":"When you sign up for an apartment, you probably have to sign a contract detailing your rights and obligations. The more you state explicitly, the easier it is to deal with bad behaving neighbors. The same is true when running processes; we should make sure that we set the rules, obligations, and expectations from the start. As we say in Dutch: a good beginning is half the work. We will start with how you can run a process in a container with a process that shuts down gracefully. There are many ways to start a process in a container. In this article, we look at processes started by commands defined in a Dockerfile. There are two ways to specify this: CMD : runs a command when the container gets started ENTRYPOINT : provides the location (entrypoint) from where commands get run when the container starts You need at least one ENTRYPOINT or CMD in a Dockerfile for it to be valid. They can be used in collaboration but they can do similar things. For more information on the details of these commands, read Docker's docs on Entrypoint vs. CMD .","title":"Start Good So You Can End Well"},{"location":"blogs/graceful-shutdown/#docker-shell-form-example","text":"We start with the shell form and see if it can do what we want; begin in such a way, we can stop it nicely. Shell form means we define a shell command without any special format or keywords. Please create Dockerfile with the content that follows. FROM ubuntu:18.04 ENTRYPOINT top -b Then build an image and run a container. docker image build --tag shell-form . docker run --name shell-form --rm shell-form The above command yields the following output. top - 16 :34:56 up 1 day, 5 :15, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 541984 free, 302668 used, 1202280 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1579380 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 4624 760 696 S 0 .0 0 .0 0 :00.05 sh 6 root 20 0 36480 2928 2580 R 0 .0 0 .1 0 :00.01 top As you can see, two processes are running, sh and top . Meaning, that killing the process, with ctrl+c for example, terminates the sh process, but not top . This happens because the sh process forked the top process, but the termination will only be send to PID 1 - in this case sh . As sh will not stop the top process for us it will continue running and leave the container alive. To kill this container, open a second terminal and execute the following command. docker rm -f shell-form Shell form doesn't do what we need. Starting a process with shell form will only lead us to the disaster of parking lots filling up unless there's a someone actively cleaning up.","title":"Docker Shell form example"},{"location":"blogs/graceful-shutdown/#docker-exec-form-example","text":"This leads us to the exec form. Hopefully, this gets us somewhere. The exec form is written as an array of parameters: ENTRYPOINT [\"top\", \"-b\"] To continue in the same line of examples, we will create a Dockerfile, build and run it. FROM ubuntu:18.04 ENTRYPOINT [ \"top\" , \"-b\" ] Then build and run it. docker image build --tag exec-form . docker run --name exec-form --rm exec-form This yields the following output. top - 18 :12:30 up 1 day, 6 :53, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 535896 free, 307196 used, 1203840 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1574880 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 36480 2940 2584 R 0 .0 0 .1 0 :00.03 top Now we got something we can work with. If something would tell this Container to stop, it will tell our only running process so it is sure to reach the correct one!","title":"Docker exec form example"},{"location":"blogs/graceful-shutdown/#gotchas","text":"Knowing we can use the exec form for our goal - gracefully shutting down our container - we can move on to the next part of our efforts. For the sake of imparting you with some hard learned lessons, we will explore two gotchas. They're optional, so you can also choose to skip to Make Sure Your Process Listens .","title":"Gotchas"},{"location":"blogs/graceful-shutdown/#docker-exec-form-with-parameters","text":"A caveat with the exec form is that it doesn't interpolate parameters. You can try the following: FROM ubuntu:18.04 ENV PARAM = \"-b\" ENTRYPOINT [ \"top\" , \"${PARAM}\" ] Then build and run it: docker image build --tag exec-param . docker run --name exec-form --rm exec-param This should yield the following: /bin/sh: 1 : [ top: not found This is where Docker created a mix between the two styles. It allows you to create an Entrypoint with a shell command - performing interpolation - but executing it as an exec form. This can be done by prefixing the shell form, with, you guessed it, exec . FROM ubuntu:18.04 ENV PARAM = \"-b\" ENTRYPOINT exec \"top\" \" ${ PARAM } \" Then build and run it: docker image build --tag exec-param . docker run --name exec-form --rm exec-param This will return the exact same as if we would've run ENTRYPOINT [\"top\", \"-b\"] . Now you can also override the param, by using the environment variable flag. docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = \"help\" exec-param Resulting in top's help string.","title":"Docker exec form with parameters"},{"location":"blogs/graceful-shutdown/#the-special-case-of-alpine","text":"One of the main best practices for Dockerfiles , is to make them as small as possible. The easiest way to do this is to start with a minimal image. This is where Alpine Linux comes in. We will revisit out shell form example, but replace ubuntu with alpine. Create the following Dockerfile. FROM alpine:3.8 ENTRYPOINT top -b Then build and run it. docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = \"help\" exec-param This yields the following output. Mem: 1509068K used, 537864K free, 640K shrd, 126756K buff, 1012436K cached CPU: 0 % usr 0 % sys 0 % nic 100 % idle 0 % io 0 % irq 0 % sirq Load average: 0 .00 0 .00 0 .00 2 /404 5 PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND 1 0 root R 1516 0 % 0 0 % top -b Aside from top 's output looking a bit different, there is only one command. Alpine Linux helps us avoid the problem of shell form altogether!","title":"The special case of Alpine"},{"location":"blogs/graceful-shutdown/#make-sure-your-process-listens","text":"It is excellent if your tenants are all signed up, know their rights and obligations. But you can't contact them when something happens, how will they ever know when to act? Translating that into our process. It starts and can be told to shut down, but does it process listen? Can it interpret the message it gets from Docker or Kubernetes? And if it does, can it relay the message correctly to its Child Processes? In order for your process to gracefully shutdown, it should know when to do so. As such, it should listen not only for itself but also on behalf of its children - yours never do anything wrong though! Some processes do, but many aren't designed to listen or tell their Children . They expect someone else to listen for them and tell them and their children - process managers. In order to listen to these signals , we can call in the help of others. We will look at two options. we let Docker manage the process and its children we use a process manager","title":"Make Sure Your Process Listens"},{"location":"blogs/graceful-shutdown/#let-docker-manage-it-for-us","text":"If you're not using Docker to run or manage your containers, you should skip to Depend on a process manager . Docker has a build in feature, that it uses a lightweight process manager to help you. So if you're running your images with Docker itself, either directly or via Compose or Swarm, you're fine. You can use the init flag in your run command or your compose file. Please, note that the below examples require a certain minimum version of Docker. run - 1.13+ compose (v 2.2) - 1.13.0+ swarm (v 3.7) - 18.06.0+","title":"Let Docker manage it for us"},{"location":"blogs/graceful-shutdown/#with-docker-run","text":"docker run --rm -ti --init caladreas/dui","title":"With Docker Run"},{"location":"blogs/graceful-shutdown/#with-docker-compose","text":"version : '2.2' services : web : image : caladreas/java-docker-signal-demo:no-tini init : true","title":"With Docker Compose"},{"location":"blogs/graceful-shutdown/#with-docker-swarm","text":"version : '3.7' services : web : image : caladreas/java-docker-signal-demo:no-tini init : true Relying on Docker does create a dependency on how your container runs. It only runs correctly in Docker-related technologies (run, compose, swarm) and only if the proper versions are available. Creating either a different experience for users running your application somewhere else or not able to meet the version requirements. So maybe another solution is to bake a process manager into your image and guarantee its behavior.","title":"With Docker Swarm"},{"location":"blogs/graceful-shutdown/#depend-on-a-process-manager","text":"One of our goals for Docker images is to keep them small. We should look for a lightweight process manager. It does not have too many a whole machine worth or processes, just one and perhaps some children. Here we would like to introduce you to Tini , a lightweight process manager designed for this purpose . It is a very successful and widely adopted process manager in the Docker world. So successful, that the before mentioned init flags from Docker are implemented by baking Tini into Docker .","title":"Depend on a process manager"},{"location":"blogs/graceful-shutdown/#debian-example","text":"For brevity, the build process is excluded, and for image size, we use Debian slim instead of default Debian. FROM debian:stable-slim ENV TINI_VERSION v0.18.0 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ \"/tini\" , \"-vv\" , \"-g\" , \"--\" , \"/usr/bin/dui/bin/dui\" , \"-XX:+UseCGroupMemoryLimitForHeap\" , \"-XX:+UnlockExperimentalVMOptions\" ] COPY --from = build /usr/bin/dui-image/ /usr/bin/dui","title":"Debian example"},{"location":"blogs/graceful-shutdown/#alpine-example","text":"Alpine Linux works wonders for Docker images, so to improve our lives, you can very easily install it if you want. FROM alpine RUN apk add --no-cache tini ENTRYPOINT [ \"/sbin/tini\" , \"-vv\" , \"-g\" , \"-s\" , \"--\" ] CMD [ \"top -b\" ]","title":"Alpine example"},{"location":"blogs/graceful-shutdown/#how-to-be-told-what-you-want-to-hear","text":"You've made it this far; your tenets are reachable so you can inform them if they need to act. However, there's another problem lurking around the corner. Do they speak your language? Our process now starts knowing it can be talked to, it has someone who takes care of listening for it and its children. Now we need to make sure it can understand what it hears, it should be able to handle the incoming signals. We have two main ways of doing this. Handle signals as they come : we should make sure our process deal with the signals as they come State the signals we want : we can also tell up front, which signals we want to hear and put the burden of translation on our callers For more details on the subject of Signals and Docker, please read this excellent blog from Grigorii Chudnov .","title":"How To Be Told What You Want To Hear"},{"location":"blogs/graceful-shutdown/#handle-signals-as-they-come","text":"Handling process signals depend on your application, programming language or framework.","title":"Handle signals as they come"},{"location":"blogs/graceful-shutdown/#state-the-signals-we-want","text":"Sometimes your language or framework of choice, doesn't handle signals all that well. It might be very rigid in what it does with specific signals, removing your ability to do the right thing. Of course, not all languages or frameworks are designed with Docker container or Microservices in mind, are yet to catch up to this more dynamic environment. Luckily Docker and Kubernetes allow you to specify what signal too sent to your process.","title":"State the signals we want"},{"location":"blogs/graceful-shutdown/#docker-run","text":"docker run --rm -ti --init --stop-signal = SIGINT \\ caladreas/java-docker-signal-demo","title":"Docker run"},{"location":"blogs/graceful-shutdown/#docker-composeswarm","text":"Docker's compose file format allows you to specify a stop signal . This is the signal sent when the container is stopped in a normal fashion. Normal in this case, meaning docker stop or when docker itself determines it should stop the container. If you forcefully remove the container, for example with docker rm -f it will directly kill the process, so don't do that. version : '2.2' services : web : image : caladreas/java-docker-signal-demo stop_signal : SIGINT stop_grace_period : 15s If you run this with docker-compose up and then in a second terminal, stop the container, you will see something like this. web_1 | HelloWorld! web_1 | Shutdown hook called! web_1 | We 're told to stop early... web_1 | java.lang.InterruptedException: sleep interrupted web_1 | at java.base/java.lang.Thread.sleep(Native Method) web_1 | at joostvdg.demo.signal@1.0/com.github.joostvdg.demo.signal.HelloWorld.printHelloWorld(Unknown Source) web_1 | at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) web_1 | at java.base/java.util.concurrent.FutureTask.run(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) web_1 | at java.base/java.lang.Thread.run(Unknown Source) web_1 | [DEBUG tini (1)] Passing signal: ' Interrupt ' web_1 | [DEBUG tini (1)] Received SIGCHLD web_1 | [DEBUG tini (1)] Reaped child with pid: ' 7 ' web_1 | [INFO tini (1)] Main child exited with signal (with signal ' Interrupt ' )","title":"Docker compose/swarm"},{"location":"blogs/graceful-shutdown/#kubernetes","text":"In Kubernetes we can make use of Container Lifecycle Hooks to manage how our container should be stopped. We could, for example, send a SIGINT (interrupt) to tell our application to stop. apiVersion : apps/v1 kind : Deployment metadata : name : java-signal-demo namespace : default labels : app : java-signal-demo spec : replicas : 1 template : metadata : labels : app : java-signal-demo spec : containers : - name : main image : caladreas/java-docker-signal-demo lifecycle : preStop : exec : command : [ \"killall\" , \"java\" , \"-INT\" ] terminationGracePeriodSeconds : 60 When you create this as deployment.yml, create and delete it - kubectl apply -f deployment.yml / kubectl delete -f deployment.yml - you will see the same behavior.","title":"Kubernetes"},{"location":"blogs/graceful-shutdown/#how-to-be-told-when-you-want-to-hear-it","text":"Our process now will now start knowing it will hear what it wants to hear. But we now have to make sure we hear it when we need to hear it. An intervention is excellent when you can still be saved, but it is a bit useless if you're already dead.","title":"How To Be Told When You Want To Hear It"},{"location":"blogs/graceful-shutdown/#docker","text":"You can either configure your health check in your Dockerfile or configure it in your docker-compose.yml for either compose or swarm. Considering only Docker can use the health check in your Dockerfile, it is strongly recommended to have health checks in your application and document how they can be used.","title":"Docker"},{"location":"blogs/graceful-shutdown/#kubernetes_1","text":"In Kubernetes we have the concept of Container Probes . This allows you to configure whether your container is ready (readinessProbe) to be used and if it is still working as expected (livenessProbe).","title":"Kubernetes"},{"location":"blogs/graceful-shutdown/#examples","text":"How to actually listen to the signals and determine which one to use will depend on your programming language. There's three examples I have worked out, one for Go (lang) and two for Java: pojo and Spring Boot.","title":"Examples"},{"location":"blogs/graceful-shutdown/#go","text":"","title":"Go"},{"location":"blogs/graceful-shutdown/#dockerfile","text":"# build stage FROM golang:latest AS build-env RUN go get -v github.com/docker/docker/client/... RUN go get -v github.com/docker/docker/api/... ADD src/ $GOPATH /flow-proxy-service-lister WORKDIR $GOPATH /flow-proxy-service-lister RUN go build -o main -tags netgo main.go # final stage FROM alpine ENTRYPOINT [ \"/app/main\" ] COPY --from = build-env /go/flow-proxy-service-lister/main /app/ RUN chmod +x /app/main","title":"Dockerfile"},{"location":"blogs/graceful-shutdown/#go-code-for-graceful-shutdown","text":"The following is a way for Go to shutdown a http server when receiving a termination signal. func main () { c := make ( chan bool ) // make channel for main <--> webserver communication go webserver . Start ( \"7777\" , webserverData , c ) // ignore the missing data stop := make ( chan os . Signal , 1 ) // make a channel that listens to is signals signal . Notify ( stop , syscall . SIGINT , syscall . SIGTERM ) // we listen to some specific syscall signals for i := 1 ; ; i ++ { // this is still infinite t := time . NewTicker ( time . Second * 30 ) // set a timer for the polling select { case <- stop : // this means we got a os signal on our channel break // so we can stop case <- t . C : // our timer expired, refresh our data continue // and continue with the loop } break } fmt . Println ( \"Shutting down webserver\" ) // if we got here, we have to inform the webserver to close shop c <- true // we do this by sending a message on the channel if b := <- c ; b { // when we get true back, that means the webserver is doing with a graceful shutdown fmt . Println ( \"Webserver shut down\" ) // webserver is done } fmt . Println ( \"Shut down app\" ) // we can close shop ourselves now }","title":"Go code for graceful shutdown"},{"location":"blogs/graceful-shutdown/#java-plain-docker-swarm","text":"This application is a Java 9 modular application, which can be found on github, github.com/joostvdg .","title":"Java plain (Docker Swarm)"},{"location":"blogs/graceful-shutdown/#dockerfile_1","text":"FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name \"*.java\" ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" LABEL version = \"0.1.0\" LABEL description = \"Docker image for playing with java applications in a concurrent, parallel and distributed manor.\" # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ \"/tini\" , \"-vv\" , \"-g\" , \"--\" , \"/usr/bin/dui/bin/dui\" ] ENV DATE_CHANGED = \"20180120-1525\" COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules","title":"Dockerfile"},{"location":"blogs/graceful-shutdown/#handling-code","text":"The code first initializes the server which and when started, creates the Shutdown Hook . Java handles certain signals in specific ways, as can be found in this table for linux. For more information, you can read the docs from Oracle . public class DockerApp { public static void main ( String [] args ) { ServiceLoader < Logger > loggers = ServiceLoader . load ( Logger . class ); Logger logger = loggers . findFirst (). isPresent () ? loggers . findFirst (). get () : null ; if ( logger == null ) { System . err . println ( \"Did not find any loggers, quiting\" ); System . exit ( 1 ); } logger . start ( LogLevel . INFO ); int pseudoRandom = new Random (). nextInt ( ProtocolConstants . POTENTIAL_SERVER_NAMES . length - 1 ); String serverName = ProtocolConstants . POTENTIAL_SERVER_NAMES [ pseudoRandom ] ; int listenPort = ProtocolConstants . EXTERNAL_COMMUNICATION_PORT_A ; String multicastGroup = ProtocolConstants . MULTICAST_GROUP ; DuiServer distributedServer = DuiServerFactory . newDistributedServer ( listenPort , multicastGroup , serverName , logger ); distributedServer . logMembership (); ExecutorService executorService = Executors . newFixedThreadPool ( 1 ); executorService . submit ( distributedServer :: startServer ); long threadId = Thread . currentThread (). getId (); Runtime . getRuntime (). addShutdownHook ( new Thread (() -> { System . out . println ( \"Shutdown hook called!\" ); logger . log ( LogLevel . WARN , \"App\" , \"ShotdownHook\" , threadId , \"Shutting down at request of Docker\" ); distributedServer . stopServer (); distributedServer . closeServer (); executorService . shutdown (); try { Thread . sleep ( 100 ); executorService . shutdownNow (); logger . stop (); } catch ( InterruptedException e ) { e . printStackTrace (); } })); } }","title":"Handling code"},{"location":"blogs/graceful-shutdown/#java-plain-kubernetes","text":"So far we've utilized the utilities from Docker itself in conjunction with it's native Docker Swarm orchestrator. Unfortunately, when it comes to popularity Kubernetes beats Swarm hands down . So this isn't complete if it doesn't also do graceful shutdown in Kubernetes.","title":"Java Plain (Kubernetes)"},{"location":"blogs/graceful-shutdown/#in-dockerfile","text":"Our original file had to be changed, as Debian's Slim image doesn't actually contain the kill package. And we need a kill package, as we cannot instruct Kubernetes to issue a specific SIGNAL. Instead, we can issue a PreStop exec command , which we can utilise to execute a killall java -INT . The command will be specified in the Kubernetes deployment definition below. FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name \"*.java\" ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" LABEL version = \"0.1.0\" LABEL description = \"Docker image for playing with java applications in a concurrent, parallel and distributed manor.\" # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ \"/tini\" , \"-vv\" , \"-g\" , \"--\" , \"/usr/bin/dui/bin/dui\" ] ENV DATE_CHANGED = \"20180120-1525\" RUN apt-get update && apt-get install --no-install-recommends -y psmisc = 22 .* && rm -rf /var/lib/apt/lists/* COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules","title":"In Dockerfile"},{"location":"blogs/graceful-shutdown/#kubernetes-deployment","text":"So here we have the image's K8s Deployment descriptor. Including the Pod's lifecycle preStop with a exec style command. You should know by now why we prefer that . apiVersion : extensions/v1beta1 kind : Deployment metadata : name : dui-deployment namespace : default labels : k8s-app : dui spec : replicas : 3 template : metadata : labels : k8s-app : dui spec : containers : - name : master image : caladreas/buming ports : - name : http containerPort : 7777 lifecycle : preStop : exec : command : [ \"killall\" , \"java\" , \"-INT\" ] terminationGracePeriodSeconds : 60","title":"Kubernetes Deployment"},{"location":"blogs/graceful-shutdown/#java-spring-boot-1x","text":"This example is for Spring Boot 1.x, in time we will have an example for 2.x. This example is for the scenario of a Fat Jar with Tomcat as container [^8].","title":"Java Spring Boot (1.x)"},{"location":"blogs/graceful-shutdown/#execute-example","text":"docker-compose build Execute the following command: docker run --rm -ti --name test spring-boot-graceful Exit the application/container via ctrl+c and you should see the application shutting down gracefully. 2018 -01-30 13 :35:46.327 INFO 7 --- [ Thread-3 ] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@6e5e91e4: startup date [ Tue Jan 30 13 :35:42 GMT 2018 ] ; root of context hierarchy 2018 -01-30 13 :35:46.405 INFO 7 --- [ Thread-3 ] BootGracefulApplication $GracefulShutdown : Tomcat was shutdown gracefully within the allotted time. 2018 -01-30 13 :35:46.408 INFO 7 --- [ Thread-3 ] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans on shutdown","title":"Execute example"},{"location":"blogs/graceful-shutdown/#dockerfile_2","text":"FROM maven:3-jdk-8 AS build ENV MAVEN_OPTS = -Dmaven.repo.local = /usr/share/maven/repository ENV WORKDIR = /usr/src/graceful RUN mkdir $WORKDIR WORKDIR $WORKDIR COPY pom.xml $WORKDIR RUN mvn -B -e org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline COPY . $WORKSPACE RUN mvn -B -e clean verify FROM anapsix/alpine-java:8_jdk_unlimited LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ \"/tini\" , \"-vv\" , \"-g\" , \"--\" ] ENV DATE_CHANGED = \"20180120-1525\" COPY --from = build /usr/src/graceful/target/spring-boot-graceful.jar /app.jar CMD [ \"java\" , \"-Xms256M\" , \"-Xmx480M\" , \"-Djava.security.egd=file:/dev/./urandom\" , \"-jar\" , \"/app.jar\" ]","title":"Dockerfile"},{"location":"blogs/graceful-shutdown/#docker-compose-file","text":"version : \"3.5\" services : web : image : spring-boot-graceful build : . stop_signal : SIGINT","title":"Docker compose file"},{"location":"blogs/graceful-shutdown/#java-handling-code","text":"package com.github.joostvdg.demo.springbootgraceful ; import org.springframework.boot.SpringApplication ; import org.springframework.boot.autoconfigure.SpringBootApplication ; import org.apache.catalina.connector.Connector ; import org.apache.tomcat.util.threads.ThreadPoolExecutor ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer ; import org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatConnectorCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory ; import org.springframework.context.ApplicationListener ; import org.springframework.context.annotation.Bean ; import org.springframework.context.event.ContextClosedEvent ; import java.util.concurrent.Executor ; import java.util.concurrent.TimeUnit ; @SpringBootApplication public class SpringBootGracefulApplication { public static void main ( String [] args ) { SpringApplication . run ( SpringBootGracefulApplication . class , args ); } @Bean public GracefulShutdown gracefulShutdown () { return new GracefulShutdown (); } @Bean public EmbeddedServletContainerCustomizer tomcatCustomizer () { return new EmbeddedServletContainerCustomizer () { @Override public void customize ( ConfigurableEmbeddedServletContainer container ) { if ( container instanceof TomcatEmbeddedServletContainerFactory ) { (( TomcatEmbeddedServletContainerFactory ) container ) . addConnectorCustomizers ( gracefulShutdown ()); } } }; } private static class GracefulShutdown implements TomcatConnectorCustomizer , ApplicationListener < ContextClosedEvent > { private static final Logger log = LoggerFactory . getLogger ( GracefulShutdown . class ); private volatile Connector connector ; @Override public void customize ( Connector connector ) { this . connector = connector ; } @Override public void onApplicationEvent ( ContextClosedEvent event ) { this . connector . pause (); Executor executor = this . connector . getProtocolHandler (). getExecutor (); if ( executor instanceof ThreadPoolExecutor ) { try { ThreadPoolExecutor threadPoolExecutor = ( ThreadPoolExecutor ) executor ; threadPoolExecutor . shutdown (); if ( ! threadPoolExecutor . awaitTermination ( 30 , TimeUnit . SECONDS )) { log . warn ( \"Tomcat thread pool did not shut down gracefully within \" + \"30 seconds. Proceeding with forceful shutdown\" ); } else { log . info ( \"Tomcat was shutdown gracefully within the allotted time.\" ); } } catch ( InterruptedException ex ) { Thread . currentThread (). interrupt (); } } } } }","title":"Java handling code"},{"location":"blogs/graceful-shutdown/#example-with-docker-swarm","text":"For now there's only an example with docker swarm , in time there will also be a Kubernetes example. Now that you can create Java applications packaged neatly in Docker images that support graceful shutdown, it would be nice to utilize. A good scenario would be a microservices architecture where services can come and go, but are registered in a service registry such as Eureka . Or a membership based protocol where members interact with each other and perhaps shard data. In these cases, of course the interactions are designed to be fault tolerant and discover faulty nodes on their own. But wouldn't it be better that if you knew you're going to quit, you inform the rest? We can reuse the caladreas/buming image and make it a docker swarm stack and run the service on every node. This way, we can easily see members coming and going and reduce the time to detect failure by notifying our peers of our impeding end.","title":"Example with Docker Swarm"},{"location":"blogs/graceful-shutdown/#docker-swarm-cluster","text":"Setting up a docker swarm cluster is easy, but has some requirements: virtual box 4.x+ docker-machine 1.12+ docker 17.06+ Warn Make sure this is the first and only virtualbox docker-machine VM being created/running, so that the ip range starts with 192.168.99.100 docker-machine create --driver virtualbox dui-1 docker-machine create --driver virtualbox dui-2 docker-machine create --driver virtualbox dui-3 eval \" $( docker-machine env dui-1 ) \" IP = 192 .168.99.100 docker swarm init --advertise-addr $IP TOKEN = $( docker swarm join-token -q worker ) eval \" $( docker-machine env dui-2 ) \" docker swarm join --token ${ TOKEN } ${ IP } :2377 eval \" $( docker-machine env dui-3 ) \" docker swarm join --token ${ TOKEN } ${ IP } :2377 eval \" $( docker-machine env dui-1 ) \" docker node ls","title":"Docker swarm cluster"},{"location":"blogs/graceful-shutdown/#docker-swarm-network-and-multicast","text":"Unfortunately, docker swarm's swarm mode network overlay does not support multicast [ 9][ 10]. Why is this a problem? Well, the application I use to test the graceful shutdown requires this, sorry. Luckily there is a very easy solution for this, its by using Weavenet 's docker network plugin. Don't want to know about it or how you install it? Don't worry, just execute the script below. #!/usr/bin/env bash echo \"=> Prepare dui-2\" eval \" $( docker-machine env dui-2 ) \" docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo \"=> Prepare dui-3\" eval \" $( docker-machine env dui-3 ) \" docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo \"=> Prepare dui-1\" eval \" $( docker-machine env dui-1 ) \" docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 docker network create --driver = weaveworks/net-plugin:2.1.3 --opt works.weave.multicast = true --attachable dui","title":"Docker swarm network and multicast"},{"location":"blogs/graceful-shutdown/#docker-stack","text":"Now to create a service that runs on every node it is the easiest to create a docker stack .","title":"Docker stack"},{"location":"blogs/graceful-shutdown/#compose-file-docker-stackyml","text":"version : \"3.5\" services : dui : image : caladreas/buming build : . stop_signal : SIGINT networks : - dui deploy : mode : global networks : dui : external : true","title":"Compose file (docker-stack.yml)"},{"location":"blogs/graceful-shutdown/#create-stack","text":"docker stack deploy --compose-file docker-stack.yml buming","title":"Create stack"},{"location":"blogs/graceful-shutdown/#execute-example_1","text":"Now that we have a docker swarm cluster and a stack - which has a service running on every node - we can showcase the power of graceful shutdown in a cluster of dependent services. Confirm the service is running correctly on every node, first lets check our nodes. eval \" $( docker-machine env dui-1 ) \" docker node ls Which should look like this: ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS f21ilm4thxegn5xbentmss5ur * dui-1 Ready Active Leader y7475bo5uplt2b58d050b4wfd dui-2 Ready Active 6ssxola6y1i6h9p8256pi7bfv dui-3 Ready Active Then check the service. docker service ps buming_dui Which should look like this. ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 3mrpr0jg31x1 buming_dui.6ssxola6y1i6h9p8256pi7bfv dui:latest dui-3 Running Running 17 seconds ago pfubtiy4j7vo buming_dui.f21ilm4thxegn5xbentmss5ur dui:latest dui-1 Running Running 17 seconds ago f4gjnmhoe3y4 buming_dui.y7475bo5uplt2b58d050b4wfd dui:latest dui-2 Running Running 17 seconds ago Now open a second terminal window. In window one, follow the service logs: eval \" $( docker-machine env dui-1 ) \" docker service logs -f buming_dui In window two, go to a different node and stop the container. eval \" $( docker-machine env dui-2 ) \" docker ps docker stop buming_dui.y7475bo5uplt2b58d050b4wfd.pnoui2x6elrz0tvkjz51njz94 In this case, you will see the other nodes receiving a leave notice and then the node stopping. buming_dui.0.ryd8szexxku3@dui-3 | [ Server-John D. Carmack ] [ WARN ] [ 14 :19:02.604011 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = '83918f6ad817' , ip = '10.0.0.7' , name = 'Ken Thompson' } buming_dui.0.so5m14sz8ksh@dui-1 | [ Server-Alan Kay ] [ WARN ] [ 14 :19:02.602082 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = '83918f6ad817' , ip = '10.0.0.7' , name = 'Ken Thompson' } buming_dui.0.pnoui2x6elrz@dui-2 | Shutdown hook called! buming_dui.0.pnoui2x6elrz@dui-2 | [ App ] [ WARN ] [ 14 :19:02.598759 ] [ 1 ] [ ShotdownHook ] Shutting down at request of Docker buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.598858 ] [ 12 ] [ Main ] Stopping buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.601008 ] [ 12 ] [ Main ] Closing","title":"Execute example"},{"location":"blogs/graceful-shutdown/#further-reading","text":"Wikipedia page on reboots Microsoft about graceful shutdown Gracefully stopping docker containers What to know about Java and shutdown hooks https://www.weave.works/blog/docker-container-networking-multicast-fast/ https://www.weave.works/docs/net/latest/install/plugin/plugin-how-it-works/ https://www.weave.works/docs/net/latest/install/plugin/plugin-v2/ https://www.auzias.net/en/docker-network-multihost/ https://forums.docker.com/t/cannot-get-zookeeper-to-work-running-in-docker-using-swarm-mode/27109 https://github.com/docker/libnetwork/issues/740","title":"Further reading"},{"location":"blogs/jenkins-pipeline-support-tool/","text":"Jenkins Pipeline Support Tools \u00b6 With Jenkins now becoming Cloud Native and a first class citizen of Kubernetes, it is time to review how we use build tools. This content assumes you're using Jenkins in a Kubernetes cluster, but most of it should also work in other Docker-based environments. Ideal Pipeline \u00b6 Anyway, one thing we often see people do wrong with Jenkins pipelines is to use the Groovy Scripts as a general-purpose programming language. This creates many problems, bloated & complicated pipelines, much more stress on the master instead of on the build agent and generally making things unreliable. A much better way is to use Jenkins pipelines only as orchestration and lean heavily on your build tools - e.g., Maven, Gradle, Yarn, Bazel - and shell scripts. Alas, if you created complicated pipelines in Groovy scripts, it is likely you'll end up the same with Bash scripts. An even better solution would be to create custom CLI applications that take care of large operations and convoluted logic. You can test and reuse them. pipeline { agent any stages { stage ( 'Build' ) { steps { sh './build.sh' } } stage ( 'Test' ) { steps { sh './test.sh' } } stage ( 'Deploy' ) { steps { sh './deploy.sh' } } } post { success { sh './successNotification.sh' } failure { sh './failureNotification.sh' } } } Now, this might look a bit like a pipe dream, but it illustrates how you should use Jenkins Pipeline. The groovy script engine allows for a lot of freedom, but only rarely is its use justified. To create robust, modular and generic pipelines, it is recommended to use build tools, shell scripts, Shared Libraries and custom CLI's . It was always a bit difficult to manage generic scripts and tools across instances of Jenkins, pipelines, and teams. But with Pod Templates we have an excellent mechanism for using, versioning and distributing them with ease. Kubernetes Pods \u00b6 When Jenkins runs in Kubernetes, it can leverage it via the Kubernetes Plugin . I realize Jenkins conjures up mixed emotions when it comes to plugins, but this setup might replace most of them. How so? By using a Kubernetes Pod as the agent where instead of putting all your tools into a single VM you can use multiple small scoped containers. You can specify Pod Templates in multiple ways, where my personal favorite is to define it as yaml inside a declarative pipeline - see example below. For each tool you need, you specify the container and its configuration - if required. By default, you will always get a container with a Jenkins JNLP client and the workspace mounted as a volume in the pod. This allows you to create several tiny containers, each containing only the tools you need for a specific job. Now, it could happen you use two or more tools together a lot - let's say npm and maven - so it is ok to sometimes deviate from this to lower the overall memory of the pod. When you need custom logic, you will have to create a script or tool. This is where PodTemplate, Docker images and our desire for small narrow focus tools come together. PodTemplate example \u00b6 pipeline { agent { kubernetes { label 'mypod' defaultContainer 'jnlp' yaml \"\"\" apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:alpine command: - cat tty: true - name: busybox image: busybox command: - cat tty: true \"\"\" } } stages { stage ( 'Run maven' ) { steps { container ( 'maven' ) { sh 'mvn -version' } container ( 'busybox' ) { sh '/bin/busybox' } } } } } Java Example \u00b6 I bet most people do not think about Java when it comes lightweight CLI applications, but I think that is a shame. Java has excellent tooling to help you build well-tested applications which can be understood and maintained by a vast majority of developers. To make the images small, we will use some of the new tools available in Java land. We will first dive into using Java Modularity and JLink to create a compact and strict binary package, and then we move onto Graal for creating a Native image. Custom JDK Image \u00b6 All the source code of this example application is at github.com/joostvdg/jpb . It is a small CLI which does only one thing; it parses a git commit log to see which folders changed. Quite a useful tool for Monorepo's or other repositories containing more than one changeable resource. Such a CLI should have specific characteristics: * testable * small memory footprint * small disk footprint * quick start * easy to setup * easy to maintain These points sound like an excellent use case for Java Modules and JLink. For those who don't know, read up on Java Modules here and read up on JLink here . JLink will create a binary image that we can use with Alpine Linux to form a minimal Java (Docker) Image. Unfortunately, the plugins for Maven (JMod and JLink) seem to have died. The support on Gradle side is not much better. So I created a solution myself with a multi-stage Docker build. Which does detract a bit from the ease of setup. But overall, it hits the other characteristics spot on. Application Model \u00b6 For ease of maintenance and testing, we separate the parts of the CLI into Java Modules, as you can see in the model below. For using JLink, we need to be a module ourselves. So I figured to expand the exercise to use it to not only create boundaries via packages but also with Modules. Build \u00b6 The current LTS version of Java is 11, which means we need at least that if we want to be up-to-date. As we want to run the application in Alpine Linux, we need to build it with Alpine Linux - if you create a custom JDK image its OS specific. To my surprise, the official LTS release is not released for Alpine, so we use OpenJDK 12. Everything is built via a Multi-Stage Docker Build. This Dockerfile can be divided into five segments. creation of the base with a JDK 11+ on Alpine Linux compiling our Java Modules in into Module Jars test our code create our custom JDK image with just our code and whatever we need from the JDK create the runtime Docker image The Dockerfile looks a bit complicated, but we did get a Java runtime that is about 44MB in size and can run as a direct binary with no startup time. The Dockerfile can be much short if we use only a single module, but as our logic grows it is a thoughtful way to separate different concerns. Still, I'm not too happy with this for creating many small CLI's. To much handwork goes into creating the images like this. Relying on unmaintained Maven or Gradle Plugins doesn't seem a better choice. Luckily, there's a new game in town, GraalVM . We'll make an image with Graal next, stay tuned. Dockerfile \u00b6 ############################################################### ############################################################### ##### 1. CREATE ALPINE BASE WITH JDK11+ #### OpenJDK image produces weird results with JLink (400mb + sizes) FROM alpine:3.8 AS build ENV JAVA_HOME = /opt/jdk \\ PATH = ${ PATH } :/opt/jdk/bin \\ LANG = C.UTF-8 RUN set -ex && \\ apk add --no-cache bash && \\ wget https://download.java.net/java/early_access/alpine/18/binaries/openjdk-12-ea+18_linux-x64-musl_bin.tar.gz -O jdk.tar.gz && \\ mkdir -p /opt/jdk && \\ tar zxvf jdk.tar.gz -C /opt/jdk --strip-components = 1 && \\ rm jdk.tar.gz && \\ rm /opt/jdk/lib/src.zip #################################### ## 2.a PREPARE COMPILE PHASE RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src ## 2.b COMPILE ALL JAVA FILES RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name \"*.java\" ) ## 2.c CREATE ALL JAVA MODULE JARS RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.jpb.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.core.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.jpb.core . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.cli.jar --module-version 1 .0 -e com.github.joostvdg.jpb.cli.JpbApp \\ -C /usr/src/mods/compiled/joostvdg.jpb.cli . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.core.test.jar --module-version 1 .0 -e com.github.joostvdg.jpb.core.test.ParseChangeListTest \\ -C /usr/src/mods/compiled/joostvdg.jpb.core.test . #################################### ## 3 RUN TESTS RUN rm -rf /usr/bin/jpb-test-image RUN jlink \\ --verbose \\ --compress 2 \\ --no-header-files \\ --no-man-pages \\ --strip-debug \\ --limit-modules java.base \\ --launcher jpb-test = joostvdg.jpb.core.test \\ --module-path /usr/src/mods/jars/: $JAVA_HOME /jmods \\ --add-modules joostvdg.jpb.core.test \\ --add-modules joostvdg.jpb.core \\ --add-modules joostvdg.jpb.api \\ --output /usr/bin/jpb-test-image RUN /usr/bin/jpb-test-image/bin/java --list-modules RUN /usr/bin/jpb-test-image/bin/jpb-test #################################### ## 4 BUILD RUNTIME - CUSTOM JDK IMAGE RUN rm -rf /usr/bin/jpb-image RUN jlink \\ --verbose \\ --compress 2 \\ --no-header-files \\ --no-man-pages \\ --strip-debug \\ --limit-modules java.base \\ --launcher jpb = joostvdg.jpb.cli \\ --module-path /usr/src/mods/jars/: $JAVA_HOME /jmods \\ --add-modules joostvdg.jpb.cli \\ --add-modules joostvdg.jpb.api \\ --add-modules joostvdg.jpb.core \\ --output /usr/bin/jpb-image RUN /usr/bin/jpb-image/bin/java --list-modules #################################### ##### 5. RUNTIME IMAGE - ALPINE FROM panga/alpine:3.8-glibc2.27 LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" LABEL version = \"0.1.0\" LABEL description = \"Docker image for running Jenkins Pipeline Binary\" ENV DATE_CHANGED = \"20181014-2035\" ENV JAVA_OPTS = \"-XX:+UseCGroupMemoryLimitForHeap -XX:+UnlockExperimentalVMOptions\" COPY --from = build /usr/bin/jpb-image/ /usr/bin/jpb ENTRYPOINT [ \"/usr/bin/jpb/bin/jpb\" ] Image disk size \u00b6 REPOSITORY TAG IMAGE ID CREATED SIZE jpb latest af7dda45732a About a minute ago 43 .8MB Graal \u00b6 GraalVM is a universal virtual machine for running applications written in JavaScript, Python, Ruby, R, JVM-based languages like Java, Scala, Kotlin, Clojure, and LLVM-based languages such as C and C++. - graalvm.org Ok, that doesn't tell you why using GraalVM is excellent for creating small CLI docker images. Maybe this quote helps: Native images compiled with GraalVM ahead-of-time improve the startup time and reduce the memory footprint of JVM-based applications. Where JLink allows you to create a custom JDK image and embed your application as a runtime binary, Graal goes one step further. It replaces the VM altogether and uses Substrate VM to run your binary. It can't do a lot of the fantastic things the JVM can do and isn't suited for long running applications or those with a large memory footprint and so on. Well, our CLI applications are single shot executions with low memory footprint, the perfect fit for Graal/Substrate! All the code from this example can is on GitHub at github.com/demomon/jpc-graal--maven . Application Model \u00b6 While building modular Java applications is excellent, the current tooling support terrible. So this time the application is a single Jar - Graal can create images from classes or jars - where packages do the separation. . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 docker-graal-build.sh \u251c\u2500\u2500 pom.xml \u2514\u2500\u2500 src \u251c\u2500\u2500 main \u2514\u2500\u2500 java \u2514\u2500\u2500 com \u2514\u2500\u2500 github \u2514\u2500\u2500 joostvdg \u2514\u2500\u2500 demo \u251c\u2500\u2500 App.java \u2514\u2500\u2500 Hello.java Build \u00b6 Graal can build a native image based on a Jar file. This allows us to use any standard Java build tool such as Maven or Gradle to build the jar. The actual Graal build will be done in a Dockerfile. The people over at Oracle have created an official Docker image reducing effort spend on our side. The Dockerfile has three segments: build the jar with Maven build the native image with Graal assembly the runtime Docker image based on Alpine As you can see below, the Graal image is only half the size of the JLink image! Let's see how that stacks up to other languages such as Go and Python. Dockerfile \u00b6 ####################################### ## 1. BUILD JAR WITH MAVEN FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src RUN mvn clean package -e ####################################### ## 2. BUILD NATIVE IMAGE WITH GRAAL FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from = BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath ####################################### ## 3. BUILD DOCKER RUNTIME IMAGE FROM alpine:3.8 CMD [ \"jpc-graal\" ] COPY --from = NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal ####################################### Image disk size \u00b6 REPOSITORY TAG IMAGE ID CREATED SIZE jpc-graal-maven latest dc33ebb10813 About an hour ago 19 .6MB Go Example \u00b6 Application Model \u00b6 Build \u00b6 Dockerfile \u00b6 Image Disk size \u00b6 REPOSITORY TAG IMAGE ID CREATED SIZE jpc-go latest bb4a8e546601 6 minutes ago 12 .3MB Python Example \u00b6 Application Model \u00b6 Build \u00b6 Dockerfile \u00b6 Image Disk size \u00b6 Container footprint \u00b6 kubectl top pods mypod-s4wpb-7dz4q --containers POD NAME CPU ( cores ) MEMORY ( bytes ) mypod-7lxnk-gw1sj jpc-go 0m 0Mi mypod-7lxnk-gw1sj java-jlink 0m 0Mi mypod-7lxnk-gw1sj java-graal 0m 0Mi mypod-7lxnk-gw1sj jnlp 150m 96Mi So, the 0Mi memory seems wrong. So I decided to dive into the Google Cloud Console, to see if there's any information in there. What I found there, is the data you can see below. The memory is indeed 0Mi, as they're using between 329 and 815 Kilobytes and not hitting the MB threshold (and thus get listed as 0Mi). We do see that graal uses more CPU and slightly less memory than the JLink setup. Both are still significantly larger than the Go CLI tool, but as long as the JNLP container takes ~100MB, I don't think we should worry about 400-500KB. CPU container/cpu/usage_time:gke_container:REDUCE_SUM ( , ps-dev-201405 ) : 0 .24 java-graal: 5e-4 java-jlink: 3e-3 jnlp: 0 .23 jpc-go: 2e-4 Memory java-graal: 729 ,088.00 java-jlink: 815 ,104.00 jnlp: 101 .507M jpc-go: 327 ,680.00 Disk java-graal: 49 ,152.00 java-jlink: 49 ,152.00 jnlp: 94 ,208.00 jpc-go: 49 ,152.00 Pipeline \u00b6 pipeline { agent { kubernetes { label 'mypod' defaultContainer 'jnlp' yaml \"\"\" apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: java-graal image: caladreas/jpc-graal:0.1.0-maven-b1 command: - cat tty: true - name: java-jlink image: caladreas/jpc-jlink:0.1.0-b1 command: - cat tty: true - name: jpc-go image: caladreas/jpc-go:0.1.0-b1 command: - cat tty: true \"\"\" } } stages { stage ( 'Test Versions' ) { steps { container ( 'java-graal' ) { echo \"java-graal\" sh '/usr/local/bin/jpc-graal' sleep 5 } container ( 'java-jlink' ) { echo \"java-jlink\" sh '/usr/bin/jpb/bin/jpb GitChangeListToFolder abc abc' sleep 5 } container ( 'jpc-go' ) { sh 'jpc-go sayHello -n joost' sleep 5 } sleep 60 } } } }","title":"Jenkins Pipeline Support Tools"},{"location":"blogs/jenkins-pipeline-support-tool/#jenkins-pipeline-support-tools","text":"With Jenkins now becoming Cloud Native and a first class citizen of Kubernetes, it is time to review how we use build tools. This content assumes you're using Jenkins in a Kubernetes cluster, but most of it should also work in other Docker-based environments.","title":"Jenkins Pipeline Support Tools"},{"location":"blogs/jenkins-pipeline-support-tool/#ideal-pipeline","text":"Anyway, one thing we often see people do wrong with Jenkins pipelines is to use the Groovy Scripts as a general-purpose programming language. This creates many problems, bloated & complicated pipelines, much more stress on the master instead of on the build agent and generally making things unreliable. A much better way is to use Jenkins pipelines only as orchestration and lean heavily on your build tools - e.g., Maven, Gradle, Yarn, Bazel - and shell scripts. Alas, if you created complicated pipelines in Groovy scripts, it is likely you'll end up the same with Bash scripts. An even better solution would be to create custom CLI applications that take care of large operations and convoluted logic. You can test and reuse them. pipeline { agent any stages { stage ( 'Build' ) { steps { sh './build.sh' } } stage ( 'Test' ) { steps { sh './test.sh' } } stage ( 'Deploy' ) { steps { sh './deploy.sh' } } } post { success { sh './successNotification.sh' } failure { sh './failureNotification.sh' } } } Now, this might look a bit like a pipe dream, but it illustrates how you should use Jenkins Pipeline. The groovy script engine allows for a lot of freedom, but only rarely is its use justified. To create robust, modular and generic pipelines, it is recommended to use build tools, shell scripts, Shared Libraries and custom CLI's . It was always a bit difficult to manage generic scripts and tools across instances of Jenkins, pipelines, and teams. But with Pod Templates we have an excellent mechanism for using, versioning and distributing them with ease.","title":"Ideal Pipeline"},{"location":"blogs/jenkins-pipeline-support-tool/#kubernetes-pods","text":"When Jenkins runs in Kubernetes, it can leverage it via the Kubernetes Plugin . I realize Jenkins conjures up mixed emotions when it comes to plugins, but this setup might replace most of them. How so? By using a Kubernetes Pod as the agent where instead of putting all your tools into a single VM you can use multiple small scoped containers. You can specify Pod Templates in multiple ways, where my personal favorite is to define it as yaml inside a declarative pipeline - see example below. For each tool you need, you specify the container and its configuration - if required. By default, you will always get a container with a Jenkins JNLP client and the workspace mounted as a volume in the pod. This allows you to create several tiny containers, each containing only the tools you need for a specific job. Now, it could happen you use two or more tools together a lot - let's say npm and maven - so it is ok to sometimes deviate from this to lower the overall memory of the pod. When you need custom logic, you will have to create a script or tool. This is where PodTemplate, Docker images and our desire for small narrow focus tools come together.","title":"Kubernetes Pods"},{"location":"blogs/jenkins-pipeline-support-tool/#podtemplate-example","text":"pipeline { agent { kubernetes { label 'mypod' defaultContainer 'jnlp' yaml \"\"\" apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:alpine command: - cat tty: true - name: busybox image: busybox command: - cat tty: true \"\"\" } } stages { stage ( 'Run maven' ) { steps { container ( 'maven' ) { sh 'mvn -version' } container ( 'busybox' ) { sh '/bin/busybox' } } } } }","title":"PodTemplate example"},{"location":"blogs/jenkins-pipeline-support-tool/#java-example","text":"I bet most people do not think about Java when it comes lightweight CLI applications, but I think that is a shame. Java has excellent tooling to help you build well-tested applications which can be understood and maintained by a vast majority of developers. To make the images small, we will use some of the new tools available in Java land. We will first dive into using Java Modularity and JLink to create a compact and strict binary package, and then we move onto Graal for creating a Native image.","title":"Java Example"},{"location":"blogs/jenkins-pipeline-support-tool/#custom-jdk-image","text":"All the source code of this example application is at github.com/joostvdg/jpb . It is a small CLI which does only one thing; it parses a git commit log to see which folders changed. Quite a useful tool for Monorepo's or other repositories containing more than one changeable resource. Such a CLI should have specific characteristics: * testable * small memory footprint * small disk footprint * quick start * easy to setup * easy to maintain These points sound like an excellent use case for Java Modules and JLink. For those who don't know, read up on Java Modules here and read up on JLink here . JLink will create a binary image that we can use with Alpine Linux to form a minimal Java (Docker) Image. Unfortunately, the plugins for Maven (JMod and JLink) seem to have died. The support on Gradle side is not much better. So I created a solution myself with a multi-stage Docker build. Which does detract a bit from the ease of setup. But overall, it hits the other characteristics spot on.","title":"Custom JDK Image"},{"location":"blogs/jenkins-pipeline-support-tool/#application-model","text":"For ease of maintenance and testing, we separate the parts of the CLI into Java Modules, as you can see in the model below. For using JLink, we need to be a module ourselves. So I figured to expand the exercise to use it to not only create boundaries via packages but also with Modules.","title":"Application Model"},{"location":"blogs/jenkins-pipeline-support-tool/#build","text":"The current LTS version of Java is 11, which means we need at least that if we want to be up-to-date. As we want to run the application in Alpine Linux, we need to build it with Alpine Linux - if you create a custom JDK image its OS specific. To my surprise, the official LTS release is not released for Alpine, so we use OpenJDK 12. Everything is built via a Multi-Stage Docker Build. This Dockerfile can be divided into five segments. creation of the base with a JDK 11+ on Alpine Linux compiling our Java Modules in into Module Jars test our code create our custom JDK image with just our code and whatever we need from the JDK create the runtime Docker image The Dockerfile looks a bit complicated, but we did get a Java runtime that is about 44MB in size and can run as a direct binary with no startup time. The Dockerfile can be much short if we use only a single module, but as our logic grows it is a thoughtful way to separate different concerns. Still, I'm not too happy with this for creating many small CLI's. To much handwork goes into creating the images like this. Relying on unmaintained Maven or Gradle Plugins doesn't seem a better choice. Luckily, there's a new game in town, GraalVM . We'll make an image with Graal next, stay tuned.","title":"Build"},{"location":"blogs/jenkins-pipeline-support-tool/#dockerfile","text":"############################################################### ############################################################### ##### 1. CREATE ALPINE BASE WITH JDK11+ #### OpenJDK image produces weird results with JLink (400mb + sizes) FROM alpine:3.8 AS build ENV JAVA_HOME = /opt/jdk \\ PATH = ${ PATH } :/opt/jdk/bin \\ LANG = C.UTF-8 RUN set -ex && \\ apk add --no-cache bash && \\ wget https://download.java.net/java/early_access/alpine/18/binaries/openjdk-12-ea+18_linux-x64-musl_bin.tar.gz -O jdk.tar.gz && \\ mkdir -p /opt/jdk && \\ tar zxvf jdk.tar.gz -C /opt/jdk --strip-components = 1 && \\ rm jdk.tar.gz && \\ rm /opt/jdk/lib/src.zip #################################### ## 2.a PREPARE COMPILE PHASE RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src ## 2.b COMPILE ALL JAVA FILES RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name \"*.java\" ) ## 2.c CREATE ALL JAVA MODULE JARS RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.jpb.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.core.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.jpb.core . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.cli.jar --module-version 1 .0 -e com.github.joostvdg.jpb.cli.JpbApp \\ -C /usr/src/mods/compiled/joostvdg.jpb.cli . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.core.test.jar --module-version 1 .0 -e com.github.joostvdg.jpb.core.test.ParseChangeListTest \\ -C /usr/src/mods/compiled/joostvdg.jpb.core.test . #################################### ## 3 RUN TESTS RUN rm -rf /usr/bin/jpb-test-image RUN jlink \\ --verbose \\ --compress 2 \\ --no-header-files \\ --no-man-pages \\ --strip-debug \\ --limit-modules java.base \\ --launcher jpb-test = joostvdg.jpb.core.test \\ --module-path /usr/src/mods/jars/: $JAVA_HOME /jmods \\ --add-modules joostvdg.jpb.core.test \\ --add-modules joostvdg.jpb.core \\ --add-modules joostvdg.jpb.api \\ --output /usr/bin/jpb-test-image RUN /usr/bin/jpb-test-image/bin/java --list-modules RUN /usr/bin/jpb-test-image/bin/jpb-test #################################### ## 4 BUILD RUNTIME - CUSTOM JDK IMAGE RUN rm -rf /usr/bin/jpb-image RUN jlink \\ --verbose \\ --compress 2 \\ --no-header-files \\ --no-man-pages \\ --strip-debug \\ --limit-modules java.base \\ --launcher jpb = joostvdg.jpb.cli \\ --module-path /usr/src/mods/jars/: $JAVA_HOME /jmods \\ --add-modules joostvdg.jpb.cli \\ --add-modules joostvdg.jpb.api \\ --add-modules joostvdg.jpb.core \\ --output /usr/bin/jpb-image RUN /usr/bin/jpb-image/bin/java --list-modules #################################### ##### 5. RUNTIME IMAGE - ALPINE FROM panga/alpine:3.8-glibc2.27 LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" LABEL version = \"0.1.0\" LABEL description = \"Docker image for running Jenkins Pipeline Binary\" ENV DATE_CHANGED = \"20181014-2035\" ENV JAVA_OPTS = \"-XX:+UseCGroupMemoryLimitForHeap -XX:+UnlockExperimentalVMOptions\" COPY --from = build /usr/bin/jpb-image/ /usr/bin/jpb ENTRYPOINT [ \"/usr/bin/jpb/bin/jpb\" ]","title":"Dockerfile"},{"location":"blogs/jenkins-pipeline-support-tool/#image-disk-size","text":"REPOSITORY TAG IMAGE ID CREATED SIZE jpb latest af7dda45732a About a minute ago 43 .8MB","title":"Image disk size"},{"location":"blogs/jenkins-pipeline-support-tool/#graal","text":"GraalVM is a universal virtual machine for running applications written in JavaScript, Python, Ruby, R, JVM-based languages like Java, Scala, Kotlin, Clojure, and LLVM-based languages such as C and C++. - graalvm.org Ok, that doesn't tell you why using GraalVM is excellent for creating small CLI docker images. Maybe this quote helps: Native images compiled with GraalVM ahead-of-time improve the startup time and reduce the memory footprint of JVM-based applications. Where JLink allows you to create a custom JDK image and embed your application as a runtime binary, Graal goes one step further. It replaces the VM altogether and uses Substrate VM to run your binary. It can't do a lot of the fantastic things the JVM can do and isn't suited for long running applications or those with a large memory footprint and so on. Well, our CLI applications are single shot executions with low memory footprint, the perfect fit for Graal/Substrate! All the code from this example can is on GitHub at github.com/demomon/jpc-graal--maven .","title":"Graal"},{"location":"blogs/jenkins-pipeline-support-tool/#application-model_1","text":"While building modular Java applications is excellent, the current tooling support terrible. So this time the application is a single Jar - Graal can create images from classes or jars - where packages do the separation. . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 docker-graal-build.sh \u251c\u2500\u2500 pom.xml \u2514\u2500\u2500 src \u251c\u2500\u2500 main \u2514\u2500\u2500 java \u2514\u2500\u2500 com \u2514\u2500\u2500 github \u2514\u2500\u2500 joostvdg \u2514\u2500\u2500 demo \u251c\u2500\u2500 App.java \u2514\u2500\u2500 Hello.java","title":"Application Model"},{"location":"blogs/jenkins-pipeline-support-tool/#build_1","text":"Graal can build a native image based on a Jar file. This allows us to use any standard Java build tool such as Maven or Gradle to build the jar. The actual Graal build will be done in a Dockerfile. The people over at Oracle have created an official Docker image reducing effort spend on our side. The Dockerfile has three segments: build the jar with Maven build the native image with Graal assembly the runtime Docker image based on Alpine As you can see below, the Graal image is only half the size of the JLink image! Let's see how that stacks up to other languages such as Go and Python.","title":"Build"},{"location":"blogs/jenkins-pipeline-support-tool/#dockerfile_1","text":"####################################### ## 1. BUILD JAR WITH MAVEN FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src RUN mvn clean package -e ####################################### ## 2. BUILD NATIVE IMAGE WITH GRAAL FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from = BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath ####################################### ## 3. BUILD DOCKER RUNTIME IMAGE FROM alpine:3.8 CMD [ \"jpc-graal\" ] COPY --from = NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal #######################################","title":"Dockerfile"},{"location":"blogs/jenkins-pipeline-support-tool/#image-disk-size_1","text":"REPOSITORY TAG IMAGE ID CREATED SIZE jpc-graal-maven latest dc33ebb10813 About an hour ago 19 .6MB","title":"Image disk size"},{"location":"blogs/jenkins-pipeline-support-tool/#go-example","text":"","title":"Go Example"},{"location":"blogs/jenkins-pipeline-support-tool/#application-model_2","text":"","title":"Application Model"},{"location":"blogs/jenkins-pipeline-support-tool/#build_2","text":"","title":"Build"},{"location":"blogs/jenkins-pipeline-support-tool/#dockerfile_2","text":"","title":"Dockerfile"},{"location":"blogs/jenkins-pipeline-support-tool/#image-disk-size_2","text":"REPOSITORY TAG IMAGE ID CREATED SIZE jpc-go latest bb4a8e546601 6 minutes ago 12 .3MB","title":"Image Disk size"},{"location":"blogs/jenkins-pipeline-support-tool/#python-example","text":"","title":"Python Example"},{"location":"blogs/jenkins-pipeline-support-tool/#application-model_3","text":"","title":"Application Model"},{"location":"blogs/jenkins-pipeline-support-tool/#build_3","text":"","title":"Build"},{"location":"blogs/jenkins-pipeline-support-tool/#dockerfile_3","text":"","title":"Dockerfile"},{"location":"blogs/jenkins-pipeline-support-tool/#image-disk-size_3","text":"","title":"Image Disk size"},{"location":"blogs/jenkins-pipeline-support-tool/#container-footprint","text":"kubectl top pods mypod-s4wpb-7dz4q --containers POD NAME CPU ( cores ) MEMORY ( bytes ) mypod-7lxnk-gw1sj jpc-go 0m 0Mi mypod-7lxnk-gw1sj java-jlink 0m 0Mi mypod-7lxnk-gw1sj java-graal 0m 0Mi mypod-7lxnk-gw1sj jnlp 150m 96Mi So, the 0Mi memory seems wrong. So I decided to dive into the Google Cloud Console, to see if there's any information in there. What I found there, is the data you can see below. The memory is indeed 0Mi, as they're using between 329 and 815 Kilobytes and not hitting the MB threshold (and thus get listed as 0Mi). We do see that graal uses more CPU and slightly less memory than the JLink setup. Both are still significantly larger than the Go CLI tool, but as long as the JNLP container takes ~100MB, I don't think we should worry about 400-500KB. CPU container/cpu/usage_time:gke_container:REDUCE_SUM ( , ps-dev-201405 ) : 0 .24 java-graal: 5e-4 java-jlink: 3e-3 jnlp: 0 .23 jpc-go: 2e-4 Memory java-graal: 729 ,088.00 java-jlink: 815 ,104.00 jnlp: 101 .507M jpc-go: 327 ,680.00 Disk java-graal: 49 ,152.00 java-jlink: 49 ,152.00 jnlp: 94 ,208.00 jpc-go: 49 ,152.00","title":"Container footprint"},{"location":"blogs/jenkins-pipeline-support-tool/#pipeline","text":"pipeline { agent { kubernetes { label 'mypod' defaultContainer 'jnlp' yaml \"\"\" apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: java-graal image: caladreas/jpc-graal:0.1.0-maven-b1 command: - cat tty: true - name: java-jlink image: caladreas/jpc-jlink:0.1.0-b1 command: - cat tty: true - name: jpc-go image: caladreas/jpc-go:0.1.0-b1 command: - cat tty: true \"\"\" } } stages { stage ( 'Test Versions' ) { steps { container ( 'java-graal' ) { echo \"java-graal\" sh '/usr/local/bin/jpc-graal' sleep 5 } container ( 'java-jlink' ) { echo \"java-jlink\" sh '/usr/bin/jpb/bin/jpb GitChangeListToFolder abc abc' sleep 5 } container ( 'jpc-go' ) { sh 'jpc-go sayHello -n joost' sleep 5 } sleep 60 } } } }","title":"Pipeline"},{"location":"blogs/jenkins-x/","text":"Jenkins X \u00b6 Choose your distribution \u00b6 GKE via JX binary \u00b6 export JX_TOOL_PSW = ZfwYM0odeI5W41GGzXgGqFmP export MACHINE_TYPE = n1-standard-2 export GKE_ZONE = europe-west4-a export K8S_VERSION = 1 .11.2-gke.18 export GKE_NAME = joostvdg-jx-nov18-1 export GIT_API_TOKEN = export PROJECT_ID = jx create cluster gke \\ --cluster-name = ${ GKE_NAME } \\ --default-admin-password = ${ JX_TOOL_PSW } \\ --domain = 'kearos.net' \\ --git-api-token = ${ GIT_API_TOKEN } \\ --git-username = 'joostvdg' \\ --no-tiller \\ --project-id = ${ PROJECT_ID } \\ --prow = true \\ --vault = true \\ --zone = ${ GKE_ZONE } \\ --machine-type = ${ MACHINE_TYPE } \\ --labels = 'owner=jvandergriendt,purpose=practice,team=ps' \\ --skip-login = true \\ --max-num-nodes = '3' \\ --min-num-nodes = '2' \\ --kubernetes-version = ${ K8S_VERSION } \\ --batch-mode = true --skip-installation = false: Provision cluster only, don ' t install Jenkins X into it --skip-login = false: Skip Google auth if already logged in via gcloud auth","title":"Jenkins X"},{"location":"blogs/jenkins-x/#jenkins-x","text":"","title":"Jenkins X"},{"location":"blogs/jenkins-x/#choose-your-distribution","text":"","title":"Choose your distribution"},{"location":"blogs/jenkins-x/#gke-via-jx-binary","text":"export JX_TOOL_PSW = ZfwYM0odeI5W41GGzXgGqFmP export MACHINE_TYPE = n1-standard-2 export GKE_ZONE = europe-west4-a export K8S_VERSION = 1 .11.2-gke.18 export GKE_NAME = joostvdg-jx-nov18-1 export GIT_API_TOKEN = export PROJECT_ID = jx create cluster gke \\ --cluster-name = ${ GKE_NAME } \\ --default-admin-password = ${ JX_TOOL_PSW } \\ --domain = 'kearos.net' \\ --git-api-token = ${ GIT_API_TOKEN } \\ --git-username = 'joostvdg' \\ --no-tiller \\ --project-id = ${ PROJECT_ID } \\ --prow = true \\ --vault = true \\ --zone = ${ GKE_ZONE } \\ --machine-type = ${ MACHINE_TYPE } \\ --labels = 'owner=jvandergriendt,purpose=practice,team=ps' \\ --skip-login = true \\ --max-num-nodes = '3' \\ --min-num-nodes = '2' \\ --kubernetes-version = ${ K8S_VERSION } \\ --batch-mode = true --skip-installation = false: Provision cluster only, don ' t install Jenkins X into it --skip-login = false: Skip Google auth if already logged in via gcloud auth","title":"GKE via JX binary"},{"location":"blogs/k8s-controller/","text":"Create your own custom Kubernetes controller \u00b6 Before we dive into the why and how of creating a Kubernetes Controller, let's take a brief look at what it does. What is a Controller \u00b6 I will only briefly touch on what a controller is. If you already know what it is you can safely skip this paragraph. In applications of robotics and automation, a control loop is a non-terminating loop that regulates the state of the system. In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the API server and makes changes attempting to move the current state towards the desired state. Examples of controllers that ship with Kubernetes today are the replication controller, endpoints controller, namespace controller, and serviceaccounts controller. So the purpose of controllers is to control - hence the name - the state of the system - our Kubernetes cluster. A controller is generally created to watch a single resource type and make sure that its desired state is met. As there is already very well written material on the details of controllers, I'll leave it at this. For more information on controllers and how they work, I recommend reading bitnami's deepdive and the kubernetes documentation . When to create your controller \u00b6 Great, you're still reading this. So when would you put in the effort to create your controller? I'm pretty sure there will be more cases, but the following two are the main ones. Process events on Core resources, Core being the resources any Kubernetes ships with Process events on Customer resources Examples of customer controllers for the first use case are tools such as Kubediff, which will compare resources in the cluster with their definition in a Git repository. For the second use case - custom controller for custom resource - there are many more examples. As most custom resources will have their controller to act on the events of the resources because existing controllers will not process the custom resource. Additionally, in most cases having resources sitting in a cluster with nothing happening is a bit of a waste. So we write a controller to match the resource. How to create your controller \u00b6 When it comes to making a controller, it will be some Go (lang) code using the Kubernetes client library. This is straightforward if you're creating a controller for the core resources, but quite a few steps if you write a custom controller. Write a core resource controller \u00b6 To ease ourselves into it lets first create a core resource controller. We're aiming for a controller that can read our ConfigMaps resources. To be able to do this, we need the following: Handler : for the events (Created, Deleted, Updated) Controller : retrieves events from an informer, puts work on a queue, and delegates the events to the handler Entrypoint : typically a main.go file, that creates a connection to the Kubernetes API server and ties all of the resources together Dockerfile : to package our binary for running inside the cluster Resource Definition YAML : typical Kubernetes resource definition file, in our case a Deployment, so our controller will run as a pod/container Handler \u00b6 Controller \u00b6 Entrypoint \u00b6 Dockerfile \u00b6 Resource Definition \u00b6 Resources \u00b6 https://medium.com/@trstringer/create-kubernetes-controllers-for-core-and-custom-resources-62fc35ad64a3 https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/ https://coreos.com/blog/introducing-operators.html https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html https://github.com/joostvdg/k8s-core-resource-controller https://github.com/kubernetes/sample-controller/blob/master/controller.go","title":"Create your own custom Kubernetes controller"},{"location":"blogs/k8s-controller/#create-your-own-custom-kubernetes-controller","text":"Before we dive into the why and how of creating a Kubernetes Controller, let's take a brief look at what it does.","title":"Create your own custom Kubernetes controller"},{"location":"blogs/k8s-controller/#what-is-a-controller","text":"I will only briefly touch on what a controller is. If you already know what it is you can safely skip this paragraph. In applications of robotics and automation, a control loop is a non-terminating loop that regulates the state of the system. In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the API server and makes changes attempting to move the current state towards the desired state. Examples of controllers that ship with Kubernetes today are the replication controller, endpoints controller, namespace controller, and serviceaccounts controller. So the purpose of controllers is to control - hence the name - the state of the system - our Kubernetes cluster. A controller is generally created to watch a single resource type and make sure that its desired state is met. As there is already very well written material on the details of controllers, I'll leave it at this. For more information on controllers and how they work, I recommend reading bitnami's deepdive and the kubernetes documentation .","title":"What is a Controller"},{"location":"blogs/k8s-controller/#when-to-create-your-controller","text":"Great, you're still reading this. So when would you put in the effort to create your controller? I'm pretty sure there will be more cases, but the following two are the main ones. Process events on Core resources, Core being the resources any Kubernetes ships with Process events on Customer resources Examples of customer controllers for the first use case are tools such as Kubediff, which will compare resources in the cluster with their definition in a Git repository. For the second use case - custom controller for custom resource - there are many more examples. As most custom resources will have their controller to act on the events of the resources because existing controllers will not process the custom resource. Additionally, in most cases having resources sitting in a cluster with nothing happening is a bit of a waste. So we write a controller to match the resource.","title":"When to create your controller"},{"location":"blogs/k8s-controller/#how-to-create-your-controller","text":"When it comes to making a controller, it will be some Go (lang) code using the Kubernetes client library. This is straightforward if you're creating a controller for the core resources, but quite a few steps if you write a custom controller.","title":"How to create your controller"},{"location":"blogs/k8s-controller/#write-a-core-resource-controller","text":"To ease ourselves into it lets first create a core resource controller. We're aiming for a controller that can read our ConfigMaps resources. To be able to do this, we need the following: Handler : for the events (Created, Deleted, Updated) Controller : retrieves events from an informer, puts work on a queue, and delegates the events to the handler Entrypoint : typically a main.go file, that creates a connection to the Kubernetes API server and ties all of the resources together Dockerfile : to package our binary for running inside the cluster Resource Definition YAML : typical Kubernetes resource definition file, in our case a Deployment, so our controller will run as a pod/container","title":"Write a core resource controller"},{"location":"blogs/k8s-controller/#handler","text":"","title":"Handler"},{"location":"blogs/k8s-controller/#controller","text":"","title":"Controller"},{"location":"blogs/k8s-controller/#entrypoint","text":"","title":"Entrypoint"},{"location":"blogs/k8s-controller/#dockerfile","text":"","title":"Dockerfile"},{"location":"blogs/k8s-controller/#resource-definition","text":"","title":"Resource Definition"},{"location":"blogs/k8s-controller/#resources","text":"https://medium.com/@trstringer/create-kubernetes-controllers-for-core-and-custom-resources-62fc35ad64a3 https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/ https://coreos.com/blog/introducing-operators.html https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html https://github.com/joostvdg/k8s-core-resource-controller https://github.com/kubernetes/sample-controller/blob/master/controller.go","title":"Resources"},{"location":"blogs/k8s-crd/","text":"Kubernetes CRD \u00b6 Kubernetes is a fantastic platform that allows you to run a lot of different workloads in various ways. It has APIs front and center, allowing you to choose different implementation as they suit you. Sometimes you feel something is missing. There is a concept with your application or something you want from the cluster that isn't (however) available in Kubernetes. It is then that you can look for extending Kubernetes itself. Either its API or by creating a new kind of resource: a Custom Resource Definition or CRD. What you need \u00b6 resource definition : the yaml definition of your custom resource custom controller : a controller to interact with your custom resource Resource Definition \u00b6 As with any Kubernetes resource, you need a yaml file that defines it with the lexicon of Kubernetes. In this case, the Kind is CustomerResourceDefinition . apiVersion : apiextensions.k8s.io/v1beta1 kind : CustomResourceDefinition metadata : name : manifests.cat.kearos.net spec : group : cat.kearos.net version : v1 names : kind : ApplicationManifest plural : applicationmanifests singular : applicationmanifest shortNames : - cam scope : Namespaced apiVersion : as the name implies, it's an API extension kind : has to be CustomResourceDefinition else it wouldn't be a CRD name : name must match the spec fields below, and be in the form: . group : API group name so that you can group multiple resources somewhat together names : kind : the resource kind, used for other resource definitions plural is the official name used in the Kubernetes API, also the default for interaction with kubectl singular : alias for the API usage in kubectl and used as the display value shortNames : shortNames allow a shorter string to match your resource on the CLI scope : can either be Namespaced , tied to a specific namespace, or Cluster where it must be cluster-wide unique Install CRD \u00b6 Taking the above example and saving it as application-manifest.yml , we can install the CRD into the cluster as follows. kubectl create -f application-manifest.yml Resource Usage Example \u00b6 apiVersion : cat.kearos.net/v1 kind : ApplicationManifest metadata : name : manifest-cat spec : name : cat description : Central Application Tracker namespace : cat artifactIDs : - github.com/joostvdg/cat sources : - git@github.com:joostvdg/cat.git Looking at this example, you might wonder how this works. There is a specification in there - spec - with all kinds of custom fields. But where do they come from? Nowhere really, so you cannot validate this with the CRD alone. You can put any arbitrary field in there. So what do you do with the CRD then? You can create a custom controller that processes your custom resources. Because creating a custom controller for your custom resources is complicated and takes several steps, we will do this in a separate article.","title":"Create your own k8s resource"},{"location":"blogs/k8s-crd/#kubernetes-crd","text":"Kubernetes is a fantastic platform that allows you to run a lot of different workloads in various ways. It has APIs front and center, allowing you to choose different implementation as they suit you. Sometimes you feel something is missing. There is a concept with your application or something you want from the cluster that isn't (however) available in Kubernetes. It is then that you can look for extending Kubernetes itself. Either its API or by creating a new kind of resource: a Custom Resource Definition or CRD.","title":"Kubernetes CRD"},{"location":"blogs/k8s-crd/#what-you-need","text":"resource definition : the yaml definition of your custom resource custom controller : a controller to interact with your custom resource","title":"What you need"},{"location":"blogs/k8s-crd/#resource-definition","text":"As with any Kubernetes resource, you need a yaml file that defines it with the lexicon of Kubernetes. In this case, the Kind is CustomerResourceDefinition . apiVersion : apiextensions.k8s.io/v1beta1 kind : CustomResourceDefinition metadata : name : manifests.cat.kearos.net spec : group : cat.kearos.net version : v1 names : kind : ApplicationManifest plural : applicationmanifests singular : applicationmanifest shortNames : - cam scope : Namespaced apiVersion : as the name implies, it's an API extension kind : has to be CustomResourceDefinition else it wouldn't be a CRD name : name must match the spec fields below, and be in the form: . group : API group name so that you can group multiple resources somewhat together names : kind : the resource kind, used for other resource definitions plural is the official name used in the Kubernetes API, also the default for interaction with kubectl singular : alias for the API usage in kubectl and used as the display value shortNames : shortNames allow a shorter string to match your resource on the CLI scope : can either be Namespaced , tied to a specific namespace, or Cluster where it must be cluster-wide unique","title":"Resource Definition"},{"location":"blogs/k8s-crd/#install-crd","text":"Taking the above example and saving it as application-manifest.yml , we can install the CRD into the cluster as follows. kubectl create -f application-manifest.yml","title":"Install CRD"},{"location":"blogs/k8s-crd/#resource-usage-example","text":"apiVersion : cat.kearos.net/v1 kind : ApplicationManifest metadata : name : manifest-cat spec : name : cat description : Central Application Tracker namespace : cat artifactIDs : - github.com/joostvdg/cat sources : - git@github.com:joostvdg/cat.git Looking at this example, you might wonder how this works. There is a specification in there - spec - with all kinds of custom fields. But where do they come from? Nowhere really, so you cannot validate this with the CRD alone. You can put any arbitrary field in there. So what do you do with the CRD then? You can create a custom controller that processes your custom resources. Because creating a custom controller for your custom resources is complicated and takes several steps, we will do this in a separate article.","title":"Resource Usage Example"},{"location":"blogs/k8s-lets-encrypt/","text":"Let's Encrypt for Kubernetes \u00b6 Let's Encrypt is a free, automated and open Certificate Authority. The kind of service you need if you want to have a secure website with https - yes I know that requires more than that - and it's now more straightforward to use than ever. This about using Let's Encrypt for generating a certificate for your service on Kubernetes. There are several ways to do this, with more or less automation, cluster-wide or namespace bound or with a DNS or HTTP validation check. I'll choose the route that was the easiest for me, and then I'll briefly look at the other options. Prerequisites \u00b6 There are some prerequisites required, that are best discussed on their own. So we will continue with the assumption that you have these in order. valid Class A or CNAME domain name kubernetes cluster with ingress controller (such as nginx) with helm and tiller installed in the cluster web application Steps \u00b6 The steps to take to get a web application to get a certificate from Let's Encrypt are the following. install cert-manager from the official helm chart deploy a Issuer resource deploy a certificate resource confirm certificate and secret are created/filled use in web app Install Cert Manager \u00b6 For more details on Cert Manager, I recommend reading their introduction . In essence, it's a tool that helps you initiate a certificate request with a service such as Let's Encrypt. You can install it via Helm, and it's meant to be installed only once per cluster. The once per cluster restriction is due to the usage of Custom Resource Definitions (CRD's) which will block any (re-)installation. To confirm if there are any CRD's from cert-manager, you can issue the following command. kubectl get customresourcedefinitions.apiextensions.k8s.io The CRD's belonging to cert-manager are the following: certificates.certmanager.k8s.io clusterissuers.certmanager.k8s.io issuers.certmanager.k8s.io You will notice later, that we will use these CRD's for getting our certificate, so keep them in mind. To remove them in case of a re-install, you can issue the command below. kubectl delete customresourcedefinitions.apiextensions.k8s.io \\ certificates.certmanager.k8s.io \\ clusterissuers.certmanager.k8s.io \\ issuers.certmanager.k8s.io When you're sure there are no CRD's left, you can install cert-manager via it's helm chart. It has some options in case you need them, you read about them here , but in my case that wasn't needed. helm install --name cert-manager --namespace default stable/cert-manager Deploy Issuer \u00b6 To be able to use a certificate we need to have a Certificate Issuer. If you remember from our cert-manager , there are two CRD's that can take this role: ClusterIssuer : clusterissuers.certmanager.k8s.io Issuer : issuers.certmanager.k8s.io Both issuer type can use two ways of providing the proof of ownership, either by dns-01 or http-01 . We'll be using the http-01 method, for the dns-01 method, refer to the cert-manager documenation . ClusterIssuer \u00b6 As the resource Kind implies, a ClusterIssuer is a cluster-wide resource and not bound to a specific namespace. apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : # The ACME server URL server : https://acme-staging.api.letsencrypt.org/directory # Email address used for ACME registration email : user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} Issuer \u00b6 Not everyone wants a cluster-wide resource, and not everyone has the rights to install something elsewhere than their namespace. I prefer having as much as possible tied to a namespace - either a team or an application - I will use this type. apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : myapp-letsencrypt-staging namespace : myapp spec : acme : # The ACME server URL server : https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email : myadmin@myapp.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : myapp-letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} There's a few things to note here: server : this refers to the server executing the ACME test, in this case: Let's Encrypt Staging (with the v2 API) email : this will be the account it will use for registering the certificate privateKeySecretRef : this is the Kubernetes secret resource in which the privateKey will be stored, just in case you need or want to remove it Deploy Certificate Resource \u00b6 Next up is our Certificate resource, this is where cert-manager will store our certificate details to be used by our application. In case you forgot, this is one of the three CRD's provided by cert-manager. apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : myapp-example-com namespace : myapp spec : secretName : myapp-tls dnsNames : - myapp.example.com acme : config : - http01 : ingressClass : nginx domains : - myapp.example.com issuerRef : name : myapp-letsencrypt-staging kind : Issuer The things to note here: name : so far I've found it a naming convention to write the domain name where - replaces the . 's. secretName : the name of the Kubernetes secret that will house the certificate and certificate key dnsNames : you can specify more than one name, in our case just a single one, should match acme.config.domains acme.config : this defines the configuration for how the ownership proof should be done, this should match the method defined in the Issuer issuerRef : in good Kubernetes fashion, we reference the Issuer that should issue our certificate, the name and kind should match our Issue resource Confirm Resources \u00b6 We have defined our set of resources that should create our valid - though not trusted, as it is staging - certificate. Before we use it, we should confirm our secret and our certificate are both valid. kubectl describe certificate myapp-example-com --namespace myapp The response includes the latest status, which looks like this: Status : Acme : Order : URL : https://acme-staging-v02.api.letsencrypt.org/acme/order/705.../960... Conditions : Last Transition Time : 2018-10-02T21:17:34Z Message : Certificate issued successfully Reason : CertIssued Status : True Type : Ready Next up is our secret, containing the actual certificate and the certificate key. kubectl describe secret myapp-tls --namespace myapp Which results in something like this: Name: myapp-tls Namespace: myapp Labels: certmanager.k8s.io/certificate-name = myapp-example-com Annotations: certmanager.k8s.io/alt-names: myapp.example.com certmanager.k8s.io/common-name: myapp.example.com certmanager.k8s.io/issuer-kind: Issuer certmanager.k8s.io/issuer-name: myapp-letsencrypt-staging Type: kubernetes.io/tls Data ==== tls.crt: 3797 bytes tls.key: 1679 bytes Use certificate to enable https \u00b6 Assuming the secret and the certificate are correct, we can use them to enable https on our web app. We put the tls certificate on the ingress of the application, in this case, Nginx, which assumes the following about the app: it has a deployment or stateful set it has a service which provides and endpoint to one or more instances it has an nginx ingress which points to the service Deployment \u00b6 kind : Deployment apiVersion : apps/v1 metadata : name : myapp namespace : myapp labels : app : myapp spec : replicas : 3 selector : matchLabels : app : myapp template : metadata : labels : app : myapp spec : containers : - name : myapp image : caladreas/catnip-master imagePullPolicy : Always ports : - containerPort : 8087 You should always include livenessprobe, resource limits and so on. But for the sake of brevity, these are omitted! Service \u00b6 apiVersion : v1 kind : Service metadata : name : myapp namespace : myapp labels : app : myapp spec : selector : app : myapp ports : - name : http port : 80 targetPort : 8087 protocol : TCP Ingress for Issuer \u00b6 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : \"nginx\" ingress.kubernetes.io/ssl-redirect : \"true\" certmanager.k8s.io/issuer : myapp-letsencrypt-staging certmanager.k8s.io/acme-challenge-type : http01 spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls Ingress for ClusterIssuer \u00b6 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : \"nginx\" ingress.kubernetes.io/ssl-redirect : \"true\" certmanager.k8s.io/cluster-issuer : letsencrypt-staging certmanager.k8s.io/acme-challenge-type : http01 spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls Further resources \u00b6 How Does Let's Encrypt Work Tutorial that inspired this page Amazone EKS Ingress Guide Kuberetes EKS Ingress and TLS How To Configure LTS for Nginx Ingress","title":"Let's Encrypt for Kubernetes Apps"},{"location":"blogs/k8s-lets-encrypt/#lets-encrypt-for-kubernetes","text":"Let's Encrypt is a free, automated and open Certificate Authority. The kind of service you need if you want to have a secure website with https - yes I know that requires more than that - and it's now more straightforward to use than ever. This about using Let's Encrypt for generating a certificate for your service on Kubernetes. There are several ways to do this, with more or less automation, cluster-wide or namespace bound or with a DNS or HTTP validation check. I'll choose the route that was the easiest for me, and then I'll briefly look at the other options.","title":"Let's Encrypt for Kubernetes"},{"location":"blogs/k8s-lets-encrypt/#prerequisites","text":"There are some prerequisites required, that are best discussed on their own. So we will continue with the assumption that you have these in order. valid Class A or CNAME domain name kubernetes cluster with ingress controller (such as nginx) with helm and tiller installed in the cluster web application","title":"Prerequisites"},{"location":"blogs/k8s-lets-encrypt/#steps","text":"The steps to take to get a web application to get a certificate from Let's Encrypt are the following. install cert-manager from the official helm chart deploy a Issuer resource deploy a certificate resource confirm certificate and secret are created/filled use in web app","title":"Steps"},{"location":"blogs/k8s-lets-encrypt/#install-cert-manager","text":"For more details on Cert Manager, I recommend reading their introduction . In essence, it's a tool that helps you initiate a certificate request with a service such as Let's Encrypt. You can install it via Helm, and it's meant to be installed only once per cluster. The once per cluster restriction is due to the usage of Custom Resource Definitions (CRD's) which will block any (re-)installation. To confirm if there are any CRD's from cert-manager, you can issue the following command. kubectl get customresourcedefinitions.apiextensions.k8s.io The CRD's belonging to cert-manager are the following: certificates.certmanager.k8s.io clusterissuers.certmanager.k8s.io issuers.certmanager.k8s.io You will notice later, that we will use these CRD's for getting our certificate, so keep them in mind. To remove them in case of a re-install, you can issue the command below. kubectl delete customresourcedefinitions.apiextensions.k8s.io \\ certificates.certmanager.k8s.io \\ clusterissuers.certmanager.k8s.io \\ issuers.certmanager.k8s.io When you're sure there are no CRD's left, you can install cert-manager via it's helm chart. It has some options in case you need them, you read about them here , but in my case that wasn't needed. helm install --name cert-manager --namespace default stable/cert-manager","title":"Install Cert Manager"},{"location":"blogs/k8s-lets-encrypt/#deploy-issuer","text":"To be able to use a certificate we need to have a Certificate Issuer. If you remember from our cert-manager , there are two CRD's that can take this role: ClusterIssuer : clusterissuers.certmanager.k8s.io Issuer : issuers.certmanager.k8s.io Both issuer type can use two ways of providing the proof of ownership, either by dns-01 or http-01 . We'll be using the http-01 method, for the dns-01 method, refer to the cert-manager documenation .","title":"Deploy Issuer"},{"location":"blogs/k8s-lets-encrypt/#clusterissuer","text":"As the resource Kind implies, a ClusterIssuer is a cluster-wide resource and not bound to a specific namespace. apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : # The ACME server URL server : https://acme-staging.api.letsencrypt.org/directory # Email address used for ACME registration email : user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {}","title":"ClusterIssuer"},{"location":"blogs/k8s-lets-encrypt/#issuer","text":"Not everyone wants a cluster-wide resource, and not everyone has the rights to install something elsewhere than their namespace. I prefer having as much as possible tied to a namespace - either a team or an application - I will use this type. apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : myapp-letsencrypt-staging namespace : myapp spec : acme : # The ACME server URL server : https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email : myadmin@myapp.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : myapp-letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} There's a few things to note here: server : this refers to the server executing the ACME test, in this case: Let's Encrypt Staging (with the v2 API) email : this will be the account it will use for registering the certificate privateKeySecretRef : this is the Kubernetes secret resource in which the privateKey will be stored, just in case you need or want to remove it","title":"Issuer"},{"location":"blogs/k8s-lets-encrypt/#deploy-certificate-resource","text":"Next up is our Certificate resource, this is where cert-manager will store our certificate details to be used by our application. In case you forgot, this is one of the three CRD's provided by cert-manager. apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : myapp-example-com namespace : myapp spec : secretName : myapp-tls dnsNames : - myapp.example.com acme : config : - http01 : ingressClass : nginx domains : - myapp.example.com issuerRef : name : myapp-letsencrypt-staging kind : Issuer The things to note here: name : so far I've found it a naming convention to write the domain name where - replaces the . 's. secretName : the name of the Kubernetes secret that will house the certificate and certificate key dnsNames : you can specify more than one name, in our case just a single one, should match acme.config.domains acme.config : this defines the configuration for how the ownership proof should be done, this should match the method defined in the Issuer issuerRef : in good Kubernetes fashion, we reference the Issuer that should issue our certificate, the name and kind should match our Issue resource","title":"Deploy Certificate Resource"},{"location":"blogs/k8s-lets-encrypt/#confirm-resources","text":"We have defined our set of resources that should create our valid - though not trusted, as it is staging - certificate. Before we use it, we should confirm our secret and our certificate are both valid. kubectl describe certificate myapp-example-com --namespace myapp The response includes the latest status, which looks like this: Status : Acme : Order : URL : https://acme-staging-v02.api.letsencrypt.org/acme/order/705.../960... Conditions : Last Transition Time : 2018-10-02T21:17:34Z Message : Certificate issued successfully Reason : CertIssued Status : True Type : Ready Next up is our secret, containing the actual certificate and the certificate key. kubectl describe secret myapp-tls --namespace myapp Which results in something like this: Name: myapp-tls Namespace: myapp Labels: certmanager.k8s.io/certificate-name = myapp-example-com Annotations: certmanager.k8s.io/alt-names: myapp.example.com certmanager.k8s.io/common-name: myapp.example.com certmanager.k8s.io/issuer-kind: Issuer certmanager.k8s.io/issuer-name: myapp-letsencrypt-staging Type: kubernetes.io/tls Data ==== tls.crt: 3797 bytes tls.key: 1679 bytes","title":"Confirm Resources"},{"location":"blogs/k8s-lets-encrypt/#use-certificate-to-enable-https","text":"Assuming the secret and the certificate are correct, we can use them to enable https on our web app. We put the tls certificate on the ingress of the application, in this case, Nginx, which assumes the following about the app: it has a deployment or stateful set it has a service which provides and endpoint to one or more instances it has an nginx ingress which points to the service","title":"Use certificate to enable https"},{"location":"blogs/k8s-lets-encrypt/#deployment","text":"kind : Deployment apiVersion : apps/v1 metadata : name : myapp namespace : myapp labels : app : myapp spec : replicas : 3 selector : matchLabels : app : myapp template : metadata : labels : app : myapp spec : containers : - name : myapp image : caladreas/catnip-master imagePullPolicy : Always ports : - containerPort : 8087 You should always include livenessprobe, resource limits and so on. But for the sake of brevity, these are omitted!","title":"Deployment"},{"location":"blogs/k8s-lets-encrypt/#service","text":"apiVersion : v1 kind : Service metadata : name : myapp namespace : myapp labels : app : myapp spec : selector : app : myapp ports : - name : http port : 80 targetPort : 8087 protocol : TCP","title":"Service"},{"location":"blogs/k8s-lets-encrypt/#ingress-for-issuer","text":"apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : \"nginx\" ingress.kubernetes.io/ssl-redirect : \"true\" certmanager.k8s.io/issuer : myapp-letsencrypt-staging certmanager.k8s.io/acme-challenge-type : http01 spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls","title":"Ingress for Issuer"},{"location":"blogs/k8s-lets-encrypt/#ingress-for-clusterissuer","text":"apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : \"nginx\" ingress.kubernetes.io/ssl-redirect : \"true\" certmanager.k8s.io/cluster-issuer : letsencrypt-staging certmanager.k8s.io/acme-challenge-type : http01 spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls","title":"Ingress for ClusterIssuer"},{"location":"blogs/k8s-lets-encrypt/#further-resources","text":"How Does Let's Encrypt Work Tutorial that inspired this page Amazone EKS Ingress Guide Kuberetes EKS Ingress and TLS How To Configure LTS for Nginx Ingress","title":"Further resources"},{"location":"blogs/kubernetes-post-install/","text":"Kubernetes Post Install \u00b6 What to do after you've installed your Kubernetes cluster, whether that was EKS via eksctl or GKE via gcloud. make network more secure with encryption weavenet for example install package manager install helm & tiller use nginx for ssl termination together with Let's Encrypt install nginx install cert-manager Helm & Tiller \u00b6 Helm is the defacto standard package manager for Kubernetes. Its current iteration is version 2, which has a client component - Helm - and a serverside component, Tiller. There's a problem with that, due this setup with Helm and Tiller, Tiller is aking to a cluster admin. This isn't very secure and there are several ways around that. JenkinsX : its binary ( jx ) can install helm charts without using Tiller. It generates the kubernetes resource files and installs these directly custom RBAC setup : you can also setup RBAC in such a way that every separate namespace gets its own Tiller, limiting the reach of any Tiller Tiller Custom RBAC Example \u00b6 Namespaces \u00b6 kind : Namespace apiVersion : v1 metadata : name : sre --- kind : Namespace apiVersion : v1 metadata : name : dev1 --- kind : Namespace apiVersion : v1 metadata : name : dev2 Service Accounts \u00b6 kind : ServiceAccount apiVersion : v1 metadata : name : tiller namespace : dev1 --- kind : ServiceAccount apiVersion : v1 metadata : name : tiller namespace : dev2 --- kind : ServiceAccount apiVersion : v1 metadata : name : helm namespace : sre Roles \u00b6 kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-manager namespace : dev1 rules : - apiGroups : [ \"\" , \"batch\" , \"extensions\" , \"apps\" ] resources : [ \"*\" ] verbs : [ \"*\" ] --- kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-manager namespace : dev2 rules : - apiGroups : [ \"\" , \"batch\" , \"extensions\" , \"apps\" ] resources : [ \"*\" ] verbs : [ \"*\" ] --- kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1 metadata : name : helm-clusterrole rules : - apiGroups : [ \"\" ] resources : [ \"pods/portforward\" ] verbs : [ \"create\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"list\" , \"get\" ] RoleBindings \u00b6 kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-binding namespace : dev1 subjects : - kind : ServiceAccount name : tiller namespace : dev1 roleRef : kind : Role name : tiller-manager apiGroup : rbac.authorization.k8s.io --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-binding namespace : dev2 subjects : - kind : ServiceAccount name : tiller namespace : dev2 roleRef : kind : Role name : tiller-manager apiGroup : rbac.authorization.k8s.io --- kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : helm-clusterrolebinding roleRef : kind : ClusterRole apiGroup : rbac.authorization.k8s.io name : helm-clusterrole subjects : - kind : ServiceAccount name : helm namespace : sre Install Tiller \u00b6 helm init --service-account tiller --tiller-namespace dev1 helm init --service-account tiller --tiller-namespace dev2 Create KubeConfig for Helm client \u00b6 # Find the secret associated with the Service Account SECRET = $( kubectl -n sre get sa helm -o jsonpath = '{.secrets[].name}' ) # Retrieve the token from the secret and decode it TOKEN = $( kubectl get secrets -n sre $SECRET -o jsonpath = '{.data.token}' | base64 -D ) # Retrieve the CA from the secret, decode it and write it to disk kubectl get secrets -n sre $SECRET -o jsonpath = '{.data.ca\\.crt}' | base64 -D > ca.crt # Retrieve the current context CONTEXT = $( kubectl config current-context ) # Retrieve the cluster name CLUSTER_NAME = $( kubectl config get-contexts $CONTEXT --no-headers = true | awk '{print $3}' ) # Retrieve the API endpoint SERVER = $( kubectl config view -o jsonpath = \"{.clusters[?(@.name == \\\" ${ CLUSTER_NAME } \\\")].cluster.server}\" ) # Set up variables KUBECONFIG_FILE = config USER = helm CA = ca.crt # Set up config kubectl config set-cluster $CLUSTER_NAME \\ --kubeconfig = $KUBECONFIG_FILE \\ --server = $SERVER \\ --certificate-authority = $CA \\ --embed-certs = true # Set token credentials kubectl config set-credentials \\ $USER \\ --kubeconfig = $KUBECONFIG_FILE \\ --token = $TOKEN # Set context entry kubectl config set-context \\ $USER \\ --kubeconfig = $KUBECONFIG_FILE \\ --cluster = $CLUSTER_NAME \\ --user = $USER # Set the current-context kubectl config use-context $USER \\ --kubeconfig = $KUBECONFIG_FILE Helm Install \u00b6 helm install \\ --name prometheus \\ stable/prometheus \\ --tiller-namespace dev1 \\ --kubeconfig config \\ --namespace dev1 \\ --set rbac.create = false NAME: prometheus LAST DEPLOYED: Sun Oct 28 16 :22:46 2018 NAMESPACE: dev1 STATUS: DEPLOYED helm install --name grafana \\ stable/grafana \\ --tiller-namespace dev2 \\ --kubeconfig config \\ --namespace dev2 \\ --set rbac.pspEnabled = false \\ --set rbac.create = false NAME: grafana LAST DEPLOYED: Sun Oct 28 16 :25:18 2018 NAMESPACE: dev2 STATUS: DEPLOYED References \u00b6 https://medium.com/@elijudah/configuring-minimal-rbac-permissions-for-helm-and-tiller-e7d792511d10 https://medium.com/virtuslab/think-twice-before-using-helm-25fbb18bc822 https://jenkins-x.io/architecture/helm3/ https://gist.github.com/innovia/fbba8259042f71db98ea8d4ad19bd708#file-kubernetes_add_service_account_kubeconfig-sh","title":"Kubernetes Post Install"},{"location":"blogs/kubernetes-post-install/#kubernetes-post-install","text":"What to do after you've installed your Kubernetes cluster, whether that was EKS via eksctl or GKE via gcloud. make network more secure with encryption weavenet for example install package manager install helm & tiller use nginx for ssl termination together with Let's Encrypt install nginx install cert-manager","title":"Kubernetes Post Install"},{"location":"blogs/kubernetes-post-install/#helm-tiller","text":"Helm is the defacto standard package manager for Kubernetes. Its current iteration is version 2, which has a client component - Helm - and a serverside component, Tiller. There's a problem with that, due this setup with Helm and Tiller, Tiller is aking to a cluster admin. This isn't very secure and there are several ways around that. JenkinsX : its binary ( jx ) can install helm charts without using Tiller. It generates the kubernetes resource files and installs these directly custom RBAC setup : you can also setup RBAC in such a way that every separate namespace gets its own Tiller, limiting the reach of any Tiller","title":"Helm &amp; Tiller"},{"location":"blogs/kubernetes-post-install/#tiller-custom-rbac-example","text":"","title":"Tiller Custom RBAC Example"},{"location":"blogs/kubernetes-post-install/#namespaces","text":"kind : Namespace apiVersion : v1 metadata : name : sre --- kind : Namespace apiVersion : v1 metadata : name : dev1 --- kind : Namespace apiVersion : v1 metadata : name : dev2","title":"Namespaces"},{"location":"blogs/kubernetes-post-install/#service-accounts","text":"kind : ServiceAccount apiVersion : v1 metadata : name : tiller namespace : dev1 --- kind : ServiceAccount apiVersion : v1 metadata : name : tiller namespace : dev2 --- kind : ServiceAccount apiVersion : v1 metadata : name : helm namespace : sre","title":"Service Accounts"},{"location":"blogs/kubernetes-post-install/#roles","text":"kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-manager namespace : dev1 rules : - apiGroups : [ \"\" , \"batch\" , \"extensions\" , \"apps\" ] resources : [ \"*\" ] verbs : [ \"*\" ] --- kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-manager namespace : dev2 rules : - apiGroups : [ \"\" , \"batch\" , \"extensions\" , \"apps\" ] resources : [ \"*\" ] verbs : [ \"*\" ] --- kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1 metadata : name : helm-clusterrole rules : - apiGroups : [ \"\" ] resources : [ \"pods/portforward\" ] verbs : [ \"create\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"list\" , \"get\" ]","title":"Roles"},{"location":"blogs/kubernetes-post-install/#rolebindings","text":"kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-binding namespace : dev1 subjects : - kind : ServiceAccount name : tiller namespace : dev1 roleRef : kind : Role name : tiller-manager apiGroup : rbac.authorization.k8s.io --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-binding namespace : dev2 subjects : - kind : ServiceAccount name : tiller namespace : dev2 roleRef : kind : Role name : tiller-manager apiGroup : rbac.authorization.k8s.io --- kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : helm-clusterrolebinding roleRef : kind : ClusterRole apiGroup : rbac.authorization.k8s.io name : helm-clusterrole subjects : - kind : ServiceAccount name : helm namespace : sre","title":"RoleBindings"},{"location":"blogs/kubernetes-post-install/#install-tiller","text":"helm init --service-account tiller --tiller-namespace dev1 helm init --service-account tiller --tiller-namespace dev2","title":"Install Tiller"},{"location":"blogs/kubernetes-post-install/#create-kubeconfig-for-helm-client","text":"# Find the secret associated with the Service Account SECRET = $( kubectl -n sre get sa helm -o jsonpath = '{.secrets[].name}' ) # Retrieve the token from the secret and decode it TOKEN = $( kubectl get secrets -n sre $SECRET -o jsonpath = '{.data.token}' | base64 -D ) # Retrieve the CA from the secret, decode it and write it to disk kubectl get secrets -n sre $SECRET -o jsonpath = '{.data.ca\\.crt}' | base64 -D > ca.crt # Retrieve the current context CONTEXT = $( kubectl config current-context ) # Retrieve the cluster name CLUSTER_NAME = $( kubectl config get-contexts $CONTEXT --no-headers = true | awk '{print $3}' ) # Retrieve the API endpoint SERVER = $( kubectl config view -o jsonpath = \"{.clusters[?(@.name == \\\" ${ CLUSTER_NAME } \\\")].cluster.server}\" ) # Set up variables KUBECONFIG_FILE = config USER = helm CA = ca.crt # Set up config kubectl config set-cluster $CLUSTER_NAME \\ --kubeconfig = $KUBECONFIG_FILE \\ --server = $SERVER \\ --certificate-authority = $CA \\ --embed-certs = true # Set token credentials kubectl config set-credentials \\ $USER \\ --kubeconfig = $KUBECONFIG_FILE \\ --token = $TOKEN # Set context entry kubectl config set-context \\ $USER \\ --kubeconfig = $KUBECONFIG_FILE \\ --cluster = $CLUSTER_NAME \\ --user = $USER # Set the current-context kubectl config use-context $USER \\ --kubeconfig = $KUBECONFIG_FILE","title":"Create KubeConfig for Helm client"},{"location":"blogs/kubernetes-post-install/#helm-install","text":"helm install \\ --name prometheus \\ stable/prometheus \\ --tiller-namespace dev1 \\ --kubeconfig config \\ --namespace dev1 \\ --set rbac.create = false NAME: prometheus LAST DEPLOYED: Sun Oct 28 16 :22:46 2018 NAMESPACE: dev1 STATUS: DEPLOYED helm install --name grafana \\ stable/grafana \\ --tiller-namespace dev2 \\ --kubeconfig config \\ --namespace dev2 \\ --set rbac.pspEnabled = false \\ --set rbac.create = false NAME: grafana LAST DEPLOYED: Sun Oct 28 16 :25:18 2018 NAMESPACE: dev2 STATUS: DEPLOYED","title":"Helm Install"},{"location":"blogs/kubernetes-post-install/#references","text":"https://medium.com/@elijudah/configuring-minimal-rbac-permissions-for-helm-and-tiller-e7d792511d10 https://medium.com/virtuslab/think-twice-before-using-helm-25fbb18bc822 https://jenkins-x.io/architecture/helm3/ https://gist.github.com/innovia/fbba8259042f71db98ea8d4ad19bd708#file-kubernetes_add_service_account_kubeconfig-sh","title":"References"},{"location":"blogs/kubernetes-sso-keycloak/","text":"Single Sign On on Kubernetes with Keycloak \u00b6 This article is about setting up an Apache Keycloak 1 instance for Single Sign-On 2 (SSO) on Kubernetes. Important This guide is created to help you show how you can do this from a technical point of view. If you are in an enterprise, please consult your in-house - if available - or external security professionals before running tools such as Keycloak in production. Goal \u00b6 The goal is to show why using Keycloak as SSO can be valuable and guide on how to do so. For what's in the How , look at the next paragraph: Steps . At the outset, this article is not about comparing SSO solutions or about claiming Keycloak is the best. Its a solution, and if you're interested in using this, it helps you with the how. Below are some links to alternatives as reference, but are not discussed. Audience \u00b6 The expected audience are those who are using Kubernetes 3 and are: looking to host a SSO solution themselves want to use LDAP 4 as User Federation for Keycloak want to setup a SSO with Jenkins 5 and/or SonarQube 6 Steps \u00b6 These are the steps we will execute. configure an LDAP server with a test data set package LDAP server in a Docker container and run it as an kubernetes Deployment you might think, why not a StatefulSet , but our LDAP will have static data and the idea of this setup, is that we do not control LDAP, meaning, we should expect LDAP's data to be immutable install Keycloak with Helm 8 we use Helm 2, as Helm 3 9 is still in beta at the time of writing (September 2019) configure Keycloak to use LDAP for User Federation install Jenkins and SonarQube with Helm 8 configure SSO with Keycloak in Jenkins configure SSO with Keycloak in SonarQube LDAP \u00b6 We will use OpenDJ 7 as LDAP implementation. There are many alternatives out there, feel free to use those. But for this guide, we will use OpenDJ's community edition, it works well and is easy to configure. We need the following: configured with a test data set including users and groups Docker container image definition Kubernetes Deployment definition to run Kubernetes Service definition to access a stable address Test Data Set \u00b6 example.ldiff This a full example. The different parts will be explained below. dn: dc=example,dc=com objectclass: top objectclass: domain dc: example dn: ou=People, dc=example,dc=com objectclass: top objectclass: organizationalunit ou: People aci: (targetattr=\"*||+\")(version 3.0; acl \"IDM Access\"; allow (all) userdn=\"ldap:///uid=idm,ou=Administrators,dc=example,dc=com\";) dn: uid=cptjack, ou=People, dc=example,dc=com cn: cpt. Jack Sparrow sn: Sparrow givenname: Jack objectclass: top objectclass: person objectclass: organizationalPerson objectclass: inetOrgPerson ou: Operations ou: People l: Caribbean uid: cptjack mail: jack@example.com telephonenumber: +421 910 123456 facsimiletelephonenumber: +1 408 555 1111 roomnumber: 666 userpassword: MyAwesomePassword dn: uid=djones, ou=People, dc=example,dc=com cn: Davy Jones sn: Jones givenname: Davy objectclass: top objectclass: person objectclass: organizationalPerson objectclass: inetOrgPerson ou: Operations ou: People l: Caribbean uid: djones mail: d.jones@example.com telephonenumber: +421 910 382735 facsimiletelephonenumber: +1 408 555 1112 roomnumber: 112 userpassword: MyAwesomePassword dn: ou=Groups, dc=example,dc=com objectclass: top objectclass: organizationalunit ou: Groups aci: (targetattr=\"*||+\")(version 3.0; acl \"IDM Access\"; allow (all) userdn=\"ldap:///uid=idm,ou=Administrators,dc=example,dc=com\";) dn: cn=Pirates,ou=Groups,dc=example,dc=com objectclass: top objectclass: groupOfUniqueNames cn: Pirates ou: Groups uniquemember: uid=cptjack, ou=People, dc=example,dc=com uniquemember: uid=djones, ou=People, dc=example,dc=com description: Arrrrr! dn: cn=Catmins,ou=Groups,dc=example,dc=com objectclass: top objectclass: groupOfUniqueNames cn: Catmins ou: Groups uniquemember: uid=djones, ou=People, dc=example,dc=com description: Purrrr! dn: ou=Administrators, dc=example,dc=com objectclass: top objectclass: organizationalunit ou: Administrators dn: uid=idm, ou=Administrators,dc=example,dc=com objectclass: top objectclass: person objectclass: organizationalPerson objectclass: inetOrgPerson uid: idm cn: IDM Administrator sn: IDM Administrator description: Special LDAP acccount used by the IDM to access the LDAP data. ou: Administrators userPassword: MySecretAdminPassword ds-privilege-name: unindexed-search ds-privilege-name: password-reset Basics \u00b6 We need some basic meta data for our server. The two most important elements being: the base dn ( Distinguished Name ), where all of our data lives build up out of dc ( domainComponent ) elements, representing a domain name, usually companyName.com the dn where our user data will live and which user owns it (the userdn assigned in the aci ) dn: dc=example,dc=com objectclass: top objectclass: domain dc: example dn: ou=People, dc=example,dc=com objectclass: top objectclass: organizationalunit ou: People aci: (targetattr=\"*||+\")(version 3.0; acl \"IDM Access\"; allow (all) userdn=\"ldap:///uid=idm,ou=Administrators,dc=example,dc=com\";) User Entries \u00b6 After these two main metadata dn elements, we have all the user entries. We start with the identifier of the entry, designated by the dn , followed by attributes and classifications. While LDAP stands for Lightweight , it has a lot of attributes which are not easily understood because they are commonly used by their two-letter acronym only. Below we explain a few of the attributes, but there are more complete lists available 10 . dn : the full identifier of the data entry, in reverse tree uid : the unique id of the user within the organizational structure cn : Common Name , generally how humans would identify this resource l : Location ou : Organizational Unit , these commonly represent departments objectclass : defines a object hierarchy, derived from Object Oriented Programming top : is commonly the root object class, every other object inheriting from there object classes are used to create typed objects which have their own schema this allows you to define what values are required and which are optional object classes are also used in search filters, so you can distinguish different value types (users, groups) Organizational Tree . \u251c\u2500\u2500 com \u2502 \u2514\u2500\u2500 example \u2502 \u251c\u2500\u2500 Operations \u2502 \u2514\u2500\u2500 People \u2502 \u2514\u2500\u2500 jwick dn: uid=jwick, ou=People, dc=example,dc=com cn: John Wick sn: Wick givenname: John objectclass: top objectclass: person objectclass: organizationalPerson objectclass: inetOrgPerson ou: Operations ou: People l: New York uid: jwick mail: jwick@example.com telephonenumber: +1 408 555 1236 facsimiletelephonenumber: +1 408 555 4323 roomnumber: 233 userpassword: myawesomepassword Group \u00b6 We first create the OU housing our Groups, including which object classes it represents. Then we can attach groups to this OU by creating dn 's under it. There are two flavors of groups, we have groupOfNames and groupOfUniqueNames , I'm sure you understand what the difference is. We explain wich kind of group we are by the objectclass: groupOfUniqueNames , and include a list of members. In the case of groupOfNames we use member: <member dn> , in our case we user uniquemember: uid=cptjack, ou=People, dc=example,dc=com . To make it a list, add more than one entry, each with a unique value. dn: ou=Groups, dc=example,dc=com objectclass: top objectclass: organizationalunit ou: Groups aci: (targetattr=\"*||+\")(version 3.0; acl \"IDM Access\"; allow (all) userdn=\"ldap:///uid=idm,ou=Administrators,dc=example,dc=com\";) dn: cn=Pirates,ou=Groups,dc=example,dc=com objectclass: top objectclass: groupOfUniqueNames cn: Pirates ou: Groups uniquemember: uid=cptjack, ou=People, dc=example,dc=com uniquemember: uid=will, ou=People, dc=example,dc=com uniquemember: uid=djones, ou=People, dc=example,dc=com description: Arrrrr! Administrator \u00b6 We have to configure our main administrator, as we've already given it a lot of privileges to manage the other resources (via the aci rules). As usual, we start with defining a ou that houses our Administrators . In this case, we give it two more special permissions via ds-privilege-name . So this user can also reset passwords, just in case. dn: ou=Administrators, dc=example,dc=com objectclass: top objectclass: organizationalunit ou: Administrators dn: uid=idm, ou=Administrators,dc=example,dc=com objectclass: top objectclass: person objectclass: organizationalPerson objectclass: inetOrgPerson uid: idm cn: IDM Administrator sn: IDM Administrator description: Special LDAP acccount used by the IDM to access the LDAP data. ou: Administrators userPassword: MySecretAdminPassword ds-privilege-name: unindexed-search ds-privilege-name: password-reset Configure OpenDJ \u00b6 One of the reasons I like OpenDJ is because it is very easy to operate. It has many tools to help you manage the server. Some are separate tools that interact with a running server, others interact with the configure while the server is offline. We have to do two configuration actions, 1) we intialize it with our root dn , which port to use and so on, 2) we add our test data set, so we have our groups and users to work with. /opt/opendj/setup --cli \\ -p 1389 \\ --ldapsPort 1636 \\ --enableStartTLS \\ --generateSelfSignedCertificate \\ --baseDN dc = example,dc = com \\ -h localhost \\ --rootUserDN \" $ROOT_USER_DN \" \\ --rootUserPassword \" $ROOT_PASSWORD \" \\ --acceptLicense \\ --no-prompt \\ --doNotStart /opt/opendj/bin/import-ldif \\ --includeBranch dc = example,dc = com \\ --backendID userRoot \\ --offline \\ --ldifFile example.ldiff Dockerfile \u00b6 For completeness I will also include my Docker image. We use Tini 11 order to manage the process of OpenDJ nicely, even in the face of being shutdown. For more information, read my article on Docker Graceful Shutdown . We use a jre as we only need to run Java, so no need for a JDK. We use the OpenJDK distribution from Azul 15 in order to avoid possible violations of Oracle's License 12 . We use Azul's Alpine 13 based image as it is much smaller than those based of full-fledged OS's. This makes the image smaller in size on disk and in memory and reduces the attack vector and recommended by the likes of Docker and Snyk 14 . We download the latest version available which, as of this writing in September 2019, is 4.4.3 which you can retrieve from the OpenDJ Community's releases page on GitHub 13 . FROM azul/zulu-openjdk-alpine:8u222-jre LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" LABEL version = \"0.2.0\" LABEL description = \"OpenDJ container\" WORKDIR /opt EXPOSE 1389 1636 4444 ENV CHANGE_DATE = '20190916-2100' ENV JAVA_HOME /usr/lib/jvm/zulu-8 ENV OPENDJ_JAVA_HOME /usr/lib/jvm/zulu-8 ENV VERSION = 4 .4.3 ENV ROOT_USER_DN = 'cn=admin' ENV ROOT_PASSWORD = 'changeme' RUN apk add --no-cache tini ENTRYPOINT [ \"/sbin/tini\" , \"-vv\" , \"-g\" , \"-s\" , \"--\" ] CMD [ \"/opt/opendj/bin/start-ds\" , \"--nodetach\" ] RUN wget --quiet \\ https://github.com/OpenIdentityPlatform/OpenDJ/releases/download/ $VERSION /opendj- $VERSION .zip && \\ unzip opendj- $VERSION .zip && \\ rm -r opendj- $VERSION .zip RUN /opt/opendj/setup --cli \\ -p 1389 \\ --ldapsPort 1636 \\ --enableStartTLS \\ --generateSelfSignedCertificate \\ --baseDN dc = example,dc = com \\ -h localhost \\ --rootUserDN \" $ROOT_USER_DN \" \\ --rootUserPassword \" $ROOT_PASSWORD \" \\ --acceptLicense \\ --no-prompt \\ --doNotStart ADD Example.ldif /var/tmp/example.ldiff # RUN /opt/opendj/bin/import-ldif --help RUN /opt/opendj/bin/import-ldif --includeBranch dc = example,dc = com --backendID userRoot --offline --ldifFile /var/tmp/example.ldiff Kubernetes Deployment \u00b6 In production you might want to run LDAP in a StatefulSet and give it some permanent storage. But in this guide the goal of LDAP is to show how to use it with Keycloak and we stick to a Deployment as it is easier. apiVersion : apps/v1 kind : Deployment metadata : name : opendj4 labels : app : opendj4 spec : replicas : 1 selector : matchLabels : app : opendj4 template : metadata : labels : app : opendj4 spec : containers : - name : opendj4 image : caladreas/opendj:4.4.3-1 ports : - containerPort : 1389 name : ldap resources : requests : memory : \"250Mi\" cpu : \"50m\" limits : memory : \"500Mi\" cpu : \"250m\" Kubernetes Service \u00b6 When we use a Deployment our container instance will have a generated name and a new ip address on every (re-)start. So we use a Service to create a stable endpoint, which means that we will now access our LDAP server via our service: ldap://opendj4:389 . apiVersion : v1 kind : Service metadata : labels : app : opendj4 name : opendj4 spec : ports : - name : http port : 389 targetPort : 1389 protocol : TCP selector : app : opendj4 Caution If you deploy the Service and Deployment in a different namespace than where you want to access them from, you will have to add the namespace to the access url. If you've configured them in namespace ldap , the access url becomes ldap://opendj4.ldap:389 . Homegrown Helm chart with GitHub \u00b6 You can also package the above explained Docker Image + Kubernetes Yaml definition as a Helm package in a GitHub repository 16 . So if you do not want to use any of the above, feel free to use my personal Helm Repository 17 . helm repo add joostvdg https://raw.githubusercontent.com/joostvdg/helm-repo/master/ helm repo update helm install joostvdg/opendj4 --name ldap --namespace ldap The service, retrieved by kubectl get service now has a different name. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ldap-opendj4 ClusterIP 10 .100.19.29 <none> 389 /TCP 105s So now we access the LDAP server via ldap://ldap-opendj4:389 . Keycloak \u00b6 Pre-requisites \u00b6 Helm installed TLS certificate Using a tool as Keycloak to do SSO well, feels wrong without using TLS certificates. So I wholeheartedly recommend configuring Keycloak with a Domain name and TLS Certificate. For the TLS certificate, you use Let's Encrypt 20 with Cert Manager 19 , if you unsure how to proceed in Kubernetes read my guide on Let's Encrypt on Kubernetes . Install Via Helm \u00b6 helm repo add codecentric https://codecentric.github.io/helm-charts helm repo update kubectl apply -f keycloak-certificate.yaml helm install --name keycloak codecentric/keycloak -f keycloak-values.yaml Note Make sure you replace the dns name keycloak.my.domain.com with your own domain. If you do not have a domain, you can use `nip.io 18 . keycloak-certificate.yaml apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : keycloak.my.domain.com spec : secretName : tls-keycloak dnsNames : - keycloak.my.domain.com acme : config : - http01 : ingressClass : nginx domains : - keycloak.my.domain.com issuerRef : name : letsencrypt-prod kind : ClusterIssuer keycloak-values.yaml keycloak : password : notsosecret ingress : enabled : true path : / annotations : kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : \"true\" ingress.kubernetes.io/affinity : cookie hosts : - keycloak.my.domain.com tls : - hosts : - keycloak.my.domain.com secretName : tls-keycloak LDAP As User Federation \u00b6 Assuming you have Keycloak running now, login with the admin user, keycloak , and the password you set in the keycloak-values.yaml . In the left hand menu, you can select User Federation , this is where we can add ldap and kerberos providers. As Keycloak supports multiple sources, these will be listed by their priority (the smaller the number, the higher). The federations are consulted in the order according to their priority. Create LDAP \u00b6 Let's create the new LDAP provider, select the Add provider dropdown - top right - and choose ldap . You will now see a whole list of values to fill in, don't worry, many can be kept as default. And to be sure, I will list them all here. For more information, each field has a little question mark, hover over it. Info When you've filled in the Connection URL , you should test the configuration with the Test connection button. The same is true for when you've configured the fields Users DN , Bind Type , Bind DN , Bind Credential with the button Test authentication . Console Display Name : display name, should be a name that tells you what this provider is Priority : the priority of this provider, will be used for the order that the federation is accessed Import Users : wether or not to import the users, I just leave this to On as to cache the users Import Users : READ_ONLY , in this guide we assume the LDAP server is not under your control, so read only Vendor : Other Username LDAP attribute : uid RDN LDAP attribute : uid UUID LDAP attribute : entryUUID User Object Classes : inetOrgPerson, organizationalPerson Connection URL : ldap://opendj4:389 Users DN : dc=example,dc=com Bind Type : simple Bind DN : uid=idm, ou=Administrators,dc=example,dc=com Bind Credential : secret -> or what ever you've set it in the example.diff Custom User LDAP Filter : `` Search Scope : Subtree Validate Password Policy : OFF Use Truststore SPI : Only for ldap Connection Pooling : ON Connection Timeout : `` Read Timeout : `` Pagination : ON Sync Settings: Cache Policy : 1000 Periodic Full Sync : On Full Sync Period : 604800 Periodic Changed Users Sync : On Changed Users Sync Period : 86400 Cache Settings: Cache Policy : DEFAULT Add Group Mapping \u00b6 If the value is not mentioned, the default value should be fine. Some values listed here are default, but listed all the same. Name : groups Mapper Type : group-ldap-mapper LDAP Groups DN : ou=Groups, dc=example,dc=com Group Name LDAP Attribute : cn Group Object Classes : groupOfUniqueNames Membership LDAP Attribute : uniquemember User Groups Retrieve Strategy : LOAD_GROUPS_BY_MEMBER_ATTRIBUTE When you hit Save , you can synchronize the groups to Keycloak - if you don't need to, it will confirm the configuration works. Hit the Sync LDAP Groups To Keycloak button, and on top there should be a temporary banner stating how many groups were synchronized (if all categories are 0 , something is wrong). Note Everytime you change a value, you first have to save the page before you can synchronize again. Mix LDAP Users With Other Sources \u00b6 One of the reasons for this guide to exist, is to be able to encapsulate an LDAP over which you have no control and add additional accounts and groups. There's multiple ways forward here, you can use Identity Providers and User Federation to create more sources of user accounts. Perhaps the simplest way is to manage these extra accounts in Keycloak itself. It has its own User database and Groups database. In addition to that, it allows you to assign users created in Keycloak to be a member of a group derived from LDAP - if you've synched them. UI \u00b6 We can use the UI in Keycloak to manage Users and Groups. We initiate this by going to the Users view and hit Add user . We can then fill in all the details of the User. REST API \u00b6 Keycloak has a rich REST API with good decent documentation 21 . The thing missing is some examples for how to use them correctly. I recommend using HTTPie 28 rather than cUrl, as it is easier to use for these more complex calls. Get Token \u00b6 Warning The Bearer Token is only valid for a short period of time. If you wait too long, you will get 401 unauthorized . The default REALM is master . httpie http --form POST \" ${ KEYCLOAK_URL } /auth/realms/ ${ REALM } /protocol/openid-connect/token\" username = \"keycloak\" password = \" ${ PASS } \" client_id = \"admin-cli\" grant_type = \"password\" curl curl ${ KEYCLOAK_URL } /auth/realms/ ${ REALM } /protocol/openid-connect/token -u keycloak: ${ PASS } Get Users \u00b6 httpie http \" ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users\" \"Authorization: Bearer $TOKEN \" httpie - get user http \" ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users/ ${ userId } \" \"Authorization: Bearer $TOKEN \" curl curl -v ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users -H \"Authorization: Bearer $TOKEN \" | jq Create User \u00b6 httpie http POST \" ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users\" \\ \"Authorization: Bearer $TOKEN \" \\ credentials: = \"[{\\\"value\\\" : \\\"mypass\\\", \\\"type\\\": \\\"password\\\" }]\" \\ email = \"user@example.com\" \\ firstName = \"hannibal\" \\ lastName = \"lecter\" \\ username = \"hlecter\" \\ groups: = '[\"Robots\"]' \\ emailVerified: = true \\ enabled: = true curl curl -v ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users -H \"Authorization: Bearer $TOKEN \" | jq Verify \u00b6 To verify everything is working as it should, you can go to the Groups and Users pages within the Manage menu (left hand side). With Groups you have View all groups , which should have the groups from LDAP and any group you have created within Keycloak - if not yet, you can do so here as well. To view details of a Group, double click the name - it's not obvious you can do so - and you will go to the details page. Here you can also see the members of the Group. You can do the same with the Users menu item. By default the page is empty, if your list of users is not too large, click on View all users . You can view the details page of a user by clicking the link of the User ID. Within the Groups tab you can add the user to more groups, this should contain all the groups known to Keycloak - both Keycloak internal and from LDAP. SSO with Jenkins \u00b6 To configure Jenkins to use Keycloak we have two plugins at our disposal, OpenId Connect( oic-auth ) 22 and Keycloak 23 . While the Keycloak plugin is easier to configure for authentication, I found it difficult to configure groups. As I feel the group management is mandatory we're going with the OpenId Connect plugin. You can install plugins in Jenkins via the UI 24 , via the new Jenkins CLI 25 , or via the values.yaml when installing via the Helm Chart 27 . Important Install the OpenId Connect plugin before configuring the next parts, and restart your Jenkins instance for best results. In order to configure OpenId Connect with Jenkins, it is the easiest to use the well-known endpoint url. This endpoint contains all the configuration information the plugin needs to configure itself. Usually, this is ${KEYCLOAK_URL}/auth/realms/${REALM}/.well-known/openid-configuration . From Keycloak's perspective we have to register Jenkins as a Client. Keycloak Client \u00b6 We go to the Clients screen in Keycloak and hit the Create button. In the next screen, we need to supply three values: Client ID : the name of your client, jenkins would be good example Client Protocol : openid-connect is recommended Root URL : the main url of your installation, for example https://jenkins.my.domain.com Main Settings \u00b6 Once we hit save, we get a details view. We have to change some values here. Access Type : we need a Client ID and Client Secret, we only get this when we select confidential Valid Redirect URIs : confirm this is ${yourJenkinsURL}/* Once you hit save, you get a new Tab in the Details screen of this Client. It is called Credentials , and here you can see the Client Secret which we will need to enter in Jenkins. Group Mappings \u00b6 Just as we had to add the Group mapping to the LDAP configuration, we will need to configure the Group mapping in the Client. If we go into the Client details, we see there's a Tab called Mappers . We create a new Mapping here by hitting the Create button. Give the mapping a name and then select the Mapper Type Group Membership . The Token Claim Name is important as well, we use this in our Jenkins configuration, give it a descriptive name such as group-membership . Configure Via UI \u00b6 In Jenkins we go to Manage Jenkins -> Configure Global Security and here we select Login with Openid Connect in the Security Realm block. Client id : the client we've configured in Keycloak, if you've followed this guide, it should be jenkins Client secret : the secret of the client configured in Keycloak, if you've lost it, go back to Keycloak -> Clients -> jenkins -> Credentials and copy the value in the field Secret . We now get a Configuration mode block where we can select either Automatic configuration on Manual configuration . We select Automatic and enter our Well-known configuration endpoint URL from Keycloak we've written down earlier. If you don't remember, the format is usually this: ${KEYCLOAK_URL}/auth/realms/${REALM}/.well-known/openid-configuration . User name field name : preferred_username Full name field name : name Email field name : email Groups field name : group-membership Note The field Groups field name refers back to the Token Claim Name we configured in Keycloak within our Client's Mapping for Group Membership. Configure Via Configuration-as-Code \u00b6 We can use the amazing Jenkins Configuration-as-Code 26 to make sure our SSO configuration is configured out-of-the-box! In order to avoid the having to store the Client ID and Client Secret, we're going to create these as Kubernetes Secrets first. There's more ways to create these, but this is to keep it simple. kubectl create secret generic oic-auth \\ --from-literal = clientID = \" ${ CLIENT_ID } \" \\ --from-literal = clientSecret = \" ${ CLIENT_SECRET } \" \\ --from-literal = keycloakUrl = ${ keycloakUrl } \\ --namespace jenkins JCASC-config.yaml jenkins : securityRealm : oic : clientId : \"${clientID}\" clientSecret : \"${clientSecret}\" wellKnownOpenIDConfigurationUrl : \"${keycloakUrl}/auth/realms/master/.well-known/openid-configuration\" tokenServerUrl : \"${keycloakUrl}auth/realms/master/protocol/openid-connect/token\" authorizationServerUrl : \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/auth\" userInfoServerUrl : \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/userinfo\" userNameField : \"preferred_username\" fullNameFieldName : \"name\" emailFieldName : \"email\" groupsFieldName : \"group-membership\" scopes : \"web-origins address phone openid offline_access profile roles microprofile-jwt email\" disableSslVerification : false logoutFromOpenidProvider : true endSessionUrl : \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/logout\" postLogoutRedirectUrl : \"\" escapeHatchEnabled : false escapeHatchSecret : \"\" complete-jenkins-helm-values.yaml master : ingress : enabled : true hostName : jenkins.my.doamain.com csrf : defaultCrumbIssuer : enabled : true proxyCompatability : true cli : false installPlugins : - kubernetes:latest - kubernetes-credentials:latest - workflow-aggregator:latest - workflow-job:latest - credentials-binding:latest - git:latest - blueocean:latest - prometheus:latest - matrix-auth:latest - keycloak:latest - oic-auth:latest JCasC : enabled : true pluginVersion : \"1.30\" configScripts : welcome-message : | jenkins: systemMessage: Welcome, this Jenkins is configured and managed as code. ldap-settings : | jenkins: securityRealm: oic: clientId: \"${clientID}\" clientSecret: \"${clientSecret}\" wellKnownOpenIDConfigurationUrl: \"${keycloakUrl}/auth/realms/master/.well-known/openid-configuration\" tokenServerUrl: \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/token\" authorizationServerUrl: \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/auth\" userInfoServerUrl: \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/userinfo\" userNameField: \"preferred_username\" fullNameFieldName: \"name\" emailFieldName: \"email\" groupsFieldName: \"group-membership\" scopes: \"web-origins address phone openid offline_access profile roles microprofile-jwt email\" disableSslVerification: false logoutFromOpenidProvider: true endSessionUrl: \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/logout\" postLogoutRedirectUrl: \"\" escapeHatchEnabled: false escapeHatchSecret: \"\" matrix-auth : | jenkins: authorizationStrategy: globalMatrix: permissions: - \"Overall/Read:authenticated\" - \"Overall/Administer:barbossa\" - \"Overall/Administer:Catmins\" - \"Overall/Administer:Robots\" - \"Overall/Administer:/Catmins\" - \"Overall/Administer:/Robots\" persistence : enabled : true volumes : - name : oic-auth-clientid secret : secretName : oic-auth items : - key : clientID path : clientID - name : oic-auth-clientsecret secret : secretName : oic-auth items : - key : clientSecret path : clientSecret - name : oic-auth-keycloakurl secret : secretName : oic-auth items : - key : keycloakUrl path : keycloakUrl mounts : - name : oic-auth-clientid mountPath : /run/secrets/clientID subPath : clientID - name : oic-auth-clientsecret mountPath : /run/secrets/clientSecret subPath : clientSecret - name : oic-auth-keycloakurl mountPath : /run/secrets/keycloakUrl subPath : keycloakUrl Use Keycloak Groups In Jenkins \u00b6 Warning Unfortuantely, there is one caveat about using Keycloak as intermediary between LDAP and Jenkins. Groups do not come across the same as before, they're now prefixed with / . I'm sure it is down to a misconfiguration on my end, so please let me know how to resolve that if you figure it out. This means, that a LDAP group called Catmins will have to be used in Jenkins via /Catmins in Matrix , Project Matrix or other authorization schemes. Verify \u00b6 To verify, take the leap of faith to save the configuration, logout (top right) and then log back in. If everything goes well, you will be redirected to Keycloak, once successfully logged in, redirected back to Jenkins! Note One thing to remark, that if you're logged into Jenkins and you want to see the Groups, you can select your User (top right, click on your name). Alternatively, you can go to ${JENKINS_URL}/whoAmI (notice the capital casing of the 'A' and 'I', it is required). References \u00b6 Apache Keycloak Home \u21a9 Wikipedia Definition on Single sign-on \u21a9 Kubernetes Home \u21a9 Lightweight Directory Access Protocol (LDAP) \u21a9 Jenkins Home \u21a9 SonarQube Home \u21a9 OpenDJ Community Edition \u21a9 Helm - Package Manager for Kubernetes \u21a9 \u21a9 Helm 3 Beta \u21a9 Common Used LDAP Attributes Explained \u21a9 tini - process manager \u21a9 Overops Article on Java License in Docker images \u21a9 Alpine Docker Image \u21a9 \u21a9 Snyk - 10 docker image security best practices \u21a9 Azul OpenJDK Alpine Image \u21a9 Hosting Helm Private Repository from GitHub \u21a9 Joost van der Griendt's Helm Repository \u21a9 Nip Io - Dead simple wildcard DNS for any IP Address \u21a9 Cert Manager \u21a9 Let's Encrypt - Let's Encrypt the world! \u21a9 Keycloak Admin REST API \u21a9 Jenkins OpenId Connect Plugin \u21a9 Jenkins Keycloak Plugin \u21a9 Jenkins Plugin Management \u21a9 Jenkins JCLI \u21a9 Jenkins Configuration As Code \u21a9 Jenkins Helm Chart \u21a9 HTTPie http commandline interface \u21a9","title":"Kubernetes SSO Keycloak"},{"location":"blogs/kubernetes-sso-keycloak/#single-sign-on-on-kubernetes-with-keycloak","text":"This article is about setting up an Apache Keycloak 1 instance for Single Sign-On 2 (SSO) on Kubernetes. Important This guide is created to help you show how you can do this from a technical point of view. If you are in an enterprise, please consult your in-house - if available - or external security professionals before running tools such as Keycloak in production.","title":"Single Sign On on Kubernetes with Keycloak"},{"location":"blogs/kubernetes-sso-keycloak/#goal","text":"The goal is to show why using Keycloak as SSO can be valuable and guide on how to do so. For what's in the How , look at the next paragraph: Steps . At the outset, this article is not about comparing SSO solutions or about claiming Keycloak is the best. Its a solution, and if you're interested in using this, it helps you with the how. Below are some links to alternatives as reference, but are not discussed.","title":"Goal"},{"location":"blogs/kubernetes-sso-keycloak/#audience","text":"The expected audience are those who are using Kubernetes 3 and are: looking to host a SSO solution themselves want to use LDAP 4 as User Federation for Keycloak want to setup a SSO with Jenkins 5 and/or SonarQube 6","title":"Audience"},{"location":"blogs/kubernetes-sso-keycloak/#steps","text":"These are the steps we will execute. configure an LDAP server with a test data set package LDAP server in a Docker container and run it as an kubernetes Deployment you might think, why not a StatefulSet , but our LDAP will have static data and the idea of this setup, is that we do not control LDAP, meaning, we should expect LDAP's data to be immutable install Keycloak with Helm 8 we use Helm 2, as Helm 3 9 is still in beta at the time of writing (September 2019) configure Keycloak to use LDAP for User Federation install Jenkins and SonarQube with Helm 8 configure SSO with Keycloak in Jenkins configure SSO with Keycloak in SonarQube","title":"Steps"},{"location":"blogs/kubernetes-sso-keycloak/#ldap","text":"We will use OpenDJ 7 as LDAP implementation. There are many alternatives out there, feel free to use those. But for this guide, we will use OpenDJ's community edition, it works well and is easy to configure. We need the following: configured with a test data set including users and groups Docker container image definition Kubernetes Deployment definition to run Kubernetes Service definition to access a stable address","title":"LDAP"},{"location":"blogs/kubernetes-sso-keycloak/#test-data-set","text":"example.ldiff This a full example. The different parts will be explained below. dn: dc=example,dc=com objectclass: top objectclass: domain dc: example dn: ou=People, dc=example,dc=com objectclass: top objectclass: organizationalunit ou: People aci: (targetattr=\"*||+\")(version 3.0; acl \"IDM Access\"; allow (all) userdn=\"ldap:///uid=idm,ou=Administrators,dc=example,dc=com\";) dn: uid=cptjack, ou=People, dc=example,dc=com cn: cpt. Jack Sparrow sn: Sparrow givenname: Jack objectclass: top objectclass: person objectclass: organizationalPerson objectclass: inetOrgPerson ou: Operations ou: People l: Caribbean uid: cptjack mail: jack@example.com telephonenumber: +421 910 123456 facsimiletelephonenumber: +1 408 555 1111 roomnumber: 666 userpassword: MyAwesomePassword dn: uid=djones, ou=People, dc=example,dc=com cn: Davy Jones sn: Jones givenname: Davy objectclass: top objectclass: person objectclass: organizationalPerson objectclass: inetOrgPerson ou: Operations ou: People l: Caribbean uid: djones mail: d.jones@example.com telephonenumber: +421 910 382735 facsimiletelephonenumber: +1 408 555 1112 roomnumber: 112 userpassword: MyAwesomePassword dn: ou=Groups, dc=example,dc=com objectclass: top objectclass: organizationalunit ou: Groups aci: (targetattr=\"*||+\")(version 3.0; acl \"IDM Access\"; allow (all) userdn=\"ldap:///uid=idm,ou=Administrators,dc=example,dc=com\";) dn: cn=Pirates,ou=Groups,dc=example,dc=com objectclass: top objectclass: groupOfUniqueNames cn: Pirates ou: Groups uniquemember: uid=cptjack, ou=People, dc=example,dc=com uniquemember: uid=djones, ou=People, dc=example,dc=com description: Arrrrr! dn: cn=Catmins,ou=Groups,dc=example,dc=com objectclass: top objectclass: groupOfUniqueNames cn: Catmins ou: Groups uniquemember: uid=djones, ou=People, dc=example,dc=com description: Purrrr! dn: ou=Administrators, dc=example,dc=com objectclass: top objectclass: organizationalunit ou: Administrators dn: uid=idm, ou=Administrators,dc=example,dc=com objectclass: top objectclass: person objectclass: organizationalPerson objectclass: inetOrgPerson uid: idm cn: IDM Administrator sn: IDM Administrator description: Special LDAP acccount used by the IDM to access the LDAP data. ou: Administrators userPassword: MySecretAdminPassword ds-privilege-name: unindexed-search ds-privilege-name: password-reset","title":"Test Data Set"},{"location":"blogs/kubernetes-sso-keycloak/#basics","text":"We need some basic meta data for our server. The two most important elements being: the base dn ( Distinguished Name ), where all of our data lives build up out of dc ( domainComponent ) elements, representing a domain name, usually companyName.com the dn where our user data will live and which user owns it (the userdn assigned in the aci ) dn: dc=example,dc=com objectclass: top objectclass: domain dc: example dn: ou=People, dc=example,dc=com objectclass: top objectclass: organizationalunit ou: People aci: (targetattr=\"*||+\")(version 3.0; acl \"IDM Access\"; allow (all) userdn=\"ldap:///uid=idm,ou=Administrators,dc=example,dc=com\";)","title":"Basics"},{"location":"blogs/kubernetes-sso-keycloak/#user-entries","text":"After these two main metadata dn elements, we have all the user entries. We start with the identifier of the entry, designated by the dn , followed by attributes and classifications. While LDAP stands for Lightweight , it has a lot of attributes which are not easily understood because they are commonly used by their two-letter acronym only. Below we explain a few of the attributes, but there are more complete lists available 10 . dn : the full identifier of the data entry, in reverse tree uid : the unique id of the user within the organizational structure cn : Common Name , generally how humans would identify this resource l : Location ou : Organizational Unit , these commonly represent departments objectclass : defines a object hierarchy, derived from Object Oriented Programming top : is commonly the root object class, every other object inheriting from there object classes are used to create typed objects which have their own schema this allows you to define what values are required and which are optional object classes are also used in search filters, so you can distinguish different value types (users, groups) Organizational Tree . \u251c\u2500\u2500 com \u2502 \u2514\u2500\u2500 example \u2502 \u251c\u2500\u2500 Operations \u2502 \u2514\u2500\u2500 People \u2502 \u2514\u2500\u2500 jwick dn: uid=jwick, ou=People, dc=example,dc=com cn: John Wick sn: Wick givenname: John objectclass: top objectclass: person objectclass: organizationalPerson objectclass: inetOrgPerson ou: Operations ou: People l: New York uid: jwick mail: jwick@example.com telephonenumber: +1 408 555 1236 facsimiletelephonenumber: +1 408 555 4323 roomnumber: 233 userpassword: myawesomepassword","title":"User Entries"},{"location":"blogs/kubernetes-sso-keycloak/#group","text":"We first create the OU housing our Groups, including which object classes it represents. Then we can attach groups to this OU by creating dn 's under it. There are two flavors of groups, we have groupOfNames and groupOfUniqueNames , I'm sure you understand what the difference is. We explain wich kind of group we are by the objectclass: groupOfUniqueNames , and include a list of members. In the case of groupOfNames we use member: <member dn> , in our case we user uniquemember: uid=cptjack, ou=People, dc=example,dc=com . To make it a list, add more than one entry, each with a unique value. dn: ou=Groups, dc=example,dc=com objectclass: top objectclass: organizationalunit ou: Groups aci: (targetattr=\"*||+\")(version 3.0; acl \"IDM Access\"; allow (all) userdn=\"ldap:///uid=idm,ou=Administrators,dc=example,dc=com\";) dn: cn=Pirates,ou=Groups,dc=example,dc=com objectclass: top objectclass: groupOfUniqueNames cn: Pirates ou: Groups uniquemember: uid=cptjack, ou=People, dc=example,dc=com uniquemember: uid=will, ou=People, dc=example,dc=com uniquemember: uid=djones, ou=People, dc=example,dc=com description: Arrrrr!","title":"Group"},{"location":"blogs/kubernetes-sso-keycloak/#administrator","text":"We have to configure our main administrator, as we've already given it a lot of privileges to manage the other resources (via the aci rules). As usual, we start with defining a ou that houses our Administrators . In this case, we give it two more special permissions via ds-privilege-name . So this user can also reset passwords, just in case. dn: ou=Administrators, dc=example,dc=com objectclass: top objectclass: organizationalunit ou: Administrators dn: uid=idm, ou=Administrators,dc=example,dc=com objectclass: top objectclass: person objectclass: organizationalPerson objectclass: inetOrgPerson uid: idm cn: IDM Administrator sn: IDM Administrator description: Special LDAP acccount used by the IDM to access the LDAP data. ou: Administrators userPassword: MySecretAdminPassword ds-privilege-name: unindexed-search ds-privilege-name: password-reset","title":"Administrator"},{"location":"blogs/kubernetes-sso-keycloak/#configure-opendj","text":"One of the reasons I like OpenDJ is because it is very easy to operate. It has many tools to help you manage the server. Some are separate tools that interact with a running server, others interact with the configure while the server is offline. We have to do two configuration actions, 1) we intialize it with our root dn , which port to use and so on, 2) we add our test data set, so we have our groups and users to work with. /opt/opendj/setup --cli \\ -p 1389 \\ --ldapsPort 1636 \\ --enableStartTLS \\ --generateSelfSignedCertificate \\ --baseDN dc = example,dc = com \\ -h localhost \\ --rootUserDN \" $ROOT_USER_DN \" \\ --rootUserPassword \" $ROOT_PASSWORD \" \\ --acceptLicense \\ --no-prompt \\ --doNotStart /opt/opendj/bin/import-ldif \\ --includeBranch dc = example,dc = com \\ --backendID userRoot \\ --offline \\ --ldifFile example.ldiff","title":"Configure OpenDJ"},{"location":"blogs/kubernetes-sso-keycloak/#dockerfile","text":"For completeness I will also include my Docker image. We use Tini 11 order to manage the process of OpenDJ nicely, even in the face of being shutdown. For more information, read my article on Docker Graceful Shutdown . We use a jre as we only need to run Java, so no need for a JDK. We use the OpenJDK distribution from Azul 15 in order to avoid possible violations of Oracle's License 12 . We use Azul's Alpine 13 based image as it is much smaller than those based of full-fledged OS's. This makes the image smaller in size on disk and in memory and reduces the attack vector and recommended by the likes of Docker and Snyk 14 . We download the latest version available which, as of this writing in September 2019, is 4.4.3 which you can retrieve from the OpenDJ Community's releases page on GitHub 13 . FROM azul/zulu-openjdk-alpine:8u222-jre LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" LABEL version = \"0.2.0\" LABEL description = \"OpenDJ container\" WORKDIR /opt EXPOSE 1389 1636 4444 ENV CHANGE_DATE = '20190916-2100' ENV JAVA_HOME /usr/lib/jvm/zulu-8 ENV OPENDJ_JAVA_HOME /usr/lib/jvm/zulu-8 ENV VERSION = 4 .4.3 ENV ROOT_USER_DN = 'cn=admin' ENV ROOT_PASSWORD = 'changeme' RUN apk add --no-cache tini ENTRYPOINT [ \"/sbin/tini\" , \"-vv\" , \"-g\" , \"-s\" , \"--\" ] CMD [ \"/opt/opendj/bin/start-ds\" , \"--nodetach\" ] RUN wget --quiet \\ https://github.com/OpenIdentityPlatform/OpenDJ/releases/download/ $VERSION /opendj- $VERSION .zip && \\ unzip opendj- $VERSION .zip && \\ rm -r opendj- $VERSION .zip RUN /opt/opendj/setup --cli \\ -p 1389 \\ --ldapsPort 1636 \\ --enableStartTLS \\ --generateSelfSignedCertificate \\ --baseDN dc = example,dc = com \\ -h localhost \\ --rootUserDN \" $ROOT_USER_DN \" \\ --rootUserPassword \" $ROOT_PASSWORD \" \\ --acceptLicense \\ --no-prompt \\ --doNotStart ADD Example.ldif /var/tmp/example.ldiff # RUN /opt/opendj/bin/import-ldif --help RUN /opt/opendj/bin/import-ldif --includeBranch dc = example,dc = com --backendID userRoot --offline --ldifFile /var/tmp/example.ldiff","title":"Dockerfile"},{"location":"blogs/kubernetes-sso-keycloak/#kubernetes-deployment","text":"In production you might want to run LDAP in a StatefulSet and give it some permanent storage. But in this guide the goal of LDAP is to show how to use it with Keycloak and we stick to a Deployment as it is easier. apiVersion : apps/v1 kind : Deployment metadata : name : opendj4 labels : app : opendj4 spec : replicas : 1 selector : matchLabels : app : opendj4 template : metadata : labels : app : opendj4 spec : containers : - name : opendj4 image : caladreas/opendj:4.4.3-1 ports : - containerPort : 1389 name : ldap resources : requests : memory : \"250Mi\" cpu : \"50m\" limits : memory : \"500Mi\" cpu : \"250m\"","title":"Kubernetes Deployment"},{"location":"blogs/kubernetes-sso-keycloak/#kubernetes-service","text":"When we use a Deployment our container instance will have a generated name and a new ip address on every (re-)start. So we use a Service to create a stable endpoint, which means that we will now access our LDAP server via our service: ldap://opendj4:389 . apiVersion : v1 kind : Service metadata : labels : app : opendj4 name : opendj4 spec : ports : - name : http port : 389 targetPort : 1389 protocol : TCP selector : app : opendj4 Caution If you deploy the Service and Deployment in a different namespace than where you want to access them from, you will have to add the namespace to the access url. If you've configured them in namespace ldap , the access url becomes ldap://opendj4.ldap:389 .","title":"Kubernetes Service"},{"location":"blogs/kubernetes-sso-keycloak/#homegrown-helm-chart-with-github","text":"You can also package the above explained Docker Image + Kubernetes Yaml definition as a Helm package in a GitHub repository 16 . So if you do not want to use any of the above, feel free to use my personal Helm Repository 17 . helm repo add joostvdg https://raw.githubusercontent.com/joostvdg/helm-repo/master/ helm repo update helm install joostvdg/opendj4 --name ldap --namespace ldap The service, retrieved by kubectl get service now has a different name. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ldap-opendj4 ClusterIP 10 .100.19.29 <none> 389 /TCP 105s So now we access the LDAP server via ldap://ldap-opendj4:389 .","title":"Homegrown Helm chart with GitHub"},{"location":"blogs/kubernetes-sso-keycloak/#keycloak","text":"","title":"Keycloak"},{"location":"blogs/kubernetes-sso-keycloak/#pre-requisites","text":"Helm installed TLS certificate Using a tool as Keycloak to do SSO well, feels wrong without using TLS certificates. So I wholeheartedly recommend configuring Keycloak with a Domain name and TLS Certificate. For the TLS certificate, you use Let's Encrypt 20 with Cert Manager 19 , if you unsure how to proceed in Kubernetes read my guide on Let's Encrypt on Kubernetes .","title":"Pre-requisites"},{"location":"blogs/kubernetes-sso-keycloak/#install-via-helm","text":"helm repo add codecentric https://codecentric.github.io/helm-charts helm repo update kubectl apply -f keycloak-certificate.yaml helm install --name keycloak codecentric/keycloak -f keycloak-values.yaml Note Make sure you replace the dns name keycloak.my.domain.com with your own domain. If you do not have a domain, you can use `nip.io 18 . keycloak-certificate.yaml apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : keycloak.my.domain.com spec : secretName : tls-keycloak dnsNames : - keycloak.my.domain.com acme : config : - http01 : ingressClass : nginx domains : - keycloak.my.domain.com issuerRef : name : letsencrypt-prod kind : ClusterIssuer keycloak-values.yaml keycloak : password : notsosecret ingress : enabled : true path : / annotations : kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : \"true\" ingress.kubernetes.io/affinity : cookie hosts : - keycloak.my.domain.com tls : - hosts : - keycloak.my.domain.com secretName : tls-keycloak","title":"Install Via Helm"},{"location":"blogs/kubernetes-sso-keycloak/#ldap-as-user-federation","text":"Assuming you have Keycloak running now, login with the admin user, keycloak , and the password you set in the keycloak-values.yaml . In the left hand menu, you can select User Federation , this is where we can add ldap and kerberos providers. As Keycloak supports multiple sources, these will be listed by their priority (the smaller the number, the higher). The federations are consulted in the order according to their priority.","title":"LDAP As User Federation"},{"location":"blogs/kubernetes-sso-keycloak/#create-ldap","text":"Let's create the new LDAP provider, select the Add provider dropdown - top right - and choose ldap . You will now see a whole list of values to fill in, don't worry, many can be kept as default. And to be sure, I will list them all here. For more information, each field has a little question mark, hover over it. Info When you've filled in the Connection URL , you should test the configuration with the Test connection button. The same is true for when you've configured the fields Users DN , Bind Type , Bind DN , Bind Credential with the button Test authentication . Console Display Name : display name, should be a name that tells you what this provider is Priority : the priority of this provider, will be used for the order that the federation is accessed Import Users : wether or not to import the users, I just leave this to On as to cache the users Import Users : READ_ONLY , in this guide we assume the LDAP server is not under your control, so read only Vendor : Other Username LDAP attribute : uid RDN LDAP attribute : uid UUID LDAP attribute : entryUUID User Object Classes : inetOrgPerson, organizationalPerson Connection URL : ldap://opendj4:389 Users DN : dc=example,dc=com Bind Type : simple Bind DN : uid=idm, ou=Administrators,dc=example,dc=com Bind Credential : secret -> or what ever you've set it in the example.diff Custom User LDAP Filter : `` Search Scope : Subtree Validate Password Policy : OFF Use Truststore SPI : Only for ldap Connection Pooling : ON Connection Timeout : `` Read Timeout : `` Pagination : ON Sync Settings: Cache Policy : 1000 Periodic Full Sync : On Full Sync Period : 604800 Periodic Changed Users Sync : On Changed Users Sync Period : 86400 Cache Settings: Cache Policy : DEFAULT","title":"Create LDAP"},{"location":"blogs/kubernetes-sso-keycloak/#add-group-mapping","text":"If the value is not mentioned, the default value should be fine. Some values listed here are default, but listed all the same. Name : groups Mapper Type : group-ldap-mapper LDAP Groups DN : ou=Groups, dc=example,dc=com Group Name LDAP Attribute : cn Group Object Classes : groupOfUniqueNames Membership LDAP Attribute : uniquemember User Groups Retrieve Strategy : LOAD_GROUPS_BY_MEMBER_ATTRIBUTE When you hit Save , you can synchronize the groups to Keycloak - if you don't need to, it will confirm the configuration works. Hit the Sync LDAP Groups To Keycloak button, and on top there should be a temporary banner stating how many groups were synchronized (if all categories are 0 , something is wrong). Note Everytime you change a value, you first have to save the page before you can synchronize again.","title":"Add Group Mapping"},{"location":"blogs/kubernetes-sso-keycloak/#mix-ldap-users-with-other-sources","text":"One of the reasons for this guide to exist, is to be able to encapsulate an LDAP over which you have no control and add additional accounts and groups. There's multiple ways forward here, you can use Identity Providers and User Federation to create more sources of user accounts. Perhaps the simplest way is to manage these extra accounts in Keycloak itself. It has its own User database and Groups database. In addition to that, it allows you to assign users created in Keycloak to be a member of a group derived from LDAP - if you've synched them.","title":"Mix LDAP Users With Other Sources"},{"location":"blogs/kubernetes-sso-keycloak/#ui","text":"We can use the UI in Keycloak to manage Users and Groups. We initiate this by going to the Users view and hit Add user . We can then fill in all the details of the User.","title":"UI"},{"location":"blogs/kubernetes-sso-keycloak/#rest-api","text":"Keycloak has a rich REST API with good decent documentation 21 . The thing missing is some examples for how to use them correctly. I recommend using HTTPie 28 rather than cUrl, as it is easier to use for these more complex calls.","title":"REST API"},{"location":"blogs/kubernetes-sso-keycloak/#get-token","text":"Warning The Bearer Token is only valid for a short period of time. If you wait too long, you will get 401 unauthorized . The default REALM is master . httpie http --form POST \" ${ KEYCLOAK_URL } /auth/realms/ ${ REALM } /protocol/openid-connect/token\" username = \"keycloak\" password = \" ${ PASS } \" client_id = \"admin-cli\" grant_type = \"password\" curl curl ${ KEYCLOAK_URL } /auth/realms/ ${ REALM } /protocol/openid-connect/token -u keycloak: ${ PASS }","title":"Get Token"},{"location":"blogs/kubernetes-sso-keycloak/#get-users","text":"httpie http \" ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users\" \"Authorization: Bearer $TOKEN \" httpie - get user http \" ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users/ ${ userId } \" \"Authorization: Bearer $TOKEN \" curl curl -v ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users -H \"Authorization: Bearer $TOKEN \" | jq","title":"Get Users"},{"location":"blogs/kubernetes-sso-keycloak/#create-user","text":"httpie http POST \" ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users\" \\ \"Authorization: Bearer $TOKEN \" \\ credentials: = \"[{\\\"value\\\" : \\\"mypass\\\", \\\"type\\\": \\\"password\\\" }]\" \\ email = \"user@example.com\" \\ firstName = \"hannibal\" \\ lastName = \"lecter\" \\ username = \"hlecter\" \\ groups: = '[\"Robots\"]' \\ emailVerified: = true \\ enabled: = true curl curl -v ${ KEYCLOAK_URL } /auth/admin/realms/ ${ REALM } /users -H \"Authorization: Bearer $TOKEN \" | jq","title":"Create User"},{"location":"blogs/kubernetes-sso-keycloak/#verify","text":"To verify everything is working as it should, you can go to the Groups and Users pages within the Manage menu (left hand side). With Groups you have View all groups , which should have the groups from LDAP and any group you have created within Keycloak - if not yet, you can do so here as well. To view details of a Group, double click the name - it's not obvious you can do so - and you will go to the details page. Here you can also see the members of the Group. You can do the same with the Users menu item. By default the page is empty, if your list of users is not too large, click on View all users . You can view the details page of a user by clicking the link of the User ID. Within the Groups tab you can add the user to more groups, this should contain all the groups known to Keycloak - both Keycloak internal and from LDAP.","title":"Verify"},{"location":"blogs/kubernetes-sso-keycloak/#sso-with-jenkins","text":"To configure Jenkins to use Keycloak we have two plugins at our disposal, OpenId Connect( oic-auth ) 22 and Keycloak 23 . While the Keycloak plugin is easier to configure for authentication, I found it difficult to configure groups. As I feel the group management is mandatory we're going with the OpenId Connect plugin. You can install plugins in Jenkins via the UI 24 , via the new Jenkins CLI 25 , or via the values.yaml when installing via the Helm Chart 27 . Important Install the OpenId Connect plugin before configuring the next parts, and restart your Jenkins instance for best results. In order to configure OpenId Connect with Jenkins, it is the easiest to use the well-known endpoint url. This endpoint contains all the configuration information the plugin needs to configure itself. Usually, this is ${KEYCLOAK_URL}/auth/realms/${REALM}/.well-known/openid-configuration . From Keycloak's perspective we have to register Jenkins as a Client.","title":"SSO with Jenkins"},{"location":"blogs/kubernetes-sso-keycloak/#keycloak-client","text":"We go to the Clients screen in Keycloak and hit the Create button. In the next screen, we need to supply three values: Client ID : the name of your client, jenkins would be good example Client Protocol : openid-connect is recommended Root URL : the main url of your installation, for example https://jenkins.my.domain.com","title":"Keycloak Client"},{"location":"blogs/kubernetes-sso-keycloak/#main-settings","text":"Once we hit save, we get a details view. We have to change some values here. Access Type : we need a Client ID and Client Secret, we only get this when we select confidential Valid Redirect URIs : confirm this is ${yourJenkinsURL}/* Once you hit save, you get a new Tab in the Details screen of this Client. It is called Credentials , and here you can see the Client Secret which we will need to enter in Jenkins.","title":"Main Settings"},{"location":"blogs/kubernetes-sso-keycloak/#group-mappings","text":"Just as we had to add the Group mapping to the LDAP configuration, we will need to configure the Group mapping in the Client. If we go into the Client details, we see there's a Tab called Mappers . We create a new Mapping here by hitting the Create button. Give the mapping a name and then select the Mapper Type Group Membership . The Token Claim Name is important as well, we use this in our Jenkins configuration, give it a descriptive name such as group-membership .","title":"Group Mappings"},{"location":"blogs/kubernetes-sso-keycloak/#configure-via-ui","text":"In Jenkins we go to Manage Jenkins -> Configure Global Security and here we select Login with Openid Connect in the Security Realm block. Client id : the client we've configured in Keycloak, if you've followed this guide, it should be jenkins Client secret : the secret of the client configured in Keycloak, if you've lost it, go back to Keycloak -> Clients -> jenkins -> Credentials and copy the value in the field Secret . We now get a Configuration mode block where we can select either Automatic configuration on Manual configuration . We select Automatic and enter our Well-known configuration endpoint URL from Keycloak we've written down earlier. If you don't remember, the format is usually this: ${KEYCLOAK_URL}/auth/realms/${REALM}/.well-known/openid-configuration . User name field name : preferred_username Full name field name : name Email field name : email Groups field name : group-membership Note The field Groups field name refers back to the Token Claim Name we configured in Keycloak within our Client's Mapping for Group Membership.","title":"Configure Via UI"},{"location":"blogs/kubernetes-sso-keycloak/#configure-via-configuration-as-code","text":"We can use the amazing Jenkins Configuration-as-Code 26 to make sure our SSO configuration is configured out-of-the-box! In order to avoid the having to store the Client ID and Client Secret, we're going to create these as Kubernetes Secrets first. There's more ways to create these, but this is to keep it simple. kubectl create secret generic oic-auth \\ --from-literal = clientID = \" ${ CLIENT_ID } \" \\ --from-literal = clientSecret = \" ${ CLIENT_SECRET } \" \\ --from-literal = keycloakUrl = ${ keycloakUrl } \\ --namespace jenkins JCASC-config.yaml jenkins : securityRealm : oic : clientId : \"${clientID}\" clientSecret : \"${clientSecret}\" wellKnownOpenIDConfigurationUrl : \"${keycloakUrl}/auth/realms/master/.well-known/openid-configuration\" tokenServerUrl : \"${keycloakUrl}auth/realms/master/protocol/openid-connect/token\" authorizationServerUrl : \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/auth\" userInfoServerUrl : \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/userinfo\" userNameField : \"preferred_username\" fullNameFieldName : \"name\" emailFieldName : \"email\" groupsFieldName : \"group-membership\" scopes : \"web-origins address phone openid offline_access profile roles microprofile-jwt email\" disableSslVerification : false logoutFromOpenidProvider : true endSessionUrl : \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/logout\" postLogoutRedirectUrl : \"\" escapeHatchEnabled : false escapeHatchSecret : \"\" complete-jenkins-helm-values.yaml master : ingress : enabled : true hostName : jenkins.my.doamain.com csrf : defaultCrumbIssuer : enabled : true proxyCompatability : true cli : false installPlugins : - kubernetes:latest - kubernetes-credentials:latest - workflow-aggregator:latest - workflow-job:latest - credentials-binding:latest - git:latest - blueocean:latest - prometheus:latest - matrix-auth:latest - keycloak:latest - oic-auth:latest JCasC : enabled : true pluginVersion : \"1.30\" configScripts : welcome-message : | jenkins: systemMessage: Welcome, this Jenkins is configured and managed as code. ldap-settings : | jenkins: securityRealm: oic: clientId: \"${clientID}\" clientSecret: \"${clientSecret}\" wellKnownOpenIDConfigurationUrl: \"${keycloakUrl}/auth/realms/master/.well-known/openid-configuration\" tokenServerUrl: \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/token\" authorizationServerUrl: \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/auth\" userInfoServerUrl: \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/userinfo\" userNameField: \"preferred_username\" fullNameFieldName: \"name\" emailFieldName: \"email\" groupsFieldName: \"group-membership\" scopes: \"web-origins address phone openid offline_access profile roles microprofile-jwt email\" disableSslVerification: false logoutFromOpenidProvider: true endSessionUrl: \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/logout\" postLogoutRedirectUrl: \"\" escapeHatchEnabled: false escapeHatchSecret: \"\" matrix-auth : | jenkins: authorizationStrategy: globalMatrix: permissions: - \"Overall/Read:authenticated\" - \"Overall/Administer:barbossa\" - \"Overall/Administer:Catmins\" - \"Overall/Administer:Robots\" - \"Overall/Administer:/Catmins\" - \"Overall/Administer:/Robots\" persistence : enabled : true volumes : - name : oic-auth-clientid secret : secretName : oic-auth items : - key : clientID path : clientID - name : oic-auth-clientsecret secret : secretName : oic-auth items : - key : clientSecret path : clientSecret - name : oic-auth-keycloakurl secret : secretName : oic-auth items : - key : keycloakUrl path : keycloakUrl mounts : - name : oic-auth-clientid mountPath : /run/secrets/clientID subPath : clientID - name : oic-auth-clientsecret mountPath : /run/secrets/clientSecret subPath : clientSecret - name : oic-auth-keycloakurl mountPath : /run/secrets/keycloakUrl subPath : keycloakUrl","title":"Configure Via Configuration-as-Code"},{"location":"blogs/kubernetes-sso-keycloak/#use-keycloak-groups-in-jenkins","text":"Warning Unfortuantely, there is one caveat about using Keycloak as intermediary between LDAP and Jenkins. Groups do not come across the same as before, they're now prefixed with / . I'm sure it is down to a misconfiguration on my end, so please let me know how to resolve that if you figure it out. This means, that a LDAP group called Catmins will have to be used in Jenkins via /Catmins in Matrix , Project Matrix or other authorization schemes.","title":"Use Keycloak Groups In Jenkins"},{"location":"blogs/kubernetes-sso-keycloak/#verify_1","text":"To verify, take the leap of faith to save the configuration, logout (top right) and then log back in. If everything goes well, you will be redirected to Keycloak, once successfully logged in, redirected back to Jenkins! Note One thing to remark, that if you're logged into Jenkins and you want to see the Groups, you can select your User (top right, click on your name). Alternatively, you can go to ${JENKINS_URL}/whoAmI (notice the capital casing of the 'A' and 'I', it is required).","title":"Verify"},{"location":"blogs/kubernetes-sso-keycloak/#references","text":"Apache Keycloak Home \u21a9 Wikipedia Definition on Single sign-on \u21a9 Kubernetes Home \u21a9 Lightweight Directory Access Protocol (LDAP) \u21a9 Jenkins Home \u21a9 SonarQube Home \u21a9 OpenDJ Community Edition \u21a9 Helm - Package Manager for Kubernetes \u21a9 \u21a9 Helm 3 Beta \u21a9 Common Used LDAP Attributes Explained \u21a9 tini - process manager \u21a9 Overops Article on Java License in Docker images \u21a9 Alpine Docker Image \u21a9 \u21a9 Snyk - 10 docker image security best practices \u21a9 Azul OpenJDK Alpine Image \u21a9 Hosting Helm Private Repository from GitHub \u21a9 Joost van der Griendt's Helm Repository \u21a9 Nip Io - Dead simple wildcard DNS for any IP Address \u21a9 Cert Manager \u21a9 Let's Encrypt - Let's Encrypt the world! \u21a9 Keycloak Admin REST API \u21a9 Jenkins OpenId Connect Plugin \u21a9 Jenkins Keycloak Plugin \u21a9 Jenkins Plugin Management \u21a9 Jenkins JCLI \u21a9 Jenkins Configuration As Code \u21a9 Jenkins Helm Chart \u21a9 HTTPie http commandline interface \u21a9","title":"References"},{"location":"blogs/monitor-jenkins-on-k8s/","text":"Monitor Jenkins on Kubernetes \u00b6 Additional Metrics \u00b6 metrics-diskusage disk-usage Next Steps \u00b6 Replace Node Builds \u00b6 make them a single metric, and calculate builds per label","title":"Monitor Jenkins on Kubernetes"},{"location":"blogs/monitor-jenkins-on-k8s/#monitor-jenkins-on-kubernetes","text":"","title":"Monitor Jenkins on Kubernetes"},{"location":"blogs/monitor-jenkins-on-k8s/#additional-metrics","text":"metrics-diskusage disk-usage","title":"Additional Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/#next-steps","text":"","title":"Next Steps"},{"location":"blogs/monitor-jenkins-on-k8s/#replace-node-builds","text":"make them a single metric, and calculate builds per label","title":"Replace Node Builds"},{"location":"blogs/sso-azure-ad/","text":"Azure AD & CloudBees Core \u00b6 In this article we're going to configure Azure AD as Single-Sign-Solution for CloudBees Core. The configuration rests on three points; 1) Azure AD, 2) Jenkins' SAML plugin, and 3) CloudBees Core's Role Base Access Control or RBAC for short. Important Currently there's a discrepency between the TimeToLive (TTL) of the RefreshToken in Azure AD and the default configuration in the SAML plugin. This creates the problem that you can be logged in, in Azure, but when going to Jenkins you get the Logged Out page. The only way to log back in, is to logout in Azure and back in. The reason is as follows: The max lifetime of the Access Token in Azure AD seems to be 24 hours where the refresh token can live for a maximum of 14 days (if the access token expires the refresh token is used to try to obtain a new access token). The Jenkins setting in Configure Global Security > SAML Identity Provider Settings > Maximum Authentication Lifetime is 24 hours (86400 in seconds) upping this to 1209600 (which is 14 days in seconds/the max lifetime of the Refresh Token). The recommended resolution is to set Maximum Authentication Lifetime to the same value of your RefreshToken TTL. If you haven't changed this in Azure AD, it is 1209600 . Manage Jenkins -> Configure Global Security > SAML Identity Provider Settings > Maximum Authentication Lifetime = 1209600 Prerequisites \u00b6 Before we start, there are some requirements. a running CloudBees Core Operations Center instance this instance is accessible via https. if you do not have a valid certificate and you're running on Kubernetes take a look at my cert-manager guide Let's Encrypt can now also work with nip.io addresses active Azure subscription have an Azure subscription Administrator on hand Configure Azure \u00b6 Warning We use https://cloudbees-core.example.com as the example URL for CloudBees Core. Please replace the domain to your actual domain in all the examples! Steps to execute \u00b6 We do the following steps on the Azure side. create the Azure Active Directory create users and groups create App Registration URL: https://cloudbees-core.example.com/cjoc/securityRealm/finishLogin replace example.com with your domain, https is required! update manifest (for groups) change: \"groupMembershipClaims\": null, (usually line 11) to: \"groupMembershipClaims\": \"SecurityGroup\", create SP ID / App ID URI grant admin consent Info If you use the Azure AD plugin, you also create a client secret. Information To Note Down \u00b6 The following information is unique to your installation, so you need to record them as you go along. App ID URI Object ID 's of Users and Groups you want to give rights Federation Metadata Document Endpoint Azure AD -> App Registrations -> -> Endpoints (circular icon on top) you can use either the URL or the document contents make sure the URL contains the Tenant ID of your Azure Active Directory URL example: https://login.microsoftonline.com/${TENANT_ID}/federationmetadata/2007-06/federationmetadata.xml You can find your Tenant ID in Azure Active Directory -> Properties -> Directory ID (different name, same ID) Visual Guide \u00b6 Below is a visual guide with screenshots. Pay attention to these hints in the screenshots. red : this is the main thing orange : this is how we got to the current page/view blue : while you're in this screen, there might be other things you could do Create New App Registration \u00b6 If you have an Azure account, you have an Azure Active Directory (Azure AD) pre-created. This guide assumes you have an Azure AD ready to use. That means the next step is to create an Application Registration. Give the registration a useful name, select who can authenticate and the redirect URL . This URL needs to be configured and MUST be the actual URL of your CloudBees Core installation. Important To have Client Masters as a fallback for login, incase Operations Center is down, you have to add another redirect URL for each Master. Azure AD -> App Registrations -> -> Authentication -> Web -> https://example.com/teams-cat/securityRealm/finishLogin App Registration Data To Write Down \u00b6 Depending on the plugin you're going to use (SAML or Azure AD) you need information from your Azure AD / App Registration. Tentant ID Object ID Client ID Federation Metadata Document you can use the document XML content or the URL Click on the Endpoints button to open the side-bar with the links. App ID \u00b6 We need the App ID - even if the SAML plugin doesn't mention it. Azure generates an APP ID URI for you. You can also use CloudBees Core's URL as the URI. The URI is shown when there is an error, so it recommended to use a value you can recognize. Info App ID must match in both Azure AD (set as App ID URI ) and the SAML plugin (set as Entity ID ) configuration in Jenkins. So write it down. API Permissions \u00b6 Of course, things wouldn't be proper IAM/LDAP/AD without giving some permissions. If we want to retrieve group information and other fields, we need to be able to read the Directory information. You find the Directory information via the Microsoft Graph api button. We select Application Permissions and then check Directory.Read.All . We don't need to write. The Permissions have changed, so we require an Administrator account to consent with the new permissions. Update Manifest \u00b6 As with the permissions, the default Manifest doesn't give us all the information we want. We want the groups so we can configure RBAC, and thus we have to set the groupMembershipsClaims claim attribute. We change the null to \"SecurityGroup\" . Please consult the Microsoft docs (see reference below) for other options. We can download, edit, and upload the manifest file. Alternatively, we can edit inline and hit save on top. Retrieve Group Object ID \u00b6 If we want to assign Azure AD groups to groups or roles in Jenkins' RBAC, we need to use the Object ID 's. Each Group and User has an Object ID , which have a handy Copy this button on the end of the value box! Configure Jenkins \u00b6 We now get to the point where we configure Jenkins. The SAML plugin is opensource, and thus it's configuration can also be used for Jenkins or CloudBees Jenkins Distribution . Steps \u00b6 Here are the steps we perform to configure Azure AD as the Single-Sign-On solution. install the SAML plugin I assume you know how to install plugins, so we skip this if you don't know Read the Managing Plugins Guide configure saml 2.0 in Jenkins setup groups (RBAC) administrators -> admin group browsers -> all other groups Visual Guide \u00b6 Below is a visual guide with screenshots. Please pay attention to the hints in the screenshots. Red : this is the main thing Orange : this is how we got to the current page/view Blue : while you're in this screen, there might be other things you could do Configure Security \u00b6 To go to Jenkins' security configuration, follow this route: login with an Admin user go to the Operations Center Manage Jenkins -> Global Security Configuration Configure RBAC \u00b6 The SAML plugin configuration pollutes the screen with fields. My advice is to enable RBAC first. If you haven't got any groups/roles yet, I recommend using the Typical Initial Setup from the dropdown. The logged-in user is automatically registered as administrator. So if your Azure AD configuration doesn't work, this user can still manage Jenkins. Important Make sure you know the credentials of the current admin user. It will automatically be added to the Administrators group, and it will be your go-to account when you mess up the SAML configuration and you have to reset security. For how to reset the security configuration, see the For When You Mess Up paragraph. Configure SAML \u00b6 Select SAML 2.0 from the Security Realm options. Here we first supply our Federation Metadata Document content or it's URL. Each option - document content or URL - has its own Validate ... button, hit it and confirm it says Success . Info You can leave Displayname empty, which gives you the default naming scheme from Azure AD. I think this is ugly, as it amounts to something like ${EMAIL_ADDRESS}_${AD_DOMAIN}_${AZURE_CORP_DOMAIN} . There are other options, I've settled for givenname , as there isn't a fullname by default, and well, I prefer Joost to a long hard to recognize string. Fields \u00b6 Displayname : http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname Group : http://schemas.microsoft.com/ws/2008/06/identity/claims/groups Username : http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name Email : http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress SP Entity ID : the App ID URI you configured in Azure AD (hidden behind Advanced Configuration ) Configure RBAC Groups \u00b6 Tip Once Azure AD is configured, and it works, you can configure groups for RBAC just as you're used to. Both for classic RBAC and Team Masters. Just make sure you use the Azure AD Object ID 's of the groups to map them. Bonus tip, add every Azure AD group to Browsers , so you can directly map their groups to Team Master roles without problems. XML Config \u00b6 <useSecurity> true </useSecurity> <authorizationStrategy class= \"nectar.plugins.rbac.strategy.RoleMatrixAuthorizationStrategyImpl\" /> <securityRealm class= \"org.jenkinsci.plugins.saml.SamlSecurityRealm\" plugin= \"saml@1.1.2\" > <displayNameAttributeName> http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname </displayNameAttributeName> <groupsAttributeName> http://schemas.microsoft.com/ws/2008/06/identity/claims/groups </groupsAttributeName> <maximumAuthenticationLifetime> 86400 </maximumAuthenticationLifetime> <emailAttributeName> http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress </emailAttributeName> <usernameCaseConversion> none </usernameCaseConversion> <usernameAttributeName> http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name </usernameAttributeName> <binding> urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect </binding> <advancedConfiguration> <forceAuthn> false </forceAuthn> <spEntityId> https://cloudbees-core.kearos.net </spEntityId> </advancedConfiguration> <idpMetadataConfiguration> <xml></xml> <url> https://login.microsoftonline.com/95b46e09-0307-488b-a6fc-1d2717ba9c49/federationmetadata/2007-06/federationmetadata.xml </url> <period> 5 </period> </idpMetadataConfiguration> </securityRealm> <disableRememberMe> false </disableRememberMe> Logout URL \u00b6 Depending on the requirements, you may want to specify a logout url in the SAML configuration to log you completely out of SAML, not just Core. An example https://login.windows.net/<tenant_id_of_your_app>/oauth2/logout?post_logout_redirect_uri=<logout_URL_of_your_app>/logout For When You Mess Up \u00b6 This is the default config for security in CloudBees Core. This file is in ${JENKINS_HOME}/config.xml , the XML tags we want to look at are quite near the top. <useSecurity> true </useSecurity> <authorizationStrategy class= \"hudson.security.FullControlOnceLoggedInAuthorizationStrategy\" > <denyAnonymousReadAccess> true </denyAnonymousReadAccess> </authorizationStrategy> <securityRealm class= \"hudson.security.HudsonPrivateSecurityRealm\" > <disableSignup> true </disableSignup> <enableCaptcha> false </enableCaptcha> </securityRealm> <disableRememberMe> false </disableRememberMe> On CloudBees Core Modern / Kubernetes \u00b6 To rectify a failed configuration, execute the following steps: exec into the cjoc-0 container: kubectl exec -ti cjoc-0 -- bash open config.xml : vi /var/jenkins_home/config.xml replace conflicting lines with the above snippet save the changes exit the container: exit kill the pod: kubectl delete po cjoc-0 Tip For removing a whole line, stay in \"normal\" mode, and press d d (two times the d key). To add the new lines, go into insert mode by pressing the i key. Go back to \"normal\" mode by pressing the esc key. Then, save and quit, by writing: :wq followed by enter . References \u00b6 CloudBees Guide on Azure AD for Core SSO (outdated) SAML Plugin Docs for Azure AD (outdated) Microsoft Doc for Azure AD Tokens Microsoft Doc for Azure AD Optional Tokens Microsoft Doc for Azure AD Custom Tokens Alternative Azure AD Plugin (very new) Info Currently, there is a limitation which requires you to use the Object ID 's which make searching groups and people less than ideal. When the Alternative Azure AD Plugin is approved this may provide a more satisfactory solution.","title":"CloudBees SSO Azure AD"},{"location":"blogs/sso-azure-ad/#azure-ad-cloudbees-core","text":"In this article we're going to configure Azure AD as Single-Sign-Solution for CloudBees Core. The configuration rests on three points; 1) Azure AD, 2) Jenkins' SAML plugin, and 3) CloudBees Core's Role Base Access Control or RBAC for short. Important Currently there's a discrepency between the TimeToLive (TTL) of the RefreshToken in Azure AD and the default configuration in the SAML plugin. This creates the problem that you can be logged in, in Azure, but when going to Jenkins you get the Logged Out page. The only way to log back in, is to logout in Azure and back in. The reason is as follows: The max lifetime of the Access Token in Azure AD seems to be 24 hours where the refresh token can live for a maximum of 14 days (if the access token expires the refresh token is used to try to obtain a new access token). The Jenkins setting in Configure Global Security > SAML Identity Provider Settings > Maximum Authentication Lifetime is 24 hours (86400 in seconds) upping this to 1209600 (which is 14 days in seconds/the max lifetime of the Refresh Token). The recommended resolution is to set Maximum Authentication Lifetime to the same value of your RefreshToken TTL. If you haven't changed this in Azure AD, it is 1209600 . Manage Jenkins -> Configure Global Security > SAML Identity Provider Settings > Maximum Authentication Lifetime = 1209600","title":"Azure AD &amp; CloudBees Core"},{"location":"blogs/sso-azure-ad/#prerequisites","text":"Before we start, there are some requirements. a running CloudBees Core Operations Center instance this instance is accessible via https. if you do not have a valid certificate and you're running on Kubernetes take a look at my cert-manager guide Let's Encrypt can now also work with nip.io addresses active Azure subscription have an Azure subscription Administrator on hand","title":"Prerequisites"},{"location":"blogs/sso-azure-ad/#configure-azure","text":"Warning We use https://cloudbees-core.example.com as the example URL for CloudBees Core. Please replace the domain to your actual domain in all the examples!","title":"Configure Azure"},{"location":"blogs/sso-azure-ad/#steps-to-execute","text":"We do the following steps on the Azure side. create the Azure Active Directory create users and groups create App Registration URL: https://cloudbees-core.example.com/cjoc/securityRealm/finishLogin replace example.com with your domain, https is required! update manifest (for groups) change: \"groupMembershipClaims\": null, (usually line 11) to: \"groupMembershipClaims\": \"SecurityGroup\", create SP ID / App ID URI grant admin consent Info If you use the Azure AD plugin, you also create a client secret.","title":"Steps to execute"},{"location":"blogs/sso-azure-ad/#information-to-note-down","text":"The following information is unique to your installation, so you need to record them as you go along. App ID URI Object ID 's of Users and Groups you want to give rights Federation Metadata Document Endpoint Azure AD -> App Registrations -> -> Endpoints (circular icon on top) you can use either the URL or the document contents make sure the URL contains the Tenant ID of your Azure Active Directory URL example: https://login.microsoftonline.com/${TENANT_ID}/federationmetadata/2007-06/federationmetadata.xml You can find your Tenant ID in Azure Active Directory -> Properties -> Directory ID (different name, same ID)","title":"Information To Note Down"},{"location":"blogs/sso-azure-ad/#visual-guide","text":"Below is a visual guide with screenshots. Pay attention to these hints in the screenshots. red : this is the main thing orange : this is how we got to the current page/view blue : while you're in this screen, there might be other things you could do","title":"Visual Guide"},{"location":"blogs/sso-azure-ad/#create-new-app-registration","text":"If you have an Azure account, you have an Azure Active Directory (Azure AD) pre-created. This guide assumes you have an Azure AD ready to use. That means the next step is to create an Application Registration. Give the registration a useful name, select who can authenticate and the redirect URL . This URL needs to be configured and MUST be the actual URL of your CloudBees Core installation. Important To have Client Masters as a fallback for login, incase Operations Center is down, you have to add another redirect URL for each Master. Azure AD -> App Registrations -> -> Authentication -> Web -> https://example.com/teams-cat/securityRealm/finishLogin","title":"Create New App Registration"},{"location":"blogs/sso-azure-ad/#app-registration-data-to-write-down","text":"Depending on the plugin you're going to use (SAML or Azure AD) you need information from your Azure AD / App Registration. Tentant ID Object ID Client ID Federation Metadata Document you can use the document XML content or the URL Click on the Endpoints button to open the side-bar with the links.","title":"App Registration Data To Write Down"},{"location":"blogs/sso-azure-ad/#app-id","text":"We need the App ID - even if the SAML plugin doesn't mention it. Azure generates an APP ID URI for you. You can also use CloudBees Core's URL as the URI. The URI is shown when there is an error, so it recommended to use a value you can recognize. Info App ID must match in both Azure AD (set as App ID URI ) and the SAML plugin (set as Entity ID ) configuration in Jenkins. So write it down.","title":"App ID"},{"location":"blogs/sso-azure-ad/#api-permissions","text":"Of course, things wouldn't be proper IAM/LDAP/AD without giving some permissions. If we want to retrieve group information and other fields, we need to be able to read the Directory information. You find the Directory information via the Microsoft Graph api button. We select Application Permissions and then check Directory.Read.All . We don't need to write. The Permissions have changed, so we require an Administrator account to consent with the new permissions.","title":"API Permissions"},{"location":"blogs/sso-azure-ad/#update-manifest","text":"As with the permissions, the default Manifest doesn't give us all the information we want. We want the groups so we can configure RBAC, and thus we have to set the groupMembershipsClaims claim attribute. We change the null to \"SecurityGroup\" . Please consult the Microsoft docs (see reference below) for other options. We can download, edit, and upload the manifest file. Alternatively, we can edit inline and hit save on top.","title":"Update Manifest"},{"location":"blogs/sso-azure-ad/#retrieve-group-object-id","text":"If we want to assign Azure AD groups to groups or roles in Jenkins' RBAC, we need to use the Object ID 's. Each Group and User has an Object ID , which have a handy Copy this button on the end of the value box!","title":"Retrieve Group Object ID"},{"location":"blogs/sso-azure-ad/#configure-jenkins","text":"We now get to the point where we configure Jenkins. The SAML plugin is opensource, and thus it's configuration can also be used for Jenkins or CloudBees Jenkins Distribution .","title":"Configure Jenkins"},{"location":"blogs/sso-azure-ad/#steps","text":"Here are the steps we perform to configure Azure AD as the Single-Sign-On solution. install the SAML plugin I assume you know how to install plugins, so we skip this if you don't know Read the Managing Plugins Guide configure saml 2.0 in Jenkins setup groups (RBAC) administrators -> admin group browsers -> all other groups","title":"Steps"},{"location":"blogs/sso-azure-ad/#visual-guide_1","text":"Below is a visual guide with screenshots. Please pay attention to the hints in the screenshots. Red : this is the main thing Orange : this is how we got to the current page/view Blue : while you're in this screen, there might be other things you could do","title":"Visual Guide"},{"location":"blogs/sso-azure-ad/#configure-security","text":"To go to Jenkins' security configuration, follow this route: login with an Admin user go to the Operations Center Manage Jenkins -> Global Security Configuration","title":"Configure Security"},{"location":"blogs/sso-azure-ad/#configure-rbac","text":"The SAML plugin configuration pollutes the screen with fields. My advice is to enable RBAC first. If you haven't got any groups/roles yet, I recommend using the Typical Initial Setup from the dropdown. The logged-in user is automatically registered as administrator. So if your Azure AD configuration doesn't work, this user can still manage Jenkins. Important Make sure you know the credentials of the current admin user. It will automatically be added to the Administrators group, and it will be your go-to account when you mess up the SAML configuration and you have to reset security. For how to reset the security configuration, see the For When You Mess Up paragraph.","title":"Configure RBAC"},{"location":"blogs/sso-azure-ad/#configure-saml","text":"Select SAML 2.0 from the Security Realm options. Here we first supply our Federation Metadata Document content or it's URL. Each option - document content or URL - has its own Validate ... button, hit it and confirm it says Success . Info You can leave Displayname empty, which gives you the default naming scheme from Azure AD. I think this is ugly, as it amounts to something like ${EMAIL_ADDRESS}_${AD_DOMAIN}_${AZURE_CORP_DOMAIN} . There are other options, I've settled for givenname , as there isn't a fullname by default, and well, I prefer Joost to a long hard to recognize string.","title":"Configure SAML"},{"location":"blogs/sso-azure-ad/#fields","text":"Displayname : http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname Group : http://schemas.microsoft.com/ws/2008/06/identity/claims/groups Username : http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name Email : http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress SP Entity ID : the App ID URI you configured in Azure AD (hidden behind Advanced Configuration )","title":"Fields"},{"location":"blogs/sso-azure-ad/#configure-rbac-groups","text":"Tip Once Azure AD is configured, and it works, you can configure groups for RBAC just as you're used to. Both for classic RBAC and Team Masters. Just make sure you use the Azure AD Object ID 's of the groups to map them. Bonus tip, add every Azure AD group to Browsers , so you can directly map their groups to Team Master roles without problems.","title":"Configure RBAC Groups"},{"location":"blogs/sso-azure-ad/#xml-config","text":"<useSecurity> true </useSecurity> <authorizationStrategy class= \"nectar.plugins.rbac.strategy.RoleMatrixAuthorizationStrategyImpl\" /> <securityRealm class= \"org.jenkinsci.plugins.saml.SamlSecurityRealm\" plugin= \"saml@1.1.2\" > <displayNameAttributeName> http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname </displayNameAttributeName> <groupsAttributeName> http://schemas.microsoft.com/ws/2008/06/identity/claims/groups </groupsAttributeName> <maximumAuthenticationLifetime> 86400 </maximumAuthenticationLifetime> <emailAttributeName> http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress </emailAttributeName> <usernameCaseConversion> none </usernameCaseConversion> <usernameAttributeName> http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name </usernameAttributeName> <binding> urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect </binding> <advancedConfiguration> <forceAuthn> false </forceAuthn> <spEntityId> https://cloudbees-core.kearos.net </spEntityId> </advancedConfiguration> <idpMetadataConfiguration> <xml></xml> <url> https://login.microsoftonline.com/95b46e09-0307-488b-a6fc-1d2717ba9c49/federationmetadata/2007-06/federationmetadata.xml </url> <period> 5 </period> </idpMetadataConfiguration> </securityRealm> <disableRememberMe> false </disableRememberMe>","title":"XML Config"},{"location":"blogs/sso-azure-ad/#logout-url","text":"Depending on the requirements, you may want to specify a logout url in the SAML configuration to log you completely out of SAML, not just Core. An example https://login.windows.net/<tenant_id_of_your_app>/oauth2/logout?post_logout_redirect_uri=<logout_URL_of_your_app>/logout","title":"Logout URL"},{"location":"blogs/sso-azure-ad/#for-when-you-mess-up","text":"This is the default config for security in CloudBees Core. This file is in ${JENKINS_HOME}/config.xml , the XML tags we want to look at are quite near the top. <useSecurity> true </useSecurity> <authorizationStrategy class= \"hudson.security.FullControlOnceLoggedInAuthorizationStrategy\" > <denyAnonymousReadAccess> true </denyAnonymousReadAccess> </authorizationStrategy> <securityRealm class= \"hudson.security.HudsonPrivateSecurityRealm\" > <disableSignup> true </disableSignup> <enableCaptcha> false </enableCaptcha> </securityRealm> <disableRememberMe> false </disableRememberMe>","title":"For When You Mess Up"},{"location":"blogs/sso-azure-ad/#on-cloudbees-core-modern-kubernetes","text":"To rectify a failed configuration, execute the following steps: exec into the cjoc-0 container: kubectl exec -ti cjoc-0 -- bash open config.xml : vi /var/jenkins_home/config.xml replace conflicting lines with the above snippet save the changes exit the container: exit kill the pod: kubectl delete po cjoc-0 Tip For removing a whole line, stay in \"normal\" mode, and press d d (two times the d key). To add the new lines, go into insert mode by pressing the i key. Go back to \"normal\" mode by pressing the esc key. Then, save and quit, by writing: :wq followed by enter .","title":"On CloudBees Core Modern / Kubernetes"},{"location":"blogs/sso-azure-ad/#references","text":"CloudBees Guide on Azure AD for Core SSO (outdated) SAML Plugin Docs for Azure AD (outdated) Microsoft Doc for Azure AD Tokens Microsoft Doc for Azure AD Optional Tokens Microsoft Doc for Azure AD Custom Tokens Alternative Azure AD Plugin (very new) Info Currently, there is a limitation which requires you to use the Object ID 's which make searching groups and people less than ideal. When the Alternative Azure AD Plugin is approved this may provide a more satisfactory solution.","title":"References"},{"location":"blogs/teams-automation/","text":"Core Modern Teams Automation \u00b6 CloudBees Core on Modern (meaning, on Kubernetes) has two main types of Jenkins Masters, a Managed Master, and a Team Master. In this article, we're going to automate the creation and management of the Team Masters. Hint If you do not want to read any of the code here, or just want to take a look at the end result - pipeline wise - you can find working examples on GitHub. Template Repository - creates a new team template and a PR to the GitOps repository GitOps Repository - applies GitOps principles to manage the CloudBees Core Team Masters Goals \u00b6 Just automating the creation of a Team Master is relatively easy, as this can be done via the Client Jar . So we're going to set some additional goals to create a decent challenge. GitOps : I want to be able to create and delete Team Masters by managing configuration in a Git repository Configuration-as-Code : as much of the configuration as possible should be stored in the Git repository Namespace : one of the major reasons for Team Masters to exist is to increase (Product) Team autonomy, which in Kubernetes should correspond to a namespace . So I want each Team Master to be in its own Namespace! Self-Service : the solution should cater to (semi-)autonomous teams and lower the workload of the team managing CloudBees Core. So requesting a Team Master should be doable by everyone Before We Start \u00b6 Some assumptions need to be taken care off before we start. Kubernetes cluster in which you are ClusterAdmin if you don't have one yet, there are guides on this elsewhere on the site your cluster has enough capacity (at least two nodes of 4gb memory) your cluster has CloudBees Core Modern installed if you don't have this yet look at one of the guides on this site or look at the guides on CloudBees.com have administrator access to CloudBees Core Cloud Operations Center Code Examples The more extensive Code Examples, such as Kubernetes Yaml files, are collapsed by default. You can open them by clicking on them. On the right, the code snippet will have a [ ] copy icon. Below is an example. Code Snippet Example Here's a code snippet. pipeline { agent any stages { stage ( 'Hello' ) { steps { echo 'Hello World!' } } } } Bootstrapping \u00b6 All right, so we want to use GitOps and to process the changes we need a Pipeline that can be triggered by a Webhook. I believe in everything as code - except Secrets and such - which includes the Pipeline. Unfortunately, Operations Center cannot run such pipelines. To get over this hurdle, we will create a special Ops Team Master. This Master will be configured to be able to Manage the other Team Masters for us. Log into your Operations Center with a user that has administrative access. Create API Token \u00b6 Create a new API Token for your administrator user by clicking on the user's name - top right corner. Select the Configuration menu on the left and then you should see a section where you can Create a API Token . This Token will disappear, so write it down. Get & Configure Client Jar \u00b6 Replace the values marked by < ... > . The Operations Center URL should look like this: http://cbcore.mydomain.com/cjoc . Setup the connection variables. OC_URL = <your operations center url> USR = <your username> TKN = <api token> Download the Client Jar. curl ${ OC_URL } /jnlpJars/jenkins-cli.jar -o jenkins-cli.jar Create Alias & Test \u00b6 alias cboc = \"java -jar jenkins-cli.jar -noKeyAuth -auth ${ USR } : ${ TKN } -s ${ OC_URL } \" cboc version Create Team Ops \u00b6 As the tasks of the Team Masters for managing Operations are quite specific and demand special rights, I'd recommend putting this in its own namespace . To do so properly, we need to configure a few things. allows Operations Center access to this namespace (so it can create the Team Master) give the ServiceAccount the permissions to create namespace 's for the other Team Masters add config map for the Jenkins Agents temporarily change Operations Center's operating Namespace (where it will spawn resources in) use the CLI to create the team-ops Team Master reset Operations Center's operating Namespace Update & Create Kubernetes Namespaces \u00b6 Create Team Ops Namespace \u00b6 kubectl apply -f team-ops-namespace.yaml team-ops-namespace.yaml This creates the team-ops namespace including all the resources required such as ResourceQuota , ServiceAccount and so on. apiVersion : v1 kind : Namespace metadata : name : team-ops --- apiVersion : v1 kind : ResourceQuota metadata : name : resource-quota namespace : team-ops spec : hard : pods : \"20\" requests.cpu : \"4\" requests.memory : 6Gi limits.cpu : \"5\" limits.memory : 10Gi services.loadbalancers : \"0\" services.nodeports : \"0\" persistentvolumeclaims : \"10\" --- apiVersion : v1 kind : ServiceAccount metadata : name : jenkins namespace : team-ops --- kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : pods-all namespace : team-ops rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/log\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : jenkins namespace : team-ops roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pods-all subjects : - kind : ServiceAccount name : jenkins namespace : team-ops --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : cjoc roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : master-management subjects : - kind : ServiceAccount name : jenkins namespace : team-ops --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : create-namespaces rules : - apiGroups : [ \"*\" ] resources : [ \"serviceaccounts\" , \"rolebindings\" , \"roles\" , \"resourcequotas\" , \"namespaces\" ] verbs : [ \"create\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"configmaps\" , \"rolebindings\" , \"roles\" , \"resourcequotas\" , \"namespaces\" ] verbs : [ \"create\" , \"get\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"events\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"persistentvolumeclaims\" , \"pods\" , \"pods/exec\" , \"services\" , \"statefulsets\" , \"ingresses\" , \"extensions\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/log\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"secrets\" ] verbs : [ \"list\" ] - apiGroups : [ \"apps\" ] resources : [ \"statefulsets\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : ops-namespace roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : create-namespaces subjects : - kind : ServiceAccount name : jenkins namespace : team-ops Update Operation Center ServiceAccount \u00b6 The ServiceAccount under which Operation Center runs, only has rights in it's own namespace . Which means it cannot create our Team Ops Master. Below is the .yaml file for Kubernetes and the command to apply it. Warning I assume you're using the default cloudbees-core as per Cloudbees' documentation. If this is not the case, change the last line, namespace: cloudbees-core with the namespace your Operation Center runs in. kubectl apply -f patch-oc-serviceaccount.yaml -n team-ops patch-oc-serviceaccount.yaml This patches the existing Operation Center's ServiceAccount to also have the correct rights in the team-ops namespace. kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : master-management rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/log\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] - apiGroups : [ \"apps\" ] resources : [ \"statefulsets\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"persistentvolumeclaims\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"secrets\" ] verbs : [ \"list\" ] - apiGroups : [ \"\" ] resources : [ \"events\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : cjoc roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : master-management subjects : - kind : ServiceAccount name : cjoc namespace : cloudbees-core Jenkins Agent ConfigMap \u00b6 kubectl apply -f jenkins-agent-config-map.yaml -n team-ops jenkins-agent-config-map.yaml Creates the Jenkins Agent ConfigMap, which contains the information the Jenkins Agent - within a PodTemplate - uses to connect to the Jenkins Master. apiVersion : v1 kind : ConfigMap metadata : name : jenkins-agent data : jenkins-agent : | #!/usr/bin/env sh # The MIT License # # Copyright (c) 2015, CloudBees, Inc. # # Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files (the \"Software\"), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN # THE SOFTWARE. # Usage jenkins-slave.sh [options] -url http://jenkins [SECRET] [AGENT_NAME] # Optional environment variables : # * JENKINS_TUNNEL : HOST:PORT for a tunnel to route TCP traffic to jenkins host, when jenkins can't be directly accessed over network # * JENKINS_URL : alternate jenkins URL # * JENKINS_SECRET : agent secret, if not set as an argument # * JENKINS_AGENT_NAME : agent name, if not set as an argument if [ $# -eq 1 ]; then # if `docker run` only has one arguments, we assume user is running alternate command like `bash` to inspect the image exec \"$@\" else # if -tunnel is not provided try env vars case \"$@\" in *\"-tunnel \"*) ;; *) if [ ! -z \"$JENKINS_TUNNEL\" ]; then TUNNEL=\"-tunnel $JENKINS_TUNNEL\" fi ;; esac if [ -n \"$JENKINS_URL\" ]; then URL=\"-url $JENKINS_URL\" fi if [ -n \"$JENKINS_NAME\" ]; then JENKINS_AGENT_NAME=\"$JENKINS_NAME\" fi if [ -z \"$JNLP_PROTOCOL_OPTS\" ]; then echo \"Warning: JnlpProtocol3 is disabled by default, use JNLP_PROTOCOL_OPTS to alter the behavior\" JNLP_PROTOCOL_OPTS=\"-Dorg.jenkinsci.remoting.engine.JnlpProtocol3.disabled=true\" fi # If both required options are defined, do not pass the parameters OPT_JENKINS_SECRET=\"\" if [ -n \"$JENKINS_SECRET\" ]; then case \"$@\" in *\"${JENKINS_SECRET}\"*) echo \"Warning: SECRET is defined twice in command-line arguments and the environment variable\" ;; *) OPT_JENKINS_SECRET=\"${JENKINS_SECRET}\" ;; esac fi OPT_JENKINS_AGENT_NAME=\"\" if [ -n \"$JENKINS_AGENT_NAME\" ]; then case \"$@\" in *\"${JENKINS_AGENT_NAME}\"*) echo \"Warning: AGENT_NAME is defined twice in command-line arguments and the environment variable\" ;; *) OPT_JENKINS_AGENT_NAME=\"${JENKINS_AGENT_NAME}\" ;; esac fi SLAVE_JAR=/usr/share/jenkins/slave.jar if [ ! -f \"$SLAVE_JAR\" ]; then tmpfile=$(mktemp) if hash wget > /dev/null 2>&1; then wget -O \"$tmpfile\" \"$JENKINS_URL/jnlpJars/slave.jar\" elif hash curl > /dev/null 2>&1; then curl -o \"$tmpfile\" \"$JENKINS_URL/jnlpJars/slave.jar\" else echo \"Image does not include $SLAVE_JAR and could not find wget or curl to download it\" return 1 fi SLAVE_JAR=$tmpfile fi #TODO: Handle the case when the command-line and Environment variable contain different values. #It is fine it blows up for now since it should lead to an error anyway. exec java $JAVA_OPTS $JNLP_PROTOCOL_OPTS -cp $SLAVE_JAR hudson.remoting.jnlp.Main -headless $TUNNEL $URL $OPT_JENKINS_SECRET $OPT_JENKINS_AGENT_NAME \"$@\" fi Create Initial Master \u00b6 To make it easier to change the namespace if needed, its extracted out from the command. OriginalNamespace = cloudbees-core This script changes the Operations Center's operating namespace , creates a Team Master with the name ops , and then resets the namespace. cboc groovy = < configure-oc-namespace.groovy team-ops cboc teams ops --put < team-ops.json cboc groovy = < configure-oc-namespace.groovy $OriginalNamespace team-ops.json This json file that describes a team. By default there are three roles defined on a team, TEAM_ADMIN , TEAM_MEMBER , and TEAM_GUEST . Don't forget to change the id 's to Group ID's from your Single-Sign-On solution. { \"version\" : \"1\" , \"data\" : { \"name\" : \"ops\" , \"displayName\" : \"Operations\" , \"provisioningRecipe\" : \"basic\" , \"members\" : [{ \"id\" : \"Catmins\" , \"roles\" : [ \"TEAM_ADMIN\" ] }, { \"id\" : \"Pirates\" , \"roles\" : [ \"TEAM_MEMBER\" ] }, { \"id\" : \"Continental\" , \"roles\" : [ \"TEAM_GUEST\" ] } ], \"icon\" : { \"name\" : \"hexagons\" , \"color\" : \"#8d7ec1\" } } } configure-oc-namespace.groovy This is a Jenkins Configuration or System Groovy script. It will change the namespace Operation Center uses to create resources. You can change this in the UI by going to Operations Center -> Manage Jenkins -> System Configuration -> Master Provisioning -> Namespace . import hudson.* import hudson.util.Secret ; import hudson.util.Scrambler ; import hudson.util.FormValidation ; import jenkins.* import jenkins.model.* import hudson.security.* import com.cloudbees.masterprovisioning.kubernetes.KubernetesMasterProvisioning import com.cloudbees.masterprovisioning.kubernetes.KubernetesClusterEndpoint println \"=== KubernetesMasterProvisioning Configuration - start\" println \"== Retrieving main configuration\" def descriptor = Jenkins . getInstance (). getInjector (). getInstance ( KubernetesMasterProvisioning . DescriptorImpl . class ) def namespace = this . args [ 0 ] def currentKubernetesClusterEndpoint = descriptor . getClusterEndpoints (). get ( 0 ) println \"= Found current endpoint\" println \"= \" + currentKubernetesClusterEndpoint . toString () def id = currentKubernetesClusterEndpoint . getId () def name = currentKubernetesClusterEndpoint . getName () def url = currentKubernetesClusterEndpoint . getUrl () def credentialsId = currentKubernetesClusterEndpoint . getCredentialsId () println \"== Setting Namspace to \" + namespace def updatedKubernetesClusterEndpoint = new KubernetesClusterEndpoint ( id , name , url , credentialsId , namespace ) def clusterEndpoints = new ArrayList < KubernetesClusterEndpoint >() clusterEndpoints . add ( updatedKubernetesClusterEndpoint ) descriptor . setClusterEndpoints ( clusterEndpoints ) println \"== Saving Jenkins configuration\" descriptor . save () println \"=== KubernetesMasterProvisioning Configuration - finish\" Configure Team Ops Master \u00b6 Now that we've created the Operations Team Master (Team Ops), we can configure it. The Pipelines we need will require credentials, we describe them below. githubtoken_token : GitHub API Token only, credentials type Secret Text (for the PR pipeline) githubtoken : GitHub username and API Token jenkins-api : Username and API Token for Operations Center. Just like the one we used for Client Jar. We also need to have a Global Pipeline Library defined by the name github.com/joostvdg/jpl-core . This, as the name suggests, should point to https://github.com/joostvdg/jpl-core.git . Create GitOps Pipeline \u00b6 In total, we need two repositories and two or three Pipelines. You can either use my CLI Docker Image or roll your own. I will proceed as if you will create your own. CLI Image Pipeline : this will create a CLI Docker Image that is used to talk to Operations Center via the Client Jar (CLI) PR Pipeline : I like the idea of Self-Service, but in order to keep things in check, you might want to provide that via a PullRequest (PR) rather than a direct write to the Master branch. This is also Repository One, as I prefer having each pipeline in their own Repository, but you don't need to. Main Pipeline : will trigger on a commit to the Master branch and create the new team. I'll even throw in a free manage your Team Recipes for free as well. Create CLI Image Pipeline \u00b6 In a Kubernetes cluster, you should not build with Docker directly, use an in-cluster builder such as Kaniko or Buildah . You can read more about the why and how elsewhere on this site . Tip If you do not want to create your own, you can re-use my images. There should be one available for every recent version of CloudBees Core that will work with your Operations Center. The images are available in DockerHub at caladreas/cbcore-cli Kaniko Configuration \u00b6 Kaniko uses a Docker Image to build your Docker Image in cluster. It does however need to directly communicate to your Docker Registry. This requires a Kubernetes Secret of type docker-registry . How you can do this and more, you can read on the CloudBees Core Docs . Pipeline \u00b6 Now that you have Kaniko configured, you can use this Pipeline to create your own CLI Images. Caution Make sure you replace the environment variables with values that make sense to you. CJOC_URL internal url in Kubernets, usually http://cjoc.<namespace>/cjoc REGISTRY : index.docker.io = DockerHub REPO : docker repository name IMAGE : docker image name Jenkinsfile Jenkins Declarative Pipeline for the CLI Image geberation. pipeline { agent { kubernetes { //cloud 'kubernetes' label 'test' yaml \"\"\" kind: Pod metadata: name: test spec: containers: - name: curl image: byrnedo/alpine-curl command: - cat tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: jenkins-docker-cfg projected: sources: - secret: name: docker-credentials items: - key: .dockerconfigjson path: .docker/config.json \"\"\" } } environment { CJOC_URL = 'http://cjoc.cloudbees-core/cjoc' CLI_VERSION = '' REGISTRY = 'index.docker.io' REPO = 'caladreas' IMAGE = 'cbcore-cli' } stages { stage ( 'Download CLI' ) { steps { container ( 'curl' ) { sh 'curl --version' sh 'echo ${CJOC_URL}/jnlpJars/jenkins-cli.jar' sh 'curl ${CJOC_URL}/jnlpJars/jenkins-cli.jar --output jenkins-cli.jar' sh 'ls -lath' } } } stage ( 'Prepare' ) { parallel { stage ( 'Verify CLI' ) { environment { CREDS = credentials ( 'jenkins-api' ) CLI = \"java -jar jenkins-cli.jar -noKeyAuth -s ${CJOC_URL} -auth\" } steps { sh 'echo ${CLI}' script { CLI_VERSION = sh returnStdout: true , script: '${CLI} ${CREDS} version' } sh 'echo ${CLI_VERSION}' } } stage ( 'Prepare Dockerfile' ) { steps { writeFile encoding: 'UTF-8' , file: 'Dockerfile' , text: \"\"\"FROM mcr.microsoft.com/java/jre-headless:8u192-zulu-alpine WORKDIR /usr/bin ADD jenkins-cli.jar . RUN pwd RUN ls -lath \"\"\" } } } } stage ( 'Build with Kaniko' ) { environment { PATH = \"/busybox:/kaniko:$PATH\" TAG = \"${CLI_VERSION}\" } steps { sh 'echo image fqn=${REGISTRY}/${REPO}/${IMAGE}:${TAG}' container ( name: 'kaniko' , shell: '/busybox/sh' ) { sh '''#!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:${TAG} /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:latest ''' } } } } } PR Pipeline \u00b6 Caution The PR Pipeline example builds upon the GitHub API, if you're not using GItHub, you will have to figure out another way to make the PR. Tools Used \u00b6 yq : commandline tool for processing Yaml files jq commandline tool for pressing Json files Kustomize templating tool for Kubernetes Yaml, as of Kubernetes 1.13 , this is part of the Client (note, your server can be older, don't worry!) Hub commandline client for GitHub Repository Layout \u00b6 folder: team-master-template with file simple.json folder: namespace-creation with folder: kustomize this contains the Kustomize configuration Simple.json This is a template for the team JSON definition. { \"version\" : \"1\" , \"data\" : { \"name\" : \"NAME\" , \"displayName\" : \"DISPLAY_NAME\" , \"provisioningRecipe\" : \"RECIPE\" , \"members\" : [ { \"id\" : \"ADMINS\" , \"roles\" : [ \"TEAM_ADMIN\" ] }, { \"id\" : \"MEMBERS\" , \"roles\" : [ \"TEAM_MEMBER\" ] }, { \"id\" : \"GUESTS\" , \"roles\" : [ \"TEAM_GUEST\" ] } ], \"icon\" : { \"name\" : \"ICON\" , \"color\" : \"HEX_COLOR\" } } } Kustomize Configuration \u00b6 Kustomize is a tool for template Kubernetes YAML definitions, which is what we need here. However, only for the namespace creation & configuration. So if you don't want to do that, you can skip this. The Kustomize configuration has two parts, a folder called team-example with a kustomization.yaml . This will be what we configure to generate a new yaml definition. The main template is in the folder base , where the entrypoint will be again kustomization.yaml . This time, the kustomization.yaml will link to all the template files we need. As posting all these yaml files again is a bit much, I'll link to my example repo. Feel free to fork it instead: cb-team-gitops-template configmap.yaml : the Jenkins Agent ConfigMap namespace.yaml : the new namespace resource-quota.yaml : resource quota's for the namespace role-binding-cjoc.yaml : a role binding for the CJOC ServiceAccount, so it create create the new Master in the new namespace role-binding.yaml : the role binding for the jenkins ServiceAccount, which allows the new Master to create and manage Pods (for PodTemplates) role-cjoc.yaml : the role for CJOC for the ability to create a Master in the new Namspace role.yaml : the role for the jenkins ServiceAccount for the new Master service-account.yaml : the ServiceAccount, jenkins , used by the new Master Pipeline \u00b6 The Pipeline will do the following: capture input parameters to be used to customize the Team Master update the Kustomize template to make sure every resource is correct for the new namespace ( teams-<name of team> ) execute Kustomize to generate a single yaml file that defines the configuration for the new Team Masters' namespace process the simple.json to generate a team.json file for the new Team Master for use with the Jenkins CLI checkout your GIT_REPO that contains your team definitions create a new PR to your GIT_REPO for the new team Jenkinsfile Variables to update: GIT_REPO : the GitHub repository in which the Team Definitions are stored RESET_NAMESPACE : the namespace Operations Center should use as default pipeline { agent { kubernetes { label 'team-automation' yaml \"\"\" kind: Pod spec: containers: - name: hub image: caladreas/hub command: [\"cat\"] tty: true resources: requests: memory: \"50Mi\" cpu: \"150m\" limits: memory: \"50Mi\" cpu: \"150m\" - name: kubectl image: bitnami/kubectl:latest command: [\"cat\"] tty: true securityContext: runAsUser: 1000 fsGroup: 1000 resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"150Mi\" cpu: \"200m\" - name: yq image: mikefarah/yq command: ['cat'] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" - name: jq image: colstrom/jq command: ['cat'] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" \"\"\" } } libraries { lib ( 'github.com/joostvdg/jpl-core' ) } options { disableConcurrentBuilds () // we don't want more than one at a time checkoutToSubdirectory 'templates' // we need to do two checkouts buildDiscarder logRotator ( artifactDaysToKeepStr: '' , artifactNumToKeepStr: '' , daysToKeepStr: '5' , numToKeepStr: '5' ) // always clean up } environment { envGitInfo = '' RESET_NAMESPACE = 'jx-production' TEAM_BASE_NAME = '' NAMESPACE_TO_CREATE = '' DISPLAY_NAME = '' TEAM_RECIPE = '' ICON = '' ICON_COLOR_CODE = '' ADMINS_ROLE = '' MEMBERS_ROLE = '' GUESTS_ROLE = '' RECORD_LOC = '' GIT_REPO = '' } stages { stage ( 'Team Details' ) { input { message \"Please enter the team details.\" ok \"Looks good, proceed\" parameters { string ( name: 'Name' , defaultValue: 'hex' , description: 'Please specify a team name' ) string ( name: 'DisplayName' , defaultValue: 'Hex' , description: 'Please specify a team display name' ) choice choices: [ 'joostvdg' , 'basic' , 'java-web' ], description: 'Please select a Team Recipe' , name: 'TeamRecipe' choice choices: [ 'anchor' , 'bear' , 'bowler-hat' , 'briefcase' , 'bug' , 'calculator' , 'calculatorcart' , 'clock' , 'cloud' , 'cloudbees' , 'connect' , 'dollar-bill' , 'dollar-symbol' , 'file' , 'flag' , 'flower-carnation' , 'flower-daisy' , 'help' , 'hexagon' , 'high-heels' , 'jenkins' , 'key' , 'marker' , 'monocle' , 'mustache' , 'office' , 'panther' , 'paw-print' , 'teacup' , 'tiger' , 'truck' ], description: 'Please select an Icon' , name: 'Icon' string ( name: 'IconColorCode' , defaultValue: '#CCCCCC' , description: 'Please specify a valid html hexcode for the color (https://htmlcolorcodes.com/)' ) string ( name: 'Admins' , defaultValue: 'Catmins' , description: 'Please specify a groupid or userid for the TEAM_ADMIN role' ) string ( name: 'Members' , defaultValue: 'Pirates' , description: 'Please specify a groupid or userid for the TEAM_MEMBER role' ) string ( name: 'Guests' , defaultValue: 'Continental' , description: 'Please specify a groupid or userid for the TEAM_GUEST role' ) } } steps { println \"Name=${Name}\" println \"DisplayName=${DisplayName}\" println \"TeamRecipe=${TeamRecipe}\" println \"Icon=${Icon}\" println \"IconColorCode=${IconColorCode}\" println \"Admins=${Admins}\" println \"Members=${Members}\" println \"Guests=${Guests}\" script { TEAM_BASE_NAME = \"${Name}\" NAMESPACE_TO_CREATE = \"cb-teams-${Name}\" DISPLAY_NAME = \"${DisplayName}\" TEAM_RECIPE = \"${TeamRecipe}\" ICON = \"${Icon}\" ICON_COLOR_CODE = \"${IconColorCode}\" ADMINS_ROLE = \"${Admins}\" MEMBERS_ROLE = \"${Members}\" GUESTS_ROLE = \"${Guests}\" RECORD_LOC = \"templates/teams/${Name}\" sh \"mkdir -p ${RECORD_LOC}\" } } } stage ( 'Create Team Config' ) { environment { BASE = 'templates/namespace-creation/kustomize' NAMESPACE = \"${NAMESPACE_TO_CREATE}\" RECORD_LOC = \"templates/teams/${TEAM_BASE_NAME}\" } parallel { stage ( 'Namespace' ) { steps { container ( 'yq' ) { sh 'yq w -i ${BASE}/base/role-binding.yaml subjects[0].namespace ${NAMESPACE}' sh 'yq w -i ${BASE}/base/namespace.yaml metadata.name ${NAMESPACE}' sh 'yq w -i ${BASE}/team-example/kustomization.yaml namespace ${NAMESPACE}' } container ( 'kubectl' ) { sh ''' kubectl kustomize ${BASE}/team-example > ${RECORD_LOC}/team.yaml cat ${RECORD_LOC}/team.yaml ''' } } } stage ( 'Team Master JSON' ) { steps { container ( 'jq' ) { sh \"\"\"jq \\ '.data.name = \"${TEAM_BASE_NAME}\" |\\ .data.displayName = \"${DISPLAY_NAME}\" |\\ .data.provisioningRecipe = \"${TEAM_RECIPE}\" |\\ .data.icon.name = \"${ICON}\" |\\ .data.icon.color = \"${ICON_COLOR_CODE}\" |\\ .data.members[0].id = \"${ADMINS_ROLE}\" |\\ .data.members[1].id = \"${MEMBERS_ROLE}\" |\\ .data.members[2].id = \"${GUESTS_ROLE}\"'\\ templates/team-master-template/simple.json > ${RECORD_LOC}/team.json \"\"\" } sh 'cat ${RECORD_LOC}/team.json' } } } } stage ( 'Create PR' ) { when { branch 'master' } environment { RECORD_OLD_LOC = \"templates/teams/${TEAM_BASE_NAME}\" RECORD_LOC = \"teams/${TEAM_BASE_NAME}\" PR_CHANGE_NAME = \"add_team_${TEAM_BASE_NAME}\" } steps { container ( 'hub' ) { dir ( 'cb-team-gitops' ) { script { envGitInfo = git \"${GIT_REPO}\" } sh 'git checkout -b ${PR_CHANGE_NAME}' sh 'ls -lath ../${RECORD_OLD_LOC}' sh 'cp -R ../${RECORD_OLD_LOC} ./teams' sh 'ls -lath' sh 'ls -lath teams/' gitRemoteConfigByUrl ( envGitInfo . GIT_URL , 'githubtoken_token' ) // must be a API Token ONLY -> secret text sh ''' git config --global user.email \"jenkins@jenkins.io\" git config --global user.name \"Jenkins\" git add ${RECORD_LOC} git status git commit -m \"add team ${TEAM_BASE_NAME}\" git push origin ${PR_CHANGE_NAME} ''' // has to be indented like that, else the indents will be in the pr description writeFile encoding: 'UTF-8' , file: 'pr-info.md' , text: \"\"\"Add ${TEAM_BASE_NAME} \\n This pr is automatically generated via CloudBees.\\\\n \\n The job: ${env.JOB_URL} \"\"\" // TODO: unfortunately, environment {}'s credentials have fixed environment variable names // TODO: in this case, they need to be EXACTLY GITHUB_PASSWORD and GITHUB_USER script { withCredentials ([ usernamePassword ( credentialsId: 'githubtoken' , passwordVariable: 'GITHUB_PASSWORD' , usernameVariable: 'GITHUB_USER' )]) { sh \"\"\" set +x hub pull-request --force -F pr-info.md -l '${TEAM_BASE_NAME}' --no-edit \"\"\" } } } } } } } } Main Pipeline \u00b6 The main Pipeline should be part of a repository. The Repository should look like this: recipes (folder) recipes.json -> current complete list of CloudBees Core Team Recipes definition teams (folder) folder per team team.json -> CloudBees Core Team definition team.yaml -> Kubernetes YAML definition of the namespace and all its resources Process \u00b6 The pipeline can be a bit hard to grasp, so let me break it down into individual steps. We have the following stages: Create Team - which is broken into sub-stages via the sequential stages feature . * Parse Changelog * Create Namespace * Change OC Namespace * Create Team Master Test CLI Connection Update Team Recipes Notable Statements \u00b6 disableConcurrentBuilds We change the namespace of Operation Center to a different value only for the duration of creating this master. This is something that should probably be part of the Team Master creation, but as it is a single configuration option for all that Operation Center does, we need to be careful. By ensuring we only run one build concurrently, we reduce the risk of this blowing up in our face. options { disableConcurrentBuilds () } when { } The When Directive allows us to creating effective conditions for when a stage should be executed. The snippet below shows the use of a combination of both the branch and changeset built-in filters. changeset looks at the commit being build and validates that there was a change in that file path. when { allOf { branch 'master' ; changeset \"teams/**/team.*\" } } post { always { } } The Post Directive allows us to run certain commands after the main pipeline has run depending on the outcome (compared or not to the previous outcome). In this case, we want to make sure we reset the namespace used by Operations Center to the original value. By using post { always {} } , it will ALWAYS run, regardless of the status of the pipeline. So we should be safe. post { always { container ( 'cli' ) { sh '${CLI} ${CREDS} groovy = < resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE}' } } } stages { stage { parallel { stage() { stages { stage { Oke, you might've noticed this massive indenting depth and probably have some questions. By combining sequential stages with parallel stages we can create a set of stages that will be executed in sequence but can be controlled by a single when {} statement whether or not they get executed. This prevents mistakes being made in the condition and accidentally running one or other but not all the required steps. stages { stage ( 'Create Team' ) { parallel { stage ( 'Main' ) { stages { stage ( 'Parse Changelog' ) { changetSetData & container('jpb') {} Alright, so even if we know a team was added in /teams/<team-name> , we still don't know the following two things: 1) what is the name of this team, 2) was this team changed or deleted? So we have to process the changelog to be able to answer these questions as well. There are different ways of getting the changelog and parsing it. I've written one you can do on ANY machine, regardless of Jenkins by leveraging Git and my own custom binary ( jpb -> Jenkins Pipeline Binary). The code for my binary is at GitHub: github.com/joostvdg/jpb . An alternative approach is described by CloudBees Support here , which leverages Jenkins groovy powers. COMMIT_INFO = \"${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT}\" def changeSetData = sh returnStdout: true , script: \"git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO}\" changeSetData = changeSetData . replace ( \"\\n\" , \"\\\\n\" ) container ( 'jpb' ) { changeSetFolders = sh returnStdout: true , script: \"/usr/bin/jpb/bin/jpb GitChangeListToFolder '${changeSetData}' 'teams/'\" changeSetFolders = changeSetFolders . split ( ',' ) } Files \u00b6 recipes.json The default Team Recipes that ships with CloudBees Core Modern. { \"version\" : \"1\" , \"data\" : [{ \"name\" : \"basic\" , \"displayName\" : \"Basic\" , \"description\" : \"The minimalistic setup.\" , \"plugins\" : [ \"bluesteel-master\" , \"cloudbees-folders-plus\" , \"cloudbees-jsync-archiver\" , \"cloudbees-monitoring\" , \"cloudbees-nodes-plus\" , \"cloudbees-ssh-slaves\" , \"cloudbees-support\" , \"cloudbees-workflow-template\" , \"credentials-binding\" , \"email-ext\" , \"git\" , \"git-client\" , \"github-branch-source\" , \"github-organization-folder\" , \"infradna-backup\" , \"ldap\" , \"mailer\" , \"operations-center-analytics-reporter\" , \"operations-center-cloud\" , \"pipeline-model-definition\" , \"ssh-credentials\" , \"wikitext\" , \"workflow-aggregator\" , \"workflow-cps-checkpoint\" ], \"default\" : true }, { \"name\" : \"java-web\" , \"displayName\" : \"Java & Web Development\" , \"description\" : \"The essential tools to build, release and deploy Java Web applications including integration with Maven, Gradle and Node JS.\" , \"plugins\" : [ \"bluesteel-master\" , \"cloudbees-folders-plus\" , \"cloudbees-jsync-archiver\" , \"cloudbees-monitoring\" , \"cloudbees-nodes-plus\" , \"cloudbees-ssh-slaves\" , \"cloudbees-support\" , \"cloudbees-workflow-template\" , \"credentials-binding\" , \"email-ext\" , \"git\" , \"git-client\" , \"github-branch-source\" , \"github-organization-folder\" , \"infradna-backup\" , \"ldap\" , \"mailer\" , \"operations-center-analytics-reporter\" , \"operations-center-cloud\" , \"pipeline-model-definition\" , \"ssh-credentials\" , \"wikitext\" , \"workflow-aggregator\" , \"workflow-cps-checkpoint\" , \"config-file-provider\" , \"cloudbees-aws-cli\" , \"cloudbees-cloudfoundry-cli\" , \"findbugs\" , \"gradle\" , \"jira\" , \"junit\" , \"nodejs\" , \"openshift-cli\" , \"pipeline-maven\" , \"tasks\" , \"warnings\" ], \"default\" : false }] } Jenkinsfile This is the pipeline that will process the commit to the repository and, if it detects a new team is created will apply the changes. Variables to overwrite: GIT_REPO : the https url to the Git Repository your GitOps code/configuration is stored RESET_NAMESPACE : the namespace your Operation Center normally operates in CLI : this command depends on the namespace Operation Center is in ( http://<service name>.<namespace>/cjoc ) pipeline { agent { kubernetes { label 'jenkins-agent' yaml ''' apiVersion: v1 kind: Pod spec: serviceAccountName: jenkins containers: - name: cli image: caladreas/cbcore-cli:2.176.2.3 imagePullPolicy: Always command: - cat tty: true resources: requests: memory: \"50Mi\" cpu: \"150m\" limits: memory: \"50Mi\" cpu: \"150m\" - name: kubectl image: bitnami/kubectl:latest command: [\"cat\"] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"150Mi\" cpu: \"200m\" - name: yq image: mikefarah/yq command: ['cat'] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" - name: jpb image: caladreas/jpb command: - cat tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" securityContext: runAsUser: 1000 fsGroup: 1000 ''' } } options { disableConcurrentBuilds () buildDiscarder logRotator ( artifactDaysToKeepStr: '' , artifactNumToKeepStr: '' , daysToKeepStr: '5' , numToKeepStr: '5' ) } environment { RESET_NAMESPACE = 'cloudbees-core' CREDS = credentials ( 'jenkins-api' ) CLI = \"java -jar /usr/bin/jenkins-cli.jar -noKeyAuth -s http://cjoc.cloudbees-core/cjoc -auth\" COMMIT_INFO = '' TEAM = '' GIT_REPO = '' } stages { stage ( 'Create Team' ) { when { allOf { branch 'master' ; changeset \"teams/**/team.*\" } } parallel { stage ( 'Main' ) { stages { stage ( 'Parse Changelog' ) { steps { // Alternative approach: https://support.cloudbees.com/hc/en-us/articles/217630098-How-to-access-Changelogs-in-a-Pipeline-Job- // However, that runs on the master, JPB runs in an agent! script { scmVars = git \"${GIT_REPO}\" COMMIT_INFO = \"${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT}\" def changeSetData = sh returnStdout: true , script: \"git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO}\" changeSetData = changeSetData . replace ( \"\\n\" , \"\\\\n\" ) container ( 'jpb' ) { changeSetFolders = sh returnStdout: true , script: \"/usr/bin/jpb/bin/jpb GitChangeListToFolder '${changeSetData}' 'teams/'\" changeSetFolders = changeSetFolders . split ( ',' ) } if ( changeSetFolders . length > 0 ) { TEAM = changeSetFolders [ 0 ] TEAM = TEAM . trim () // to protect against a team being removed def exists = fileExists \"teams/${TEAM}/team.yaml\" if (! exists ) { TEAM = '' } } else { TEAM = '' } echo \"Team that changed: |${TEAM}|\" } } } stage ( 'Create Namespace' ) { when { expression { return ! TEAM . equals ( '' ) } } environment { NAMESPACE = \"cb-teams-${TEAM}\" RECORD_LOC = \"teams/${TEAM}\" } steps { container ( 'kubectl' ) { sh ''' cat ${RECORD_LOC}/team.yaml kubectl apply -f ${RECORD_LOC}/team.yaml ''' } } } stage ( 'Change OC Namespace' ) { when { expression { return ! TEAM . equals ( '' ) } } environment { NAMESPACE = \"cb-teams-${TEAM}\" } steps { container ( 'cli' ) { sh 'echo ${NAMESPACE}' script { def response = sh encoding: 'UTF-8' , label: 'create team' , returnStatus: true , script: '${CLI} ${CREDS} groovy = < resources/bootstrap/configure-oc-namespace.groovy ${NAMESPACE}' println \"Response: ${response}\" } } } } stage ( 'Create Team Master' ) { when { expression { return ! TEAM . equals ( '' ) } } environment { TEAM_NAME = \"${TEAM}\" } steps { container ( 'cli' ) { println \"TEAM_NAME=${TEAM_NAME}\" sh 'ls -lath' sh 'ls -lath teams/' script { def response = sh encoding: 'UTF-8' , label: 'create team' , returnStatus: true , script: '${CLI} ${CREDS} teams ${TEAM_NAME} --put < \"teams/${TEAM_NAME}/team.json\"' println \"Response: ${response}\" } } } } } } } } stage ( 'Test CLI Connection' ) { steps { container ( 'cli' ) { script { def response = sh encoding: 'UTF-8' , label: 'retrieve version' , returnStatus: true , script: '${CLI} ${CREDS} version' println \"Response: ${response}\" } } } } stage ( 'Update Team Recipes' ) { when { allOf { branch 'master' ; changeset \"recipes/recipes.json\" } } steps { container ( 'cli' ) { sh 'ls -lath' sh 'ls -lath recipes/' script { def response = sh encoding: 'UTF-8' , label: 'update team recipe' , returnStatus: true , script: '${CLI} ${CREDS} team-creation-recipes --put < \"recipes/recipes.json\"' println \"Response: ${response}\" } } } } } post { always { container ( 'cli' ) { sh '${CLI} ${CREDS} groovy = < resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE}' } } } }","title":"CloudBees Automate Teams"},{"location":"blogs/teams-automation/#core-modern-teams-automation","text":"CloudBees Core on Modern (meaning, on Kubernetes) has two main types of Jenkins Masters, a Managed Master, and a Team Master. In this article, we're going to automate the creation and management of the Team Masters. Hint If you do not want to read any of the code here, or just want to take a look at the end result - pipeline wise - you can find working examples on GitHub. Template Repository - creates a new team template and a PR to the GitOps repository GitOps Repository - applies GitOps principles to manage the CloudBees Core Team Masters","title":"Core Modern Teams Automation"},{"location":"blogs/teams-automation/#goals","text":"Just automating the creation of a Team Master is relatively easy, as this can be done via the Client Jar . So we're going to set some additional goals to create a decent challenge. GitOps : I want to be able to create and delete Team Masters by managing configuration in a Git repository Configuration-as-Code : as much of the configuration as possible should be stored in the Git repository Namespace : one of the major reasons for Team Masters to exist is to increase (Product) Team autonomy, which in Kubernetes should correspond to a namespace . So I want each Team Master to be in its own Namespace! Self-Service : the solution should cater to (semi-)autonomous teams and lower the workload of the team managing CloudBees Core. So requesting a Team Master should be doable by everyone","title":"Goals"},{"location":"blogs/teams-automation/#before-we-start","text":"Some assumptions need to be taken care off before we start. Kubernetes cluster in which you are ClusterAdmin if you don't have one yet, there are guides on this elsewhere on the site your cluster has enough capacity (at least two nodes of 4gb memory) your cluster has CloudBees Core Modern installed if you don't have this yet look at one of the guides on this site or look at the guides on CloudBees.com have administrator access to CloudBees Core Cloud Operations Center Code Examples The more extensive Code Examples, such as Kubernetes Yaml files, are collapsed by default. You can open them by clicking on them. On the right, the code snippet will have a [ ] copy icon. Below is an example. Code Snippet Example Here's a code snippet. pipeline { agent any stages { stage ( 'Hello' ) { steps { echo 'Hello World!' } } } }","title":"Before We Start"},{"location":"blogs/teams-automation/#bootstrapping","text":"All right, so we want to use GitOps and to process the changes we need a Pipeline that can be triggered by a Webhook. I believe in everything as code - except Secrets and such - which includes the Pipeline. Unfortunately, Operations Center cannot run such pipelines. To get over this hurdle, we will create a special Ops Team Master. This Master will be configured to be able to Manage the other Team Masters for us. Log into your Operations Center with a user that has administrative access.","title":"Bootstrapping"},{"location":"blogs/teams-automation/#create-api-token","text":"Create a new API Token for your administrator user by clicking on the user's name - top right corner. Select the Configuration menu on the left and then you should see a section where you can Create a API Token . This Token will disappear, so write it down.","title":"Create API Token"},{"location":"blogs/teams-automation/#get-configure-client-jar","text":"Replace the values marked by < ... > . The Operations Center URL should look like this: http://cbcore.mydomain.com/cjoc . Setup the connection variables. OC_URL = <your operations center url> USR = <your username> TKN = <api token> Download the Client Jar. curl ${ OC_URL } /jnlpJars/jenkins-cli.jar -o jenkins-cli.jar","title":"Get &amp; Configure Client Jar"},{"location":"blogs/teams-automation/#create-alias-test","text":"alias cboc = \"java -jar jenkins-cli.jar -noKeyAuth -auth ${ USR } : ${ TKN } -s ${ OC_URL } \" cboc version","title":"Create Alias &amp; Test"},{"location":"blogs/teams-automation/#create-team-ops","text":"As the tasks of the Team Masters for managing Operations are quite specific and demand special rights, I'd recommend putting this in its own namespace . To do so properly, we need to configure a few things. allows Operations Center access to this namespace (so it can create the Team Master) give the ServiceAccount the permissions to create namespace 's for the other Team Masters add config map for the Jenkins Agents temporarily change Operations Center's operating Namespace (where it will spawn resources in) use the CLI to create the team-ops Team Master reset Operations Center's operating Namespace","title":"Create Team Ops"},{"location":"blogs/teams-automation/#update-create-kubernetes-namespaces","text":"","title":"Update &amp; Create Kubernetes Namespaces"},{"location":"blogs/teams-automation/#create-team-ops-namespace","text":"kubectl apply -f team-ops-namespace.yaml team-ops-namespace.yaml This creates the team-ops namespace including all the resources required such as ResourceQuota , ServiceAccount and so on. apiVersion : v1 kind : Namespace metadata : name : team-ops --- apiVersion : v1 kind : ResourceQuota metadata : name : resource-quota namespace : team-ops spec : hard : pods : \"20\" requests.cpu : \"4\" requests.memory : 6Gi limits.cpu : \"5\" limits.memory : 10Gi services.loadbalancers : \"0\" services.nodeports : \"0\" persistentvolumeclaims : \"10\" --- apiVersion : v1 kind : ServiceAccount metadata : name : jenkins namespace : team-ops --- kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : pods-all namespace : team-ops rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/log\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : jenkins namespace : team-ops roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pods-all subjects : - kind : ServiceAccount name : jenkins namespace : team-ops --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : cjoc roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : master-management subjects : - kind : ServiceAccount name : jenkins namespace : team-ops --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : create-namespaces rules : - apiGroups : [ \"*\" ] resources : [ \"serviceaccounts\" , \"rolebindings\" , \"roles\" , \"resourcequotas\" , \"namespaces\" ] verbs : [ \"create\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"configmaps\" , \"rolebindings\" , \"roles\" , \"resourcequotas\" , \"namespaces\" ] verbs : [ \"create\" , \"get\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"events\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"persistentvolumeclaims\" , \"pods\" , \"pods/exec\" , \"services\" , \"statefulsets\" , \"ingresses\" , \"extensions\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/log\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"secrets\" ] verbs : [ \"list\" ] - apiGroups : [ \"apps\" ] resources : [ \"statefulsets\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : ops-namespace roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : create-namespaces subjects : - kind : ServiceAccount name : jenkins namespace : team-ops","title":"Create Team Ops Namespace"},{"location":"blogs/teams-automation/#update-operation-center-serviceaccount","text":"The ServiceAccount under which Operation Center runs, only has rights in it's own namespace . Which means it cannot create our Team Ops Master. Below is the .yaml file for Kubernetes and the command to apply it. Warning I assume you're using the default cloudbees-core as per Cloudbees' documentation. If this is not the case, change the last line, namespace: cloudbees-core with the namespace your Operation Center runs in. kubectl apply -f patch-oc-serviceaccount.yaml -n team-ops patch-oc-serviceaccount.yaml This patches the existing Operation Center's ServiceAccount to also have the correct rights in the team-ops namespace. kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : master-management rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/log\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] - apiGroups : [ \"apps\" ] resources : [ \"statefulsets\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"persistentvolumeclaims\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"secrets\" ] verbs : [ \"list\" ] - apiGroups : [ \"\" ] resources : [ \"events\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : cjoc roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : master-management subjects : - kind : ServiceAccount name : cjoc namespace : cloudbees-core","title":"Update Operation Center ServiceAccount"},{"location":"blogs/teams-automation/#jenkins-agent-configmap","text":"kubectl apply -f jenkins-agent-config-map.yaml -n team-ops jenkins-agent-config-map.yaml Creates the Jenkins Agent ConfigMap, which contains the information the Jenkins Agent - within a PodTemplate - uses to connect to the Jenkins Master. apiVersion : v1 kind : ConfigMap metadata : name : jenkins-agent data : jenkins-agent : | #!/usr/bin/env sh # The MIT License # # Copyright (c) 2015, CloudBees, Inc. # # Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files (the \"Software\"), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN # THE SOFTWARE. # Usage jenkins-slave.sh [options] -url http://jenkins [SECRET] [AGENT_NAME] # Optional environment variables : # * JENKINS_TUNNEL : HOST:PORT for a tunnel to route TCP traffic to jenkins host, when jenkins can't be directly accessed over network # * JENKINS_URL : alternate jenkins URL # * JENKINS_SECRET : agent secret, if not set as an argument # * JENKINS_AGENT_NAME : agent name, if not set as an argument if [ $# -eq 1 ]; then # if `docker run` only has one arguments, we assume user is running alternate command like `bash` to inspect the image exec \"$@\" else # if -tunnel is not provided try env vars case \"$@\" in *\"-tunnel \"*) ;; *) if [ ! -z \"$JENKINS_TUNNEL\" ]; then TUNNEL=\"-tunnel $JENKINS_TUNNEL\" fi ;; esac if [ -n \"$JENKINS_URL\" ]; then URL=\"-url $JENKINS_URL\" fi if [ -n \"$JENKINS_NAME\" ]; then JENKINS_AGENT_NAME=\"$JENKINS_NAME\" fi if [ -z \"$JNLP_PROTOCOL_OPTS\" ]; then echo \"Warning: JnlpProtocol3 is disabled by default, use JNLP_PROTOCOL_OPTS to alter the behavior\" JNLP_PROTOCOL_OPTS=\"-Dorg.jenkinsci.remoting.engine.JnlpProtocol3.disabled=true\" fi # If both required options are defined, do not pass the parameters OPT_JENKINS_SECRET=\"\" if [ -n \"$JENKINS_SECRET\" ]; then case \"$@\" in *\"${JENKINS_SECRET}\"*) echo \"Warning: SECRET is defined twice in command-line arguments and the environment variable\" ;; *) OPT_JENKINS_SECRET=\"${JENKINS_SECRET}\" ;; esac fi OPT_JENKINS_AGENT_NAME=\"\" if [ -n \"$JENKINS_AGENT_NAME\" ]; then case \"$@\" in *\"${JENKINS_AGENT_NAME}\"*) echo \"Warning: AGENT_NAME is defined twice in command-line arguments and the environment variable\" ;; *) OPT_JENKINS_AGENT_NAME=\"${JENKINS_AGENT_NAME}\" ;; esac fi SLAVE_JAR=/usr/share/jenkins/slave.jar if [ ! -f \"$SLAVE_JAR\" ]; then tmpfile=$(mktemp) if hash wget > /dev/null 2>&1; then wget -O \"$tmpfile\" \"$JENKINS_URL/jnlpJars/slave.jar\" elif hash curl > /dev/null 2>&1; then curl -o \"$tmpfile\" \"$JENKINS_URL/jnlpJars/slave.jar\" else echo \"Image does not include $SLAVE_JAR and could not find wget or curl to download it\" return 1 fi SLAVE_JAR=$tmpfile fi #TODO: Handle the case when the command-line and Environment variable contain different values. #It is fine it blows up for now since it should lead to an error anyway. exec java $JAVA_OPTS $JNLP_PROTOCOL_OPTS -cp $SLAVE_JAR hudson.remoting.jnlp.Main -headless $TUNNEL $URL $OPT_JENKINS_SECRET $OPT_JENKINS_AGENT_NAME \"$@\" fi","title":"Jenkins Agent ConfigMap"},{"location":"blogs/teams-automation/#create-initial-master","text":"To make it easier to change the namespace if needed, its extracted out from the command. OriginalNamespace = cloudbees-core This script changes the Operations Center's operating namespace , creates a Team Master with the name ops , and then resets the namespace. cboc groovy = < configure-oc-namespace.groovy team-ops cboc teams ops --put < team-ops.json cboc groovy = < configure-oc-namespace.groovy $OriginalNamespace team-ops.json This json file that describes a team. By default there are three roles defined on a team, TEAM_ADMIN , TEAM_MEMBER , and TEAM_GUEST . Don't forget to change the id 's to Group ID's from your Single-Sign-On solution. { \"version\" : \"1\" , \"data\" : { \"name\" : \"ops\" , \"displayName\" : \"Operations\" , \"provisioningRecipe\" : \"basic\" , \"members\" : [{ \"id\" : \"Catmins\" , \"roles\" : [ \"TEAM_ADMIN\" ] }, { \"id\" : \"Pirates\" , \"roles\" : [ \"TEAM_MEMBER\" ] }, { \"id\" : \"Continental\" , \"roles\" : [ \"TEAM_GUEST\" ] } ], \"icon\" : { \"name\" : \"hexagons\" , \"color\" : \"#8d7ec1\" } } } configure-oc-namespace.groovy This is a Jenkins Configuration or System Groovy script. It will change the namespace Operation Center uses to create resources. You can change this in the UI by going to Operations Center -> Manage Jenkins -> System Configuration -> Master Provisioning -> Namespace . import hudson.* import hudson.util.Secret ; import hudson.util.Scrambler ; import hudson.util.FormValidation ; import jenkins.* import jenkins.model.* import hudson.security.* import com.cloudbees.masterprovisioning.kubernetes.KubernetesMasterProvisioning import com.cloudbees.masterprovisioning.kubernetes.KubernetesClusterEndpoint println \"=== KubernetesMasterProvisioning Configuration - start\" println \"== Retrieving main configuration\" def descriptor = Jenkins . getInstance (). getInjector (). getInstance ( KubernetesMasterProvisioning . DescriptorImpl . class ) def namespace = this . args [ 0 ] def currentKubernetesClusterEndpoint = descriptor . getClusterEndpoints (). get ( 0 ) println \"= Found current endpoint\" println \"= \" + currentKubernetesClusterEndpoint . toString () def id = currentKubernetesClusterEndpoint . getId () def name = currentKubernetesClusterEndpoint . getName () def url = currentKubernetesClusterEndpoint . getUrl () def credentialsId = currentKubernetesClusterEndpoint . getCredentialsId () println \"== Setting Namspace to \" + namespace def updatedKubernetesClusterEndpoint = new KubernetesClusterEndpoint ( id , name , url , credentialsId , namespace ) def clusterEndpoints = new ArrayList < KubernetesClusterEndpoint >() clusterEndpoints . add ( updatedKubernetesClusterEndpoint ) descriptor . setClusterEndpoints ( clusterEndpoints ) println \"== Saving Jenkins configuration\" descriptor . save () println \"=== KubernetesMasterProvisioning Configuration - finish\"","title":"Create Initial Master"},{"location":"blogs/teams-automation/#configure-team-ops-master","text":"Now that we've created the Operations Team Master (Team Ops), we can configure it. The Pipelines we need will require credentials, we describe them below. githubtoken_token : GitHub API Token only, credentials type Secret Text (for the PR pipeline) githubtoken : GitHub username and API Token jenkins-api : Username and API Token for Operations Center. Just like the one we used for Client Jar. We also need to have a Global Pipeline Library defined by the name github.com/joostvdg/jpl-core . This, as the name suggests, should point to https://github.com/joostvdg/jpl-core.git .","title":"Configure Team Ops Master"},{"location":"blogs/teams-automation/#create-gitops-pipeline","text":"In total, we need two repositories and two or three Pipelines. You can either use my CLI Docker Image or roll your own. I will proceed as if you will create your own. CLI Image Pipeline : this will create a CLI Docker Image that is used to talk to Operations Center via the Client Jar (CLI) PR Pipeline : I like the idea of Self-Service, but in order to keep things in check, you might want to provide that via a PullRequest (PR) rather than a direct write to the Master branch. This is also Repository One, as I prefer having each pipeline in their own Repository, but you don't need to. Main Pipeline : will trigger on a commit to the Master branch and create the new team. I'll even throw in a free manage your Team Recipes for free as well.","title":"Create GitOps Pipeline"},{"location":"blogs/teams-automation/#create-cli-image-pipeline","text":"In a Kubernetes cluster, you should not build with Docker directly, use an in-cluster builder such as Kaniko or Buildah . You can read more about the why and how elsewhere on this site . Tip If you do not want to create your own, you can re-use my images. There should be one available for every recent version of CloudBees Core that will work with your Operations Center. The images are available in DockerHub at caladreas/cbcore-cli","title":"Create CLI Image Pipeline"},{"location":"blogs/teams-automation/#kaniko-configuration","text":"Kaniko uses a Docker Image to build your Docker Image in cluster. It does however need to directly communicate to your Docker Registry. This requires a Kubernetes Secret of type docker-registry . How you can do this and more, you can read on the CloudBees Core Docs .","title":"Kaniko Configuration"},{"location":"blogs/teams-automation/#pipeline","text":"Now that you have Kaniko configured, you can use this Pipeline to create your own CLI Images. Caution Make sure you replace the environment variables with values that make sense to you. CJOC_URL internal url in Kubernets, usually http://cjoc.<namespace>/cjoc REGISTRY : index.docker.io = DockerHub REPO : docker repository name IMAGE : docker image name Jenkinsfile Jenkins Declarative Pipeline for the CLI Image geberation. pipeline { agent { kubernetes { //cloud 'kubernetes' label 'test' yaml \"\"\" kind: Pod metadata: name: test spec: containers: - name: curl image: byrnedo/alpine-curl command: - cat tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: jenkins-docker-cfg projected: sources: - secret: name: docker-credentials items: - key: .dockerconfigjson path: .docker/config.json \"\"\" } } environment { CJOC_URL = 'http://cjoc.cloudbees-core/cjoc' CLI_VERSION = '' REGISTRY = 'index.docker.io' REPO = 'caladreas' IMAGE = 'cbcore-cli' } stages { stage ( 'Download CLI' ) { steps { container ( 'curl' ) { sh 'curl --version' sh 'echo ${CJOC_URL}/jnlpJars/jenkins-cli.jar' sh 'curl ${CJOC_URL}/jnlpJars/jenkins-cli.jar --output jenkins-cli.jar' sh 'ls -lath' } } } stage ( 'Prepare' ) { parallel { stage ( 'Verify CLI' ) { environment { CREDS = credentials ( 'jenkins-api' ) CLI = \"java -jar jenkins-cli.jar -noKeyAuth -s ${CJOC_URL} -auth\" } steps { sh 'echo ${CLI}' script { CLI_VERSION = sh returnStdout: true , script: '${CLI} ${CREDS} version' } sh 'echo ${CLI_VERSION}' } } stage ( 'Prepare Dockerfile' ) { steps { writeFile encoding: 'UTF-8' , file: 'Dockerfile' , text: \"\"\"FROM mcr.microsoft.com/java/jre-headless:8u192-zulu-alpine WORKDIR /usr/bin ADD jenkins-cli.jar . RUN pwd RUN ls -lath \"\"\" } } } } stage ( 'Build with Kaniko' ) { environment { PATH = \"/busybox:/kaniko:$PATH\" TAG = \"${CLI_VERSION}\" } steps { sh 'echo image fqn=${REGISTRY}/${REPO}/${IMAGE}:${TAG}' container ( name: 'kaniko' , shell: '/busybox/sh' ) { sh '''#!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:${TAG} /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:latest ''' } } } } }","title":"Pipeline"},{"location":"blogs/teams-automation/#pr-pipeline","text":"Caution The PR Pipeline example builds upon the GitHub API, if you're not using GItHub, you will have to figure out another way to make the PR.","title":"PR Pipeline"},{"location":"blogs/teams-automation/#tools-used","text":"yq : commandline tool for processing Yaml files jq commandline tool for pressing Json files Kustomize templating tool for Kubernetes Yaml, as of Kubernetes 1.13 , this is part of the Client (note, your server can be older, don't worry!) Hub commandline client for GitHub","title":"Tools Used"},{"location":"blogs/teams-automation/#repository-layout","text":"folder: team-master-template with file simple.json folder: namespace-creation with folder: kustomize this contains the Kustomize configuration Simple.json This is a template for the team JSON definition. { \"version\" : \"1\" , \"data\" : { \"name\" : \"NAME\" , \"displayName\" : \"DISPLAY_NAME\" , \"provisioningRecipe\" : \"RECIPE\" , \"members\" : [ { \"id\" : \"ADMINS\" , \"roles\" : [ \"TEAM_ADMIN\" ] }, { \"id\" : \"MEMBERS\" , \"roles\" : [ \"TEAM_MEMBER\" ] }, { \"id\" : \"GUESTS\" , \"roles\" : [ \"TEAM_GUEST\" ] } ], \"icon\" : { \"name\" : \"ICON\" , \"color\" : \"HEX_COLOR\" } } }","title":"Repository Layout"},{"location":"blogs/teams-automation/#kustomize-configuration","text":"Kustomize is a tool for template Kubernetes YAML definitions, which is what we need here. However, only for the namespace creation & configuration. So if you don't want to do that, you can skip this. The Kustomize configuration has two parts, a folder called team-example with a kustomization.yaml . This will be what we configure to generate a new yaml definition. The main template is in the folder base , where the entrypoint will be again kustomization.yaml . This time, the kustomization.yaml will link to all the template files we need. As posting all these yaml files again is a bit much, I'll link to my example repo. Feel free to fork it instead: cb-team-gitops-template configmap.yaml : the Jenkins Agent ConfigMap namespace.yaml : the new namespace resource-quota.yaml : resource quota's for the namespace role-binding-cjoc.yaml : a role binding for the CJOC ServiceAccount, so it create create the new Master in the new namespace role-binding.yaml : the role binding for the jenkins ServiceAccount, which allows the new Master to create and manage Pods (for PodTemplates) role-cjoc.yaml : the role for CJOC for the ability to create a Master in the new Namspace role.yaml : the role for the jenkins ServiceAccount for the new Master service-account.yaml : the ServiceAccount, jenkins , used by the new Master","title":"Kustomize Configuration"},{"location":"blogs/teams-automation/#pipeline_1","text":"The Pipeline will do the following: capture input parameters to be used to customize the Team Master update the Kustomize template to make sure every resource is correct for the new namespace ( teams-<name of team> ) execute Kustomize to generate a single yaml file that defines the configuration for the new Team Masters' namespace process the simple.json to generate a team.json file for the new Team Master for use with the Jenkins CLI checkout your GIT_REPO that contains your team definitions create a new PR to your GIT_REPO for the new team Jenkinsfile Variables to update: GIT_REPO : the GitHub repository in which the Team Definitions are stored RESET_NAMESPACE : the namespace Operations Center should use as default pipeline { agent { kubernetes { label 'team-automation' yaml \"\"\" kind: Pod spec: containers: - name: hub image: caladreas/hub command: [\"cat\"] tty: true resources: requests: memory: \"50Mi\" cpu: \"150m\" limits: memory: \"50Mi\" cpu: \"150m\" - name: kubectl image: bitnami/kubectl:latest command: [\"cat\"] tty: true securityContext: runAsUser: 1000 fsGroup: 1000 resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"150Mi\" cpu: \"200m\" - name: yq image: mikefarah/yq command: ['cat'] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" - name: jq image: colstrom/jq command: ['cat'] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" \"\"\" } } libraries { lib ( 'github.com/joostvdg/jpl-core' ) } options { disableConcurrentBuilds () // we don't want more than one at a time checkoutToSubdirectory 'templates' // we need to do two checkouts buildDiscarder logRotator ( artifactDaysToKeepStr: '' , artifactNumToKeepStr: '' , daysToKeepStr: '5' , numToKeepStr: '5' ) // always clean up } environment { envGitInfo = '' RESET_NAMESPACE = 'jx-production' TEAM_BASE_NAME = '' NAMESPACE_TO_CREATE = '' DISPLAY_NAME = '' TEAM_RECIPE = '' ICON = '' ICON_COLOR_CODE = '' ADMINS_ROLE = '' MEMBERS_ROLE = '' GUESTS_ROLE = '' RECORD_LOC = '' GIT_REPO = '' } stages { stage ( 'Team Details' ) { input { message \"Please enter the team details.\" ok \"Looks good, proceed\" parameters { string ( name: 'Name' , defaultValue: 'hex' , description: 'Please specify a team name' ) string ( name: 'DisplayName' , defaultValue: 'Hex' , description: 'Please specify a team display name' ) choice choices: [ 'joostvdg' , 'basic' , 'java-web' ], description: 'Please select a Team Recipe' , name: 'TeamRecipe' choice choices: [ 'anchor' , 'bear' , 'bowler-hat' , 'briefcase' , 'bug' , 'calculator' , 'calculatorcart' , 'clock' , 'cloud' , 'cloudbees' , 'connect' , 'dollar-bill' , 'dollar-symbol' , 'file' , 'flag' , 'flower-carnation' , 'flower-daisy' , 'help' , 'hexagon' , 'high-heels' , 'jenkins' , 'key' , 'marker' , 'monocle' , 'mustache' , 'office' , 'panther' , 'paw-print' , 'teacup' , 'tiger' , 'truck' ], description: 'Please select an Icon' , name: 'Icon' string ( name: 'IconColorCode' , defaultValue: '#CCCCCC' , description: 'Please specify a valid html hexcode for the color (https://htmlcolorcodes.com/)' ) string ( name: 'Admins' , defaultValue: 'Catmins' , description: 'Please specify a groupid or userid for the TEAM_ADMIN role' ) string ( name: 'Members' , defaultValue: 'Pirates' , description: 'Please specify a groupid or userid for the TEAM_MEMBER role' ) string ( name: 'Guests' , defaultValue: 'Continental' , description: 'Please specify a groupid or userid for the TEAM_GUEST role' ) } } steps { println \"Name=${Name}\" println \"DisplayName=${DisplayName}\" println \"TeamRecipe=${TeamRecipe}\" println \"Icon=${Icon}\" println \"IconColorCode=${IconColorCode}\" println \"Admins=${Admins}\" println \"Members=${Members}\" println \"Guests=${Guests}\" script { TEAM_BASE_NAME = \"${Name}\" NAMESPACE_TO_CREATE = \"cb-teams-${Name}\" DISPLAY_NAME = \"${DisplayName}\" TEAM_RECIPE = \"${TeamRecipe}\" ICON = \"${Icon}\" ICON_COLOR_CODE = \"${IconColorCode}\" ADMINS_ROLE = \"${Admins}\" MEMBERS_ROLE = \"${Members}\" GUESTS_ROLE = \"${Guests}\" RECORD_LOC = \"templates/teams/${Name}\" sh \"mkdir -p ${RECORD_LOC}\" } } } stage ( 'Create Team Config' ) { environment { BASE = 'templates/namespace-creation/kustomize' NAMESPACE = \"${NAMESPACE_TO_CREATE}\" RECORD_LOC = \"templates/teams/${TEAM_BASE_NAME}\" } parallel { stage ( 'Namespace' ) { steps { container ( 'yq' ) { sh 'yq w -i ${BASE}/base/role-binding.yaml subjects[0].namespace ${NAMESPACE}' sh 'yq w -i ${BASE}/base/namespace.yaml metadata.name ${NAMESPACE}' sh 'yq w -i ${BASE}/team-example/kustomization.yaml namespace ${NAMESPACE}' } container ( 'kubectl' ) { sh ''' kubectl kustomize ${BASE}/team-example > ${RECORD_LOC}/team.yaml cat ${RECORD_LOC}/team.yaml ''' } } } stage ( 'Team Master JSON' ) { steps { container ( 'jq' ) { sh \"\"\"jq \\ '.data.name = \"${TEAM_BASE_NAME}\" |\\ .data.displayName = \"${DISPLAY_NAME}\" |\\ .data.provisioningRecipe = \"${TEAM_RECIPE}\" |\\ .data.icon.name = \"${ICON}\" |\\ .data.icon.color = \"${ICON_COLOR_CODE}\" |\\ .data.members[0].id = \"${ADMINS_ROLE}\" |\\ .data.members[1].id = \"${MEMBERS_ROLE}\" |\\ .data.members[2].id = \"${GUESTS_ROLE}\"'\\ templates/team-master-template/simple.json > ${RECORD_LOC}/team.json \"\"\" } sh 'cat ${RECORD_LOC}/team.json' } } } } stage ( 'Create PR' ) { when { branch 'master' } environment { RECORD_OLD_LOC = \"templates/teams/${TEAM_BASE_NAME}\" RECORD_LOC = \"teams/${TEAM_BASE_NAME}\" PR_CHANGE_NAME = \"add_team_${TEAM_BASE_NAME}\" } steps { container ( 'hub' ) { dir ( 'cb-team-gitops' ) { script { envGitInfo = git \"${GIT_REPO}\" } sh 'git checkout -b ${PR_CHANGE_NAME}' sh 'ls -lath ../${RECORD_OLD_LOC}' sh 'cp -R ../${RECORD_OLD_LOC} ./teams' sh 'ls -lath' sh 'ls -lath teams/' gitRemoteConfigByUrl ( envGitInfo . GIT_URL , 'githubtoken_token' ) // must be a API Token ONLY -> secret text sh ''' git config --global user.email \"jenkins@jenkins.io\" git config --global user.name \"Jenkins\" git add ${RECORD_LOC} git status git commit -m \"add team ${TEAM_BASE_NAME}\" git push origin ${PR_CHANGE_NAME} ''' // has to be indented like that, else the indents will be in the pr description writeFile encoding: 'UTF-8' , file: 'pr-info.md' , text: \"\"\"Add ${TEAM_BASE_NAME} \\n This pr is automatically generated via CloudBees.\\\\n \\n The job: ${env.JOB_URL} \"\"\" // TODO: unfortunately, environment {}'s credentials have fixed environment variable names // TODO: in this case, they need to be EXACTLY GITHUB_PASSWORD and GITHUB_USER script { withCredentials ([ usernamePassword ( credentialsId: 'githubtoken' , passwordVariable: 'GITHUB_PASSWORD' , usernameVariable: 'GITHUB_USER' )]) { sh \"\"\" set +x hub pull-request --force -F pr-info.md -l '${TEAM_BASE_NAME}' --no-edit \"\"\" } } } } } } } }","title":"Pipeline"},{"location":"blogs/teams-automation/#main-pipeline","text":"The main Pipeline should be part of a repository. The Repository should look like this: recipes (folder) recipes.json -> current complete list of CloudBees Core Team Recipes definition teams (folder) folder per team team.json -> CloudBees Core Team definition team.yaml -> Kubernetes YAML definition of the namespace and all its resources","title":"Main Pipeline"},{"location":"blogs/teams-automation/#process","text":"The pipeline can be a bit hard to grasp, so let me break it down into individual steps. We have the following stages: Create Team - which is broken into sub-stages via the sequential stages feature . * Parse Changelog * Create Namespace * Change OC Namespace * Create Team Master Test CLI Connection Update Team Recipes","title":"Process"},{"location":"blogs/teams-automation/#notable-statements","text":"disableConcurrentBuilds We change the namespace of Operation Center to a different value only for the duration of creating this master. This is something that should probably be part of the Team Master creation, but as it is a single configuration option for all that Operation Center does, we need to be careful. By ensuring we only run one build concurrently, we reduce the risk of this blowing up in our face. options { disableConcurrentBuilds () } when { } The When Directive allows us to creating effective conditions for when a stage should be executed. The snippet below shows the use of a combination of both the branch and changeset built-in filters. changeset looks at the commit being build and validates that there was a change in that file path. when { allOf { branch 'master' ; changeset \"teams/**/team.*\" } } post { always { } } The Post Directive allows us to run certain commands after the main pipeline has run depending on the outcome (compared or not to the previous outcome). In this case, we want to make sure we reset the namespace used by Operations Center to the original value. By using post { always {} } , it will ALWAYS run, regardless of the status of the pipeline. So we should be safe. post { always { container ( 'cli' ) { sh '${CLI} ${CREDS} groovy = < resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE}' } } } stages { stage { parallel { stage() { stages { stage { Oke, you might've noticed this massive indenting depth and probably have some questions. By combining sequential stages with parallel stages we can create a set of stages that will be executed in sequence but can be controlled by a single when {} statement whether or not they get executed. This prevents mistakes being made in the condition and accidentally running one or other but not all the required steps. stages { stage ( 'Create Team' ) { parallel { stage ( 'Main' ) { stages { stage ( 'Parse Changelog' ) { changetSetData & container('jpb') {} Alright, so even if we know a team was added in /teams/<team-name> , we still don't know the following two things: 1) what is the name of this team, 2) was this team changed or deleted? So we have to process the changelog to be able to answer these questions as well. There are different ways of getting the changelog and parsing it. I've written one you can do on ANY machine, regardless of Jenkins by leveraging Git and my own custom binary ( jpb -> Jenkins Pipeline Binary). The code for my binary is at GitHub: github.com/joostvdg/jpb . An alternative approach is described by CloudBees Support here , which leverages Jenkins groovy powers. COMMIT_INFO = \"${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT}\" def changeSetData = sh returnStdout: true , script: \"git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO}\" changeSetData = changeSetData . replace ( \"\\n\" , \"\\\\n\" ) container ( 'jpb' ) { changeSetFolders = sh returnStdout: true , script: \"/usr/bin/jpb/bin/jpb GitChangeListToFolder '${changeSetData}' 'teams/'\" changeSetFolders = changeSetFolders . split ( ',' ) }","title":"Notable Statements"},{"location":"blogs/teams-automation/#files","text":"recipes.json The default Team Recipes that ships with CloudBees Core Modern. { \"version\" : \"1\" , \"data\" : [{ \"name\" : \"basic\" , \"displayName\" : \"Basic\" , \"description\" : \"The minimalistic setup.\" , \"plugins\" : [ \"bluesteel-master\" , \"cloudbees-folders-plus\" , \"cloudbees-jsync-archiver\" , \"cloudbees-monitoring\" , \"cloudbees-nodes-plus\" , \"cloudbees-ssh-slaves\" , \"cloudbees-support\" , \"cloudbees-workflow-template\" , \"credentials-binding\" , \"email-ext\" , \"git\" , \"git-client\" , \"github-branch-source\" , \"github-organization-folder\" , \"infradna-backup\" , \"ldap\" , \"mailer\" , \"operations-center-analytics-reporter\" , \"operations-center-cloud\" , \"pipeline-model-definition\" , \"ssh-credentials\" , \"wikitext\" , \"workflow-aggregator\" , \"workflow-cps-checkpoint\" ], \"default\" : true }, { \"name\" : \"java-web\" , \"displayName\" : \"Java & Web Development\" , \"description\" : \"The essential tools to build, release and deploy Java Web applications including integration with Maven, Gradle and Node JS.\" , \"plugins\" : [ \"bluesteel-master\" , \"cloudbees-folders-plus\" , \"cloudbees-jsync-archiver\" , \"cloudbees-monitoring\" , \"cloudbees-nodes-plus\" , \"cloudbees-ssh-slaves\" , \"cloudbees-support\" , \"cloudbees-workflow-template\" , \"credentials-binding\" , \"email-ext\" , \"git\" , \"git-client\" , \"github-branch-source\" , \"github-organization-folder\" , \"infradna-backup\" , \"ldap\" , \"mailer\" , \"operations-center-analytics-reporter\" , \"operations-center-cloud\" , \"pipeline-model-definition\" , \"ssh-credentials\" , \"wikitext\" , \"workflow-aggregator\" , \"workflow-cps-checkpoint\" , \"config-file-provider\" , \"cloudbees-aws-cli\" , \"cloudbees-cloudfoundry-cli\" , \"findbugs\" , \"gradle\" , \"jira\" , \"junit\" , \"nodejs\" , \"openshift-cli\" , \"pipeline-maven\" , \"tasks\" , \"warnings\" ], \"default\" : false }] } Jenkinsfile This is the pipeline that will process the commit to the repository and, if it detects a new team is created will apply the changes. Variables to overwrite: GIT_REPO : the https url to the Git Repository your GitOps code/configuration is stored RESET_NAMESPACE : the namespace your Operation Center normally operates in CLI : this command depends on the namespace Operation Center is in ( http://<service name>.<namespace>/cjoc ) pipeline { agent { kubernetes { label 'jenkins-agent' yaml ''' apiVersion: v1 kind: Pod spec: serviceAccountName: jenkins containers: - name: cli image: caladreas/cbcore-cli:2.176.2.3 imagePullPolicy: Always command: - cat tty: true resources: requests: memory: \"50Mi\" cpu: \"150m\" limits: memory: \"50Mi\" cpu: \"150m\" - name: kubectl image: bitnami/kubectl:latest command: [\"cat\"] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"150Mi\" cpu: \"200m\" - name: yq image: mikefarah/yq command: ['cat'] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" - name: jpb image: caladreas/jpb command: - cat tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" securityContext: runAsUser: 1000 fsGroup: 1000 ''' } } options { disableConcurrentBuilds () buildDiscarder logRotator ( artifactDaysToKeepStr: '' , artifactNumToKeepStr: '' , daysToKeepStr: '5' , numToKeepStr: '5' ) } environment { RESET_NAMESPACE = 'cloudbees-core' CREDS = credentials ( 'jenkins-api' ) CLI = \"java -jar /usr/bin/jenkins-cli.jar -noKeyAuth -s http://cjoc.cloudbees-core/cjoc -auth\" COMMIT_INFO = '' TEAM = '' GIT_REPO = '' } stages { stage ( 'Create Team' ) { when { allOf { branch 'master' ; changeset \"teams/**/team.*\" } } parallel { stage ( 'Main' ) { stages { stage ( 'Parse Changelog' ) { steps { // Alternative approach: https://support.cloudbees.com/hc/en-us/articles/217630098-How-to-access-Changelogs-in-a-Pipeline-Job- // However, that runs on the master, JPB runs in an agent! script { scmVars = git \"${GIT_REPO}\" COMMIT_INFO = \"${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT}\" def changeSetData = sh returnStdout: true , script: \"git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO}\" changeSetData = changeSetData . replace ( \"\\n\" , \"\\\\n\" ) container ( 'jpb' ) { changeSetFolders = sh returnStdout: true , script: \"/usr/bin/jpb/bin/jpb GitChangeListToFolder '${changeSetData}' 'teams/'\" changeSetFolders = changeSetFolders . split ( ',' ) } if ( changeSetFolders . length > 0 ) { TEAM = changeSetFolders [ 0 ] TEAM = TEAM . trim () // to protect against a team being removed def exists = fileExists \"teams/${TEAM}/team.yaml\" if (! exists ) { TEAM = '' } } else { TEAM = '' } echo \"Team that changed: |${TEAM}|\" } } } stage ( 'Create Namespace' ) { when { expression { return ! TEAM . equals ( '' ) } } environment { NAMESPACE = \"cb-teams-${TEAM}\" RECORD_LOC = \"teams/${TEAM}\" } steps { container ( 'kubectl' ) { sh ''' cat ${RECORD_LOC}/team.yaml kubectl apply -f ${RECORD_LOC}/team.yaml ''' } } } stage ( 'Change OC Namespace' ) { when { expression { return ! TEAM . equals ( '' ) } } environment { NAMESPACE = \"cb-teams-${TEAM}\" } steps { container ( 'cli' ) { sh 'echo ${NAMESPACE}' script { def response = sh encoding: 'UTF-8' , label: 'create team' , returnStatus: true , script: '${CLI} ${CREDS} groovy = < resources/bootstrap/configure-oc-namespace.groovy ${NAMESPACE}' println \"Response: ${response}\" } } } } stage ( 'Create Team Master' ) { when { expression { return ! TEAM . equals ( '' ) } } environment { TEAM_NAME = \"${TEAM}\" } steps { container ( 'cli' ) { println \"TEAM_NAME=${TEAM_NAME}\" sh 'ls -lath' sh 'ls -lath teams/' script { def response = sh encoding: 'UTF-8' , label: 'create team' , returnStatus: true , script: '${CLI} ${CREDS} teams ${TEAM_NAME} --put < \"teams/${TEAM_NAME}/team.json\"' println \"Response: ${response}\" } } } } } } } } stage ( 'Test CLI Connection' ) { steps { container ( 'cli' ) { script { def response = sh encoding: 'UTF-8' , label: 'retrieve version' , returnStatus: true , script: '${CLI} ${CREDS} version' println \"Response: ${response}\" } } } } stage ( 'Update Team Recipes' ) { when { allOf { branch 'master' ; changeset \"recipes/recipes.json\" } } steps { container ( 'cli' ) { sh 'ls -lath' sh 'ls -lath recipes/' script { def response = sh encoding: 'UTF-8' , label: 'update team recipe' , returnStatus: true , script: '${CLI} ${CREDS} team-creation-recipes --put < \"recipes/recipes.json\"' println \"Response: ${response}\" } } } } } post { always { container ( 'cli' ) { sh '${CLI} ${CREDS} groovy = < resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE}' } } } }","title":"Files"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/","text":"Alerts With Alertmanager \u00b6 List Of Alerts \u00b6 health score < 0.95 ingress performance vm heap usage ratio > 80% file descriptor > 75% job queue > 10 over x minutes job success ratio < 50% master executor count > 0 good http request ratio < 90% offline nodes > 5 over 30 minutes healtcheck duration > 0.002 plugin updates available > 10 An alert that triggers if any of the health reports are failing An alert that triggers if the file descriptor usage on the master goes above 80% vm.file.descriptor.ratio -> vm_file_descriptor_ratio An alert that triggers if the JVM heap memory usage is over 80% for more than a minute vm.memory.heap.usage -> vm_memory_heap_usage An alert that triggers if the 5 minute average of HTTP/404 responses goes above 10 per minute for more than five minutes http.responseCodes.badRequest -> http_responseCodes_badRequest Alert Manager Configuration \u00b6 We can configure Alert Manager via the Prometheus Helm Chart. All the configuration elements below are part of the prom-values.yaml we used to when installing Prometheus via Helm. Get Slack Endpoint \u00b6 There are many ways to get the Alerts out, for all options you can read the Prometheus documentation . In this guide, I've chosen to use slack, as I find it convenient personally. Slack has a guide on creating webhooks , once you've created an App you can retrieve an endpoint which you can use directly in the Alertmanager configuration. Alerts Configuration \u00b6 We configure the alerts within Prometheus itself via a ConfigMap . We configure the body of the alert configuration file via serverFiles . alerts . groups and serverFiles . rules . We can have a list of rules and a list of groups of rules. For more information how you can configure these rules, consult the Prometheus documentation . serverFiles : alerts : groups : # alerts come here rules : {} Alert Example \u00b6 Below is an example of an Alert. We have the following fields: alert : the name of the alert expr : the query that should evaluate to true or false for (optional): duration of the expressions equating to true before it fires labels (optional): you can add key-value pairs to encode more information on the alert, you can use this to select different receiver (e.g., email vs. slack, or different slack channels) annotations : we're expected to fill in summary and description as shown below, they will header and body of the alert - alert : JenkinsTooManyJobsQueued expr : sum(jenkins_queue_size_value) > 5 for : 1m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} too many jobs queued\" description : \"{{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs stuck in the queue\" Alertmanager Configuration \u00b6 We use Alertmanager for what to do with alerts once they happen. We configure this in the same prom-values.yaml file, in this under alertmanagerFiles . alertmanager.yml . We can create different routes that match on labels or other values. For simplicity sake - this guide is not on Alertmanager's capabilities - we stick to the most straightforward example without any such matching or grouping. For more information on configuring routes, please read the Prometheus configuration documentation . alertmanagerFiles : alertmanager.yml : global : {} route : group_by : [ alertname , app_kubernetes_io_instance ] receiver : default receivers : - name : default slack_configs : - api_url : '<REPLACE_WITH_YOUR_SLACK_API_ENDPOINT>' username : 'Alertmanager' channel : '#notify' send_resolved : true title : \"{{ .CommonAnnotations.summary }} \" text : \"{{ .CommonAnnotations.description }} {{ .CommonLabels.app_kubernetes_io_instance}} \" title_link : http://my-prometheus.com/alerts Group Alerts \u00b6 You can group alerts if they are similar or the same with different trigger values (warning vs critical). serverFiles : alerts : groups : - name : healthcheck rules : - alert : JenkinsHealthScoreToLow # alert info - alert : JenkinsTooSlowHealthCheck # alert info - name : jobs rules : - alert : JenkinsTooManyJobsQueued # alert info - alert : JenkinsTooManyJobsStuckInQueue # alert info The Alerts \u00b6 Behold, my awesome - eh, simple example - alerts. These are by no means the best alerts to create and are by no means alerts you should directly put into production. Please see them as examples to learn from! Caution One thing to note especially, the values for the exp and for are generally set very low. This is intentional, so they are easy to copy past and test. They should be relatively easy to trigger so you can learn about the relationship between the situation in your master and the alert firing. Too Many Jobs Queued \u00b6 If there are too many Jobs queued in the Jenkins Master. This event fires if there's more than 10 jobs in the queue for at least 10 minutes. - alert : JenkinsTooManyJobsQueued expr : sum(jenkins_queue_size_value) > 10 for : 10m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} too many jobs queued\" description : \"{{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs stuck in the queue\" Jobs Stuck In Queue \u00b6 Sometimes Jobs depend on other Jobs, which means they're not just in the queue, they're stuck in the queue. - alert : JenkinsTooManyJobsStuckInQueue expr : sum(jenkins_queue_stuck_value) by (app_kubernetes_io_instance) > 5 for : 5m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} too many jobs queued\" description : \" {{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs in queue\" Jobs Waiting Too Long To Start \u00b6 If Jobs are generally waiting a long time to start, waiting for a build agent to be available or otherwise, we want to know. This value is not very useful - although not completely useless - if you only have PodTemplates as build agents. When you use PodTemplates, this value is the time between the job being scheduled and when the Pod is scheduled in Kubernetes. - alert : JenkinsWaitingTooMuchOnJobStart expr : sum (jenkins_job_waiting_duration) by (app_kubernetes_io_instance) > 0.05 for : 1m labels : severity : notify annotations : summary : \"{{ $labels.app_kubernetes_io_instance }} waits too long for jobs\" description : \"{{ $labels.app_kubernetes_io_instance }} is waiting on average {{ $value }} seconds to start a job\" health score < 1 \u00b6 By default, each Jenkins Master has a health check consisting out of four values. Some plugins will add an entry, such as the CloudBees ElasticSearch Reporter for CloudBees Core. This values range from 0-1, and likely will show 0.25 , 0.50 , 0.75 and 1 as values. - alert : JenkinsHealthScoreToLow expr : sum(jenkins_health_check_score) by (app_kubernetes_io_instance) < 1 for : 5m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} has a to low health score\" description : \" {{ $labels.app_kubernetes_io_instance }} a health score lower than 100%\" Ingress Too Slow \u00b6 This alert looks at the ingress controller request duration. It fires if the request duration in 0.25 seconds or faster is not achieved for the 95% percentile. - alert : AppTooSlow expr : sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{le=\"0.25\"}[5m])) by (ingress) / sum(rate(nginx_ingress_controller_request_duration_seconds_count[5m])) by (ingress) < 0.95 for : 5m labels : severity : notify annotations : summary : \"Application - {{ $labels.ingress }} - is too slow\" description : \" {{ $labels.ingress }} - More then 5% of requests are slower than 0.25s\" HTTP Requests Too Slow \u00b6 These are the HTTP requests in Jenkins' webserver itself. We should hold this by must stricter standards than the Ingress controller - which goes through many more layers. - alert : JenkinsTooSlow expr : sum(http_requests{quantile=\"0.99\"} ) by (app_kubernetes_io_instance) > 1 for : 3m labels : severity : notify annotations : summary : \"{{ $labels.app_kubernetes_io_instance }} is too slow\" description : \"{{ $labels.app_kubernetes_io_instance }} More then 1% of requests are slower than 1s (request time: {{ $value }})\" Too Many Plugin Updates \u00b6 I always prefer having my instance up-to-date, don't you? So why not send an alert if there's more than X number of plugins waiting for an update. - alert : JenkinsTooManyPluginsNeedUpate expr : sum(jenkins_plugins_withUpdate) by (app_kubernetes_io_instance) > 3 for : 1m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} too many plugins updates\" description : \" {{ $labels.app_kubernetes_io_instance }} has {{ $value }} plugins that require an update\" File Descriptor Ratio > 40% \u00b6 According to CloudBees' documentation, the File Descriptor Ratio should not exceed 40%. Warning I don't truly know the correct value level of this metric. So wether this should be 0.0040 or 0.40 I'm not sure. Also, does this make sense in Containers with remote storage? So before you put this in production, please re-evaluate this! - alert : JenkinsToManyOpenFiles expr : sum(vm_file_descriptor_ratio) by (app_kubernetes_io_instance) > 0.040 for : 5m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} has a to many open files\" description : \" {{ $labels.app_kubernetes_io_instance }} instance has used {{ $value }} of available open files\" Job Success Ratio < 50% \u00b6 Please, please do not use Job success ratios to punish people. But if it is at all possible - which it almost certainly is - keep a respectable level of success. When practicing Continuous Integration, a broken build is a stop the world event, fix it before moving on. 100% success rate should be strived for. It is ok, not to achieve it, yet, one should be as close as possible and not let broken builds rot. - alert : JenkinsTooLowJobSuccessRate expr : sum(jenkins_runs_success_total) by (app_kubernetes_io_instance) / sum(jenkins_runs_total_total) by (app_kubernetes_io_instance) < 0.5 for : 5m labels : severity : notify annotations : summary : \"{{$labels.app_kubernetes_io_instance}} has a too low job success rate\" description : \"{{$labels.app_kubernetes_io_instance}} instance has less than 50% of jobs being successful\" Offline nodes > 5 over 10 minutes \u00b6 Having nodes offline for quite some time is usually a bad sign. It can be a static agent that can be enabled or reconnect at will, so it isn't bad on its own. Having multiple offline for a long period is likely an issue somewhere, though. - alert : JenkinsTooManyOfflineNodes expr : sum(jenkins_node_offline_value) by (app_kubernetes_io_instance) > 5 for : 10m labels : severity : notify annotations : summary : \"{{ $labels.app_kubernetes_io_instance }} has a too many offline nodes\" description : \"{{ $labels.app_kubernetes_io_instance }} has {{ $value }} nodes that are offline for some time (5 minutes)\" healtcheck duration > 0.002 \u00b6 The health check within Jenkins is talking to itself. This means it is generally really fast. We should be very very strict here, if Jenkins start having trouble measuring its own health, it is a first sign of trouble. - alert : JenkinsTooSlowHealthCheck expr : sum(jenkins_health_check_duration{quantile=\"0.999\"}) by (app_kubernetes_io_instance) > 0.001 for : 1m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} responds too slow to health check\" description : \" {{ $labels.app_kubernetes_io_instance }} is responding too slow to the regular health check\" GC ThroughPut Too Low \u00b6 Ok, here I am on thin ice. I'm not a JVM expert, so this is just an inspiration. I do not know what would be a reasonable value for triggering an alert here. I'd say, test it! - alert : JenkinsTooManyPluginsNeedUpate expr : 1 - sum(vm_gc_G1_Young_Generation_time)by (app_kubernetes_io_instance) / sum (vm_uptime_milliseconds) by (app_kubernetes_io_instance) < 0.99 for : 30m labels : severity : notify annotations : summary : \"{{ $labels.instance }} too low GC throughput\" description : \"{{ $labels.instance }} has too low Garbage Collection throughput\" vm heap usage ratio > 70% \u00b6 According to the CloudBees guide on tuning the JVM - which redirects to Oracle - the ration of JVM Heap memory usage should not exceed about 60%. So if we get over 70% for quite some time, expect trouble. As with any of these values, please do not take my word on it, and understand it yourself. - alert : JenkinsVMMemoryRationTooHigh expr : sum(vm_memory_heap_usage) by (app_kubernetes_io_instance) > 0.70 for : 3m labels : severity : notify annotations : summary : \"{{$labels.app_kubernetes_io_instance}} too high memory ration\" description : \"{{$labels.app_kubernetes_io_instance}} has a too high VM memory ration\" Uptime Less Than Two Hours \u00b6 I absolutely love servers that have excellent uptime. Running services in Containers makes that a thing of the past, such a shame. Still, I'd like my applications - such as Jenkins - to be up for reasonable lengths of time. In this case we can get notifications on Masters that have restart - for example, when OOMKilled by Kubernetes. We also get an alert when a new Master is created, which if there's selfservice involved is a nice bonus. - alert : JenkinsNewOrRestarted expr : sum(vm_uptime_milliseconds) by (app_kubernetes_io_instance) / 3600000 < 2 for : 3m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} has low uptime\" description : \" {{ $labels.app_kubernetes_io_instance }} has low uptime and was either restarted or is a new instance (uptime: {{ $value }} hours)\" Full Example \u00b6 server : ingress : enabled : true annotations : ingress.kubernetes.io/ssl-redirect : \"false\" nginx.ingress.kubernetes.io/ssl-redirect : \"false\" resources : limits : cpu : 100m memory : 1000Mi requests : cpu : 10m memory : 500Mi alertmanager : ingress : enabled : true annotations : ingress.kubernetes.io/ssl-redirect : \"false\" nginx.ingress.kubernetes.io/ssl-redirect : \"false\" resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi kubeStateMetrics : resources : limits : cpu : 10m memory : 50Mi requests : cpu : 5m memory : 25Mi nodeExporter : resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi pushgateway : resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi serverFiles : alerts : groups : - name : jobs rules : - alert : JenkinsTooManyJobsQueued expr : sum(jenkins_queue_size_value) > 5 for : 1m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} too many jobs queued\" description : \"{{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs stuck in the queue\" - alert : JenkinsTooManyJobsStuckInQueue expr : sum(jenkins_queue_stuck_value) by (app_kubernetes_io_instance) > 5 for : 1m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} too many jobs queued\" description : \" {{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs in queue\" - alert : JenkinsWaitingTooMuchOnJobStart expr : sum (jenkins_job_waiting_duration) by (app_kubernetes_io_instance) > 0.05 for : 1m labels : severity : notify annotations : summary : \"{{ $labels.app_kubernetes_io_instance }} waits too long for jobs\" description : \"{{ $labels.app_kubernetes_io_instance }} is waiting on average {{ $value }} seconds to start a job\" - alert : JenkinsTooLowJobSuccessRate expr : sum(jenkins_runs_success_total) by (app_kubernetes_io_instance) / sum(jenkins_runs_total_total) by (app_kubernetes_io_instance) < 0.60 for : 1m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} has a too low job success rate\" description : \" {{ $labels.app_kubernetes_io_instance }} instance has {{ $value }}% of jobs being successful\" - name : uptime rules : - alert : JenkinsNewOrRestarted expr : sum(vm_uptime_milliseconds) by (app_kubernetes_io_instance) / 3600000 < 2 for : 3m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} has low uptime\" description : \" {{ $labels.app_kubernetes_io_instance }} has low uptime and was either restarted or is a new instance (uptime: {{ $value }} hours)\" - name : plugins rules : - alert : JenkinsTooManyPluginsNeedUpate expr : sum(jenkins_plugins_withUpdate) by (app_kubernetes_io_instance) > 3 for : 1m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} too many plugins updates\" description : \" {{ $labels.app_kubernetes_io_instance }} has {{ $value }} plugins that require an update\" - name : jvm rules : - alert : JenkinsToManyOpenFiles expr : sum(vm_file_descriptor_ratio) by (app_kubernetes_io_instance) > 0.040 for : 5m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} has a to many open files\" description : \" {{ $labels.app_kubernetes_io_instance }} instance has used {{ $value }} of available open files\" - alert : JenkinsVMMemoryRationTooHigh expr : sum(vm_memory_heap_usage) by (app_kubernetes_io_instance) > 0.70 for : 3m labels : severity : notify annotations : summary : \"{{$labels.app_kubernetes_io_instance}} too high memory ration\" description : \"{{$labels.app_kubernetes_io_instance}} has a too high VM memory ration\" - alert : JenkinsTooManyPluginsNeedUpate expr : 1 - sum(vm_gc_G1_Young_Generation_time)by (app_kubernetes_io_instance) / sum (vm_uptime_milliseconds) by (app_kubernetes_io_instance) < 0.99 for : 30m labels : severity : notify annotations : summary : \"{{ $labels.instance }} too low GC throughput\" description : \"{{ $labels.instance }} has too low Garbage Collection throughput\" - name : web rules : - alert : JenkinsTooSlow expr : sum(http_requests{quantile=\"0.99\"} ) by (app_kubernetes_io_instance) > 1 for : 3m labels : severity : notify annotations : summary : \"{{ $labels.app_kubernetes_io_instance }} is too slow\" description : \"{{ $labels.app_kubernetes_io_instance }} More then 1% of requests are slower than 1s (request time: {{ $value }})\" - alert : AppTooSlow expr : sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{le=\"0.25\"}[5m])) by (ingress) / sum(rate(nginx_ingress_controller_request_duration_seconds_count[5m])) by (ingress) < 0.95 for : 5m labels : severity : notify annotations : summary : \"Application - {{ $labels.ingress }} - is too slow\" description : \" {{ $labels.ingress }} - More then 5% of requests are slower than 0.25s\" - name : healthcheck rules : - alert : JenkinsHealthScoreToLow expr : sum(jenkins_health_check_score) by (app_kubernetes_io_instance) < 1 for : 5m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} has a to low health score\" description : \" {{ $labels.app_kubernetes_io_instance }} a health score lower than 100%\" - alert : JenkinsTooSlowHealthCheck expr : sum(jenkins_health_check_duration{quantile=\"0.999\"}) by (app_kubernetes_io_instance) > 0.001 for : 1m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} responds too slow to health check\" description : \" {{ $labels.app_kubernetes_io_instance }} is responding too slow to the regular health check\" - name : nodes rules : - alert : JenkinsTooManyOfflineNodes expr : sum(jenkins_node_offline_value) by (app_kubernetes_io_instance) > 3 for : 1m labels : severity : notify annotations : summary : \"{{ $labels.app_kubernetes_io_instance }} has a too many offline nodes\" description : \"{{ $labels.app_kubernetes_io_instance }} has {{ $value }} nodes that are offline for some time (5 minutes)\" alertmanagerFiles : alertmanager.yml : global : {} route : group_by : [ alertname , app_kubernetes_io_instance ] receiver : default receivers : - name : default slack_configs : - api_url : '<REPLACE_WITH_YOUR_SLACK_API_URL>' username : 'Alertmanager' channel : '#notify' send_resolved : true title : \"{{ .CommonAnnotations.summary }} \" text : \"{{ .CommonAnnotations.description }} {{ .CommonLabels.app_kubernetes_io_instance}} \" title_link : http://my-prometheus.com/alerts","title":"Alerts"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#alerts-with-alertmanager","text":"","title":"Alerts With Alertmanager"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#list-of-alerts","text":"health score < 0.95 ingress performance vm heap usage ratio > 80% file descriptor > 75% job queue > 10 over x minutes job success ratio < 50% master executor count > 0 good http request ratio < 90% offline nodes > 5 over 30 minutes healtcheck duration > 0.002 plugin updates available > 10 An alert that triggers if any of the health reports are failing An alert that triggers if the file descriptor usage on the master goes above 80% vm.file.descriptor.ratio -> vm_file_descriptor_ratio An alert that triggers if the JVM heap memory usage is over 80% for more than a minute vm.memory.heap.usage -> vm_memory_heap_usage An alert that triggers if the 5 minute average of HTTP/404 responses goes above 10 per minute for more than five minutes http.responseCodes.badRequest -> http_responseCodes_badRequest","title":"List Of Alerts"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#alert-manager-configuration","text":"We can configure Alert Manager via the Prometheus Helm Chart. All the configuration elements below are part of the prom-values.yaml we used to when installing Prometheus via Helm.","title":"Alert Manager Configuration"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#get-slack-endpoint","text":"There are many ways to get the Alerts out, for all options you can read the Prometheus documentation . In this guide, I've chosen to use slack, as I find it convenient personally. Slack has a guide on creating webhooks , once you've created an App you can retrieve an endpoint which you can use directly in the Alertmanager configuration.","title":"Get Slack Endpoint"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#alerts-configuration","text":"We configure the alerts within Prometheus itself via a ConfigMap . We configure the body of the alert configuration file via serverFiles . alerts . groups and serverFiles . rules . We can have a list of rules and a list of groups of rules. For more information how you can configure these rules, consult the Prometheus documentation . serverFiles : alerts : groups : # alerts come here rules : {}","title":"Alerts Configuration"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#alert-example","text":"Below is an example of an Alert. We have the following fields: alert : the name of the alert expr : the query that should evaluate to true or false for (optional): duration of the expressions equating to true before it fires labels (optional): you can add key-value pairs to encode more information on the alert, you can use this to select different receiver (e.g., email vs. slack, or different slack channels) annotations : we're expected to fill in summary and description as shown below, they will header and body of the alert - alert : JenkinsTooManyJobsQueued expr : sum(jenkins_queue_size_value) > 5 for : 1m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} too many jobs queued\" description : \"{{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs stuck in the queue\"","title":"Alert Example"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#alertmanager-configuration","text":"We use Alertmanager for what to do with alerts once they happen. We configure this in the same prom-values.yaml file, in this under alertmanagerFiles . alertmanager.yml . We can create different routes that match on labels or other values. For simplicity sake - this guide is not on Alertmanager's capabilities - we stick to the most straightforward example without any such matching or grouping. For more information on configuring routes, please read the Prometheus configuration documentation . alertmanagerFiles : alertmanager.yml : global : {} route : group_by : [ alertname , app_kubernetes_io_instance ] receiver : default receivers : - name : default slack_configs : - api_url : '<REPLACE_WITH_YOUR_SLACK_API_ENDPOINT>' username : 'Alertmanager' channel : '#notify' send_resolved : true title : \"{{ .CommonAnnotations.summary }} \" text : \"{{ .CommonAnnotations.description }} {{ .CommonLabels.app_kubernetes_io_instance}} \" title_link : http://my-prometheus.com/alerts","title":"Alertmanager Configuration"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#group-alerts","text":"You can group alerts if they are similar or the same with different trigger values (warning vs critical). serverFiles : alerts : groups : - name : healthcheck rules : - alert : JenkinsHealthScoreToLow # alert info - alert : JenkinsTooSlowHealthCheck # alert info - name : jobs rules : - alert : JenkinsTooManyJobsQueued # alert info - alert : JenkinsTooManyJobsStuckInQueue # alert info","title":"Group Alerts"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#the-alerts","text":"Behold, my awesome - eh, simple example - alerts. These are by no means the best alerts to create and are by no means alerts you should directly put into production. Please see them as examples to learn from! Caution One thing to note especially, the values for the exp and for are generally set very low. This is intentional, so they are easy to copy past and test. They should be relatively easy to trigger so you can learn about the relationship between the situation in your master and the alert firing.","title":"The Alerts"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#too-many-jobs-queued","text":"If there are too many Jobs queued in the Jenkins Master. This event fires if there's more than 10 jobs in the queue for at least 10 minutes. - alert : JenkinsTooManyJobsQueued expr : sum(jenkins_queue_size_value) > 10 for : 10m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} too many jobs queued\" description : \"{{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs stuck in the queue\"","title":"Too Many Jobs Queued"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#jobs-stuck-in-queue","text":"Sometimes Jobs depend on other Jobs, which means they're not just in the queue, they're stuck in the queue. - alert : JenkinsTooManyJobsStuckInQueue expr : sum(jenkins_queue_stuck_value) by (app_kubernetes_io_instance) > 5 for : 5m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} too many jobs queued\" description : \" {{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs in queue\"","title":"Jobs Stuck In Queue"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#jobs-waiting-too-long-to-start","text":"If Jobs are generally waiting a long time to start, waiting for a build agent to be available or otherwise, we want to know. This value is not very useful - although not completely useless - if you only have PodTemplates as build agents. When you use PodTemplates, this value is the time between the job being scheduled and when the Pod is scheduled in Kubernetes. - alert : JenkinsWaitingTooMuchOnJobStart expr : sum (jenkins_job_waiting_duration) by (app_kubernetes_io_instance) > 0.05 for : 1m labels : severity : notify annotations : summary : \"{{ $labels.app_kubernetes_io_instance }} waits too long for jobs\" description : \"{{ $labels.app_kubernetes_io_instance }} is waiting on average {{ $value }} seconds to start a job\"","title":"Jobs Waiting Too Long To Start"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#health-score-1","text":"By default, each Jenkins Master has a health check consisting out of four values. Some plugins will add an entry, such as the CloudBees ElasticSearch Reporter for CloudBees Core. This values range from 0-1, and likely will show 0.25 , 0.50 , 0.75 and 1 as values. - alert : JenkinsHealthScoreToLow expr : sum(jenkins_health_check_score) by (app_kubernetes_io_instance) < 1 for : 5m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} has a to low health score\" description : \" {{ $labels.app_kubernetes_io_instance }} a health score lower than 100%\"","title":"health score &lt; 1"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#ingress-too-slow","text":"This alert looks at the ingress controller request duration. It fires if the request duration in 0.25 seconds or faster is not achieved for the 95% percentile. - alert : AppTooSlow expr : sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{le=\"0.25\"}[5m])) by (ingress) / sum(rate(nginx_ingress_controller_request_duration_seconds_count[5m])) by (ingress) < 0.95 for : 5m labels : severity : notify annotations : summary : \"Application - {{ $labels.ingress }} - is too slow\" description : \" {{ $labels.ingress }} - More then 5% of requests are slower than 0.25s\"","title":"Ingress Too Slow"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#http-requests-too-slow","text":"These are the HTTP requests in Jenkins' webserver itself. We should hold this by must stricter standards than the Ingress controller - which goes through many more layers. - alert : JenkinsTooSlow expr : sum(http_requests{quantile=\"0.99\"} ) by (app_kubernetes_io_instance) > 1 for : 3m labels : severity : notify annotations : summary : \"{{ $labels.app_kubernetes_io_instance }} is too slow\" description : \"{{ $labels.app_kubernetes_io_instance }} More then 1% of requests are slower than 1s (request time: {{ $value }})\"","title":"HTTP Requests Too Slow"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#too-many-plugin-updates","text":"I always prefer having my instance up-to-date, don't you? So why not send an alert if there's more than X number of plugins waiting for an update. - alert : JenkinsTooManyPluginsNeedUpate expr : sum(jenkins_plugins_withUpdate) by (app_kubernetes_io_instance) > 3 for : 1m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} too many plugins updates\" description : \" {{ $labels.app_kubernetes_io_instance }} has {{ $value }} plugins that require an update\"","title":"Too Many Plugin Updates"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#file-descriptor-ratio-40","text":"According to CloudBees' documentation, the File Descriptor Ratio should not exceed 40%. Warning I don't truly know the correct value level of this metric. So wether this should be 0.0040 or 0.40 I'm not sure. Also, does this make sense in Containers with remote storage? So before you put this in production, please re-evaluate this! - alert : JenkinsToManyOpenFiles expr : sum(vm_file_descriptor_ratio) by (app_kubernetes_io_instance) > 0.040 for : 5m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} has a to many open files\" description : \" {{ $labels.app_kubernetes_io_instance }} instance has used {{ $value }} of available open files\"","title":"File Descriptor Ratio &gt; 40%"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#job-success-ratio-50","text":"Please, please do not use Job success ratios to punish people. But if it is at all possible - which it almost certainly is - keep a respectable level of success. When practicing Continuous Integration, a broken build is a stop the world event, fix it before moving on. 100% success rate should be strived for. It is ok, not to achieve it, yet, one should be as close as possible and not let broken builds rot. - alert : JenkinsTooLowJobSuccessRate expr : sum(jenkins_runs_success_total) by (app_kubernetes_io_instance) / sum(jenkins_runs_total_total) by (app_kubernetes_io_instance) < 0.5 for : 5m labels : severity : notify annotations : summary : \"{{$labels.app_kubernetes_io_instance}} has a too low job success rate\" description : \"{{$labels.app_kubernetes_io_instance}} instance has less than 50% of jobs being successful\"","title":"Job Success Ratio &lt; 50%"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#offline-nodes-5-over-10-minutes","text":"Having nodes offline for quite some time is usually a bad sign. It can be a static agent that can be enabled or reconnect at will, so it isn't bad on its own. Having multiple offline for a long period is likely an issue somewhere, though. - alert : JenkinsTooManyOfflineNodes expr : sum(jenkins_node_offline_value) by (app_kubernetes_io_instance) > 5 for : 10m labels : severity : notify annotations : summary : \"{{ $labels.app_kubernetes_io_instance }} has a too many offline nodes\" description : \"{{ $labels.app_kubernetes_io_instance }} has {{ $value }} nodes that are offline for some time (5 minutes)\"","title":"Offline nodes &gt; 5 over 10 minutes"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#healtcheck-duration-0002","text":"The health check within Jenkins is talking to itself. This means it is generally really fast. We should be very very strict here, if Jenkins start having trouble measuring its own health, it is a first sign of trouble. - alert : JenkinsTooSlowHealthCheck expr : sum(jenkins_health_check_duration{quantile=\"0.999\"}) by (app_kubernetes_io_instance) > 0.001 for : 1m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} responds too slow to health check\" description : \" {{ $labels.app_kubernetes_io_instance }} is responding too slow to the regular health check\"","title":"healtcheck duration &gt; 0.002"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#gc-throughput-too-low","text":"Ok, here I am on thin ice. I'm not a JVM expert, so this is just an inspiration. I do not know what would be a reasonable value for triggering an alert here. I'd say, test it! - alert : JenkinsTooManyPluginsNeedUpate expr : 1 - sum(vm_gc_G1_Young_Generation_time)by (app_kubernetes_io_instance) / sum (vm_uptime_milliseconds) by (app_kubernetes_io_instance) < 0.99 for : 30m labels : severity : notify annotations : summary : \"{{ $labels.instance }} too low GC throughput\" description : \"{{ $labels.instance }} has too low Garbage Collection throughput\"","title":"GC ThroughPut Too Low"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#vm-heap-usage-ratio-70","text":"According to the CloudBees guide on tuning the JVM - which redirects to Oracle - the ration of JVM Heap memory usage should not exceed about 60%. So if we get over 70% for quite some time, expect trouble. As with any of these values, please do not take my word on it, and understand it yourself. - alert : JenkinsVMMemoryRationTooHigh expr : sum(vm_memory_heap_usage) by (app_kubernetes_io_instance) > 0.70 for : 3m labels : severity : notify annotations : summary : \"{{$labels.app_kubernetes_io_instance}} too high memory ration\" description : \"{{$labels.app_kubernetes_io_instance}} has a too high VM memory ration\"","title":"vm heap usage ratio &gt; 70%"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#uptime-less-than-two-hours","text":"I absolutely love servers that have excellent uptime. Running services in Containers makes that a thing of the past, such a shame. Still, I'd like my applications - such as Jenkins - to be up for reasonable lengths of time. In this case we can get notifications on Masters that have restart - for example, when OOMKilled by Kubernetes. We also get an alert when a new Master is created, which if there's selfservice involved is a nice bonus. - alert : JenkinsNewOrRestarted expr : sum(vm_uptime_milliseconds) by (app_kubernetes_io_instance) / 3600000 < 2 for : 3m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} has low uptime\" description : \" {{ $labels.app_kubernetes_io_instance }} has low uptime and was either restarted or is a new instance (uptime: {{ $value }} hours)\"","title":"Uptime Less Than Two Hours"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#full-example","text":"server : ingress : enabled : true annotations : ingress.kubernetes.io/ssl-redirect : \"false\" nginx.ingress.kubernetes.io/ssl-redirect : \"false\" resources : limits : cpu : 100m memory : 1000Mi requests : cpu : 10m memory : 500Mi alertmanager : ingress : enabled : true annotations : ingress.kubernetes.io/ssl-redirect : \"false\" nginx.ingress.kubernetes.io/ssl-redirect : \"false\" resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi kubeStateMetrics : resources : limits : cpu : 10m memory : 50Mi requests : cpu : 5m memory : 25Mi nodeExporter : resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi pushgateway : resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi serverFiles : alerts : groups : - name : jobs rules : - alert : JenkinsTooManyJobsQueued expr : sum(jenkins_queue_size_value) > 5 for : 1m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} too many jobs queued\" description : \"{{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs stuck in the queue\" - alert : JenkinsTooManyJobsStuckInQueue expr : sum(jenkins_queue_stuck_value) by (app_kubernetes_io_instance) > 5 for : 1m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} too many jobs queued\" description : \" {{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs in queue\" - alert : JenkinsWaitingTooMuchOnJobStart expr : sum (jenkins_job_waiting_duration) by (app_kubernetes_io_instance) > 0.05 for : 1m labels : severity : notify annotations : summary : \"{{ $labels.app_kubernetes_io_instance }} waits too long for jobs\" description : \"{{ $labels.app_kubernetes_io_instance }} is waiting on average {{ $value }} seconds to start a job\" - alert : JenkinsTooLowJobSuccessRate expr : sum(jenkins_runs_success_total) by (app_kubernetes_io_instance) / sum(jenkins_runs_total_total) by (app_kubernetes_io_instance) < 0.60 for : 1m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} has a too low job success rate\" description : \" {{ $labels.app_kubernetes_io_instance }} instance has {{ $value }}% of jobs being successful\" - name : uptime rules : - alert : JenkinsNewOrRestarted expr : sum(vm_uptime_milliseconds) by (app_kubernetes_io_instance) / 3600000 < 2 for : 3m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} has low uptime\" description : \" {{ $labels.app_kubernetes_io_instance }} has low uptime and was either restarted or is a new instance (uptime: {{ $value }} hours)\" - name : plugins rules : - alert : JenkinsTooManyPluginsNeedUpate expr : sum(jenkins_plugins_withUpdate) by (app_kubernetes_io_instance) > 3 for : 1m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} too many plugins updates\" description : \" {{ $labels.app_kubernetes_io_instance }} has {{ $value }} plugins that require an update\" - name : jvm rules : - alert : JenkinsToManyOpenFiles expr : sum(vm_file_descriptor_ratio) by (app_kubernetes_io_instance) > 0.040 for : 5m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} has a to many open files\" description : \" {{ $labels.app_kubernetes_io_instance }} instance has used {{ $value }} of available open files\" - alert : JenkinsVMMemoryRationTooHigh expr : sum(vm_memory_heap_usage) by (app_kubernetes_io_instance) > 0.70 for : 3m labels : severity : notify annotations : summary : \"{{$labels.app_kubernetes_io_instance}} too high memory ration\" description : \"{{$labels.app_kubernetes_io_instance}} has a too high VM memory ration\" - alert : JenkinsTooManyPluginsNeedUpate expr : 1 - sum(vm_gc_G1_Young_Generation_time)by (app_kubernetes_io_instance) / sum (vm_uptime_milliseconds) by (app_kubernetes_io_instance) < 0.99 for : 30m labels : severity : notify annotations : summary : \"{{ $labels.instance }} too low GC throughput\" description : \"{{ $labels.instance }} has too low Garbage Collection throughput\" - name : web rules : - alert : JenkinsTooSlow expr : sum(http_requests{quantile=\"0.99\"} ) by (app_kubernetes_io_instance) > 1 for : 3m labels : severity : notify annotations : summary : \"{{ $labels.app_kubernetes_io_instance }} is too slow\" description : \"{{ $labels.app_kubernetes_io_instance }} More then 1% of requests are slower than 1s (request time: {{ $value }})\" - alert : AppTooSlow expr : sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{le=\"0.25\"}[5m])) by (ingress) / sum(rate(nginx_ingress_controller_request_duration_seconds_count[5m])) by (ingress) < 0.95 for : 5m labels : severity : notify annotations : summary : \"Application - {{ $labels.ingress }} - is too slow\" description : \" {{ $labels.ingress }} - More then 5% of requests are slower than 0.25s\" - name : healthcheck rules : - alert : JenkinsHealthScoreToLow expr : sum(jenkins_health_check_score) by (app_kubernetes_io_instance) < 1 for : 5m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} has a to low health score\" description : \" {{ $labels.app_kubernetes_io_instance }} a health score lower than 100%\" - alert : JenkinsTooSlowHealthCheck expr : sum(jenkins_health_check_duration{quantile=\"0.999\"}) by (app_kubernetes_io_instance) > 0.001 for : 1m labels : severity : notify annotations : summary : \" {{ $labels.app_kubernetes_io_instance }} responds too slow to health check\" description : \" {{ $labels.app_kubernetes_io_instance }} is responding too slow to the regular health check\" - name : nodes rules : - alert : JenkinsTooManyOfflineNodes expr : sum(jenkins_node_offline_value) by (app_kubernetes_io_instance) > 3 for : 1m labels : severity : notify annotations : summary : \"{{ $labels.app_kubernetes_io_instance }} has a too many offline nodes\" description : \"{{ $labels.app_kubernetes_io_instance }} has {{ $value }} nodes that are offline for some time (5 minutes)\" alertmanagerFiles : alertmanager.yml : global : {} route : group_by : [ alertname , app_kubernetes_io_instance ] receiver : default receivers : - name : default slack_configs : - api_url : '<REPLACE_WITH_YOUR_SLACK_API_URL>' username : 'Alertmanager' channel : '#notify' send_resolved : true title : \"{{ .CommonAnnotations.summary }} \" text : \"{{ .CommonAnnotations.description }} {{ .CommonLabels.app_kubernetes_io_instance}} \" title_link : http://my-prometheus.com/alerts","title":"Full Example"},{"location":"blogs/monitor-jenkins-on-k8s/cloudbees/","text":"CloudBees Core \u00b6 add prometheus plugin to team-recipe update CJOC's Master Provisioning with prometheus annotations apiVersion : \"apps/v1\" kind : \"StatefulSet\" spec : template : metadata : annotations : prometheus.io/path : /${name}/prometheus prometheus.io/port : \"8080\" prometheus.io/scrape : \"true\" labels : app.kubernetes.io/component : Managed-Master app.kubernetes.io/instance : ${name} app.kubernetes.io/managed-by : CloudBees-Core-Cloud-Operations-Center app.kubernetes.io/name : ${name}","title":"CloudBees"},{"location":"blogs/monitor-jenkins-on-k8s/cloudbees/#cloudbees-core","text":"add prometheus plugin to team-recipe update CJOC's Master Provisioning with prometheus annotations apiVersion : \"apps/v1\" kind : \"StatefulSet\" spec : template : metadata : annotations : prometheus.io/path : /${name}/prometheus prometheus.io/port : \"8080\" prometheus.io/scrape : \"true\" labels : app.kubernetes.io/component : Managed-Master app.kubernetes.io/instance : ${name} app.kubernetes.io/managed-by : CloudBees-Core-Cloud-Operations-Center app.kubernetes.io/name : ${name}","title":"CloudBees Core"},{"location":"blogs/monitor-jenkins-on-k8s/dashboard/","text":"Grafana Dashboard \u00b6 Example Screenshot \u00b6 Grafana Variables \u00b6 cluster type : datasource datasource type : prometheus node type : query query : label_values(kube_node_info{component=\"kube-state-metrics\"}, node) label : K8S Node multivalue include all namespace type : query query : label_values(jenkins_health_check_duration, kubernetes_namespace) label : Namespace multivalue include all instance type : query query : label_values(jenkins_health_check_duration, app_kubernetes_io_instance) label : Master multivalue include all Dashboard json \u00b6 { \"annotations\" : { \"list\" : [ { \"builtIn\" : 1 , \"datasource\" : \"-- Grafana --\" , \"enable\" : true , \"hide\" : true , \"iconColor\" : \"rgba(0, 211, 255, 1)\" , \"name\" : \"Annotations & Alerts\" , \"type\" : \"dashboard\" } ] }, \"description\" : \"Dashboard for when you are using multiple Jenkins Masters\" , \"editable\" : true , \"gnetId\" : null , \"graphTooltip\" : 1 , \"id\" : 9 , \"iteration\" : 1565906968208 , \"links\" : [], \"panels\" : [ { \"collapsed\" : false , \"gridPos\" : { \"h\" : 1 , \"w\" : 24 , \"x\" : 0 , \"y\" : 0 }, \"id\" : 20 , \"panels\" : [], \"title\" : \"Performance\" , \"type\" : \"row\" }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"decimals\" : 3 , \"description\" : \"The ratio of ok (200) request out of all requests.\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 6 , \"x\" : 0 , \"y\" : 1 }, \"id\" : 36 , \"legend\" : { \"alignAsTable\" : true , \"avg\" : false , \"current\" : true , \"max\" : false , \"min\" : false , \"rightSide\" : true , \"show\" : true , \"total\" : false , \"values\" : true }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(http_responseCodes_ok_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) \\nby (app_kubernetes_io_instance) / \\nsum(http_requests_count{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) \\nby (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [ { \"colorMode\" : \"warning\" , \"fill\" : true , \"line\" : true , \"op\" : \"lt\" , \"value\" : 0.991 , \"yaxis\" : \"left\" }, { \"colorMode\" : \"critical\" , \"fill\" : true , \"line\" : true , \"op\" : \"lt\" , \"value\" : 0.981 , \"yaxis\" : \"left\" } ], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Good HTTP Request Ratio\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"decimals\" : 3 , \"format\" : \"percentunit\" , \"label\" : null , \"logBase\" : 1 , \"max\" : \"1\" , \"min\" : \"0.95\" , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : false } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"decimals\" : 0 , \"description\" : \"Http Server Errors (500)\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 5 , \"x\" : 6 , \"y\" : 1 }, \"id\" : 38 , \"legend\" : { \"alignAsTable\" : true , \"avg\" : false , \"current\" : true , \"hideEmpty\" : true , \"hideZero\" : false , \"max\" : false , \"min\" : false , \"rightSide\" : true , \"show\" : true , \"total\" : false , \"values\" : true }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"http_responseCodes_serverError_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [ { \"colorMode\" : \"warning\" , \"fill\" : true , \"line\" : true , \"op\" : \"gt\" , \"value\" : 1 , \"yaxis\" : \"left\" } ], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Server Errors\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 6 , \"x\" : 11 , \"y\" : 1 }, \"id\" : 42 , \"interval\" : \"1m\" , \"legend\" : { \"avg\" : false , \"current\" : false , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum (jenkins_job_waiting_duration{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"instant\" : false , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Job Wait Duration\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"s\" , \"label\" : null , \"logBase\" : 2 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : false } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 6 , \"x\" : 17 , \"y\" : 1 }, \"id\" : 44 , \"interval\" : \"1m\" , \"legend\" : { \"avg\" : false , \"current\" : false , \"hideEmpty\" : false , \"hideZero\" : false , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(jenkins_job_building_duration{quantile=\\\"0.5\\\",kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})\\nby (app_kubernetes_io_instance) / 60\" , \"format\" : \"time_series\" , \"instant\" : false , \"interval\" : \"\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"p50-{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" }, { \"expr\" : \"sum(jenkins_node_builds{quantile=\\\"0.999\\\",kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})\\nby (app_kubernetes_io_instance) / 60\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"p999-{{app_kubernetes_io_instance}}\" , \"refId\" : \"B\" } ], \"thresholds\" : [], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Avg Build Duration Minutes\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"m\" , \"label\" : null , \"logBase\" : 2 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"decimals\" : 1 , \"format\" : \"m\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : false } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"description\" : \"How long the health check takes to complete at the 99th percentile.\\nHigher numbers signify problems\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 7 , \"x\" : 0 , \"y\" : 7 }, \"id\" : 18 , \"interval\" : \"1m\" , \"legend\" : { \"avg\" : false , \"current\" : false , \"hideEmpty\" : true , \"hideZero\" : true , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null as zero\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(jenkins_health_check_duration{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\", quantile=\\\"0.99\\\"}) \\n by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Health Check Duration (99%)\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"description\" : \"The 99% percentile of HTTP Requests handled by Jenkins masters.\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 8 , \"x\" : 7 , \"y\" : 7 }, \"id\" : 52 , \"legend\" : { \"avg\" : false , \"current\" : false , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(http_requests{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\",quantile=\\\"0.99\\\"} ) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"HTTP Request Duration (99%)\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"description\" : \"Shows performance of Ingress Controller connection that lasts longer than 250 milliseconds\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 8 , \"x\" : 15 , \"y\" : 7 }, \"id\" : 32 , \"legend\" : { \"avg\" : false , \"current\" : false , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(rate(\\n nginx_ingress_controller_request_duration_seconds_bucket{\\n le=\\\"0.25\\\",\\n namespace=~\\\"$namespace\\\",\\n ingress=~\\\"jenkins*\\\"\\n }[5m]\\n)) \\nby (ingress) / \\nsum(rate(\\n nginx_ingress_controller_request_duration_seconds_count{\\n namespace=~\\\"$namespace\\\",\\n ingress=~\\\"jenkins*\\\"\\n }[5m]\\n)) \\nby (ingress) \" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{ingress}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [ { \"colorMode\" : \"custom\" , \"fill\" : false , \"fillColor\" : \"rgba(50, 116, 217, 0.2)\" , \"line\" : true , \"lineColor\" : \"#B877D9\" , \"op\" : \"gt\" , \"value\" : 0.5 , \"yaxis\" : \"left\" }, { \"colorMode\" : \"warning\" , \"fill\" : false , \"fillColor\" : \"rgba(50, 116, 217, 0.2)\" , \"line\" : true , \"lineColor\" : \"rgba(31, 96, 196, 0.6)\" , \"op\" : \"gt\" , \"value\" : 1.5 , \"yaxis\" : \"left\" }, { \"colorMode\" : \"critical\" , \"fill\" : false , \"line\" : true , \"op\" : \"gt\" , \"value\" : 3 , \"yaxis\" : \"left\" } ], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Ingress Perfomance\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"collapsed\" : false , \"gridPos\" : { \"h\" : 1 , \"w\" : 24 , \"x\" : 0 , \"y\" : 13 }, \"id\" : 12 , \"panels\" : [], \"title\" : \"General Info\" , \"type\" : \"row\" }, { \"cacheTimeout\" : null , \"colorBackground\" : true , \"colorPostfix\" : false , \"colorValue\" : false , \"colors\" : [ \"#F2495C\" , \"#FFCB7D\" , \"#5794F2\" ], \"description\" : \"Amount of Masters healthy\" , \"format\" : \"none\" , \"gauge\" : { \"maxValue\" : 100 , \"minValue\" : 0 , \"show\" : false , \"thresholdLabels\" : false , \"thresholdMarkers\" : true }, \"gridPos\" : { \"h\" : 6 , \"w\" : 4 , \"x\" : 0 , \"y\" : 14 }, \"id\" : 26 , \"interval\" : null , \"links\" : [], \"mappingType\" : 1 , \"mappingTypes\" : [ { \"name\" : \"value to text\" , \"value\" : 1 }, { \"name\" : \"range to text\" , \"value\" : 2 } ], \"maxDataPoints\" : 100 , \"nullPointMode\" : \"connected\" , \"nullText\" : null , \"options\" : {}, \"pluginVersion\" : \"6.2.4\" , \"postfix\" : \"\" , \"postfixFontSize\" : \"50%\" , \"prefix\" : \"\" , \"prefixFontSize\" : \"50%\" , \"rangeMaps\" : [ { \"from\" : \"null\" , \"text\" : \"N/A\" , \"to\" : \"null\" } ], \"sparkline\" : { \"fillColor\" : \"rgba(31, 118, 189, 0.18)\" , \"full\" : false , \"lineColor\" : \"rgb(31, 120, 193)\" , \"show\" : false }, \"tableColumn\" : \"\" , \"targets\" : [ { \"expr\" : \"sum(jenkins_health_check_score{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"\" , \"refId\" : \"A\" } ], \"thresholds\" : \"0,1\" , \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Number of Masters\" , \"type\" : \"singlestat\" , \"valueFontSize\" : \"200%\" , \"valueMaps\" : [ { \"op\" : \"=\" , \"text\" : \"N/A\" , \"value\" : \"null\" } ], \"valueName\" : \"current\" }, { \"cacheTimeout\" : null , \"columns\" : [ { \"text\" : \"Avg\" , \"value\" : \"avg\" } ], \"description\" : \"Dropwizard based Health Score derived from other metrics\" , \"fontSize\" : \"100%\" , \"gridPos\" : { \"h\" : 6 , \"w\" : 5 , \"x\" : 4 , \"y\" : 14 }, \"id\" : 40 , \"links\" : [], \"options\" : {}, \"pageSize\" : 5 , \"pluginVersion\" : \"6.2.4\" , \"scroll\" : true , \"showHeader\" : true , \"sort\" : { \"col\" : 0 , \"desc\" : true }, \"styles\" : [ { \"alias\" : \"Score\" , \"colorMode\" : \"row\" , \"colors\" : [ \"#5794F2\" , \"#FF9830\" , \"#F2495C\" ], \"decimals\" : 0 , \"pattern\" : \"/.*/\" , \"thresholds\" : [ \"90\" , \"95\" ], \"type\" : \"number\" , \"unit\" : \"percentunit\" } ], \"targets\" : [ { \"expr\" : \"jenkins_health_check_score{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\" , \"format\" : \"time_series\" , \"instant\" : true , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Health Score\" , \"transform\" : \"timeseries_aggregations\" , \"type\" : \"table\" }, { \"gridPos\" : { \"h\" : 6 , \"w\" : 14 , \"x\" : 9 , \"y\" : 14 }, \"id\" : 46 , \"links\" : [], \"options\" : { \"fieldOptions\" : { \"calcs\" : [ \"last\" ], \"defaults\" : { \"decimals\" : 1 , \"max\" : 100 , \"min\" : 0 , \"title\" : \"\" , \"unit\" : \"percent\" }, \"mappings\" : [], \"override\" : {}, \"thresholds\" : [ { \"color\" : \"red\" , \"index\" : 0 , \"value\" : null }, { \"color\" : \"purple\" , \"index\" : 1 , \"value\" : 50 }, { \"color\" : \"blue\" , \"index\" : 2 , \"value\" : 75 } ], \"values\" : false }, \"orientation\" : \"auto\" , \"showThresholdLabels\" : false , \"showThresholdMarkers\" : true }, \"pluginVersion\" : \"6.2.4\" , \"targets\" : [ { \"expr\" : \"sum(jenkins_runs_success_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})\\nby (app_kubernetes_io_instance) /\\nsum(jenkins_runs_total_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})\\nby (app_kubernetes_io_instance) * 100\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Job Success Ratio\" , \"type\" : \"gauge\" }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"decimals\" : 0 , \"description\" : \"Amount of Jobs Currenty in the Queue\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 7 , \"x\" : 0 , \"y\" : 20 }, \"id\" : 30 , \"legend\" : { \"alignAsTable\" : false , \"avg\" : false , \"current\" : false , \"hideEmpty\" : false , \"hideZero\" : true , \"max\" : false , \"min\" : false , \"rightSide\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(jenkins_queue_size_value{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [ { \"colorMode\" : \"critical\" , \"fill\" : false , \"line\" : true , \"op\" : \"gt\" , \"value\" : 10 , \"yaxis\" : \"left\" }, { \"colorMode\" : \"warning\" , \"fill\" : false , \"line\" : true , \"op\" : \"gt\" , \"value\" : 5 , \"yaxis\" : \"left\" } ], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Job Queue\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"cacheTimeout\" : null , \"dashLength\" : 10 , \"dashes\" : false , \"decimals\" : 0 , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 8 , \"x\" : 7 , \"y\" : 20 }, \"hideTimeOverride\" : false , \"id\" : 50 , \"interval\" : \"\" , \"legend\" : { \"alignAsTable\" : true , \"avg\" : false , \"current\" : false , \"hideEmpty\" : true , \"hideZero\" : true , \"max\" : true , \"min\" : false , \"rightSide\" : true , \"show\" : true , \"total\" : false , \"values\" : true }, \"lines\" : true , \"linewidth\" : 2 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pluginVersion\" : \"6.2.4\" , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(increase(jenkins_runs_total_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}[24h])) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"interval\" : \"\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}-total\" , \"refId\" : \"A\" }, { \"expr\" : \"sum(increase(jenkins_runs_failure_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}[24h])) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"interval\" : \"\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}-failed\" , \"refId\" : \"B\" }, { \"expr\" : \"sum(increase(jenkins_runs_aborted_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}[24h])) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"interval\" : \"\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}-aborted\" , \"refId\" : \"C\" } ], \"thresholds\" : [], \"timeFrom\" : \"12h\" , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Runs Per Day\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"decimals\" : 0 , \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"decimals\" : 0 , \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"description\" : \"Active Build Runs\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 8 , \"x\" : 15 , \"y\" : 20 }, \"id\" : 56 , \"legend\" : { \"avg\" : false , \"current\" : false , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(jenkins_executor_in_use_history{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Build Runs\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"cards\" : { \"cardPadding\" : null , \"cardRound\" : null }, \"color\" : { \"cardColor\" : \"#F2495C\" , \"colorScale\" : \"sqrt\" , \"colorScheme\" : \"interpolateSpectral\" , \"exponent\" : 0.8 , \"max\" : null , \"min\" : 0 , \"mode\" : \"spectrum\" }, \"dataFormat\" : \"tsbuckets\" , \"description\" : \"Heatmap of when Jobs are scheduled\" , \"gridPos\" : { \"h\" : 7 , \"w\" : 23 , \"x\" : 0 , \"y\" : 26 }, \"heatmap\" : {}, \"hideTimeOverride\" : false , \"hideZeroBuckets\" : false , \"highlightCards\" : true , \"id\" : 58 , \"interval\" : \"5m\" , \"legend\" : { \"show\" : true }, \"links\" : [], \"options\" : {}, \"reverseYBuckets\" : false , \"targets\" : [ { \"expr\" : \"sum(increase(jenkins_runs_total_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}[2h])) by (app_kubernetes_io_instance)\" , \"format\" : \"heatmap\" , \"instant\" : false , \"interval\" : \"\" , \"intervalFactor\" : 10 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : \"168h\" , \"timeShift\" : null , \"title\" : \"Job Runs Heatmap\" , \"tooltip\" : { \"show\" : true , \"showHistogram\" : false }, \"type\" : \"heatmap\" , \"xAxis\" : { \"show\" : true }, \"xBucketNumber\" : null , \"xBucketSize\" : null , \"yAxis\" : { \"decimals\" : 0 , \"format\" : \"short\" , \"logBase\" : 1 , \"max\" : null , \"min\" : \"0\" , \"show\" : true , \"splitFactor\" : null }, \"yBucketBound\" : \"auto\" , \"yBucketNumber\" : null , \"yBucketSize\" : null }, { \"columns\" : [ { \"text\" : \"Current\" , \"value\" : \"current\" } ], \"description\" : \"Jenkins Master Plugin Count\" , \"fontSize\" : \"100%\" , \"gridPos\" : { \"h\" : 5 , \"w\" : 7 , \"x\" : 0 , \"y\" : 33 }, \"id\" : 14 , \"interval\" : \"1m\" , \"links\" : [], \"options\" : {}, \"pageSize\" : null , \"pluginVersion\" : \"6.2.4\" , \"scroll\" : true , \"showHeader\" : true , \"sort\" : { \"col\" : 0 , \"desc\" : true }, \"styles\" : [ { \"alias\" : \"Time\" , \"dateFormat\" : \"YYYY-MM-DD HH:mm:ss\" , \"pattern\" : \"Time\" , \"type\" : \"date\" }, { \"alias\" : \"Master\" , \"colorMode\" : \"row\" , \"colors\" : [ \"#5794F2\" , \"#FF9830\" , \"#F2495C\" ], \"decimals\" : 0 , \"link\" : false , \"pattern\" : \"/.*/\" , \"thresholds\" : [ \"95\" , \"130\" ], \"type\" : \"number\" , \"unit\" : \"short\" } ], \"targets\" : [ { \"expr\" : \"jenkins_plugins_active{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\" , \"format\" : \"time_series\" , \"instant\" : true , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Plugin Count\" , \"transform\" : \"timeseries_to_rows\" , \"type\" : \"table\" }, { \"columns\" : [ { \"text\" : \"Current\" , \"value\" : \"current\" } ], \"description\" : \"Amount of plugins that are available for updating\" , \"fontSize\" : \"100%\" , \"gridPos\" : { \"h\" : 5 , \"w\" : 8 , \"x\" : 7 , \"y\" : 33 }, \"id\" : 22 , \"links\" : [], \"options\" : {}, \"pageSize\" : null , \"scroll\" : true , \"showHeader\" : true , \"sort\" : { \"col\" : 0 , \"desc\" : true }, \"styles\" : [ { \"alias\" : \"Time\" , \"dateFormat\" : \"YYYY-MM-DD HH:mm:ss\" , \"pattern\" : \"Time\" , \"type\" : \"date\" }, { \"alias\" : \"\" , \"colorMode\" : \"row\" , \"colors\" : [ \"#5794F2\" , \"#FF9830\" , \"#F2495C\" ], \"decimals\" : 0 , \"pattern\" : \"/.*/\" , \"thresholds\" : [ \"3\" , \"10\" ], \"type\" : \"number\" , \"unit\" : \"short\" } ], \"targets\" : [ { \"expr\" : \"sum(jenkins_plugins_withUpdate{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"instant\" : true , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Plugin Updates Available\" , \"transform\" : \"timeseries_aggregations\" , \"type\" : \"table\" }, { \"columns\" : [ { \"text\" : \"Current\" , \"value\" : \"current\" }, { \"text\" : \"Max\" , \"value\" : \"max\" } ], \"description\" : \"Jenkins Master Job Count\" , \"fontSize\" : \"100%\" , \"gridPos\" : { \"h\" : 5 , \"w\" : 8 , \"x\" : 15 , \"y\" : 33 }, \"id\" : 16 , \"links\" : [], \"options\" : {}, \"pageSize\" : null , \"scroll\" : true , \"showHeader\" : true , \"sort\" : { \"col\" : 0 , \"desc\" : true }, \"styles\" : [ { \"alias\" : \"Time\" , \"dateFormat\" : \"YYYY-MM-DD HH:mm:ss\" , \"pattern\" : \"Time\" , \"type\" : \"date\" }, { \"alias\" : \"\" , \"colorMode\" : \"row\" , \"colors\" : [ \"#F2495C\" , \"#FF9830\" , \"#5794F2\" ], \"decimals\" : 0 , \"pattern\" : \"/.*/\" , \"thresholds\" : [ \"1\" ], \"type\" : \"number\" , \"unit\" : \"short\" } ], \"targets\" : [ { \"expr\" : \"jenkins_job_count_value{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\" , \"format\" : \"time_series\" , \"instant\" : true , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Job Count\" , \"transform\" : \"timeseries_aggregations\" , \"type\" : \"table\" }, { \"columns\" : [ { \"text\" : \"Current\" , \"value\" : \"current\" } ], \"description\" : \"Counts offline build nodes that are connected to this master\" , \"fontSize\" : \"100%\" , \"gridPos\" : { \"h\" : 6 , \"w\" : 7 , \"x\" : 0 , \"y\" : 38 }, \"id\" : 24 , \"links\" : [], \"options\" : {}, \"pageSize\" : null , \"scroll\" : true , \"showHeader\" : true , \"sort\" : { \"col\" : 0 , \"desc\" : true }, \"styles\" : [ { \"alias\" : \"Time\" , \"dateFormat\" : \"YYYY-MM-DD HH:mm:ss\" , \"pattern\" : \"Time\" , \"type\" : \"date\" }, { \"alias\" : \"\" , \"colorMode\" : \"row\" , \"colors\" : [ \"#5794F2\" , \"#B877D9\" , \"#F2495C\" ], \"decimals\" : 2 , \"pattern\" : \"/.*/\" , \"thresholds\" : [ \"1\" , \"3\" ], \"type\" : \"number\" , \"unit\" : \"short\" } ], \"targets\" : [ { \"expr\" : \"sum(jenkins_node_offline_value{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"instant\" : true , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Offline Nodes\" , \"transform\" : \"timeseries_aggregations\" , \"type\" : \"table\" }, { \"cacheTimeout\" : null , \"columns\" : [ { \"text\" : \"Current\" , \"value\" : \"current\" } ], \"description\" : \"Uptime in hours\" , \"fontSize\" : \"100%\" , \"gridPos\" : { \"h\" : 6 , \"w\" : 8 , \"x\" : 7 , \"y\" : 38 }, \"id\" : 6 , \"links\" : [], \"options\" : {}, \"pageSize\" : null , \"pluginVersion\" : \"6.2.4\" , \"scroll\" : true , \"showHeader\" : true , \"sort\" : { \"col\" : null , \"desc\" : false }, \"styles\" : [ { \"alias\" : \"Time\" , \"dateFormat\" : \"YYYY-MM-DD HH:mm:ss\" , \"pattern\" : \"Time\" , \"type\" : \"date\" }, { \"alias\" : \"\" , \"colorMode\" : \"row\" , \"colors\" : [ \"#F2495C\" , \"#FF9830\" , \"#5794F2\" ], \"decimals\" : 0 , \"pattern\" : \"/.*/\" , \"thresholds\" : [ \"1\" , \"24\" ], \"type\" : \"number\" , \"unit\" : \"short\" } ], \"targets\" : [ { \"expr\" : \"vm_uptime_milliseconds{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"} / 3600000 \" , \"format\" : \"time_series\" , \"instant\" : true , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Uptime\" , \"transform\" : \"timeseries_aggregations\" , \"type\" : \"table\" }, { \"columns\" : [ { \"text\" : \"Current\" , \"value\" : \"current\" } ], \"description\" : \"The current Master executor count, masters should not have executors, so only 0 is green.\" , \"fontSize\" : \"100%\" , \"gridPos\" : { \"h\" : 6 , \"w\" : 8 , \"x\" : 15 , \"y\" : 38 }, \"id\" : 34 , \"interval\" : \"\" , \"links\" : [], \"options\" : {}, \"pageSize\" : null , \"scroll\" : true , \"showHeader\" : true , \"sort\" : { \"col\" : 0 , \"desc\" : true }, \"styles\" : [ { \"alias\" : \"Time\" , \"dateFormat\" : \"YYYY-MM-DD HH:mm:ss\" , \"pattern\" : \"Time\" , \"type\" : \"date\" }, { \"alias\" : \"\" , \"colorMode\" : \"row\" , \"colors\" : [ \"#F2495C\" , \"#FF9830\" , \"#5794F2\" ], \"decimals\" : 2 , \"pattern\" : \"/.*/\" , \"thresholds\" : [ \"0\" , \"0\" , \"1\" ], \"type\" : \"number\" , \"unit\" : \"short\" } ], \"targets\" : [ { \"expr\" : \"sum(jenkins_executor_count_value{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"instant\" : true , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Master Executor Count\" , \"transform\" : \"timeseries_aggregations\" , \"type\" : \"table\" }, { \"collapsed\" : false , \"gridPos\" : { \"h\" : 1 , \"w\" : 24 , \"x\" : 0 , \"y\" : 44 }, \"id\" : 2 , \"panels\" : [], \"title\" : \"JVM Metrics\" , \"type\" : \"row\" }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"fill\" : 1 , \"gridPos\" : { \"h\" : 7 , \"w\" : 10 , \"x\" : 0 , \"y\" : 45 }, \"id\" : 48 , \"legend\" : { \"avg\" : false , \"current\" : false , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"1 - sum(vm_gc_G1_Young_Generation_time{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})by (app_kubernetes_io_instance) \\n/ \\nsum (vm_uptime_milliseconds{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"instant\" : false , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [ { \"colorMode\" : \"warning\" , \"fill\" : true , \"line\" : true , \"op\" : \"lt\" , \"value\" : 0.998 , \"yaxis\" : \"left\" }, { \"colorMode\" : \"critical\" , \"fill\" : true , \"line\" : true , \"op\" : \"lt\" , \"value\" : 0.98 , \"yaxis\" : \"left\" } ], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"JVM GC Throughput\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"decimals\" : 5 , \"format\" : \"percentunit\" , \"label\" : null , \"logBase\" : 1 , \"max\" : \"1\" , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : false } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"description\" : \"Ratio of JVM Memory used\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 7 , \"w\" : 13 , \"x\" : 10 , \"y\" : 45 }, \"id\" : 10 , \"legend\" : { \"alignAsTable\" : false , \"avg\" : false , \"current\" : false , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 2 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(vm_memory_heap_usage{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [ { \"colorMode\" : \"critical\" , \"fill\" : true , \"line\" : true , \"op\" : \"gt\" , \"value\" : 0.75 , \"yaxis\" : \"left\" } ], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Memory Ratio\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"percentunit\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : false } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"decimals\" : 4 , \"description\" : \"JVM CPU Load\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 7 , \"w\" : 10 , \"x\" : 0 , \"y\" : 52 }, \"id\" : 4 , \"legend\" : { \"alignAsTable\" : true , \"avg\" : false , \"current\" : true , \"hideEmpty\" : true , \"hideZero\" : true , \"max\" : true , \"min\" : false , \"rightSide\" : true , \"show\" : false , \"total\" : false , \"values\" : true }, \"lines\" : true , \"linewidth\" : 2 , \"links\" : [], \"nullPointMode\" : \"connected\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 1 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : true , \"targets\" : [ { \"expr\" : \"vm_cpu_load{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\" , \"format\" : \"time_series\" , \"instant\" : false , \"intervalFactor\" : 5 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"CPU Load\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : \"\" , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : \"\" , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : false } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"decimals\" : null , \"description\" : \"JVM Memory usage\" , \"fill\" : 2 , \"gridPos\" : { \"h\" : 7 , \"w\" : 13 , \"x\" : 10 , \"y\" : 52 }, \"id\" : 8 , \"legend\" : { \"alignAsTable\" : true , \"avg\" : false , \"current\" : true , \"max\" : true , \"min\" : false , \"rightSide\" : true , \"show\" : true , \"total\" : false , \"values\" : true }, \"lines\" : true , \"linewidth\" : 2 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"vm_memory_total_used{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\" , \"format\" : \"time_series\" , \"intervalFactor\" : 2 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Memory\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 1 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"description\" : \"Lists memory usage of the Pod vs. Kubernetes Requests\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 8 , \"w\" : 10 , \"x\" : 0 , \"y\" : 59 }, \"id\" : 54 , \"legend\" : { \"avg\" : false , \"current\" : false , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"connected\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum (label_join(container_memory_usage_bytes{\\n container_name=\\\"jenkins\\\",\\n namespace=~\\\"$namespace\\\"\\n }, \\n \\\"pod\\\", \\n \\\",\\\", \\n \\\"pod_name\\\"\\n)) by (pod) / \\nsum (kube_pod_container_resource_requests_memory_bytes { \\n container=\\\"jenkins\\\",\\n namespace=~\\\"$namespace\\\"\\n }\\n) by (pod)\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{pod}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Memory Usage vs. Request\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"description\" : \"The ratio of open file descriptors\\nSee: https://support.cloudbees.com/hc/en-us/articles/204246140-Too-many-open-files\" , \"gridPos\" : { \"h\" : 8 , \"w\" : 5 , \"x\" : 10 , \"y\" : 59 }, \"id\" : 28 , \"links\" : [], \"options\" : { \"displayMode\" : \"basic\" , \"fieldOptions\" : { \"calcs\" : [ \"last\" ], \"defaults\" : { \"max\" : 1 , \"min\" : 0 , \"unit\" : \"percentunit\" }, \"mappings\" : [], \"override\" : {}, \"thresholds\" : [ { \"color\" : \"blue\" , \"index\" : 0 , \"value\" : null }, { \"color\" : \"orange\" , \"index\" : 1 , \"value\" : 60 }, { \"color\" : \"red\" , \"index\" : 2 , \"value\" : 80 } ], \"values\" : false }, \"orientation\" : \"horizontal\" }, \"targets\" : [ { \"expr\" : \"vm_file_descriptor_ratio{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\" , \"format\" : \"time_series\" , \"instant\" : true , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"File Descriptor Ratio\" , \"type\" : \"bargauge\" } ], \"refresh\" : \"30s\" , \"schemaVersion\" : 18 , \"style\" : \"dark\" , \"tags\" : [ \"Jenkins\" , \"Prometheus\" ], \"templating\" : { \"list\" : [ { \"current\" : { \"text\" : \"Prometheus\" , \"value\" : \"Prometheus\" }, \"hide\" : 0 , \"includeAll\" : false , \"label\" : null , \"multi\" : false , \"name\" : \"cluster\" , \"options\" : [], \"query\" : \"prometheus\" , \"refresh\" : 1 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"type\" : \"datasource\" }, { \"allValue\" : null , \"current\" : { \"text\" : \"All\" , \"value\" : [ \"$__all\" ] }, \"datasource\" : \"Prometheus\" , \"definition\" : \"label_values(jenkins_health_check_duration, app_kubernetes_io_instance)\" , \"hide\" : 0 , \"includeAll\" : true , \"label\" : \"Master\" , \"multi\" : true , \"name\" : \"instance\" , \"options\" : [], \"query\" : \"label_values(jenkins_health_check_duration, app_kubernetes_io_instance)\" , \"refresh\" : 2 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"sort\" : 1 , \"tagValuesQuery\" : \"\" , \"tags\" : [], \"tagsQuery\" : \"\" , \"type\" : \"query\" , \"useTags\" : false }, { \"allValue\" : null , \"current\" : { \"text\" : \"All\" , \"value\" : \"$__all\" }, \"datasource\" : \"Prometheus\" , \"definition\" : \"label_values(jenkins_health_check_duration, kubernetes_namespace)\" , \"hide\" : 0 , \"includeAll\" : true , \"label\" : \"Namespace\" , \"multi\" : true , \"name\" : \"namespace\" , \"options\" : [], \"query\" : \"label_values(jenkins_health_check_duration, kubernetes_namespace)\" , \"refresh\" : 2 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"sort\" : 1 , \"tagValuesQuery\" : \"\" , \"tags\" : [], \"tagsQuery\" : \"\" , \"type\" : \"query\" , \"useTags\" : false }, { \"allValue\" : null , \"current\" : { \"text\" : \"All\" , \"value\" : \"$__all\" }, \"datasource\" : \"Prometheus\" , \"definition\" : \"label_values(kube_node_info{component=\\\"kube-state-metrics\\\"}, node)\" , \"hide\" : 0 , \"includeAll\" : true , \"label\" : \"K8S Node\" , \"multi\" : true , \"name\" : \"node\" , \"options\" : [], \"query\" : \"label_values(kube_node_info{component=\\\"kube-state-metrics\\\"}, node)\" , \"refresh\" : 1 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"sort\" : 5 , \"tagValuesQuery\" : \"\" , \"tags\" : [], \"tagsQuery\" : \"\" , \"type\" : \"query\" , \"useTags\" : false }, { \"allValue\" : null , \"current\" : { \"text\" : \"jx-production\" , \"value\" : \"jx-production\" }, \"datasource\" : \"Prometheus\" , \"definition\" : \"label_values(jenkins_health_check_duration, kubernetes_namespace)\" , \"hide\" : 0 , \"includeAll\" : false , \"label\" : null , \"multi\" : false , \"name\" : \"Test\" , \"options\" : [], \"query\" : \"label_values(jenkins_health_check_duration, kubernetes_namespace)\" , \"refresh\" : 2 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"sort\" : 1 , \"tagValuesQuery\" : \"\" , \"tags\" : [], \"tagsQuery\" : \"\" , \"type\" : \"query\" , \"useTags\" : false } ] }, \"time\" : { \"from\" : \"now-6h\" , \"to\" : \"now\" }, \"timepicker\" : { \"refresh_intervals\" : [ \"5s\" , \"10s\" , \"30s\" , \"1m\" , \"5m\" , \"15m\" , \"30m\" , \"1h\" , \"2h\" , \"1d\" ], \"time_options\" : [ \"5m\" , \"15m\" , \"1h\" , \"6h\" , \"12h\" , \"24h\" , \"2d\" , \"7d\" , \"30d\" ] }, \"timezone\" : \"\" , \"title\" : \"Jenkins Masters\" , \"uid\" : \"8Z9-POHWz\" , \"version\" : 9 }","title":"Dashboard"},{"location":"blogs/monitor-jenkins-on-k8s/dashboard/#grafana-dashboard","text":"","title":"Grafana Dashboard"},{"location":"blogs/monitor-jenkins-on-k8s/dashboard/#example-screenshot","text":"","title":"Example Screenshot"},{"location":"blogs/monitor-jenkins-on-k8s/dashboard/#grafana-variables","text":"cluster type : datasource datasource type : prometheus node type : query query : label_values(kube_node_info{component=\"kube-state-metrics\"}, node) label : K8S Node multivalue include all namespace type : query query : label_values(jenkins_health_check_duration, kubernetes_namespace) label : Namespace multivalue include all instance type : query query : label_values(jenkins_health_check_duration, app_kubernetes_io_instance) label : Master multivalue include all","title":"Grafana Variables"},{"location":"blogs/monitor-jenkins-on-k8s/dashboard/#dashboard-json","text":"{ \"annotations\" : { \"list\" : [ { \"builtIn\" : 1 , \"datasource\" : \"-- Grafana --\" , \"enable\" : true , \"hide\" : true , \"iconColor\" : \"rgba(0, 211, 255, 1)\" , \"name\" : \"Annotations & Alerts\" , \"type\" : \"dashboard\" } ] }, \"description\" : \"Dashboard for when you are using multiple Jenkins Masters\" , \"editable\" : true , \"gnetId\" : null , \"graphTooltip\" : 1 , \"id\" : 9 , \"iteration\" : 1565906968208 , \"links\" : [], \"panels\" : [ { \"collapsed\" : false , \"gridPos\" : { \"h\" : 1 , \"w\" : 24 , \"x\" : 0 , \"y\" : 0 }, \"id\" : 20 , \"panels\" : [], \"title\" : \"Performance\" , \"type\" : \"row\" }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"decimals\" : 3 , \"description\" : \"The ratio of ok (200) request out of all requests.\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 6 , \"x\" : 0 , \"y\" : 1 }, \"id\" : 36 , \"legend\" : { \"alignAsTable\" : true , \"avg\" : false , \"current\" : true , \"max\" : false , \"min\" : false , \"rightSide\" : true , \"show\" : true , \"total\" : false , \"values\" : true }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(http_responseCodes_ok_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) \\nby (app_kubernetes_io_instance) / \\nsum(http_requests_count{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) \\nby (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [ { \"colorMode\" : \"warning\" , \"fill\" : true , \"line\" : true , \"op\" : \"lt\" , \"value\" : 0.991 , \"yaxis\" : \"left\" }, { \"colorMode\" : \"critical\" , \"fill\" : true , \"line\" : true , \"op\" : \"lt\" , \"value\" : 0.981 , \"yaxis\" : \"left\" } ], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Good HTTP Request Ratio\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"decimals\" : 3 , \"format\" : \"percentunit\" , \"label\" : null , \"logBase\" : 1 , \"max\" : \"1\" , \"min\" : \"0.95\" , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : false } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"decimals\" : 0 , \"description\" : \"Http Server Errors (500)\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 5 , \"x\" : 6 , \"y\" : 1 }, \"id\" : 38 , \"legend\" : { \"alignAsTable\" : true , \"avg\" : false , \"current\" : true , \"hideEmpty\" : true , \"hideZero\" : false , \"max\" : false , \"min\" : false , \"rightSide\" : true , \"show\" : true , \"total\" : false , \"values\" : true }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"http_responseCodes_serverError_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [ { \"colorMode\" : \"warning\" , \"fill\" : true , \"line\" : true , \"op\" : \"gt\" , \"value\" : 1 , \"yaxis\" : \"left\" } ], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Server Errors\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 6 , \"x\" : 11 , \"y\" : 1 }, \"id\" : 42 , \"interval\" : \"1m\" , \"legend\" : { \"avg\" : false , \"current\" : false , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum (jenkins_job_waiting_duration{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"instant\" : false , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Job Wait Duration\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"s\" , \"label\" : null , \"logBase\" : 2 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : false } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 6 , \"x\" : 17 , \"y\" : 1 }, \"id\" : 44 , \"interval\" : \"1m\" , \"legend\" : { \"avg\" : false , \"current\" : false , \"hideEmpty\" : false , \"hideZero\" : false , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(jenkins_job_building_duration{quantile=\\\"0.5\\\",kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})\\nby (app_kubernetes_io_instance) / 60\" , \"format\" : \"time_series\" , \"instant\" : false , \"interval\" : \"\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"p50-{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" }, { \"expr\" : \"sum(jenkins_node_builds{quantile=\\\"0.999\\\",kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})\\nby (app_kubernetes_io_instance) / 60\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"p999-{{app_kubernetes_io_instance}}\" , \"refId\" : \"B\" } ], \"thresholds\" : [], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Avg Build Duration Minutes\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"m\" , \"label\" : null , \"logBase\" : 2 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"decimals\" : 1 , \"format\" : \"m\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : false } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"description\" : \"How long the health check takes to complete at the 99th percentile.\\nHigher numbers signify problems\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 7 , \"x\" : 0 , \"y\" : 7 }, \"id\" : 18 , \"interval\" : \"1m\" , \"legend\" : { \"avg\" : false , \"current\" : false , \"hideEmpty\" : true , \"hideZero\" : true , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null as zero\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(jenkins_health_check_duration{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\", quantile=\\\"0.99\\\"}) \\n by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Health Check Duration (99%)\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"description\" : \"The 99% percentile of HTTP Requests handled by Jenkins masters.\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 8 , \"x\" : 7 , \"y\" : 7 }, \"id\" : 52 , \"legend\" : { \"avg\" : false , \"current\" : false , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(http_requests{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\",quantile=\\\"0.99\\\"} ) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"HTTP Request Duration (99%)\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"description\" : \"Shows performance of Ingress Controller connection that lasts longer than 250 milliseconds\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 8 , \"x\" : 15 , \"y\" : 7 }, \"id\" : 32 , \"legend\" : { \"avg\" : false , \"current\" : false , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(rate(\\n nginx_ingress_controller_request_duration_seconds_bucket{\\n le=\\\"0.25\\\",\\n namespace=~\\\"$namespace\\\",\\n ingress=~\\\"jenkins*\\\"\\n }[5m]\\n)) \\nby (ingress) / \\nsum(rate(\\n nginx_ingress_controller_request_duration_seconds_count{\\n namespace=~\\\"$namespace\\\",\\n ingress=~\\\"jenkins*\\\"\\n }[5m]\\n)) \\nby (ingress) \" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{ingress}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [ { \"colorMode\" : \"custom\" , \"fill\" : false , \"fillColor\" : \"rgba(50, 116, 217, 0.2)\" , \"line\" : true , \"lineColor\" : \"#B877D9\" , \"op\" : \"gt\" , \"value\" : 0.5 , \"yaxis\" : \"left\" }, { \"colorMode\" : \"warning\" , \"fill\" : false , \"fillColor\" : \"rgba(50, 116, 217, 0.2)\" , \"line\" : true , \"lineColor\" : \"rgba(31, 96, 196, 0.6)\" , \"op\" : \"gt\" , \"value\" : 1.5 , \"yaxis\" : \"left\" }, { \"colorMode\" : \"critical\" , \"fill\" : false , \"line\" : true , \"op\" : \"gt\" , \"value\" : 3 , \"yaxis\" : \"left\" } ], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Ingress Perfomance\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"collapsed\" : false , \"gridPos\" : { \"h\" : 1 , \"w\" : 24 , \"x\" : 0 , \"y\" : 13 }, \"id\" : 12 , \"panels\" : [], \"title\" : \"General Info\" , \"type\" : \"row\" }, { \"cacheTimeout\" : null , \"colorBackground\" : true , \"colorPostfix\" : false , \"colorValue\" : false , \"colors\" : [ \"#F2495C\" , \"#FFCB7D\" , \"#5794F2\" ], \"description\" : \"Amount of Masters healthy\" , \"format\" : \"none\" , \"gauge\" : { \"maxValue\" : 100 , \"minValue\" : 0 , \"show\" : false , \"thresholdLabels\" : false , \"thresholdMarkers\" : true }, \"gridPos\" : { \"h\" : 6 , \"w\" : 4 , \"x\" : 0 , \"y\" : 14 }, \"id\" : 26 , \"interval\" : null , \"links\" : [], \"mappingType\" : 1 , \"mappingTypes\" : [ { \"name\" : \"value to text\" , \"value\" : 1 }, { \"name\" : \"range to text\" , \"value\" : 2 } ], \"maxDataPoints\" : 100 , \"nullPointMode\" : \"connected\" , \"nullText\" : null , \"options\" : {}, \"pluginVersion\" : \"6.2.4\" , \"postfix\" : \"\" , \"postfixFontSize\" : \"50%\" , \"prefix\" : \"\" , \"prefixFontSize\" : \"50%\" , \"rangeMaps\" : [ { \"from\" : \"null\" , \"text\" : \"N/A\" , \"to\" : \"null\" } ], \"sparkline\" : { \"fillColor\" : \"rgba(31, 118, 189, 0.18)\" , \"full\" : false , \"lineColor\" : \"rgb(31, 120, 193)\" , \"show\" : false }, \"tableColumn\" : \"\" , \"targets\" : [ { \"expr\" : \"sum(jenkins_health_check_score{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"\" , \"refId\" : \"A\" } ], \"thresholds\" : \"0,1\" , \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Number of Masters\" , \"type\" : \"singlestat\" , \"valueFontSize\" : \"200%\" , \"valueMaps\" : [ { \"op\" : \"=\" , \"text\" : \"N/A\" , \"value\" : \"null\" } ], \"valueName\" : \"current\" }, { \"cacheTimeout\" : null , \"columns\" : [ { \"text\" : \"Avg\" , \"value\" : \"avg\" } ], \"description\" : \"Dropwizard based Health Score derived from other metrics\" , \"fontSize\" : \"100%\" , \"gridPos\" : { \"h\" : 6 , \"w\" : 5 , \"x\" : 4 , \"y\" : 14 }, \"id\" : 40 , \"links\" : [], \"options\" : {}, \"pageSize\" : 5 , \"pluginVersion\" : \"6.2.4\" , \"scroll\" : true , \"showHeader\" : true , \"sort\" : { \"col\" : 0 , \"desc\" : true }, \"styles\" : [ { \"alias\" : \"Score\" , \"colorMode\" : \"row\" , \"colors\" : [ \"#5794F2\" , \"#FF9830\" , \"#F2495C\" ], \"decimals\" : 0 , \"pattern\" : \"/.*/\" , \"thresholds\" : [ \"90\" , \"95\" ], \"type\" : \"number\" , \"unit\" : \"percentunit\" } ], \"targets\" : [ { \"expr\" : \"jenkins_health_check_score{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\" , \"format\" : \"time_series\" , \"instant\" : true , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Health Score\" , \"transform\" : \"timeseries_aggregations\" , \"type\" : \"table\" }, { \"gridPos\" : { \"h\" : 6 , \"w\" : 14 , \"x\" : 9 , \"y\" : 14 }, \"id\" : 46 , \"links\" : [], \"options\" : { \"fieldOptions\" : { \"calcs\" : [ \"last\" ], \"defaults\" : { \"decimals\" : 1 , \"max\" : 100 , \"min\" : 0 , \"title\" : \"\" , \"unit\" : \"percent\" }, \"mappings\" : [], \"override\" : {}, \"thresholds\" : [ { \"color\" : \"red\" , \"index\" : 0 , \"value\" : null }, { \"color\" : \"purple\" , \"index\" : 1 , \"value\" : 50 }, { \"color\" : \"blue\" , \"index\" : 2 , \"value\" : 75 } ], \"values\" : false }, \"orientation\" : \"auto\" , \"showThresholdLabels\" : false , \"showThresholdMarkers\" : true }, \"pluginVersion\" : \"6.2.4\" , \"targets\" : [ { \"expr\" : \"sum(jenkins_runs_success_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})\\nby (app_kubernetes_io_instance) /\\nsum(jenkins_runs_total_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})\\nby (app_kubernetes_io_instance) * 100\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Job Success Ratio\" , \"type\" : \"gauge\" }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"decimals\" : 0 , \"description\" : \"Amount of Jobs Currenty in the Queue\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 7 , \"x\" : 0 , \"y\" : 20 }, \"id\" : 30 , \"legend\" : { \"alignAsTable\" : false , \"avg\" : false , \"current\" : false , \"hideEmpty\" : false , \"hideZero\" : true , \"max\" : false , \"min\" : false , \"rightSide\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(jenkins_queue_size_value{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [ { \"colorMode\" : \"critical\" , \"fill\" : false , \"line\" : true , \"op\" : \"gt\" , \"value\" : 10 , \"yaxis\" : \"left\" }, { \"colorMode\" : \"warning\" , \"fill\" : false , \"line\" : true , \"op\" : \"gt\" , \"value\" : 5 , \"yaxis\" : \"left\" } ], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Job Queue\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"cacheTimeout\" : null , \"dashLength\" : 10 , \"dashes\" : false , \"decimals\" : 0 , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 8 , \"x\" : 7 , \"y\" : 20 }, \"hideTimeOverride\" : false , \"id\" : 50 , \"interval\" : \"\" , \"legend\" : { \"alignAsTable\" : true , \"avg\" : false , \"current\" : false , \"hideEmpty\" : true , \"hideZero\" : true , \"max\" : true , \"min\" : false , \"rightSide\" : true , \"show\" : true , \"total\" : false , \"values\" : true }, \"lines\" : true , \"linewidth\" : 2 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pluginVersion\" : \"6.2.4\" , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(increase(jenkins_runs_total_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}[24h])) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"interval\" : \"\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}-total\" , \"refId\" : \"A\" }, { \"expr\" : \"sum(increase(jenkins_runs_failure_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}[24h])) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"interval\" : \"\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}-failed\" , \"refId\" : \"B\" }, { \"expr\" : \"sum(increase(jenkins_runs_aborted_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}[24h])) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"interval\" : \"\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}-aborted\" , \"refId\" : \"C\" } ], \"thresholds\" : [], \"timeFrom\" : \"12h\" , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Runs Per Day\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"decimals\" : 0 , \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"decimals\" : 0 , \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"description\" : \"Active Build Runs\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 6 , \"w\" : 8 , \"x\" : 15 , \"y\" : 20 }, \"id\" : 56 , \"legend\" : { \"avg\" : false , \"current\" : false , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(jenkins_executor_in_use_history{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Build Runs\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"cards\" : { \"cardPadding\" : null , \"cardRound\" : null }, \"color\" : { \"cardColor\" : \"#F2495C\" , \"colorScale\" : \"sqrt\" , \"colorScheme\" : \"interpolateSpectral\" , \"exponent\" : 0.8 , \"max\" : null , \"min\" : 0 , \"mode\" : \"spectrum\" }, \"dataFormat\" : \"tsbuckets\" , \"description\" : \"Heatmap of when Jobs are scheduled\" , \"gridPos\" : { \"h\" : 7 , \"w\" : 23 , \"x\" : 0 , \"y\" : 26 }, \"heatmap\" : {}, \"hideTimeOverride\" : false , \"hideZeroBuckets\" : false , \"highlightCards\" : true , \"id\" : 58 , \"interval\" : \"5m\" , \"legend\" : { \"show\" : true }, \"links\" : [], \"options\" : {}, \"reverseYBuckets\" : false , \"targets\" : [ { \"expr\" : \"sum(increase(jenkins_runs_total_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}[2h])) by (app_kubernetes_io_instance)\" , \"format\" : \"heatmap\" , \"instant\" : false , \"interval\" : \"\" , \"intervalFactor\" : 10 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : \"168h\" , \"timeShift\" : null , \"title\" : \"Job Runs Heatmap\" , \"tooltip\" : { \"show\" : true , \"showHistogram\" : false }, \"type\" : \"heatmap\" , \"xAxis\" : { \"show\" : true }, \"xBucketNumber\" : null , \"xBucketSize\" : null , \"yAxis\" : { \"decimals\" : 0 , \"format\" : \"short\" , \"logBase\" : 1 , \"max\" : null , \"min\" : \"0\" , \"show\" : true , \"splitFactor\" : null }, \"yBucketBound\" : \"auto\" , \"yBucketNumber\" : null , \"yBucketSize\" : null }, { \"columns\" : [ { \"text\" : \"Current\" , \"value\" : \"current\" } ], \"description\" : \"Jenkins Master Plugin Count\" , \"fontSize\" : \"100%\" , \"gridPos\" : { \"h\" : 5 , \"w\" : 7 , \"x\" : 0 , \"y\" : 33 }, \"id\" : 14 , \"interval\" : \"1m\" , \"links\" : [], \"options\" : {}, \"pageSize\" : null , \"pluginVersion\" : \"6.2.4\" , \"scroll\" : true , \"showHeader\" : true , \"sort\" : { \"col\" : 0 , \"desc\" : true }, \"styles\" : [ { \"alias\" : \"Time\" , \"dateFormat\" : \"YYYY-MM-DD HH:mm:ss\" , \"pattern\" : \"Time\" , \"type\" : \"date\" }, { \"alias\" : \"Master\" , \"colorMode\" : \"row\" , \"colors\" : [ \"#5794F2\" , \"#FF9830\" , \"#F2495C\" ], \"decimals\" : 0 , \"link\" : false , \"pattern\" : \"/.*/\" , \"thresholds\" : [ \"95\" , \"130\" ], \"type\" : \"number\" , \"unit\" : \"short\" } ], \"targets\" : [ { \"expr\" : \"jenkins_plugins_active{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\" , \"format\" : \"time_series\" , \"instant\" : true , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Plugin Count\" , \"transform\" : \"timeseries_to_rows\" , \"type\" : \"table\" }, { \"columns\" : [ { \"text\" : \"Current\" , \"value\" : \"current\" } ], \"description\" : \"Amount of plugins that are available for updating\" , \"fontSize\" : \"100%\" , \"gridPos\" : { \"h\" : 5 , \"w\" : 8 , \"x\" : 7 , \"y\" : 33 }, \"id\" : 22 , \"links\" : [], \"options\" : {}, \"pageSize\" : null , \"scroll\" : true , \"showHeader\" : true , \"sort\" : { \"col\" : 0 , \"desc\" : true }, \"styles\" : [ { \"alias\" : \"Time\" , \"dateFormat\" : \"YYYY-MM-DD HH:mm:ss\" , \"pattern\" : \"Time\" , \"type\" : \"date\" }, { \"alias\" : \"\" , \"colorMode\" : \"row\" , \"colors\" : [ \"#5794F2\" , \"#FF9830\" , \"#F2495C\" ], \"decimals\" : 0 , \"pattern\" : \"/.*/\" , \"thresholds\" : [ \"3\" , \"10\" ], \"type\" : \"number\" , \"unit\" : \"short\" } ], \"targets\" : [ { \"expr\" : \"sum(jenkins_plugins_withUpdate{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"instant\" : true , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Plugin Updates Available\" , \"transform\" : \"timeseries_aggregations\" , \"type\" : \"table\" }, { \"columns\" : [ { \"text\" : \"Current\" , \"value\" : \"current\" }, { \"text\" : \"Max\" , \"value\" : \"max\" } ], \"description\" : \"Jenkins Master Job Count\" , \"fontSize\" : \"100%\" , \"gridPos\" : { \"h\" : 5 , \"w\" : 8 , \"x\" : 15 , \"y\" : 33 }, \"id\" : 16 , \"links\" : [], \"options\" : {}, \"pageSize\" : null , \"scroll\" : true , \"showHeader\" : true , \"sort\" : { \"col\" : 0 , \"desc\" : true }, \"styles\" : [ { \"alias\" : \"Time\" , \"dateFormat\" : \"YYYY-MM-DD HH:mm:ss\" , \"pattern\" : \"Time\" , \"type\" : \"date\" }, { \"alias\" : \"\" , \"colorMode\" : \"row\" , \"colors\" : [ \"#F2495C\" , \"#FF9830\" , \"#5794F2\" ], \"decimals\" : 0 , \"pattern\" : \"/.*/\" , \"thresholds\" : [ \"1\" ], \"type\" : \"number\" , \"unit\" : \"short\" } ], \"targets\" : [ { \"expr\" : \"jenkins_job_count_value{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\" , \"format\" : \"time_series\" , \"instant\" : true , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Job Count\" , \"transform\" : \"timeseries_aggregations\" , \"type\" : \"table\" }, { \"columns\" : [ { \"text\" : \"Current\" , \"value\" : \"current\" } ], \"description\" : \"Counts offline build nodes that are connected to this master\" , \"fontSize\" : \"100%\" , \"gridPos\" : { \"h\" : 6 , \"w\" : 7 , \"x\" : 0 , \"y\" : 38 }, \"id\" : 24 , \"links\" : [], \"options\" : {}, \"pageSize\" : null , \"scroll\" : true , \"showHeader\" : true , \"sort\" : { \"col\" : 0 , \"desc\" : true }, \"styles\" : [ { \"alias\" : \"Time\" , \"dateFormat\" : \"YYYY-MM-DD HH:mm:ss\" , \"pattern\" : \"Time\" , \"type\" : \"date\" }, { \"alias\" : \"\" , \"colorMode\" : \"row\" , \"colors\" : [ \"#5794F2\" , \"#B877D9\" , \"#F2495C\" ], \"decimals\" : 2 , \"pattern\" : \"/.*/\" , \"thresholds\" : [ \"1\" , \"3\" ], \"type\" : \"number\" , \"unit\" : \"short\" } ], \"targets\" : [ { \"expr\" : \"sum(jenkins_node_offline_value{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"instant\" : true , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Offline Nodes\" , \"transform\" : \"timeseries_aggregations\" , \"type\" : \"table\" }, { \"cacheTimeout\" : null , \"columns\" : [ { \"text\" : \"Current\" , \"value\" : \"current\" } ], \"description\" : \"Uptime in hours\" , \"fontSize\" : \"100%\" , \"gridPos\" : { \"h\" : 6 , \"w\" : 8 , \"x\" : 7 , \"y\" : 38 }, \"id\" : 6 , \"links\" : [], \"options\" : {}, \"pageSize\" : null , \"pluginVersion\" : \"6.2.4\" , \"scroll\" : true , \"showHeader\" : true , \"sort\" : { \"col\" : null , \"desc\" : false }, \"styles\" : [ { \"alias\" : \"Time\" , \"dateFormat\" : \"YYYY-MM-DD HH:mm:ss\" , \"pattern\" : \"Time\" , \"type\" : \"date\" }, { \"alias\" : \"\" , \"colorMode\" : \"row\" , \"colors\" : [ \"#F2495C\" , \"#FF9830\" , \"#5794F2\" ], \"decimals\" : 0 , \"pattern\" : \"/.*/\" , \"thresholds\" : [ \"1\" , \"24\" ], \"type\" : \"number\" , \"unit\" : \"short\" } ], \"targets\" : [ { \"expr\" : \"vm_uptime_milliseconds{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"} / 3600000 \" , \"format\" : \"time_series\" , \"instant\" : true , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Uptime\" , \"transform\" : \"timeseries_aggregations\" , \"type\" : \"table\" }, { \"columns\" : [ { \"text\" : \"Current\" , \"value\" : \"current\" } ], \"description\" : \"The current Master executor count, masters should not have executors, so only 0 is green.\" , \"fontSize\" : \"100%\" , \"gridPos\" : { \"h\" : 6 , \"w\" : 8 , \"x\" : 15 , \"y\" : 38 }, \"id\" : 34 , \"interval\" : \"\" , \"links\" : [], \"options\" : {}, \"pageSize\" : null , \"scroll\" : true , \"showHeader\" : true , \"sort\" : { \"col\" : 0 , \"desc\" : true }, \"styles\" : [ { \"alias\" : \"Time\" , \"dateFormat\" : \"YYYY-MM-DD HH:mm:ss\" , \"pattern\" : \"Time\" , \"type\" : \"date\" }, { \"alias\" : \"\" , \"colorMode\" : \"row\" , \"colors\" : [ \"#F2495C\" , \"#FF9830\" , \"#5794F2\" ], \"decimals\" : 2 , \"pattern\" : \"/.*/\" , \"thresholds\" : [ \"0\" , \"0\" , \"1\" ], \"type\" : \"number\" , \"unit\" : \"short\" } ], \"targets\" : [ { \"expr\" : \"sum(jenkins_executor_count_value{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"instant\" : true , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"Master Executor Count\" , \"transform\" : \"timeseries_aggregations\" , \"type\" : \"table\" }, { \"collapsed\" : false , \"gridPos\" : { \"h\" : 1 , \"w\" : 24 , \"x\" : 0 , \"y\" : 44 }, \"id\" : 2 , \"panels\" : [], \"title\" : \"JVM Metrics\" , \"type\" : \"row\" }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"fill\" : 1 , \"gridPos\" : { \"h\" : 7 , \"w\" : 10 , \"x\" : 0 , \"y\" : 45 }, \"id\" : 48 , \"legend\" : { \"avg\" : false , \"current\" : false , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"1 - sum(vm_gc_G1_Young_Generation_time{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})by (app_kubernetes_io_instance) \\n/ \\nsum (vm_uptime_milliseconds{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"instant\" : false , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [ { \"colorMode\" : \"warning\" , \"fill\" : true , \"line\" : true , \"op\" : \"lt\" , \"value\" : 0.998 , \"yaxis\" : \"left\" }, { \"colorMode\" : \"critical\" , \"fill\" : true , \"line\" : true , \"op\" : \"lt\" , \"value\" : 0.98 , \"yaxis\" : \"left\" } ], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"JVM GC Throughput\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"decimals\" : 5 , \"format\" : \"percentunit\" , \"label\" : null , \"logBase\" : 1 , \"max\" : \"1\" , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : false } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"description\" : \"Ratio of JVM Memory used\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 7 , \"w\" : 13 , \"x\" : 10 , \"y\" : 45 }, \"id\" : 10 , \"legend\" : { \"alignAsTable\" : false , \"avg\" : false , \"current\" : false , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 2 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum(vm_memory_heap_usage{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [ { \"colorMode\" : \"critical\" , \"fill\" : true , \"line\" : true , \"op\" : \"gt\" , \"value\" : 0.75 , \"yaxis\" : \"left\" } ], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Memory Ratio\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"percentunit\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : false } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"decimals\" : 4 , \"description\" : \"JVM CPU Load\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 7 , \"w\" : 10 , \"x\" : 0 , \"y\" : 52 }, \"id\" : 4 , \"legend\" : { \"alignAsTable\" : true , \"avg\" : false , \"current\" : true , \"hideEmpty\" : true , \"hideZero\" : true , \"max\" : true , \"min\" : false , \"rightSide\" : true , \"show\" : false , \"total\" : false , \"values\" : true }, \"lines\" : true , \"linewidth\" : 2 , \"links\" : [], \"nullPointMode\" : \"connected\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 1 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : true , \"targets\" : [ { \"expr\" : \"vm_cpu_load{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\" , \"format\" : \"time_series\" , \"instant\" : false , \"intervalFactor\" : 5 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"CPU Load\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : \"\" , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : \"\" , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : false } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"decimals\" : null , \"description\" : \"JVM Memory usage\" , \"fill\" : 2 , \"gridPos\" : { \"h\" : 7 , \"w\" : 13 , \"x\" : 10 , \"y\" : 52 }, \"id\" : 8 , \"legend\" : { \"alignAsTable\" : true , \"avg\" : false , \"current\" : true , \"max\" : true , \"min\" : false , \"rightSide\" : true , \"show\" : true , \"total\" : false , \"values\" : true }, \"lines\" : true , \"linewidth\" : 2 , \"links\" : [], \"nullPointMode\" : \"null\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"vm_memory_total_used{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\" , \"format\" : \"time_series\" , \"intervalFactor\" : 2 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Memory\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 1 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"aliasColors\" : {}, \"bars\" : false , \"dashLength\" : 10 , \"dashes\" : false , \"description\" : \"Lists memory usage of the Pod vs. Kubernetes Requests\" , \"fill\" : 1 , \"gridPos\" : { \"h\" : 8 , \"w\" : 10 , \"x\" : 0 , \"y\" : 59 }, \"id\" : 54 , \"legend\" : { \"avg\" : false , \"current\" : false , \"max\" : false , \"min\" : false , \"show\" : true , \"total\" : false , \"values\" : false }, \"lines\" : true , \"linewidth\" : 1 , \"links\" : [], \"nullPointMode\" : \"connected\" , \"options\" : {}, \"percentage\" : false , \"pointradius\" : 2 , \"points\" : false , \"renderer\" : \"flot\" , \"seriesOverrides\" : [], \"spaceLength\" : 10 , \"stack\" : false , \"steppedLine\" : false , \"targets\" : [ { \"expr\" : \"sum (label_join(container_memory_usage_bytes{\\n container_name=\\\"jenkins\\\",\\n namespace=~\\\"$namespace\\\"\\n }, \\n \\\"pod\\\", \\n \\\",\\\", \\n \\\"pod_name\\\"\\n)) by (pod) / \\nsum (kube_pod_container_resource_requests_memory_bytes { \\n container=\\\"jenkins\\\",\\n namespace=~\\\"$namespace\\\"\\n }\\n) by (pod)\" , \"format\" : \"time_series\" , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{pod}}\" , \"refId\" : \"A\" } ], \"thresholds\" : [], \"timeFrom\" : null , \"timeRegions\" : [], \"timeShift\" : null , \"title\" : \"Memory Usage vs. Request\" , \"tooltip\" : { \"shared\" : true , \"sort\" : 0 , \"value_type\" : \"individual\" }, \"type\" : \"graph\" , \"xaxis\" : { \"buckets\" : null , \"mode\" : \"time\" , \"name\" : null , \"show\" : true , \"values\" : [] }, \"yaxes\" : [ { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true }, { \"format\" : \"short\" , \"label\" : null , \"logBase\" : 1 , \"max\" : null , \"min\" : null , \"show\" : true } ], \"yaxis\" : { \"align\" : false , \"alignLevel\" : null } }, { \"description\" : \"The ratio of open file descriptors\\nSee: https://support.cloudbees.com/hc/en-us/articles/204246140-Too-many-open-files\" , \"gridPos\" : { \"h\" : 8 , \"w\" : 5 , \"x\" : 10 , \"y\" : 59 }, \"id\" : 28 , \"links\" : [], \"options\" : { \"displayMode\" : \"basic\" , \"fieldOptions\" : { \"calcs\" : [ \"last\" ], \"defaults\" : { \"max\" : 1 , \"min\" : 0 , \"unit\" : \"percentunit\" }, \"mappings\" : [], \"override\" : {}, \"thresholds\" : [ { \"color\" : \"blue\" , \"index\" : 0 , \"value\" : null }, { \"color\" : \"orange\" , \"index\" : 1 , \"value\" : 60 }, { \"color\" : \"red\" , \"index\" : 2 , \"value\" : 80 } ], \"values\" : false }, \"orientation\" : \"horizontal\" }, \"targets\" : [ { \"expr\" : \"vm_file_descriptor_ratio{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\" , \"format\" : \"time_series\" , \"instant\" : true , \"intervalFactor\" : 1 , \"legendFormat\" : \"{{app_kubernetes_io_instance}}\" , \"refId\" : \"A\" } ], \"timeFrom\" : null , \"timeShift\" : null , \"title\" : \"File Descriptor Ratio\" , \"type\" : \"bargauge\" } ], \"refresh\" : \"30s\" , \"schemaVersion\" : 18 , \"style\" : \"dark\" , \"tags\" : [ \"Jenkins\" , \"Prometheus\" ], \"templating\" : { \"list\" : [ { \"current\" : { \"text\" : \"Prometheus\" , \"value\" : \"Prometheus\" }, \"hide\" : 0 , \"includeAll\" : false , \"label\" : null , \"multi\" : false , \"name\" : \"cluster\" , \"options\" : [], \"query\" : \"prometheus\" , \"refresh\" : 1 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"type\" : \"datasource\" }, { \"allValue\" : null , \"current\" : { \"text\" : \"All\" , \"value\" : [ \"$__all\" ] }, \"datasource\" : \"Prometheus\" , \"definition\" : \"label_values(jenkins_health_check_duration, app_kubernetes_io_instance)\" , \"hide\" : 0 , \"includeAll\" : true , \"label\" : \"Master\" , \"multi\" : true , \"name\" : \"instance\" , \"options\" : [], \"query\" : \"label_values(jenkins_health_check_duration, app_kubernetes_io_instance)\" , \"refresh\" : 2 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"sort\" : 1 , \"tagValuesQuery\" : \"\" , \"tags\" : [], \"tagsQuery\" : \"\" , \"type\" : \"query\" , \"useTags\" : false }, { \"allValue\" : null , \"current\" : { \"text\" : \"All\" , \"value\" : \"$__all\" }, \"datasource\" : \"Prometheus\" , \"definition\" : \"label_values(jenkins_health_check_duration, kubernetes_namespace)\" , \"hide\" : 0 , \"includeAll\" : true , \"label\" : \"Namespace\" , \"multi\" : true , \"name\" : \"namespace\" , \"options\" : [], \"query\" : \"label_values(jenkins_health_check_duration, kubernetes_namespace)\" , \"refresh\" : 2 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"sort\" : 1 , \"tagValuesQuery\" : \"\" , \"tags\" : [], \"tagsQuery\" : \"\" , \"type\" : \"query\" , \"useTags\" : false }, { \"allValue\" : null , \"current\" : { \"text\" : \"All\" , \"value\" : \"$__all\" }, \"datasource\" : \"Prometheus\" , \"definition\" : \"label_values(kube_node_info{component=\\\"kube-state-metrics\\\"}, node)\" , \"hide\" : 0 , \"includeAll\" : true , \"label\" : \"K8S Node\" , \"multi\" : true , \"name\" : \"node\" , \"options\" : [], \"query\" : \"label_values(kube_node_info{component=\\\"kube-state-metrics\\\"}, node)\" , \"refresh\" : 1 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"sort\" : 5 , \"tagValuesQuery\" : \"\" , \"tags\" : [], \"tagsQuery\" : \"\" , \"type\" : \"query\" , \"useTags\" : false }, { \"allValue\" : null , \"current\" : { \"text\" : \"jx-production\" , \"value\" : \"jx-production\" }, \"datasource\" : \"Prometheus\" , \"definition\" : \"label_values(jenkins_health_check_duration, kubernetes_namespace)\" , \"hide\" : 0 , \"includeAll\" : false , \"label\" : null , \"multi\" : false , \"name\" : \"Test\" , \"options\" : [], \"query\" : \"label_values(jenkins_health_check_duration, kubernetes_namespace)\" , \"refresh\" : 2 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"sort\" : 1 , \"tagValuesQuery\" : \"\" , \"tags\" : [], \"tagsQuery\" : \"\" , \"type\" : \"query\" , \"useTags\" : false } ] }, \"time\" : { \"from\" : \"now-6h\" , \"to\" : \"now\" }, \"timepicker\" : { \"refresh_intervals\" : [ \"5s\" , \"10s\" , \"30s\" , \"1m\" , \"5m\" , \"15m\" , \"30m\" , \"1h\" , \"2h\" , \"1d\" ], \"time_options\" : [ \"5m\" , \"15m\" , \"1h\" , \"6h\" , \"12h\" , \"24h\" , \"2d\" , \"7d\" , \"30d\" ] }, \"timezone\" : \"\" , \"title\" : \"Jenkins Masters\" , \"uid\" : \"8Z9-POHWz\" , \"version\" : 9 }","title":"Dashboard json"},{"location":"blogs/monitor-jenkins-on-k8s/install/","text":"Install Components for Monitoring \u00b6 This chapter is about installing all the tools we need for completing this guide. If you already have these tools installed, feel free to skip the actual installations. However, do make sure to confirm you have a compatible configuration. Important This guide is written during August/September 2019, during which Helm 3 entered Beta. This guide assumes Helm 2, be mindful of the Helm version you are running! Prepare \u00b6 First, we make sure we have hostnames for our services, including Prometheus, Alertmanager, and Grafana. export DOMAIN = export PROM_ADDR = mon. ${ DOMAIN } export AM_ADDR = alertmanager. ${ DOMAIN } export GRAFANA_ADDR = \"grafana. ${ DOMAIN } \" Then we create a namespace to host the monitoring tools. kubectl create namespace mon kubens mon Install Prometheus & Alertmanager \u00b6 By default, the Helm chart of Prometheus installs Alertmanager as well. To access the UI of Alertmanager, we also set its Ingress' hostname. helm upgrade -i prometheus \\ stable/prometheus \\ --namespace mon \\ --version 7 .1.3 \\ --set server.ingress.hosts ={ $PROM_ADDR } \\ --set alertmanager.ingress.hosts ={ $AM_ADDR } \\ -f prom-values.yaml Use the below command to wait for the deployment of Prometheus to be completed. kubectl -n mon \\ rollout status \\ deploy prometheus-server prom-values.yaml Below is an example helm values.yaml for Prometheus. It shows how to set resources limits and request, some alerts, and how to configure sending these alerts to Slack. server : ingress : enabled : true annotations : ingress.kubernetes.io/ssl-redirect : \"false\" nginx.ingress.kubernetes.io/ssl-redirect : \"false\" resources : limits : cpu : 100m memory : 1000Mi requests : cpu : 10m memory : 500Mi alertmanager : ingress : enabled : true annotations : ingress.kubernetes.io/ssl-redirect : \"false\" nginx.ingress.kubernetes.io/ssl-redirect : \"false\" resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi kubeStateMetrics : resources : limits : cpu : 10m memory : 50Mi requests : cpu : 5m memory : 25Mi nodeExporter : resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi pushgateway : resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi serverFiles : alerts : groups : - name : nodes rules : - alert : JenkinsToManyJobsQueued expr : sum(jenkins_queue_size_value) > 5 for : 3m labels : severity : notify annotations : summary : Jenkins to many jobs queued description : A Jenkins instance is failing a health check alertmanagerFiles : alertmanager.yml : global : {} route : group_wait : 10s group_interval : 5m receiver : slack repeat_interval : 3h routes : - receiver : slack repeat_interval : 5d match : severity : notify frequency : low receivers : - name : slack slack_configs : - api_url : \"XXXXXXXXXX\" send_resolved : true title : \"{{ .CommonAnnotations.summary }}\" text : \"{{ .CommonAnnotations.description }}\" title_link : http://example.com Install Grafana \u00b6 !!! note At the time of writing (September 2019) we cannot use the latest version of the Grafana helm chart. 1 2 * https://github.com/helm/charts/pull/15702 * https://github.com/helm/charts/issues/15725 We install Grafana in the same namespace as Prometheus and Alertmanager. helm upgrade -i grafana stable/grafana \\ --version 3 .5.5 \\ --namespace mon \\ --set ingress.hosts = \"{ $GRAFANA_ADDR }\" \\ --values grafana-values.yaml kubectl -n mon rollout status deployment grafana Once the deployment is rolled out, we can either directly open the Grafana UI or echo the address and copy & paste it. echo \"http:// $GRAFANA_ADDR \" open \"http:// $GRAFANA_ADDR \" By default, the Grafana helm chart generates a password for you, with the command below you can retrieve it. kubectl -n mon \\ get secret grafana \\ -o jsonpath = \"{.data.admin-password}\" \\ | base64 --decode ; echo open \"https://grafana.com/dashboards\" grafana-values.yaml Below is an example configuration for a helm values.yaml , which also includes some useful dashboards by default. We've also configured a default Datasource, pointing to the Prometheus installed earlier. ingress : enabled : true persistence : enabled : true accessModes : - ReadWriteOnce size : 1Gi resources : limits : cpu : 20m memory : 50Mi requests : cpu : 5m memory : 25Mi datasources : datasources.yaml : apiVersion : 1 datasources : - name : Prometheus type : prometheus url : http://prometheus-server access : proxy isDefault : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'Default' orgId : 1 folder : 'default' type : file disableDeletion : true editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : Costs-Pod : gnetId : 6879 revision : 1 datasource : Prometheus Costs : gnetId : 8670 revision : 1 datasource : Prometheus Summary : gnetId : 8685 revision : 1 datasource : Prometheus Capacity : gnetId : 5228 revision : 6 datasource : Prometheus Deployments : gnetId : 8588 revision : 1 datasource : Prometheus Volumes : gnetId : 6739 revision : 1 datasource : Prometheus Install Jenkins \u00b6 Now that we've taken care of the monitoring tools, we can install Jenkins. We start by creating a namespace for Jenkins to land in. kubectl create namespace jenkins kubens jenkins There are many ways of installing Jenkins. There is a very well maintained Helm chart, which is well suited for what we want to achieve. !!! note It is recommended to spread teams and applications across Jenkins masters rather than put everything into a single instance. So in this guide we create two identical Jenkins Masters, each with a unique hostname, to simulate this and show that the alerts and dashboards work for one or more Jenkins masters. Although the Helm chart is a very good starting point, we still need a values.yaml file to configure a few things. Helm Values Explained \u00b6 Let's explain some of the values: installPlugins : we want blueocean for a more beautiful Pipeline UI and prometheus to expose the metrics in a Prometheus format resources : always specify your resources, if these are wrong, our monitoring alerts and dashboard should help use tweak these values javaOpts : for some reason, the default configuration doesn't have the recommended JVM and Garbage Collection configuration, so we have to specify this, see CloudBees' JVM Troubleshoot Guide for more details ingress : because I believe every publicly available service should only be accessible via TLS, we have to configure TLS and certmanager annotations (as we're using Certmanager to manage our certificate) podAnnotations : the default metrics endpoint that Prometheus scrapes from is /metrics , unfortunately, the by default included Metrics Plugin exposes the metrics on that endpoint in the wrong format. This means we have to inform Prometheus how to retrieve the metrics Make sure both jenkins-values.X.yaml and jenkins-certificate.X.yaml are created according to the template files below. Replace the X for each master, if you want three, you'll have .1.yaml , .2.yaml and .3.yaml for each of the files. Replace the <ReplaceWithYourDNS> with your DNS Host name and the X with the appropriate number. For example, if your host name is example.com , you will have the following: hostName : jenkins1.example.com tls : - secretName : tls-jenkins-1 hosts : - jenkins1.example.com Use this file as the starting point for each of the masters. I would recommend making your changes in this file first and then make two copies and update the X value with 1 and 2 respectively. jenkins-values.X.yaml master : serviceType : ClusterIP installPlugins : - blueocean:1.17.0 - prometheus:2.0.0 - kubernetes:1.17.2 resources : requests : cpu : \"250m\" memory : \"1024Mi\" limits : cpu : \"1000m\" memory : \"2048Mi\" javaOpts : \"-XX:+AlwaysPreTouch -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+ParallelRefProcEnabled -XX:+DisableExplicitGC -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\" ingress : enabled : true hostName : jenkinsX.<ReplaceWithYourDNS> tls : - secretName : tls-jenkins-X hosts : - jenkinsX.<ReplaceWithYourDNS> annotations : certmanager.k8s.io/cluster-issuer : \"letsencrypt-prod\" kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : \"false\" nginx.ingress.kubernetes.io/proxy-body-size : 50m nginx.ingress.kubernetes.io/proxy-request-buffering : \"off\" nginx.ingress.kubernetes.io/ssl-redirect : \"true\" podAnnotations : prometheus.io/path : /prometheus prometheus.io/port : \"8080\" prometheus.io/scrape : \"true\" agent : enabled : true rbac : create : true If you want to use TLS for Jenkins, this is an example Certificate. If you don't already have certmanager configured, take a look at my guide on leveraging Let's Encrypt in Kubernetes . jenkins-certificate.X.yaml apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : jenkinsX.<ReplaceWithYourDNS> spec : secretName : tls-jenkins-X dnsNames : - jenkinsX.<ReplaceWithYourDNS> acme : config : - http01 : ingressClass : nginx domains : - jenkinsX.<ReplaceWithYourDNS> issuerRef : name : letsencrypt-prod kind : ClusterIssuer Master One \u00b6 Assuming you've created unique Helm values files for both Master One and Master Two, we can start with creating the first one. helm upgrade -i jenkins \\ stable/jenkins \\ --namespace jenkins \\ -f jenkins-values.1.yaml Apply Certificate \u00b6 If you have the certificate, apply it to the cluster. kubectl apply -f jenkins-certificate.1.yaml Wait for rollout \u00b6 If you want to wait for the Jenkins deployment to be completed, use the following command. kubectl -n jenkins rollout status deployment jenkins1 Retrieve Password \u00b6 The Jenkins Helm chart also generates a admin password for you. See the command below on how to retrieve it. printf $( kubectl get secret --namespace jenkins jenkins1 -o jsonpath = \"{.data.jenkins-admin-password}\" | base64 --decode ) ; echo Master Two \u00b6 Let's create Master Two as well, same deal as before. The commands are here for convenience, so you can use the [] in the top right to copy and paste easily. helm upgrade -i jenkins2 \\ stable/jenkins \\ --namespace jenkins \\ -f jenkins-values.2.yaml Wait for rollout \u00b6 kubectl -n jenkins rollout status deployment jenkins2 Apply Certificate \u00b6 kubectl apply -f jenkins-certificate.2.yaml Retrieve Password \u00b6 printf $( kubectl get secret --namespace jenkins jenkins2 -o jsonpath = \"{.data.jenkins-admin-password}\" | base64 --decode ) ; echo","title":"Install Tools"},{"location":"blogs/monitor-jenkins-on-k8s/install/#install-components-for-monitoring","text":"This chapter is about installing all the tools we need for completing this guide. If you already have these tools installed, feel free to skip the actual installations. However, do make sure to confirm you have a compatible configuration. Important This guide is written during August/September 2019, during which Helm 3 entered Beta. This guide assumes Helm 2, be mindful of the Helm version you are running!","title":"Install Components for Monitoring"},{"location":"blogs/monitor-jenkins-on-k8s/install/#prepare","text":"First, we make sure we have hostnames for our services, including Prometheus, Alertmanager, and Grafana. export DOMAIN = export PROM_ADDR = mon. ${ DOMAIN } export AM_ADDR = alertmanager. ${ DOMAIN } export GRAFANA_ADDR = \"grafana. ${ DOMAIN } \" Then we create a namespace to host the monitoring tools. kubectl create namespace mon kubens mon","title":"Prepare"},{"location":"blogs/monitor-jenkins-on-k8s/install/#install-prometheus-alertmanager","text":"By default, the Helm chart of Prometheus installs Alertmanager as well. To access the UI of Alertmanager, we also set its Ingress' hostname. helm upgrade -i prometheus \\ stable/prometheus \\ --namespace mon \\ --version 7 .1.3 \\ --set server.ingress.hosts ={ $PROM_ADDR } \\ --set alertmanager.ingress.hosts ={ $AM_ADDR } \\ -f prom-values.yaml Use the below command to wait for the deployment of Prometheus to be completed. kubectl -n mon \\ rollout status \\ deploy prometheus-server prom-values.yaml Below is an example helm values.yaml for Prometheus. It shows how to set resources limits and request, some alerts, and how to configure sending these alerts to Slack. server : ingress : enabled : true annotations : ingress.kubernetes.io/ssl-redirect : \"false\" nginx.ingress.kubernetes.io/ssl-redirect : \"false\" resources : limits : cpu : 100m memory : 1000Mi requests : cpu : 10m memory : 500Mi alertmanager : ingress : enabled : true annotations : ingress.kubernetes.io/ssl-redirect : \"false\" nginx.ingress.kubernetes.io/ssl-redirect : \"false\" resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi kubeStateMetrics : resources : limits : cpu : 10m memory : 50Mi requests : cpu : 5m memory : 25Mi nodeExporter : resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi pushgateway : resources : limits : cpu : 10m memory : 20Mi requests : cpu : 5m memory : 10Mi serverFiles : alerts : groups : - name : nodes rules : - alert : JenkinsToManyJobsQueued expr : sum(jenkins_queue_size_value) > 5 for : 3m labels : severity : notify annotations : summary : Jenkins to many jobs queued description : A Jenkins instance is failing a health check alertmanagerFiles : alertmanager.yml : global : {} route : group_wait : 10s group_interval : 5m receiver : slack repeat_interval : 3h routes : - receiver : slack repeat_interval : 5d match : severity : notify frequency : low receivers : - name : slack slack_configs : - api_url : \"XXXXXXXXXX\" send_resolved : true title : \"{{ .CommonAnnotations.summary }}\" text : \"{{ .CommonAnnotations.description }}\" title_link : http://example.com","title":"Install Prometheus &amp; Alertmanager"},{"location":"blogs/monitor-jenkins-on-k8s/install/#install-grafana","text":"!!! note At the time of writing (September 2019) we cannot use the latest version of the Grafana helm chart. 1 2 * https://github.com/helm/charts/pull/15702 * https://github.com/helm/charts/issues/15725 We install Grafana in the same namespace as Prometheus and Alertmanager. helm upgrade -i grafana stable/grafana \\ --version 3 .5.5 \\ --namespace mon \\ --set ingress.hosts = \"{ $GRAFANA_ADDR }\" \\ --values grafana-values.yaml kubectl -n mon rollout status deployment grafana Once the deployment is rolled out, we can either directly open the Grafana UI or echo the address and copy & paste it. echo \"http:// $GRAFANA_ADDR \" open \"http:// $GRAFANA_ADDR \" By default, the Grafana helm chart generates a password for you, with the command below you can retrieve it. kubectl -n mon \\ get secret grafana \\ -o jsonpath = \"{.data.admin-password}\" \\ | base64 --decode ; echo open \"https://grafana.com/dashboards\" grafana-values.yaml Below is an example configuration for a helm values.yaml , which also includes some useful dashboards by default. We've also configured a default Datasource, pointing to the Prometheus installed earlier. ingress : enabled : true persistence : enabled : true accessModes : - ReadWriteOnce size : 1Gi resources : limits : cpu : 20m memory : 50Mi requests : cpu : 5m memory : 25Mi datasources : datasources.yaml : apiVersion : 1 datasources : - name : Prometheus type : prometheus url : http://prometheus-server access : proxy isDefault : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'Default' orgId : 1 folder : 'default' type : file disableDeletion : true editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : Costs-Pod : gnetId : 6879 revision : 1 datasource : Prometheus Costs : gnetId : 8670 revision : 1 datasource : Prometheus Summary : gnetId : 8685 revision : 1 datasource : Prometheus Capacity : gnetId : 5228 revision : 6 datasource : Prometheus Deployments : gnetId : 8588 revision : 1 datasource : Prometheus Volumes : gnetId : 6739 revision : 1 datasource : Prometheus","title":"Install Grafana"},{"location":"blogs/monitor-jenkins-on-k8s/install/#install-jenkins","text":"Now that we've taken care of the monitoring tools, we can install Jenkins. We start by creating a namespace for Jenkins to land in. kubectl create namespace jenkins kubens jenkins There are many ways of installing Jenkins. There is a very well maintained Helm chart, which is well suited for what we want to achieve. !!! note It is recommended to spread teams and applications across Jenkins masters rather than put everything into a single instance. So in this guide we create two identical Jenkins Masters, each with a unique hostname, to simulate this and show that the alerts and dashboards work for one or more Jenkins masters. Although the Helm chart is a very good starting point, we still need a values.yaml file to configure a few things.","title":"Install Jenkins"},{"location":"blogs/monitor-jenkins-on-k8s/install/#helm-values-explained","text":"Let's explain some of the values: installPlugins : we want blueocean for a more beautiful Pipeline UI and prometheus to expose the metrics in a Prometheus format resources : always specify your resources, if these are wrong, our monitoring alerts and dashboard should help use tweak these values javaOpts : for some reason, the default configuration doesn't have the recommended JVM and Garbage Collection configuration, so we have to specify this, see CloudBees' JVM Troubleshoot Guide for more details ingress : because I believe every publicly available service should only be accessible via TLS, we have to configure TLS and certmanager annotations (as we're using Certmanager to manage our certificate) podAnnotations : the default metrics endpoint that Prometheus scrapes from is /metrics , unfortunately, the by default included Metrics Plugin exposes the metrics on that endpoint in the wrong format. This means we have to inform Prometheus how to retrieve the metrics Make sure both jenkins-values.X.yaml and jenkins-certificate.X.yaml are created according to the template files below. Replace the X for each master, if you want three, you'll have .1.yaml , .2.yaml and .3.yaml for each of the files. Replace the <ReplaceWithYourDNS> with your DNS Host name and the X with the appropriate number. For example, if your host name is example.com , you will have the following: hostName : jenkins1.example.com tls : - secretName : tls-jenkins-1 hosts : - jenkins1.example.com Use this file as the starting point for each of the masters. I would recommend making your changes in this file first and then make two copies and update the X value with 1 and 2 respectively. jenkins-values.X.yaml master : serviceType : ClusterIP installPlugins : - blueocean:1.17.0 - prometheus:2.0.0 - kubernetes:1.17.2 resources : requests : cpu : \"250m\" memory : \"1024Mi\" limits : cpu : \"1000m\" memory : \"2048Mi\" javaOpts : \"-XX:+AlwaysPreTouch -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+ParallelRefProcEnabled -XX:+DisableExplicitGC -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\" ingress : enabled : true hostName : jenkinsX.<ReplaceWithYourDNS> tls : - secretName : tls-jenkins-X hosts : - jenkinsX.<ReplaceWithYourDNS> annotations : certmanager.k8s.io/cluster-issuer : \"letsencrypt-prod\" kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : \"false\" nginx.ingress.kubernetes.io/proxy-body-size : 50m nginx.ingress.kubernetes.io/proxy-request-buffering : \"off\" nginx.ingress.kubernetes.io/ssl-redirect : \"true\" podAnnotations : prometheus.io/path : /prometheus prometheus.io/port : \"8080\" prometheus.io/scrape : \"true\" agent : enabled : true rbac : create : true If you want to use TLS for Jenkins, this is an example Certificate. If you don't already have certmanager configured, take a look at my guide on leveraging Let's Encrypt in Kubernetes . jenkins-certificate.X.yaml apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : jenkinsX.<ReplaceWithYourDNS> spec : secretName : tls-jenkins-X dnsNames : - jenkinsX.<ReplaceWithYourDNS> acme : config : - http01 : ingressClass : nginx domains : - jenkinsX.<ReplaceWithYourDNS> issuerRef : name : letsencrypt-prod kind : ClusterIssuer","title":"Helm Values Explained"},{"location":"blogs/monitor-jenkins-on-k8s/install/#master-one","text":"Assuming you've created unique Helm values files for both Master One and Master Two, we can start with creating the first one. helm upgrade -i jenkins \\ stable/jenkins \\ --namespace jenkins \\ -f jenkins-values.1.yaml","title":"Master One"},{"location":"blogs/monitor-jenkins-on-k8s/install/#apply-certificate","text":"If you have the certificate, apply it to the cluster. kubectl apply -f jenkins-certificate.1.yaml","title":"Apply Certificate"},{"location":"blogs/monitor-jenkins-on-k8s/install/#wait-for-rollout","text":"If you want to wait for the Jenkins deployment to be completed, use the following command. kubectl -n jenkins rollout status deployment jenkins1","title":"Wait for rollout"},{"location":"blogs/monitor-jenkins-on-k8s/install/#retrieve-password","text":"The Jenkins Helm chart also generates a admin password for you. See the command below on how to retrieve it. printf $( kubectl get secret --namespace jenkins jenkins1 -o jsonpath = \"{.data.jenkins-admin-password}\" | base64 --decode ) ; echo","title":"Retrieve Password"},{"location":"blogs/monitor-jenkins-on-k8s/install/#master-two","text":"Let's create Master Two as well, same deal as before. The commands are here for convenience, so you can use the [] in the top right to copy and paste easily. helm upgrade -i jenkins2 \\ stable/jenkins \\ --namespace jenkins \\ -f jenkins-values.2.yaml","title":"Master Two"},{"location":"blogs/monitor-jenkins-on-k8s/install/#wait-for-rollout_1","text":"kubectl -n jenkins rollout status deployment jenkins2","title":"Wait for rollout"},{"location":"blogs/monitor-jenkins-on-k8s/install/#apply-certificate_1","text":"kubectl apply -f jenkins-certificate.2.yaml","title":"Apply Certificate"},{"location":"blogs/monitor-jenkins-on-k8s/install/#retrieve-password_1","text":"printf $( kubectl get secret --namespace jenkins jenkins2 -o jsonpath = \"{.data.jenkins-admin-password}\" | base64 --decode ) ; echo","title":"Retrieve Password"},{"location":"blogs/monitor-jenkins-on-k8s/introduction/","text":"Introduction \u00b6 This is a guide on monitoring Jenkins on Kubernetes with Prometheus and Grafana. There are other solutions out there, and if you want to learn those, I suggest to look elsewhere. If you're interested in diving in Jenkins Metrics and how to make sense of them with Prometheus and Grafana, read on! What We Will Do \u00b6 The outline of the steps to take is below. Each has its own page, so if you feel you have create a Kubernetes cluster configure the cluster (e.g. Helm) install Prometheus and Grafana install one or more Jenkins instances get metrics from running Jenkins instance(s) have queries for understanding the state and performance of the Jenkins instance(s) have a dashboard to aid debugging an issue or determine new alerts have alerts that fire when (potential) problematic conditions occur get metrics from Jenkins Pipelines Resources \u00b6 The list below is both a shout out to the resources I learned from and as a reference for you if you want to learn more. https://go.cloudbees.com/docs/solutions/jvm-troubleshooting/ https://go.cloudbees.com/docs/cloudbees-documentation/devoptics-user-guide/run_insights/ https://medium.com/@eng.mohamed.m.saeed/monitoring-jenkins-with-grafana-and-prometheus-a7e037cbb376 https://stackoverflow.com/questions/52230653/graphite-jenkins-job-level-metrics https://towardsdatascience.com/jenkins-events-logs-and-metrics-7c3e8b28962b https://github.com/nvgoldin/jenkins-graphite https://www.weave.works/blog/promql-queries-for-the-rest-of-us/ https://medium.com/quiq-blog/prometheus-relabeling-tricks-6ae62c56cbda https://docs.google.com/presentation/d/1gtqEfTKM3oLr1N9zjAeXtOcS1eAQS--Xz0D4hwlo_KQ/edit#slide=id.g5bbd4fcccc_10_10 https://go.cloudbees.com/docs/cloudbees-documentation/devoptics-user-guide/security_privacy/#_verification_of_connection_to_the_devoptics_service https://sysdig.com/blog/golden-signals-kubernetes/ https://stackoverflow.com/questions/47141967/how-to-use-the-selected-period-of-time-in-a-query/47173828#47173828 https://www.robustperception.io/rate-then-sum-never-sum-then-rate https://www.innoq.com/en/blog/prometheus-counters/ https://www.robustperception.io/dont-put-the-value-in-alert-labels https://blog.pvincent.io/2017/12/prometheus-blog-series-part-5-alerting-rules/ https://docs.google.com/document/d/199PqyG3UsyXlwieHaqbGiWVa8eMWi8zzAn0YfcApr8Q/edit https://wiki.jenkins.io/display/JENKINS/Metrics+Plugin https://www.robustperception.io/controlling-the-instance-label https://www.robustperception.io/target-labels-are-for-life-not-just-for-christmas https://prometheus.io/docs/alerting/notifications/ https://piotrminkowski.wordpress.com/2017/08/29/visualizing-jenkins-pipeline-results-in-grafana/ https://medium.com/@jotak/designing-prometheus-metrics-72dcff88c2e5 https://github.com/prometheus/pushgateway https://github.com/prometheus/client_golang https://blog.pvincent.io/2017/12/prometheus-blog-series-part-2-metric-types/ https://prometheus.io/docs/concepts/jobs_instances/","title":"Introduction"},{"location":"blogs/monitor-jenkins-on-k8s/introduction/#introduction","text":"This is a guide on monitoring Jenkins on Kubernetes with Prometheus and Grafana. There are other solutions out there, and if you want to learn those, I suggest to look elsewhere. If you're interested in diving in Jenkins Metrics and how to make sense of them with Prometheus and Grafana, read on!","title":"Introduction"},{"location":"blogs/monitor-jenkins-on-k8s/introduction/#what-we-will-do","text":"The outline of the steps to take is below. Each has its own page, so if you feel you have create a Kubernetes cluster configure the cluster (e.g. Helm) install Prometheus and Grafana install one or more Jenkins instances get metrics from running Jenkins instance(s) have queries for understanding the state and performance of the Jenkins instance(s) have a dashboard to aid debugging an issue or determine new alerts have alerts that fire when (potential) problematic conditions occur get metrics from Jenkins Pipelines","title":"What We Will Do"},{"location":"blogs/monitor-jenkins-on-k8s/introduction/#resources","text":"The list below is both a shout out to the resources I learned from and as a reference for you if you want to learn more. https://go.cloudbees.com/docs/solutions/jvm-troubleshooting/ https://go.cloudbees.com/docs/cloudbees-documentation/devoptics-user-guide/run_insights/ https://medium.com/@eng.mohamed.m.saeed/monitoring-jenkins-with-grafana-and-prometheus-a7e037cbb376 https://stackoverflow.com/questions/52230653/graphite-jenkins-job-level-metrics https://towardsdatascience.com/jenkins-events-logs-and-metrics-7c3e8b28962b https://github.com/nvgoldin/jenkins-graphite https://www.weave.works/blog/promql-queries-for-the-rest-of-us/ https://medium.com/quiq-blog/prometheus-relabeling-tricks-6ae62c56cbda https://docs.google.com/presentation/d/1gtqEfTKM3oLr1N9zjAeXtOcS1eAQS--Xz0D4hwlo_KQ/edit#slide=id.g5bbd4fcccc_10_10 https://go.cloudbees.com/docs/cloudbees-documentation/devoptics-user-guide/security_privacy/#_verification_of_connection_to_the_devoptics_service https://sysdig.com/blog/golden-signals-kubernetes/ https://stackoverflow.com/questions/47141967/how-to-use-the-selected-period-of-time-in-a-query/47173828#47173828 https://www.robustperception.io/rate-then-sum-never-sum-then-rate https://www.innoq.com/en/blog/prometheus-counters/ https://www.robustperception.io/dont-put-the-value-in-alert-labels https://blog.pvincent.io/2017/12/prometheus-blog-series-part-5-alerting-rules/ https://docs.google.com/document/d/199PqyG3UsyXlwieHaqbGiWVa8eMWi8zzAn0YfcApr8Q/edit https://wiki.jenkins.io/display/JENKINS/Metrics+Plugin https://www.robustperception.io/controlling-the-instance-label https://www.robustperception.io/target-labels-are-for-life-not-just-for-christmas https://prometheus.io/docs/alerting/notifications/ https://piotrminkowski.wordpress.com/2017/08/29/visualizing-jenkins-pipeline-results-in-grafana/ https://medium.com/@jotak/designing-prometheus-metrics-72dcff88c2e5 https://github.com/prometheus/pushgateway https://github.com/prometheus/client_golang https://blog.pvincent.io/2017/12/prometheus-blog-series-part-2-metric-types/ https://prometheus.io/docs/concepts/jobs_instances/","title":"Resources"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/","text":"Metrics \u00b6 Jenkins is a Java Web application, in this case running in Kubernetes. Let's categorize the metrics we want to look at and deal with each group individually. JVM : the JVM metrics are exposed, we should leverage this for particular queries and alerts Jenkins Configuration : the default configuration exposes some configuration elements, a few of these have strong recommended values, such as Master Executor Slots (should always be 0 ) Jenkins Usage : jobs running in Jenkins, or Jobs not running in Jenkins can also tell us about (potential) problems Web : although it is not the primary function of Jenkins, web access gives hints about performance trends Pod Metrics : any generic metric from a Kubernetes Pod perspective can be helpful to look at Kubernetes Labels This guide assumes you install Jenkins via the Helm chart as explained elsewhere in the guide. This means it assumes the Jenkins instances all have the label app.kubernetes.io/instance . In Prometheus, this becomes app_kubernetes_io_instance . If you install your applications (such as Jenkins) in other ways, either change the queries presented here accordingly or add the label. Types of Metrics to evaluate \u00b6 We are in the age where SRE, DevOps Engineer, and Platform Engineer are hyped terms. Hyped they may be, there is a good reason people are making noise about monitoring. A lot is written about which kinds of metrics to observe and which to ignore. There's enough written about this - including Viktor Farcic's excellent DevOps Toolkit 2.5 - so we skip diving into these. In case you haven't read anything about it, let's briefly look at the types of metrics. Latency : response times of your application, in our case, both external access via Ingress and internal access. We can measure latency on internal access via Jenkins' own metrics, which also has percentile information (for example, p99) Errors : we can take a look at network errors such as HTTP 500, which we get straight from Jenkins' webserver (Netty) and at failed jobs Traffic : the number of connections to our service, in our case we have web traffic and jobs running, both we get from Jenkins Saturation : how much the system is used compared to the available resources, core resources such as CPU and Memory primarily depend on your Kubernetes Nodes. However, we can take a look at the Pod's limits vs. requests and Jenkins' job wait time - which roughly translates to saturation JVM Metrics \u00b6 We have some basic JVM metrics, such as CPU and Memory usage, and uptime. Uptime In the age of containers, uptime is not a very useful or sexy metric to observe. I include it because we can use uptime as a proxy metric. For example, if a service never goes beyond a particular value - Prometheus records Max values - it can signify a problem elsewhere. vm_cpu_load (vm_memory_total_max - vm_memory_total_used) / vm_memory_total_max * 100.0 vm_uptime_milliseconds Garbage Collection \u00b6 For fine-tuning the JVM's garbage collection for Jenkins, there are two central guides from CloudBees. Which also explain the JVM_OPTIONS in the jenkins-values.yaml we used for the Helm installation. Guide On Preparing Jenkins For Support JVM Troubleshooting Guide The second article contains much information on how to analyze the Garbage Collection logs and metrics. To process the data thoroughly requires experts with specially designed tools. I am not such an expert, nor is this the document to guide you through this. Summarizing the two guides: measure core metrics and Garbage Collection Throughput. If you need more, consult experts. Garbage Collection Throughput 1 -sum( rate( vm_gc_G1_Young_Generation_time[5m] ) ) by (app_kubernetes_io_instance) / sum ( vm_uptime_milliseconds ) by (app_kubernetes_io_instance) Check for too many open files \u00b6 When looking at the CloudBees guide on tuning performance on Linux , one of the main things to look are core metrics (Memory and CPU) and Open Files. There's even an explicit guide on monitoring the number of open files . vm_file_descriptor_ratio Jenkins Config Metrics \u00b6 Some of the metrics are derived from the configuration of a Jenkins Master. Plugins \u00b6 While Jenkins' extensive community is often praised for the number of plugins created and maintained, the plugins are also a big source of risk. You probably want to set a baseline and determine a value for when to send an alert. jenkins_plugins_active Jenkins Build Nodes \u00b6 Jenkins should never build on a master, always on a node or agent. jenkins_executor_count_value You might use static agents or, while we're in Kubernetes, only have dynamic agents. Either way, having nodes offline for a while signifies a problem. Maybe the Node configuration is wrong, or the PodTemplate has a mistake, or maybe your ServiceAccount doesn't have the correct permissions. jenkins_node_offline_value Jenkins Usage Metrics \u00b6 Most of Jenkins' metrics relate to its usage, though. Think about metrics regarding HTTP request duration, number server errors (HTTP 500), and all the metrics related to builds. Builds Per Day \u00b6 sum(increase(jenkins_runs_total_total[24h])) by (app_kubernetes_io_instance) Job duration \u00b6 default_jenkins_builds_last_build_duration_milliseconds Job Count \u00b6 jenkins_job_count_value Jobs in Queue \u00b6 If a Jenkins master is overloaded, it is likely to fall behind building jobs that are scheduled. Jenkins observes the duration a job spends in the queue ( jenkins_job_queuing_duration ) and the current queue size ( jenkins_queue_size_value ). jenkins_job_queuing_duration sum(jenkins_queue_size_value) by (app_kubernetes_io_instance) Web Metrics \u00b6 As Jenkins is also a web application, it makes to look at its HTTP related metrics as well. Route Of External Traffic It is important to note that the HTTP traffic of user interaction with Jenkins when running in Kubernetes can contain quite a lot of layers. Problems can arise in any of these layers, so it is crucial to monitor traffic to a service on multiple layers to speed debug time. Tracing is a great solution but out of scope for this guide. HTTP Requests \u00b6 The 99 th percentile of HTTP Requests handled by Jenkins masters. sum(http_requests{quantile=\"0.99\"} ) by (app_kubernetes_io_instance) 99 th percentile We look at percentiles because average times are not very helpful. For more information on why this is so, please consult the Google SRE book which is free online. Health Check Duration \u00b6 How long the health check takes to complete at the 99 th percentile. Higher numbers signify problems. sum ( rate ( jenkins_health_check_duration { quantile = \"0.99\" }[ 5 m ])) by ( app_kubernetes_io_instance ) Ingress Performance \u00b6 In this case, we look at the metrics of the Nginx Ingress Controller. If you use a different controller, rewrite the query to a sensible alternative. sum(rate( nginx_ingress_controller_request_duration_seconds_bucket{ le=\"0.25\" }[5m] )) by (ingress) / sum(rate( nginx_ingress_controller_request_duration_seconds_count[5m] )) by (ingress) Number of Good Request vs. Request \u00b6 sum(http_responseCodes_ok_total) by (kubernetes_pod_name) / sum(http_requests_count) by (kubernetes_pod_name) Pod Metrics \u00b6 These metrics are purely related to the Kubernetes Pods. They are as such, applicable to more applications than just Jenkins. CPU Usage \u00b6 sum(rate( container_cpu_usage_seconds_total{ container_name=\"jenkins*\" }[5m] )) by (pod_name) Query Filters In this case, we filter on those containers with name jenkins* , which means any container whose name has jenkins as the prefix. If you want to have more than one prefix or suffix, you can use || . So, if you would want to combine Jenkins with, let's say, prometheus , you will get the following. container_name=\"jenkins*||prometheus*\" Oversubscription of Pod memory \u00b6 While requests are not meant to be binding, if you think your application requests around 1GB and it is using well over 3GB, something is off. Either you are too naive and should update the requests , or something is wrong, and you need to take action. sum (label_join(container_memory_usage_bytes{ container_name=\"jenkins\" }, \"pod\", \",\", \"pod_name\" )) by (pod) / sum (kube_pod_container_resource_requests_memory_bytes { container=\"jenkins\" } ) by (pod) DevOptics Metrics \u00b6 CloudBees made parts of its DevOptics product free. This product contains - amongst other things - a feature set called Run Insights . This is a monitoring solution where your Jenkins Master uploads its metrics to the CloudBees service, and you get a dashboard with many of the same things already discussed. You might not want to leverage this free service but like some of its dashboard features. I've tried to recreate some of these - in a minimal fashion. Active Runs \u00b6 To know how many current builds there are, we can watch the executors that are in use. sum(jenkins_executor_in_use_history) by (app_kubernetes_io_instance) Idle Executors \u00b6 When using Kubernetes' Pods as an agent, the only idle executors we'll have are Pods that are done with their build and in the process of being terminated. Not very useful, but in case you want to know how: sum(jenkins_executor_free_history) by (app_kubernetes_io_instance) Average Time Waiting to Start \u00b6 With Kubernetes PodTemplates we cannot calculate this. The only wait time we get is the one that is between queue'ing a job and requesting the Pod, which isn't very meaningful. Completed Runs Per Day \u00b6 sum(increase(jenkins_runs_total_total[24h])) by (app_kubernetes_io_instance) Average Time to complete \u00b6 sum(jenkins_job_building_duration) by (app_kubernetes_io_instance) / sum(jenkins_job_building_duration_count) by (app_kubernetes_io_instance)","title":"Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#metrics","text":"Jenkins is a Java Web application, in this case running in Kubernetes. Let's categorize the metrics we want to look at and deal with each group individually. JVM : the JVM metrics are exposed, we should leverage this for particular queries and alerts Jenkins Configuration : the default configuration exposes some configuration elements, a few of these have strong recommended values, such as Master Executor Slots (should always be 0 ) Jenkins Usage : jobs running in Jenkins, or Jobs not running in Jenkins can also tell us about (potential) problems Web : although it is not the primary function of Jenkins, web access gives hints about performance trends Pod Metrics : any generic metric from a Kubernetes Pod perspective can be helpful to look at Kubernetes Labels This guide assumes you install Jenkins via the Helm chart as explained elsewhere in the guide. This means it assumes the Jenkins instances all have the label app.kubernetes.io/instance . In Prometheus, this becomes app_kubernetes_io_instance . If you install your applications (such as Jenkins) in other ways, either change the queries presented here accordingly or add the label.","title":"Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#types-of-metrics-to-evaluate","text":"We are in the age where SRE, DevOps Engineer, and Platform Engineer are hyped terms. Hyped they may be, there is a good reason people are making noise about monitoring. A lot is written about which kinds of metrics to observe and which to ignore. There's enough written about this - including Viktor Farcic's excellent DevOps Toolkit 2.5 - so we skip diving into these. In case you haven't read anything about it, let's briefly look at the types of metrics. Latency : response times of your application, in our case, both external access via Ingress and internal access. We can measure latency on internal access via Jenkins' own metrics, which also has percentile information (for example, p99) Errors : we can take a look at network errors such as HTTP 500, which we get straight from Jenkins' webserver (Netty) and at failed jobs Traffic : the number of connections to our service, in our case we have web traffic and jobs running, both we get from Jenkins Saturation : how much the system is used compared to the available resources, core resources such as CPU and Memory primarily depend on your Kubernetes Nodes. However, we can take a look at the Pod's limits vs. requests and Jenkins' job wait time - which roughly translates to saturation","title":"Types of Metrics to evaluate"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#jvm-metrics","text":"We have some basic JVM metrics, such as CPU and Memory usage, and uptime. Uptime In the age of containers, uptime is not a very useful or sexy metric to observe. I include it because we can use uptime as a proxy metric. For example, if a service never goes beyond a particular value - Prometheus records Max values - it can signify a problem elsewhere. vm_cpu_load (vm_memory_total_max - vm_memory_total_used) / vm_memory_total_max * 100.0 vm_uptime_milliseconds","title":"JVM Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#garbage-collection","text":"For fine-tuning the JVM's garbage collection for Jenkins, there are two central guides from CloudBees. Which also explain the JVM_OPTIONS in the jenkins-values.yaml we used for the Helm installation. Guide On Preparing Jenkins For Support JVM Troubleshooting Guide The second article contains much information on how to analyze the Garbage Collection logs and metrics. To process the data thoroughly requires experts with specially designed tools. I am not such an expert, nor is this the document to guide you through this. Summarizing the two guides: measure core metrics and Garbage Collection Throughput. If you need more, consult experts. Garbage Collection Throughput 1 -sum( rate( vm_gc_G1_Young_Generation_time[5m] ) ) by (app_kubernetes_io_instance) / sum ( vm_uptime_milliseconds ) by (app_kubernetes_io_instance)","title":"Garbage Collection"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#check-for-too-many-open-files","text":"When looking at the CloudBees guide on tuning performance on Linux , one of the main things to look are core metrics (Memory and CPU) and Open Files. There's even an explicit guide on monitoring the number of open files . vm_file_descriptor_ratio","title":"Check for too many open files"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#jenkins-config-metrics","text":"Some of the metrics are derived from the configuration of a Jenkins Master.","title":"Jenkins Config Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#plugins","text":"While Jenkins' extensive community is often praised for the number of plugins created and maintained, the plugins are also a big source of risk. You probably want to set a baseline and determine a value for when to send an alert. jenkins_plugins_active","title":"Plugins"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#jenkins-build-nodes","text":"Jenkins should never build on a master, always on a node or agent. jenkins_executor_count_value You might use static agents or, while we're in Kubernetes, only have dynamic agents. Either way, having nodes offline for a while signifies a problem. Maybe the Node configuration is wrong, or the PodTemplate has a mistake, or maybe your ServiceAccount doesn't have the correct permissions. jenkins_node_offline_value","title":"Jenkins Build Nodes"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#jenkins-usage-metrics","text":"Most of Jenkins' metrics relate to its usage, though. Think about metrics regarding HTTP request duration, number server errors (HTTP 500), and all the metrics related to builds.","title":"Jenkins Usage Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#builds-per-day","text":"sum(increase(jenkins_runs_total_total[24h])) by (app_kubernetes_io_instance)","title":"Builds Per Day"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#job-duration","text":"default_jenkins_builds_last_build_duration_milliseconds","title":"Job duration"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#job-count","text":"jenkins_job_count_value","title":"Job Count"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#jobs-in-queue","text":"If a Jenkins master is overloaded, it is likely to fall behind building jobs that are scheduled. Jenkins observes the duration a job spends in the queue ( jenkins_job_queuing_duration ) and the current queue size ( jenkins_queue_size_value ). jenkins_job_queuing_duration sum(jenkins_queue_size_value) by (app_kubernetes_io_instance)","title":"Jobs in Queue"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#web-metrics","text":"As Jenkins is also a web application, it makes to look at its HTTP related metrics as well. Route Of External Traffic It is important to note that the HTTP traffic of user interaction with Jenkins when running in Kubernetes can contain quite a lot of layers. Problems can arise in any of these layers, so it is crucial to monitor traffic to a service on multiple layers to speed debug time. Tracing is a great solution but out of scope for this guide.","title":"Web Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#http-requests","text":"The 99 th percentile of HTTP Requests handled by Jenkins masters. sum(http_requests{quantile=\"0.99\"} ) by (app_kubernetes_io_instance) 99 th percentile We look at percentiles because average times are not very helpful. For more information on why this is so, please consult the Google SRE book which is free online.","title":"HTTP Requests"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#health-check-duration","text":"How long the health check takes to complete at the 99 th percentile. Higher numbers signify problems. sum ( rate ( jenkins_health_check_duration { quantile = \"0.99\" }[ 5 m ])) by ( app_kubernetes_io_instance )","title":"Health Check Duration"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#ingress-performance","text":"In this case, we look at the metrics of the Nginx Ingress Controller. If you use a different controller, rewrite the query to a sensible alternative. sum(rate( nginx_ingress_controller_request_duration_seconds_bucket{ le=\"0.25\" }[5m] )) by (ingress) / sum(rate( nginx_ingress_controller_request_duration_seconds_count[5m] )) by (ingress)","title":"Ingress Performance"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#number-of-good-request-vs-request","text":"sum(http_responseCodes_ok_total) by (kubernetes_pod_name) / sum(http_requests_count) by (kubernetes_pod_name)","title":"Number of Good Request vs. Request"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#pod-metrics","text":"These metrics are purely related to the Kubernetes Pods. They are as such, applicable to more applications than just Jenkins.","title":"Pod Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#cpu-usage","text":"sum(rate( container_cpu_usage_seconds_total{ container_name=\"jenkins*\" }[5m] )) by (pod_name) Query Filters In this case, we filter on those containers with name jenkins* , which means any container whose name has jenkins as the prefix. If you want to have more than one prefix or suffix, you can use || . So, if you would want to combine Jenkins with, let's say, prometheus , you will get the following. container_name=\"jenkins*||prometheus*\"","title":"CPU Usage"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#oversubscription-of-pod-memory","text":"While requests are not meant to be binding, if you think your application requests around 1GB and it is using well over 3GB, something is off. Either you are too naive and should update the requests , or something is wrong, and you need to take action. sum (label_join(container_memory_usage_bytes{ container_name=\"jenkins\" }, \"pod\", \",\", \"pod_name\" )) by (pod) / sum (kube_pod_container_resource_requests_memory_bytes { container=\"jenkins\" } ) by (pod)","title":"Oversubscription of Pod memory"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#devoptics-metrics","text":"CloudBees made parts of its DevOptics product free. This product contains - amongst other things - a feature set called Run Insights . This is a monitoring solution where your Jenkins Master uploads its metrics to the CloudBees service, and you get a dashboard with many of the same things already discussed. You might not want to leverage this free service but like some of its dashboard features. I've tried to recreate some of these - in a minimal fashion.","title":"DevOptics Metrics"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#active-runs","text":"To know how many current builds there are, we can watch the executors that are in use. sum(jenkins_executor_in_use_history) by (app_kubernetes_io_instance)","title":"Active Runs"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#idle-executors","text":"When using Kubernetes' Pods as an agent, the only idle executors we'll have are Pods that are done with their build and in the process of being terminated. Not very useful, but in case you want to know how: sum(jenkins_executor_free_history) by (app_kubernetes_io_instance)","title":"Idle Executors"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#average-time-waiting-to-start","text":"With Kubernetes PodTemplates we cannot calculate this. The only wait time we get is the one that is between queue'ing a job and requesting the Pod, which isn't very meaningful.","title":"Average Time Waiting to Start"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#completed-runs-per-day","text":"sum(increase(jenkins_runs_total_total[24h])) by (app_kubernetes_io_instance)","title":"Completed Runs Per Day"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#average-time-to-complete","text":"sum(jenkins_job_building_duration) by (app_kubernetes_io_instance) / sum(jenkins_job_building_duration_count) by (app_kubernetes_io_instance)","title":"Average Time to complete"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/","text":"Track Metrics of Pipelines \u00b6 Get Data From Jobs \u00b6 Use Prometheus Push Gateway via shared lib JX sh step -> tekton -> write interceptor Configure Prometheus Push Gateway \u00b6 Make sure it is enabled in the prometheus helm chart pushgateway : enabled : true Identification Data \u00b6 canonical FQN: application ID source URI Questions to answer \u00b6 Metrics to gather \u00b6 test coverage shared libraries used duration of stages duration of job status of job status of stage time-to-fix git source node label languages (github languages parser) Test Coverage \u00b6 Send a Gauge with coverage as value. Potential Labels: Application ID Source URI Job Instance RunId Send Metric From Jenkins Pipeline \u00b6 Bash \u00b6 Simple \u00b6 echo \"some_metric 3.14\" | curl --data-binary @- http://pushgateway.example.org:9091/metrics/job/some_job Complex \u00b6 cat <<EOF | curl --data-binary @- http://pushgateway.example.org:9091/metrics/job/some_job/instance/some_instance # TYPE some_metric counter some_metric{label=\"val1\"} 42 # TYPE another_metric gauge # HELP another_metric Just an example. another_metric 2398.283 EOF Delete by instance \u00b6 curl -X DELETE http://pushgateway.example.org:9091/metrics/job/some_job/instance/some_instance Delete by (prometheus) job \u00b6 curl -X DELETE http://pushgateway.example.org:9091/metrics/job/some_job Go lang client \u00b6 Queries \u00b6 Total Stages Duration In Seconds \u00b6 sum(jenkins_pipeline_run_stages_hist_sum) by (jobName, runId) / 1000 Coverage Metric \u00b6 jenkins_pipeline_run_test_coverage Last Push Of Metric \u00b6 time () - push_time_seconds Stage Statusses \u00b6 m ?? \u00b6 ( sum ( jenkins_pipeline_run_hist_sum ) by ( jobName ) / 1000 ) / sum ( jenkins_pipeline_run_hist_count ) by ( jobName ) sum(jenkins_pipeline_run_hist_sum) by (jobName, runId) sum(jenkins_pipeline_run_hist_count) by (instance, appId) sum(jenkins_pipeline_run_stages_hist_sum) by (instance, jobName, runId) sum(jenkins_pipeline_run_hist_count offset 3d) by (jobName) sum(jenkins_pipeline_run_hist_count) by (jobName) Success Rate of Stages \u00b6 count(jenkins_pipeline_run_hist_sum{ result=\"SUCCESS\"}) by (jobName, runId) / count(jenkins_pipeline_run_hist_sum) by (jobName, runId) Github Autostatus \u00b6 install influxDB configure influxDB into Grafana as Datasource install plugin in Jenkins Plugin: https://plugins.jenkins.io/github-autostatus configure in jenkins config to us influxdb import dashboard into Grafana 5786 SELECT \"jobtime\", \"buildnumber\", \"passed\", \"branch\", \"buildurl\" FROM \"job\" WHERE (\"owner\" = 'joostvdg') AND $timeFilter GROUP BY \"repo\" Things to look at \u00b6 Scraping of Gateway means metrics are retrieved more often than they are created you can reduce the error by creating a rewrite rule https://www.robustperception.io/aggregating-across-batch-job-runs-with-push_time_seconds Counter does not aggregate https://stackoverflow.com/questions/50923880/prometheus-intrumentation-for-distributed-accumulated-batch-jobs if you want aggregation, use Prometheus Aggregation Gateway from Weaveworks https://github.com/weaveworks/prom-aggregation-gateway Pipeline Example - Curl \u00b6 pipeline { agent { kubernetes { label 'jpb-mon' yaml \"\"\" kind: Pod metadata: labels: build: prom-test spec: containers: - name: curl image: byrnedo/alpine-curl command: [\"cat\"] tty: true - name: golang image: golang:1.9 command: [\"cat\"] tty: true \"\"\" } } environment { CREDS = credentials ( 'api' ) TEST_COVERAGE = '' PROM_URL = 'http://prometheus-pushgateway.obs:9091/metrics/job/devops25' } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/go-demo-5.git' } } stage ( 'Prepare' ) { steps { container ( 'golang' ) { sh 'go get -d -v -t' } } } stage ( 'Build & Test' ) { steps { container ( 'golang' ) { script { sh 'go build' def coverage = sh encoding: 'UTF-8' , label: 'go test' , returnStdout: true , script: 'go test --cover -v ./... --run UnitTest | grep coverage:' coverage = coverage . trim () coverage = coverage . replace ( 'coverage: ' , '' ) coverage = coverage . replace ( '% of statements' , '' ) TEST_COVERAGE = \"${coverage}\" println \"coverage=${coverage}\" } sh 'ls -lath' } } } stage ( 'Push Metrics' ) { environment { COVERAGE = \"${TEST_COVERAGE}\" } steps { println \"COVERAGE=${COVERAGE}\" container ( 'curl' ) { sh 'echo \"TEST_COVERAGE=${COVERAGE}\"' sh 'echo \"PROM_URL=${PROM_URL}\"' sh 'echo \"BUILD_ID=${BUILD_ID}\"' sh 'echo \"JOB_NAME=${JOB_NAME}\"' sh 'echo \"jenkins_pipeline_run_test_coverage{instance=\\\"$JENKINS_URL\\\",jobName=\\\"$JOB_NAME\\\", run=\\\"$BUILD_ID\\\"} ${TEST_COVERAGE}\" | curl --data-binary @- ${PROM_URL}' } } } } } Pipeline Example - CLI \u00b6 The tool Jenkins Pipeline Binary - Monitoring will retrieve the Stages Nodes from Jenkins and translate them to Gauges in Prometheus. pipeline { agent { kubernetes { label 'jpb-mon' yaml \"\"\" kind: Pod metadata: labels: build: prom-test-4 spec: containers: - name: jpb image: caladreas/jpb-mon:0.23.0 command: ['/bin/jpb-mon', 'sleep', '--sleep', '3m'] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" \"\"\" } } environment { CREDS = credentials ( 'api' ) } stages { stage ( 'Test1' ) { steps { print 'Hello World' } } stage ( 'Test2' ) { environment { MASTER = 'jenkins1' } steps { sh 'echo \"Hello World!\"' } } } post { always { container ( 'jpb' ) { sh \"/bin/jpb-mon get-run --verbose --host ${JENKINS_URL} --job ${JOB_NAME} --run ${BUILD_ID} --username ${CREDS_USR} --password ${CREDS_PSW} --push\" } } } } Resources \u00b6 https://stackoverflow.com/questions/37009906/access-stage-results-in-workflow-pipeline-plugin https://github.com/jenkinsci/blueocean-plugin/tree/master/blueocean-rest#get-pipeline-run-nodes https://github.com/jenkinsci/pipeline-model-definition-plugin/wiki/Getting-Started","title":"Pipeline"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#track-metrics-of-pipelines","text":"","title":"Track Metrics of Pipelines"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#get-data-from-jobs","text":"Use Prometheus Push Gateway via shared lib JX sh step -> tekton -> write interceptor","title":"Get Data From Jobs"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#configure-prometheus-push-gateway","text":"Make sure it is enabled in the prometheus helm chart pushgateway : enabled : true","title":"Configure Prometheus Push Gateway"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#identification-data","text":"canonical FQN: application ID source URI","title":"Identification Data"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#questions-to-answer","text":"","title":"Questions to answer"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#metrics-to-gather","text":"test coverage shared libraries used duration of stages duration of job status of job status of stage time-to-fix git source node label languages (github languages parser)","title":"Metrics to gather"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#test-coverage","text":"Send a Gauge with coverage as value. Potential Labels: Application ID Source URI Job Instance RunId","title":"Test Coverage"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#send-metric-from-jenkins-pipeline","text":"","title":"Send Metric From Jenkins Pipeline"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#bash","text":"","title":"Bash"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#simple","text":"echo \"some_metric 3.14\" | curl --data-binary @- http://pushgateway.example.org:9091/metrics/job/some_job","title":"Simple"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#complex","text":"cat <<EOF | curl --data-binary @- http://pushgateway.example.org:9091/metrics/job/some_job/instance/some_instance # TYPE some_metric counter some_metric{label=\"val1\"} 42 # TYPE another_metric gauge # HELP another_metric Just an example. another_metric 2398.283 EOF","title":"Complex"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#delete-by-instance","text":"curl -X DELETE http://pushgateway.example.org:9091/metrics/job/some_job/instance/some_instance","title":"Delete by instance"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#delete-by-prometheus-job","text":"curl -X DELETE http://pushgateway.example.org:9091/metrics/job/some_job","title":"Delete by (prometheus) job"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#go-lang-client","text":"","title":"Go lang client"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#queries","text":"","title":"Queries"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#total-stages-duration-in-seconds","text":"sum(jenkins_pipeline_run_stages_hist_sum) by (jobName, runId) / 1000","title":"Total Stages Duration In Seconds"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#coverage-metric","text":"jenkins_pipeline_run_test_coverage","title":"Coverage Metric"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#last-push-of-metric","text":"time () - push_time_seconds","title":"Last Push Of Metric"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#stage-statusses","text":"m","title":"Stage Statusses"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#_1","text":"( sum ( jenkins_pipeline_run_hist_sum ) by ( jobName ) / 1000 ) / sum ( jenkins_pipeline_run_hist_count ) by ( jobName ) sum(jenkins_pipeline_run_hist_sum) by (jobName, runId) sum(jenkins_pipeline_run_hist_count) by (instance, appId) sum(jenkins_pipeline_run_stages_hist_sum) by (instance, jobName, runId) sum(jenkins_pipeline_run_hist_count offset 3d) by (jobName) sum(jenkins_pipeline_run_hist_count) by (jobName)","title":"??"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#success-rate-of-stages","text":"count(jenkins_pipeline_run_hist_sum{ result=\"SUCCESS\"}) by (jobName, runId) / count(jenkins_pipeline_run_hist_sum) by (jobName, runId)","title":"Success Rate of Stages"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#github-autostatus","text":"install influxDB configure influxDB into Grafana as Datasource install plugin in Jenkins Plugin: https://plugins.jenkins.io/github-autostatus configure in jenkins config to us influxdb import dashboard into Grafana 5786 SELECT \"jobtime\", \"buildnumber\", \"passed\", \"branch\", \"buildurl\" FROM \"job\" WHERE (\"owner\" = 'joostvdg') AND $timeFilter GROUP BY \"repo\"","title":"Github Autostatus"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#things-to-look-at","text":"Scraping of Gateway means metrics are retrieved more often than they are created you can reduce the error by creating a rewrite rule https://www.robustperception.io/aggregating-across-batch-job-runs-with-push_time_seconds Counter does not aggregate https://stackoverflow.com/questions/50923880/prometheus-intrumentation-for-distributed-accumulated-batch-jobs if you want aggregation, use Prometheus Aggregation Gateway from Weaveworks https://github.com/weaveworks/prom-aggregation-gateway","title":"Things to look at"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#pipeline-example-curl","text":"pipeline { agent { kubernetes { label 'jpb-mon' yaml \"\"\" kind: Pod metadata: labels: build: prom-test spec: containers: - name: curl image: byrnedo/alpine-curl command: [\"cat\"] tty: true - name: golang image: golang:1.9 command: [\"cat\"] tty: true \"\"\" } } environment { CREDS = credentials ( 'api' ) TEST_COVERAGE = '' PROM_URL = 'http://prometheus-pushgateway.obs:9091/metrics/job/devops25' } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/go-demo-5.git' } } stage ( 'Prepare' ) { steps { container ( 'golang' ) { sh 'go get -d -v -t' } } } stage ( 'Build & Test' ) { steps { container ( 'golang' ) { script { sh 'go build' def coverage = sh encoding: 'UTF-8' , label: 'go test' , returnStdout: true , script: 'go test --cover -v ./... --run UnitTest | grep coverage:' coverage = coverage . trim () coverage = coverage . replace ( 'coverage: ' , '' ) coverage = coverage . replace ( '% of statements' , '' ) TEST_COVERAGE = \"${coverage}\" println \"coverage=${coverage}\" } sh 'ls -lath' } } } stage ( 'Push Metrics' ) { environment { COVERAGE = \"${TEST_COVERAGE}\" } steps { println \"COVERAGE=${COVERAGE}\" container ( 'curl' ) { sh 'echo \"TEST_COVERAGE=${COVERAGE}\"' sh 'echo \"PROM_URL=${PROM_URL}\"' sh 'echo \"BUILD_ID=${BUILD_ID}\"' sh 'echo \"JOB_NAME=${JOB_NAME}\"' sh 'echo \"jenkins_pipeline_run_test_coverage{instance=\\\"$JENKINS_URL\\\",jobName=\\\"$JOB_NAME\\\", run=\\\"$BUILD_ID\\\"} ${TEST_COVERAGE}\" | curl --data-binary @- ${PROM_URL}' } } } } }","title":"Pipeline Example - Curl"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#pipeline-example-cli","text":"The tool Jenkins Pipeline Binary - Monitoring will retrieve the Stages Nodes from Jenkins and translate them to Gauges in Prometheus. pipeline { agent { kubernetes { label 'jpb-mon' yaml \"\"\" kind: Pod metadata: labels: build: prom-test-4 spec: containers: - name: jpb image: caladreas/jpb-mon:0.23.0 command: ['/bin/jpb-mon', 'sleep', '--sleep', '3m'] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" \"\"\" } } environment { CREDS = credentials ( 'api' ) } stages { stage ( 'Test1' ) { steps { print 'Hello World' } } stage ( 'Test2' ) { environment { MASTER = 'jenkins1' } steps { sh 'echo \"Hello World!\"' } } } post { always { container ( 'jpb' ) { sh \"/bin/jpb-mon get-run --verbose --host ${JENKINS_URL} --job ${JOB_NAME} --run ${BUILD_ID} --username ${CREDS_USR} --password ${CREDS_PSW} --push\" } } } }","title":"Pipeline Example - CLI"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#resources","text":"https://stackoverflow.com/questions/37009906/access-stage-results-in-workflow-pipeline-plugin https://github.com/jenkinsci/blueocean-plugin/tree/master/blueocean-rest#get-pipeline-run-nodes https://github.com/jenkinsci/pipeline-model-definition-plugin/wiki/Getting-Started","title":"Resources"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/","text":"Prepare Environment \u00b6 This is a guide on monitoring Jenkins on Kubernetes, which makes it rather handy to have a Kubernetes cluster at hand. There are many ways to create a Kubernetes cluster, below is a guide on creating a cluster with Google Cloud's GKE. Elsewhere on this site, there are alternatives, such as Azure's AKS and AWS's EKS . Things To Do \u00b6 create a cluster install and configure Helm for easily installing other applications on the cluster install and configure Certmanager for managing TLS certificates with Let's Encrypt Create GKE Cluster \u00b6 Enough talk about what we should be doing, let's create the cluster! Prerequisites \u00b6 gcloud command-line utility Google Cloud account that is activated Variables \u00b6 Variables we need for the gcloud create cluster command. To make it easy to copy and paste the command. K8S_VERSION = 1 .13.7-gke.8 REGION = europe-west4 CLUSTER_NAME = <your cluster name> PROJECT_ID = <your google project id> Query available versions \u00b6 If you want to see which versions are available in your Google Cloud Region, set the REGION variable and execute the command below. The list you get back will contain two lists, one for worker nodes and one for master nodes . Only the versions for master nodes can be used to create a cluster. gcloud container get-server-config --region $REGION Create Cluster \u00b6 gcloud container clusters create ${ CLUSTER_NAME } \\ --region ${ REGION } \\ --cluster-version ${ K8S_VERSION } \\ --num-nodes 2 --machine-type n1-standard-2 \\ --addons = HorizontalPodAutoscaling \\ --min-nodes 2 --max-nodes 3 \\ --enable-autoupgrade \\ --enable-autoscaling \\ --enable-network-policy \\ --labels = purpose = practice Set ClusterAdmin \u00b6 For some later commands, such as Helm, we need to be ClusterAdmin. kubectl create clusterrolebinding \\ cluster-admin-binding \\ --clusterrole cluster-admin \\ --user $( gcloud config get-value account ) Install Ingress Controller \u00b6 An ingress controller is what allows you to access the applications you install on your Kubernetes cluster from the outside. We need to do this for the tools we will use. So we need to install an ingress controller. Any will do, but ingress-nginx (based on the widely use nginx application) is the most commonly used. kubectl apply \\ -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/1cd17cd12c98563407ad03812aebac46ca4442f2/deploy/mandatory.yaml kubectl apply \\ -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/1cd17cd12c98563407ad03812aebac46ca4442f2/deploy/provider/cloud-generic.yaml For exposing our applications to the outside, we need to have a valid DNS name. For that, we need to have the IP address of our LoadBalancer. The command below retrieves that address. If it is empty, wait a few minutes and try again. export LB_IP = $( kubectl -n ingress-nginx \\ get svc ingress-nginx \\ -o jsonpath = \"{.status.loadBalancer.ingress[0].ip}\" ) echo $LB_IP Tip If you don't get an address back, check to see if your ingress controller has a service and that the service has an EXTERNAL IP address. kubectl get svc -n ingress-nginx -o wide The response should look something like this: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx LoadBalancer 10.48.14.43 34.90.67.21 80:32762/TCP,443:31389/TCP 21d Install Helm \u00b6 Helm is a, or the , package manager for Kubernetes. We will use it to install the other applications. Read more here . kubectl create \\ -f https://raw.githubusercontent.com/vfarcic/k8s-specs/master/helm/tiller-rbac.yml \\ --record --save-config Now that we've installed Helm, we can initialize the server component via a helm init . helm init --service-account tiller And now we wait. kubectl -n kube-system \\ rollout status deploy tiller-deploy Install Cert-Manager \u00b6 Cert-manager will help users automate installing TLS Certificates. Read more about cert-manager here . This creates the cert-manager specific resource definitions, also call CustomerResourceDefinitions or ***CRD***s. kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8/deploy/manifests/00-crds.yaml Due to how cert-manager works, it is best installed into its own namespace. There's a chicken and egg problem because it needs a Root Certificate Authority (or, RootCA) to exist, but every Certificate needs to be validated against this Certificate. Which is why we add the special label. kubectl create namespace cert-manager kubectl label namespace cert-manager certmanager.k8s.io/disable-validation = true Now that the CRD's and the namespace are ready, we can install cert-manager. Well, almost. The Helm Chart - that is how we call Helm packages - is in another Castle, eh, Helm Repository. So we first have to tell Helm where to get the Chart. helm repo add jetstack https://charts.jetstack.io helm repo update We can now install cert-manager via Helm! helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.8.0 \\ jetstack/cert-manager Configure ClusterIssuer \u00b6 Cert-manager can leverage Let's Encrypt to generate valid certificates. We need to instruct cert-manager which service to use, we do that by creating a ClusterIssuer resource. kubectl apply -f cluster-issuer.yaml cluster-issuer.yaml Don't forget to replace <replacewith your email address> with an actual email address you can access. apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-prod spec : acme : email : <replacewith your email address> server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-prod http01 : {}","title":"Prepare Environment"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#prepare-environment","text":"This is a guide on monitoring Jenkins on Kubernetes, which makes it rather handy to have a Kubernetes cluster at hand. There are many ways to create a Kubernetes cluster, below is a guide on creating a cluster with Google Cloud's GKE. Elsewhere on this site, there are alternatives, such as Azure's AKS and AWS's EKS .","title":"Prepare Environment"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#things-to-do","text":"create a cluster install and configure Helm for easily installing other applications on the cluster install and configure Certmanager for managing TLS certificates with Let's Encrypt","title":"Things To Do"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#create-gke-cluster","text":"Enough talk about what we should be doing, let's create the cluster!","title":"Create GKE Cluster"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#prerequisites","text":"gcloud command-line utility Google Cloud account that is activated","title":"Prerequisites"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#variables","text":"Variables we need for the gcloud create cluster command. To make it easy to copy and paste the command. K8S_VERSION = 1 .13.7-gke.8 REGION = europe-west4 CLUSTER_NAME = <your cluster name> PROJECT_ID = <your google project id>","title":"Variables"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#query-available-versions","text":"If you want to see which versions are available in your Google Cloud Region, set the REGION variable and execute the command below. The list you get back will contain two lists, one for worker nodes and one for master nodes . Only the versions for master nodes can be used to create a cluster. gcloud container get-server-config --region $REGION","title":"Query available versions"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#create-cluster","text":"gcloud container clusters create ${ CLUSTER_NAME } \\ --region ${ REGION } \\ --cluster-version ${ K8S_VERSION } \\ --num-nodes 2 --machine-type n1-standard-2 \\ --addons = HorizontalPodAutoscaling \\ --min-nodes 2 --max-nodes 3 \\ --enable-autoupgrade \\ --enable-autoscaling \\ --enable-network-policy \\ --labels = purpose = practice","title":"Create Cluster"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#set-clusteradmin","text":"For some later commands, such as Helm, we need to be ClusterAdmin. kubectl create clusterrolebinding \\ cluster-admin-binding \\ --clusterrole cluster-admin \\ --user $( gcloud config get-value account )","title":"Set ClusterAdmin"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#install-ingress-controller","text":"An ingress controller is what allows you to access the applications you install on your Kubernetes cluster from the outside. We need to do this for the tools we will use. So we need to install an ingress controller. Any will do, but ingress-nginx (based on the widely use nginx application) is the most commonly used. kubectl apply \\ -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/1cd17cd12c98563407ad03812aebac46ca4442f2/deploy/mandatory.yaml kubectl apply \\ -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/1cd17cd12c98563407ad03812aebac46ca4442f2/deploy/provider/cloud-generic.yaml For exposing our applications to the outside, we need to have a valid DNS name. For that, we need to have the IP address of our LoadBalancer. The command below retrieves that address. If it is empty, wait a few minutes and try again. export LB_IP = $( kubectl -n ingress-nginx \\ get svc ingress-nginx \\ -o jsonpath = \"{.status.loadBalancer.ingress[0].ip}\" ) echo $LB_IP Tip If you don't get an address back, check to see if your ingress controller has a service and that the service has an EXTERNAL IP address. kubectl get svc -n ingress-nginx -o wide The response should look something like this: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx LoadBalancer 10.48.14.43 34.90.67.21 80:32762/TCP,443:31389/TCP 21d","title":"Install Ingress Controller"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#install-helm","text":"Helm is a, or the , package manager for Kubernetes. We will use it to install the other applications. Read more here . kubectl create \\ -f https://raw.githubusercontent.com/vfarcic/k8s-specs/master/helm/tiller-rbac.yml \\ --record --save-config Now that we've installed Helm, we can initialize the server component via a helm init . helm init --service-account tiller And now we wait. kubectl -n kube-system \\ rollout status deploy tiller-deploy","title":"Install Helm"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#install-cert-manager","text":"Cert-manager will help users automate installing TLS Certificates. Read more about cert-manager here . This creates the cert-manager specific resource definitions, also call CustomerResourceDefinitions or ***CRD***s. kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8/deploy/manifests/00-crds.yaml Due to how cert-manager works, it is best installed into its own namespace. There's a chicken and egg problem because it needs a Root Certificate Authority (or, RootCA) to exist, but every Certificate needs to be validated against this Certificate. Which is why we add the special label. kubectl create namespace cert-manager kubectl label namespace cert-manager certmanager.k8s.io/disable-validation = true Now that the CRD's and the namespace are ready, we can install cert-manager. Well, almost. The Helm Chart - that is how we call Helm packages - is in another Castle, eh, Helm Repository. So we first have to tell Helm where to get the Chart. helm repo add jetstack https://charts.jetstack.io helm repo update We can now install cert-manager via Helm! helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.8.0 \\ jetstack/cert-manager","title":"Install Cert-Manager"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#configure-clusterissuer","text":"Cert-manager can leverage Let's Encrypt to generate valid certificates. We need to instruct cert-manager which service to use, we do that by creating a ClusterIssuer resource. kubectl apply -f cluster-issuer.yaml cluster-issuer.yaml Don't forget to replace <replacewith your email address> with an actual email address you can access. apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-prod spec : acme : email : <replacewith your email address> server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-prod http01 : {}","title":"Configure ClusterIssuer"},{"location":"certificates/lets-encrypt-k8s/","text":"Let's Encrypt for Kubernetes \u00b6 Let's Encrypt is a free, automated and open Certificate Authority. The kind of service you need if you want to have a secure website with https - yes I know that requires more than that - and it's now more straightforward to use than ever. This about using Let's Encrypt for generating a certificate for your service on Kubernetes. There are several ways to do this, with more or less automation, cluster-wide or namespace bound or with a DNS or HTTP validation check. I'll choose the route that was the easiest for me, and then I'll briefly look at the other options. Prerequisites \u00b6 There are some prerequisites required, that are best discussed on their own. So we will continue with the assumption that you have these in order. valid Class A or CNAME domain name kubernetes cluster with ingress controller (such as nginx) with helm and tiller installed in the cluster web application Steps \u00b6 The steps to take to get a web application to get a certificate from Let's Encrypt are the following. install cert-manager from the official helm chart deploy a Issuer resource deploy a certificate resource confirm certificate and secret are created/filled use in web app Install Cert Manager \u00b6 For more details on Cert Manager, I recommend reading their introduction . In essence, it's a tool that helps you initiate a certificate request with a service such as Let's Encrypt. You can install it via Helm, and it's meant to be installed only once per cluster. The once per cluster restriction is due to the usage of Custom Resource Definitions (CRD's) which will block any (re-)installation. To confirm if there are any CRD's from cert-manager, you can issue the following command. kubectl get customresourcedefinitions.apiextensions.k8s.io The CRD's belonging to cert-manager are the following: certificates.certmanager.k8s.io clusterissuers.certmanager.k8s.io issuers.certmanager.k8s.io You will notice later, that we will use these CRD's for getting our certificate, so keep them in mind. To remove them in case of a re-install, you can issue the command below. kubectl delete customresourcedefinitions.apiextensions.k8s.io \\ certificates.certmanager.k8s.io \\ clusterissuers.certmanager.k8s.io \\ issuers.certmanager.k8s.io When you're sure there are no CRD's left, you can install cert-manager via it's helm chart. It has some options in case you need them, you read about them here , but in my case that wasn't needed. helm install --name cert-manager --namespace default stable/cert-manager Deploy Issuer \u00b6 To be able to use a certificate we need to have a Certificate Issuer. If you remember from our cert-manager , there are two CRD's that can take this role: ClusterIssuer : clusterissuers.certmanager.k8s.io Issuer : issuers.certmanager.k8s.io Both issuer type can use two ways of providing the proof of ownership, either by dns-01 or http-01 . We'll be using the http-01 method, for the dns-01 method, refer to the cert-manager documenation . ClusterIssuer \u00b6 As the resource Kind implies, a ClusterIssuer is a cluster-wide resource and not bound to a specific namespace. apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : # The ACME server URL server : https://acme-staging.api.letsencrypt.org/directory # Email address used for ACME registration email : user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} Issuer \u00b6 Not everyone wants a cluster-wide resource, and not everyone has the rights to install something elsewhere than their namespace. I prefer having as much as possible tied to a namespace - either a team or an application - I will use this type. apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : myapp-letsencrypt-staging namespace : myapp spec : acme : # The ACME server URL server : https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email : myadmin@myapp.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : myapp-letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} There's a few things to note here: server : this refers to the server executing the ACME test, in this case: Let's Encrypt Staging (with the v2 API) email : this will be the account it will use for registering the certificate privateKeySecretRef : this is the Kubernetes secret resource in which the privateKey will be stored, just in case you need or want to remove it Deploy Certificate Resource \u00b6 Next up is our Certificate resource, this is where cert-manager will store our certificate details to be used by our application. In case you forgot, this is one of the three CRD's provided by cert-manager. apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : myapp-example-com namespace : myapp spec : secretName : myapp-tls dnsNames : - myapp.example.com acme : config : - http01 : ingressClass : nginx domains : - myapp.example.com issuerRef : name : myapp-letsencrypt-staging kind : Issuer The things to note here: name : so far I've found it a naming convention to write the domain name where - replaces the . 's. secretName : the name of the Kubernetes secret that will house the certificate and certificate key dnsNames : you can specify more than one name, in our case just a single one, should match acme.config.domains acme.config : this defines the configuration for how the ownership proof should be done, this should match the method defined in the Issuer issuerRef : in good Kubernetes fashion, we reference the Issuer that should issue our certificate, the name and kind should match our Issue resource Confirm Resources \u00b6 We have defined our set of resources that should create our valid - though not trusted, as it is staging - certificate. Before we use it, we should confirm our secret and our certificate are both valid. kubectl describe certificate myapp-example-com --namespace myapp The response includes the latest status, which looks like this: Status : Acme : Order : URL : https://acme-staging-v02.api.letsencrypt.org/acme/order/705.../960... Conditions : Last Transition Time : 2018-10-02T21:17:34Z Message : Certificate issued successfully Reason : CertIssued Status : True Type : Ready Next up is our secret, containing the actual certificate and the certificate key. kubectl describe secret myapp-tls --namespace myapp Which results in something like this: Name: myapp-tls Namespace: myapp Labels: certmanager.k8s.io/certificate-name = myapp-example-com Annotations: certmanager.k8s.io/alt-names: myapp.example.com certmanager.k8s.io/common-name: myapp.example.com certmanager.k8s.io/issuer-kind: Issuer certmanager.k8s.io/issuer-name: myapp-letsencrypt-staging Type: kubernetes.io/tls Data ==== tls.crt: 3797 bytes tls.key: 1679 bytes Use certificate to enable https \u00b6 Assuming the secret and the certificate are correct, we can use them to enable https on our web app. We put the tls certificate on the ingress of the application, in this case, Nginx, which assumes the following about the app: it has a deployment or stateful set it has a service which provides and endpoint to one or more instances it has an nginx ingress which points to the service Deployment \u00b6 kind : Deployment apiVersion : apps/v1 metadata : name : myapp namespace : myapp labels : app : myapp spec : replicas : 3 selector : matchLabels : app : myapp template : metadata : labels : app : myapp spec : containers : - name : myapp image : caladreas/catnip-master imagePullPolicy : Always ports : - containerPort : 8087 You should always include livenessprobe, resource limits and so on. But for the sake of brevity, these are omitted! Service \u00b6 apiVersion : v1 kind : Service metadata : name : myapp namespace : myapp labels : app : myapp spec : selector : app : myapp ports : - name : http port : 80 targetPort : 8087 protocol : TCP Ingress \u00b6 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : \"nginx\" ingress.kubernetes.io/ssl-redirect : \"true\" certmanager.k8s.io/issuer-kind : Issuer certmanager.k8s.io/issuer-name : myapp-letsencrypt-staging spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls Further resources \u00b6 How Does Let's Encrypt Work Tutorial that inspired this page Amazone EKS Ingress Guide Kuberetes EKS Ingress and TLS How To Configure LTS for Nginx Ingress","title":"Let's Encrypt K8S"},{"location":"certificates/lets-encrypt-k8s/#lets-encrypt-for-kubernetes","text":"Let's Encrypt is a free, automated and open Certificate Authority. The kind of service you need if you want to have a secure website with https - yes I know that requires more than that - and it's now more straightforward to use than ever. This about using Let's Encrypt for generating a certificate for your service on Kubernetes. There are several ways to do this, with more or less automation, cluster-wide or namespace bound or with a DNS or HTTP validation check. I'll choose the route that was the easiest for me, and then I'll briefly look at the other options.","title":"Let's Encrypt for Kubernetes"},{"location":"certificates/lets-encrypt-k8s/#prerequisites","text":"There are some prerequisites required, that are best discussed on their own. So we will continue with the assumption that you have these in order. valid Class A or CNAME domain name kubernetes cluster with ingress controller (such as nginx) with helm and tiller installed in the cluster web application","title":"Prerequisites"},{"location":"certificates/lets-encrypt-k8s/#steps","text":"The steps to take to get a web application to get a certificate from Let's Encrypt are the following. install cert-manager from the official helm chart deploy a Issuer resource deploy a certificate resource confirm certificate and secret are created/filled use in web app","title":"Steps"},{"location":"certificates/lets-encrypt-k8s/#install-cert-manager","text":"For more details on Cert Manager, I recommend reading their introduction . In essence, it's a tool that helps you initiate a certificate request with a service such as Let's Encrypt. You can install it via Helm, and it's meant to be installed only once per cluster. The once per cluster restriction is due to the usage of Custom Resource Definitions (CRD's) which will block any (re-)installation. To confirm if there are any CRD's from cert-manager, you can issue the following command. kubectl get customresourcedefinitions.apiextensions.k8s.io The CRD's belonging to cert-manager are the following: certificates.certmanager.k8s.io clusterissuers.certmanager.k8s.io issuers.certmanager.k8s.io You will notice later, that we will use these CRD's for getting our certificate, so keep them in mind. To remove them in case of a re-install, you can issue the command below. kubectl delete customresourcedefinitions.apiextensions.k8s.io \\ certificates.certmanager.k8s.io \\ clusterissuers.certmanager.k8s.io \\ issuers.certmanager.k8s.io When you're sure there are no CRD's left, you can install cert-manager via it's helm chart. It has some options in case you need them, you read about them here , but in my case that wasn't needed. helm install --name cert-manager --namespace default stable/cert-manager","title":"Install Cert Manager"},{"location":"certificates/lets-encrypt-k8s/#deploy-issuer","text":"To be able to use a certificate we need to have a Certificate Issuer. If you remember from our cert-manager , there are two CRD's that can take this role: ClusterIssuer : clusterissuers.certmanager.k8s.io Issuer : issuers.certmanager.k8s.io Both issuer type can use two ways of providing the proof of ownership, either by dns-01 or http-01 . We'll be using the http-01 method, for the dns-01 method, refer to the cert-manager documenation .","title":"Deploy Issuer"},{"location":"certificates/lets-encrypt-k8s/#clusterissuer","text":"As the resource Kind implies, a ClusterIssuer is a cluster-wide resource and not bound to a specific namespace. apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : # The ACME server URL server : https://acme-staging.api.letsencrypt.org/directory # Email address used for ACME registration email : user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {}","title":"ClusterIssuer"},{"location":"certificates/lets-encrypt-k8s/#issuer","text":"Not everyone wants a cluster-wide resource, and not everyone has the rights to install something elsewhere than their namespace. I prefer having as much as possible tied to a namespace - either a team or an application - I will use this type. apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : myapp-letsencrypt-staging namespace : myapp spec : acme : # The ACME server URL server : https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email : myadmin@myapp.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : myapp-letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} There's a few things to note here: server : this refers to the server executing the ACME test, in this case: Let's Encrypt Staging (with the v2 API) email : this will be the account it will use for registering the certificate privateKeySecretRef : this is the Kubernetes secret resource in which the privateKey will be stored, just in case you need or want to remove it","title":"Issuer"},{"location":"certificates/lets-encrypt-k8s/#deploy-certificate-resource","text":"Next up is our Certificate resource, this is where cert-manager will store our certificate details to be used by our application. In case you forgot, this is one of the three CRD's provided by cert-manager. apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : myapp-example-com namespace : myapp spec : secretName : myapp-tls dnsNames : - myapp.example.com acme : config : - http01 : ingressClass : nginx domains : - myapp.example.com issuerRef : name : myapp-letsencrypt-staging kind : Issuer The things to note here: name : so far I've found it a naming convention to write the domain name where - replaces the . 's. secretName : the name of the Kubernetes secret that will house the certificate and certificate key dnsNames : you can specify more than one name, in our case just a single one, should match acme.config.domains acme.config : this defines the configuration for how the ownership proof should be done, this should match the method defined in the Issuer issuerRef : in good Kubernetes fashion, we reference the Issuer that should issue our certificate, the name and kind should match our Issue resource","title":"Deploy Certificate Resource"},{"location":"certificates/lets-encrypt-k8s/#confirm-resources","text":"We have defined our set of resources that should create our valid - though not trusted, as it is staging - certificate. Before we use it, we should confirm our secret and our certificate are both valid. kubectl describe certificate myapp-example-com --namespace myapp The response includes the latest status, which looks like this: Status : Acme : Order : URL : https://acme-staging-v02.api.letsencrypt.org/acme/order/705.../960... Conditions : Last Transition Time : 2018-10-02T21:17:34Z Message : Certificate issued successfully Reason : CertIssued Status : True Type : Ready Next up is our secret, containing the actual certificate and the certificate key. kubectl describe secret myapp-tls --namespace myapp Which results in something like this: Name: myapp-tls Namespace: myapp Labels: certmanager.k8s.io/certificate-name = myapp-example-com Annotations: certmanager.k8s.io/alt-names: myapp.example.com certmanager.k8s.io/common-name: myapp.example.com certmanager.k8s.io/issuer-kind: Issuer certmanager.k8s.io/issuer-name: myapp-letsencrypt-staging Type: kubernetes.io/tls Data ==== tls.crt: 3797 bytes tls.key: 1679 bytes","title":"Confirm Resources"},{"location":"certificates/lets-encrypt-k8s/#use-certificate-to-enable-https","text":"Assuming the secret and the certificate are correct, we can use them to enable https on our web app. We put the tls certificate on the ingress of the application, in this case, Nginx, which assumes the following about the app: it has a deployment or stateful set it has a service which provides and endpoint to one or more instances it has an nginx ingress which points to the service","title":"Use certificate to enable https"},{"location":"certificates/lets-encrypt-k8s/#deployment","text":"kind : Deployment apiVersion : apps/v1 metadata : name : myapp namespace : myapp labels : app : myapp spec : replicas : 3 selector : matchLabels : app : myapp template : metadata : labels : app : myapp spec : containers : - name : myapp image : caladreas/catnip-master imagePullPolicy : Always ports : - containerPort : 8087 You should always include livenessprobe, resource limits and so on. But for the sake of brevity, these are omitted!","title":"Deployment"},{"location":"certificates/lets-encrypt-k8s/#service","text":"apiVersion : v1 kind : Service metadata : name : myapp namespace : myapp labels : app : myapp spec : selector : app : myapp ports : - name : http port : 80 targetPort : 8087 protocol : TCP","title":"Service"},{"location":"certificates/lets-encrypt-k8s/#ingress","text":"apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : \"nginx\" ingress.kubernetes.io/ssl-redirect : \"true\" certmanager.k8s.io/issuer-kind : Issuer certmanager.k8s.io/issuer-name : myapp-letsencrypt-staging spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls","title":"Ingress"},{"location":"certificates/lets-encrypt-k8s/#further-resources","text":"How Does Let's Encrypt Work Tutorial that inspired this page Amazone EKS Ingress Guide Kuberetes EKS Ingress and TLS How To Configure LTS for Nginx Ingress","title":"Further resources"},{"location":"cloud/automation-pulumi/","text":"Pulumi \u00b6 Just pointing to the documentation and GitHub repository is a bit boring and most of all, lazy. So I've gone through the trouble to create a demo project that shows what Pulumi can do. One of the things to keep in mind is that Pulumi keeps its state in a remote location by default - Pulumi's SaaS service. Much like using an S3 bucket to store your Terraform state. The goal of the demo is to show what you can do with Pulumi. My personal opinion is that Pulumi is excellent for managing entire systems as code. From public cloud infrastructure to public open source resources right down to your applications and their configurations. The system that I've written with Pulumi is a CI/CD system running in a GKE cluster. The system contains Jenkins, Artifactory and a custom LDAP server. To make it interesting, we'll use different aspects of Pulumi to create the system. We will create the GKE cluster via Pulumi's wrapper for the gcloud cli - in a similar way as JenkinsX does -, install Jenkins and Artifactory via Helm and install the LDAP server via Pulumi's Kubernetes resource classes. Steps taken \u00b6 For more info Pulumi.io install: brew install pulumi clone demo: git clone https://github.com/demomon/pulumi-demo-1 init stack: pulumi stack init demomon-pulumi-demo-1 connect to GitHub set kubernetes config pulumi config set kubernetes:context gke_ps-dev-201405_europe-west4_joostvdg-reg-dec18-1 pulumi config set isMinikube false install npm resources: npm install (I've used the TypeScript demo's) pulumi config set username administrator pulumi config set password 3OvlgaockdnTsYRU5JAcgM1o --secret pulumi preview incase pulumi loses your stack: pulumi stack select demomon-pulumi-demo-1 pulumi destroy Based on the following demo's: https://pulumi.io/quickstart/kubernetes/tutorial-exposed-deployment.html https://github.com/pulumi/examples/tree/master/kubernetes-ts-jenkins Artifactory via Helm \u00b6 To install the helm charts of Artifactory using Helm, we will first need to add JFrog's repository. helm repo add jfrog https://charts.jfrog.io helm repo update GKE Cluster \u00b6 Below is the code for the cluster. import * as gcp from \"@pulumi/gcp\" ; import * as k8s from \"@pulumi/kubernetes\" ; import * as pulumi from \"@pulumi/pulumi\" ; import { nodeCount , nodeMachineType , password , username } from \"./gke-config\" ; export const k8sCluster = new gcp . container . Cluster ( \"gke-cluster\" , { name : \"joostvdg-dec-2018-pulumi\" , initialNodeCount : nodeCount , nodeVersion : \"latest\" , minMasterVersion : \"latest\" , nodeConfig : { machineType : nodeMachineType , oauthScopes : [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" ], }, }); GKE Config \u00b6 As you could see, we import variables from a configuration file gke-config . import { Config } from \"@pulumi/pulumi\" ; const config = new Config (); export const nodeCount = config . getNumber ( \"nodeCount\" ) || 3 ; export const nodeMachineType = config . get ( \"nodeMachineType\" ) || \"n1-standard-2\" ; // username is the admin username for the cluster. export const username = config . get ( \"username\" ) || \"admin\" ; // password is the password for the admin user in the cluster. export const password = config . require ( \"password\" ); Kubeconfig \u00b6 As you probably will want to install other things (Helm charts, services) inside this cluster, we need to make sure we get the kubeconfig file of our cluster - which is yet to be created. Below is the code - courtesy of Pulumi's GKE Example - that generates and exports the Kubernetes Client Configuration. This client configuration can then be used by Helm charts or other Kubernetes services. Pulumi will then also understand it depends on the cluster and create/update it first before moving on to the others. // Manufacture a GKE-style Kubeconfig. Note that this is slightly \"different\" because of the way GKE requires // gcloud to be in the picture for cluster authentication (rather than using the client cert/key directly). export const k8sConfig = pulumi . all ([ k8sCluster . name , k8sCluster . endpoint , k8sCluster . masterAuth ]). apply (([ name , endpoint , auth ]) => { const context = ` ${ gcp . config . project } _ ${ gcp . config . zone } _ ${ name } ` ; return `apiVersion: v1 clusters: - cluster: certificate-authority-data: ${ auth . clusterCaCertificate } server: https:// ${ endpoint } name: ${ context } contexts: - context: cluster: ${ context } user: ${ context } name: ${ context } current-context: ${ context } kind: Config preferences: {} users: - name: ${ context } user: auth-provider: config: cmd-args: config config-helper --format=json cmd-path: gcloud expiry-key: '{.credential.token_expiry}' token-key: '{.credential.access_token}' name: gcp ` ; }); // Export a Kubernetes provider instance that uses our cluster from above. export const k8sProvider = new k8s . Provider ( \"gkeK8s\" , { kubeconfig : k8sConfig , }); Pulumi GCP Config \u00b6 https://github.com/pulumi/examples/blob/master/gcp-ts-gke/README.md export GCP_PROJECT = ... export GCP_ZONE = europe-west4-a export CLUSTER_PASSWORD = ... export GCP_SA_NAME = ... Make sure you have a Google SA (Service Account) by that name first, as you can read here . For me it worked best to NOT set any environment variables mentioned. They invairably caused authentication or authorization issues. Just make sure the SA account and it's credential file (see below) are authorized and the gcloud cli works. gcloud iam service-accounts keys create gcp-credentials.json \\ --iam-account ${ GCP_SA_NAME } @ ${ GCP_PROJECT } .iam.gserviceaccount.com gcloud auth activate-service-account --key-file gcp-credentials.json gcloud auth application-default login pulumi config set gcp:project ${ GCP_PROJECT } pulumi config set gcp:zone ${ GCP_ZONE } pulumi config set password --secret ${ CLUSTER_PASSWORD } Post Cluster Creation \u00b6 gcloud container clusters get-credentials joostvdg-dec-2018-pulumi kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $( gcloud config get-value account ) Install failed \u00b6 Failed to install kubernetes:rbac.authorization.k8s.io:Role artifactory-artifactory . Probably due to missing rights, so probably have to execute the admin binding before the helm charts. error: Plan apply failed: roles.rbac.authorization.k8s.io \"artifactory-artifactory\" is forbidden: attempt to grant extra privileges: ... Helm Charts \u00b6 Using Pulumi to install a Helm Chart feels a bit like adding layers of wrapping upon wrapping. The power of Pulumi becomes visible when using more than one related service on the same cluster - for example a SDLC Tool Chain. This example application installs two helm charts, Jenkins and Artifactory, on a GKE cluster that is also created and managed by Pulumi. Below is an example of installing a Helm chart of Jenkins, where we provide the Kubernetes config from the GKE cluster as Provider. This way, Pulumi knows it must install the helm chart in that GKE cluster and not in the current Kubeconfig. import { k8sProvider , k8sConfig } from \"./gke-cluster\" ; const jenkins = new k8s . helm . v2 . Chart ( \"jenkins\" , { repo : \"stable\" , version : \"0.25.1\" , chart : \"jenkins\" , }, { providers : { kubernetes : k8sProvider } } ); Deployment & Service \u00b6 First, make sure you have an interface for the configuration arguments. export interface LdapArgs { readonly name : string , readonly imageName : string , readonly imageTag : string } Then, create a exportable Pulumi resource class that can be reused. export class LdapInstallation extends pulumi . ComponentResource { public readonly deployment : k8s.apps.v1.Deployment ; public readonly service : k8s.core.v1.Service ; // constructor } Inside the constructor placehold we will create a constructor method. It will do all the configuration we need to do for this resource, in this case a Kubernetes Service and Deployment. constructor ( args : LdapArgs ) { super ( \"k8stypes:service:LdapInstallation\" , args . name , {}); const labels = { app : args.name }; const name = args . name } First Kubernetes resource to create is a container specification for the Deployment. const container : k8stypes.core.v1.Container = { name , image : args.imageName + \":\" + args . imageTag , resources : { requests : { cpu : \"100m\" , memory : \"200Mi\" }, limits : { cpu : \"100m\" , memory : \"200Mi\" }, }, ports : [{ name : \"ldap\" , containerPort : 1389 , }, ] }; As the configuration arguments can be any TypeScript type, you can allow people to override entire segments (such as Resources). Which you would do as follows: resources : args.resources || { requests : { cpu : \"100m\" , memory : \"200Mi\" }, limits : { cpu : \"100m\" , memory : \"200Mi\" }, }, The Deployment and Service construction are quite similar. this . deployment = new k8s . apps . v1 . Deployment ( args . name , { spec : { selector : { matchLabels : labels }, replicas : 1 , template : { metadata : { labels : labels }, spec : { containers : [ container ] }, }, }, },{ provider : cluster.k8sProvider }); this . service = new k8s . core . v1 . Service ( args . name , { metadata : { labels : this.deployment.metadata.apply ( meta => meta . labels ), }, spec : { ports : [{ name : \"ldap\" , port : 389 , targetPort : \"ldap\" , protocol : \"TCP\" }, ], selector : this.deployment.spec.apply ( spec => spec . template . metadata . labels ), type : \"ClusterIP\" , }, }, { provider : cluster.k8sProvider });","title":"Pulumi"},{"location":"cloud/automation-pulumi/#pulumi","text":"Just pointing to the documentation and GitHub repository is a bit boring and most of all, lazy. So I've gone through the trouble to create a demo project that shows what Pulumi can do. One of the things to keep in mind is that Pulumi keeps its state in a remote location by default - Pulumi's SaaS service. Much like using an S3 bucket to store your Terraform state. The goal of the demo is to show what you can do with Pulumi. My personal opinion is that Pulumi is excellent for managing entire systems as code. From public cloud infrastructure to public open source resources right down to your applications and their configurations. The system that I've written with Pulumi is a CI/CD system running in a GKE cluster. The system contains Jenkins, Artifactory and a custom LDAP server. To make it interesting, we'll use different aspects of Pulumi to create the system. We will create the GKE cluster via Pulumi's wrapper for the gcloud cli - in a similar way as JenkinsX does -, install Jenkins and Artifactory via Helm and install the LDAP server via Pulumi's Kubernetes resource classes.","title":"Pulumi"},{"location":"cloud/automation-pulumi/#steps-taken","text":"For more info Pulumi.io install: brew install pulumi clone demo: git clone https://github.com/demomon/pulumi-demo-1 init stack: pulumi stack init demomon-pulumi-demo-1 connect to GitHub set kubernetes config pulumi config set kubernetes:context gke_ps-dev-201405_europe-west4_joostvdg-reg-dec18-1 pulumi config set isMinikube false install npm resources: npm install (I've used the TypeScript demo's) pulumi config set username administrator pulumi config set password 3OvlgaockdnTsYRU5JAcgM1o --secret pulumi preview incase pulumi loses your stack: pulumi stack select demomon-pulumi-demo-1 pulumi destroy Based on the following demo's: https://pulumi.io/quickstart/kubernetes/tutorial-exposed-deployment.html https://github.com/pulumi/examples/tree/master/kubernetes-ts-jenkins","title":"Steps taken"},{"location":"cloud/automation-pulumi/#artifactory-via-helm","text":"To install the helm charts of Artifactory using Helm, we will first need to add JFrog's repository. helm repo add jfrog https://charts.jfrog.io helm repo update","title":"Artifactory via Helm"},{"location":"cloud/automation-pulumi/#gke-cluster","text":"Below is the code for the cluster. import * as gcp from \"@pulumi/gcp\" ; import * as k8s from \"@pulumi/kubernetes\" ; import * as pulumi from \"@pulumi/pulumi\" ; import { nodeCount , nodeMachineType , password , username } from \"./gke-config\" ; export const k8sCluster = new gcp . container . Cluster ( \"gke-cluster\" , { name : \"joostvdg-dec-2018-pulumi\" , initialNodeCount : nodeCount , nodeVersion : \"latest\" , minMasterVersion : \"latest\" , nodeConfig : { machineType : nodeMachineType , oauthScopes : [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" ], }, });","title":"GKE Cluster"},{"location":"cloud/automation-pulumi/#gke-config","text":"As you could see, we import variables from a configuration file gke-config . import { Config } from \"@pulumi/pulumi\" ; const config = new Config (); export const nodeCount = config . getNumber ( \"nodeCount\" ) || 3 ; export const nodeMachineType = config . get ( \"nodeMachineType\" ) || \"n1-standard-2\" ; // username is the admin username for the cluster. export const username = config . get ( \"username\" ) || \"admin\" ; // password is the password for the admin user in the cluster. export const password = config . require ( \"password\" );","title":"GKE Config"},{"location":"cloud/automation-pulumi/#kubeconfig","text":"As you probably will want to install other things (Helm charts, services) inside this cluster, we need to make sure we get the kubeconfig file of our cluster - which is yet to be created. Below is the code - courtesy of Pulumi's GKE Example - that generates and exports the Kubernetes Client Configuration. This client configuration can then be used by Helm charts or other Kubernetes services. Pulumi will then also understand it depends on the cluster and create/update it first before moving on to the others. // Manufacture a GKE-style Kubeconfig. Note that this is slightly \"different\" because of the way GKE requires // gcloud to be in the picture for cluster authentication (rather than using the client cert/key directly). export const k8sConfig = pulumi . all ([ k8sCluster . name , k8sCluster . endpoint , k8sCluster . masterAuth ]). apply (([ name , endpoint , auth ]) => { const context = ` ${ gcp . config . project } _ ${ gcp . config . zone } _ ${ name } ` ; return `apiVersion: v1 clusters: - cluster: certificate-authority-data: ${ auth . clusterCaCertificate } server: https:// ${ endpoint } name: ${ context } contexts: - context: cluster: ${ context } user: ${ context } name: ${ context } current-context: ${ context } kind: Config preferences: {} users: - name: ${ context } user: auth-provider: config: cmd-args: config config-helper --format=json cmd-path: gcloud expiry-key: '{.credential.token_expiry}' token-key: '{.credential.access_token}' name: gcp ` ; }); // Export a Kubernetes provider instance that uses our cluster from above. export const k8sProvider = new k8s . Provider ( \"gkeK8s\" , { kubeconfig : k8sConfig , });","title":"Kubeconfig"},{"location":"cloud/automation-pulumi/#pulumi-gcp-config","text":"https://github.com/pulumi/examples/blob/master/gcp-ts-gke/README.md export GCP_PROJECT = ... export GCP_ZONE = europe-west4-a export CLUSTER_PASSWORD = ... export GCP_SA_NAME = ... Make sure you have a Google SA (Service Account) by that name first, as you can read here . For me it worked best to NOT set any environment variables mentioned. They invairably caused authentication or authorization issues. Just make sure the SA account and it's credential file (see below) are authorized and the gcloud cli works. gcloud iam service-accounts keys create gcp-credentials.json \\ --iam-account ${ GCP_SA_NAME } @ ${ GCP_PROJECT } .iam.gserviceaccount.com gcloud auth activate-service-account --key-file gcp-credentials.json gcloud auth application-default login pulumi config set gcp:project ${ GCP_PROJECT } pulumi config set gcp:zone ${ GCP_ZONE } pulumi config set password --secret ${ CLUSTER_PASSWORD }","title":"Pulumi GCP Config"},{"location":"cloud/automation-pulumi/#post-cluster-creation","text":"gcloud container clusters get-credentials joostvdg-dec-2018-pulumi kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $( gcloud config get-value account )","title":"Post Cluster Creation"},{"location":"cloud/automation-pulumi/#install-failed","text":"Failed to install kubernetes:rbac.authorization.k8s.io:Role artifactory-artifactory . Probably due to missing rights, so probably have to execute the admin binding before the helm charts. error: Plan apply failed: roles.rbac.authorization.k8s.io \"artifactory-artifactory\" is forbidden: attempt to grant extra privileges: ...","title":"Install failed"},{"location":"cloud/automation-pulumi/#helm-charts","text":"Using Pulumi to install a Helm Chart feels a bit like adding layers of wrapping upon wrapping. The power of Pulumi becomes visible when using more than one related service on the same cluster - for example a SDLC Tool Chain. This example application installs two helm charts, Jenkins and Artifactory, on a GKE cluster that is also created and managed by Pulumi. Below is an example of installing a Helm chart of Jenkins, where we provide the Kubernetes config from the GKE cluster as Provider. This way, Pulumi knows it must install the helm chart in that GKE cluster and not in the current Kubeconfig. import { k8sProvider , k8sConfig } from \"./gke-cluster\" ; const jenkins = new k8s . helm . v2 . Chart ( \"jenkins\" , { repo : \"stable\" , version : \"0.25.1\" , chart : \"jenkins\" , }, { providers : { kubernetes : k8sProvider } } );","title":"Helm Charts"},{"location":"cloud/automation-pulumi/#deployment-service","text":"First, make sure you have an interface for the configuration arguments. export interface LdapArgs { readonly name : string , readonly imageName : string , readonly imageTag : string } Then, create a exportable Pulumi resource class that can be reused. export class LdapInstallation extends pulumi . ComponentResource { public readonly deployment : k8s.apps.v1.Deployment ; public readonly service : k8s.core.v1.Service ; // constructor } Inside the constructor placehold we will create a constructor method. It will do all the configuration we need to do for this resource, in this case a Kubernetes Service and Deployment. constructor ( args : LdapArgs ) { super ( \"k8stypes:service:LdapInstallation\" , args . name , {}); const labels = { app : args.name }; const name = args . name } First Kubernetes resource to create is a container specification for the Deployment. const container : k8stypes.core.v1.Container = { name , image : args.imageName + \":\" + args . imageTag , resources : { requests : { cpu : \"100m\" , memory : \"200Mi\" }, limits : { cpu : \"100m\" , memory : \"200Mi\" }, }, ports : [{ name : \"ldap\" , containerPort : 1389 , }, ] }; As the configuration arguments can be any TypeScript type, you can allow people to override entire segments (such as Resources). Which you would do as follows: resources : args.resources || { requests : { cpu : \"100m\" , memory : \"200Mi\" }, limits : { cpu : \"100m\" , memory : \"200Mi\" }, }, The Deployment and Service construction are quite similar. this . deployment = new k8s . apps . v1 . Deployment ( args . name , { spec : { selector : { matchLabels : labels }, replicas : 1 , template : { metadata : { labels : labels }, spec : { containers : [ container ] }, }, }, },{ provider : cluster.k8sProvider }); this . service = new k8s . core . v1 . Service ( args . name , { metadata : { labels : this.deployment.metadata.apply ( meta => meta . labels ), }, spec : { ports : [{ name : \"ldap\" , port : 389 , targetPort : \"ldap\" , protocol : \"TCP\" }, ], selector : this.deployment.spec.apply ( spec => spec . template . metadata . labels ), type : \"ClusterIP\" , }, }, { provider : cluster.k8sProvider });","title":"Deployment &amp; Service"},{"location":"cloudbees/cbc-eks/","text":"CloudBees Core on AWS EKS \u00b6 The basics of installing CloudBees Core on EKS can found in the CloudBees Core Install Guide . For details around the architecture and possibilities for TLS termination (L4 or L7 with ELB), see the CloudBees Core EKS Manual Install guide . This guide is here to do the installation with TLS on the Ingress Controller and certificates managed by Let's Encrypt. It will also go beyond the installation and continue with configuration of the Operations Center and create some Masters. Note This guide is originally written during 2019, when CloudBees Core didn't have a helm chart yet. If you prefer a Helm install, please refer to CloudBees Core EKS Helm Install guide . Create EKS Cluster \u00b6 See my guide on creating a EKS cluster with EKSCTL , which is the recommended solution with regards to Kubernetes on AWS. Certmanager \u00b6 echo \"apiVersion: certmanager.k8s.io/v1alpha1 kind: Certificate metadata: name: cloudbeescore-kearos-net namespace: cje spec: secretName: cjoc-tls-prd dnsNames: - cloudbees-core.kearos.net acme: config: - http01: ingressClass: nginx domains: - cloudbees-core.kearos.net issuerRef: name: letsencrypt-prd kind: ClusterIssuer\" > cjoc-cert.yml eks apply -f cjoc-cert.yml Create Namespace CJE \u00b6 echo \"apiVersion: v1 kind: Namespace metadata: labels: name: cje name: cje\" > cje-namespace.yaml eks create -f cje-namespace.yaml eks config set-context $( eks config current-context ) --namespace = cje CB Core Install \u00b6 Download from downloads.cloudbees.com Configure DNS \u00b6 DOMAIN_NAME = cloudbees-core.kearos.net sed -e s,cje.example.com, $DOMAIN_NAME ,g < cloudbees-core.yml > tmp && mv tmp cloudbees-core.yml Configure k8s yaml file: add certmanager.k8s.io/cluster-issuer: letsencrypt-prd to cjoc ingress's metadata.annotations add secretName: cjoc-tls-prd to cjoc ingress' spec.tls.host[0] confirm cjoc ingress's host and tls host is cloudbees-core.kearos.net Install \u00b6 eks apply -f cloudbees-core.yml eks rollout status sts cjoc Retrieve initial password \u00b6 eks exec cjoc-0 -- cat /var/jenkins_home/secrets/initialAdminPassword Jenkins CLI \u00b6 export CJOC_URL = https://cloudbees-core.kearos.net/cjoc/ http --download ${ CJOC_URL } /jnlpJars/jenkins-cli.jar --verify false export USR = jvandergriendt export TKN = 11b1016f80ddbb8a35bcbb5389599f7881 alias cbc = \"java -jar jenkins-cli.jar -noKeyAuth -auth ${ USR } : ${ TKN } -s ${ CJOC_URL } \" cbc teams Create team CAT \u00b6 cbc teams cat --put < team-cat.json Use EFS \u00b6 https://go.cloudbees.com/docs/cloudbees-core/cloud-reference-architecture/kubernetes-efs/ Create EFS in AWS performance: general purpose throughput: provisioned, 160mb/s encrypted: yes EFS_KEY_ARN = arn:aws:kms:eu-west-1:324005994172:key/4bfd8d70-c7de-4e7a-ab83-10792be5daaa Destroy cluster \u00b6 External Client - The Hard Way \u00b6 create a new master (hat) confirm remoting works on expected port 50000+n, where n is incremental count of number of masters for example, if hat is the first new \"team\", it will be 50001 create a new node external-agent launch via java webstart download client jar confirm port is NOT accessable open port on LB confirm port is open Open Port on LB \u00b6 export DOMAIN_NAME = cloudbees-core.example.com export TEAM_NAME = hat export MASTER_NAME = teams- ${ TEAM_NAME } export USR = export PSS = Test Port \u00b6 Get Remoting Port \u00b6 http --print = hH --auth $USR : $PSS https:// $DOMAIN_NAME / $MASTER_NAME / | grep X-Jenkins-CLI-Port Configure Config Map \u00b6 If you already configured tcp-services before, you will need to retrieve the current configmap using kubectl get configmap tcp-services -n ingress-nginx -o yaml > tcp-services.yaml and edit it accordingly kubectl get configmap tcp-services -n ingress-nginx -o yaml > tcp-services.yaml Else: export JNLP_MASTER_PORT = 50001 apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : ingress-nginx data : $JNLP_MASTER_PORT : \\\"$NAMESPACE/$MASTER_NAME:$JNLP_MASTER_PORT:PROXY\\\" For example: apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : ingress-nginx data : 50001 : \"default/teams-hat:50001:PROXY\" Create Patch for Deployment (ingress) \u00b6 spec : template : spec : containers : - name : nginx-ingress-controller ports : - containerPort : $JNLP_MASTER_PORT name : $JNLP_MASTER_PORT-tcp protocol : TCP Example: spec : template : spec : containers : - name : nginx-ingress-controller ports : - containerPort : 50001 name : 50001-tcp protocol : TCP Create Patch for Service (ingress) \u00b6 spec : ports : - name : $JNLP_MASTER_PORT-tcp port : $JNLP_MASTER_PORT protocol : TCP targetPort : $JNLP_MASTER_PORT-tcp Example: spec : ports : - name : 50001-tcp port : 50001 protocol : TCP targetPort : 50001-tcp Apply patches \u00b6 export NGINX_POD = $( kubectl get deployment -l app.kubernetes.io/name = ingress-nginx -n ingress-nginx -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl apply -f tcp-services.yaml kubectl patch deployment ${ NGINX_POD } -n ingress-nginx -p \" $( cat deployment-patch.yaml ) \" kubectl patch service ingress-nginx -n ingress-nginx -p \" $( cat service-patch.yaml ) \" kubectl annotate -n ingress-nginx service/ingress-nginx service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout = \"3600\" --overwrite Delete cluster \u00b6 aws cloudformation delete-stack --stack-name ${ STACK_NAME } --region ${ REGION } --profile ${ PROFILE }","title":"CloudBees Core on EKS"},{"location":"cloudbees/cbc-eks/#cloudbees-core-on-aws-eks","text":"The basics of installing CloudBees Core on EKS can found in the CloudBees Core Install Guide . For details around the architecture and possibilities for TLS termination (L4 or L7 with ELB), see the CloudBees Core EKS Manual Install guide . This guide is here to do the installation with TLS on the Ingress Controller and certificates managed by Let's Encrypt. It will also go beyond the installation and continue with configuration of the Operations Center and create some Masters. Note This guide is originally written during 2019, when CloudBees Core didn't have a helm chart yet. If you prefer a Helm install, please refer to CloudBees Core EKS Helm Install guide .","title":"CloudBees Core on AWS EKS"},{"location":"cloudbees/cbc-eks/#create-eks-cluster","text":"See my guide on creating a EKS cluster with EKSCTL , which is the recommended solution with regards to Kubernetes on AWS.","title":"Create EKS Cluster"},{"location":"cloudbees/cbc-eks/#certmanager","text":"echo \"apiVersion: certmanager.k8s.io/v1alpha1 kind: Certificate metadata: name: cloudbeescore-kearos-net namespace: cje spec: secretName: cjoc-tls-prd dnsNames: - cloudbees-core.kearos.net acme: config: - http01: ingressClass: nginx domains: - cloudbees-core.kearos.net issuerRef: name: letsencrypt-prd kind: ClusterIssuer\" > cjoc-cert.yml eks apply -f cjoc-cert.yml","title":"Certmanager"},{"location":"cloudbees/cbc-eks/#create-namespace-cje","text":"echo \"apiVersion: v1 kind: Namespace metadata: labels: name: cje name: cje\" > cje-namespace.yaml eks create -f cje-namespace.yaml eks config set-context $( eks config current-context ) --namespace = cje","title":"Create Namespace CJE"},{"location":"cloudbees/cbc-eks/#cb-core-install","text":"Download from downloads.cloudbees.com","title":"CB Core Install"},{"location":"cloudbees/cbc-eks/#configure-dns","text":"DOMAIN_NAME = cloudbees-core.kearos.net sed -e s,cje.example.com, $DOMAIN_NAME ,g < cloudbees-core.yml > tmp && mv tmp cloudbees-core.yml Configure k8s yaml file: add certmanager.k8s.io/cluster-issuer: letsencrypt-prd to cjoc ingress's metadata.annotations add secretName: cjoc-tls-prd to cjoc ingress' spec.tls.host[0] confirm cjoc ingress's host and tls host is cloudbees-core.kearos.net","title":"Configure DNS"},{"location":"cloudbees/cbc-eks/#install","text":"eks apply -f cloudbees-core.yml eks rollout status sts cjoc","title":"Install"},{"location":"cloudbees/cbc-eks/#retrieve-initial-password","text":"eks exec cjoc-0 -- cat /var/jenkins_home/secrets/initialAdminPassword","title":"Retrieve initial password"},{"location":"cloudbees/cbc-eks/#jenkins-cli","text":"export CJOC_URL = https://cloudbees-core.kearos.net/cjoc/ http --download ${ CJOC_URL } /jnlpJars/jenkins-cli.jar --verify false export USR = jvandergriendt export TKN = 11b1016f80ddbb8a35bcbb5389599f7881 alias cbc = \"java -jar jenkins-cli.jar -noKeyAuth -auth ${ USR } : ${ TKN } -s ${ CJOC_URL } \" cbc teams","title":"Jenkins CLI"},{"location":"cloudbees/cbc-eks/#create-team-cat","text":"cbc teams cat --put < team-cat.json","title":"Create team CAT"},{"location":"cloudbees/cbc-eks/#use-efs","text":"https://go.cloudbees.com/docs/cloudbees-core/cloud-reference-architecture/kubernetes-efs/ Create EFS in AWS performance: general purpose throughput: provisioned, 160mb/s encrypted: yes EFS_KEY_ARN = arn:aws:kms:eu-west-1:324005994172:key/4bfd8d70-c7de-4e7a-ab83-10792be5daaa","title":"Use EFS"},{"location":"cloudbees/cbc-eks/#destroy-cluster","text":"","title":"Destroy cluster"},{"location":"cloudbees/cbc-eks/#external-client-the-hard-way","text":"create a new master (hat) confirm remoting works on expected port 50000+n, where n is incremental count of number of masters for example, if hat is the first new \"team\", it will be 50001 create a new node external-agent launch via java webstart download client jar confirm port is NOT accessable open port on LB confirm port is open","title":"External Client - The Hard Way"},{"location":"cloudbees/cbc-eks/#open-port-on-lb","text":"export DOMAIN_NAME = cloudbees-core.example.com export TEAM_NAME = hat export MASTER_NAME = teams- ${ TEAM_NAME } export USR = export PSS =","title":"Open Port on LB"},{"location":"cloudbees/cbc-eks/#test-port","text":"","title":"Test Port"},{"location":"cloudbees/cbc-eks/#get-remoting-port","text":"http --print = hH --auth $USR : $PSS https:// $DOMAIN_NAME / $MASTER_NAME / | grep X-Jenkins-CLI-Port","title":"Get Remoting Port"},{"location":"cloudbees/cbc-eks/#configure-config-map","text":"If you already configured tcp-services before, you will need to retrieve the current configmap using kubectl get configmap tcp-services -n ingress-nginx -o yaml > tcp-services.yaml and edit it accordingly kubectl get configmap tcp-services -n ingress-nginx -o yaml > tcp-services.yaml Else: export JNLP_MASTER_PORT = 50001 apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : ingress-nginx data : $JNLP_MASTER_PORT : \\\"$NAMESPACE/$MASTER_NAME:$JNLP_MASTER_PORT:PROXY\\\" For example: apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : ingress-nginx data : 50001 : \"default/teams-hat:50001:PROXY\"","title":"Configure Config Map"},{"location":"cloudbees/cbc-eks/#create-patch-for-deployment-ingress","text":"spec : template : spec : containers : - name : nginx-ingress-controller ports : - containerPort : $JNLP_MASTER_PORT name : $JNLP_MASTER_PORT-tcp protocol : TCP Example: spec : template : spec : containers : - name : nginx-ingress-controller ports : - containerPort : 50001 name : 50001-tcp protocol : TCP","title":"Create Patch for Deployment (ingress)"},{"location":"cloudbees/cbc-eks/#create-patch-for-service-ingress","text":"spec : ports : - name : $JNLP_MASTER_PORT-tcp port : $JNLP_MASTER_PORT protocol : TCP targetPort : $JNLP_MASTER_PORT-tcp Example: spec : ports : - name : 50001-tcp port : 50001 protocol : TCP targetPort : 50001-tcp","title":"Create Patch for Service (ingress)"},{"location":"cloudbees/cbc-eks/#apply-patches","text":"export NGINX_POD = $( kubectl get deployment -l app.kubernetes.io/name = ingress-nginx -n ingress-nginx -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl apply -f tcp-services.yaml kubectl patch deployment ${ NGINX_POD } -n ingress-nginx -p \" $( cat deployment-patch.yaml ) \" kubectl patch service ingress-nginx -n ingress-nginx -p \" $( cat service-patch.yaml ) \" kubectl annotate -n ingress-nginx service/ingress-nginx service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout = \"3600\" --overwrite","title":"Apply patches"},{"location":"cloudbees/cbc-eks/#delete-cluster","text":"aws cloudformation delete-stack --stack-name ${ STACK_NAME } --region ${ REGION } --profile ${ PROFILE }","title":"Delete cluster"},{"location":"cloudbees/cbc-gke-helm/","text":"Install CloudBees Core On GKE \u00b6 Prerequisite \u00b6 Have a GKE cluster in which you're ClusterAdmin . Don't have one yet? Read here how to create one! Prepare \u00b6 kubectl create namespace cloudbees-core helm repo add cloudbees https://charts.cloudbees.com/public/cloudbees helm repo update kubens cloudbees-core Info kubense is a subcommand of the kubecontext tool. Install ClusterIssuer/Cert \u00b6 This assumes you have Cert-Manager installed. kubectl apply -f clusterissuer.yaml kubectl apply -f certificate.yaml clusterissuer.yaml \u00b6 Make sure to replace <REPLACE_WITH_YOUR_EMAIL_ADDRESS> with your own email address. Let's Encrypt will use this to register the certificate and will notify you there when it expires. apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-prod spec : acme : email : <REPLACE_WITH_YOUR_EMAIL_ADDRESS> server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-prod http01 : {} certificate.yaml \u00b6 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : <MyHostName> namespace : cloudbees-core spec : secretName : tls-cloudbees-core-kearos-net dnsNames : - cloudbees-core.kearos.net acme : config : - http01 : ingressClass : nginx domains : - cloudbees-core.kearos.net issuerRef : name : letsencrypt-prod kind : ClusterIssuer Install with values.yaml \u00b6 helm install --name cloudbees-core \\ -f cloudbees-core-values.yaml \\ --namespace = cloudbees-core \\ cloudbees/cloudbees-core kubectl rollout status statefulset cjoc kubectl get po cjoc-0 kubectl logs -f cjoc-0 stern cjoc Get Initial Password \u00b6 kubectl -n cloudbees-core exec cjoc-0 cat /var/jenkins_home/secrets/initialAdminPassword values.yaml \u00b6 # A helm example values file for standard kubernetes install. # An nginx-ingress controller is not installed and ssl isn't installed. # Install an nginx-ingress controller nginx-ingress : Enabled : false OperationsCenter : # Set the HostName for the Operation Center HostName : 'cloudbees-core.kearos.net' # Setting ServiceType to ClusterIP creates ingress ServiceType : ClusterIP CSRF : ProxyCompatibility : true Ingress : Annotations : certmanager.k8s.io/cluster-issuer : \"letsencrypt-prod\" kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : \"false\" nginx.ingress.kubernetes.io/app-root : https://$best_http_host/cjoc/teams-check/ nginx.ingress.kubernetes.io/proxy-body-size : 50m nginx.ingress.kubernetes.io/proxy-request-buffering : \"off\" nginx.ingress.kubernetes.io/ssl-redirect : \"true\" tls : ## Set this to true in order to enable TLS on the ingress record Enable : true # Create a certificate kubernetes and use it here. SecretName : tls-cloudbees-core-kearos-net Host : cloudbees-core.kearos.net Core Post Install \u00b6 Setup API Token \u00b6 Go to http://<MyHostName>/cjoc , login with your admin user. Click on the user's name (top right corner) -> Configure -> Generate Token . Warning You will see this token only once, so copy it and store it somewhere. Get CLI \u00b6 export CJOC_URL = https://<MyHostName>/cjoc/ http --download ${ CJOC_URL } /jnlpJars/jenkins-cli.jar --verify false Alias CLI \u00b6 USR = admin TKN = alias cboc = \"java -jar jenkins-cli.jar -noKeyAuth -auth ${ USR } : ${ TKN } -s ${ CJOC_URL } \" cboc version For More CLI \u00b6 Go to http://<MyHostName>/cjoc/cli","title":"CloudBees Core on GKE"},{"location":"cloudbees/cbc-gke-helm/#install-cloudbees-core-on-gke","text":"","title":"Install CloudBees Core On GKE"},{"location":"cloudbees/cbc-gke-helm/#prerequisite","text":"Have a GKE cluster in which you're ClusterAdmin . Don't have one yet? Read here how to create one!","title":"Prerequisite"},{"location":"cloudbees/cbc-gke-helm/#prepare","text":"kubectl create namespace cloudbees-core helm repo add cloudbees https://charts.cloudbees.com/public/cloudbees helm repo update kubens cloudbees-core Info kubense is a subcommand of the kubecontext tool.","title":"Prepare"},{"location":"cloudbees/cbc-gke-helm/#install-clusterissuercert","text":"This assumes you have Cert-Manager installed. kubectl apply -f clusterissuer.yaml kubectl apply -f certificate.yaml","title":"Install ClusterIssuer/Cert"},{"location":"cloudbees/cbc-gke-helm/#clusterissueryaml","text":"Make sure to replace <REPLACE_WITH_YOUR_EMAIL_ADDRESS> with your own email address. Let's Encrypt will use this to register the certificate and will notify you there when it expires. apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-prod spec : acme : email : <REPLACE_WITH_YOUR_EMAIL_ADDRESS> server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-prod http01 : {}","title":"clusterissuer.yaml"},{"location":"cloudbees/cbc-gke-helm/#certificateyaml","text":"apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : <MyHostName> namespace : cloudbees-core spec : secretName : tls-cloudbees-core-kearos-net dnsNames : - cloudbees-core.kearos.net acme : config : - http01 : ingressClass : nginx domains : - cloudbees-core.kearos.net issuerRef : name : letsencrypt-prod kind : ClusterIssuer","title":"certificate.yaml"},{"location":"cloudbees/cbc-gke-helm/#install-with-valuesyaml","text":"helm install --name cloudbees-core \\ -f cloudbees-core-values.yaml \\ --namespace = cloudbees-core \\ cloudbees/cloudbees-core kubectl rollout status statefulset cjoc kubectl get po cjoc-0 kubectl logs -f cjoc-0 stern cjoc","title":"Install with values.yaml"},{"location":"cloudbees/cbc-gke-helm/#get-initial-password","text":"kubectl -n cloudbees-core exec cjoc-0 cat /var/jenkins_home/secrets/initialAdminPassword","title":"Get Initial Password"},{"location":"cloudbees/cbc-gke-helm/#valuesyaml","text":"# A helm example values file for standard kubernetes install. # An nginx-ingress controller is not installed and ssl isn't installed. # Install an nginx-ingress controller nginx-ingress : Enabled : false OperationsCenter : # Set the HostName for the Operation Center HostName : 'cloudbees-core.kearos.net' # Setting ServiceType to ClusterIP creates ingress ServiceType : ClusterIP CSRF : ProxyCompatibility : true Ingress : Annotations : certmanager.k8s.io/cluster-issuer : \"letsencrypt-prod\" kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : \"false\" nginx.ingress.kubernetes.io/app-root : https://$best_http_host/cjoc/teams-check/ nginx.ingress.kubernetes.io/proxy-body-size : 50m nginx.ingress.kubernetes.io/proxy-request-buffering : \"off\" nginx.ingress.kubernetes.io/ssl-redirect : \"true\" tls : ## Set this to true in order to enable TLS on the ingress record Enable : true # Create a certificate kubernetes and use it here. SecretName : tls-cloudbees-core-kearos-net Host : cloudbees-core.kearos.net","title":"values.yaml"},{"location":"cloudbees/cbc-gke-helm/#core-post-install","text":"","title":"Core Post Install"},{"location":"cloudbees/cbc-gke-helm/#setup-api-token","text":"Go to http://<MyHostName>/cjoc , login with your admin user. Click on the user's name (top right corner) -> Configure -> Generate Token . Warning You will see this token only once, so copy it and store it somewhere.","title":"Setup API Token"},{"location":"cloudbees/cbc-gke-helm/#get-cli","text":"export CJOC_URL = https://<MyHostName>/cjoc/ http --download ${ CJOC_URL } /jnlpJars/jenkins-cli.jar --verify false","title":"Get CLI"},{"location":"cloudbees/cbc-gke-helm/#alias-cli","text":"USR = admin TKN = alias cboc = \"java -jar jenkins-cli.jar -noKeyAuth -auth ${ USR } : ${ TKN } -s ${ CJOC_URL } \" cboc version","title":"Alias CLI"},{"location":"cloudbees/cbc-gke-helm/#for-more-cli","text":"Go to http://<MyHostName>/cjoc/cli","title":"For More CLI"},{"location":"cloudbees/cbc-post-install-tips/","text":"CloudBees Core Post Install Tips \u00b6","title":"CloudBees Core Post Install Tips"},{"location":"cloudbees/cbc-post-install-tips/#cloudbees-core-post-install-tips","text":"","title":"CloudBees Core Post Install Tips"},{"location":"cloudbees/cbc-team-namespace/","text":"Team Master In Alternative Namespace \u00b6 Goal \u00b6 The goal of this document is to show how to create a Team Master with CloudBees Core Modern in a different [Kubernetes] Namespace than where the Operations Center resides. Audience \u00b6 For anyone working with CloudBees Core Modern as an Administrator or Cluster Administrator. Pre-requisites \u00b6 working installation of CloudBees Core Modern kubectl access to the cluster with sufficient rights to set permissions Helm installed CloudBees Helm Chart configured Kubectx installed Prepare Helm \u00b6 helm repo add cloudbees https://charts.cloudbees.com/public/cloudbees helm repo update Process \u00b6 create & configure the new Namespace configure additional Kubernetes endpoint in CJOC create Team Master via Team UI using new Kubernetes Endpoint Important Unlike a Managed Master, we cannot choose the Namespace in which we want to create the Team Master. Not in the Teams UI nor in the Jenkins CLI. To achieve our goal, we leverage the Kubernetes Endpoint configuration with Master Provisioning . Configure New Namespace \u00b6 If we want the Operations Center to create our Team Master in a different Namespace, we have to create this namespace first. That isn't enough; we also have to configure this Namespace so that Operations Center has the permissions to create resources. Additionally, we need to make sure that the new Team Master can create build agents (via PodTemplates) in the new Namespace as well. Create Namespace \u00b6 First, we create the Namespace via kubectl . NAMESPACE = kubectl create namespace $NAMESPACE This sets the new Namespace as default. kubens $NAMESPACE Note kubens is part of the kubectx tool. Create Helm Values File \u00b6 To configure the Namespace with everything the Operations Center and the to-be-created Team Master need, we can leverage the CloudBees Core Helm Chart. The Helm chart has a built-in feature to generate the Namespace configuration for a secondary namespace. To do so, we need to set two values at least: Master.OperationsCenterNamespace = ${NAMESPACE} OperationsCenter.Enabled = false In yaml form ( namespace-values.yaml ): Master : OperationsCenterNamespace : cloudbees-core OperationsCenter : Enabled : false Note Make sure that OperationsCenterNamespace is the Namespace your Operations Center is configured in. By default, it should be cloudbees-core . Fetch Helm Chart \u00b6 We then have to retrieve the Helm Chart itself, so Helm can use it for templating. helm fetch \\ --repo https://charts.cloudbees.com/public/cloudbees \\ --version 3 .8.0+a0d07461ae1c \\ cloudbees-core Note Make sure to change the version to reflect the version you've downloaded. Create Namespace Configuration \u00b6 We have the values and the Chart. We can now let Helm create the configuration via helm template . helm template cloudbees-core-namespace \\ cloudbees-core-3.8.0+a0d07461ae1c.tgz \\ -f namespace-values.yaml \\ --namespace ${ NAMESPACE } \\ > cloudbees-core-namespace.yml !!! note Make sure to change the Chart filename to reflect the version you've downloaded. Apply Namespace Configuration \u00b6 Now that we have the complete configuration file of the Namespace, we can apply it via kubectl apply -f . kubectl apply -f cloudbees-core-namespace.yml --namespace ${ NAMESPACE } Configure Kubernetes Endpoint In Operations Center \u00b6 Now that we have the Namespace configured, we can create a new Kubernetes Endpoint definition in Operations Center. Go to Operations Center -> Manage Jenkins -> Configure System -> Kubernetes Master Provisioning and click Add . Here we configure the endpoint. We change the namespace only, and stay within the same cluster, so we leave the following fields blank: API endpoint URL Credentials Server Certificate You have to fill in the fields Display Name and Namespace , I'd recommend using the same value for both, the namespace we just created and configured. We also have to fill in the field Jenkins URL , we can take the base name from the default endpoint (should be http://cjoc.cloudbees-core.svc.cluster.local/cjoc ). We then have to add the namespace of where Operations Center is in, to the URL. Which in my case is, is in cloudbees-core . The end result being: http://cjoc.cloudbees-core.svc.cluster.local/cjoc . Info Make sure to hit the Validate button to ensure the configuration works. Create Team Master In Alternative Namespace \u00b6 First, open the Teams UI. Second, start the New Team Wizard. And finally, select your new endpoint.","title":"Team Master Namespace"},{"location":"cloudbees/cbc-team-namespace/#team-master-in-alternative-namespace","text":"","title":"Team Master In Alternative Namespace"},{"location":"cloudbees/cbc-team-namespace/#goal","text":"The goal of this document is to show how to create a Team Master with CloudBees Core Modern in a different [Kubernetes] Namespace than where the Operations Center resides.","title":"Goal"},{"location":"cloudbees/cbc-team-namespace/#audience","text":"For anyone working with CloudBees Core Modern as an Administrator or Cluster Administrator.","title":"Audience"},{"location":"cloudbees/cbc-team-namespace/#pre-requisites","text":"working installation of CloudBees Core Modern kubectl access to the cluster with sufficient rights to set permissions Helm installed CloudBees Helm Chart configured Kubectx installed","title":"Pre-requisites"},{"location":"cloudbees/cbc-team-namespace/#prepare-helm","text":"helm repo add cloudbees https://charts.cloudbees.com/public/cloudbees helm repo update","title":"Prepare Helm"},{"location":"cloudbees/cbc-team-namespace/#process","text":"create & configure the new Namespace configure additional Kubernetes endpoint in CJOC create Team Master via Team UI using new Kubernetes Endpoint Important Unlike a Managed Master, we cannot choose the Namespace in which we want to create the Team Master. Not in the Teams UI nor in the Jenkins CLI. To achieve our goal, we leverage the Kubernetes Endpoint configuration with Master Provisioning .","title":"Process"},{"location":"cloudbees/cbc-team-namespace/#configure-new-namespace","text":"If we want the Operations Center to create our Team Master in a different Namespace, we have to create this namespace first. That isn't enough; we also have to configure this Namespace so that Operations Center has the permissions to create resources. Additionally, we need to make sure that the new Team Master can create build agents (via PodTemplates) in the new Namespace as well.","title":"Configure New Namespace"},{"location":"cloudbees/cbc-team-namespace/#create-namespace","text":"First, we create the Namespace via kubectl . NAMESPACE = kubectl create namespace $NAMESPACE This sets the new Namespace as default. kubens $NAMESPACE Note kubens is part of the kubectx tool.","title":"Create Namespace"},{"location":"cloudbees/cbc-team-namespace/#create-helm-values-file","text":"To configure the Namespace with everything the Operations Center and the to-be-created Team Master need, we can leverage the CloudBees Core Helm Chart. The Helm chart has a built-in feature to generate the Namespace configuration for a secondary namespace. To do so, we need to set two values at least: Master.OperationsCenterNamespace = ${NAMESPACE} OperationsCenter.Enabled = false In yaml form ( namespace-values.yaml ): Master : OperationsCenterNamespace : cloudbees-core OperationsCenter : Enabled : false Note Make sure that OperationsCenterNamespace is the Namespace your Operations Center is configured in. By default, it should be cloudbees-core .","title":"Create Helm Values File"},{"location":"cloudbees/cbc-team-namespace/#fetch-helm-chart","text":"We then have to retrieve the Helm Chart itself, so Helm can use it for templating. helm fetch \\ --repo https://charts.cloudbees.com/public/cloudbees \\ --version 3 .8.0+a0d07461ae1c \\ cloudbees-core Note Make sure to change the version to reflect the version you've downloaded.","title":"Fetch Helm Chart"},{"location":"cloudbees/cbc-team-namespace/#create-namespace-configuration","text":"We have the values and the Chart. We can now let Helm create the configuration via helm template . helm template cloudbees-core-namespace \\ cloudbees-core-3.8.0+a0d07461ae1c.tgz \\ -f namespace-values.yaml \\ --namespace ${ NAMESPACE } \\ > cloudbees-core-namespace.yml !!! note Make sure to change the Chart filename to reflect the version you've downloaded.","title":"Create Namespace Configuration"},{"location":"cloudbees/cbc-team-namespace/#apply-namespace-configuration","text":"Now that we have the complete configuration file of the Namespace, we can apply it via kubectl apply -f . kubectl apply -f cloudbees-core-namespace.yml --namespace ${ NAMESPACE }","title":"Apply Namespace Configuration"},{"location":"cloudbees/cbc-team-namespace/#configure-kubernetes-endpoint-in-operations-center","text":"Now that we have the Namespace configured, we can create a new Kubernetes Endpoint definition in Operations Center. Go to Operations Center -> Manage Jenkins -> Configure System -> Kubernetes Master Provisioning and click Add . Here we configure the endpoint. We change the namespace only, and stay within the same cluster, so we leave the following fields blank: API endpoint URL Credentials Server Certificate You have to fill in the fields Display Name and Namespace , I'd recommend using the same value for both, the namespace we just created and configured. We also have to fill in the field Jenkins URL , we can take the base name from the default endpoint (should be http://cjoc.cloudbees-core.svc.cluster.local/cjoc ). We then have to add the namespace of where Operations Center is in, to the URL. Which in my case is, is in cloudbees-core . The end result being: http://cjoc.cloudbees-core.svc.cluster.local/cjoc . Info Make sure to hit the Validate button to ensure the configuration works.","title":"Configure Kubernetes Endpoint In Operations Center"},{"location":"cloudbees/cbc-team-namespace/#create-team-master-in-alternative-namespace","text":"First, open the Teams UI. Second, start the New Team Wizard. And finally, select your new endpoint.","title":"Create Team Master In Alternative Namespace"},{"location":"cloudbees/multi-cluster-temp/","text":"CloudBees Core On Multiple Clusters \u00b6 https://docs.cloudbees.com/docs/cloudbees-core/latest/cloud-admin-guide/multiple-clusters https://docs.microsoft.com/en-us/azure/aks/ingress-static-ip Pre-Requisites \u00b6 GKE Cluster: Primary AKS Cluster: Secondary Configure Primary Cluster \u00b6 create cluster retrieve account token retrieve Kubernetes API Endpoint retrieve Kubernetes Root CA hint -> echo \" \" | base64 -D install CloudBees Core via Helm (or Jenkins X) Configure Secondary Cluster \u00b6 https://docs.microsoft.com/en-us/azure/aks/ingress-tls Install Ingress Controller \u00b6 Create a namespace for your ingress resources Add the official stable repository helm repo add stable https://kubernetes-charts.storage.googleapis.com/ Use Helm to deploy an NGINX ingress controller Create a values file, ingress-values.yaml . The reason is stated as this: Since version 0.22.1 of stable/nginx-ingress chart, ClusterRole and ClusterRoleBinding are not created automatically when the controller scope is enabled. They are required for this functionality to work. To use the controller scope feature, see the article Helm install of stable/nginx-ingress fails to deploy the Ingress Controller . rbac : create : true defaultBackend : enabled : false controller : ingressClass : \"nginx\" metrics : enabled : \"true\" replicaCount : 2 nodeSelector : beta\\.kubernetes.io/os : linux scope : enabled : \"true\" namespace : cbmasters service : externalTrafficPolicy : \"Cluster\" kubectl create namespace ingress-nginx Helm V3 helm install nginx-ingress stable/nginx-ingress \\ --namespace ingress-nginx \\ --values ingress-values.yaml \\ --version 1 .29.6 Helm V2 helm install \\ --name nginx-ingress stable/nginx-ingress \\ --namespace ingress-nginx \\ --values ingress-values.yaml \\ --version 1 .29.6 Certmanager \u00b6 Install the CustomResourceDefinition resources separately kubectl apply --validate = false \\ -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.13/deploy/manifests/00-crds.yaml Label the ingress-basic namespace to disable resource validation kubectl label namespace ingress-basic certmanager.k8s.io/disable-validation = true Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io Update your local Helm chart repository cache helm repo update Create a certmanager-values.yaml file. ingressShim : defaultIssuerName : letsencrypt defaultIssuerKind : ClusterIssuer Install the cert-manager Helm chart. Helm V3 helm install cert-manager \\ --namespace cert-manager \\ --version v0.13.0 \\ --values certmanager-values.yaml \\ jetstack/cert-manager Helm V2 helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.13.0 \\ --values certmanager-values.yaml \\ jetstack/cert-manager Configure Certificate Issuer \u00b6 apiVersion : cert-manager.io/v1alpha2 kind : ClusterIssuer metadata : name : letsencrypt spec : acme : server : https://acme-v02.api.letsencrypt.org/directory email : MY_EMAIL_ADDRESS privateKeySecretRef : name : letsencrypt solvers : - http01 : ingress : class : nginx kubectl apply -f cluster-issuer.yaml --namespace ingress-basic Configure Certificate \u00b6 Todo Prepare Receiving Namespace \u00b6 NAMESPACE = kubectl create namespace $NAMESPACE Create a configuration for CloudBees Core Helm chart, called cloudbees-values.yaml . OperationsCenter : Enabled : false Master : Enabled : true OperationsCenterNamespace : jx-staging Agents : Enabled : true ``` ``` bash helm fetch \\ --repo https://charts.cloudbees.com/public/cloudbees \\ --version 3.8.0+a0d07461ae1c \\ cloudbees-core helm template cloudbees-core-namespace \\ cloudbees-core-3.8.0+a0d07461ae1c.tgz \\ -f cloudbees-values.yaml \\ --namespace ${ NAMESPACE } \\ > cloudbees-core-namespace.yml kubectl apply -f cloudbees-core-namespace.yml --namespace ${ NAMESPACE } Configure CloudBees Core \u00b6 Client Secret -> secret text -> the token from Terraforms output Configre Siodecar Injector \u00b6 helm fetch \\ --repo https://charts.cloudbees.com/public/cloudbees \\ --version 2 .0.1 \\ cloudbees-sidecar-injector Untar the config, and update the configmap -> templates/configmap.yaml . Set requiresExplicitInjection to true. requiresExplicitInjection: true Generate the end result: helm template cloudbees-sidecar-injector \\ cloudbees-sidecar-injector \\ --namespace cloudbees-sidecar-injector \\ > cloudbees-sidecar-injector.yml And apply the file. kubectl apply -f cloudbees-sidecar-injector.yml Debugging SSL Issue \u00b6 Add System properties: javax.net.ssl.trustStore = /etc/ssl/certs/java/cacerts javax.net.ssl.trustStorePassword = changeit javax.net.debug = SSL,trustmanager Add annotation com.cloudbees.sidecar-injector/inject: yes , and change this in the master configuration. apiVersion : \"apps/v1\" kind : \"StatefulSet\" spec : template : metadata : annotations : prometheus.io/path : /${name}/prometheus prometheus.io/port : \"8080\" prometheus.io/scrape : \"true\" com.cloudbees.sidecar-injector/inject : yes labels : app.kubernetes.io/component : Managed-Master app.kubernetes.io/instance : ${name} app.kubernetes.io/managed-by : CloudBees-Core-Cloud-Operations-Center app.kubernetes.io/name : ${name} trustStore is: /etc/ssl/certs/java/cacerts trustStore type is: jks trustStore provider is: ----- BEGIN CONNECTION DETAILS ----- H4sIAAAAAAAAAA3KQQ7CIBBA0bvMWqBMKLS9zTAi1lowMF0Z7y6bn7zkfyE3KgIbLC56Gx6sJhtn 5XBFFe0IYiA3zeviA8MN9vt4UZ0n+aGrvQefIp++GcO1Jd2F8l6yzkfSR6JWuy5JDL8qG/j9ATek aGdwAAAA ----- END CONNECTION DETAILS ----- Problem is, Jenkins uses an outdated SSL library (openSSL) that doesn't support SNI (servername). This cause Nginx Ingress Controller to return an invalid certificate: Issuer: CN=Kubernetes Ingress Controller Fake Certificate, O=Acme Co . As this is not a CA cert, it cannot be directly trusted. MiniCA Solution \u00b6 The gist: * create custom CA with minica * generate wildcard certificate for primary domain * set wildcard cert as default ssl certificate * - --default-ssl-certificate=default/cloudbees-core.kearos.net-tls * add custom CA to cacerts truststore and ca-certificates.cert * update ca-bundles configmap solution with sidecar injector minica --domains kearos.net cat minica.pem >> ca-certificates.crt keytool -import -noprompt -keystore cacerts -file minica.pem -storepass changeit -alias kearos-net ; kubectl create configmap --from-file = ca-certificates.crt,cacerts ca-bundles kubectl create secret tls tls-fake-kearos-net --key ./kearos.net/key.pem --cert ./kearos.net/cert.pem --namespace default kubectl edit deployment nginx-ingress-controller spec : containers : - args : - /nginx-ingress-controller - --configmap=$(POD_NAMESPACE)/nginx-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/udp-services - --publish-service=$(POD_NAMESPACE)/ingress-nginx - --annotations-prefix=nginx.ingress.kubernetes.io - --default-ssl-certificate=default/tls-fake-kearos-net kubectl create configmap --from-file = ca-certificates.crt,cacerts ca-bundles Change cjoc-0 ingress: Add - backend : serviceName : cjoc servicePort : 80 path : / Remove: metadata : annotations : nginx.ingress.kubernetes.io/app-root : https://$best_http_host/cjoc/teams-check/ Fix Port 50000 issue \u00b6 # nginx-config-map.yaml apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : kube-system data : 50000 : \"jx-staging/cjoc:50000\" kubectl apply -f nginx-config-map.yaml - containerPort : 50000 name : jnlp protocol : TCP And in its args, add --tcp-services-configmap and point to the tcp-services configmap you created. args: ... - --tcp-services-configmap = kube-system/tcp-services kubectl edit -n kube-system deployment jxing-nginx-ingress-default-backend https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/ - name : jnlp port : 50000 protocol : TCP targetPort : jnlp kubectl edit -n kube-system svc jxing-nginx-ingress-controller Ticket for Core V2 \u00b6 In the 2.204 Operations Center image, the location of the truststore ( cacerts ) and the ca certificate bundle ( ca-certificates.crt ) has changed. In general there isn't a direct issue, but we mention it in a lot of places. This should be checked, propably tested and communicated to docs. KBs \u00b6 https://support.cloudbees.com/hc/en-us/articles/360018267271 https://support.cloudbees.com/hc/en-us/articles/360018094412-Deploy-Self-Signed-Certificates-in-Masters-and-Agents-Custom-Location- Core Docs \u00b6 https://docs.cloudbees.com/docs/cloudbees-core/latest/cloud-admin-guide/kubernetes-self-signed-certificates Sidecar Injector \u00b6 https://github.com/cloudbees/sidecar-injector/blob/master/charts/cloudbees-sidecar-injector/README.md#create-a-certificate-bundle https://github.com/cloudbees/sidecar-injector/blob/c965497a51bc68f6dc6df8e9aef2403819f7902f/charts/cloudbees-sidecar-injector/values.yaml","title":"CloudBees Core Modern Multi-Cluster"},{"location":"cloudbees/multi-cluster-temp/#cloudbees-core-on-multiple-clusters","text":"https://docs.cloudbees.com/docs/cloudbees-core/latest/cloud-admin-guide/multiple-clusters https://docs.microsoft.com/en-us/azure/aks/ingress-static-ip","title":"CloudBees Core On Multiple Clusters"},{"location":"cloudbees/multi-cluster-temp/#pre-requisites","text":"GKE Cluster: Primary AKS Cluster: Secondary","title":"Pre-Requisites"},{"location":"cloudbees/multi-cluster-temp/#configure-primary-cluster","text":"create cluster retrieve account token retrieve Kubernetes API Endpoint retrieve Kubernetes Root CA hint -> echo \" \" | base64 -D install CloudBees Core via Helm (or Jenkins X)","title":"Configure Primary Cluster"},{"location":"cloudbees/multi-cluster-temp/#configure-secondary-cluster","text":"https://docs.microsoft.com/en-us/azure/aks/ingress-tls","title":"Configure Secondary Cluster"},{"location":"cloudbees/multi-cluster-temp/#install-ingress-controller","text":"Create a namespace for your ingress resources Add the official stable repository helm repo add stable https://kubernetes-charts.storage.googleapis.com/ Use Helm to deploy an NGINX ingress controller Create a values file, ingress-values.yaml . The reason is stated as this: Since version 0.22.1 of stable/nginx-ingress chart, ClusterRole and ClusterRoleBinding are not created automatically when the controller scope is enabled. They are required for this functionality to work. To use the controller scope feature, see the article Helm install of stable/nginx-ingress fails to deploy the Ingress Controller . rbac : create : true defaultBackend : enabled : false controller : ingressClass : \"nginx\" metrics : enabled : \"true\" replicaCount : 2 nodeSelector : beta\\.kubernetes.io/os : linux scope : enabled : \"true\" namespace : cbmasters service : externalTrafficPolicy : \"Cluster\" kubectl create namespace ingress-nginx Helm V3 helm install nginx-ingress stable/nginx-ingress \\ --namespace ingress-nginx \\ --values ingress-values.yaml \\ --version 1 .29.6 Helm V2 helm install \\ --name nginx-ingress stable/nginx-ingress \\ --namespace ingress-nginx \\ --values ingress-values.yaml \\ --version 1 .29.6","title":"Install Ingress Controller"},{"location":"cloudbees/multi-cluster-temp/#certmanager","text":"Install the CustomResourceDefinition resources separately kubectl apply --validate = false \\ -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.13/deploy/manifests/00-crds.yaml Label the ingress-basic namespace to disable resource validation kubectl label namespace ingress-basic certmanager.k8s.io/disable-validation = true Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io Update your local Helm chart repository cache helm repo update Create a certmanager-values.yaml file. ingressShim : defaultIssuerName : letsencrypt defaultIssuerKind : ClusterIssuer Install the cert-manager Helm chart. Helm V3 helm install cert-manager \\ --namespace cert-manager \\ --version v0.13.0 \\ --values certmanager-values.yaml \\ jetstack/cert-manager Helm V2 helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.13.0 \\ --values certmanager-values.yaml \\ jetstack/cert-manager","title":"Certmanager"},{"location":"cloudbees/multi-cluster-temp/#configure-certificate-issuer","text":"apiVersion : cert-manager.io/v1alpha2 kind : ClusterIssuer metadata : name : letsencrypt spec : acme : server : https://acme-v02.api.letsencrypt.org/directory email : MY_EMAIL_ADDRESS privateKeySecretRef : name : letsencrypt solvers : - http01 : ingress : class : nginx kubectl apply -f cluster-issuer.yaml --namespace ingress-basic","title":"Configure Certificate Issuer"},{"location":"cloudbees/multi-cluster-temp/#configure-certificate","text":"Todo","title":"Configure Certificate"},{"location":"cloudbees/multi-cluster-temp/#prepare-receiving-namespace","text":"NAMESPACE = kubectl create namespace $NAMESPACE Create a configuration for CloudBees Core Helm chart, called cloudbees-values.yaml . OperationsCenter : Enabled : false Master : Enabled : true OperationsCenterNamespace : jx-staging Agents : Enabled : true ``` ``` bash helm fetch \\ --repo https://charts.cloudbees.com/public/cloudbees \\ --version 3.8.0+a0d07461ae1c \\ cloudbees-core helm template cloudbees-core-namespace \\ cloudbees-core-3.8.0+a0d07461ae1c.tgz \\ -f cloudbees-values.yaml \\ --namespace ${ NAMESPACE } \\ > cloudbees-core-namespace.yml kubectl apply -f cloudbees-core-namespace.yml --namespace ${ NAMESPACE }","title":"Prepare Receiving Namespace"},{"location":"cloudbees/multi-cluster-temp/#configure-cloudbees-core","text":"Client Secret -> secret text -> the token from Terraforms output","title":"Configure CloudBees Core"},{"location":"cloudbees/multi-cluster-temp/#configre-siodecar-injector","text":"helm fetch \\ --repo https://charts.cloudbees.com/public/cloudbees \\ --version 2 .0.1 \\ cloudbees-sidecar-injector Untar the config, and update the configmap -> templates/configmap.yaml . Set requiresExplicitInjection to true. requiresExplicitInjection: true Generate the end result: helm template cloudbees-sidecar-injector \\ cloudbees-sidecar-injector \\ --namespace cloudbees-sidecar-injector \\ > cloudbees-sidecar-injector.yml And apply the file. kubectl apply -f cloudbees-sidecar-injector.yml","title":"Configre Siodecar Injector"},{"location":"cloudbees/multi-cluster-temp/#debugging-ssl-issue","text":"Add System properties: javax.net.ssl.trustStore = /etc/ssl/certs/java/cacerts javax.net.ssl.trustStorePassword = changeit javax.net.debug = SSL,trustmanager Add annotation com.cloudbees.sidecar-injector/inject: yes , and change this in the master configuration. apiVersion : \"apps/v1\" kind : \"StatefulSet\" spec : template : metadata : annotations : prometheus.io/path : /${name}/prometheus prometheus.io/port : \"8080\" prometheus.io/scrape : \"true\" com.cloudbees.sidecar-injector/inject : yes labels : app.kubernetes.io/component : Managed-Master app.kubernetes.io/instance : ${name} app.kubernetes.io/managed-by : CloudBees-Core-Cloud-Operations-Center app.kubernetes.io/name : ${name} trustStore is: /etc/ssl/certs/java/cacerts trustStore type is: jks trustStore provider is: ----- BEGIN CONNECTION DETAILS ----- H4sIAAAAAAAAAA3KQQ7CIBBA0bvMWqBMKLS9zTAi1lowMF0Z7y6bn7zkfyE3KgIbLC56Gx6sJhtn 5XBFFe0IYiA3zeviA8MN9vt4UZ0n+aGrvQefIp++GcO1Jd2F8l6yzkfSR6JWuy5JDL8qG/j9ATek aGdwAAAA ----- END CONNECTION DETAILS ----- Problem is, Jenkins uses an outdated SSL library (openSSL) that doesn't support SNI (servername). This cause Nginx Ingress Controller to return an invalid certificate: Issuer: CN=Kubernetes Ingress Controller Fake Certificate, O=Acme Co . As this is not a CA cert, it cannot be directly trusted.","title":"Debugging SSL Issue"},{"location":"cloudbees/multi-cluster-temp/#minica-solution","text":"The gist: * create custom CA with minica * generate wildcard certificate for primary domain * set wildcard cert as default ssl certificate * - --default-ssl-certificate=default/cloudbees-core.kearos.net-tls * add custom CA to cacerts truststore and ca-certificates.cert * update ca-bundles configmap solution with sidecar injector minica --domains kearos.net cat minica.pem >> ca-certificates.crt keytool -import -noprompt -keystore cacerts -file minica.pem -storepass changeit -alias kearos-net ; kubectl create configmap --from-file = ca-certificates.crt,cacerts ca-bundles kubectl create secret tls tls-fake-kearos-net --key ./kearos.net/key.pem --cert ./kearos.net/cert.pem --namespace default kubectl edit deployment nginx-ingress-controller spec : containers : - args : - /nginx-ingress-controller - --configmap=$(POD_NAMESPACE)/nginx-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/udp-services - --publish-service=$(POD_NAMESPACE)/ingress-nginx - --annotations-prefix=nginx.ingress.kubernetes.io - --default-ssl-certificate=default/tls-fake-kearos-net kubectl create configmap --from-file = ca-certificates.crt,cacerts ca-bundles Change cjoc-0 ingress: Add - backend : serviceName : cjoc servicePort : 80 path : / Remove: metadata : annotations : nginx.ingress.kubernetes.io/app-root : https://$best_http_host/cjoc/teams-check/","title":"MiniCA Solution"},{"location":"cloudbees/multi-cluster-temp/#fix-port-50000-issue","text":"# nginx-config-map.yaml apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : kube-system data : 50000 : \"jx-staging/cjoc:50000\" kubectl apply -f nginx-config-map.yaml - containerPort : 50000 name : jnlp protocol : TCP And in its args, add --tcp-services-configmap and point to the tcp-services configmap you created. args: ... - --tcp-services-configmap = kube-system/tcp-services kubectl edit -n kube-system deployment jxing-nginx-ingress-default-backend https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/ - name : jnlp port : 50000 protocol : TCP targetPort : jnlp kubectl edit -n kube-system svc jxing-nginx-ingress-controller","title":"Fix Port 50000 issue"},{"location":"cloudbees/multi-cluster-temp/#ticket-for-core-v2","text":"In the 2.204 Operations Center image, the location of the truststore ( cacerts ) and the ca certificate bundle ( ca-certificates.crt ) has changed. In general there isn't a direct issue, but we mention it in a lot of places. This should be checked, propably tested and communicated to docs.","title":"Ticket for Core V2"},{"location":"cloudbees/multi-cluster-temp/#kbs","text":"https://support.cloudbees.com/hc/en-us/articles/360018267271 https://support.cloudbees.com/hc/en-us/articles/360018094412-Deploy-Self-Signed-Certificates-in-Masters-and-Agents-Custom-Location-","title":"KBs"},{"location":"cloudbees/multi-cluster-temp/#core-docs","text":"https://docs.cloudbees.com/docs/cloudbees-core/latest/cloud-admin-guide/kubernetes-self-signed-certificates","title":"Core Docs"},{"location":"cloudbees/multi-cluster-temp/#sidecar-injector","text":"https://github.com/cloudbees/sidecar-injector/blob/master/charts/cloudbees-sidecar-injector/README.md#create-a-certificate-bundle https://github.com/cloudbees/sidecar-injector/blob/c965497a51bc68f6dc6df8e9aef2403819f7902f/charts/cloudbees-sidecar-injector/values.yaml","title":"Sidecar Injector"},{"location":"cloudbees/multi-cluster/","text":"Imagine you have a whole range of departments and development teams. Preferably you want to serve them with a standardized SDA(Software Delivery Automation) platform, but at the same time, make sure they pay for their usage. I don't think this is too far fetched or something terrible. I think it makes sense. In this light, CloudBees Core now supports running on multiple Kubernetes clusters . In this guide, we're going to explore how to run CloudBees Core on a GKE and AKS cluster at the same time. And how to deal with the details of getting the namespace configured, certificates, Kubernetes API tokens and so on and so forth. Prerequisites \u00b6 First off, we need a [GKE cluster] (/kubernetes/distributions/gke-terraform/) and a AKS cluster . Next, we make the GKE cluster the default one. Here we will install CloudBees Core. You can either install CloudBees Core directly or via Jenkins X . You're also welcome to follow the CloudBees documentation . The expected state is that you access Operations Center via a proper DNS name with TLS enabled, served via Nginx Ingress. And the AKS cluster is ready to be used - but empty. Some further assumptions: you have admin rights on both clusters the TLS certificate is from Let's Encrypt via Certmanager the AKS cluster is empty You can break these assumptions, but that means you have to change the examples to reflect your situation. Steps to do \u00b6 High over, we have to do the following steps: tweak the GKE cluster configure the AKS cluster configure Operations Center create a Managed Master Tweak GKE Cluster \u00b6 At this time of writing, Jenkins is suffering from an outdated OpenSSL library . This version doesn't support SNI , which is what the Nginx Ingress Controller relies on. When a Client Master (Managed or not) connects to the Operations Center via the Nginx Controller without SNI, the controller will not route you to the correct path. Because of this, when TLS is enabled, you will get the default controller's TLS certificate. As this certificate, by default, is an invalid one, you get TLS handshake errors. We have to rectify this. There are many ways to do this. For example, you make the TLS certificate for Operations Center the default TLS certificate. While this might not be nice for other applications, it will suit our case. Info If there are many other applications in the same cluster, consider having more than one Nginx Ingress Controller. Via the configuration parameters controller.scope.enabled and controller.scope.namespace , you can limit the controller to a single namespace. This way, you can configure the default TLS certificate only for the Ingress Controller that manages the Operations Center's namespace \u2014 limiting the effect of the workaround. In addition to the change for the default certificate, we also need to make sure external Masters can connect to the Operations Center. They require two ports to be open, 443 (or 80 if no TLS) and 50000 (if you've kept the default). For this we enable tcp-services and configure the 50000 port on the deployment and service (Kubernetes resources). How to change the Nginx Ingress configuration depends on how you've installed it. For sake of brevity, I will simply edit the resources via a kubectl edit (hint: don't do this in production). TCP Services \u00b6 To enable the TCP services, we need to do two things: * create a config map with the configuration * enable tcp services in the deployment and point to the configmap # nginx-config-map.yaml apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : kube-system data : 50000 : \"<CJOC_NAMESPACE>/cjoc:50000\" Info Do replace CJOC_NAMESPACE with the actual namespace! kubectl apply -f nginx-config-map.yaml Once this is done, we can edit the controller's deployment. We have to make two changes: * add the tcp-services coniguration * add the port 50000 configuration kubectl -n ${ NAMESPACE } edit deployment nginx-ingress-controller Among the container args, we add --tcp-services-configmap . spec : containers : - args : - /nginx-ingress-controller ... - --tcp-services-configmap=kube-system/tcp-services Next, we edit the same deployment, and add the tcp port. We will add the 50000 to the container ports. - containerPort : 50000 name : jnlp protocol : TCP It will then look something like this. ports : - containerPort : 80 name : http protocol : TCP - containerPort : 443 name : https protocol : TCP - containerPort : 50000 name : jnlp protocol : TCP Last but not least, we also have to update the Nginx Ingress service resource. kubectl -n $NAMESPACE edit svc nginx-ingress-controller We add a similar port definition. - name : jnlp nodePort : 31559 port : 50000 protocol : TCP targetPort : jnlp Which will then look something like this (clusterIp and nodePort's will be different). spec : clusterIP : ..... externalTrafficPolicy : Cluster ports : - name : http nodePort : 31406 port : 80 protocol : TCP targetPort : http - name : https nodePort : 30391 port : 443 protocol : TCP targetPort : https - name : jnlp nodePort : 31559 port : 50000 protocol : TCP targetPort : jnlp Set Default TLS Certificate \u00b6 The first thing you'll have to do is to find out the name of the secret container the certificate. If you've created it yourself, great, else this is a way. kubectl get certificate -A The output should look like this: NAMESPACE NAME READY SECRET AGE jx-staging tls-staging-gke-kearos-net-p True tls-staging-gke-kearos-net-p 4d jx tls-dev-gke-kearos-net-p True tls-dev-gke-kearos-net-p 15d It should tell you which secret belongs to which certificate and in which namespace it resides. Note down both the secret name and the namespace. Info Hint, the -A flag means --all-namespaces but then with less typing. kubectl -n ${ NAMESPACE } edit deployment nginx-ingress-controller Among the container args, - --default-ssl-certificate=<namespace>/<secretName> . And it should then look something like this: spec : containers : - args : - /nginx-ingress-controller ... - --default-ssl-certificate=default/tls-fake-kearos-net Configure AKS Cluster \u00b6 We need to do the following steps: retrieve the Kubernetes API endpoint retrieve the Kubernetes API endpoint's certificate retrieve the service account token install an Ingress controller create & configure namespace for Managed Master(s) configure CloudBees Sidecar Injector Gather Kubernetes Credential & Certificate \u00b6 If you haven't already retrieved the Kubernetes credentials for your AKS cluster, you can do so via the az or AzureCLI . az aks get-credentials --resource-group ${ AKS_RESOURCE_GROUP } --name ${ AKS_CLUSTER_NAME } All three parts that we need, the Kubernetes API endpoint , Kubernetes API endpoint's certificate , Service Account Token will be added to our .kubeconfig . In my case, it is located at ~/.kube/config . Kubernetes API endpoint: cluster.server Kubernetes API endpoint's certificate: cluster.certificate-authority-data Service Account Token: users.user.token Mind you, the certificate is Base64 encoded. To decode, you can usually use a command line tool. macOs echo \"LS0tLS1C....LS0tCg==\" | base64 -D The decoded thing should look like this: -----BEGIN CERTIFICATE----- MIIEyTCCArGgAwIBAgIQK0sOS0aRjfZPYLM1TaRQMjANBgkqhkiG9w0BAQsFADAN .......... kKrPcnzV0gRdWNGNoJtRh9EGtKDP1VZUBiwdH44 = -----END CERTIFICATE----- Save this into a .pem file, we're going to need it. Let's call it aks-kubernetes-api-server.pem . Install an Ingress controller \u00b6 We're going to use Helm to install the Nginx Ingres Controller. Let's start by creating a namespace for your ingress resources kubectl create namespace ingress-nginx Add the official stable repository, just to be sure. helm repo add stable https://kubernetes-charts.storage.googleapis.com/ And update the helm repos. helm repo update Create a values file, ingress-values.yaml , we will need to change some values. rbac : create : true defaultBackend : enabled : false controller : ingressClass : \"nginx\" metrics : enabled : \"true\" replicaCount : 2 nodeSelector : beta\\.kubernetes.io/os : linux scope : enabled : \"true\" namespace : cbmasters service : externalTrafficPolicy : \"Cluster\" Helm V3 helm install nginx-ingress stable/nginx-ingress \\ --namespace ingress-nginx \\ --values ingress-values.yaml \\ --version 1 .29.6 Helm V2 helm install \\ --name nginx-ingress stable/nginx-ingress \\ --namespace ingress-nginx \\ --values ingress-values.yaml \\ --version 1 .29.6 Create & Configure Namespace \u00b6 We're going to use a feature from the official CloudBees Core helm chart. We start by creating the namespace that will house our Managed Master(s). NAMESPACE = kubectl create namespace $NAMESPACE Create a configuration for CloudBees Core Helm chart, called cloudbees-values.yaml . The namespace of the Operations Center doesn't really matter, it is in a different cluster anyway. OperationsCenter : Enabled : false Master : Enabled : true OperationsCenterNamespace : cloudbees-core Agents : Enabled : true To be able to generate the Kubernetes resources files, we first have to use helm fetch to retrieve the chart. helm fetch \\ --repo https://charts.cloudbees.com/public/cloudbees \\ --version 3 .8.0+a0d07461ae1c \\ cloudbees-core Now we can generate the Kubernetes resource definitions and store them as a single yaml file. helm template cloudbees-core-namespace \\ cloudbees-core-3.8.0+a0d07461ae1c.tgz \\ -f cloudbees-values.yaml \\ --namespace ${ NAMESPACE } \\ > cloudbees-core-namespace.yml Which we then apply. kubectl apply -f cloudbees-core-namespace.yml --namespace ${ NAMESPACE } Configure CloudBees Sidecar Injector \u00b6 Unfortunately, the Let's Encrypt Root CA certificates are not yet in the trust stores of our images. So we have to ensure they end up in the Masters we run in this (AKS) cluster, so they do not run into SSL errors. You can find the certificates on the certificates page of letsencrypt.org. Make sure to download and save the PEM of ISRGRoot X1 (self-signed) , Let's Encrypt Authority X3 (IdenTrust Cross-signed) , and Let's Encrypt Authority X3 (Signed By ISRG Root X1) . Next, we have to add them to a bundle and a truststore, we can then put into a ConfigMap. We first have retrieve the current bundle and truststore. We can get them from Operations Center via a kubectl cp . The process is described in the CloudBees Cloud Admin Guide . Unfortunately, it seems as of CloudBees Core 2.204.+ the location has changed. The new locations are below. kubectl cp cjoc-0:etc/pki/ca-trust/extracted/java/cacerts ca-bundle/cacerts kubectl cp cjoc-0:etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem ca-bundle/ca-certificates.crt Now that we have the files to start with, we can add our Let's Encrypt Root and Intermediary certificates. This assumes we're in the folder ca-bundle . cat valid-isrgrootx1-letsencrypt-org.pem >> ca-certificates.crt keytool -import -noprompt -keystore cacerts -file valid-isrgrootx1-letsencrypt-org.pem -storepass changeit -alias letsencrypt ; We also have to add the aks-kubernetes-api-server.pem to ensure we can talk to the Kubernetes API Endpoint. cat aks-kubernetes-api-server.pem >> ca-certificates.crt keytool -import -noprompt -keystore cacerts -file aks-kubernetes-api-server.pem -storepass changeit -alias kubeapi ; Info If you use custom or self-signed certificates, the process is the same. Simply substitute the Let's Encrypt certificates mentioned by your own / your provider's. This assumes we're in the folder ca-bundle . kubectl create configmap --from-file = ca-certificates.crt,cacerts ca-bundles Now we can focus on installing the CloudBees Sidecar Injector . It is a tool that creates a Sidecar to inject the certificate bundle and truststore from the Configmap into containers. As before, we first fetch chart. Unfortunately, the value we need to change is not parameterized, so we have to un tar the config. We can let the Helm fetch do that via the --untar flag, followed by the --untardir <folderName> flag. Tip If you haven't done so already, configure the CloudBees Helm repo by the following commands. helm repo add cloudbees https://charts.cloudbees.com/public/cloudbees helm repo update Update the configmap, in the file templates/configmap.yaml , set requiresExplicitInjection to true. requiresExplicitInjection: true Generate the end result via helm template . helm template cloudbees-sidecar-injector \\ cloudbees-sidecar-injector \\ --namespace cloudbees-sidecar-injector \\ > cloudbees-sidecar-injector.yml And apply the file. kubectl apply -f cloudbees-sidecar-injector.yml Configure Operations Center \u00b6 We have to configure two things in Operations Center. First, we create the AKS endpoint, and then we configure the Master template by adding a YAML snippet. We go to Operations Center -> Manage Jenkins -> Configure System -> Kubernetes Master Provisioning . Configure Endpoint \u00b6 Every field is essential here. API endpoint URL : the Kubernetes API server url, you should have this saved somewhere (e.g. ~/.kube/config ) Display Name : the name by which you can refer to this endpoint Credentials : credentials of the Service Account that can access the Kubernetes API, you should have this saved somewhere (e.g. ~/.kube/config , users.user.token ) Server Certificate : the certificate of the Kubernetes API Server of the target cluster, copy paste the contents of aks-kubernetes-api-server.pem Namespace : the namespace we created on the AKS cluster which is configured to be used by Operations Center Master URL Pattern : assuming that your domain name for the other cluster is different, make sure you have a pattern in here http://ake.mydomain.com/*/ ensure all Master will have that host name and the Master name will replace the * Jenkins URL : the URL of this Operations Center, you can leave this blank Master Template \u00b6 We can open up the Master template configuration by hitting the Advanced button in the Kubernetes Master Provisioning block. We have to add the annotation com.cloudbees.sidecar-injector/inject: yes to ensure the Masters will receive the certificates from the CloudBees Sidecar Injector - so they can talk to Operations Center. The full snippet becomes this: apiVersion : \"apps/v1\" kind : \"StatefulSet\" spec : template : metadata : annotations : com.cloudbees.sidecar-injector/inject : yes And then hit the Save button at the bottom to persist our changes. Create a Managed Master \u00b6 To create a Managed Master, we go to the home page of the Operations Center. If you're unsure, go to <cjocHost>/cjoc/ or click on the top left breadcrumb Jenkins . On the top right, hit the button New Master . Give the Master an appropriate name and hit Go . Tip Sometimes the button is not clickable. Click next to the button with your mouse to make the text field lose focus. It should now be clickable. The first thing we have to change, is the Cluster endpoint . This should now be a dropdown with kubernetes and Azure (or whatever you've named your endpoint). Select the one that points to your other cluster, in my case, Azure . We have one more change to make. We have to tell the JVM where our truststore is with the additional certificates. To ensure we can contact the Operations Center and the Kubernetes API. We do this by supplying the javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword properties in the System Properties field. Each should be on its own line. If you're wondering what the location is, this is where the CloudBees Sidecar Injector will place the truststore. javax.net.ssl.trustStore = /etc/ssl/certs/java/cacerts javax.net.ssl.trustStorePassword = changeit If you run into SSL or truststore issues -not trust issues, I cannot help you with those - you can enable debug logging by adding the following property. javax.net.debug = SSL,trustmanager Hit the Save button at the bottom, and you should be good to go!","title":"Multi-Cluster"},{"location":"cloudbees/multi-cluster/#prerequisites","text":"First off, we need a [GKE cluster] (/kubernetes/distributions/gke-terraform/) and a AKS cluster . Next, we make the GKE cluster the default one. Here we will install CloudBees Core. You can either install CloudBees Core directly or via Jenkins X . You're also welcome to follow the CloudBees documentation . The expected state is that you access Operations Center via a proper DNS name with TLS enabled, served via Nginx Ingress. And the AKS cluster is ready to be used - but empty. Some further assumptions: you have admin rights on both clusters the TLS certificate is from Let's Encrypt via Certmanager the AKS cluster is empty You can break these assumptions, but that means you have to change the examples to reflect your situation.","title":"Prerequisites"},{"location":"cloudbees/multi-cluster/#steps-to-do","text":"High over, we have to do the following steps: tweak the GKE cluster configure the AKS cluster configure Operations Center create a Managed Master","title":"Steps to do"},{"location":"cloudbees/multi-cluster/#tweak-gke-cluster","text":"At this time of writing, Jenkins is suffering from an outdated OpenSSL library . This version doesn't support SNI , which is what the Nginx Ingress Controller relies on. When a Client Master (Managed or not) connects to the Operations Center via the Nginx Controller without SNI, the controller will not route you to the correct path. Because of this, when TLS is enabled, you will get the default controller's TLS certificate. As this certificate, by default, is an invalid one, you get TLS handshake errors. We have to rectify this. There are many ways to do this. For example, you make the TLS certificate for Operations Center the default TLS certificate. While this might not be nice for other applications, it will suit our case. Info If there are many other applications in the same cluster, consider having more than one Nginx Ingress Controller. Via the configuration parameters controller.scope.enabled and controller.scope.namespace , you can limit the controller to a single namespace. This way, you can configure the default TLS certificate only for the Ingress Controller that manages the Operations Center's namespace \u2014 limiting the effect of the workaround. In addition to the change for the default certificate, we also need to make sure external Masters can connect to the Operations Center. They require two ports to be open, 443 (or 80 if no TLS) and 50000 (if you've kept the default). For this we enable tcp-services and configure the 50000 port on the deployment and service (Kubernetes resources). How to change the Nginx Ingress configuration depends on how you've installed it. For sake of brevity, I will simply edit the resources via a kubectl edit (hint: don't do this in production).","title":"Tweak GKE Cluster"},{"location":"cloudbees/multi-cluster/#tcp-services","text":"To enable the TCP services, we need to do two things: * create a config map with the configuration * enable tcp services in the deployment and point to the configmap # nginx-config-map.yaml apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : kube-system data : 50000 : \"<CJOC_NAMESPACE>/cjoc:50000\" Info Do replace CJOC_NAMESPACE with the actual namespace! kubectl apply -f nginx-config-map.yaml Once this is done, we can edit the controller's deployment. We have to make two changes: * add the tcp-services coniguration * add the port 50000 configuration kubectl -n ${ NAMESPACE } edit deployment nginx-ingress-controller Among the container args, we add --tcp-services-configmap . spec : containers : - args : - /nginx-ingress-controller ... - --tcp-services-configmap=kube-system/tcp-services Next, we edit the same deployment, and add the tcp port. We will add the 50000 to the container ports. - containerPort : 50000 name : jnlp protocol : TCP It will then look something like this. ports : - containerPort : 80 name : http protocol : TCP - containerPort : 443 name : https protocol : TCP - containerPort : 50000 name : jnlp protocol : TCP Last but not least, we also have to update the Nginx Ingress service resource. kubectl -n $NAMESPACE edit svc nginx-ingress-controller We add a similar port definition. - name : jnlp nodePort : 31559 port : 50000 protocol : TCP targetPort : jnlp Which will then look something like this (clusterIp and nodePort's will be different). spec : clusterIP : ..... externalTrafficPolicy : Cluster ports : - name : http nodePort : 31406 port : 80 protocol : TCP targetPort : http - name : https nodePort : 30391 port : 443 protocol : TCP targetPort : https - name : jnlp nodePort : 31559 port : 50000 protocol : TCP targetPort : jnlp","title":"TCP Services"},{"location":"cloudbees/multi-cluster/#set-default-tls-certificate","text":"The first thing you'll have to do is to find out the name of the secret container the certificate. If you've created it yourself, great, else this is a way. kubectl get certificate -A The output should look like this: NAMESPACE NAME READY SECRET AGE jx-staging tls-staging-gke-kearos-net-p True tls-staging-gke-kearos-net-p 4d jx tls-dev-gke-kearos-net-p True tls-dev-gke-kearos-net-p 15d It should tell you which secret belongs to which certificate and in which namespace it resides. Note down both the secret name and the namespace. Info Hint, the -A flag means --all-namespaces but then with less typing. kubectl -n ${ NAMESPACE } edit deployment nginx-ingress-controller Among the container args, - --default-ssl-certificate=<namespace>/<secretName> . And it should then look something like this: spec : containers : - args : - /nginx-ingress-controller ... - --default-ssl-certificate=default/tls-fake-kearos-net","title":"Set Default TLS Certificate"},{"location":"cloudbees/multi-cluster/#configure-aks-cluster","text":"We need to do the following steps: retrieve the Kubernetes API endpoint retrieve the Kubernetes API endpoint's certificate retrieve the service account token install an Ingress controller create & configure namespace for Managed Master(s) configure CloudBees Sidecar Injector","title":"Configure AKS Cluster"},{"location":"cloudbees/multi-cluster/#gather-kubernetes-credential-certificate","text":"If you haven't already retrieved the Kubernetes credentials for your AKS cluster, you can do so via the az or AzureCLI . az aks get-credentials --resource-group ${ AKS_RESOURCE_GROUP } --name ${ AKS_CLUSTER_NAME } All three parts that we need, the Kubernetes API endpoint , Kubernetes API endpoint's certificate , Service Account Token will be added to our .kubeconfig . In my case, it is located at ~/.kube/config . Kubernetes API endpoint: cluster.server Kubernetes API endpoint's certificate: cluster.certificate-authority-data Service Account Token: users.user.token Mind you, the certificate is Base64 encoded. To decode, you can usually use a command line tool. macOs echo \"LS0tLS1C....LS0tCg==\" | base64 -D The decoded thing should look like this: -----BEGIN CERTIFICATE----- MIIEyTCCArGgAwIBAgIQK0sOS0aRjfZPYLM1TaRQMjANBgkqhkiG9w0BAQsFADAN .......... kKrPcnzV0gRdWNGNoJtRh9EGtKDP1VZUBiwdH44 = -----END CERTIFICATE----- Save this into a .pem file, we're going to need it. Let's call it aks-kubernetes-api-server.pem .","title":"Gather Kubernetes Credential &amp; Certificate"},{"location":"cloudbees/multi-cluster/#install-an-ingress-controller","text":"We're going to use Helm to install the Nginx Ingres Controller. Let's start by creating a namespace for your ingress resources kubectl create namespace ingress-nginx Add the official stable repository, just to be sure. helm repo add stable https://kubernetes-charts.storage.googleapis.com/ And update the helm repos. helm repo update Create a values file, ingress-values.yaml , we will need to change some values. rbac : create : true defaultBackend : enabled : false controller : ingressClass : \"nginx\" metrics : enabled : \"true\" replicaCount : 2 nodeSelector : beta\\.kubernetes.io/os : linux scope : enabled : \"true\" namespace : cbmasters service : externalTrafficPolicy : \"Cluster\" Helm V3 helm install nginx-ingress stable/nginx-ingress \\ --namespace ingress-nginx \\ --values ingress-values.yaml \\ --version 1 .29.6 Helm V2 helm install \\ --name nginx-ingress stable/nginx-ingress \\ --namespace ingress-nginx \\ --values ingress-values.yaml \\ --version 1 .29.6","title":"Install an Ingress controller"},{"location":"cloudbees/multi-cluster/#create-configure-namespace","text":"We're going to use a feature from the official CloudBees Core helm chart. We start by creating the namespace that will house our Managed Master(s). NAMESPACE = kubectl create namespace $NAMESPACE Create a configuration for CloudBees Core Helm chart, called cloudbees-values.yaml . The namespace of the Operations Center doesn't really matter, it is in a different cluster anyway. OperationsCenter : Enabled : false Master : Enabled : true OperationsCenterNamespace : cloudbees-core Agents : Enabled : true To be able to generate the Kubernetes resources files, we first have to use helm fetch to retrieve the chart. helm fetch \\ --repo https://charts.cloudbees.com/public/cloudbees \\ --version 3 .8.0+a0d07461ae1c \\ cloudbees-core Now we can generate the Kubernetes resource definitions and store them as a single yaml file. helm template cloudbees-core-namespace \\ cloudbees-core-3.8.0+a0d07461ae1c.tgz \\ -f cloudbees-values.yaml \\ --namespace ${ NAMESPACE } \\ > cloudbees-core-namespace.yml Which we then apply. kubectl apply -f cloudbees-core-namespace.yml --namespace ${ NAMESPACE }","title":"Create &amp; Configure Namespace"},{"location":"cloudbees/multi-cluster/#configure-cloudbees-sidecar-injector","text":"Unfortunately, the Let's Encrypt Root CA certificates are not yet in the trust stores of our images. So we have to ensure they end up in the Masters we run in this (AKS) cluster, so they do not run into SSL errors. You can find the certificates on the certificates page of letsencrypt.org. Make sure to download and save the PEM of ISRGRoot X1 (self-signed) , Let's Encrypt Authority X3 (IdenTrust Cross-signed) , and Let's Encrypt Authority X3 (Signed By ISRG Root X1) . Next, we have to add them to a bundle and a truststore, we can then put into a ConfigMap. We first have retrieve the current bundle and truststore. We can get them from Operations Center via a kubectl cp . The process is described in the CloudBees Cloud Admin Guide . Unfortunately, it seems as of CloudBees Core 2.204.+ the location has changed. The new locations are below. kubectl cp cjoc-0:etc/pki/ca-trust/extracted/java/cacerts ca-bundle/cacerts kubectl cp cjoc-0:etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem ca-bundle/ca-certificates.crt Now that we have the files to start with, we can add our Let's Encrypt Root and Intermediary certificates. This assumes we're in the folder ca-bundle . cat valid-isrgrootx1-letsencrypt-org.pem >> ca-certificates.crt keytool -import -noprompt -keystore cacerts -file valid-isrgrootx1-letsencrypt-org.pem -storepass changeit -alias letsencrypt ; We also have to add the aks-kubernetes-api-server.pem to ensure we can talk to the Kubernetes API Endpoint. cat aks-kubernetes-api-server.pem >> ca-certificates.crt keytool -import -noprompt -keystore cacerts -file aks-kubernetes-api-server.pem -storepass changeit -alias kubeapi ; Info If you use custom or self-signed certificates, the process is the same. Simply substitute the Let's Encrypt certificates mentioned by your own / your provider's. This assumes we're in the folder ca-bundle . kubectl create configmap --from-file = ca-certificates.crt,cacerts ca-bundles Now we can focus on installing the CloudBees Sidecar Injector . It is a tool that creates a Sidecar to inject the certificate bundle and truststore from the Configmap into containers. As before, we first fetch chart. Unfortunately, the value we need to change is not parameterized, so we have to un tar the config. We can let the Helm fetch do that via the --untar flag, followed by the --untardir <folderName> flag. Tip If you haven't done so already, configure the CloudBees Helm repo by the following commands. helm repo add cloudbees https://charts.cloudbees.com/public/cloudbees helm repo update Update the configmap, in the file templates/configmap.yaml , set requiresExplicitInjection to true. requiresExplicitInjection: true Generate the end result via helm template . helm template cloudbees-sidecar-injector \\ cloudbees-sidecar-injector \\ --namespace cloudbees-sidecar-injector \\ > cloudbees-sidecar-injector.yml And apply the file. kubectl apply -f cloudbees-sidecar-injector.yml","title":"Configure CloudBees Sidecar Injector"},{"location":"cloudbees/multi-cluster/#configure-operations-center","text":"We have to configure two things in Operations Center. First, we create the AKS endpoint, and then we configure the Master template by adding a YAML snippet. We go to Operations Center -> Manage Jenkins -> Configure System -> Kubernetes Master Provisioning .","title":"Configure Operations Center"},{"location":"cloudbees/multi-cluster/#configure-endpoint","text":"Every field is essential here. API endpoint URL : the Kubernetes API server url, you should have this saved somewhere (e.g. ~/.kube/config ) Display Name : the name by which you can refer to this endpoint Credentials : credentials of the Service Account that can access the Kubernetes API, you should have this saved somewhere (e.g. ~/.kube/config , users.user.token ) Server Certificate : the certificate of the Kubernetes API Server of the target cluster, copy paste the contents of aks-kubernetes-api-server.pem Namespace : the namespace we created on the AKS cluster which is configured to be used by Operations Center Master URL Pattern : assuming that your domain name for the other cluster is different, make sure you have a pattern in here http://ake.mydomain.com/*/ ensure all Master will have that host name and the Master name will replace the * Jenkins URL : the URL of this Operations Center, you can leave this blank","title":"Configure Endpoint"},{"location":"cloudbees/multi-cluster/#master-template","text":"We can open up the Master template configuration by hitting the Advanced button in the Kubernetes Master Provisioning block. We have to add the annotation com.cloudbees.sidecar-injector/inject: yes to ensure the Masters will receive the certificates from the CloudBees Sidecar Injector - so they can talk to Operations Center. The full snippet becomes this: apiVersion : \"apps/v1\" kind : \"StatefulSet\" spec : template : metadata : annotations : com.cloudbees.sidecar-injector/inject : yes And then hit the Save button at the bottom to persist our changes.","title":"Master Template"},{"location":"cloudbees/multi-cluster/#create-a-managed-master","text":"To create a Managed Master, we go to the home page of the Operations Center. If you're unsure, go to <cjocHost>/cjoc/ or click on the top left breadcrumb Jenkins . On the top right, hit the button New Master . Give the Master an appropriate name and hit Go . Tip Sometimes the button is not clickable. Click next to the button with your mouse to make the text field lose focus. It should now be clickable. The first thing we have to change, is the Cluster endpoint . This should now be a dropdown with kubernetes and Azure (or whatever you've named your endpoint). Select the one that points to your other cluster, in my case, Azure . We have one more change to make. We have to tell the JVM where our truststore is with the additional certificates. To ensure we can contact the Operations Center and the Kubernetes API. We do this by supplying the javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword properties in the System Properties field. Each should be on its own line. If you're wondering what the location is, this is where the CloudBees Sidecar Injector will place the truststore. javax.net.ssl.trustStore = /etc/ssl/certs/java/cacerts javax.net.ssl.trustStorePassword = changeit If you run into SSL or truststore issues -not trust issues, I cannot help you with those - you can enable debug logging by adding the following property. javax.net.debug = SSL,trustmanager Hit the Save button at the bottom, and you should be good to go!","title":"Create a Managed Master"},{"location":"cloudbees/sso-azure-ad/","text":"Azure AD & CloudBees Core \u00b6 In this article we're going to configure Azure AD as Single-Sign-Solution for CloudBees Core. The configuration rests on three points; 1) Azure AD, 2) Jenkins' SAML plugin, and 3) CloudBees Core's Role Base Access Control or RBAC for short. Important Currently there's a discrepency between the TimeToLive (TTL) of the RefreshToken in Azure AD and the default configuration in the SAML plugin. This creates the problem that you can be logged in, in Azure, but when going to Jenkins you get the Logged Out page. The only way to log back in, is to logout in Azure and back in. The reason is as follows: The max lifetime of the Access Token in Azure AD seems to be 24 hours where the refresh token can live for a maximum of 14 days (if the access token expires the refresh token is used to try to obtain a new access token). The Jenkins setting in Configure Global Security > SAML Identity Provider Settings > Maximum Authentication Lifetime is 24 hours (86400 in seconds) upping this to 1209600 (which is 14 days in seconds/the max lifetime of the Refresh Token). The recommended resolution is to set Maximum Authentication Lifetime to the same value of your RefreshToken TTL. If you haven't changed this in Azure AD, it is 1209600 . Manage Jenkins -> Configure Global Security > SAML Identity Provider Settings > Maximum Authentication Lifetime = 1209600 Prerequisites \u00b6 Before we start, there are some requirements. a running CloudBees Core Operations Center instance this instance is accessible via https. if you do not have a valid certificate and you're running on Kubernetes take a look at my cert-manager guide Let's Encrypt can now also work with nip.io addresses active Azure subscription have an Azure subscription Administrator on hand Configure Azure \u00b6 Warning We use https://cloudbees-core.example.com as the example URL for CloudBees Core. Please replace the domain to your actual domain in all the examples! Steps to execute \u00b6 We do the following steps on the Azure side. create the Azure Active Directory create users and groups create App Registration URL: https://cloudbees-core.example.com/cjoc/securityRealm/finishLogin replace example.com with your domain, https is required! update manifest (for groups) change: \"groupMembershipClaims\": null, (usually line 11) to: \"groupMembershipClaims\": \"SecurityGroup\", create SP ID / App ID URI grant admin consent Info If you use the Azure AD plugin, you also create a client secret. Information To Note Down \u00b6 The following information is unique to your installation, so you need to record them as you go along. App ID URI Object ID 's of Users and Groups you want to give rights Federation Metadata Document Endpoint Azure AD -> App Registrations -> -> Endpoints (circular icon on top) you can use either the URL or the document contents make sure the URL contains the Tenant ID of your Azure Active Directory URL example: https://login.microsoftonline.com/${TENANT_ID}/federationmetadata/2007-06/federationmetadata.xml You can find your Tenant ID in Azure Active Directory -> Properties -> Directory ID (different name, same ID) Visual Guide \u00b6 Below is a visual guide with screenshots. Pay attention to these hints in the screenshots. red : this is the main thing orange : this is how we got to the current page/view blue : while you're in this screen, there might be other things you could do Create New App Registration \u00b6 If you have an Azure account, you have an Azure Active Directory (Azure AD) pre-created. This guide assumes you have an Azure AD ready to use. That means the next step is to create an Application Registration. Give the registration a useful name, select who can authenticate and the redirect URL . This URL needs to be configured and MUST be the actual URL of your CloudBees Core installation. Important To have Client Masters as a fallback for login, incase Operations Center is down, you have to add another redirect URL for each Master. Azure AD -> App Registrations -> -> Authentication -> Web -> https://example.com/teams-cat/securityRealm/finishLogin App Registration Data To Write Down \u00b6 Depending on the plugin you're going to use (SAML or Azure AD) you need information from your Azure AD / App Registration. Tentant ID Object ID Client ID Federation Metadata Document you can use the document XML content or the URL Click on the Endpoints button to open the side-bar with the links. App ID \u00b6 We need the App ID - even if the SAML plugin doesn't mention it. Azure generates an APP ID URI for you. You can also use CloudBees Core's URL as the URI. The URI is shown when there is an error, so it recommended to use a value you can recognize. Info App ID must match in both Azure AD (set as App ID URI ) and the SAML plugin (set as Entity ID ) configuration in Jenkins. So write it down. API Permissions \u00b6 Of course, things wouldn't be proper IAM/LDAP/AD without giving some permissions. If we want to retrieve group information and other fields, we need to be able to read the Directory information. You find the Directory information via the Microsoft Graph api button. We select Application Permissions and then check Directory.Read.All . We don't need to write. The Permissions have changed, so we require an Administrator account to consent with the new permissions. Update Manifest \u00b6 As with the permissions, the default Manifest doesn't give us all the information we want. We want the groups so we can configure RBAC, and thus we have to set the groupMembershipsClaims claim attribute. We change the null to \"SecurityGroup\" . Please consult the Microsoft docs (see reference below) for other options. We can download, edit, and upload the manifest file. Alternatively, we can edit inline and hit save on top. Retrieve Group Object ID \u00b6 If we want to assign Azure AD groups to groups or roles in Jenkins' RBAC, we need to use the Object ID 's. Each Group and User has an Object ID , which have a handy Copy this button on the end of the value box! Configure Jenkins \u00b6 We now get to the point where we configure Jenkins. The SAML plugin is opensource, and thus it's configuration can also be used for Jenkins or CloudBees Jenkins Distribution . Steps \u00b6 Here are the steps we perform to configure Azure AD as the Single-Sign-On solution. install the SAML plugin I assume you know how to install plugins, so we skip this if you don't know Read the Managing Plugins Guide configure saml 2.0 in Jenkins setup groups (RBAC) administrators -> admin group browsers -> all other groups Visual Guide \u00b6 Below is a visual guide with screenshots. Please pay attention to the hints in the screenshots. Red : this is the main thing Orange : this is how we got to the current page/view Blue : while you're in this screen, there might be other things you could do Configure Security \u00b6 To go to Jenkins' security configuration, follow this route: login with an Admin user go to the Operations Center Manage Jenkins -> Global Security Configuration Configure RBAC \u00b6 The SAML plugin configuration pollutes the screen with fields. My advice is to enable RBAC first. If you haven't got any groups/roles yet, I recommend using the Typical Initial Setup from the dropdown. The logged-in user is automatically registered as administrator. So if your Azure AD configuration doesn't work, this user can still manage Jenkins. Important Make sure you know the credentials of the current admin user. It will automatically be added to the Administrators group, and it will be your go-to account when you mess up the SAML configuration and you have to reset security. For how to reset the security configuration, see the For When You Mess Up paragraph. Configure SAML \u00b6 Select SAML 2.0 from the Security Realm options. Here we first supply our Federation Metadata Document content or it's URL. Each option - document content or URL - has its own Validate ... button, hit it and confirm it says Success . Info You can leave Displayname empty, which gives you the default naming scheme from Azure AD. I think this is ugly, as it amounts to something like ${EMAIL_ADDRESS}_${AD_DOMAIN}_${AZURE_CORP_DOMAIN} . There are other options, I've settled for givenname , as there isn't a fullname by default, and well, I prefer Joost to a long hard to recognize string. Fields \u00b6 Displayname : http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname Group : http://schemas.microsoft.com/ws/2008/06/identity/claims/groups Username : http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name Email : http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress SP Entity ID : the App ID URI you configured in Azure AD (hidden behind Advanced Configuration ) Configure RBAC Groups \u00b6 Tip Once Azure AD is configured, and it works, you can configure groups for RBAC just as you're used to. Both for classic RBAC and Team Masters. Just make sure you use the Azure AD Object ID 's of the groups to map them. Bonus tip, add every Azure AD group to Browsers , so you can directly map their groups to Team Master roles without problems. XML Config \u00b6 <useSecurity> true </useSecurity> <authorizationStrategy class= \"nectar.plugins.rbac.strategy.RoleMatrixAuthorizationStrategyImpl\" /> <securityRealm class= \"org.jenkinsci.plugins.saml.SamlSecurityRealm\" plugin= \"saml@1.1.2\" > <displayNameAttributeName> http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname </displayNameAttributeName> <groupsAttributeName> http://schemas.microsoft.com/ws/2008/06/identity/claims/groups </groupsAttributeName> <maximumAuthenticationLifetime> 86400 </maximumAuthenticationLifetime> <emailAttributeName> http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress </emailAttributeName> <usernameCaseConversion> none </usernameCaseConversion> <usernameAttributeName> http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name </usernameAttributeName> <binding> urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect </binding> <advancedConfiguration> <forceAuthn> false </forceAuthn> <spEntityId> https://cloudbees-core.kearos.net </spEntityId> </advancedConfiguration> <idpMetadataConfiguration> <xml></xml> <url> https://login.microsoftonline.com/95b46e09-0307-488b-a6fc-1d2717ba9c49/federationmetadata/2007-06/federationmetadata.xml </url> <period> 5 </period> </idpMetadataConfiguration> </securityRealm> <disableRememberMe> false </disableRememberMe> Logout URL \u00b6 Depending on the requirements, you may want to specify a logout url in the SAML configuration to log you completely out of SAML, not just Core. An example https://login.windows.net/<tenant_id_of_your_app>/oauth2/logout?post_logout_redirect_uri=<logout_URL_of_your_app>/logout For When You Mess Up \u00b6 This is the default config for security in CloudBees Core. This file is in ${JENKINS_HOME}/config.xml , the XML tags we want to look at are quite near the top. <useSecurity> true </useSecurity> <authorizationStrategy class= \"hudson.security.FullControlOnceLoggedInAuthorizationStrategy\" > <denyAnonymousReadAccess> true </denyAnonymousReadAccess> </authorizationStrategy> <securityRealm class= \"hudson.security.HudsonPrivateSecurityRealm\" > <disableSignup> true </disableSignup> <enableCaptcha> false </enableCaptcha> </securityRealm> <disableRememberMe> false </disableRememberMe> On CloudBees Core Modern / Kubernetes \u00b6 To rectify a failed configuration, execute the following steps: exec into the cjoc-0 container: kubectl exec -ti cjoc-0 -- bash open config.xml : vi /var/jenkins_home/config.xml replace conflicting lines with the above snippet save the changes exit the container: exit kill the pod: kubectl delete po cjoc-0 Tip For removing a whole line, stay in \"normal\" mode, and press d d (two times the d key). To add the new lines, go into insert mode by pressing the i key. Go back to \"normal\" mode by pressing the esc key. Then, save and quit, by writing: :wq followed by enter . References \u00b6 CloudBees Guide on Azure AD for Core SSO (outdated) SAML Plugin Docs for Azure AD (outdated) Microsoft Doc for Azure AD Tokens Microsoft Doc for Azure AD Optional Tokens Microsoft Doc for Azure AD Custom Tokens Alternative Azure AD Plugin (very new) Info Currently, there is a limitation which requires you to use the Object ID 's which make searching groups and people less than ideal. When the Alternative Azure AD Plugin is approved this may provide a more satisfactory solution.","title":"Single Sign On - AzureAD"},{"location":"cloudbees/sso-azure-ad/#azure-ad-cloudbees-core","text":"In this article we're going to configure Azure AD as Single-Sign-Solution for CloudBees Core. The configuration rests on three points; 1) Azure AD, 2) Jenkins' SAML plugin, and 3) CloudBees Core's Role Base Access Control or RBAC for short. Important Currently there's a discrepency between the TimeToLive (TTL) of the RefreshToken in Azure AD and the default configuration in the SAML plugin. This creates the problem that you can be logged in, in Azure, but when going to Jenkins you get the Logged Out page. The only way to log back in, is to logout in Azure and back in. The reason is as follows: The max lifetime of the Access Token in Azure AD seems to be 24 hours where the refresh token can live for a maximum of 14 days (if the access token expires the refresh token is used to try to obtain a new access token). The Jenkins setting in Configure Global Security > SAML Identity Provider Settings > Maximum Authentication Lifetime is 24 hours (86400 in seconds) upping this to 1209600 (which is 14 days in seconds/the max lifetime of the Refresh Token). The recommended resolution is to set Maximum Authentication Lifetime to the same value of your RefreshToken TTL. If you haven't changed this in Azure AD, it is 1209600 . Manage Jenkins -> Configure Global Security > SAML Identity Provider Settings > Maximum Authentication Lifetime = 1209600","title":"Azure AD &amp; CloudBees Core"},{"location":"cloudbees/sso-azure-ad/#prerequisites","text":"Before we start, there are some requirements. a running CloudBees Core Operations Center instance this instance is accessible via https. if you do not have a valid certificate and you're running on Kubernetes take a look at my cert-manager guide Let's Encrypt can now also work with nip.io addresses active Azure subscription have an Azure subscription Administrator on hand","title":"Prerequisites"},{"location":"cloudbees/sso-azure-ad/#configure-azure","text":"Warning We use https://cloudbees-core.example.com as the example URL for CloudBees Core. Please replace the domain to your actual domain in all the examples!","title":"Configure Azure"},{"location":"cloudbees/sso-azure-ad/#steps-to-execute","text":"We do the following steps on the Azure side. create the Azure Active Directory create users and groups create App Registration URL: https://cloudbees-core.example.com/cjoc/securityRealm/finishLogin replace example.com with your domain, https is required! update manifest (for groups) change: \"groupMembershipClaims\": null, (usually line 11) to: \"groupMembershipClaims\": \"SecurityGroup\", create SP ID / App ID URI grant admin consent Info If you use the Azure AD plugin, you also create a client secret.","title":"Steps to execute"},{"location":"cloudbees/sso-azure-ad/#information-to-note-down","text":"The following information is unique to your installation, so you need to record them as you go along. App ID URI Object ID 's of Users and Groups you want to give rights Federation Metadata Document Endpoint Azure AD -> App Registrations -> -> Endpoints (circular icon on top) you can use either the URL or the document contents make sure the URL contains the Tenant ID of your Azure Active Directory URL example: https://login.microsoftonline.com/${TENANT_ID}/federationmetadata/2007-06/federationmetadata.xml You can find your Tenant ID in Azure Active Directory -> Properties -> Directory ID (different name, same ID)","title":"Information To Note Down"},{"location":"cloudbees/sso-azure-ad/#visual-guide","text":"Below is a visual guide with screenshots. Pay attention to these hints in the screenshots. red : this is the main thing orange : this is how we got to the current page/view blue : while you're in this screen, there might be other things you could do","title":"Visual Guide"},{"location":"cloudbees/sso-azure-ad/#create-new-app-registration","text":"If you have an Azure account, you have an Azure Active Directory (Azure AD) pre-created. This guide assumes you have an Azure AD ready to use. That means the next step is to create an Application Registration. Give the registration a useful name, select who can authenticate and the redirect URL . This URL needs to be configured and MUST be the actual URL of your CloudBees Core installation. Important To have Client Masters as a fallback for login, incase Operations Center is down, you have to add another redirect URL for each Master. Azure AD -> App Registrations -> -> Authentication -> Web -> https://example.com/teams-cat/securityRealm/finishLogin","title":"Create New App Registration"},{"location":"cloudbees/sso-azure-ad/#app-registration-data-to-write-down","text":"Depending on the plugin you're going to use (SAML or Azure AD) you need information from your Azure AD / App Registration. Tentant ID Object ID Client ID Federation Metadata Document you can use the document XML content or the URL Click on the Endpoints button to open the side-bar with the links.","title":"App Registration Data To Write Down"},{"location":"cloudbees/sso-azure-ad/#app-id","text":"We need the App ID - even if the SAML plugin doesn't mention it. Azure generates an APP ID URI for you. You can also use CloudBees Core's URL as the URI. The URI is shown when there is an error, so it recommended to use a value you can recognize. Info App ID must match in both Azure AD (set as App ID URI ) and the SAML plugin (set as Entity ID ) configuration in Jenkins. So write it down.","title":"App ID"},{"location":"cloudbees/sso-azure-ad/#api-permissions","text":"Of course, things wouldn't be proper IAM/LDAP/AD without giving some permissions. If we want to retrieve group information and other fields, we need to be able to read the Directory information. You find the Directory information via the Microsoft Graph api button. We select Application Permissions and then check Directory.Read.All . We don't need to write. The Permissions have changed, so we require an Administrator account to consent with the new permissions.","title":"API Permissions"},{"location":"cloudbees/sso-azure-ad/#update-manifest","text":"As with the permissions, the default Manifest doesn't give us all the information we want. We want the groups so we can configure RBAC, and thus we have to set the groupMembershipsClaims claim attribute. We change the null to \"SecurityGroup\" . Please consult the Microsoft docs (see reference below) for other options. We can download, edit, and upload the manifest file. Alternatively, we can edit inline and hit save on top.","title":"Update Manifest"},{"location":"cloudbees/sso-azure-ad/#retrieve-group-object-id","text":"If we want to assign Azure AD groups to groups or roles in Jenkins' RBAC, we need to use the Object ID 's. Each Group and User has an Object ID , which have a handy Copy this button on the end of the value box!","title":"Retrieve Group Object ID"},{"location":"cloudbees/sso-azure-ad/#configure-jenkins","text":"We now get to the point where we configure Jenkins. The SAML plugin is opensource, and thus it's configuration can also be used for Jenkins or CloudBees Jenkins Distribution .","title":"Configure Jenkins"},{"location":"cloudbees/sso-azure-ad/#steps","text":"Here are the steps we perform to configure Azure AD as the Single-Sign-On solution. install the SAML plugin I assume you know how to install plugins, so we skip this if you don't know Read the Managing Plugins Guide configure saml 2.0 in Jenkins setup groups (RBAC) administrators -> admin group browsers -> all other groups","title":"Steps"},{"location":"cloudbees/sso-azure-ad/#visual-guide_1","text":"Below is a visual guide with screenshots. Please pay attention to the hints in the screenshots. Red : this is the main thing Orange : this is how we got to the current page/view Blue : while you're in this screen, there might be other things you could do","title":"Visual Guide"},{"location":"cloudbees/sso-azure-ad/#configure-security","text":"To go to Jenkins' security configuration, follow this route: login with an Admin user go to the Operations Center Manage Jenkins -> Global Security Configuration","title":"Configure Security"},{"location":"cloudbees/sso-azure-ad/#configure-rbac","text":"The SAML plugin configuration pollutes the screen with fields. My advice is to enable RBAC first. If you haven't got any groups/roles yet, I recommend using the Typical Initial Setup from the dropdown. The logged-in user is automatically registered as administrator. So if your Azure AD configuration doesn't work, this user can still manage Jenkins. Important Make sure you know the credentials of the current admin user. It will automatically be added to the Administrators group, and it will be your go-to account when you mess up the SAML configuration and you have to reset security. For how to reset the security configuration, see the For When You Mess Up paragraph.","title":"Configure RBAC"},{"location":"cloudbees/sso-azure-ad/#configure-saml","text":"Select SAML 2.0 from the Security Realm options. Here we first supply our Federation Metadata Document content or it's URL. Each option - document content or URL - has its own Validate ... button, hit it and confirm it says Success . Info You can leave Displayname empty, which gives you the default naming scheme from Azure AD. I think this is ugly, as it amounts to something like ${EMAIL_ADDRESS}_${AD_DOMAIN}_${AZURE_CORP_DOMAIN} . There are other options, I've settled for givenname , as there isn't a fullname by default, and well, I prefer Joost to a long hard to recognize string.","title":"Configure SAML"},{"location":"cloudbees/sso-azure-ad/#fields","text":"Displayname : http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname Group : http://schemas.microsoft.com/ws/2008/06/identity/claims/groups Username : http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name Email : http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress SP Entity ID : the App ID URI you configured in Azure AD (hidden behind Advanced Configuration )","title":"Fields"},{"location":"cloudbees/sso-azure-ad/#configure-rbac-groups","text":"Tip Once Azure AD is configured, and it works, you can configure groups for RBAC just as you're used to. Both for classic RBAC and Team Masters. Just make sure you use the Azure AD Object ID 's of the groups to map them. Bonus tip, add every Azure AD group to Browsers , so you can directly map their groups to Team Master roles without problems.","title":"Configure RBAC Groups"},{"location":"cloudbees/sso-azure-ad/#xml-config","text":"<useSecurity> true </useSecurity> <authorizationStrategy class= \"nectar.plugins.rbac.strategy.RoleMatrixAuthorizationStrategyImpl\" /> <securityRealm class= \"org.jenkinsci.plugins.saml.SamlSecurityRealm\" plugin= \"saml@1.1.2\" > <displayNameAttributeName> http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname </displayNameAttributeName> <groupsAttributeName> http://schemas.microsoft.com/ws/2008/06/identity/claims/groups </groupsAttributeName> <maximumAuthenticationLifetime> 86400 </maximumAuthenticationLifetime> <emailAttributeName> http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress </emailAttributeName> <usernameCaseConversion> none </usernameCaseConversion> <usernameAttributeName> http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name </usernameAttributeName> <binding> urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect </binding> <advancedConfiguration> <forceAuthn> false </forceAuthn> <spEntityId> https://cloudbees-core.kearos.net </spEntityId> </advancedConfiguration> <idpMetadataConfiguration> <xml></xml> <url> https://login.microsoftonline.com/95b46e09-0307-488b-a6fc-1d2717ba9c49/federationmetadata/2007-06/federationmetadata.xml </url> <period> 5 </period> </idpMetadataConfiguration> </securityRealm> <disableRememberMe> false </disableRememberMe>","title":"XML Config"},{"location":"cloudbees/sso-azure-ad/#logout-url","text":"Depending on the requirements, you may want to specify a logout url in the SAML configuration to log you completely out of SAML, not just Core. An example https://login.windows.net/<tenant_id_of_your_app>/oauth2/logout?post_logout_redirect_uri=<logout_URL_of_your_app>/logout","title":"Logout URL"},{"location":"cloudbees/sso-azure-ad/#for-when-you-mess-up","text":"This is the default config for security in CloudBees Core. This file is in ${JENKINS_HOME}/config.xml , the XML tags we want to look at are quite near the top. <useSecurity> true </useSecurity> <authorizationStrategy class= \"hudson.security.FullControlOnceLoggedInAuthorizationStrategy\" > <denyAnonymousReadAccess> true </denyAnonymousReadAccess> </authorizationStrategy> <securityRealm class= \"hudson.security.HudsonPrivateSecurityRealm\" > <disableSignup> true </disableSignup> <enableCaptcha> false </enableCaptcha> </securityRealm> <disableRememberMe> false </disableRememberMe>","title":"For When You Mess Up"},{"location":"cloudbees/sso-azure-ad/#on-cloudbees-core-modern-kubernetes","text":"To rectify a failed configuration, execute the following steps: exec into the cjoc-0 container: kubectl exec -ti cjoc-0 -- bash open config.xml : vi /var/jenkins_home/config.xml replace conflicting lines with the above snippet save the changes exit the container: exit kill the pod: kubectl delete po cjoc-0 Tip For removing a whole line, stay in \"normal\" mode, and press d d (two times the d key). To add the new lines, go into insert mode by pressing the i key. Go back to \"normal\" mode by pressing the esc key. Then, save and quit, by writing: :wq followed by enter .","title":"On CloudBees Core Modern / Kubernetes"},{"location":"cloudbees/sso-azure-ad/#references","text":"CloudBees Guide on Azure AD for Core SSO (outdated) SAML Plugin Docs for Azure AD (outdated) Microsoft Doc for Azure AD Tokens Microsoft Doc for Azure AD Optional Tokens Microsoft Doc for Azure AD Custom Tokens Alternative Azure AD Plugin (very new) Info Currently, there is a limitation which requires you to use the Object ID 's which make searching groups and people less than ideal. When the Alternative Azure AD Plugin is approved this may provide a more satisfactory solution.","title":"References"},{"location":"cloudbees/teams-automation/","text":"Core Modern Teams Automation \u00b6 CloudBees Core on Modern (meaning, on Kubernetes) has two main types of Jenkins Masters, a Managed Master, and a Team Master. In this article, we're going to automate the creation and management of the Team Masters. Hint If you do not want to read any of the code here, or just want to take a look at the end result - pipeline wise - you can find working examples on GitHub. Template Repository - creates a new team template and a PR to the GitOps repository GitOps Repository - applies GitOps principles to manage the CloudBees Core Team Masters Goals \u00b6 Just automating the creation of a Team Master is relatively easy, as this can be done via the Client Jar . So we're going to set some additional goals to create a decent challenge. GitOps : I want to be able to create and delete Team Masters by managing configuration in a Git repository Configuration-as-Code : as much of the configuration as possible should be stored in the Git repository Namespace : one of the major reasons for Team Masters to exist is to increase (Product) Team autonomy, which in Kubernetes should correspond to a namespace . So I want each Team Master to be in its own Namespace! Self-Service : the solution should cater to (semi-)autonomous teams and lower the workload of the team managing CloudBees Core. So requesting a Team Master should be doable by everyone Before We Start \u00b6 Some assumptions need to be taken care off before we start. Kubernetes cluster in which you are ClusterAdmin if you don't have one yet, there are guides on this elsewhere on the site your cluster has enough capacity (at least two nodes of 4gb memory) your cluster has CloudBees Core Modern installed if you don't have this yet look at one of the guides on this site or look at the guides on CloudBees.com have administrator access to CloudBees Core Cloud Operations Center Code Examples The more extensive Code Examples, such as Kubernetes Yaml files, are collapsed by default. You can open them by clicking on them. On the right, the code snippet will have a [ ] copy icon. Below is an example. Code Snippet Example Here's a code snippet. pipeline { agent any stages { stage ( 'Hello' ) { steps { echo 'Hello World!' } } } } Bootstrapping \u00b6 All right, so we want to use GitOps and to process the changes we need a Pipeline that can be triggered by a Webhook. I believe in everything as code - except Secrets and such - which includes the Pipeline. Unfortunately, Operations Center cannot run such pipelines. To get over this hurdle, we will create a special Ops Team Master. This Master will be configured to be able to Manage the other Team Masters for us. Log into your Operations Center with a user that has administrative access. Create API Token \u00b6 Create a new API Token for your administrator user by clicking on the user's name - top right corner. Select the Configuration menu on the left and then you should see a section where you can Create a API Token . This Token will disappear, so write it down. Get & Configure Client Jar \u00b6 Replace the values marked by < ... > . The Operations Center URL should look like this: http://cbcore.mydomain.com/cjoc . Setup the connection variables. OC_URL = <your operations center url> USR = <your username> TKN = <api token> Download the Client Jar. curl ${ OC_URL } /jnlpJars/jenkins-cli.jar -o jenkins-cli.jar Create Alias & Test \u00b6 alias cboc = \"java -jar jenkins-cli.jar -noKeyAuth -auth ${ USR } : ${ TKN } -s ${ OC_URL } \" cboc version Create Team Ops \u00b6 As the tasks of the Team Masters for managing Operations are quite specific and demand special rights, I'd recommend putting this in its own namespace . To do so properly, we need to configure a few things. allows Operations Center access to this namespace (so it can create the Team Master) give the ServiceAccount the permissions to create namespace 's for the other Team Masters add config map for the Jenkins Agents temporarily change Operations Center's operating Namespace (where it will spawn resources in) use the CLI to create the team-ops Team Master reset Operations Center's operating Namespace Update & Create Kubernetes Namespaces \u00b6 Create Team Ops Namespace \u00b6 kubectl apply -f team-ops-namespace.yaml team-ops-namespace.yaml This creates the team-ops namespace including all the resources required such as ResourceQuota , ServiceAccount and so on. apiVersion : v1 kind : Namespace metadata : name : team-ops --- apiVersion : v1 kind : ResourceQuota metadata : name : resource-quota namespace : team-ops spec : hard : pods : \"20\" requests.cpu : \"4\" requests.memory : 6Gi limits.cpu : \"5\" limits.memory : 10Gi services.loadbalancers : \"0\" services.nodeports : \"0\" persistentvolumeclaims : \"10\" --- apiVersion : v1 kind : ServiceAccount metadata : name : jenkins namespace : team-ops --- kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : pods-all namespace : team-ops rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/log\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : jenkins namespace : team-ops roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pods-all subjects : - kind : ServiceAccount name : jenkins namespace : team-ops --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : cjoc roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : master-management subjects : - kind : ServiceAccount name : jenkins namespace : team-ops --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : create-namespaces rules : - apiGroups : [ \"*\" ] resources : [ \"serviceaccounts\" , \"rolebindings\" , \"roles\" , \"resourcequotas\" , \"namespaces\" ] verbs : [ \"create\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"configmaps\" , \"rolebindings\" , \"roles\" , \"resourcequotas\" , \"namespaces\" ] verbs : [ \"create\" , \"get\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"events\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"persistentvolumeclaims\" , \"pods\" , \"pods/exec\" , \"services\" , \"statefulsets\" , \"ingresses\" , \"extensions\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/log\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"secrets\" ] verbs : [ \"list\" ] - apiGroups : [ \"apps\" ] resources : [ \"statefulsets\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : ops-namespace roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : create-namespaces subjects : - kind : ServiceAccount name : jenkins namespace : team-ops Update Operation Center ServiceAccount \u00b6 The ServiceAccount under which Operation Center runs, only has rights in it's own namespace . Which means it cannot create our Team Ops Master. Below is the .yaml file for Kubernetes and the command to apply it. Warning I assume you're using the default cloudbees-core as per Cloudbees' documentation. If this is not the case, change the last line, namespace: cloudbees-core with the namespace your Operation Center runs in. kubectl apply -f patch-oc-serviceaccount.yaml -n team-ops patch-oc-serviceaccount.yaml This patches the existing Operation Center's ServiceAccount to also have the correct rights in the team-ops namespace. kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : master-management rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/log\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] - apiGroups : [ \"apps\" ] resources : [ \"statefulsets\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"persistentvolumeclaims\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"secrets\" ] verbs : [ \"list\" ] - apiGroups : [ \"\" ] resources : [ \"events\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : cjoc roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : master-management subjects : - kind : ServiceAccount name : cjoc namespace : cloudbees-core Jenkins Agent ConfigMap \u00b6 kubectl apply -f jenkins-agent-config-map.yaml -n team-ops jenkins-agent-config-map.yaml Creates the Jenkins Agent ConfigMap, which contains the information the Jenkins Agent - within a PodTemplate - uses to connect to the Jenkins Master. apiVersion : v1 kind : ConfigMap metadata : name : jenkins-agent data : jenkins-agent : | #!/usr/bin/env sh # The MIT License # # Copyright (c) 2015, CloudBees, Inc. # # Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files (the \"Software\"), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN # THE SOFTWARE. # Usage jenkins-slave.sh [options] -url http://jenkins [SECRET] [AGENT_NAME] # Optional environment variables : # * JENKINS_TUNNEL : HOST:PORT for a tunnel to route TCP traffic to jenkins host, when jenkins can't be directly accessed over network # * JENKINS_URL : alternate jenkins URL # * JENKINS_SECRET : agent secret, if not set as an argument # * JENKINS_AGENT_NAME : agent name, if not set as an argument if [ $# -eq 1 ]; then # if `docker run` only has one arguments, we assume user is running alternate command like `bash` to inspect the image exec \"$@\" else # if -tunnel is not provided try env vars case \"$@\" in *\"-tunnel \"*) ;; *) if [ ! -z \"$JENKINS_TUNNEL\" ]; then TUNNEL=\"-tunnel $JENKINS_TUNNEL\" fi ;; esac if [ -n \"$JENKINS_URL\" ]; then URL=\"-url $JENKINS_URL\" fi if [ -n \"$JENKINS_NAME\" ]; then JENKINS_AGENT_NAME=\"$JENKINS_NAME\" fi if [ -z \"$JNLP_PROTOCOL_OPTS\" ]; then echo \"Warning: JnlpProtocol3 is disabled by default, use JNLP_PROTOCOL_OPTS to alter the behavior\" JNLP_PROTOCOL_OPTS=\"-Dorg.jenkinsci.remoting.engine.JnlpProtocol3.disabled=true\" fi # If both required options are defined, do not pass the parameters OPT_JENKINS_SECRET=\"\" if [ -n \"$JENKINS_SECRET\" ]; then case \"$@\" in *\"${JENKINS_SECRET}\"*) echo \"Warning: SECRET is defined twice in command-line arguments and the environment variable\" ;; *) OPT_JENKINS_SECRET=\"${JENKINS_SECRET}\" ;; esac fi OPT_JENKINS_AGENT_NAME=\"\" if [ -n \"$JENKINS_AGENT_NAME\" ]; then case \"$@\" in *\"${JENKINS_AGENT_NAME}\"*) echo \"Warning: AGENT_NAME is defined twice in command-line arguments and the environment variable\" ;; *) OPT_JENKINS_AGENT_NAME=\"${JENKINS_AGENT_NAME}\" ;; esac fi SLAVE_JAR=/usr/share/jenkins/slave.jar if [ ! -f \"$SLAVE_JAR\" ]; then tmpfile=$(mktemp) if hash wget > /dev/null 2>&1; then wget -O \"$tmpfile\" \"$JENKINS_URL/jnlpJars/slave.jar\" elif hash curl > /dev/null 2>&1; then curl -o \"$tmpfile\" \"$JENKINS_URL/jnlpJars/slave.jar\" else echo \"Image does not include $SLAVE_JAR and could not find wget or curl to download it\" return 1 fi SLAVE_JAR=$tmpfile fi #TODO: Handle the case when the command-line and Environment variable contain different values. #It is fine it blows up for now since it should lead to an error anyway. exec java $JAVA_OPTS $JNLP_PROTOCOL_OPTS -cp $SLAVE_JAR hudson.remoting.jnlp.Main -headless $TUNNEL $URL $OPT_JENKINS_SECRET $OPT_JENKINS_AGENT_NAME \"$@\" fi Create Initial Master \u00b6 To make it easier to change the namespace if needed, its extracted out from the command. OriginalNamespace = cloudbees-core This script changes the Operations Center's operating namespace , creates a Team Master with the name ops , and then resets the namespace. cboc groovy = < configure-oc-namespace.groovy team-ops cboc teams ops --put < team-ops.json cboc groovy = < configure-oc-namespace.groovy $OriginalNamespace team-ops.json This json file that describes a team. By default there are three roles defined on a team, TEAM_ADMIN , TEAM_MEMBER , and TEAM_GUEST . Don't forget to change the id 's to Group ID's from your Single-Sign-On solution. { \"version\" : \"1\" , \"data\" : { \"name\" : \"ops\" , \"displayName\" : \"Operations\" , \"provisioningRecipe\" : \"basic\" , \"members\" : [{ \"id\" : \"Catmins\" , \"roles\" : [ \"TEAM_ADMIN\" ] }, { \"id\" : \"Pirates\" , \"roles\" : [ \"TEAM_MEMBER\" ] }, { \"id\" : \"Continental\" , \"roles\" : [ \"TEAM_GUEST\" ] } ], \"icon\" : { \"name\" : \"hexagons\" , \"color\" : \"#8d7ec1\" } } } configure-oc-namespace.groovy This is a Jenkins Configuration or System Groovy script. It will change the namespace Operation Center uses to create resources. You can change this in the UI by going to Operations Center -> Manage Jenkins -> System Configuration -> Master Provisioning -> Namespace . import hudson.* import hudson.util.Secret ; import hudson.util.Scrambler ; import hudson.util.FormValidation ; import jenkins.* import jenkins.model.* import hudson.security.* import com.cloudbees.masterprovisioning.kubernetes.KubernetesMasterProvisioning import com.cloudbees.masterprovisioning.kubernetes.KubernetesClusterEndpoint println \"=== KubernetesMasterProvisioning Configuration - start\" println \"== Retrieving main configuration\" def descriptor = Jenkins . getInstance (). getInjector (). getInstance ( KubernetesMasterProvisioning . DescriptorImpl . class ) def namespace = this . args [ 0 ] def currentKubernetesClusterEndpoint = descriptor . getClusterEndpoints (). get ( 0 ) println \"= Found current endpoint\" println \"= \" + currentKubernetesClusterEndpoint . toString () def id = currentKubernetesClusterEndpoint . getId () def name = currentKubernetesClusterEndpoint . getName () def url = currentKubernetesClusterEndpoint . getUrl () def credentialsId = currentKubernetesClusterEndpoint . getCredentialsId () println \"== Setting Namspace to \" + namespace def updatedKubernetesClusterEndpoint = new KubernetesClusterEndpoint ( id , name , url , credentialsId , namespace ) def clusterEndpoints = new ArrayList < KubernetesClusterEndpoint >() clusterEndpoints . add ( updatedKubernetesClusterEndpoint ) descriptor . setClusterEndpoints ( clusterEndpoints ) println \"== Saving Jenkins configuration\" descriptor . save () println \"=== KubernetesMasterProvisioning Configuration - finish\" Configure Team Ops Master \u00b6 Now that we've created the Operations Team Master (Team Ops), we can configure it. The Pipelines we need will require credentials, we describe them below. githubtoken_token : GitHub API Token only, credentials type Secret Text (for the PR pipeline) githubtoken : GitHub username and API Token jenkins-api : Username and API Token for Operations Center. Just like the one we used for Client Jar. We also need to have a Global Pipeline Library defined by the name github.com/joostvdg/jpl-core . This, as the name suggests, should point to https://github.com/joostvdg/jpl-core.git . Create GitOps Pipeline \u00b6 In total, we need two repositories and two or three Pipelines. You can either use my CLI Docker Image or roll your own. I will proceed as if you will create your own. CLI Image Pipeline : this will create a CLI Docker Image that is used to talk to Operations Center via the Client Jar (CLI) PR Pipeline : I like the idea of Self-Service, but in order to keep things in check, you might want to provide that via a PullRequest (PR) rather than a direct write to the Master branch. This is also Repository One, as I prefer having each pipeline in their own Repository, but you don't need to. Main Pipeline : will trigger on a commit to the Master branch and create the new team. I'll even throw in a free manage your Team Recipes for free as well. Create CLI Image Pipeline \u00b6 In a Kubernetes cluster, you should not build with Docker directly, use an in-cluster builder such as Kaniko or Buildah . You can read more about the why and how elsewhere on this site . Tip If you do not want to create your own, you can re-use my images. There should be one available for every recent version of CloudBees Core that will work with your Operations Center. The images are available in DockerHub at caladreas/cbcore-cli Kaniko Configuration \u00b6 Kaniko uses a Docker Image to build your Docker Image in cluster. It does however need to directly communicate to your Docker Registry. This requires a Kubernetes Secret of type docker-registry . How you can do this and more, you can read on the CloudBees Core Docs . Pipeline \u00b6 Now that you have Kaniko configured, you can use this Pipeline to create your own CLI Images. Caution Make sure you replace the environment variables with values that make sense to you. CJOC_URL internal url in Kubernets, usually http://cjoc.<namespace>/cjoc REGISTRY : index.docker.io = DockerHub REPO : docker repository name IMAGE : docker image name Jenkinsfile Jenkins Declarative Pipeline for the CLI Image geberation. pipeline { agent { kubernetes { //cloud 'kubernetes' label 'test' yaml \"\"\" kind: Pod metadata: name: test spec: containers: - name: curl image: byrnedo/alpine-curl command: - cat tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: jenkins-docker-cfg projected: sources: - secret: name: docker-credentials items: - key: .dockerconfigjson path: .docker/config.json \"\"\" } } environment { CJOC_URL = 'http://cjoc.cloudbees-core/cjoc' CLI_VERSION = '' REGISTRY = 'index.docker.io' REPO = 'caladreas' IMAGE = 'cbcore-cli' } stages { stage ( 'Download CLI' ) { steps { container ( 'curl' ) { sh 'curl --version' sh 'echo ${CJOC_URL}/jnlpJars/jenkins-cli.jar' sh 'curl ${CJOC_URL}/jnlpJars/jenkins-cli.jar --output jenkins-cli.jar' sh 'ls -lath' } } } stage ( 'Prepare' ) { parallel { stage ( 'Verify CLI' ) { environment { CREDS = credentials ( 'jenkins-api' ) CLI = \"java -jar jenkins-cli.jar -noKeyAuth -s ${CJOC_URL} -auth\" } steps { sh 'echo ${CLI}' script { CLI_VERSION = sh returnStdout: true , script: '${CLI} ${CREDS} version' } sh 'echo ${CLI_VERSION}' } } stage ( 'Prepare Dockerfile' ) { steps { writeFile encoding: 'UTF-8' , file: 'Dockerfile' , text: \"\"\"FROM mcr.microsoft.com/java/jre-headless:8u192-zulu-alpine WORKDIR /usr/bin ADD jenkins-cli.jar . RUN pwd RUN ls -lath \"\"\" } } } } stage ( 'Build with Kaniko' ) { environment { PATH = \"/busybox:/kaniko:$PATH\" TAG = \"${CLI_VERSION}\" } steps { sh 'echo image fqn=${REGISTRY}/${REPO}/${IMAGE}:${TAG}' container ( name: 'kaniko' , shell: '/busybox/sh' ) { sh '''#!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:${TAG} /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:latest ''' } } } } } PR Pipeline \u00b6 Caution The PR Pipeline example builds upon the GitHub API, if you're not using GItHub, you will have to figure out another way to make the PR. Tools Used \u00b6 yq : commandline tool for processing Yaml files jq commandline tool for pressing Json files Kustomize templating tool for Kubernetes Yaml, as of Kubernetes 1.13 , this is part of the Client (note, your server can be older, don't worry!) Hub commandline client for GitHub Repository Layout \u00b6 folder: team-master-template with file simple.json folder: namespace-creation with folder: kustomize this contains the Kustomize configuration Simple.json This is a template for the team JSON definition. { \"version\" : \"1\" , \"data\" : { \"name\" : \"NAME\" , \"displayName\" : \"DISPLAY_NAME\" , \"provisioningRecipe\" : \"RECIPE\" , \"members\" : [ { \"id\" : \"ADMINS\" , \"roles\" : [ \"TEAM_ADMIN\" ] }, { \"id\" : \"MEMBERS\" , \"roles\" : [ \"TEAM_MEMBER\" ] }, { \"id\" : \"GUESTS\" , \"roles\" : [ \"TEAM_GUEST\" ] } ], \"icon\" : { \"name\" : \"ICON\" , \"color\" : \"HEX_COLOR\" } } } Kustomize Configuration \u00b6 Kustomize is a tool for template Kubernetes YAML definitions, which is what we need here. However, only for the namespace creation & configuration. So if you don't want to do that, you can skip this. The Kustomize configuration has two parts, a folder called team-example with a kustomization.yaml . This will be what we configure to generate a new yaml definition. The main template is in the folder base , where the entrypoint will be again kustomization.yaml . This time, the kustomization.yaml will link to all the template files we need. As posting all these yaml files again is a bit much, I'll link to my example repo. Feel free to fork it instead: cb-team-gitops-template configmap.yaml : the Jenkins Agent ConfigMap namespace.yaml : the new namespace resource-quota.yaml : resource quota's for the namespace role-binding-cjoc.yaml : a role binding for the CJOC ServiceAccount, so it create create the new Master in the new namespace role-binding.yaml : the role binding for the jenkins ServiceAccount, which allows the new Master to create and manage Pods (for PodTemplates) role-cjoc.yaml : the role for CJOC for the ability to create a Master in the new Namspace role.yaml : the role for the jenkins ServiceAccount for the new Master service-account.yaml : the ServiceAccount, jenkins , used by the new Master Pipeline \u00b6 The Pipeline will do the following: capture input parameters to be used to customize the Team Master update the Kustomize template to make sure every resource is correct for the new namespace ( teams-<name of team> ) execute Kustomize to generate a single yaml file that defines the configuration for the new Team Masters' namespace process the simple.json to generate a team.json file for the new Team Master for use with the Jenkins CLI checkout your GIT_REPO that contains your team definitions create a new PR to your GIT_REPO for the new team Jenkinsfile Variables to update: GIT_REPO : the GitHub repository in which the Team Definitions are stored RESET_NAMESPACE : the namespace Operations Center should use as default pipeline { agent { kubernetes { label 'team-automation' yaml \"\"\" kind: Pod spec: containers: - name: hub image: caladreas/hub command: [\"cat\"] tty: true resources: requests: memory: \"50Mi\" cpu: \"150m\" limits: memory: \"50Mi\" cpu: \"150m\" - name: kubectl image: bitnami/kubectl:latest command: [\"cat\"] tty: true securityContext: runAsUser: 1000 fsGroup: 1000 resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"150Mi\" cpu: \"200m\" - name: yq image: mikefarah/yq command: ['cat'] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" - name: jq image: colstrom/jq command: ['cat'] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" \"\"\" } } libraries { lib ( 'github.com/joostvdg/jpl-core' ) } options { disableConcurrentBuilds () // we don't want more than one at a time checkoutToSubdirectory 'templates' // we need to do two checkouts buildDiscarder logRotator ( artifactDaysToKeepStr: '' , artifactNumToKeepStr: '' , daysToKeepStr: '5' , numToKeepStr: '5' ) // always clean up } environment { envGitInfo = '' RESET_NAMESPACE = 'jx-production' TEAM_BASE_NAME = '' NAMESPACE_TO_CREATE = '' DISPLAY_NAME = '' TEAM_RECIPE = '' ICON = '' ICON_COLOR_CODE = '' ADMINS_ROLE = '' MEMBERS_ROLE = '' GUESTS_ROLE = '' RECORD_LOC = '' GIT_REPO = '' } stages { stage ( 'Team Details' ) { input { message \"Please enter the team details.\" ok \"Looks good, proceed\" parameters { string ( name: 'Name' , defaultValue: 'hex' , description: 'Please specify a team name' ) string ( name: 'DisplayName' , defaultValue: 'Hex' , description: 'Please specify a team display name' ) choice choices: [ 'joostvdg' , 'basic' , 'java-web' ], description: 'Please select a Team Recipe' , name: 'TeamRecipe' choice choices: [ 'anchor' , 'bear' , 'bowler-hat' , 'briefcase' , 'bug' , 'calculator' , 'calculatorcart' , 'clock' , 'cloud' , 'cloudbees' , 'connect' , 'dollar-bill' , 'dollar-symbol' , 'file' , 'flag' , 'flower-carnation' , 'flower-daisy' , 'help' , 'hexagon' , 'high-heels' , 'jenkins' , 'key' , 'marker' , 'monocle' , 'mustache' , 'office' , 'panther' , 'paw-print' , 'teacup' , 'tiger' , 'truck' ], description: 'Please select an Icon' , name: 'Icon' string ( name: 'IconColorCode' , defaultValue: '#CCCCCC' , description: 'Please specify a valid html hexcode for the color (https://htmlcolorcodes.com/)' ) string ( name: 'Admins' , defaultValue: 'Catmins' , description: 'Please specify a groupid or userid for the TEAM_ADMIN role' ) string ( name: 'Members' , defaultValue: 'Pirates' , description: 'Please specify a groupid or userid for the TEAM_MEMBER role' ) string ( name: 'Guests' , defaultValue: 'Continental' , description: 'Please specify a groupid or userid for the TEAM_GUEST role' ) } } steps { println \"Name=${Name}\" println \"DisplayName=${DisplayName}\" println \"TeamRecipe=${TeamRecipe}\" println \"Icon=${Icon}\" println \"IconColorCode=${IconColorCode}\" println \"Admins=${Admins}\" println \"Members=${Members}\" println \"Guests=${Guests}\" script { TEAM_BASE_NAME = \"${Name}\" NAMESPACE_TO_CREATE = \"cb-teams-${Name}\" DISPLAY_NAME = \"${DisplayName}\" TEAM_RECIPE = \"${TeamRecipe}\" ICON = \"${Icon}\" ICON_COLOR_CODE = \"${IconColorCode}\" ADMINS_ROLE = \"${Admins}\" MEMBERS_ROLE = \"${Members}\" GUESTS_ROLE = \"${Guests}\" RECORD_LOC = \"templates/teams/${Name}\" sh \"mkdir -p ${RECORD_LOC}\" } } } stage ( 'Create Team Config' ) { environment { BASE = 'templates/namespace-creation/kustomize' NAMESPACE = \"${NAMESPACE_TO_CREATE}\" RECORD_LOC = \"templates/teams/${TEAM_BASE_NAME}\" } parallel { stage ( 'Namespace' ) { steps { container ( 'yq' ) { sh 'yq w -i ${BASE}/base/role-binding.yaml subjects[0].namespace ${NAMESPACE}' sh 'yq w -i ${BASE}/base/namespace.yaml metadata.name ${NAMESPACE}' sh 'yq w -i ${BASE}/team-example/kustomization.yaml namespace ${NAMESPACE}' } container ( 'kubectl' ) { sh ''' kubectl kustomize ${BASE}/team-example > ${RECORD_LOC}/team.yaml cat ${RECORD_LOC}/team.yaml ''' } } } stage ( 'Team Master JSON' ) { steps { container ( 'jq' ) { sh \"\"\"jq \\ '.data.name = \"${TEAM_BASE_NAME}\" |\\ .data.displayName = \"${DISPLAY_NAME}\" |\\ .data.provisioningRecipe = \"${TEAM_RECIPE}\" |\\ .data.icon.name = \"${ICON}\" |\\ .data.icon.color = \"${ICON_COLOR_CODE}\" |\\ .data.members[0].id = \"${ADMINS_ROLE}\" |\\ .data.members[1].id = \"${MEMBERS_ROLE}\" |\\ .data.members[2].id = \"${GUESTS_ROLE}\"'\\ templates/team-master-template/simple.json > ${RECORD_LOC}/team.json \"\"\" } sh 'cat ${RECORD_LOC}/team.json' } } } } stage ( 'Create PR' ) { when { branch 'master' } environment { RECORD_OLD_LOC = \"templates/teams/${TEAM_BASE_NAME}\" RECORD_LOC = \"teams/${TEAM_BASE_NAME}\" PR_CHANGE_NAME = \"add_team_${TEAM_BASE_NAME}\" } steps { container ( 'hub' ) { dir ( 'cb-team-gitops' ) { script { envGitInfo = git \"${GIT_REPO}\" } sh 'git checkout -b ${PR_CHANGE_NAME}' sh 'ls -lath ../${RECORD_OLD_LOC}' sh 'cp -R ../${RECORD_OLD_LOC} ./teams' sh 'ls -lath' sh 'ls -lath teams/' gitRemoteConfigByUrl ( envGitInfo . GIT_URL , 'githubtoken_token' ) // must be a API Token ONLY -> secret text sh ''' git config --global user.email \"jenkins@jenkins.io\" git config --global user.name \"Jenkins\" git add ${RECORD_LOC} git status git commit -m \"add team ${TEAM_BASE_NAME}\" git push origin ${PR_CHANGE_NAME} ''' // has to be indented like that, else the indents will be in the pr description writeFile encoding: 'UTF-8' , file: 'pr-info.md' , text: \"\"\"Add ${TEAM_BASE_NAME} \\n This pr is automatically generated via CloudBees.\\\\n \\n The job: ${env.JOB_URL} \"\"\" // TODO: unfortunately, environment {}'s credentials have fixed environment variable names // TODO: in this case, they need to be EXACTLY GITHUB_PASSWORD and GITHUB_USER script { withCredentials ([ usernamePassword ( credentialsId: 'githubtoken' , passwordVariable: 'GITHUB_PASSWORD' , usernameVariable: 'GITHUB_USER' )]) { sh \"\"\" set +x hub pull-request --force -F pr-info.md -l '${TEAM_BASE_NAME}' --no-edit \"\"\" } } } } } } } } Main Pipeline \u00b6 The main Pipeline should be part of a repository. The Repository should look like this: recipes (folder) recipes.json -> current complete list of CloudBees Core Team Recipes definition teams (folder) folder per team team.json -> CloudBees Core Team definition team.yaml -> Kubernetes YAML definition of the namespace and all its resources Process \u00b6 The pipeline can be a bit hard to grasp, so let me break it down into individual steps. We have the following stages: Create Team - which is broken into sub-stages via the sequential stages feature . * Parse Changelog * Create Namespace * Change OC Namespace * Create Team Master Test CLI Connection Update Team Recipes Notable Statements \u00b6 disableConcurrentBuilds We change the namespace of Operation Center to a different value only for the duration of creating this master. This is something that should probably be part of the Team Master creation, but as it is a single configuration option for all that Operation Center does, we need to be careful. By ensuring we only run one build concurrently, we reduce the risk of this blowing up in our face. options { disableConcurrentBuilds () } when { } The When Directive allows us to creating effective conditions for when a stage should be executed. The snippet below shows the use of a combination of both the branch and changeset built-in filters. changeset looks at the commit being build and validates that there was a change in that file path. when { allOf { branch 'master' ; changeset \"teams/**/team.*\" } } post { always { } } The Post Directive allows us to run certain commands after the main pipeline has run depending on the outcome (compared or not to the previous outcome). In this case, we want to make sure we reset the namespace used by Operations Center to the original value. By using post { always {} } , it will ALWAYS run, regardless of the status of the pipeline. So we should be safe. post { always { container ( 'cli' ) { sh '${CLI} ${CREDS} groovy = < resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE}' } } } stages { stage { parallel { stage() { stages { stage { Oke, you might've noticed this massive indenting depth and probably have some questions. By combining sequential stages with parallel stages we can create a set of stages that will be executed in sequence but can be controlled by a single when {} statement whether or not they get executed. This prevents mistakes being made in the condition and accidentally running one or other but not all the required steps. stages { stage ( 'Create Team' ) { parallel { stage ( 'Main' ) { stages { stage ( 'Parse Changelog' ) { changetSetData & container('jpb') {} Alright, so even if we know a team was added in /teams/<team-name> , we still don't know the following two things: 1) what is the name of this team, 2) was this team changed or deleted? So we have to process the changelog to be able to answer these questions as well. There are different ways of getting the changelog and parsing it. I've written one you can do on ANY machine, regardless of Jenkins by leveraging Git and my own custom binary ( jpb -> Jenkins Pipeline Binary). The code for my binary is at GitHub: github.com/joostvdg/jpb . An alternative approach is described by CloudBees Support here , which leverages Jenkins groovy powers. COMMIT_INFO = \"${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT}\" def changeSetData = sh returnStdout: true , script: \"git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO}\" changeSetData = changeSetData . replace ( \"\\n\" , \"\\\\n\" ) container ( 'jpb' ) { changeSetFolders = sh returnStdout: true , script: \"/usr/bin/jpb/bin/jpb GitChangeListToFolder '${changeSetData}' 'teams/'\" changeSetFolders = changeSetFolders . split ( ',' ) } Files \u00b6 recipes.json The default Team Recipes that ships with CloudBees Core Modern. { \"version\" : \"1\" , \"data\" : [{ \"name\" : \"basic\" , \"displayName\" : \"Basic\" , \"description\" : \"The minimalistic setup.\" , \"plugins\" : [ \"bluesteel-master\" , \"cloudbees-folders-plus\" , \"cloudbees-jsync-archiver\" , \"cloudbees-monitoring\" , \"cloudbees-nodes-plus\" , \"cloudbees-ssh-slaves\" , \"cloudbees-support\" , \"cloudbees-workflow-template\" , \"credentials-binding\" , \"email-ext\" , \"git\" , \"git-client\" , \"github-branch-source\" , \"github-organization-folder\" , \"infradna-backup\" , \"ldap\" , \"mailer\" , \"operations-center-analytics-reporter\" , \"operations-center-cloud\" , \"pipeline-model-definition\" , \"ssh-credentials\" , \"wikitext\" , \"workflow-aggregator\" , \"workflow-cps-checkpoint\" ], \"default\" : true }, { \"name\" : \"java-web\" , \"displayName\" : \"Java & Web Development\" , \"description\" : \"The essential tools to build, release and deploy Java Web applications including integration with Maven, Gradle and Node JS.\" , \"plugins\" : [ \"bluesteel-master\" , \"cloudbees-folders-plus\" , \"cloudbees-jsync-archiver\" , \"cloudbees-monitoring\" , \"cloudbees-nodes-plus\" , \"cloudbees-ssh-slaves\" , \"cloudbees-support\" , \"cloudbees-workflow-template\" , \"credentials-binding\" , \"email-ext\" , \"git\" , \"git-client\" , \"github-branch-source\" , \"github-organization-folder\" , \"infradna-backup\" , \"ldap\" , \"mailer\" , \"operations-center-analytics-reporter\" , \"operations-center-cloud\" , \"pipeline-model-definition\" , \"ssh-credentials\" , \"wikitext\" , \"workflow-aggregator\" , \"workflow-cps-checkpoint\" , \"config-file-provider\" , \"cloudbees-aws-cli\" , \"cloudbees-cloudfoundry-cli\" , \"findbugs\" , \"gradle\" , \"jira\" , \"junit\" , \"nodejs\" , \"openshift-cli\" , \"pipeline-maven\" , \"tasks\" , \"warnings\" ], \"default\" : false }] } Jenkinsfile This is the pipeline that will process the commit to the repository and, if it detects a new team is created will apply the changes. Variables to overwrite: GIT_REPO : the https url to the Git Repository your GitOps code/configuration is stored RESET_NAMESPACE : the namespace your Operation Center normally operates in CLI : this command depends on the namespace Operation Center is in ( http://<service name>.<namespace>/cjoc ) pipeline { agent { kubernetes { label 'jenkins-agent' yaml ''' apiVersion: v1 kind: Pod spec: serviceAccountName: jenkins containers: - name: cli image: caladreas/cbcore-cli:2.176.2.3 imagePullPolicy: Always command: - cat tty: true resources: requests: memory: \"50Mi\" cpu: \"150m\" limits: memory: \"50Mi\" cpu: \"150m\" - name: kubectl image: bitnami/kubectl:latest command: [\"cat\"] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"150Mi\" cpu: \"200m\" - name: yq image: mikefarah/yq command: ['cat'] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" - name: jpb image: caladreas/jpb command: - cat tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" securityContext: runAsUser: 1000 fsGroup: 1000 ''' } } options { disableConcurrentBuilds () buildDiscarder logRotator ( artifactDaysToKeepStr: '' , artifactNumToKeepStr: '' , daysToKeepStr: '5' , numToKeepStr: '5' ) } environment { RESET_NAMESPACE = 'cloudbees-core' CREDS = credentials ( 'jenkins-api' ) CLI = \"java -jar /usr/bin/jenkins-cli.jar -noKeyAuth -s http://cjoc.cloudbees-core/cjoc -auth\" COMMIT_INFO = '' TEAM = '' GIT_REPO = '' } stages { stage ( 'Create Team' ) { when { allOf { branch 'master' ; changeset \"teams/**/team.*\" } } parallel { stage ( 'Main' ) { stages { stage ( 'Parse Changelog' ) { steps { // Alternative approach: https://support.cloudbees.com/hc/en-us/articles/217630098-How-to-access-Changelogs-in-a-Pipeline-Job- // However, that runs on the master, JPB runs in an agent! script { scmVars = git \"${GIT_REPO}\" COMMIT_INFO = \"${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT}\" def changeSetData = sh returnStdout: true , script: \"git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO}\" changeSetData = changeSetData . replace ( \"\\n\" , \"\\\\n\" ) container ( 'jpb' ) { changeSetFolders = sh returnStdout: true , script: \"/usr/bin/jpb/bin/jpb GitChangeListToFolder '${changeSetData}' 'teams/'\" changeSetFolders = changeSetFolders . split ( ',' ) } if ( changeSetFolders . length > 0 ) { TEAM = changeSetFolders [ 0 ] TEAM = TEAM . trim () // to protect against a team being removed def exists = fileExists \"teams/${TEAM}/team.yaml\" if (! exists ) { TEAM = '' } } else { TEAM = '' } echo \"Team that changed: |${TEAM}|\" } } } stage ( 'Create Namespace' ) { when { expression { return ! TEAM . equals ( '' ) } } environment { NAMESPACE = \"cb-teams-${TEAM}\" RECORD_LOC = \"teams/${TEAM}\" } steps { container ( 'kubectl' ) { sh ''' cat ${RECORD_LOC}/team.yaml kubectl apply -f ${RECORD_LOC}/team.yaml ''' } } } stage ( 'Change OC Namespace' ) { when { expression { return ! TEAM . equals ( '' ) } } environment { NAMESPACE = \"cb-teams-${TEAM}\" } steps { container ( 'cli' ) { sh 'echo ${NAMESPACE}' script { def response = sh encoding: 'UTF-8' , label: 'create team' , returnStatus: true , script: '${CLI} ${CREDS} groovy = < resources/bootstrap/configure-oc-namespace.groovy ${NAMESPACE}' println \"Response: ${response}\" } } } } stage ( 'Create Team Master' ) { when { expression { return ! TEAM . equals ( '' ) } } environment { TEAM_NAME = \"${TEAM}\" } steps { container ( 'cli' ) { println \"TEAM_NAME=${TEAM_NAME}\" sh 'ls -lath' sh 'ls -lath teams/' script { def response = sh encoding: 'UTF-8' , label: 'create team' , returnStatus: true , script: '${CLI} ${CREDS} teams ${TEAM_NAME} --put < \"teams/${TEAM_NAME}/team.json\"' println \"Response: ${response}\" } } } } } } } } stage ( 'Test CLI Connection' ) { steps { container ( 'cli' ) { script { def response = sh encoding: 'UTF-8' , label: 'retrieve version' , returnStatus: true , script: '${CLI} ${CREDS} version' println \"Response: ${response}\" } } } } stage ( 'Update Team Recipes' ) { when { allOf { branch 'master' ; changeset \"recipes/recipes.json\" } } steps { container ( 'cli' ) { sh 'ls -lath' sh 'ls -lath recipes/' script { def response = sh encoding: 'UTF-8' , label: 'update team recipe' , returnStatus: true , script: '${CLI} ${CREDS} team-creation-recipes --put < \"recipes/recipes.json\"' println \"Response: ${response}\" } } } } } post { always { container ( 'cli' ) { sh '${CLI} ${CREDS} groovy = < resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE}' } } } }","title":"CloudBees Automate Teams"},{"location":"cloudbees/teams-automation/#core-modern-teams-automation","text":"CloudBees Core on Modern (meaning, on Kubernetes) has two main types of Jenkins Masters, a Managed Master, and a Team Master. In this article, we're going to automate the creation and management of the Team Masters. Hint If you do not want to read any of the code here, or just want to take a look at the end result - pipeline wise - you can find working examples on GitHub. Template Repository - creates a new team template and a PR to the GitOps repository GitOps Repository - applies GitOps principles to manage the CloudBees Core Team Masters","title":"Core Modern Teams Automation"},{"location":"cloudbees/teams-automation/#goals","text":"Just automating the creation of a Team Master is relatively easy, as this can be done via the Client Jar . So we're going to set some additional goals to create a decent challenge. GitOps : I want to be able to create and delete Team Masters by managing configuration in a Git repository Configuration-as-Code : as much of the configuration as possible should be stored in the Git repository Namespace : one of the major reasons for Team Masters to exist is to increase (Product) Team autonomy, which in Kubernetes should correspond to a namespace . So I want each Team Master to be in its own Namespace! Self-Service : the solution should cater to (semi-)autonomous teams and lower the workload of the team managing CloudBees Core. So requesting a Team Master should be doable by everyone","title":"Goals"},{"location":"cloudbees/teams-automation/#before-we-start","text":"Some assumptions need to be taken care off before we start. Kubernetes cluster in which you are ClusterAdmin if you don't have one yet, there are guides on this elsewhere on the site your cluster has enough capacity (at least two nodes of 4gb memory) your cluster has CloudBees Core Modern installed if you don't have this yet look at one of the guides on this site or look at the guides on CloudBees.com have administrator access to CloudBees Core Cloud Operations Center Code Examples The more extensive Code Examples, such as Kubernetes Yaml files, are collapsed by default. You can open them by clicking on them. On the right, the code snippet will have a [ ] copy icon. Below is an example. Code Snippet Example Here's a code snippet. pipeline { agent any stages { stage ( 'Hello' ) { steps { echo 'Hello World!' } } } }","title":"Before We Start"},{"location":"cloudbees/teams-automation/#bootstrapping","text":"All right, so we want to use GitOps and to process the changes we need a Pipeline that can be triggered by a Webhook. I believe in everything as code - except Secrets and such - which includes the Pipeline. Unfortunately, Operations Center cannot run such pipelines. To get over this hurdle, we will create a special Ops Team Master. This Master will be configured to be able to Manage the other Team Masters for us. Log into your Operations Center with a user that has administrative access.","title":"Bootstrapping"},{"location":"cloudbees/teams-automation/#create-api-token","text":"Create a new API Token for your administrator user by clicking on the user's name - top right corner. Select the Configuration menu on the left and then you should see a section where you can Create a API Token . This Token will disappear, so write it down.","title":"Create API Token"},{"location":"cloudbees/teams-automation/#get-configure-client-jar","text":"Replace the values marked by < ... > . The Operations Center URL should look like this: http://cbcore.mydomain.com/cjoc . Setup the connection variables. OC_URL = <your operations center url> USR = <your username> TKN = <api token> Download the Client Jar. curl ${ OC_URL } /jnlpJars/jenkins-cli.jar -o jenkins-cli.jar","title":"Get &amp; Configure Client Jar"},{"location":"cloudbees/teams-automation/#create-alias-test","text":"alias cboc = \"java -jar jenkins-cli.jar -noKeyAuth -auth ${ USR } : ${ TKN } -s ${ OC_URL } \" cboc version","title":"Create Alias &amp; Test"},{"location":"cloudbees/teams-automation/#create-team-ops","text":"As the tasks of the Team Masters for managing Operations are quite specific and demand special rights, I'd recommend putting this in its own namespace . To do so properly, we need to configure a few things. allows Operations Center access to this namespace (so it can create the Team Master) give the ServiceAccount the permissions to create namespace 's for the other Team Masters add config map for the Jenkins Agents temporarily change Operations Center's operating Namespace (where it will spawn resources in) use the CLI to create the team-ops Team Master reset Operations Center's operating Namespace","title":"Create Team Ops"},{"location":"cloudbees/teams-automation/#update-create-kubernetes-namespaces","text":"","title":"Update &amp; Create Kubernetes Namespaces"},{"location":"cloudbees/teams-automation/#create-team-ops-namespace","text":"kubectl apply -f team-ops-namespace.yaml team-ops-namespace.yaml This creates the team-ops namespace including all the resources required such as ResourceQuota , ServiceAccount and so on. apiVersion : v1 kind : Namespace metadata : name : team-ops --- apiVersion : v1 kind : ResourceQuota metadata : name : resource-quota namespace : team-ops spec : hard : pods : \"20\" requests.cpu : \"4\" requests.memory : 6Gi limits.cpu : \"5\" limits.memory : 10Gi services.loadbalancers : \"0\" services.nodeports : \"0\" persistentvolumeclaims : \"10\" --- apiVersion : v1 kind : ServiceAccount metadata : name : jenkins namespace : team-ops --- kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : pods-all namespace : team-ops rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/log\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : jenkins namespace : team-ops roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pods-all subjects : - kind : ServiceAccount name : jenkins namespace : team-ops --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : cjoc roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : master-management subjects : - kind : ServiceAccount name : jenkins namespace : team-ops --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : create-namespaces rules : - apiGroups : [ \"*\" ] resources : [ \"serviceaccounts\" , \"rolebindings\" , \"roles\" , \"resourcequotas\" , \"namespaces\" ] verbs : [ \"create\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"configmaps\" , \"rolebindings\" , \"roles\" , \"resourcequotas\" , \"namespaces\" ] verbs : [ \"create\" , \"get\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"events\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"persistentvolumeclaims\" , \"pods\" , \"pods/exec\" , \"services\" , \"statefulsets\" , \"ingresses\" , \"extensions\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/log\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"secrets\" ] verbs : [ \"list\" ] - apiGroups : [ \"apps\" ] resources : [ \"statefulsets\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : ops-namespace roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : create-namespaces subjects : - kind : ServiceAccount name : jenkins namespace : team-ops","title":"Create Team Ops Namespace"},{"location":"cloudbees/teams-automation/#update-operation-center-serviceaccount","text":"The ServiceAccount under which Operation Center runs, only has rights in it's own namespace . Which means it cannot create our Team Ops Master. Below is the .yaml file for Kubernetes and the command to apply it. Warning I assume you're using the default cloudbees-core as per Cloudbees' documentation. If this is not the case, change the last line, namespace: cloudbees-core with the namespace your Operation Center runs in. kubectl apply -f patch-oc-serviceaccount.yaml -n team-ops patch-oc-serviceaccount.yaml This patches the existing Operation Center's ServiceAccount to also have the correct rights in the team-ops namespace. kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : master-management rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"pods/log\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] - apiGroups : [ \"apps\" ] resources : [ \"statefulsets\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"persistentvolumeclaims\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" , \"patch\" , \"update\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"secrets\" ] verbs : [ \"list\" ] - apiGroups : [ \"\" ] resources : [ \"events\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : RoleBinding metadata : name : cjoc roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : master-management subjects : - kind : ServiceAccount name : cjoc namespace : cloudbees-core","title":"Update Operation Center ServiceAccount"},{"location":"cloudbees/teams-automation/#jenkins-agent-configmap","text":"kubectl apply -f jenkins-agent-config-map.yaml -n team-ops jenkins-agent-config-map.yaml Creates the Jenkins Agent ConfigMap, which contains the information the Jenkins Agent - within a PodTemplate - uses to connect to the Jenkins Master. apiVersion : v1 kind : ConfigMap metadata : name : jenkins-agent data : jenkins-agent : | #!/usr/bin/env sh # The MIT License # # Copyright (c) 2015, CloudBees, Inc. # # Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files (the \"Software\"), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN # THE SOFTWARE. # Usage jenkins-slave.sh [options] -url http://jenkins [SECRET] [AGENT_NAME] # Optional environment variables : # * JENKINS_TUNNEL : HOST:PORT for a tunnel to route TCP traffic to jenkins host, when jenkins can't be directly accessed over network # * JENKINS_URL : alternate jenkins URL # * JENKINS_SECRET : agent secret, if not set as an argument # * JENKINS_AGENT_NAME : agent name, if not set as an argument if [ $# -eq 1 ]; then # if `docker run` only has one arguments, we assume user is running alternate command like `bash` to inspect the image exec \"$@\" else # if -tunnel is not provided try env vars case \"$@\" in *\"-tunnel \"*) ;; *) if [ ! -z \"$JENKINS_TUNNEL\" ]; then TUNNEL=\"-tunnel $JENKINS_TUNNEL\" fi ;; esac if [ -n \"$JENKINS_URL\" ]; then URL=\"-url $JENKINS_URL\" fi if [ -n \"$JENKINS_NAME\" ]; then JENKINS_AGENT_NAME=\"$JENKINS_NAME\" fi if [ -z \"$JNLP_PROTOCOL_OPTS\" ]; then echo \"Warning: JnlpProtocol3 is disabled by default, use JNLP_PROTOCOL_OPTS to alter the behavior\" JNLP_PROTOCOL_OPTS=\"-Dorg.jenkinsci.remoting.engine.JnlpProtocol3.disabled=true\" fi # If both required options are defined, do not pass the parameters OPT_JENKINS_SECRET=\"\" if [ -n \"$JENKINS_SECRET\" ]; then case \"$@\" in *\"${JENKINS_SECRET}\"*) echo \"Warning: SECRET is defined twice in command-line arguments and the environment variable\" ;; *) OPT_JENKINS_SECRET=\"${JENKINS_SECRET}\" ;; esac fi OPT_JENKINS_AGENT_NAME=\"\" if [ -n \"$JENKINS_AGENT_NAME\" ]; then case \"$@\" in *\"${JENKINS_AGENT_NAME}\"*) echo \"Warning: AGENT_NAME is defined twice in command-line arguments and the environment variable\" ;; *) OPT_JENKINS_AGENT_NAME=\"${JENKINS_AGENT_NAME}\" ;; esac fi SLAVE_JAR=/usr/share/jenkins/slave.jar if [ ! -f \"$SLAVE_JAR\" ]; then tmpfile=$(mktemp) if hash wget > /dev/null 2>&1; then wget -O \"$tmpfile\" \"$JENKINS_URL/jnlpJars/slave.jar\" elif hash curl > /dev/null 2>&1; then curl -o \"$tmpfile\" \"$JENKINS_URL/jnlpJars/slave.jar\" else echo \"Image does not include $SLAVE_JAR and could not find wget or curl to download it\" return 1 fi SLAVE_JAR=$tmpfile fi #TODO: Handle the case when the command-line and Environment variable contain different values. #It is fine it blows up for now since it should lead to an error anyway. exec java $JAVA_OPTS $JNLP_PROTOCOL_OPTS -cp $SLAVE_JAR hudson.remoting.jnlp.Main -headless $TUNNEL $URL $OPT_JENKINS_SECRET $OPT_JENKINS_AGENT_NAME \"$@\" fi","title":"Jenkins Agent ConfigMap"},{"location":"cloudbees/teams-automation/#create-initial-master","text":"To make it easier to change the namespace if needed, its extracted out from the command. OriginalNamespace = cloudbees-core This script changes the Operations Center's operating namespace , creates a Team Master with the name ops , and then resets the namespace. cboc groovy = < configure-oc-namespace.groovy team-ops cboc teams ops --put < team-ops.json cboc groovy = < configure-oc-namespace.groovy $OriginalNamespace team-ops.json This json file that describes a team. By default there are three roles defined on a team, TEAM_ADMIN , TEAM_MEMBER , and TEAM_GUEST . Don't forget to change the id 's to Group ID's from your Single-Sign-On solution. { \"version\" : \"1\" , \"data\" : { \"name\" : \"ops\" , \"displayName\" : \"Operations\" , \"provisioningRecipe\" : \"basic\" , \"members\" : [{ \"id\" : \"Catmins\" , \"roles\" : [ \"TEAM_ADMIN\" ] }, { \"id\" : \"Pirates\" , \"roles\" : [ \"TEAM_MEMBER\" ] }, { \"id\" : \"Continental\" , \"roles\" : [ \"TEAM_GUEST\" ] } ], \"icon\" : { \"name\" : \"hexagons\" , \"color\" : \"#8d7ec1\" } } } configure-oc-namespace.groovy This is a Jenkins Configuration or System Groovy script. It will change the namespace Operation Center uses to create resources. You can change this in the UI by going to Operations Center -> Manage Jenkins -> System Configuration -> Master Provisioning -> Namespace . import hudson.* import hudson.util.Secret ; import hudson.util.Scrambler ; import hudson.util.FormValidation ; import jenkins.* import jenkins.model.* import hudson.security.* import com.cloudbees.masterprovisioning.kubernetes.KubernetesMasterProvisioning import com.cloudbees.masterprovisioning.kubernetes.KubernetesClusterEndpoint println \"=== KubernetesMasterProvisioning Configuration - start\" println \"== Retrieving main configuration\" def descriptor = Jenkins . getInstance (). getInjector (). getInstance ( KubernetesMasterProvisioning . DescriptorImpl . class ) def namespace = this . args [ 0 ] def currentKubernetesClusterEndpoint = descriptor . getClusterEndpoints (). get ( 0 ) println \"= Found current endpoint\" println \"= \" + currentKubernetesClusterEndpoint . toString () def id = currentKubernetesClusterEndpoint . getId () def name = currentKubernetesClusterEndpoint . getName () def url = currentKubernetesClusterEndpoint . getUrl () def credentialsId = currentKubernetesClusterEndpoint . getCredentialsId () println \"== Setting Namspace to \" + namespace def updatedKubernetesClusterEndpoint = new KubernetesClusterEndpoint ( id , name , url , credentialsId , namespace ) def clusterEndpoints = new ArrayList < KubernetesClusterEndpoint >() clusterEndpoints . add ( updatedKubernetesClusterEndpoint ) descriptor . setClusterEndpoints ( clusterEndpoints ) println \"== Saving Jenkins configuration\" descriptor . save () println \"=== KubernetesMasterProvisioning Configuration - finish\"","title":"Create Initial Master"},{"location":"cloudbees/teams-automation/#configure-team-ops-master","text":"Now that we've created the Operations Team Master (Team Ops), we can configure it. The Pipelines we need will require credentials, we describe them below. githubtoken_token : GitHub API Token only, credentials type Secret Text (for the PR pipeline) githubtoken : GitHub username and API Token jenkins-api : Username and API Token for Operations Center. Just like the one we used for Client Jar. We also need to have a Global Pipeline Library defined by the name github.com/joostvdg/jpl-core . This, as the name suggests, should point to https://github.com/joostvdg/jpl-core.git .","title":"Configure Team Ops Master"},{"location":"cloudbees/teams-automation/#create-gitops-pipeline","text":"In total, we need two repositories and two or three Pipelines. You can either use my CLI Docker Image or roll your own. I will proceed as if you will create your own. CLI Image Pipeline : this will create a CLI Docker Image that is used to talk to Operations Center via the Client Jar (CLI) PR Pipeline : I like the idea of Self-Service, but in order to keep things in check, you might want to provide that via a PullRequest (PR) rather than a direct write to the Master branch. This is also Repository One, as I prefer having each pipeline in their own Repository, but you don't need to. Main Pipeline : will trigger on a commit to the Master branch and create the new team. I'll even throw in a free manage your Team Recipes for free as well.","title":"Create GitOps Pipeline"},{"location":"cloudbees/teams-automation/#create-cli-image-pipeline","text":"In a Kubernetes cluster, you should not build with Docker directly, use an in-cluster builder such as Kaniko or Buildah . You can read more about the why and how elsewhere on this site . Tip If you do not want to create your own, you can re-use my images. There should be one available for every recent version of CloudBees Core that will work with your Operations Center. The images are available in DockerHub at caladreas/cbcore-cli","title":"Create CLI Image Pipeline"},{"location":"cloudbees/teams-automation/#kaniko-configuration","text":"Kaniko uses a Docker Image to build your Docker Image in cluster. It does however need to directly communicate to your Docker Registry. This requires a Kubernetes Secret of type docker-registry . How you can do this and more, you can read on the CloudBees Core Docs .","title":"Kaniko Configuration"},{"location":"cloudbees/teams-automation/#pipeline","text":"Now that you have Kaniko configured, you can use this Pipeline to create your own CLI Images. Caution Make sure you replace the environment variables with values that make sense to you. CJOC_URL internal url in Kubernets, usually http://cjoc.<namespace>/cjoc REGISTRY : index.docker.io = DockerHub REPO : docker repository name IMAGE : docker image name Jenkinsfile Jenkins Declarative Pipeline for the CLI Image geberation. pipeline { agent { kubernetes { //cloud 'kubernetes' label 'test' yaml \"\"\" kind: Pod metadata: name: test spec: containers: - name: curl image: byrnedo/alpine-curl command: - cat tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: jenkins-docker-cfg projected: sources: - secret: name: docker-credentials items: - key: .dockerconfigjson path: .docker/config.json \"\"\" } } environment { CJOC_URL = 'http://cjoc.cloudbees-core/cjoc' CLI_VERSION = '' REGISTRY = 'index.docker.io' REPO = 'caladreas' IMAGE = 'cbcore-cli' } stages { stage ( 'Download CLI' ) { steps { container ( 'curl' ) { sh 'curl --version' sh 'echo ${CJOC_URL}/jnlpJars/jenkins-cli.jar' sh 'curl ${CJOC_URL}/jnlpJars/jenkins-cli.jar --output jenkins-cli.jar' sh 'ls -lath' } } } stage ( 'Prepare' ) { parallel { stage ( 'Verify CLI' ) { environment { CREDS = credentials ( 'jenkins-api' ) CLI = \"java -jar jenkins-cli.jar -noKeyAuth -s ${CJOC_URL} -auth\" } steps { sh 'echo ${CLI}' script { CLI_VERSION = sh returnStdout: true , script: '${CLI} ${CREDS} version' } sh 'echo ${CLI_VERSION}' } } stage ( 'Prepare Dockerfile' ) { steps { writeFile encoding: 'UTF-8' , file: 'Dockerfile' , text: \"\"\"FROM mcr.microsoft.com/java/jre-headless:8u192-zulu-alpine WORKDIR /usr/bin ADD jenkins-cli.jar . RUN pwd RUN ls -lath \"\"\" } } } } stage ( 'Build with Kaniko' ) { environment { PATH = \"/busybox:/kaniko:$PATH\" TAG = \"${CLI_VERSION}\" } steps { sh 'echo image fqn=${REGISTRY}/${REPO}/${IMAGE}:${TAG}' container ( name: 'kaniko' , shell: '/busybox/sh' ) { sh '''#!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:${TAG} /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:latest ''' } } } } }","title":"Pipeline"},{"location":"cloudbees/teams-automation/#pr-pipeline","text":"Caution The PR Pipeline example builds upon the GitHub API, if you're not using GItHub, you will have to figure out another way to make the PR.","title":"PR Pipeline"},{"location":"cloudbees/teams-automation/#tools-used","text":"yq : commandline tool for processing Yaml files jq commandline tool for pressing Json files Kustomize templating tool for Kubernetes Yaml, as of Kubernetes 1.13 , this is part of the Client (note, your server can be older, don't worry!) Hub commandline client for GitHub","title":"Tools Used"},{"location":"cloudbees/teams-automation/#repository-layout","text":"folder: team-master-template with file simple.json folder: namespace-creation with folder: kustomize this contains the Kustomize configuration Simple.json This is a template for the team JSON definition. { \"version\" : \"1\" , \"data\" : { \"name\" : \"NAME\" , \"displayName\" : \"DISPLAY_NAME\" , \"provisioningRecipe\" : \"RECIPE\" , \"members\" : [ { \"id\" : \"ADMINS\" , \"roles\" : [ \"TEAM_ADMIN\" ] }, { \"id\" : \"MEMBERS\" , \"roles\" : [ \"TEAM_MEMBER\" ] }, { \"id\" : \"GUESTS\" , \"roles\" : [ \"TEAM_GUEST\" ] } ], \"icon\" : { \"name\" : \"ICON\" , \"color\" : \"HEX_COLOR\" } } }","title":"Repository Layout"},{"location":"cloudbees/teams-automation/#kustomize-configuration","text":"Kustomize is a tool for template Kubernetes YAML definitions, which is what we need here. However, only for the namespace creation & configuration. So if you don't want to do that, you can skip this. The Kustomize configuration has two parts, a folder called team-example with a kustomization.yaml . This will be what we configure to generate a new yaml definition. The main template is in the folder base , where the entrypoint will be again kustomization.yaml . This time, the kustomization.yaml will link to all the template files we need. As posting all these yaml files again is a bit much, I'll link to my example repo. Feel free to fork it instead: cb-team-gitops-template configmap.yaml : the Jenkins Agent ConfigMap namespace.yaml : the new namespace resource-quota.yaml : resource quota's for the namespace role-binding-cjoc.yaml : a role binding for the CJOC ServiceAccount, so it create create the new Master in the new namespace role-binding.yaml : the role binding for the jenkins ServiceAccount, which allows the new Master to create and manage Pods (for PodTemplates) role-cjoc.yaml : the role for CJOC for the ability to create a Master in the new Namspace role.yaml : the role for the jenkins ServiceAccount for the new Master service-account.yaml : the ServiceAccount, jenkins , used by the new Master","title":"Kustomize Configuration"},{"location":"cloudbees/teams-automation/#pipeline_1","text":"The Pipeline will do the following: capture input parameters to be used to customize the Team Master update the Kustomize template to make sure every resource is correct for the new namespace ( teams-<name of team> ) execute Kustomize to generate a single yaml file that defines the configuration for the new Team Masters' namespace process the simple.json to generate a team.json file for the new Team Master for use with the Jenkins CLI checkout your GIT_REPO that contains your team definitions create a new PR to your GIT_REPO for the new team Jenkinsfile Variables to update: GIT_REPO : the GitHub repository in which the Team Definitions are stored RESET_NAMESPACE : the namespace Operations Center should use as default pipeline { agent { kubernetes { label 'team-automation' yaml \"\"\" kind: Pod spec: containers: - name: hub image: caladreas/hub command: [\"cat\"] tty: true resources: requests: memory: \"50Mi\" cpu: \"150m\" limits: memory: \"50Mi\" cpu: \"150m\" - name: kubectl image: bitnami/kubectl:latest command: [\"cat\"] tty: true securityContext: runAsUser: 1000 fsGroup: 1000 resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"150Mi\" cpu: \"200m\" - name: yq image: mikefarah/yq command: ['cat'] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" - name: jq image: colstrom/jq command: ['cat'] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" \"\"\" } } libraries { lib ( 'github.com/joostvdg/jpl-core' ) } options { disableConcurrentBuilds () // we don't want more than one at a time checkoutToSubdirectory 'templates' // we need to do two checkouts buildDiscarder logRotator ( artifactDaysToKeepStr: '' , artifactNumToKeepStr: '' , daysToKeepStr: '5' , numToKeepStr: '5' ) // always clean up } environment { envGitInfo = '' RESET_NAMESPACE = 'jx-production' TEAM_BASE_NAME = '' NAMESPACE_TO_CREATE = '' DISPLAY_NAME = '' TEAM_RECIPE = '' ICON = '' ICON_COLOR_CODE = '' ADMINS_ROLE = '' MEMBERS_ROLE = '' GUESTS_ROLE = '' RECORD_LOC = '' GIT_REPO = '' } stages { stage ( 'Team Details' ) { input { message \"Please enter the team details.\" ok \"Looks good, proceed\" parameters { string ( name: 'Name' , defaultValue: 'hex' , description: 'Please specify a team name' ) string ( name: 'DisplayName' , defaultValue: 'Hex' , description: 'Please specify a team display name' ) choice choices: [ 'joostvdg' , 'basic' , 'java-web' ], description: 'Please select a Team Recipe' , name: 'TeamRecipe' choice choices: [ 'anchor' , 'bear' , 'bowler-hat' , 'briefcase' , 'bug' , 'calculator' , 'calculatorcart' , 'clock' , 'cloud' , 'cloudbees' , 'connect' , 'dollar-bill' , 'dollar-symbol' , 'file' , 'flag' , 'flower-carnation' , 'flower-daisy' , 'help' , 'hexagon' , 'high-heels' , 'jenkins' , 'key' , 'marker' , 'monocle' , 'mustache' , 'office' , 'panther' , 'paw-print' , 'teacup' , 'tiger' , 'truck' ], description: 'Please select an Icon' , name: 'Icon' string ( name: 'IconColorCode' , defaultValue: '#CCCCCC' , description: 'Please specify a valid html hexcode for the color (https://htmlcolorcodes.com/)' ) string ( name: 'Admins' , defaultValue: 'Catmins' , description: 'Please specify a groupid or userid for the TEAM_ADMIN role' ) string ( name: 'Members' , defaultValue: 'Pirates' , description: 'Please specify a groupid or userid for the TEAM_MEMBER role' ) string ( name: 'Guests' , defaultValue: 'Continental' , description: 'Please specify a groupid or userid for the TEAM_GUEST role' ) } } steps { println \"Name=${Name}\" println \"DisplayName=${DisplayName}\" println \"TeamRecipe=${TeamRecipe}\" println \"Icon=${Icon}\" println \"IconColorCode=${IconColorCode}\" println \"Admins=${Admins}\" println \"Members=${Members}\" println \"Guests=${Guests}\" script { TEAM_BASE_NAME = \"${Name}\" NAMESPACE_TO_CREATE = \"cb-teams-${Name}\" DISPLAY_NAME = \"${DisplayName}\" TEAM_RECIPE = \"${TeamRecipe}\" ICON = \"${Icon}\" ICON_COLOR_CODE = \"${IconColorCode}\" ADMINS_ROLE = \"${Admins}\" MEMBERS_ROLE = \"${Members}\" GUESTS_ROLE = \"${Guests}\" RECORD_LOC = \"templates/teams/${Name}\" sh \"mkdir -p ${RECORD_LOC}\" } } } stage ( 'Create Team Config' ) { environment { BASE = 'templates/namespace-creation/kustomize' NAMESPACE = \"${NAMESPACE_TO_CREATE}\" RECORD_LOC = \"templates/teams/${TEAM_BASE_NAME}\" } parallel { stage ( 'Namespace' ) { steps { container ( 'yq' ) { sh 'yq w -i ${BASE}/base/role-binding.yaml subjects[0].namespace ${NAMESPACE}' sh 'yq w -i ${BASE}/base/namespace.yaml metadata.name ${NAMESPACE}' sh 'yq w -i ${BASE}/team-example/kustomization.yaml namespace ${NAMESPACE}' } container ( 'kubectl' ) { sh ''' kubectl kustomize ${BASE}/team-example > ${RECORD_LOC}/team.yaml cat ${RECORD_LOC}/team.yaml ''' } } } stage ( 'Team Master JSON' ) { steps { container ( 'jq' ) { sh \"\"\"jq \\ '.data.name = \"${TEAM_BASE_NAME}\" |\\ .data.displayName = \"${DISPLAY_NAME}\" |\\ .data.provisioningRecipe = \"${TEAM_RECIPE}\" |\\ .data.icon.name = \"${ICON}\" |\\ .data.icon.color = \"${ICON_COLOR_CODE}\" |\\ .data.members[0].id = \"${ADMINS_ROLE}\" |\\ .data.members[1].id = \"${MEMBERS_ROLE}\" |\\ .data.members[2].id = \"${GUESTS_ROLE}\"'\\ templates/team-master-template/simple.json > ${RECORD_LOC}/team.json \"\"\" } sh 'cat ${RECORD_LOC}/team.json' } } } } stage ( 'Create PR' ) { when { branch 'master' } environment { RECORD_OLD_LOC = \"templates/teams/${TEAM_BASE_NAME}\" RECORD_LOC = \"teams/${TEAM_BASE_NAME}\" PR_CHANGE_NAME = \"add_team_${TEAM_BASE_NAME}\" } steps { container ( 'hub' ) { dir ( 'cb-team-gitops' ) { script { envGitInfo = git \"${GIT_REPO}\" } sh 'git checkout -b ${PR_CHANGE_NAME}' sh 'ls -lath ../${RECORD_OLD_LOC}' sh 'cp -R ../${RECORD_OLD_LOC} ./teams' sh 'ls -lath' sh 'ls -lath teams/' gitRemoteConfigByUrl ( envGitInfo . GIT_URL , 'githubtoken_token' ) // must be a API Token ONLY -> secret text sh ''' git config --global user.email \"jenkins@jenkins.io\" git config --global user.name \"Jenkins\" git add ${RECORD_LOC} git status git commit -m \"add team ${TEAM_BASE_NAME}\" git push origin ${PR_CHANGE_NAME} ''' // has to be indented like that, else the indents will be in the pr description writeFile encoding: 'UTF-8' , file: 'pr-info.md' , text: \"\"\"Add ${TEAM_BASE_NAME} \\n This pr is automatically generated via CloudBees.\\\\n \\n The job: ${env.JOB_URL} \"\"\" // TODO: unfortunately, environment {}'s credentials have fixed environment variable names // TODO: in this case, they need to be EXACTLY GITHUB_PASSWORD and GITHUB_USER script { withCredentials ([ usernamePassword ( credentialsId: 'githubtoken' , passwordVariable: 'GITHUB_PASSWORD' , usernameVariable: 'GITHUB_USER' )]) { sh \"\"\" set +x hub pull-request --force -F pr-info.md -l '${TEAM_BASE_NAME}' --no-edit \"\"\" } } } } } } } }","title":"Pipeline"},{"location":"cloudbees/teams-automation/#main-pipeline","text":"The main Pipeline should be part of a repository. The Repository should look like this: recipes (folder) recipes.json -> current complete list of CloudBees Core Team Recipes definition teams (folder) folder per team team.json -> CloudBees Core Team definition team.yaml -> Kubernetes YAML definition of the namespace and all its resources","title":"Main Pipeline"},{"location":"cloudbees/teams-automation/#process","text":"The pipeline can be a bit hard to grasp, so let me break it down into individual steps. We have the following stages: Create Team - which is broken into sub-stages via the sequential stages feature . * Parse Changelog * Create Namespace * Change OC Namespace * Create Team Master Test CLI Connection Update Team Recipes","title":"Process"},{"location":"cloudbees/teams-automation/#notable-statements","text":"disableConcurrentBuilds We change the namespace of Operation Center to a different value only for the duration of creating this master. This is something that should probably be part of the Team Master creation, but as it is a single configuration option for all that Operation Center does, we need to be careful. By ensuring we only run one build concurrently, we reduce the risk of this blowing up in our face. options { disableConcurrentBuilds () } when { } The When Directive allows us to creating effective conditions for when a stage should be executed. The snippet below shows the use of a combination of both the branch and changeset built-in filters. changeset looks at the commit being build and validates that there was a change in that file path. when { allOf { branch 'master' ; changeset \"teams/**/team.*\" } } post { always { } } The Post Directive allows us to run certain commands after the main pipeline has run depending on the outcome (compared or not to the previous outcome). In this case, we want to make sure we reset the namespace used by Operations Center to the original value. By using post { always {} } , it will ALWAYS run, regardless of the status of the pipeline. So we should be safe. post { always { container ( 'cli' ) { sh '${CLI} ${CREDS} groovy = < resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE}' } } } stages { stage { parallel { stage() { stages { stage { Oke, you might've noticed this massive indenting depth and probably have some questions. By combining sequential stages with parallel stages we can create a set of stages that will be executed in sequence but can be controlled by a single when {} statement whether or not they get executed. This prevents mistakes being made in the condition and accidentally running one or other but not all the required steps. stages { stage ( 'Create Team' ) { parallel { stage ( 'Main' ) { stages { stage ( 'Parse Changelog' ) { changetSetData & container('jpb') {} Alright, so even if we know a team was added in /teams/<team-name> , we still don't know the following two things: 1) what is the name of this team, 2) was this team changed or deleted? So we have to process the changelog to be able to answer these questions as well. There are different ways of getting the changelog and parsing it. I've written one you can do on ANY machine, regardless of Jenkins by leveraging Git and my own custom binary ( jpb -> Jenkins Pipeline Binary). The code for my binary is at GitHub: github.com/joostvdg/jpb . An alternative approach is described by CloudBees Support here , which leverages Jenkins groovy powers. COMMIT_INFO = \"${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT}\" def changeSetData = sh returnStdout: true , script: \"git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO}\" changeSetData = changeSetData . replace ( \"\\n\" , \"\\\\n\" ) container ( 'jpb' ) { changeSetFolders = sh returnStdout: true , script: \"/usr/bin/jpb/bin/jpb GitChangeListToFolder '${changeSetData}' 'teams/'\" changeSetFolders = changeSetFolders . split ( ',' ) }","title":"Notable Statements"},{"location":"cloudbees/teams-automation/#files","text":"recipes.json The default Team Recipes that ships with CloudBees Core Modern. { \"version\" : \"1\" , \"data\" : [{ \"name\" : \"basic\" , \"displayName\" : \"Basic\" , \"description\" : \"The minimalistic setup.\" , \"plugins\" : [ \"bluesteel-master\" , \"cloudbees-folders-plus\" , \"cloudbees-jsync-archiver\" , \"cloudbees-monitoring\" , \"cloudbees-nodes-plus\" , \"cloudbees-ssh-slaves\" , \"cloudbees-support\" , \"cloudbees-workflow-template\" , \"credentials-binding\" , \"email-ext\" , \"git\" , \"git-client\" , \"github-branch-source\" , \"github-organization-folder\" , \"infradna-backup\" , \"ldap\" , \"mailer\" , \"operations-center-analytics-reporter\" , \"operations-center-cloud\" , \"pipeline-model-definition\" , \"ssh-credentials\" , \"wikitext\" , \"workflow-aggregator\" , \"workflow-cps-checkpoint\" ], \"default\" : true }, { \"name\" : \"java-web\" , \"displayName\" : \"Java & Web Development\" , \"description\" : \"The essential tools to build, release and deploy Java Web applications including integration with Maven, Gradle and Node JS.\" , \"plugins\" : [ \"bluesteel-master\" , \"cloudbees-folders-plus\" , \"cloudbees-jsync-archiver\" , \"cloudbees-monitoring\" , \"cloudbees-nodes-plus\" , \"cloudbees-ssh-slaves\" , \"cloudbees-support\" , \"cloudbees-workflow-template\" , \"credentials-binding\" , \"email-ext\" , \"git\" , \"git-client\" , \"github-branch-source\" , \"github-organization-folder\" , \"infradna-backup\" , \"ldap\" , \"mailer\" , \"operations-center-analytics-reporter\" , \"operations-center-cloud\" , \"pipeline-model-definition\" , \"ssh-credentials\" , \"wikitext\" , \"workflow-aggregator\" , \"workflow-cps-checkpoint\" , \"config-file-provider\" , \"cloudbees-aws-cli\" , \"cloudbees-cloudfoundry-cli\" , \"findbugs\" , \"gradle\" , \"jira\" , \"junit\" , \"nodejs\" , \"openshift-cli\" , \"pipeline-maven\" , \"tasks\" , \"warnings\" ], \"default\" : false }] } Jenkinsfile This is the pipeline that will process the commit to the repository and, if it detects a new team is created will apply the changes. Variables to overwrite: GIT_REPO : the https url to the Git Repository your GitOps code/configuration is stored RESET_NAMESPACE : the namespace your Operation Center normally operates in CLI : this command depends on the namespace Operation Center is in ( http://<service name>.<namespace>/cjoc ) pipeline { agent { kubernetes { label 'jenkins-agent' yaml ''' apiVersion: v1 kind: Pod spec: serviceAccountName: jenkins containers: - name: cli image: caladreas/cbcore-cli:2.176.2.3 imagePullPolicy: Always command: - cat tty: true resources: requests: memory: \"50Mi\" cpu: \"150m\" limits: memory: \"50Mi\" cpu: \"150m\" - name: kubectl image: bitnami/kubectl:latest command: [\"cat\"] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"150Mi\" cpu: \"200m\" - name: yq image: mikefarah/yq command: ['cat'] tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" - name: jpb image: caladreas/jpb command: - cat tty: true resources: requests: memory: \"50Mi\" cpu: \"100m\" limits: memory: \"50Mi\" cpu: \"100m\" securityContext: runAsUser: 1000 fsGroup: 1000 ''' } } options { disableConcurrentBuilds () buildDiscarder logRotator ( artifactDaysToKeepStr: '' , artifactNumToKeepStr: '' , daysToKeepStr: '5' , numToKeepStr: '5' ) } environment { RESET_NAMESPACE = 'cloudbees-core' CREDS = credentials ( 'jenkins-api' ) CLI = \"java -jar /usr/bin/jenkins-cli.jar -noKeyAuth -s http://cjoc.cloudbees-core/cjoc -auth\" COMMIT_INFO = '' TEAM = '' GIT_REPO = '' } stages { stage ( 'Create Team' ) { when { allOf { branch 'master' ; changeset \"teams/**/team.*\" } } parallel { stage ( 'Main' ) { stages { stage ( 'Parse Changelog' ) { steps { // Alternative approach: https://support.cloudbees.com/hc/en-us/articles/217630098-How-to-access-Changelogs-in-a-Pipeline-Job- // However, that runs on the master, JPB runs in an agent! script { scmVars = git \"${GIT_REPO}\" COMMIT_INFO = \"${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT}\" def changeSetData = sh returnStdout: true , script: \"git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO}\" changeSetData = changeSetData . replace ( \"\\n\" , \"\\\\n\" ) container ( 'jpb' ) { changeSetFolders = sh returnStdout: true , script: \"/usr/bin/jpb/bin/jpb GitChangeListToFolder '${changeSetData}' 'teams/'\" changeSetFolders = changeSetFolders . split ( ',' ) } if ( changeSetFolders . length > 0 ) { TEAM = changeSetFolders [ 0 ] TEAM = TEAM . trim () // to protect against a team being removed def exists = fileExists \"teams/${TEAM}/team.yaml\" if (! exists ) { TEAM = '' } } else { TEAM = '' } echo \"Team that changed: |${TEAM}|\" } } } stage ( 'Create Namespace' ) { when { expression { return ! TEAM . equals ( '' ) } } environment { NAMESPACE = \"cb-teams-${TEAM}\" RECORD_LOC = \"teams/${TEAM}\" } steps { container ( 'kubectl' ) { sh ''' cat ${RECORD_LOC}/team.yaml kubectl apply -f ${RECORD_LOC}/team.yaml ''' } } } stage ( 'Change OC Namespace' ) { when { expression { return ! TEAM . equals ( '' ) } } environment { NAMESPACE = \"cb-teams-${TEAM}\" } steps { container ( 'cli' ) { sh 'echo ${NAMESPACE}' script { def response = sh encoding: 'UTF-8' , label: 'create team' , returnStatus: true , script: '${CLI} ${CREDS} groovy = < resources/bootstrap/configure-oc-namespace.groovy ${NAMESPACE}' println \"Response: ${response}\" } } } } stage ( 'Create Team Master' ) { when { expression { return ! TEAM . equals ( '' ) } } environment { TEAM_NAME = \"${TEAM}\" } steps { container ( 'cli' ) { println \"TEAM_NAME=${TEAM_NAME}\" sh 'ls -lath' sh 'ls -lath teams/' script { def response = sh encoding: 'UTF-8' , label: 'create team' , returnStatus: true , script: '${CLI} ${CREDS} teams ${TEAM_NAME} --put < \"teams/${TEAM_NAME}/team.json\"' println \"Response: ${response}\" } } } } } } } } stage ( 'Test CLI Connection' ) { steps { container ( 'cli' ) { script { def response = sh encoding: 'UTF-8' , label: 'retrieve version' , returnStatus: true , script: '${CLI} ${CREDS} version' println \"Response: ${response}\" } } } } stage ( 'Update Team Recipes' ) { when { allOf { branch 'master' ; changeset \"recipes/recipes.json\" } } steps { container ( 'cli' ) { sh 'ls -lath' sh 'ls -lath recipes/' script { def response = sh encoding: 'UTF-8' , label: 'update team recipe' , returnStatus: true , script: '${CLI} ${CREDS} team-creation-recipes --put < \"recipes/recipes.json\"' println \"Response: ${response}\" } } } } } post { always { container ( 'cli' ) { sh '${CLI} ${CREDS} groovy = < resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE}' } } } }","title":"Files"},{"location":"devops/","text":"DevOps Assessment \u00b6 How can I assess an organisation for what to do next. Questions \u00b6 To which extent can your development teams request/create an environment on their own, without going through lengthy approval processes? To which extent can your development teams use pre-configured/ template tool sets (e.g. Jenkins master jobs, master POM etc) which they can extend and/or modify to their needs? To which extent can your developments teams deploy to any environment (including production)? If not, what do they lack: knowledge or passwords to higher environments? Does your system of record provide you tractability from idea to production? How tightly coupled are your key delivery pipeline tools? Is it easy to replace them? Do you have different release management activities based on application blocks? Who is keeping it up-to-date? Maturity Model \u00b6 Resources \u00b6 https://www.devon.nl/continuous-delivery-at-enterprise-level-pitfall-1/ https://www.devon.nl/continuous-delivery-at-enterprise-level-pitfall-2/ https://devops-research.com/ References \u00b6","title":"DevOps Assessment"},{"location":"devops/#devops-assessment","text":"How can I assess an organisation for what to do next.","title":"DevOps Assessment"},{"location":"devops/#questions","text":"To which extent can your development teams request/create an environment on their own, without going through lengthy approval processes? To which extent can your development teams use pre-configured/ template tool sets (e.g. Jenkins master jobs, master POM etc) which they can extend and/or modify to their needs? To which extent can your developments teams deploy to any environment (including production)? If not, what do they lack: knowledge or passwords to higher environments? Does your system of record provide you tractability from idea to production? How tightly coupled are your key delivery pipeline tools? Is it easy to replace them? Do you have different release management activities based on application blocks? Who is keeping it up-to-date?","title":"Questions"},{"location":"devops/#maturity-model","text":"","title":"Maturity Model"},{"location":"devops/#resources","text":"https://www.devon.nl/continuous-delivery-at-enterprise-level-pitfall-1/ https://www.devon.nl/continuous-delivery-at-enterprise-level-pitfall-2/ https://devops-research.com/","title":"Resources"},{"location":"devops/#references","text":"","title":"References"},{"location":"devops/pipeline-model/","text":"Pipeline Modal \u00b6 Ideas \u00b6 Continuous Integration Continuous Delivery Continuous Deployment Progressive Delivery Continuous, Iterative, Cyclic Small cycle: local dev, CI goal: get it to work Large cycle: CD - CDP - ProgDY goal: deliver value to customers DevOps -> single team managing both Small and Large cycle Generic/Shared Concepts \u00b6 Jenkins \u00b6 Tekton \u00b6 Jenkins X \u00b6 Flow \u00b6 Concepts \u00b6 Pipeline Stage Task Pipeline Pipeline Run Entry Gate Exit Gate Other Server Application Resource Release Environment Application Application Deployment Microservice (seems tied to containers?) Microservice Deployment Production Bill-of-Materials Payload Process Approval Version Manifest Components Self-Service Catalog","title":"Pipeline Modal"},{"location":"devops/pipeline-model/#pipeline-modal","text":"","title":"Pipeline Modal"},{"location":"devops/pipeline-model/#ideas","text":"Continuous Integration Continuous Delivery Continuous Deployment Progressive Delivery Continuous, Iterative, Cyclic Small cycle: local dev, CI goal: get it to work Large cycle: CD - CDP - ProgDY goal: deliver value to customers DevOps -> single team managing both Small and Large cycle","title":"Ideas"},{"location":"devops/pipeline-model/#genericshared-concepts","text":"","title":"Generic/Shared Concepts"},{"location":"devops/pipeline-model/#jenkins","text":"","title":"Jenkins"},{"location":"devops/pipeline-model/#tekton","text":"","title":"Tekton"},{"location":"devops/pipeline-model/#jenkins-x","text":"","title":"Jenkins X"},{"location":"devops/pipeline-model/#flow","text":"","title":"Flow"},{"location":"devops/pipeline-model/#concepts","text":"Pipeline Stage Task Pipeline Pipeline Run Entry Gate Exit Gate Other Server Application Resource Release Environment Application Application Deployment Microservice (seems tied to containers?) Microservice Deployment Production Bill-of-Materials Payload Process Approval Version Manifest Components Self-Service Catalog","title":"Concepts"},{"location":"devops/progressive-delivery/","text":"Progressive Delivery \u00b6 Resources \u00b6 https://redmonk.com/jgovernor/2018/08/06/towards-progressive-delivery/ https://chrisshort.tumblr.com/post/176701070950/recommended-read-towards-progressive-delivery https://dzone.com/articles/gitops-workflows-for-istio-canary-deployments https://launchdarkly.com/blog/progressive-delivery-a-history-condensed/ https://tech.target.com/infrastructure/2018/06/20/enter-unimatrix.html https://dzone.com/articles/deployments-at-scale-using-kubernetes-and-launchda https://blog.csanchez.org/2019/01/22/progressive-delivery-in-kubernetes-blue-green-and-canary-deployments/ https://blog.csanchez.org/2019/01/24/progressive-delivery-with-jenkins-x/ https://blog.csanchez.org/2019/03/05/progressive-delivery-with-jenkins-x-automatic-canary-deployments/ https://devblogs.microsoft.com/devops/configuring-your-release-pipelines-for-safe-deployments/ https://www.linkedin.com/pulse/counting-down-zero-time-takes-launch-app-target-tom-kadlec-1/","title":"Progressive Delivery"},{"location":"devops/progressive-delivery/#progressive-delivery","text":"","title":"Progressive Delivery"},{"location":"devops/progressive-delivery/#resources","text":"https://redmonk.com/jgovernor/2018/08/06/towards-progressive-delivery/ https://chrisshort.tumblr.com/post/176701070950/recommended-read-towards-progressive-delivery https://dzone.com/articles/gitops-workflows-for-istio-canary-deployments https://launchdarkly.com/blog/progressive-delivery-a-history-condensed/ https://tech.target.com/infrastructure/2018/06/20/enter-unimatrix.html https://dzone.com/articles/deployments-at-scale-using-kubernetes-and-launchda https://blog.csanchez.org/2019/01/22/progressive-delivery-in-kubernetes-blue-green-and-canary-deployments/ https://blog.csanchez.org/2019/01/24/progressive-delivery-with-jenkins-x/ https://blog.csanchez.org/2019/03/05/progressive-delivery-with-jenkins-x-automatic-canary-deployments/ https://devblogs.microsoft.com/devops/configuring-your-release-pipelines-for-safe-deployments/ https://www.linkedin.com/pulse/counting-down-zero-time-takes-launch-app-target-tom-kadlec-1/","title":"Resources"},{"location":"devops/sdm/","text":"Software Development Management \u00b6 Related Concepts \u00b6 Agile scrum safe kanban xp DevOps Lean Systems Thinking Observability SRE EmpathyOps DevSecOps InfoSec Pets vs. Cattle Value Streams Automation Self-service 2-Pizza Teams Spotify Model Feature Flags Development Workflows Trunk Based Development TDD / BDD DDD Testing Automated Exploratory Functional Penetration Static/Dynamic Code Analysis Policies Unit Blacbox ... Development Paradigms Object Oriented Functional Procedural Imperative Declarative Wardley Maps Undifferentiated Heavy Lifting Layers Of Abstraction Models Three-Tier-Model MVC Enterprise Architecture Patterns Evolutionary Architecture Synchronous Asynchronous Or Reactive Streaming CQRS Event Sourcing Premature Optimization Theory of Constraints Segregation Of Duties Feedback Cycles Center Of Excellence Developer Productivity Teams Shared Services Product vs Project Design Thinking Systems Thinking 1-2-Many Evaluation Techniques Blameless Postmortem Retrospective The Five Why's Root Cause Analysis Behavioral Economics Anatomy Of A Toolchain \u00b6 Essential Processes \u00b6 versioning dependency management application tracking release deployment testing point of testing level of testing goal of testing publish Adoption \u00b6 Top Buys Bottom Adopts Workflow Cycles \u00b6 The Small Cycle \u00b6 design build test refactor document publish evaluate Resources \u00b6 https://info.container-solutions.com/info.container-solutions.com/understanding-cloud-native-cs-google-events-thankyou-1 https://unixism.net/2019/08/a-managers-guide-to-kubernetes-adoption/?utm_source=DevOps%27ish&utm_campaign=8bd4b37eb5-DEVOPSISH_145&utm_medium=email&utm_term=0_eab566bc9f-8bd4b37eb5-46458919 https://www.allthingsdistributed.com/2019/08/modern-applications-at-aws.html https://blog.gitprime.com/individual-contributor-to-manager-julie-zhuo/ https://martinfowler.com/articles/cd4ml.html https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started","title":"Software Development Management"},{"location":"devops/sdm/#software-development-management","text":"","title":"Software Development Management"},{"location":"devops/sdm/#related-concepts","text":"Agile scrum safe kanban xp DevOps Lean Systems Thinking Observability SRE EmpathyOps DevSecOps InfoSec Pets vs. Cattle Value Streams Automation Self-service 2-Pizza Teams Spotify Model Feature Flags Development Workflows Trunk Based Development TDD / BDD DDD Testing Automated Exploratory Functional Penetration Static/Dynamic Code Analysis Policies Unit Blacbox ... Development Paradigms Object Oriented Functional Procedural Imperative Declarative Wardley Maps Undifferentiated Heavy Lifting Layers Of Abstraction Models Three-Tier-Model MVC Enterprise Architecture Patterns Evolutionary Architecture Synchronous Asynchronous Or Reactive Streaming CQRS Event Sourcing Premature Optimization Theory of Constraints Segregation Of Duties Feedback Cycles Center Of Excellence Developer Productivity Teams Shared Services Product vs Project Design Thinking Systems Thinking 1-2-Many Evaluation Techniques Blameless Postmortem Retrospective The Five Why's Root Cause Analysis Behavioral Economics","title":"Related Concepts"},{"location":"devops/sdm/#anatomy-of-a-toolchain","text":"","title":"Anatomy Of A Toolchain"},{"location":"devops/sdm/#essential-processes","text":"versioning dependency management application tracking release deployment testing point of testing level of testing goal of testing publish","title":"Essential Processes"},{"location":"devops/sdm/#adoption","text":"Top Buys Bottom Adopts","title":"Adoption"},{"location":"devops/sdm/#workflow-cycles","text":"","title":"Workflow Cycles"},{"location":"devops/sdm/#the-small-cycle","text":"design build test refactor document publish evaluate","title":"The Small Cycle"},{"location":"devops/sdm/#resources","text":"https://info.container-solutions.com/info.container-solutions.com/understanding-cloud-native-cs-google-events-thankyou-1 https://unixism.net/2019/08/a-managers-guide-to-kubernetes-adoption/?utm_source=DevOps%27ish&utm_campaign=8bd4b37eb5-DEVOPSISH_145&utm_medium=email&utm_term=0_eab566bc9f-8bd4b37eb5-46458919 https://www.allthingsdistributed.com/2019/08/modern-applications-at-aws.html https://blog.gitprime.com/individual-contributor-to-manager-julie-zhuo/ https://martinfowler.com/articles/cd4ml.html https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started","title":"Resources"},{"location":"docker/build-kit/","text":"Docker Build with Build-Kit \u00b6 Instead of investing in improving docker image building via the Docker Client, Docker created a new API and client library. This library called BuildKit, is completely independent. With Docker 18.09, it is included in the Docker Client allowing anyone to use it as easily as the traditional docker image build . BuildKit is already used by some other tools, such as Buildah and IMG, and allows you to create custom DSL \"Frontends\". As long as the API of BuikdKit is adhered to, the resulting image will be OCI compliant. How To Use It \u00b6 So further remarks below and how to use it. BuildKit In-Depth session Supercharged Docker Build with BuildKit Usable from Docker 18.09 HighLights: allows custom DSL for specifying image (BuildKit) to still be used with Docker client/daemon build cache for your own files during build, think Go, Maven, Gradle... much more optimized, builds less, quicker, with more cache in less time support mounts (cache) such as secrets, during build phase # Set env variable to enable # Or configure docker's json config export DOCKER_BUILDKIT = 1 Example \u00b6 ```dockerfile syntax=docker/dockerfile:experimental \u00b6 \u00b6 1. BUILD JAR WITH MAVEN \u00b6 FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src ! RUN --mount=type=cache,target=/root/.m2/ mvn clean package -e \u00b6 2. BUILD NATIVE IMAGE WITH GRAAL \u00b6 FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from=BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath \u00b6 3. BUILD DOCKER RUNTIME IMAGE \u00b6 FROM alpine:3.8 CMD [\"jpc-graal\"] COPY --from=NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal \u00b6","title":"BuildKit"},{"location":"docker/build-kit/#docker-build-with-build-kit","text":"Instead of investing in improving docker image building via the Docker Client, Docker created a new API and client library. This library called BuildKit, is completely independent. With Docker 18.09, it is included in the Docker Client allowing anyone to use it as easily as the traditional docker image build . BuildKit is already used by some other tools, such as Buildah and IMG, and allows you to create custom DSL \"Frontends\". As long as the API of BuikdKit is adhered to, the resulting image will be OCI compliant.","title":"Docker Build with Build-Kit"},{"location":"docker/build-kit/#how-to-use-it","text":"So further remarks below and how to use it. BuildKit In-Depth session Supercharged Docker Build with BuildKit Usable from Docker 18.09 HighLights: allows custom DSL for specifying image (BuildKit) to still be used with Docker client/daemon build cache for your own files during build, think Go, Maven, Gradle... much more optimized, builds less, quicker, with more cache in less time support mounts (cache) such as secrets, during build phase # Set env variable to enable # Or configure docker's json config export DOCKER_BUILDKIT = 1","title":"How To Use It"},{"location":"docker/build-kit/#example","text":"```dockerfile","title":"Example"},{"location":"docker/build-kit/#syntaxdockerdockerfileexperimental","text":"","title":"syntax=docker/dockerfile:experimental"},{"location":"docker/build-kit/#_1","text":"","title":""},{"location":"docker/build-kit/#1-build-jar-with-maven","text":"FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src ! RUN --mount=type=cache,target=/root/.m2/ mvn clean package -e","title":"1. BUILD JAR WITH MAVEN"},{"location":"docker/build-kit/#_2","text":"","title":""},{"location":"docker/build-kit/#2-build-native-image-with-graal","text":"FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from=BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath","title":"2. BUILD NATIVE IMAGE WITH GRAAL"},{"location":"docker/build-kit/#_3","text":"","title":""},{"location":"docker/build-kit/#3-build-docker-runtime-image","text":"FROM alpine:3.8 CMD [\"jpc-graal\"] COPY --from=NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal","title":"3. BUILD DOCKER RUNTIME IMAGE"},{"location":"docker/build-kit/#_4","text":"","title":""},{"location":"docker/graceful-shutdown/","text":"Gracefully Shutting Down Applications in Docker \u00b6 I'm not sure about you, but I prefer it when my neighbors leave our shared spaces clean and don't take up parking spaces when they don't need them. Imagine you live in an apartment complex with the above-mentioned parking lot. Some tenants go away and never come back. If nothing is done to clean up after them - to reclaim their apartment and parking space - then after some time, more and more apartments are unavailable for no reason, and the parking lot fills with cars which belong to no one. Some tenants did not get a parking lot and are getting frustrated that none are becoming available. When they moved in, they were told that when others leave, they would be next in line. While they're waiting, they have to park outside the complex. Eventually, the entrance gets blocked and no one can enter or leave. The end result is a completely unlivable apartment block with trapped tenants - never to be seen or heard from again. If you agree with me that when a tenant leaves, they should clean the apartment and free the parking spot to make it ready for the next inhabitant; then please read on. We're going to dive into the equivalent of doing this with containers. We will explore running our containers with Docker (run, compose, swarm) and Kubernetes. Even if you use another way to run your containers, this article should provide you with enough insight to get you on your way. The case for graceful shutdown \u00b6 We're in an age where many applications are running in Docker containers across a multitude of clusters. These applications are then confronted with new concerns to tackle such as more moving parts, networking between these parts, remote storage and others. One significant way we defend ourselves against the perils of this distributed nature is to make our applications more robust - able to survive errors. However, even then there is still no guarantee your application is always up and running. So another concern we should tackle is how it responds when it needs to shut down. Where we can differentiate between an unexpected shutdown - we crashed - or an expected shutdown. On top of that, failing instead of trying to recover when something bad happens also adheres to \"fail fast\" - as strongly advocated by Michael Nygard in ReleaseIt. Shutting down can happen for a variety of reasons, in this post we dive into how to deal with an expected shutdown such as it being told to stop by an orchestrator such as Kubernetes. Containers can be purposefully shut down for a variety of reasons, including but not limited too: your application's health check fails your application consumed more resources than allowed the application is scaling down Just as cleaning up when leaving makes you a better tenant, having your application clean up connections, resources Moreover, the more tenants behaving in a good way increases the quality of living for all tenants. In our case, it improves the reliability and consistency of our cluster. Graceful shutdown is not unique to Docker, as it has been part of Linux's best practices for quite some years before Docker's existence. However, applying them to Docker container adds extra dimensions. Start Good So You Can End Well \u00b6 When you sign up for an apartment, you probably have to sign a contract detailing your rights and obligations. The more you state explicitly, the easier it is to deal with bad behaving neighbors. The same is true when running processes; we should make sure that we set the rules, obligations, and expectations from the start. As we say in Dutch: a good beginning is half the work. We will start with how you can run a process in a container with a process that shuts down gracefully. There are many ways to start a process in a container. In this article, we look at processes started by commands defined in a Dockerfile. There are two ways to specify this: CMD : runs a command when the container gets started ENTRYPOINT : provides the location (entrypoint) from where commands get run when the container starts You need at least one ENTRYPOINT or CMD in a Dockerfile for it to be valid. They can be used in collaboration but they can do similar things. For more information on the details of these commands, read Docker's docs on Entrypoint vs. CMD . Docker Shell form example \u00b6 We start with the shell form and see if it can do what we want; begin in such a way, we can stop it nicely. Shell form means we define a shell command without any special format or keywords. Please create Dockerfile with the content that follows. FROM ubuntu:18.04 ENTRYPOINT top -b Then build an image and run a container. docker image build --tag shell-form . docker run --name shell-form --rm shell-form The above command yields the following output. top - 16 :34:56 up 1 day, 5 :15, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 541984 free, 302668 used, 1202280 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1579380 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 4624 760 696 S 0 .0 0 .0 0 :00.05 sh 6 root 20 0 36480 2928 2580 R 0 .0 0 .1 0 :00.01 top As you can see, two processes are running, sh and top . Meaning, that killing the process, with ctrl+c for example, terminates the sh process, but not top . This happens because the sh process forked the top process, but the termination will only be send to PID 1 - in this case sh . As sh will not stop the top process for us it will continue running and leave the container alive. To kill this container, open a second terminal and execute the following command. docker rm -f shell-form Shell form doesn't do what we need. Starting a process with shell form will only lead us to the disaster of parking lots filling up unless there's a someone actively cleaning up. Docker exec form example \u00b6 This leads us to the exec form. Hopefully, this gets us somewhere. The exec form is written as an array of parameters: ENTRYPOINT [\"top\", \"-b\"] To continue in the same line of examples, we will create a Dockerfile, build and run it. FROM ubuntu:18.04 ENTRYPOINT [ \"top\" , \"-b\" ] Then build and run it. docker image build --tag exec-form . docker run --name exec-form --rm exec-form This yields the following output. top - 18 :12:30 up 1 day, 6 :53, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 535896 free, 307196 used, 1203840 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1574880 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 36480 2940 2584 R 0 .0 0 .1 0 :00.03 top Now we got something we can work with. If something would tell this Container to stop, it will tell our only running process so it is sure to reach the correct one! Gotchas \u00b6 Knowing we can use the exec form for our goal - gracefully shutting down our container - we can move on to the next part of our efforts. For the sake of imparting you with some hard learned lessons, we will explore two gotchas. They're optional, so you can also choose to skip to Make Sure Your Process Listens . Docker exec form with parameters \u00b6 A caveat with the exec form is that it doesn't interpolate parameters. You can try the following: FROM ubuntu:18.04 ENV PARAM = \"-b\" ENTRYPOINT [ \"top\" , \"${PARAM}\" ] Then build and run it: docker image build --tag exec-param . docker run --name exec-form --rm exec-param This should yield the following: /bin/sh: 1 : [ top: not found This is where Docker created a mix between the two styles. It allows you to create an Entrypoint with a shell command - performing interpolation - but executing it as an exec form. This can be done by prefixing the shell form, with, you guessed it, exec . FROM ubuntu:18.04 ENV PARAM = \"-b\" ENTRYPOINT exec \"top\" \" ${ PARAM } \" Then build and run it: docker image build --tag exec-param . docker run --name exec-form --rm exec-param This will return the exact same as if we would've run ENTRYPOINT [\"top\", \"-b\"] . Now you can also override the param, by using the environment variable flag. docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = \"help\" exec-param Resulting in top's help string. The special case of Alpine \u00b6 One of the main best practices for Dockerfiles , is to make them as small as possible. The easiest way to do this is to start with a minimal image. This is where Alpine Linux comes in. We will revisit out shell form example, but replace ubuntu with alpine. Create the following Dockerfile. FROM alpine:3.8 ENTRYPOINT top -b Then build and run it. docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = \"help\" exec-param This yields the following output. Mem: 1509068K used, 537864K free, 640K shrd, 126756K buff, 1012436K cached CPU: 0 % usr 0 % sys 0 % nic 100 % idle 0 % io 0 % irq 0 % sirq Load average: 0 .00 0 .00 0 .00 2 /404 5 PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND 1 0 root R 1516 0 % 0 0 % top -b Aside from top 's output looking a bit different, there is only one command. Alpine Linux helps us avoid the problem of shell form altogether! Make Sure Your Process Listens \u00b6 It is excellent if your tenants are all signed up, know their rights and obligations. But you can't contact them when something happens, how will they ever know when to act? Translating that into our process. It starts and can be told to shut down, but does it process listen? Can it interpret the message it gets from Docker or Kubernetes? And if it does, can it relay the message correctly to its Child Processes? In order for your process to gracefully shutdown, it should know when to do so. As such, it should listen not only for itself but also on behalf of its children - yours never do anything wrong though! Some processes do, but many aren't designed to listen or tell their Children . They expect someone else to listen for them and tell them and their children - process managers. In order to listen to these signals , we can call in the help of others. We will look at two options. we let Docker manage the process and its children we use a process manager Let Docker manage it for us \u00b6 If you're not using Docker to run or manage your containers, you should skip to Depend on a process manager . Docker has a build in feature, that it uses a lightweight process manager to help you. So if you're running your images with Docker itself, either directly or via Compose or Swarm, you're fine. You can use the init flag in your run command or your compose file. Please, note that the below examples require a certain minimum version of Docker. run - 1.13+ compose (v 2.2) - 1.13.0+ swarm (v 3.7) - 18.06.0+ With Docker Run \u00b6 docker run --rm -ti --init caladreas/dui With Docker Compose \u00b6 version : '2.2' services : web : image : caladreas/java-docker-signal-demo:no-tini init : true With Docker Swarm \u00b6 version : '3.7' services : web : image : caladreas/java-docker-signal-demo:no-tini init : true Relying on Docker does create a dependency on how your container runs. It only runs correctly in Docker-related technologies (run, compose, swarm) and only if the proper versions are available. Creating either a different experience for users running your application somewhere else or not able to meet the version requirements. So maybe another solution is to bake a process manager into your image and guarantee its behavior. Depend on a process manager \u00b6 One of our goals for Docker images is to keep them small. We should look for a lightweight process manager. It does not have too many a whole machine worth or processes, just one and perhaps some children. Here we would like to introduce you to Tini , a lightweight process manager designed for this purpose . It is a very successful and widely adopted process manager in the Docker world. So successful, that the before mentioned init flags from Docker are implemented by baking Tini into Docker . Debian example \u00b6 For brevity, the build process is excluded, and for image size, we use Debian slim instead of default Debian. FROM debian:stable-slim ENV TINI_VERSION v0.18.0 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ \"/tini\" , \"-vv\" , \"-g\" , \"--\" , \"/usr/bin/dui/bin/dui\" , \"-XX:+UseCGroupMemoryLimitForHeap\" , \"-XX:+UnlockExperimentalVMOptions\" ] COPY --from = build /usr/bin/dui-image/ /usr/bin/dui Alpine example \u00b6 Alpine Linux works wonders for Docker images, so to improve our lives, you can very easily install it if you want. FROM alpine RUN apk add --no-cache tini ENTRYPOINT [ \"/sbin/tini\" , \"-vv\" , \"-g\" , \"-s\" , \"--\" ] CMD [ \"top -b\" ] How To Be Told What You Want To Hear \u00b6 You've made it this far; your tenets are reachable so you can inform them if they need to act. However, there's another problem lurking around the corner. Do they speak your language? Our process now starts knowing it can be talked to, it has someone who takes care of listening for it and its children. Now we need to make sure it can understand what it hears, it should be able to handle the incoming signals. We have two main ways of doing this. Handle signals as they come : we should make sure our process deal with the signals as they come State the signals we want : we can also tell up front, which signals we want to hear and put the burden of translation on our callers For more details on the subject of Signals and Docker, please read this excellent blog from Grigorii Chudnov . Handle signals as they come \u00b6 Handling process signals depend on your application, programming language or framework. State the signals we want \u00b6 Sometimes your language or framework of choice, doesn't handle signals all that well. It might be very rigid in what it does with specific signals, removing your ability to do the right thing. Of course, not all languages or frameworks are designed with Docker container or Microservices in mind, are yet to catch up to this more dynamic environment. Luckily Docker and Kubernetes allow you to specify what signal too sent to your process. Docker run \u00b6 docker run --rm -ti --init --stop-signal = SIGINT \\ caladreas/java-docker-signal-demo Docker compose/swarm \u00b6 Docker's compose file format allows you to specify a stop signal . This is the signal sent when the container is stopped in a normal fashion. Normal in this case, meaning docker stop or when docker itself determines it should stop the container. If you forcefully remove the container, for example with docker rm -f it will directly kill the process, so don't do that. version : '2.2' services : web : image : caladreas/java-docker-signal-demo stop_signal : SIGINT stop_grace_period : 15s If you run this with docker-compose up and then in a second terminal, stop the container, you will see something like this. web_1 | HelloWorld! web_1 | Shutdown hook called! web_1 | We 're told to stop early... web_1 | java.lang.InterruptedException: sleep interrupted web_1 | at java.base/java.lang.Thread.sleep(Native Method) web_1 | at joostvdg.demo.signal@1.0/com.github.joostvdg.demo.signal.HelloWorld.printHelloWorld(Unknown Source) web_1 | at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) web_1 | at java.base/java.util.concurrent.FutureTask.run(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) web_1 | at java.base/java.lang.Thread.run(Unknown Source) web_1 | [DEBUG tini (1)] Passing signal: ' Interrupt ' web_1 | [DEBUG tini (1)] Received SIGCHLD web_1 | [DEBUG tini (1)] Reaped child with pid: ' 7 ' web_1 | [INFO tini (1)] Main child exited with signal (with signal ' Interrupt ' ) Kubernetes \u00b6 In Kubernetes we can make use of Container Lifecycle Hooks to manage how our container should be stopped. We could, for example, send a SIGINT (interrupt) to tell our application to stop. apiVersion : apps/v1 kind : Deployment metadata : name : java-signal-demo namespace : default labels : app : java-signal-demo spec : replicas : 1 template : metadata : labels : app : java-signal-demo spec : containers : - name : main image : caladreas/java-docker-signal-demo lifecycle : preStop : exec : command : [ \"killall\" , \"java\" , \"-INT\" ] terminationGracePeriodSeconds : 60 When you create this as deployment.yml, create and delete it - kubectl apply -f deployment.yml / kubectl delete -f deployment.yml - you will see the same behavior. How To Be Told When You Want To Hear It \u00b6 Our process now will now start knowing it will hear what it wants to hear. But we now have to make sure we hear it when we need to hear it. An intervention is excellent when you can still be saved, but it is a bit useless if you're already dead. Docker \u00b6 You can either configure your health check in your Dockerfile or configure it in your docker-compose.yml for either compose or swarm. Considering only Docker can use the health check in your Dockerfile, it is strongly recommended to have health checks in your application and document how they can be used. Kubernetes \u00b6 In Kubernetes we have the concept of Container Probes . This allows you to configure whether your container is ready (readinessProbe) to be used and if it is still working as expected (livenessProbe). Examples \u00b6 How to actually listen to the signals and determine which one to use will depend on your programming language. There's three examples I have worked out, one for Go (lang) and two for Java: pojo and Spring Boot. Go \u00b6 Dockerfile \u00b6 # build stage FROM golang:latest AS build-env RUN go get -v github.com/docker/docker/client/... RUN go get -v github.com/docker/docker/api/... ADD src/ $GOPATH /flow-proxy-service-lister WORKDIR $GOPATH /flow-proxy-service-lister RUN go build -o main -tags netgo main.go # final stage FROM alpine ENTRYPOINT [ \"/app/main\" ] COPY --from = build-env /go/flow-proxy-service-lister/main /app/ RUN chmod +x /app/main Go code for graceful shutdown \u00b6 The following is a way for Go to shutdown a http server when receiving a termination signal. func main () { c := make ( chan bool ) // make channel for main <--> webserver communication go webserver . Start ( \"7777\" , webserverData , c ) // ignore the missing data stop := make ( chan os . Signal , 1 ) // make a channel that listens to is signals signal . Notify ( stop , syscall . SIGINT , syscall . SIGTERM ) // we listen to some specific syscall signals for i := 1 ; ; i ++ { // this is still infinite t := time . NewTicker ( time . Second * 30 ) // set a timer for the polling select { case <- stop : // this means we got a os signal on our channel break // so we can stop case <- t . C : // our timer expired, refresh our data continue // and continue with the loop } break } fmt . Println ( \"Shutting down webserver\" ) // if we got here, we have to inform the webserver to close shop c <- true // we do this by sending a message on the channel if b := <- c ; b { // when we get true back, that means the webserver is doing with a graceful shutdown fmt . Println ( \"Webserver shut down\" ) // webserver is done } fmt . Println ( \"Shut down app\" ) // we can close shop ourselves now } Java plain (Docker Swarm) \u00b6 This application is a Java 9 modular application, which can be found on github, github.com/joostvdg . Dockerfile \u00b6 FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name \"*.java\" ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" LABEL version = \"0.1.0\" LABEL description = \"Docker image for playing with java applications in a concurrent, parallel and distributed manor.\" # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ \"/tini\" , \"-vv\" , \"-g\" , \"--\" , \"/usr/bin/dui/bin/dui\" ] ENV DATE_CHANGED = \"20180120-1525\" COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules Handling code \u00b6 The code first initializes the server which and when started, creates the Shutdown Hook . Java handles certain signals in specific ways, as can be found in this table for linux. For more information, you can read the docs from Oracle . public class DockerApp { public static void main ( String [] args ) { ServiceLoader < Logger > loggers = ServiceLoader . load ( Logger . class ); Logger logger = loggers . findFirst (). isPresent () ? loggers . findFirst (). get () : null ; if ( logger == null ) { System . err . println ( \"Did not find any loggers, quiting\" ); System . exit ( 1 ); } logger . start ( LogLevel . INFO ); int pseudoRandom = new Random (). nextInt ( ProtocolConstants . POTENTIAL_SERVER_NAMES . length - 1 ); String serverName = ProtocolConstants . POTENTIAL_SERVER_NAMES [ pseudoRandom ] ; int listenPort = ProtocolConstants . EXTERNAL_COMMUNICATION_PORT_A ; String multicastGroup = ProtocolConstants . MULTICAST_GROUP ; DuiServer distributedServer = DuiServerFactory . newDistributedServer ( listenPort , multicastGroup , serverName , logger ); distributedServer . logMembership (); ExecutorService executorService = Executors . newFixedThreadPool ( 1 ); executorService . submit ( distributedServer :: startServer ); long threadId = Thread . currentThread (). getId (); Runtime . getRuntime (). addShutdownHook ( new Thread (() -> { System . out . println ( \"Shutdown hook called!\" ); logger . log ( LogLevel . WARN , \"App\" , \"ShotdownHook\" , threadId , \"Shutting down at request of Docker\" ); distributedServer . stopServer (); distributedServer . closeServer (); executorService . shutdown (); try { Thread . sleep ( 100 ); executorService . shutdownNow (); logger . stop (); } catch ( InterruptedException e ) { e . printStackTrace (); } })); } } Java Plain (Kubernetes) \u00b6 So far we've utilized the utilities from Docker itself in conjunction with it's native Docker Swarm orchestrator. Unfortunately, when it comes to popularity Kubernetes beats Swarm hands down . So this isn't complete if it doesn't also do graceful shutdown in Kubernetes. In Dockerfile \u00b6 Our original file had to be changed, as Debian's Slim image doesn't actually contain the kill package. And we need a kill package, as we cannot instruct Kubernetes to issue a specific SIGNAL. Instead, we can issue a PreStop exec command , which we can utilise to execute a killall java -INT . The command will be specified in the Kubernetes deployment definition below. FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name \"*.java\" ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" LABEL version = \"0.1.0\" LABEL description = \"Docker image for playing with java applications in a concurrent, parallel and distributed manor.\" # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ \"/tini\" , \"-vv\" , \"-g\" , \"--\" , \"/usr/bin/dui/bin/dui\" ] ENV DATE_CHANGED = \"20180120-1525\" RUN apt-get update && apt-get install --no-install-recommends -y psmisc = 22 .* && rm -rf /var/lib/apt/lists/* COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules Kubernetes Deployment \u00b6 So here we have the image's K8s Deployment descriptor. Including the Pod's lifecycle preStop with a exec style command. You should know by now why we prefer that . apiVersion : extensions/v1beta1 kind : Deployment metadata : name : dui-deployment namespace : default labels : k8s-app : dui spec : replicas : 3 template : metadata : labels : k8s-app : dui spec : containers : - name : master image : caladreas/buming ports : - name : http containerPort : 7777 lifecycle : preStop : exec : command : [ \"killall\" , \"java\" , \"-INT\" ] terminationGracePeriodSeconds : 60 Java Spring Boot (1.x) \u00b6 This example is for Spring Boot 1.x, in time we will have an example for 2.x. This example is for the scenario of a Fat Jar with Tomcat as container [^8]. Execute example \u00b6 docker-compose build Execute the following command: docker run --rm -ti --name test spring-boot-graceful Exit the application/container via ctrl+c and you should see the application shutting down gracefully. 2018 -01-30 13 :35:46.327 INFO 7 --- [ Thread-3 ] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@6e5e91e4: startup date [ Tue Jan 30 13 :35:42 GMT 2018 ] ; root of context hierarchy 2018 -01-30 13 :35:46.405 INFO 7 --- [ Thread-3 ] BootGracefulApplication $GracefulShutdown : Tomcat was shutdown gracefully within the allotted time. 2018 -01-30 13 :35:46.408 INFO 7 --- [ Thread-3 ] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans on shutdown Dockerfile \u00b6 FROM maven:3-jdk-8 AS build ENV MAVEN_OPTS = -Dmaven.repo.local = /usr/share/maven/repository ENV WORKDIR = /usr/src/graceful RUN mkdir $WORKDIR WORKDIR $WORKDIR COPY pom.xml $WORKDIR RUN mvn -B -e org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline COPY . $WORKSPACE RUN mvn -B -e clean verify FROM anapsix/alpine-java:8_jdk_unlimited LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ \"/tini\" , \"-vv\" , \"-g\" , \"--\" ] ENV DATE_CHANGED = \"20180120-1525\" COPY --from = build /usr/src/graceful/target/spring-boot-graceful.jar /app.jar CMD [ \"java\" , \"-Xms256M\" , \"-Xmx480M\" , \"-Djava.security.egd=file:/dev/./urandom\" , \"-jar\" , \"/app.jar\" ] Docker compose file \u00b6 version : \"3.5\" services : web : image : spring-boot-graceful build : . stop_signal : SIGINT Java handling code \u00b6 package com.github.joostvdg.demo.springbootgraceful ; import org.springframework.boot.SpringApplication ; import org.springframework.boot.autoconfigure.SpringBootApplication ; import org.apache.catalina.connector.Connector ; import org.apache.tomcat.util.threads.ThreadPoolExecutor ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer ; import org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatConnectorCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory ; import org.springframework.context.ApplicationListener ; import org.springframework.context.annotation.Bean ; import org.springframework.context.event.ContextClosedEvent ; import java.util.concurrent.Executor ; import java.util.concurrent.TimeUnit ; @SpringBootApplication public class SpringBootGracefulApplication { public static void main ( String [] args ) { SpringApplication . run ( SpringBootGracefulApplication . class , args ); } @Bean public GracefulShutdown gracefulShutdown () { return new GracefulShutdown (); } @Bean public EmbeddedServletContainerCustomizer tomcatCustomizer () { return new EmbeddedServletContainerCustomizer () { @Override public void customize ( ConfigurableEmbeddedServletContainer container ) { if ( container instanceof TomcatEmbeddedServletContainerFactory ) { (( TomcatEmbeddedServletContainerFactory ) container ) . addConnectorCustomizers ( gracefulShutdown ()); } } }; } private static class GracefulShutdown implements TomcatConnectorCustomizer , ApplicationListener < ContextClosedEvent > { private static final Logger log = LoggerFactory . getLogger ( GracefulShutdown . class ); private volatile Connector connector ; @Override public void customize ( Connector connector ) { this . connector = connector ; } @Override public void onApplicationEvent ( ContextClosedEvent event ) { this . connector . pause (); Executor executor = this . connector . getProtocolHandler (). getExecutor (); if ( executor instanceof ThreadPoolExecutor ) { try { ThreadPoolExecutor threadPoolExecutor = ( ThreadPoolExecutor ) executor ; threadPoolExecutor . shutdown (); if ( ! threadPoolExecutor . awaitTermination ( 30 , TimeUnit . SECONDS )) { log . warn ( \"Tomcat thread pool did not shut down gracefully within \" + \"30 seconds. Proceeding with forceful shutdown\" ); } else { log . info ( \"Tomcat was shutdown gracefully within the allotted time.\" ); } } catch ( InterruptedException ex ) { Thread . currentThread (). interrupt (); } } } } } Example with Docker Swarm \u00b6 For now there's only an example with docker swarm , in time there will also be a Kubernetes example. Now that you can create Java applications packaged neatly in Docker images that support graceful shutdown, it would be nice to utilize. A good scenario would be a microservices architecture where services can come and go, but are registered in a service registry such as Eureka . Or a membership based protocol where members interact with each other and perhaps shard data. In these cases, of course the interactions are designed to be fault tolerant and discover faulty nodes on their own. But wouldn't it be better that if you knew you're going to quit, you inform the rest? We can reuse the caladreas/buming image and make it a docker swarm stack and run the service on every node. This way, we can easily see members coming and going and reduce the time to detect failure by notifying our peers of our impeding end. Docker swarm cluster \u00b6 Setting up a docker swarm cluster is easy, but has some requirements: virtual box 4.x+ docker-machine 1.12+ docker 17.06+ Warn Make sure this is the first and only virtualbox docker-machine VM being created/running, so that the ip range starts with 192.168.99.100 docker-machine create --driver virtualbox dui-1 docker-machine create --driver virtualbox dui-2 docker-machine create --driver virtualbox dui-3 eval \" $( docker-machine env dui-1 ) \" IP = 192 .168.99.100 docker swarm init --advertise-addr $IP TOKEN = $( docker swarm join-token -q worker ) eval \" $( docker-machine env dui-2 ) \" docker swarm join --token ${ TOKEN } ${ IP } :2377 eval \" $( docker-machine env dui-3 ) \" docker swarm join --token ${ TOKEN } ${ IP } :2377 eval \" $( docker-machine env dui-1 ) \" docker node ls Docker swarm network and multicast \u00b6 Unfortunately, docker swarm's swarm mode network overlay does not support multicast [ 9][ 10]. Why is this a problem? Well, the application I use to test the graceful shutdown requires this, sorry. Luckily there is a very easy solution for this, its by using Weavenet 's docker network plugin. Don't want to know about it or how you install it? Don't worry, just execute the script below. #!/usr/bin/env bash echo \"=> Prepare dui-2\" eval \" $( docker-machine env dui-2 ) \" docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo \"=> Prepare dui-3\" eval \" $( docker-machine env dui-3 ) \" docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo \"=> Prepare dui-1\" eval \" $( docker-machine env dui-1 ) \" docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 docker network create --driver = weaveworks/net-plugin:2.1.3 --opt works.weave.multicast = true --attachable dui Docker stack \u00b6 Now to create a service that runs on every node it is the easiest to create a docker stack . Compose file (docker-stack.yml) \u00b6 version : \"3.5\" services : dui : image : caladreas/buming build : . stop_signal : SIGINT networks : - dui deploy : mode : global networks : dui : external : true Create stack \u00b6 docker stack deploy --compose-file docker-stack.yml buming Execute example \u00b6 Now that we have a docker swarm cluster and a stack - which has a service running on every node - we can showcase the power of graceful shutdown in a cluster of dependent services. Confirm the service is running correctly on every node, first lets check our nodes. eval \" $( docker-machine env dui-1 ) \" docker node ls Which should look like this: ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS f21ilm4thxegn5xbentmss5ur * dui-1 Ready Active Leader y7475bo5uplt2b58d050b4wfd dui-2 Ready Active 6ssxola6y1i6h9p8256pi7bfv dui-3 Ready Active Then check the service. docker service ps buming_dui Which should look like this. ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 3mrpr0jg31x1 buming_dui.6ssxola6y1i6h9p8256pi7bfv dui:latest dui-3 Running Running 17 seconds ago pfubtiy4j7vo buming_dui.f21ilm4thxegn5xbentmss5ur dui:latest dui-1 Running Running 17 seconds ago f4gjnmhoe3y4 buming_dui.y7475bo5uplt2b58d050b4wfd dui:latest dui-2 Running Running 17 seconds ago Now open a second terminal window. In window one, follow the service logs: eval \" $( docker-machine env dui-1 ) \" docker service logs -f buming_dui In window two, go to a different node and stop the container. eval \" $( docker-machine env dui-2 ) \" docker ps docker stop buming_dui.y7475bo5uplt2b58d050b4wfd.pnoui2x6elrz0tvkjz51njz94 In this case, you will see the other nodes receiving a leave notice and then the node stopping. buming_dui.0.ryd8szexxku3@dui-3 | [ Server-John D. Carmack ] [ WARN ] [ 14 :19:02.604011 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = '83918f6ad817' , ip = '10.0.0.7' , name = 'Ken Thompson' } buming_dui.0.so5m14sz8ksh@dui-1 | [ Server-Alan Kay ] [ WARN ] [ 14 :19:02.602082 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = '83918f6ad817' , ip = '10.0.0.7' , name = 'Ken Thompson' } buming_dui.0.pnoui2x6elrz@dui-2 | Shutdown hook called! buming_dui.0.pnoui2x6elrz@dui-2 | [ App ] [ WARN ] [ 14 :19:02.598759 ] [ 1 ] [ ShotdownHook ] Shutting down at request of Docker buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.598858 ] [ 12 ] [ Main ] Stopping buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.601008 ] [ 12 ] [ Main ] Closing Further reading \u00b6 Wikipedia page on reboots Microsoft about graceful shutdown Gracefully stopping docker containers What to know about Java and shutdown hooks https://www.weave.works/blog/docker-container-networking-multicast-fast/ https://www.weave.works/docs/net/latest/install/plugin/plugin-how-it-works/ https://www.weave.works/docs/net/latest/install/plugin/plugin-v2/ https://www.auzias.net/en/docker-network-multihost/ https://forums.docker.com/t/cannot-get-zookeeper-to-work-running-in-docker-using-swarm-mode/27109 https://github.com/docker/libnetwork/issues/740","title":"Graceful Shutdown"},{"location":"docker/graceful-shutdown/#gracefully-shutting-down-applications-in-docker","text":"I'm not sure about you, but I prefer it when my neighbors leave our shared spaces clean and don't take up parking spaces when they don't need them. Imagine you live in an apartment complex with the above-mentioned parking lot. Some tenants go away and never come back. If nothing is done to clean up after them - to reclaim their apartment and parking space - then after some time, more and more apartments are unavailable for no reason, and the parking lot fills with cars which belong to no one. Some tenants did not get a parking lot and are getting frustrated that none are becoming available. When they moved in, they were told that when others leave, they would be next in line. While they're waiting, they have to park outside the complex. Eventually, the entrance gets blocked and no one can enter or leave. The end result is a completely unlivable apartment block with trapped tenants - never to be seen or heard from again. If you agree with me that when a tenant leaves, they should clean the apartment and free the parking spot to make it ready for the next inhabitant; then please read on. We're going to dive into the equivalent of doing this with containers. We will explore running our containers with Docker (run, compose, swarm) and Kubernetes. Even if you use another way to run your containers, this article should provide you with enough insight to get you on your way.","title":"Gracefully Shutting Down Applications in Docker"},{"location":"docker/graceful-shutdown/#the-case-for-graceful-shutdown","text":"We're in an age where many applications are running in Docker containers across a multitude of clusters. These applications are then confronted with new concerns to tackle such as more moving parts, networking between these parts, remote storage and others. One significant way we defend ourselves against the perils of this distributed nature is to make our applications more robust - able to survive errors. However, even then there is still no guarantee your application is always up and running. So another concern we should tackle is how it responds when it needs to shut down. Where we can differentiate between an unexpected shutdown - we crashed - or an expected shutdown. On top of that, failing instead of trying to recover when something bad happens also adheres to \"fail fast\" - as strongly advocated by Michael Nygard in ReleaseIt. Shutting down can happen for a variety of reasons, in this post we dive into how to deal with an expected shutdown such as it being told to stop by an orchestrator such as Kubernetes. Containers can be purposefully shut down for a variety of reasons, including but not limited too: your application's health check fails your application consumed more resources than allowed the application is scaling down Just as cleaning up when leaving makes you a better tenant, having your application clean up connections, resources Moreover, the more tenants behaving in a good way increases the quality of living for all tenants. In our case, it improves the reliability and consistency of our cluster. Graceful shutdown is not unique to Docker, as it has been part of Linux's best practices for quite some years before Docker's existence. However, applying them to Docker container adds extra dimensions.","title":"The case for graceful shutdown"},{"location":"docker/graceful-shutdown/#start-good-so-you-can-end-well","text":"When you sign up for an apartment, you probably have to sign a contract detailing your rights and obligations. The more you state explicitly, the easier it is to deal with bad behaving neighbors. The same is true when running processes; we should make sure that we set the rules, obligations, and expectations from the start. As we say in Dutch: a good beginning is half the work. We will start with how you can run a process in a container with a process that shuts down gracefully. There are many ways to start a process in a container. In this article, we look at processes started by commands defined in a Dockerfile. There are two ways to specify this: CMD : runs a command when the container gets started ENTRYPOINT : provides the location (entrypoint) from where commands get run when the container starts You need at least one ENTRYPOINT or CMD in a Dockerfile for it to be valid. They can be used in collaboration but they can do similar things. For more information on the details of these commands, read Docker's docs on Entrypoint vs. CMD .","title":"Start Good So You Can End Well"},{"location":"docker/graceful-shutdown/#docker-shell-form-example","text":"We start with the shell form and see if it can do what we want; begin in such a way, we can stop it nicely. Shell form means we define a shell command without any special format or keywords. Please create Dockerfile with the content that follows. FROM ubuntu:18.04 ENTRYPOINT top -b Then build an image and run a container. docker image build --tag shell-form . docker run --name shell-form --rm shell-form The above command yields the following output. top - 16 :34:56 up 1 day, 5 :15, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 541984 free, 302668 used, 1202280 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1579380 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 4624 760 696 S 0 .0 0 .0 0 :00.05 sh 6 root 20 0 36480 2928 2580 R 0 .0 0 .1 0 :00.01 top As you can see, two processes are running, sh and top . Meaning, that killing the process, with ctrl+c for example, terminates the sh process, but not top . This happens because the sh process forked the top process, but the termination will only be send to PID 1 - in this case sh . As sh will not stop the top process for us it will continue running and leave the container alive. To kill this container, open a second terminal and execute the following command. docker rm -f shell-form Shell form doesn't do what we need. Starting a process with shell form will only lead us to the disaster of parking lots filling up unless there's a someone actively cleaning up.","title":"Docker Shell form example"},{"location":"docker/graceful-shutdown/#docker-exec-form-example","text":"This leads us to the exec form. Hopefully, this gets us somewhere. The exec form is written as an array of parameters: ENTRYPOINT [\"top\", \"-b\"] To continue in the same line of examples, we will create a Dockerfile, build and run it. FROM ubuntu:18.04 ENTRYPOINT [ \"top\" , \"-b\" ] Then build and run it. docker image build --tag exec-form . docker run --name exec-form --rm exec-form This yields the following output. top - 18 :12:30 up 1 day, 6 :53, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 535896 free, 307196 used, 1203840 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1574880 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 36480 2940 2584 R 0 .0 0 .1 0 :00.03 top Now we got something we can work with. If something would tell this Container to stop, it will tell our only running process so it is sure to reach the correct one!","title":"Docker exec form example"},{"location":"docker/graceful-shutdown/#gotchas","text":"Knowing we can use the exec form for our goal - gracefully shutting down our container - we can move on to the next part of our efforts. For the sake of imparting you with some hard learned lessons, we will explore two gotchas. They're optional, so you can also choose to skip to Make Sure Your Process Listens .","title":"Gotchas"},{"location":"docker/graceful-shutdown/#docker-exec-form-with-parameters","text":"A caveat with the exec form is that it doesn't interpolate parameters. You can try the following: FROM ubuntu:18.04 ENV PARAM = \"-b\" ENTRYPOINT [ \"top\" , \"${PARAM}\" ] Then build and run it: docker image build --tag exec-param . docker run --name exec-form --rm exec-param This should yield the following: /bin/sh: 1 : [ top: not found This is where Docker created a mix between the two styles. It allows you to create an Entrypoint with a shell command - performing interpolation - but executing it as an exec form. This can be done by prefixing the shell form, with, you guessed it, exec . FROM ubuntu:18.04 ENV PARAM = \"-b\" ENTRYPOINT exec \"top\" \" ${ PARAM } \" Then build and run it: docker image build --tag exec-param . docker run --name exec-form --rm exec-param This will return the exact same as if we would've run ENTRYPOINT [\"top\", \"-b\"] . Now you can also override the param, by using the environment variable flag. docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = \"help\" exec-param Resulting in top's help string.","title":"Docker exec form with parameters"},{"location":"docker/graceful-shutdown/#the-special-case-of-alpine","text":"One of the main best practices for Dockerfiles , is to make them as small as possible. The easiest way to do this is to start with a minimal image. This is where Alpine Linux comes in. We will revisit out shell form example, but replace ubuntu with alpine. Create the following Dockerfile. FROM alpine:3.8 ENTRYPOINT top -b Then build and run it. docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = \"help\" exec-param This yields the following output. Mem: 1509068K used, 537864K free, 640K shrd, 126756K buff, 1012436K cached CPU: 0 % usr 0 % sys 0 % nic 100 % idle 0 % io 0 % irq 0 % sirq Load average: 0 .00 0 .00 0 .00 2 /404 5 PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND 1 0 root R 1516 0 % 0 0 % top -b Aside from top 's output looking a bit different, there is only one command. Alpine Linux helps us avoid the problem of shell form altogether!","title":"The special case of Alpine"},{"location":"docker/graceful-shutdown/#make-sure-your-process-listens","text":"It is excellent if your tenants are all signed up, know their rights and obligations. But you can't contact them when something happens, how will they ever know when to act? Translating that into our process. It starts and can be told to shut down, but does it process listen? Can it interpret the message it gets from Docker or Kubernetes? And if it does, can it relay the message correctly to its Child Processes? In order for your process to gracefully shutdown, it should know when to do so. As such, it should listen not only for itself but also on behalf of its children - yours never do anything wrong though! Some processes do, but many aren't designed to listen or tell their Children . They expect someone else to listen for them and tell them and their children - process managers. In order to listen to these signals , we can call in the help of others. We will look at two options. we let Docker manage the process and its children we use a process manager","title":"Make Sure Your Process Listens"},{"location":"docker/graceful-shutdown/#let-docker-manage-it-for-us","text":"If you're not using Docker to run or manage your containers, you should skip to Depend on a process manager . Docker has a build in feature, that it uses a lightweight process manager to help you. So if you're running your images with Docker itself, either directly or via Compose or Swarm, you're fine. You can use the init flag in your run command or your compose file. Please, note that the below examples require a certain minimum version of Docker. run - 1.13+ compose (v 2.2) - 1.13.0+ swarm (v 3.7) - 18.06.0+","title":"Let Docker manage it for us"},{"location":"docker/graceful-shutdown/#with-docker-run","text":"docker run --rm -ti --init caladreas/dui","title":"With Docker Run"},{"location":"docker/graceful-shutdown/#with-docker-compose","text":"version : '2.2' services : web : image : caladreas/java-docker-signal-demo:no-tini init : true","title":"With Docker Compose"},{"location":"docker/graceful-shutdown/#with-docker-swarm","text":"version : '3.7' services : web : image : caladreas/java-docker-signal-demo:no-tini init : true Relying on Docker does create a dependency on how your container runs. It only runs correctly in Docker-related technologies (run, compose, swarm) and only if the proper versions are available. Creating either a different experience for users running your application somewhere else or not able to meet the version requirements. So maybe another solution is to bake a process manager into your image and guarantee its behavior.","title":"With Docker Swarm"},{"location":"docker/graceful-shutdown/#depend-on-a-process-manager","text":"One of our goals for Docker images is to keep them small. We should look for a lightweight process manager. It does not have too many a whole machine worth or processes, just one and perhaps some children. Here we would like to introduce you to Tini , a lightweight process manager designed for this purpose . It is a very successful and widely adopted process manager in the Docker world. So successful, that the before mentioned init flags from Docker are implemented by baking Tini into Docker .","title":"Depend on a process manager"},{"location":"docker/graceful-shutdown/#debian-example","text":"For brevity, the build process is excluded, and for image size, we use Debian slim instead of default Debian. FROM debian:stable-slim ENV TINI_VERSION v0.18.0 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ \"/tini\" , \"-vv\" , \"-g\" , \"--\" , \"/usr/bin/dui/bin/dui\" , \"-XX:+UseCGroupMemoryLimitForHeap\" , \"-XX:+UnlockExperimentalVMOptions\" ] COPY --from = build /usr/bin/dui-image/ /usr/bin/dui","title":"Debian example"},{"location":"docker/graceful-shutdown/#alpine-example","text":"Alpine Linux works wonders for Docker images, so to improve our lives, you can very easily install it if you want. FROM alpine RUN apk add --no-cache tini ENTRYPOINT [ \"/sbin/tini\" , \"-vv\" , \"-g\" , \"-s\" , \"--\" ] CMD [ \"top -b\" ]","title":"Alpine example"},{"location":"docker/graceful-shutdown/#how-to-be-told-what-you-want-to-hear","text":"You've made it this far; your tenets are reachable so you can inform them if they need to act. However, there's another problem lurking around the corner. Do they speak your language? Our process now starts knowing it can be talked to, it has someone who takes care of listening for it and its children. Now we need to make sure it can understand what it hears, it should be able to handle the incoming signals. We have two main ways of doing this. Handle signals as they come : we should make sure our process deal with the signals as they come State the signals we want : we can also tell up front, which signals we want to hear and put the burden of translation on our callers For more details on the subject of Signals and Docker, please read this excellent blog from Grigorii Chudnov .","title":"How To Be Told What You Want To Hear"},{"location":"docker/graceful-shutdown/#handle-signals-as-they-come","text":"Handling process signals depend on your application, programming language or framework.","title":"Handle signals as they come"},{"location":"docker/graceful-shutdown/#state-the-signals-we-want","text":"Sometimes your language or framework of choice, doesn't handle signals all that well. It might be very rigid in what it does with specific signals, removing your ability to do the right thing. Of course, not all languages or frameworks are designed with Docker container or Microservices in mind, are yet to catch up to this more dynamic environment. Luckily Docker and Kubernetes allow you to specify what signal too sent to your process.","title":"State the signals we want"},{"location":"docker/graceful-shutdown/#docker-run","text":"docker run --rm -ti --init --stop-signal = SIGINT \\ caladreas/java-docker-signal-demo","title":"Docker run"},{"location":"docker/graceful-shutdown/#docker-composeswarm","text":"Docker's compose file format allows you to specify a stop signal . This is the signal sent when the container is stopped in a normal fashion. Normal in this case, meaning docker stop or when docker itself determines it should stop the container. If you forcefully remove the container, for example with docker rm -f it will directly kill the process, so don't do that. version : '2.2' services : web : image : caladreas/java-docker-signal-demo stop_signal : SIGINT stop_grace_period : 15s If you run this with docker-compose up and then in a second terminal, stop the container, you will see something like this. web_1 | HelloWorld! web_1 | Shutdown hook called! web_1 | We 're told to stop early... web_1 | java.lang.InterruptedException: sleep interrupted web_1 | at java.base/java.lang.Thread.sleep(Native Method) web_1 | at joostvdg.demo.signal@1.0/com.github.joostvdg.demo.signal.HelloWorld.printHelloWorld(Unknown Source) web_1 | at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) web_1 | at java.base/java.util.concurrent.FutureTask.run(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) web_1 | at java.base/java.lang.Thread.run(Unknown Source) web_1 | [DEBUG tini (1)] Passing signal: ' Interrupt ' web_1 | [DEBUG tini (1)] Received SIGCHLD web_1 | [DEBUG tini (1)] Reaped child with pid: ' 7 ' web_1 | [INFO tini (1)] Main child exited with signal (with signal ' Interrupt ' )","title":"Docker compose/swarm"},{"location":"docker/graceful-shutdown/#kubernetes","text":"In Kubernetes we can make use of Container Lifecycle Hooks to manage how our container should be stopped. We could, for example, send a SIGINT (interrupt) to tell our application to stop. apiVersion : apps/v1 kind : Deployment metadata : name : java-signal-demo namespace : default labels : app : java-signal-demo spec : replicas : 1 template : metadata : labels : app : java-signal-demo spec : containers : - name : main image : caladreas/java-docker-signal-demo lifecycle : preStop : exec : command : [ \"killall\" , \"java\" , \"-INT\" ] terminationGracePeriodSeconds : 60 When you create this as deployment.yml, create and delete it - kubectl apply -f deployment.yml / kubectl delete -f deployment.yml - you will see the same behavior.","title":"Kubernetes"},{"location":"docker/graceful-shutdown/#how-to-be-told-when-you-want-to-hear-it","text":"Our process now will now start knowing it will hear what it wants to hear. But we now have to make sure we hear it when we need to hear it. An intervention is excellent when you can still be saved, but it is a bit useless if you're already dead.","title":"How To Be Told When You Want To Hear It"},{"location":"docker/graceful-shutdown/#docker","text":"You can either configure your health check in your Dockerfile or configure it in your docker-compose.yml for either compose or swarm. Considering only Docker can use the health check in your Dockerfile, it is strongly recommended to have health checks in your application and document how they can be used.","title":"Docker"},{"location":"docker/graceful-shutdown/#kubernetes_1","text":"In Kubernetes we have the concept of Container Probes . This allows you to configure whether your container is ready (readinessProbe) to be used and if it is still working as expected (livenessProbe).","title":"Kubernetes"},{"location":"docker/graceful-shutdown/#examples","text":"How to actually listen to the signals and determine which one to use will depend on your programming language. There's three examples I have worked out, one for Go (lang) and two for Java: pojo and Spring Boot.","title":"Examples"},{"location":"docker/graceful-shutdown/#go","text":"","title":"Go"},{"location":"docker/graceful-shutdown/#dockerfile","text":"# build stage FROM golang:latest AS build-env RUN go get -v github.com/docker/docker/client/... RUN go get -v github.com/docker/docker/api/... ADD src/ $GOPATH /flow-proxy-service-lister WORKDIR $GOPATH /flow-proxy-service-lister RUN go build -o main -tags netgo main.go # final stage FROM alpine ENTRYPOINT [ \"/app/main\" ] COPY --from = build-env /go/flow-proxy-service-lister/main /app/ RUN chmod +x /app/main","title":"Dockerfile"},{"location":"docker/graceful-shutdown/#go-code-for-graceful-shutdown","text":"The following is a way for Go to shutdown a http server when receiving a termination signal. func main () { c := make ( chan bool ) // make channel for main <--> webserver communication go webserver . Start ( \"7777\" , webserverData , c ) // ignore the missing data stop := make ( chan os . Signal , 1 ) // make a channel that listens to is signals signal . Notify ( stop , syscall . SIGINT , syscall . SIGTERM ) // we listen to some specific syscall signals for i := 1 ; ; i ++ { // this is still infinite t := time . NewTicker ( time . Second * 30 ) // set a timer for the polling select { case <- stop : // this means we got a os signal on our channel break // so we can stop case <- t . C : // our timer expired, refresh our data continue // and continue with the loop } break } fmt . Println ( \"Shutting down webserver\" ) // if we got here, we have to inform the webserver to close shop c <- true // we do this by sending a message on the channel if b := <- c ; b { // when we get true back, that means the webserver is doing with a graceful shutdown fmt . Println ( \"Webserver shut down\" ) // webserver is done } fmt . Println ( \"Shut down app\" ) // we can close shop ourselves now }","title":"Go code for graceful shutdown"},{"location":"docker/graceful-shutdown/#java-plain-docker-swarm","text":"This application is a Java 9 modular application, which can be found on github, github.com/joostvdg .","title":"Java plain (Docker Swarm)"},{"location":"docker/graceful-shutdown/#dockerfile_1","text":"FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name \"*.java\" ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" LABEL version = \"0.1.0\" LABEL description = \"Docker image for playing with java applications in a concurrent, parallel and distributed manor.\" # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ \"/tini\" , \"-vv\" , \"-g\" , \"--\" , \"/usr/bin/dui/bin/dui\" ] ENV DATE_CHANGED = \"20180120-1525\" COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules","title":"Dockerfile"},{"location":"docker/graceful-shutdown/#handling-code","text":"The code first initializes the server which and when started, creates the Shutdown Hook . Java handles certain signals in specific ways, as can be found in this table for linux. For more information, you can read the docs from Oracle . public class DockerApp { public static void main ( String [] args ) { ServiceLoader < Logger > loggers = ServiceLoader . load ( Logger . class ); Logger logger = loggers . findFirst (). isPresent () ? loggers . findFirst (). get () : null ; if ( logger == null ) { System . err . println ( \"Did not find any loggers, quiting\" ); System . exit ( 1 ); } logger . start ( LogLevel . INFO ); int pseudoRandom = new Random (). nextInt ( ProtocolConstants . POTENTIAL_SERVER_NAMES . length - 1 ); String serverName = ProtocolConstants . POTENTIAL_SERVER_NAMES [ pseudoRandom ] ; int listenPort = ProtocolConstants . EXTERNAL_COMMUNICATION_PORT_A ; String multicastGroup = ProtocolConstants . MULTICAST_GROUP ; DuiServer distributedServer = DuiServerFactory . newDistributedServer ( listenPort , multicastGroup , serverName , logger ); distributedServer . logMembership (); ExecutorService executorService = Executors . newFixedThreadPool ( 1 ); executorService . submit ( distributedServer :: startServer ); long threadId = Thread . currentThread (). getId (); Runtime . getRuntime (). addShutdownHook ( new Thread (() -> { System . out . println ( \"Shutdown hook called!\" ); logger . log ( LogLevel . WARN , \"App\" , \"ShotdownHook\" , threadId , \"Shutting down at request of Docker\" ); distributedServer . stopServer (); distributedServer . closeServer (); executorService . shutdown (); try { Thread . sleep ( 100 ); executorService . shutdownNow (); logger . stop (); } catch ( InterruptedException e ) { e . printStackTrace (); } })); } }","title":"Handling code"},{"location":"docker/graceful-shutdown/#java-plain-kubernetes","text":"So far we've utilized the utilities from Docker itself in conjunction with it's native Docker Swarm orchestrator. Unfortunately, when it comes to popularity Kubernetes beats Swarm hands down . So this isn't complete if it doesn't also do graceful shutdown in Kubernetes.","title":"Java Plain (Kubernetes)"},{"location":"docker/graceful-shutdown/#in-dockerfile","text":"Our original file had to be changed, as Debian's Slim image doesn't actually contain the kill package. And we need a kill package, as we cannot instruct Kubernetes to issue a specific SIGNAL. Instead, we can issue a PreStop exec command , which we can utilise to execute a killall java -INT . The command will be specified in the Kubernetes deployment definition below. FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name \"*.java\" ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" LABEL version = \"0.1.0\" LABEL description = \"Docker image for playing with java applications in a concurrent, parallel and distributed manor.\" # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ \"/tini\" , \"-vv\" , \"-g\" , \"--\" , \"/usr/bin/dui/bin/dui\" ] ENV DATE_CHANGED = \"20180120-1525\" RUN apt-get update && apt-get install --no-install-recommends -y psmisc = 22 .* && rm -rf /var/lib/apt/lists/* COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules","title":"In Dockerfile"},{"location":"docker/graceful-shutdown/#kubernetes-deployment","text":"So here we have the image's K8s Deployment descriptor. Including the Pod's lifecycle preStop with a exec style command. You should know by now why we prefer that . apiVersion : extensions/v1beta1 kind : Deployment metadata : name : dui-deployment namespace : default labels : k8s-app : dui spec : replicas : 3 template : metadata : labels : k8s-app : dui spec : containers : - name : master image : caladreas/buming ports : - name : http containerPort : 7777 lifecycle : preStop : exec : command : [ \"killall\" , \"java\" , \"-INT\" ] terminationGracePeriodSeconds : 60","title":"Kubernetes Deployment"},{"location":"docker/graceful-shutdown/#java-spring-boot-1x","text":"This example is for Spring Boot 1.x, in time we will have an example for 2.x. This example is for the scenario of a Fat Jar with Tomcat as container [^8].","title":"Java Spring Boot (1.x)"},{"location":"docker/graceful-shutdown/#execute-example","text":"docker-compose build Execute the following command: docker run --rm -ti --name test spring-boot-graceful Exit the application/container via ctrl+c and you should see the application shutting down gracefully. 2018 -01-30 13 :35:46.327 INFO 7 --- [ Thread-3 ] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@6e5e91e4: startup date [ Tue Jan 30 13 :35:42 GMT 2018 ] ; root of context hierarchy 2018 -01-30 13 :35:46.405 INFO 7 --- [ Thread-3 ] BootGracefulApplication $GracefulShutdown : Tomcat was shutdown gracefully within the allotted time. 2018 -01-30 13 :35:46.408 INFO 7 --- [ Thread-3 ] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans on shutdown","title":"Execute example"},{"location":"docker/graceful-shutdown/#dockerfile_2","text":"FROM maven:3-jdk-8 AS build ENV MAVEN_OPTS = -Dmaven.repo.local = /usr/share/maven/repository ENV WORKDIR = /usr/src/graceful RUN mkdir $WORKDIR WORKDIR $WORKDIR COPY pom.xml $WORKDIR RUN mvn -B -e org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline COPY . $WORKSPACE RUN mvn -B -e clean verify FROM anapsix/alpine-java:8_jdk_unlimited LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/ ${ TINI_VERSION } /tini /tini RUN chmod +x /tini ENTRYPOINT [ \"/tini\" , \"-vv\" , \"-g\" , \"--\" ] ENV DATE_CHANGED = \"20180120-1525\" COPY --from = build /usr/src/graceful/target/spring-boot-graceful.jar /app.jar CMD [ \"java\" , \"-Xms256M\" , \"-Xmx480M\" , \"-Djava.security.egd=file:/dev/./urandom\" , \"-jar\" , \"/app.jar\" ]","title":"Dockerfile"},{"location":"docker/graceful-shutdown/#docker-compose-file","text":"version : \"3.5\" services : web : image : spring-boot-graceful build : . stop_signal : SIGINT","title":"Docker compose file"},{"location":"docker/graceful-shutdown/#java-handling-code","text":"package com.github.joostvdg.demo.springbootgraceful ; import org.springframework.boot.SpringApplication ; import org.springframework.boot.autoconfigure.SpringBootApplication ; import org.apache.catalina.connector.Connector ; import org.apache.tomcat.util.threads.ThreadPoolExecutor ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer ; import org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatConnectorCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory ; import org.springframework.context.ApplicationListener ; import org.springframework.context.annotation.Bean ; import org.springframework.context.event.ContextClosedEvent ; import java.util.concurrent.Executor ; import java.util.concurrent.TimeUnit ; @SpringBootApplication public class SpringBootGracefulApplication { public static void main ( String [] args ) { SpringApplication . run ( SpringBootGracefulApplication . class , args ); } @Bean public GracefulShutdown gracefulShutdown () { return new GracefulShutdown (); } @Bean public EmbeddedServletContainerCustomizer tomcatCustomizer () { return new EmbeddedServletContainerCustomizer () { @Override public void customize ( ConfigurableEmbeddedServletContainer container ) { if ( container instanceof TomcatEmbeddedServletContainerFactory ) { (( TomcatEmbeddedServletContainerFactory ) container ) . addConnectorCustomizers ( gracefulShutdown ()); } } }; } private static class GracefulShutdown implements TomcatConnectorCustomizer , ApplicationListener < ContextClosedEvent > { private static final Logger log = LoggerFactory . getLogger ( GracefulShutdown . class ); private volatile Connector connector ; @Override public void customize ( Connector connector ) { this . connector = connector ; } @Override public void onApplicationEvent ( ContextClosedEvent event ) { this . connector . pause (); Executor executor = this . connector . getProtocolHandler (). getExecutor (); if ( executor instanceof ThreadPoolExecutor ) { try { ThreadPoolExecutor threadPoolExecutor = ( ThreadPoolExecutor ) executor ; threadPoolExecutor . shutdown (); if ( ! threadPoolExecutor . awaitTermination ( 30 , TimeUnit . SECONDS )) { log . warn ( \"Tomcat thread pool did not shut down gracefully within \" + \"30 seconds. Proceeding with forceful shutdown\" ); } else { log . info ( \"Tomcat was shutdown gracefully within the allotted time.\" ); } } catch ( InterruptedException ex ) { Thread . currentThread (). interrupt (); } } } } }","title":"Java handling code"},{"location":"docker/graceful-shutdown/#example-with-docker-swarm","text":"For now there's only an example with docker swarm , in time there will also be a Kubernetes example. Now that you can create Java applications packaged neatly in Docker images that support graceful shutdown, it would be nice to utilize. A good scenario would be a microservices architecture where services can come and go, but are registered in a service registry such as Eureka . Or a membership based protocol where members interact with each other and perhaps shard data. In these cases, of course the interactions are designed to be fault tolerant and discover faulty nodes on their own. But wouldn't it be better that if you knew you're going to quit, you inform the rest? We can reuse the caladreas/buming image and make it a docker swarm stack and run the service on every node. This way, we can easily see members coming and going and reduce the time to detect failure by notifying our peers of our impeding end.","title":"Example with Docker Swarm"},{"location":"docker/graceful-shutdown/#docker-swarm-cluster","text":"Setting up a docker swarm cluster is easy, but has some requirements: virtual box 4.x+ docker-machine 1.12+ docker 17.06+ Warn Make sure this is the first and only virtualbox docker-machine VM being created/running, so that the ip range starts with 192.168.99.100 docker-machine create --driver virtualbox dui-1 docker-machine create --driver virtualbox dui-2 docker-machine create --driver virtualbox dui-3 eval \" $( docker-machine env dui-1 ) \" IP = 192 .168.99.100 docker swarm init --advertise-addr $IP TOKEN = $( docker swarm join-token -q worker ) eval \" $( docker-machine env dui-2 ) \" docker swarm join --token ${ TOKEN } ${ IP } :2377 eval \" $( docker-machine env dui-3 ) \" docker swarm join --token ${ TOKEN } ${ IP } :2377 eval \" $( docker-machine env dui-1 ) \" docker node ls","title":"Docker swarm cluster"},{"location":"docker/graceful-shutdown/#docker-swarm-network-and-multicast","text":"Unfortunately, docker swarm's swarm mode network overlay does not support multicast [ 9][ 10]. Why is this a problem? Well, the application I use to test the graceful shutdown requires this, sorry. Luckily there is a very easy solution for this, its by using Weavenet 's docker network plugin. Don't want to know about it or how you install it? Don't worry, just execute the script below. #!/usr/bin/env bash echo \"=> Prepare dui-2\" eval \" $( docker-machine env dui-2 ) \" docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo \"=> Prepare dui-3\" eval \" $( docker-machine env dui-3 ) \" docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo \"=> Prepare dui-1\" eval \" $( docker-machine env dui-1 ) \" docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 docker network create --driver = weaveworks/net-plugin:2.1.3 --opt works.weave.multicast = true --attachable dui","title":"Docker swarm network and multicast"},{"location":"docker/graceful-shutdown/#docker-stack","text":"Now to create a service that runs on every node it is the easiest to create a docker stack .","title":"Docker stack"},{"location":"docker/graceful-shutdown/#compose-file-docker-stackyml","text":"version : \"3.5\" services : dui : image : caladreas/buming build : . stop_signal : SIGINT networks : - dui deploy : mode : global networks : dui : external : true","title":"Compose file (docker-stack.yml)"},{"location":"docker/graceful-shutdown/#create-stack","text":"docker stack deploy --compose-file docker-stack.yml buming","title":"Create stack"},{"location":"docker/graceful-shutdown/#execute-example_1","text":"Now that we have a docker swarm cluster and a stack - which has a service running on every node - we can showcase the power of graceful shutdown in a cluster of dependent services. Confirm the service is running correctly on every node, first lets check our nodes. eval \" $( docker-machine env dui-1 ) \" docker node ls Which should look like this: ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS f21ilm4thxegn5xbentmss5ur * dui-1 Ready Active Leader y7475bo5uplt2b58d050b4wfd dui-2 Ready Active 6ssxola6y1i6h9p8256pi7bfv dui-3 Ready Active Then check the service. docker service ps buming_dui Which should look like this. ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 3mrpr0jg31x1 buming_dui.6ssxola6y1i6h9p8256pi7bfv dui:latest dui-3 Running Running 17 seconds ago pfubtiy4j7vo buming_dui.f21ilm4thxegn5xbentmss5ur dui:latest dui-1 Running Running 17 seconds ago f4gjnmhoe3y4 buming_dui.y7475bo5uplt2b58d050b4wfd dui:latest dui-2 Running Running 17 seconds ago Now open a second terminal window. In window one, follow the service logs: eval \" $( docker-machine env dui-1 ) \" docker service logs -f buming_dui In window two, go to a different node and stop the container. eval \" $( docker-machine env dui-2 ) \" docker ps docker stop buming_dui.y7475bo5uplt2b58d050b4wfd.pnoui2x6elrz0tvkjz51njz94 In this case, you will see the other nodes receiving a leave notice and then the node stopping. buming_dui.0.ryd8szexxku3@dui-3 | [ Server-John D. Carmack ] [ WARN ] [ 14 :19:02.604011 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = '83918f6ad817' , ip = '10.0.0.7' , name = 'Ken Thompson' } buming_dui.0.so5m14sz8ksh@dui-1 | [ Server-Alan Kay ] [ WARN ] [ 14 :19:02.602082 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = '83918f6ad817' , ip = '10.0.0.7' , name = 'Ken Thompson' } buming_dui.0.pnoui2x6elrz@dui-2 | Shutdown hook called! buming_dui.0.pnoui2x6elrz@dui-2 | [ App ] [ WARN ] [ 14 :19:02.598759 ] [ 1 ] [ ShotdownHook ] Shutting down at request of Docker buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.598858 ] [ 12 ] [ Main ] Stopping buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.601008 ] [ 12 ] [ Main ] Closing","title":"Execute example"},{"location":"docker/graceful-shutdown/#further-reading","text":"Wikipedia page on reboots Microsoft about graceful shutdown Gracefully stopping docker containers What to know about Java and shutdown hooks https://www.weave.works/blog/docker-container-networking-multicast-fast/ https://www.weave.works/docs/net/latest/install/plugin/plugin-how-it-works/ https://www.weave.works/docs/net/latest/install/plugin/plugin-v2/ https://www.auzias.net/en/docker-network-multihost/ https://forums.docker.com/t/cannot-get-zookeeper-to-work-running-in-docker-using-swarm-mode/27109 https://github.com/docker/libnetwork/issues/740","title":"Further reading"},{"location":"docker/kubernetes/","text":"Kubernetes \u00b6 Kubernetes terminology \u00b6 Kubernetes model \u00b6 Resources \u00b6 https://github.com/weaveworks/scope https://github.com/hjacobs/kube-ops-view https://coreos.com/tectonic/docs/latest/tutorials/sandbox/install.html https://github.com/kubernetes/dashboard https://blog.alexellis.io/you-need-to-know-kubernetes-and-swarm/ https://kubernetes.io/docs/reference/kubectl/cheatsheet/ https://blog.heptio.com/core-kubernetes-jazz-improv-over-orchestration-a7903ea92ca","title":"Kubernetes"},{"location":"docker/kubernetes/#kubernetes","text":"","title":"Kubernetes"},{"location":"docker/kubernetes/#kubernetes-terminology","text":"","title":"Kubernetes terminology"},{"location":"docker/kubernetes/#kubernetes-model","text":"","title":"Kubernetes model"},{"location":"docker/kubernetes/#resources","text":"https://github.com/weaveworks/scope https://github.com/hjacobs/kube-ops-view https://coreos.com/tectonic/docs/latest/tutorials/sandbox/install.html https://github.com/kubernetes/dashboard https://blog.alexellis.io/you-need-to-know-kubernetes-and-swarm/ https://kubernetes.io/docs/reference/kubectl/cheatsheet/ https://blog.heptio.com/core-kubernetes-jazz-improv-over-orchestration-a7903ea92ca","title":"Resources"},{"location":"docker/multi-stage-builds/","text":"Docker Multi-Stage Builds \u00b6","title":"Multi-Stage Builds(TBD)"},{"location":"docker/multi-stage-builds/#docker-multi-stage-builds","text":"","title":"Docker Multi-Stage Builds"},{"location":"docker/swarm/","text":"Docker Swarm (mode) \u00b6","title":"Swarm (mode) (TBD)"},{"location":"docker/swarm/#docker-swarm-mode","text":"","title":"Docker Swarm (mode)"},{"location":"java/","text":"Java \u00b6 Patterns/Anti-patterns \u00b6 Constants \u00b6 Use a class that cannot be instantiated for the use of constants. Using an interface is an anti-pattern because of what an interface implies. /** * It should also be final, else we can extend this and create a constructor allowing us to instantiate it anyway. */ public final class Constants { private Constants () {} // we should not instantiate this class public static final String HELLO = \"WORLD\" ; public static final int AMOUNT_OF_CONSTANTS = 2 ; } Other usefull things \u00b6 Random integer","title":"General"},{"location":"java/#java","text":"","title":"Java"},{"location":"java/#patternsanti-patterns","text":"","title":"Patterns/Anti-patterns"},{"location":"java/#constants","text":"Use a class that cannot be instantiated for the use of constants. Using an interface is an anti-pattern because of what an interface implies. /** * It should also be final, else we can extend this and create a constructor allowing us to instantiate it anyway. */ public final class Constants { private Constants () {} // we should not instantiate this class public static final String HELLO = \"WORLD\" ; public static final int AMOUNT_OF_CONSTANTS = 2 ; }","title":"Constants"},{"location":"java/#other-usefull-things","text":"Random integer","title":"Other usefull things"},{"location":"java/concurrency/","text":"Java Concurrency \u00b6 Terminology \u00b6 Correctness Correctness means that a class conforms to its specification . A good specification defines invariants constraining an object\u2019s state and postconditions describing the effects of its operations. 6 Thread Safe Class a class is thread-safe when it continues to behave correctly when accessed from multiple threads No set of operations performed sequentially or concurrently on instances of a thread-safe class can cause an instance to be in an invalid state. 6 Mutex Every Java object can implicitly act as a lock for purposes of synchronization; these built-in locks are called intrinsic locks or monitor locks . The lock is auto-matically acquired by the executing thread before entering a synchronized block and automatically released when control exits the synchronized block, whether by the normal control path or by throwing an exception out of the block. Intrinsic locks in Java act as mutexes (or mutual exclusion locks ), which means that at most one thread may own the lock. When thread A attempts to acquire a lock held by thread B, A must wait, or block, until B releases it. If B never releases the lock, A waits forever. 6 Reentrant locks When a thread requests a lock that is already held by another thread, the requesting thread blocks. But because intrinsic locks are reentrant , if a thread tries to acquire a lock that it already holds, the request succeeds. Reentrancy means that locks are acquired on a per-thread rather than per-invocation basis. Reentrancy is implemented by associating with each lock an acquisition count and an owning thread . When the count is zero, the lock is considered unheld. When a thread acquires a previously unheld lock, the JVM records the owner and sets the acquisition count to one. If that same thread acquires the lock again, the count is incremented, and when the owning thread exits the synchronized block , the count is decremented. When the count reaches zero, the lock is released. 6 Liveness In concurrent computing, liveness refers to a set of properties of concurrent systems, that require a system to make progress despite the fact that its concurrently executing components (\"processes\") may have to \"take turns\" in critical sections, parts of the program that cannot be simultaneously run by multiple processes. 1 Liveness guarantees are important properties in operating systems and distributed systems. 2 A liveness property cannot be violated in a finite execution of a distributed system because the \"good\" event might only theoretically occur at some time after execution ends. Eventual consistency is an example of a liveness property. 3 All properties can be expressed as the intersection of safety and liveness properties. 4 Volatile fields When a field is declared volatile , the compiler and runtime are put on notice that this variable is shared and that operations on it should not be reordered with other memory operations. Volatile variables are not cached in registers or in caches where they are hidden from other processors, so a read of a volatile variable always returns the most recent write by any thread. 6 You can use volatile variables only when all the following criteria are met: Writes to the variable do not depend on its current value, or you can ensure that only a single thread ever updates the value; The variable does not participate in invariants with other state variables; Locking is not required for any other reason while the variable is being accessed Confinement Confined objects must not escape their intended scope. An object may be confined to a class instance (such as a private class member), a lexical scope (such as a local variable), or a thread (such as an object that is passed from method to method within a thread, but not supposed to be shared across threads). Objects don\u2019t escape on their own, of course\u2014they need help from the developer, who assists by publishing the object beyond its intended scope. 6 Latch Simply put, a CountDownLatch has a counter field, which you can decrement as we require. We can then use it to block a calling thread until it\u2019s been counted down to zero. If we were doing some parallel processing, we could instantiate the CountDownLatch with the same value for the counter as a number of threads we want to work across. Then, we could just call countdown() after each thread finishes, guaranteeing that a dependent thread calling await() will block until the worker threads are finished. 7 Semaphore In computer science, a semaphore is a variable or abstract data type used to control access to a common resource by multiple processes in a concurrent system such as a multiprogramming operating system. A trivial semaphore is a plain variable that is changed (for example, incremented or decremented, or toggled) depending on programmer-defined conditions. The variable is then used as a condition to control access to some system resource. A useful way to think of a semaphore as used in the real-world systems is as a record of how many units of a particular resource are available, coupled with operations to adjust that record safely (i.e. to avoid race conditions) as units are required or become free, and, if necessary, wait until a unit of the resource becomes available. 7 Java Thread pools There are several different types of Thread pools available. FixedThreadPool : A fixed-size thread pool creates threads as tasks are submitted, up to the maximum pool size, and then attempts to keep the pool size constant (adding new threads if a thread dies due to an unexpected Exception ). CachedThreadPool : A cached thread pool has more flexibility to reap idle threads when the current size of the pool exceeds the demand for processing, and to add new threads when demand increases, but places no bounds on the size of the pool. SingleThreadExecutor : A single-threaded executor creates a single worker thread to process tasks, replacing it if it dies unexpectedly. Tasks are guaranteed to be processed sequentially according to the order imposed by the task queue (FIFO, LIFO, priority order). 4 ScheduledThreadPool : A fixed-size thread pool that supports delayed and periodic task execution, similar to Timer. 6 Interrupt Thread provides the interrupt method for interrupting a thread and for querying whether a thread has been interrupted. Each thread has a boolean property that represents its interrupted status; interrupting a thread sets this status. Interruption is a cooperative mechanism. One thread cannot force another to stop what it is doing and do something else; when thread A interrupts thread B, A is merely requesting that B stop what it is doing when it gets to a convenient stopping point\u2014if it feels like it. When your code calls a method that throws InterruptedException , then your method is a blocking method too, and must have a plan for responding to inter- ruption. For library code, there are basically two choices: Propagate the InterruptedException : This is often the most sensible policy if you can get away with it: just propagate the InterruptedException to your caller. This could involve not catching InterruptedException , or catching it and throwing it again after performing some brief activity-specific cleanup. Restore the interrupt : Sometimes you cannot throw InterruptedException , for instance when your code is part of a Runnable . In these situations, you must catch InterruptedException and restore the interrupted status by calling interrupt on the current thread, so that code higher up the call stack can see that an interrupt was issued. 6 Patterns \u00b6 Queue & Deque \u00b6 Queue & Deque A Deque is a double-ended queue that allows efficient insertion and removal from both the head and the tail. Implementations include ArrayDeque and LinkedBlockingDeque . Just as blocking queues lend themselves to the producer-consumer pattern, deques lend themselves to a related pattern called work stealing. A producer-consumer design has one shared work queue for all consumers; in a work stealing design, every consumer has its own deque. If a consumer exhausts the work in its own deque, it can steal work from the tail of someone else\u2019s deque. Work stealing can be more scalable than a traditional producer-consumer design because workers don\u2019t contend for a shared work queue; most of the time they access only their own deque, reducing contention. When a worker has to access another\u2019s queue, it does so from the tail rather than the head, further reducing contention. 6 Monitor pattern \u00b6 Resources \u00b6 concurrency-patterns-monitor-object Wikipedia article on monitor pattern e-zest blog on monitor pattern java Examples \u00b6 Confinement \u00b6 PersonSet (below) illustrates how confinement and locking can work together to make a class thread-safe even when its component state variables are not. The state of PersonSet is managed by a HashSet , which is not thread-safe. But because mySet is private and not allowed to escape, the HashSet is confined to the PersonSet. The only code paths that can access mySet are addPerson and containsPerson , and each of these acquires the lock on the PersonSet. All its state is guarded by its intrinsic lock, making PersonSet thread-safe. 6 public class PersonSet { @GuardedBy ( \"this\" ) private final Set < Person > mySet = new HashSet < Person > (); public synchronized void addPerson ( Person p ) { mySet . add ( p ); } public synchronized boolean containsPerson ( Person p ) { return mySet . contains ( p ); } } HTTP Call Counter \u00b6 Unsafe Counter \u00b6 public class UnsafeCounter { private long count = 0 ; public long getCount () { return count ; } public void service () { // do some work try { int pseudoRandom = new Random (). nextInt ( 20 ); Thread . sleep ( pseudoRandom * 100 ); ++ count ; } catch ( InterruptedException e ) { e . printStackTrace (); } } } Safe Counter \u00b6 public class SafeCounter { private final AtomicLong count = new AtomicLong ( 0 ); public long getCount () { return count . get (); } public void service () { try { int pseudoRandom = new Random (). nextInt ( 20 ); Thread . sleep ( pseudoRandom * 100 ); count . incrementAndGet (); } catch ( InterruptedException e ) { e . printStackTrace (); } } } Caller \u00b6 public class Server { public void start ( int port ) throws Exception { HttpServer server = HttpServer . create ( new InetSocketAddress ( port ), 0 ); UnsafeCounter unsafeCounter = new UnsafeCounter (); SafeCounter safeCounter = new SafeCounter (); server . createContext ( \"/test\" , new MyTestHandler ( unsafeCounter , safeCounter )); server . createContext ( \"/\" , new MyHandler ( unsafeCounter , safeCounter )); Executor executor = Executors . newFixedThreadPool ( 5 ); server . setExecutor ( executor ); // creates a default executor server . start (); } static class MyTestHandler implements HttpHandler { private UnsafeCounter unsafeCounter ; private SafeCounter safeCounter ; public MyTestHandler ( UnsafeCounter unsafeCounter , SafeCounter safeCounter ) { this . unsafeCounter = unsafeCounter ; this . safeCounter = safeCounter ; } @Override public void handle ( HttpExchange t ) throws IOException { safeCounter . service (); unsafeCounter . service (); System . out . println ( \"Got a request on /test, counts so far:\" + unsafeCounter . getCount () + \"::\" + safeCounter . getCount ()); String response = \"This is the response\" ; t . sendResponseHeaders ( 200 , response . length ()); try ( OutputStream os = t . getResponseBody ()) { os . write ( response . getBytes ()); } } } } Outcome \u00b6 Starting server on port 8080 Server started Got a request on /, counts so far:2::1 Got a request on /, counts so far:6::2 Got a request on /, counts so far:6::3 Got a request on /, counts so far:6::4 Got a request on /, counts so far:6::5 Got a request on /, counts so far:6::6 Lamport, L. (1977). \"Proving the Correctness of Multiprocess Programs\". IEEE Transactions on Software Engineering (2): 125\u2013143. doi: 10.1109/TSE.1977.229904 . \u21a9 Lu\u00eds Rodrigues, Christian Cachin; Rachid Guerraoui (2010). Introduction to reliable and secure distributed programming (2. ed.). Berlin: Springer Berlin. pp. 22\u201324. ISBN 978-3-642-15259-7 . \u21a9 Bailis, P.; Ghodsi, A. (2013). \"Eventual Consistency Today: Limitations, Extensions, and Beyond\". Queue. 11 (3): 20. doi: 10.1145/2460276.2462076 . \u21a9 Alpern, B.; Schneider, F. B. (1987). \"Recognizing safety and liveness\". Distributed Computing. 2 (3): 117. doi: 10.1007/BF01782772 . \u21a9 Liveness article Wikipedia \u21a9 Java Concurrency in Practice / Brian Goetz, with Tim Peierls. . . [et al.] Concurrency in Practice \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 Baeldung tutorial on CountDownLatch \u21a9 \u21a9 Wikipedia article on Semaphore \u21a9","title":"Java Concurrency"},{"location":"java/concurrency/#java-concurrency","text":"","title":"Java Concurrency"},{"location":"java/concurrency/#terminology","text":"Correctness Correctness means that a class conforms to its specification . A good specification defines invariants constraining an object\u2019s state and postconditions describing the effects of its operations. 6 Thread Safe Class a class is thread-safe when it continues to behave correctly when accessed from multiple threads No set of operations performed sequentially or concurrently on instances of a thread-safe class can cause an instance to be in an invalid state. 6 Mutex Every Java object can implicitly act as a lock for purposes of synchronization; these built-in locks are called intrinsic locks or monitor locks . The lock is auto-matically acquired by the executing thread before entering a synchronized block and automatically released when control exits the synchronized block, whether by the normal control path or by throwing an exception out of the block. Intrinsic locks in Java act as mutexes (or mutual exclusion locks ), which means that at most one thread may own the lock. When thread A attempts to acquire a lock held by thread B, A must wait, or block, until B releases it. If B never releases the lock, A waits forever. 6 Reentrant locks When a thread requests a lock that is already held by another thread, the requesting thread blocks. But because intrinsic locks are reentrant , if a thread tries to acquire a lock that it already holds, the request succeeds. Reentrancy means that locks are acquired on a per-thread rather than per-invocation basis. Reentrancy is implemented by associating with each lock an acquisition count and an owning thread . When the count is zero, the lock is considered unheld. When a thread acquires a previously unheld lock, the JVM records the owner and sets the acquisition count to one. If that same thread acquires the lock again, the count is incremented, and when the owning thread exits the synchronized block , the count is decremented. When the count reaches zero, the lock is released. 6 Liveness In concurrent computing, liveness refers to a set of properties of concurrent systems, that require a system to make progress despite the fact that its concurrently executing components (\"processes\") may have to \"take turns\" in critical sections, parts of the program that cannot be simultaneously run by multiple processes. 1 Liveness guarantees are important properties in operating systems and distributed systems. 2 A liveness property cannot be violated in a finite execution of a distributed system because the \"good\" event might only theoretically occur at some time after execution ends. Eventual consistency is an example of a liveness property. 3 All properties can be expressed as the intersection of safety and liveness properties. 4 Volatile fields When a field is declared volatile , the compiler and runtime are put on notice that this variable is shared and that operations on it should not be reordered with other memory operations. Volatile variables are not cached in registers or in caches where they are hidden from other processors, so a read of a volatile variable always returns the most recent write by any thread. 6 You can use volatile variables only when all the following criteria are met: Writes to the variable do not depend on its current value, or you can ensure that only a single thread ever updates the value; The variable does not participate in invariants with other state variables; Locking is not required for any other reason while the variable is being accessed Confinement Confined objects must not escape their intended scope. An object may be confined to a class instance (such as a private class member), a lexical scope (such as a local variable), or a thread (such as an object that is passed from method to method within a thread, but not supposed to be shared across threads). Objects don\u2019t escape on their own, of course\u2014they need help from the developer, who assists by publishing the object beyond its intended scope. 6 Latch Simply put, a CountDownLatch has a counter field, which you can decrement as we require. We can then use it to block a calling thread until it\u2019s been counted down to zero. If we were doing some parallel processing, we could instantiate the CountDownLatch with the same value for the counter as a number of threads we want to work across. Then, we could just call countdown() after each thread finishes, guaranteeing that a dependent thread calling await() will block until the worker threads are finished. 7 Semaphore In computer science, a semaphore is a variable or abstract data type used to control access to a common resource by multiple processes in a concurrent system such as a multiprogramming operating system. A trivial semaphore is a plain variable that is changed (for example, incremented or decremented, or toggled) depending on programmer-defined conditions. The variable is then used as a condition to control access to some system resource. A useful way to think of a semaphore as used in the real-world systems is as a record of how many units of a particular resource are available, coupled with operations to adjust that record safely (i.e. to avoid race conditions) as units are required or become free, and, if necessary, wait until a unit of the resource becomes available. 7 Java Thread pools There are several different types of Thread pools available. FixedThreadPool : A fixed-size thread pool creates threads as tasks are submitted, up to the maximum pool size, and then attempts to keep the pool size constant (adding new threads if a thread dies due to an unexpected Exception ). CachedThreadPool : A cached thread pool has more flexibility to reap idle threads when the current size of the pool exceeds the demand for processing, and to add new threads when demand increases, but places no bounds on the size of the pool. SingleThreadExecutor : A single-threaded executor creates a single worker thread to process tasks, replacing it if it dies unexpectedly. Tasks are guaranteed to be processed sequentially according to the order imposed by the task queue (FIFO, LIFO, priority order). 4 ScheduledThreadPool : A fixed-size thread pool that supports delayed and periodic task execution, similar to Timer. 6 Interrupt Thread provides the interrupt method for interrupting a thread and for querying whether a thread has been interrupted. Each thread has a boolean property that represents its interrupted status; interrupting a thread sets this status. Interruption is a cooperative mechanism. One thread cannot force another to stop what it is doing and do something else; when thread A interrupts thread B, A is merely requesting that B stop what it is doing when it gets to a convenient stopping point\u2014if it feels like it. When your code calls a method that throws InterruptedException , then your method is a blocking method too, and must have a plan for responding to inter- ruption. For library code, there are basically two choices: Propagate the InterruptedException : This is often the most sensible policy if you can get away with it: just propagate the InterruptedException to your caller. This could involve not catching InterruptedException , or catching it and throwing it again after performing some brief activity-specific cleanup. Restore the interrupt : Sometimes you cannot throw InterruptedException , for instance when your code is part of a Runnable . In these situations, you must catch InterruptedException and restore the interrupted status by calling interrupt on the current thread, so that code higher up the call stack can see that an interrupt was issued. 6","title":"Terminology"},{"location":"java/concurrency/#patterns","text":"","title":"Patterns"},{"location":"java/concurrency/#queue-deque","text":"Queue & Deque A Deque is a double-ended queue that allows efficient insertion and removal from both the head and the tail. Implementations include ArrayDeque and LinkedBlockingDeque . Just as blocking queues lend themselves to the producer-consumer pattern, deques lend themselves to a related pattern called work stealing. A producer-consumer design has one shared work queue for all consumers; in a work stealing design, every consumer has its own deque. If a consumer exhausts the work in its own deque, it can steal work from the tail of someone else\u2019s deque. Work stealing can be more scalable than a traditional producer-consumer design because workers don\u2019t contend for a shared work queue; most of the time they access only their own deque, reducing contention. When a worker has to access another\u2019s queue, it does so from the tail rather than the head, further reducing contention. 6","title":"Queue &amp; Deque"},{"location":"java/concurrency/#monitor-pattern","text":"","title":"Monitor pattern"},{"location":"java/concurrency/#resources","text":"concurrency-patterns-monitor-object Wikipedia article on monitor pattern e-zest blog on monitor pattern java","title":"Resources"},{"location":"java/concurrency/#examples","text":"","title":"Examples"},{"location":"java/concurrency/#confinement","text":"PersonSet (below) illustrates how confinement and locking can work together to make a class thread-safe even when its component state variables are not. The state of PersonSet is managed by a HashSet , which is not thread-safe. But because mySet is private and not allowed to escape, the HashSet is confined to the PersonSet. The only code paths that can access mySet are addPerson and containsPerson , and each of these acquires the lock on the PersonSet. All its state is guarded by its intrinsic lock, making PersonSet thread-safe. 6 public class PersonSet { @GuardedBy ( \"this\" ) private final Set < Person > mySet = new HashSet < Person > (); public synchronized void addPerson ( Person p ) { mySet . add ( p ); } public synchronized boolean containsPerson ( Person p ) { return mySet . contains ( p ); } }","title":"Confinement"},{"location":"java/concurrency/#http-call-counter","text":"","title":"HTTP Call Counter"},{"location":"java/concurrency/#unsafe-counter","text":"public class UnsafeCounter { private long count = 0 ; public long getCount () { return count ; } public void service () { // do some work try { int pseudoRandom = new Random (). nextInt ( 20 ); Thread . sleep ( pseudoRandom * 100 ); ++ count ; } catch ( InterruptedException e ) { e . printStackTrace (); } } }","title":"Unsafe Counter"},{"location":"java/concurrency/#safe-counter","text":"public class SafeCounter { private final AtomicLong count = new AtomicLong ( 0 ); public long getCount () { return count . get (); } public void service () { try { int pseudoRandom = new Random (). nextInt ( 20 ); Thread . sleep ( pseudoRandom * 100 ); count . incrementAndGet (); } catch ( InterruptedException e ) { e . printStackTrace (); } } }","title":"Safe Counter"},{"location":"java/concurrency/#caller","text":"public class Server { public void start ( int port ) throws Exception { HttpServer server = HttpServer . create ( new InetSocketAddress ( port ), 0 ); UnsafeCounter unsafeCounter = new UnsafeCounter (); SafeCounter safeCounter = new SafeCounter (); server . createContext ( \"/test\" , new MyTestHandler ( unsafeCounter , safeCounter )); server . createContext ( \"/\" , new MyHandler ( unsafeCounter , safeCounter )); Executor executor = Executors . newFixedThreadPool ( 5 ); server . setExecutor ( executor ); // creates a default executor server . start (); } static class MyTestHandler implements HttpHandler { private UnsafeCounter unsafeCounter ; private SafeCounter safeCounter ; public MyTestHandler ( UnsafeCounter unsafeCounter , SafeCounter safeCounter ) { this . unsafeCounter = unsafeCounter ; this . safeCounter = safeCounter ; } @Override public void handle ( HttpExchange t ) throws IOException { safeCounter . service (); unsafeCounter . service (); System . out . println ( \"Got a request on /test, counts so far:\" + unsafeCounter . getCount () + \"::\" + safeCounter . getCount ()); String response = \"This is the response\" ; t . sendResponseHeaders ( 200 , response . length ()); try ( OutputStream os = t . getResponseBody ()) { os . write ( response . getBytes ()); } } } }","title":"Caller"},{"location":"java/concurrency/#outcome","text":"Starting server on port 8080 Server started Got a request on /, counts so far:2::1 Got a request on /, counts so far:6::2 Got a request on /, counts so far:6::3 Got a request on /, counts so far:6::4 Got a request on /, counts so far:6::5 Got a request on /, counts so far:6::6 Lamport, L. (1977). \"Proving the Correctness of Multiprocess Programs\". IEEE Transactions on Software Engineering (2): 125\u2013143. doi: 10.1109/TSE.1977.229904 . \u21a9 Lu\u00eds Rodrigues, Christian Cachin; Rachid Guerraoui (2010). Introduction to reliable and secure distributed programming (2. ed.). Berlin: Springer Berlin. pp. 22\u201324. ISBN 978-3-642-15259-7 . \u21a9 Bailis, P.; Ghodsi, A. (2013). \"Eventual Consistency Today: Limitations, Extensions, and Beyond\". Queue. 11 (3): 20. doi: 10.1145/2460276.2462076 . \u21a9 Alpern, B.; Schneider, F. B. (1987). \"Recognizing safety and liveness\". Distributed Computing. 2 (3): 117. doi: 10.1007/BF01782772 . \u21a9 Liveness article Wikipedia \u21a9 Java Concurrency in Practice / Brian Goetz, with Tim Peierls. . . [et al.] Concurrency in Practice \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 Baeldung tutorial on CountDownLatch \u21a9 \u21a9 Wikipedia article on Semaphore \u21a9","title":"Outcome"},{"location":"java/ecosystem/","text":"Java Ecosysten \u00b6","title":"Java Ecosysten"},{"location":"java/ecosystem/#java-ecosysten","text":"","title":"Java Ecosysten"},{"location":"java/java-modules/","text":"Java Modules \u00b6","title":"Java Modules"},{"location":"java/java-modules/#java-modules","text":"","title":"Java Modules"},{"location":"java/networking/","text":"Java Networking \u00b6 General Remarks \u00b6 Network API works for IPv4 (32-bit adrressing) and IPv6 (128-bit addressing) Java only supports TCP/IP and UDP/IP Java proxy system params \u00b6 socksProxyHost socksProxyPort http.proxySet http.proxyHost http.proxyPort https.proxySet https.proxyHost https.proxyPort ftpProxySet ftpProxyHost ftpProxyPort gopherProxySet gopherProxyHost gopherProxyPort Special IPv4 segments \u00b6 Internal \u00b6 10. . .* 172.17. . - 172.31. . 192.168. . Local \u00b6 127. . .* Broadcast \u00b6 255.255.255.255 > Packets sent to this address are received by all nodes on the local network, though they are not routed beyond the local network Special IPv6 segments \u00b6 Local \u00b6 0:0:0:0:0:0:0:1 (or ::::::1 or ::1)","title":"Java Networking"},{"location":"java/networking/#java-networking","text":"","title":"Java Networking"},{"location":"java/networking/#general-remarks","text":"Network API works for IPv4 (32-bit adrressing) and IPv6 (128-bit addressing) Java only supports TCP/IP and UDP/IP","title":"General Remarks"},{"location":"java/networking/#java-proxy-system-params","text":"socksProxyHost socksProxyPort http.proxySet http.proxyHost http.proxyPort https.proxySet https.proxyHost https.proxyPort ftpProxySet ftpProxyHost ftpProxyPort gopherProxySet gopherProxyHost gopherProxyPort","title":"Java proxy system params"},{"location":"java/networking/#special-ipv4-segments","text":"","title":"Special IPv4 segments"},{"location":"java/networking/#internal","text":"10. . .* 172.17. . - 172.31. . 192.168. .","title":"Internal"},{"location":"java/networking/#local","text":"127. . .*","title":"Local"},{"location":"java/networking/#broadcast","text":"255.255.255.255 > Packets sent to this address are received by all nodes on the local network, though they are not routed beyond the local network","title":"Broadcast"},{"location":"java/networking/#special-ipv6-segments","text":"","title":"Special IPv6 segments"},{"location":"java/networking/#local_1","text":"0:0:0:0:0:0:0:1 (or ::::::1 or ::1)","title":"Local"},{"location":"java/streams/","text":"Java Streams \u00b6 Try-with-Resources \u00b6 try with resources can be used with any object that implements the Closeable interface, which includes almost every object you need to dispose. So far, JavaMail Transport objects are the only exceptions I\u2019ve encountered. Those still need to be disposed of explicitly. public class Main { public static void main ( String [] args ) { try ( OutputStream out = new FileOutputStream ( \"/tmp/data.txt\" )) { // work with the output stream... } catch ( IOException ex ) { System . err . println ( ex . getMessage ()); } } }","title":"Java Streams"},{"location":"java/streams/#java-streams","text":"","title":"Java Streams"},{"location":"java/streams/#try-with-resources","text":"try with resources can be used with any object that implements the Closeable interface, which includes almost every object you need to dispose. So far, JavaMail Transport objects are the only exceptions I\u2019ve encountered. Those still need to be disposed of explicitly. public class Main { public static void main ( String [] args ) { try ( OutputStream out = new FileOutputStream ( \"/tmp/data.txt\" )) { // work with the output stream... } catch ( IOException ex ) { System . err . println ( ex . getMessage ()); } } }","title":"Try-with-Resources"},{"location":"java/spring/boot/","text":"","title":"Boot"},{"location":"jenkins/","text":"Jenkins \u00b6 Cloudbees Study Guide Tuning \u00b6 Please read the following articles from Cloudbees: Prepare-Jenkins-for-support tuning-jenkins-gc-responsiveness-and-stability After-moving-a-job-symlinks-for-folders-became-actual-folders How-to-disable-the-weather-column-to-resolve-instance-slowness Accessing-graphs-on-a-Build-History-page-can-cause-Jenkins-to-become-unresponsive AutoBrowser-Feature-Can-Cause-Performance-Issues Disk-Space-Issue-after-upgrading-Branch-API-plugin JVM-Memory-settings-best-practice Pipeline as code \u00b6 The default interaction model with Jenkins, historically, has been very web UI driven, requiring users to manually create jobs, then manually fill in the details through a web browser. This requires additional effort to create and manage jobs to test and build multiple projects, it also keeps the configuration of a job to build/test/deploy separate from the actual code being built/tested/deployed. This prevents users from applying their existing CI/CD best practices to the job configurations themselves. With the introduction of the Pipeline plugin, users now can implement a project\u2019s entire build/test/deploy pipeline in a Jenkinsfile and store that alongside their code, treating their pipeline as another piece of code checked into source control. We will dive into several things that come into play when writing Jenkins pipelines. Kind of Pipeline jobs Info about Pipeline DSL (a groovy DSL) Reuse pipeline DSL scripts Things to keep in mind Do's and Don't Resources \u00b6 Pipeline Steps Pipeline Solution Pipeline as Code Dzone RefCard Type of pipeline jobs \u00b6 Pipeline (inline) Pipeline (from SCM) Multi-Branch Pipeline GitHub Organization BitBucket Team/Project Gitea Organization GitLab Integration Danger When using the stash function keep in mind that the copying goes from where you are now to the master. When you unstash, it will copy the files from the master to where you are building. When your pipeline runs on a node and you stash and then unstash, it will copy the files from the node to the master and then back to the node. This can have a severe penalty on the performance of your pipeline when you are copying over a network. API \u00b6 Jenkins has an extensive API allowing you to retrieve a lot of information from the server. Plugin \u00b6 For this way you of course have to know how to write a plugin. There are some usefull resources to get started: * https://github.com/joostvdg/hello-world-jenkins-pipeline-plugin * https://wiki.jenkins-ci.org/display/JENKINS/Plugin+tutorial * https://jenkins.io/blog/2016/05/25/update-plugin-for-pipeline/ Do's and Don't \u00b6 Aside from the Do's and Don'ts from Cloudbees, there are some we want to share. This changes the requirement for the component identifier property, as a job may only match a single group and a job listing in a group can only match a single. Thus the easiest way to make sure everything will stay unique (template names probably don\u2019t), is to make the component identifier property unique per file - let it use the name of the project. Other Resources \u00b6 Configuration As Code Jenkins CLI - for managing Plugins Jenkinsfile Runner CICD With Jenkins On Docker Compose Jenkins Helm Chart Jenkins Operator Jenkins X CloudBees Jenkins Distribution CloudBees Jenkins X Distribution","title":"Intro"},{"location":"jenkins/#jenkins","text":"Cloudbees Study Guide","title":"Jenkins"},{"location":"jenkins/#tuning","text":"Please read the following articles from Cloudbees: Prepare-Jenkins-for-support tuning-jenkins-gc-responsiveness-and-stability After-moving-a-job-symlinks-for-folders-became-actual-folders How-to-disable-the-weather-column-to-resolve-instance-slowness Accessing-graphs-on-a-Build-History-page-can-cause-Jenkins-to-become-unresponsive AutoBrowser-Feature-Can-Cause-Performance-Issues Disk-Space-Issue-after-upgrading-Branch-API-plugin JVM-Memory-settings-best-practice","title":"Tuning"},{"location":"jenkins/#pipeline-as-code","text":"The default interaction model with Jenkins, historically, has been very web UI driven, requiring users to manually create jobs, then manually fill in the details through a web browser. This requires additional effort to create and manage jobs to test and build multiple projects, it also keeps the configuration of a job to build/test/deploy separate from the actual code being built/tested/deployed. This prevents users from applying their existing CI/CD best practices to the job configurations themselves. With the introduction of the Pipeline plugin, users now can implement a project\u2019s entire build/test/deploy pipeline in a Jenkinsfile and store that alongside their code, treating their pipeline as another piece of code checked into source control. We will dive into several things that come into play when writing Jenkins pipelines. Kind of Pipeline jobs Info about Pipeline DSL (a groovy DSL) Reuse pipeline DSL scripts Things to keep in mind Do's and Don't","title":"Pipeline as code"},{"location":"jenkins/#resources","text":"Pipeline Steps Pipeline Solution Pipeline as Code Dzone RefCard","title":"Resources"},{"location":"jenkins/#type-of-pipeline-jobs","text":"Pipeline (inline) Pipeline (from SCM) Multi-Branch Pipeline GitHub Organization BitBucket Team/Project Gitea Organization GitLab Integration Danger When using the stash function keep in mind that the copying goes from where you are now to the master. When you unstash, it will copy the files from the master to where you are building. When your pipeline runs on a node and you stash and then unstash, it will copy the files from the node to the master and then back to the node. This can have a severe penalty on the performance of your pipeline when you are copying over a network.","title":"Type of pipeline jobs"},{"location":"jenkins/#api","text":"Jenkins has an extensive API allowing you to retrieve a lot of information from the server.","title":"API"},{"location":"jenkins/#plugin","text":"For this way you of course have to know how to write a plugin. There are some usefull resources to get started: * https://github.com/joostvdg/hello-world-jenkins-pipeline-plugin * https://wiki.jenkins-ci.org/display/JENKINS/Plugin+tutorial * https://jenkins.io/blog/2016/05/25/update-plugin-for-pipeline/","title":"Plugin"},{"location":"jenkins/#dos-and-dont","text":"Aside from the Do's and Don'ts from Cloudbees, there are some we want to share. This changes the requirement for the component identifier property, as a job may only match a single group and a job listing in a group can only match a single. Thus the easiest way to make sure everything will stay unique (template names probably don\u2019t), is to make the component identifier property unique per file - let it use the name of the project.","title":"Do's and Don't"},{"location":"jenkins/#other-resources","text":"Configuration As Code Jenkins CLI - for managing Plugins Jenkinsfile Runner CICD With Jenkins On Docker Compose Jenkins Helm Chart Jenkins Operator Jenkins X CloudBees Jenkins Distribution CloudBees Jenkins X Distribution","title":"Other Resources"},{"location":"jenkins/java-gradle/","text":"","title":"Java gradle"},{"location":"jenkins/plugins/","text":"","title":"Plugins"},{"location":"jenkins-jobs/jenkins-jobs-builder/","text":"Jenkins Job Builder \u00b6 The configuration setup of Jenkins Job Builder is composed of two main categories. Basic configuration and job configuration. Job configuration can be further split into several sub categories. Basic Configuration \u00b6 In the basic configuration you will have to specify how the Jenkins Job Builder CLI can connect to the Jenkins instance you want to configure and how it should act. To use such a configuration file, you add --conf to the CLI command. Example: localhost.ini [job_builder] ignore_cache=True keep_descriptions=False include_path=.:scripts:~/git/ recursive=False exclude=.*:manual:./development allow_duplicates=False [jenkins] #user=jenkins #password= url=http://localhost:8080/ For more information see http://docs.openstack.org/infra/jenkins-job-builder/installation.html . Job Configuration \u00b6 The configuration for configuring the jobs consists of several distinct parts which can all be in the same file or can be distributed in their own respected files. These different parts can also be split into two different categories, those that are strictly linked within the configuration - via template matching - and those that are separate. Separate: * Macro\u2019s * Global defaults * Job configuration defaults * External configuration files Linked: * Templates * Groups * Projects * Job definitions Here\u2019s a schematic representation on how they are linked. Exampe in YAML config: - job-template : name : '{name}-{configComponentId}-ci' description : 'CI Job of {configComponentId}' << : *config_job_defaults builders : - shell : 'jenkins-jobs test -r global/:definitions/ -o compiled/' - job-template : name : '{name}-{configComponentId}-execute' description : 'Executor Job of {configComponentId}' << : *config_job_defaults builders : - shell : 'jenkins-jobs --conf configuration/localhost.ini update definitions/' - job-group : name : '{name}-config' gitlab-user : 'jvandergriendt' jobs : - '{name}-{configComponentId}-ci' : - '{name}-{configComponentId}-execute' : - project : name : RnD-Config jobs : - '{name}-config' : configComponentId : JenkinsJobDefinitions The above will result in the following jobs: RnD-Config-JenkinsJobDefinitions-ci RnD-Config-JenkinsJobDefinitions-execute Macro\u2019s \u00b6 Macro\u2019s are what the name implies, a group of related commands which can be invoked by the group. In Jenkins Job Builder this means you can define specific configurations for a component type (e.g. builders, paramters, publishes etc). A component has a name and a macro name. In general the component name is plural and the macro name is singular. As can be seen in the examples below. Here\u2019s an example: # The 'add' macro takes a 'number' parameter and will creates a # job which prints 'Adding ' followed by the 'number' parameter: - builder : name : add builders : - shell : \"echo Adding {number}\" # A specialized macro 'addtwo' reusing the 'add' macro but with # a 'number' parameter hardcoded to 'two': - builder : name : addtwo builders : - add : number : \"two\" # Glue to have Jenkins Job Builder to expand this YAML example: - job : name : \"testingjob\" builders : # The specialized macro: - addtwo # Generic macro call with a parameter - add : number : \"ZERO\" # Generic macro called without a parameter. Never do this! # See below for the resulting wrong output :( - add To expand the schematic representation, you will get the following. - builder : name : test builders : - shell : \"jenkins-jobs test -r global/:definitions/ -o compiled/\" - builder : name : update builders : - shell : \"jenkins-jobs --conf config.ini update -r global/:definitions/\" - job-template : name : '{name}-{configComponentId}-ci' << : *config_job_defaults builders : - test - job-template : name : '{name}-{configComponentId}-update' << : *config_job_defaults builders : - update Global defaults \u00b6 Global defaults are defaults that should be global for the jobs you configure for a certain environment. It is the job counterpart of the basic configuration, usually containing variables for the specific environment. For example, url\u2019s, credential id\u2019s, JDK\u2019s etc. Example: global-defaults-localhost.yaml - defaults : name : 'global' flusso-gitlab-url : https://gitlab.flusso.nl nexus-npm-url : http://localhost:8081/nexus/content/repositories/npm-internal default-jdk : JDK 1.8 jenkinsJobsDefinitionJobName : RnD-Config-JenkinsJobDefinitions-ci credentialsId : '4f0dfb96-a7b1-421c-a4ea-b6a154f91b08' Job configuration defaults \u00b6 Job configuration defaults are nothing specific on their own. It refers to using a build in structure from YAML to create basic building blocks to be used by other configuration parts, usually the Templates. Example (definition): - config_job_defaults : &config_job_defaults name : 'config_job_defaults' project-type : freestyle disabled : false logrotate : daysToKeep : 7 numToKeep : 5 artifactDaysToKeep : -1 artifactNumToKeep : -1 jdk : '{default-jdk}' Example (usage): - job-template : name : '{name}-{configComponentId}-ci' << : *config_job_defaults Templates \u00b6 Templates are used to define job templates. You define the entirety of the job using global defaults, configuration defaults and where useful refer to placeholders to be filled in by the other downstream configuration items. You can configure almost every plugin that is available for Jenkins, these are divided in subdivisions which reflect the Jenkins\u2019 job definition sections. For these subdivision and the available plugins see: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#modules For those plugins that are not supported, you can include the raw XML generated by the plugin. For how to do this, see: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#raw-config Example: - job-template : name : '{name}-{configComponentId}-ci' display-name : '{name}-{configComponentId}-ci' description : 'CI Job of {configComponentId}' << : *config_job_defaults builders : - shell : 'jenkins-jobs test -r global/:definitions/ -o compiled/' publishers : - archive : artifacts : '{filesToArchive_1}' fingerprint : true - archive : artifacts : '{filesToArchive_2}' fingerprint : true - email : notify-every-unstable-build : true send-to-individuals : true Groups \u00b6 Groups are used to group together related components that require the same set of jobs. Where you can also specify a similar set of properties, for example, a different JDK to be used. The name property is mandatory and will be used to match Job definitions. The jobs property is also mandatory and will be used to match Templates for which a Job will be generated per matching Job definition. Example - job-group : name : '{name}-gulp' gitlab-user : 'jvandergriendt' artifactId : '{gulpComponentId}' jobs : - '{name}-{gulpComponentId}-ci' : - '{name}-{gulpComponentId}-version' : - '{name}-{gulpComponentId}-sonar' : - '{name}-{gulpComponentId}-publish' : - '{name}-{gulpComponentId}-deploy-prep' : - '{name}-{gulpComponentId}-deploy' : - '{name}-{gulpComponentId}-acceptance' : Projects \u00b6 Projects are used to list the actual Job definitions, which via grouping and Templates get generated, and can obviously be used to define jobs for a specific project. The name property is mandatory and will be passed along with a Job definition and is generally used to tie job definitions to Groups. - project : name : RnD-Maven jobs : - '{name}-keep' : gulpComponentId : keep-backend displayName : Keep-Backend Job definitions \u00b6 Job definitions are what is all about. Although they are part of the Project configuration item I treat them separately. You list the jobs under a Project and start with the name of the Group it belongs to. After that, you should define at least a name component to be able to differentiate the different jobs you want. As can be seen in the above examples with the gulpComponentId. External configuration files Sometimes you run into the situation you want to use a multi-line configuration for a plugin, or a set of commands. Or, used at in different configurations or templates. Then you run into the situation that it is very difficult to manage in them neatly inside YAML configuration files. For this situation you are able to simply include a text file, via a native YAML construct. See: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#module-jenkins_jobs.local_yaml For example - job : name : test-job-include-raw-1 builders : - shell : !include-raw include-raw001-hello-world.sh - shell : !include-raw include-raw001-vars.sh Usage \u00b6 The information to how you use the tool is very well explained in the documentation. See http://docs.openstack.org/infra/jenkins-job-builder/installation.html#running Automated maintenance If all the jobs you can administer are done via Jenkins Job Builder, you can start to automate the maintenance of these jobs. Simply make jobs that poll/push on the code base where you have your Jenkins Job Builder configuration files. Example - config_job_defaults : &config_job_defaults name : 'config_job_defaults' project-type : freestyle disabled : false logrotate : daysToKeep : 7 numToKeep : 5 artifactDaysToKeep : -1 artifactNumToKeep : -1 jdk : '{default-jdk}' triggers : - pollscm : \"H/15 * * * *\" scm : - git : url : '{flusso-gitlab-url}/{gitlab-user}/{componentGitName}.git' credentials-id : '{credentialsId}' publishers : - email : notify-every-unstable-build : true send-to-individuals : true - job-template : name : '{name}-{configComponentId}-ci' display-name : '{name}-{configComponentId}-ci' description : 'CI Job of {configComponentId}' << : *config_job_defaults builders : - shell : 'jenkins-jobs test -r global/:definitions/ -o compiled/' publishers : - archive : artifacts : '{filesToArchive_1}' fingerprint : true - archive : artifacts : '{filesToArchive_2}' fingerprint : true - email : notify-every-unstable-build : true send-to-individuals : true - job-template : name : '{name}-{configComponentId}-x' display-name : '{name}-{configComponentId}-execute' description : 'Executor Job of {configComponentId}, it will execute the update and delete old command' << : *config_job_defaults builders : - shell : 'jenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/' - job-group : name : '{name}-config' gitlab-user : 'jvandergriendt' jobs : - '{name}-{configComponentId}-ci' : - '{name}-{configComponentId}-x' : - project : name : RnD-Config jobs : - '{name}-config' : configComponentId : JenkinsJobDefinitions componentGitName : jenkins-job-definitions filesToArchive_1 : scripts/*.sh filesToArchive_2 : maven/settings.xml Tips & Trick \u00b6 As the documentation is so extensive, it can sometimes be difficult to figure out what would be a good way to deal with some constructs. Component identifier property One important thing to keep in mind is that in order to create a whole set of jobs via the groups and templates it imperative to have a component* identifier property. This way you can define hundreds of jobs in a project, dozens of groups and dozens of templates and generate thousands of unique individual jobs. Scale does not actually matter in this case, if you have more than one job in a project you will need this property. If the jobs that will be generated will not differ the execution will fail. Bulk you can combine multiple files or even entire folder structures together in a single call. For example, if you manage all the jobs of a company or a department and configure them in separate files. For example jenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/","title":"JenkinsJobsBuilder"},{"location":"jenkins-jobs/jenkins-jobs-builder/#jenkins-job-builder","text":"The configuration setup of Jenkins Job Builder is composed of two main categories. Basic configuration and job configuration. Job configuration can be further split into several sub categories.","title":"Jenkins Job Builder"},{"location":"jenkins-jobs/jenkins-jobs-builder/#basic-configuration","text":"In the basic configuration you will have to specify how the Jenkins Job Builder CLI can connect to the Jenkins instance you want to configure and how it should act. To use such a configuration file, you add --conf to the CLI command. Example: localhost.ini [job_builder] ignore_cache=True keep_descriptions=False include_path=.:scripts:~/git/ recursive=False exclude=.*:manual:./development allow_duplicates=False [jenkins] #user=jenkins #password= url=http://localhost:8080/ For more information see http://docs.openstack.org/infra/jenkins-job-builder/installation.html .","title":"Basic Configuration"},{"location":"jenkins-jobs/jenkins-jobs-builder/#job-configuration","text":"The configuration for configuring the jobs consists of several distinct parts which can all be in the same file or can be distributed in their own respected files. These different parts can also be split into two different categories, those that are strictly linked within the configuration - via template matching - and those that are separate. Separate: * Macro\u2019s * Global defaults * Job configuration defaults * External configuration files Linked: * Templates * Groups * Projects * Job definitions Here\u2019s a schematic representation on how they are linked. Exampe in YAML config: - job-template : name : '{name}-{configComponentId}-ci' description : 'CI Job of {configComponentId}' << : *config_job_defaults builders : - shell : 'jenkins-jobs test -r global/:definitions/ -o compiled/' - job-template : name : '{name}-{configComponentId}-execute' description : 'Executor Job of {configComponentId}' << : *config_job_defaults builders : - shell : 'jenkins-jobs --conf configuration/localhost.ini update definitions/' - job-group : name : '{name}-config' gitlab-user : 'jvandergriendt' jobs : - '{name}-{configComponentId}-ci' : - '{name}-{configComponentId}-execute' : - project : name : RnD-Config jobs : - '{name}-config' : configComponentId : JenkinsJobDefinitions The above will result in the following jobs: RnD-Config-JenkinsJobDefinitions-ci RnD-Config-JenkinsJobDefinitions-execute","title":"Job Configuration"},{"location":"jenkins-jobs/jenkins-jobs-builder/#macros","text":"Macro\u2019s are what the name implies, a group of related commands which can be invoked by the group. In Jenkins Job Builder this means you can define specific configurations for a component type (e.g. builders, paramters, publishes etc). A component has a name and a macro name. In general the component name is plural and the macro name is singular. As can be seen in the examples below. Here\u2019s an example: # The 'add' macro takes a 'number' parameter and will creates a # job which prints 'Adding ' followed by the 'number' parameter: - builder : name : add builders : - shell : \"echo Adding {number}\" # A specialized macro 'addtwo' reusing the 'add' macro but with # a 'number' parameter hardcoded to 'two': - builder : name : addtwo builders : - add : number : \"two\" # Glue to have Jenkins Job Builder to expand this YAML example: - job : name : \"testingjob\" builders : # The specialized macro: - addtwo # Generic macro call with a parameter - add : number : \"ZERO\" # Generic macro called without a parameter. Never do this! # See below for the resulting wrong output :( - add To expand the schematic representation, you will get the following. - builder : name : test builders : - shell : \"jenkins-jobs test -r global/:definitions/ -o compiled/\" - builder : name : update builders : - shell : \"jenkins-jobs --conf config.ini update -r global/:definitions/\" - job-template : name : '{name}-{configComponentId}-ci' << : *config_job_defaults builders : - test - job-template : name : '{name}-{configComponentId}-update' << : *config_job_defaults builders : - update","title":"Macro\u2019s"},{"location":"jenkins-jobs/jenkins-jobs-builder/#global-defaults","text":"Global defaults are defaults that should be global for the jobs you configure for a certain environment. It is the job counterpart of the basic configuration, usually containing variables for the specific environment. For example, url\u2019s, credential id\u2019s, JDK\u2019s etc. Example: global-defaults-localhost.yaml - defaults : name : 'global' flusso-gitlab-url : https://gitlab.flusso.nl nexus-npm-url : http://localhost:8081/nexus/content/repositories/npm-internal default-jdk : JDK 1.8 jenkinsJobsDefinitionJobName : RnD-Config-JenkinsJobDefinitions-ci credentialsId : '4f0dfb96-a7b1-421c-a4ea-b6a154f91b08'","title":"Global defaults"},{"location":"jenkins-jobs/jenkins-jobs-builder/#job-configuration-defaults","text":"Job configuration defaults are nothing specific on their own. It refers to using a build in structure from YAML to create basic building blocks to be used by other configuration parts, usually the Templates. Example (definition): - config_job_defaults : &config_job_defaults name : 'config_job_defaults' project-type : freestyle disabled : false logrotate : daysToKeep : 7 numToKeep : 5 artifactDaysToKeep : -1 artifactNumToKeep : -1 jdk : '{default-jdk}' Example (usage): - job-template : name : '{name}-{configComponentId}-ci' << : *config_job_defaults","title":"Job configuration defaults"},{"location":"jenkins-jobs/jenkins-jobs-builder/#templates","text":"Templates are used to define job templates. You define the entirety of the job using global defaults, configuration defaults and where useful refer to placeholders to be filled in by the other downstream configuration items. You can configure almost every plugin that is available for Jenkins, these are divided in subdivisions which reflect the Jenkins\u2019 job definition sections. For these subdivision and the available plugins see: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#modules For those plugins that are not supported, you can include the raw XML generated by the plugin. For how to do this, see: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#raw-config Example: - job-template : name : '{name}-{configComponentId}-ci' display-name : '{name}-{configComponentId}-ci' description : 'CI Job of {configComponentId}' << : *config_job_defaults builders : - shell : 'jenkins-jobs test -r global/:definitions/ -o compiled/' publishers : - archive : artifacts : '{filesToArchive_1}' fingerprint : true - archive : artifacts : '{filesToArchive_2}' fingerprint : true - email : notify-every-unstable-build : true send-to-individuals : true","title":"Templates"},{"location":"jenkins-jobs/jenkins-jobs-builder/#groups","text":"Groups are used to group together related components that require the same set of jobs. Where you can also specify a similar set of properties, for example, a different JDK to be used. The name property is mandatory and will be used to match Job definitions. The jobs property is also mandatory and will be used to match Templates for which a Job will be generated per matching Job definition. Example - job-group : name : '{name}-gulp' gitlab-user : 'jvandergriendt' artifactId : '{gulpComponentId}' jobs : - '{name}-{gulpComponentId}-ci' : - '{name}-{gulpComponentId}-version' : - '{name}-{gulpComponentId}-sonar' : - '{name}-{gulpComponentId}-publish' : - '{name}-{gulpComponentId}-deploy-prep' : - '{name}-{gulpComponentId}-deploy' : - '{name}-{gulpComponentId}-acceptance' :","title":"Groups"},{"location":"jenkins-jobs/jenkins-jobs-builder/#projects","text":"Projects are used to list the actual Job definitions, which via grouping and Templates get generated, and can obviously be used to define jobs for a specific project. The name property is mandatory and will be passed along with a Job definition and is generally used to tie job definitions to Groups. - project : name : RnD-Maven jobs : - '{name}-keep' : gulpComponentId : keep-backend displayName : Keep-Backend","title":"Projects"},{"location":"jenkins-jobs/jenkins-jobs-builder/#job-definitions","text":"Job definitions are what is all about. Although they are part of the Project configuration item I treat them separately. You list the jobs under a Project and start with the name of the Group it belongs to. After that, you should define at least a name component to be able to differentiate the different jobs you want. As can be seen in the above examples with the gulpComponentId. External configuration files Sometimes you run into the situation you want to use a multi-line configuration for a plugin, or a set of commands. Or, used at in different configurations or templates. Then you run into the situation that it is very difficult to manage in them neatly inside YAML configuration files. For this situation you are able to simply include a text file, via a native YAML construct. See: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#module-jenkins_jobs.local_yaml For example - job : name : test-job-include-raw-1 builders : - shell : !include-raw include-raw001-hello-world.sh - shell : !include-raw include-raw001-vars.sh","title":"Job definitions"},{"location":"jenkins-jobs/jenkins-jobs-builder/#usage","text":"The information to how you use the tool is very well explained in the documentation. See http://docs.openstack.org/infra/jenkins-job-builder/installation.html#running Automated maintenance If all the jobs you can administer are done via Jenkins Job Builder, you can start to automate the maintenance of these jobs. Simply make jobs that poll/push on the code base where you have your Jenkins Job Builder configuration files. Example - config_job_defaults : &config_job_defaults name : 'config_job_defaults' project-type : freestyle disabled : false logrotate : daysToKeep : 7 numToKeep : 5 artifactDaysToKeep : -1 artifactNumToKeep : -1 jdk : '{default-jdk}' triggers : - pollscm : \"H/15 * * * *\" scm : - git : url : '{flusso-gitlab-url}/{gitlab-user}/{componentGitName}.git' credentials-id : '{credentialsId}' publishers : - email : notify-every-unstable-build : true send-to-individuals : true - job-template : name : '{name}-{configComponentId}-ci' display-name : '{name}-{configComponentId}-ci' description : 'CI Job of {configComponentId}' << : *config_job_defaults builders : - shell : 'jenkins-jobs test -r global/:definitions/ -o compiled/' publishers : - archive : artifacts : '{filesToArchive_1}' fingerprint : true - archive : artifacts : '{filesToArchive_2}' fingerprint : true - email : notify-every-unstable-build : true send-to-individuals : true - job-template : name : '{name}-{configComponentId}-x' display-name : '{name}-{configComponentId}-execute' description : 'Executor Job of {configComponentId}, it will execute the update and delete old command' << : *config_job_defaults builders : - shell : 'jenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/' - job-group : name : '{name}-config' gitlab-user : 'jvandergriendt' jobs : - '{name}-{configComponentId}-ci' : - '{name}-{configComponentId}-x' : - project : name : RnD-Config jobs : - '{name}-config' : configComponentId : JenkinsJobDefinitions componentGitName : jenkins-job-definitions filesToArchive_1 : scripts/*.sh filesToArchive_2 : maven/settings.xml","title":"Usage"},{"location":"jenkins-jobs/jenkins-jobs-builder/#tips-trick","text":"As the documentation is so extensive, it can sometimes be difficult to figure out what would be a good way to deal with some constructs. Component identifier property One important thing to keep in mind is that in order to create a whole set of jobs via the groups and templates it imperative to have a component* identifier property. This way you can define hundreds of jobs in a project, dozens of groups and dozens of templates and generate thousands of unique individual jobs. Scale does not actually matter in this case, if you have more than one job in a project you will need this property. If the jobs that will be generated will not differ the execution will fail. Bulk you can combine multiple files or even entire folder structures together in a single call. For example, if you manage all the jobs of a company or a department and configure them in separate files. For example jenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/","title":"Tips &amp; Trick"},{"location":"jenkins-jobs/jobdsl/","text":"Jenkins Job DSL \u00b6 Jenkins is a wonderful system for managing builds, and people love using its UI to configure jobs. Unfortunately, as the number of jobs grows, maintaining them becomes tedious, and the paradigm of using a UI falls apart. Additionally, the common pattern in this situation is to copy jobs to create new ones, these \"children\" have a habit of diverging from their original \"template\" and consequently it becomes difficult to maintain consistency between these jobs. The Jenkins job-dsl-plugin attempts to solve this problem by allowing jobs to be defined with the absolute minimum necessary in a programmatic form, with the help of templates that are synced with the generated jobs. The goal is for your project to be able to define all the jobs they want to be related to their project, declaring their intent for the jobs, leaving the common stuff up to a template that were defined earlier or hidden behind the DSL. Pipeline with folder example \u00b6 import hudson.model.* import jenkins.model.* def dslExamplesFolder = 'DSL-Examples' def gitLabCredentialsId = 'joost-flusso-gitlab-ssh' def gitLabUrl = 'git@gitlab.flusso.nl' def gitLabNamespace = 'keep' def gitLabProject = 'keep-api' if (! jenkins . model . Jenkins . instance . getItem ( dslExamplesFolder )) { //folder doesn't exist because item doesn't exist in runtime //Therefore, create the folder. folder ( dslExamplesFolder ) { displayName ( 'DSL Examples' ) description ( 'Folder for job dsl examples' ) } } createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , 'keep' , 'keep-api' ) createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , 'keep' , 'keep-backend-spring' ) createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , 'keep' , 'keep-frontend' ) def createMultibranchPipelineJob ( def gitLabCredentialsId , def gitLabUrl , def folder , def gitNamespace , def project ) { multibranchPipelineJob ( \"${folder}/${project}-mb\" ) { branchSources { git { remote ( \"${gitLabUrl}:${gitNamespace}/${project}.git\" ) credentialsId ( gitLabCredentialsId ) } } orphanedItemStrategy { discardOldItems { numToKeep ( 20 ) } } } } Freestyle maven job \u00b6 def project = 'quidryan/aws-sdk-test' def branchApi = new URL ( \"https://api.github.com/repos/${project}/branches\" ) def branches = new groovy . json . JsonSlurper (). parse ( branchApi . newReader ()) branches . each { def branchName = it . name def jobName = \"${project}-${branchName}\" . replaceAll ( '/' , '-' ) job ( jobName ) { scm { git ( \"git://github.com/${project}.git\" , branchName ) } steps { maven ( \"test -Dproject.name=${project}/${branchName}\" ) } } } Resources \u00b6 Tutorial Live Playground Main DSL Commands API Viewer Other References \u00b6 Talks and Blogs User Power Movies DZone article Testing DSL Scripts","title":"JobDSL"},{"location":"jenkins-jobs/jobdsl/#jenkins-job-dsl","text":"Jenkins is a wonderful system for managing builds, and people love using its UI to configure jobs. Unfortunately, as the number of jobs grows, maintaining them becomes tedious, and the paradigm of using a UI falls apart. Additionally, the common pattern in this situation is to copy jobs to create new ones, these \"children\" have a habit of diverging from their original \"template\" and consequently it becomes difficult to maintain consistency between these jobs. The Jenkins job-dsl-plugin attempts to solve this problem by allowing jobs to be defined with the absolute minimum necessary in a programmatic form, with the help of templates that are synced with the generated jobs. The goal is for your project to be able to define all the jobs they want to be related to their project, declaring their intent for the jobs, leaving the common stuff up to a template that were defined earlier or hidden behind the DSL.","title":"Jenkins Job DSL"},{"location":"jenkins-jobs/jobdsl/#pipeline-with-folder-example","text":"import hudson.model.* import jenkins.model.* def dslExamplesFolder = 'DSL-Examples' def gitLabCredentialsId = 'joost-flusso-gitlab-ssh' def gitLabUrl = 'git@gitlab.flusso.nl' def gitLabNamespace = 'keep' def gitLabProject = 'keep-api' if (! jenkins . model . Jenkins . instance . getItem ( dslExamplesFolder )) { //folder doesn't exist because item doesn't exist in runtime //Therefore, create the folder. folder ( dslExamplesFolder ) { displayName ( 'DSL Examples' ) description ( 'Folder for job dsl examples' ) } } createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , 'keep' , 'keep-api' ) createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , 'keep' , 'keep-backend-spring' ) createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , 'keep' , 'keep-frontend' ) def createMultibranchPipelineJob ( def gitLabCredentialsId , def gitLabUrl , def folder , def gitNamespace , def project ) { multibranchPipelineJob ( \"${folder}/${project}-mb\" ) { branchSources { git { remote ( \"${gitLabUrl}:${gitNamespace}/${project}.git\" ) credentialsId ( gitLabCredentialsId ) } } orphanedItemStrategy { discardOldItems { numToKeep ( 20 ) } } } }","title":"Pipeline with folder example"},{"location":"jenkins-jobs/jobdsl/#freestyle-maven-job","text":"def project = 'quidryan/aws-sdk-test' def branchApi = new URL ( \"https://api.github.com/repos/${project}/branches\" ) def branches = new groovy . json . JsonSlurper (). parse ( branchApi . newReader ()) branches . each { def branchName = it . name def jobName = \"${project}-${branchName}\" . replaceAll ( '/' , '-' ) job ( jobName ) { scm { git ( \"git://github.com/${project}.git\" , branchName ) } steps { maven ( \"test -Dproject.name=${project}/${branchName}\" ) } } }","title":"Freestyle maven job"},{"location":"jenkins-jobs/jobdsl/#resources","text":"Tutorial Live Playground Main DSL Commands API Viewer","title":"Resources"},{"location":"jenkins-jobs/jobdsl/#other-references","text":"Talks and Blogs User Power Movies DZone article Testing DSL Scripts","title":"Other References"},{"location":"jenkins-pipeline/artifactory-integration/","text":"JFrog Jenkins Challenge \u00b6 Visited the stand of JFrog where they had stories about two main products: Artifactory and X-Ray. For both there is a Challenge, an X-Ray Challenge and a Jenkins & Artifactory Challenge . Jenkins Challenge \u00b6 The instructions for the Challenge were simply, follow what is stated in their GitHub repository and email a screenshot of the result. The instruction were as follows: Get an Artifactory instance (you can start a free trial on prem or in the cloud) Install Jenkins Install Artifactory Jenkins Plugin Add Artifactory credentials to Jenkins Credentials Create a new pipeline job Use the Artifactory Plugin DSL documentation to complete the following script: With a Scripted Pipeline as starting point: node { def rtServer def rtGradle def buildInfo stage ( 'Preparation' ) { git 'https://github.com/jbaruch/gradle-example.git' // create a new Artifactory server using the credentials defined in Jenkins // create a new Gradle build // set the resolver to the Gradle build to resolve from Artifactory // set the deployer to the Gradle build to deploy to Artifactory // declare that your gradle script does not use Artifactory plugin // declare that your gradle script uses Gradle wrapper } stage ( 'Build' ) { //run the artifactoryPublish gradle task and collect the build info } stage ( 'Publish Build Info' ) { //collect the environment variables to build info //publish the build info } } I don't like scripted, so I opted for Declarative with Jenkins in Kubernetes with the Jenkins Kubernetes plugin. Steps I took: get a trial license from the JFrog website install Artifactory and copy in the license when prompted change admin password create local maven repo 'libs-snapshot-local' create remote maven repo 'jcenter' (default remote value is jcenter, so only have to set the name) install Jenkins Artifactory plugin Kubernetes plugin add Artifactory username/password as credential in Jenkins create a gradle application (Spring boot via start.spring.io) which you can find here create a Jenkinsfile Installing Artifactory \u00b6 I installed Artifactory via Helm. JFrog has their own Helm repository - of course, would weird otherwise tbh - and you have to add that first. helm repo add jfrog https://charts.jfrog.io helm install --name artifactory stable/artifactory Jenkinsfile \u00b6 This uses the Gradle wrapper - as per instructions in the challenge. So we can use the standard JNLP container, which is default, so agent any will do. pipeline { agent any environment { rtServer = '' rtGradle = '' buildInfo = '' artifactoryServerAddress = 'http://..../artifactory' } stages { stage ( 'Test Container' ) { steps { container ( 'gradle' ) { sh 'which gradle' sh 'uname -a' sh 'gradle -version' } } } stage ( 'Checkout' ){ steps { git 'https://github.com/demomon/gradle-jenkins-challenge.git' } } stage ( 'Preparation' ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: artifactoryServerAddress , credentialsId: 'art-admin' // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: 'jcenter' , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: 'libs-snapshot-local' , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( 'Build' ) { steps { script { //run the artifactoryPublish gradle task and collect the build info buildInfo = rtGradle . run buildFile: 'build.gradle' , tasks: 'clean build artifactoryPublish' } } } stage ( 'Publish Build Info' ) { steps { script { //collect the environment variables to build info buildInfo . env . capture = true //publish the build info rtServer . publishBuildInfo buildInfo } } } } } Jenkinsfile without Gradle Wrapper \u00b6 I'd rather not install the Gradle tool if I can just use a pre-build container with it. Unfortunately, to use it correctly with the Artifactory plugin and a Jenkins Kubernetes plugin, we need to do two things. create a Gradle Tool in the Jenkins master because the Artifactory plugin expects a Jenkins Tool object, not a location Manage Jenkins -> Global Tool Configuration -> Gradle -> Add As value supply /usr , the Artifactory build will add /gradle/bin to it automatically set the user of build Pod to id 1000 explicitly else the build will not be allowed to touch files in /home/jenkins/workspace pipeline { agent { kubernetes { label 'mypod' yaml \"\"\"apiVersion: v1 kind: Pod spec: securityContext: runAsUser: 1000 fsGroup: 1000 containers: - name: gradle image: gradle:4.10-jdk-alpine command: ['cat'] tty: true \"\"\" } } environment { rtServer = '' rtGradle = '' buildInfo = '' CONTAINER_GRADLE_TOOL = '/usr/bin/gradle' } stages { stage ( 'Test Container' ) { steps { container ( 'gradle' ) { sh 'which gradle' sh 'uname -a' sh 'gradle -version' } } } stage ( 'Checkout' ){ steps { // git 'https://github.com/demomon/gradle-jenkins-challenge.git' checkout scm } } stage ( 'Preparation' ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: 'http://35.204.238.14/artifactory' , credentialsId: 'art-admin' // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: 'jcenter' , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: 'libs-snapshot-local' , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( 'Build' ) { //run the artifactoryPublish gradle task and collect the build info steps { script { buildInfo = rtGradle . run buildFile: 'build.gradle' , tasks: 'clean build artifactoryPublish' } } } stage ( 'Publish Build Info' ) { //collect the environment variables to build info //publish the build info steps { script { buildInfo . env . capture = true rtServer . publishBuildInfo buildInfo } } } } }","title":"Artifactory Integration"},{"location":"jenkins-pipeline/artifactory-integration/#jfrog-jenkins-challenge","text":"Visited the stand of JFrog where they had stories about two main products: Artifactory and X-Ray. For both there is a Challenge, an X-Ray Challenge and a Jenkins & Artifactory Challenge .","title":"JFrog Jenkins Challenge"},{"location":"jenkins-pipeline/artifactory-integration/#jenkins-challenge","text":"The instructions for the Challenge were simply, follow what is stated in their GitHub repository and email a screenshot of the result. The instruction were as follows: Get an Artifactory instance (you can start a free trial on prem or in the cloud) Install Jenkins Install Artifactory Jenkins Plugin Add Artifactory credentials to Jenkins Credentials Create a new pipeline job Use the Artifactory Plugin DSL documentation to complete the following script: With a Scripted Pipeline as starting point: node { def rtServer def rtGradle def buildInfo stage ( 'Preparation' ) { git 'https://github.com/jbaruch/gradle-example.git' // create a new Artifactory server using the credentials defined in Jenkins // create a new Gradle build // set the resolver to the Gradle build to resolve from Artifactory // set the deployer to the Gradle build to deploy to Artifactory // declare that your gradle script does not use Artifactory plugin // declare that your gradle script uses Gradle wrapper } stage ( 'Build' ) { //run the artifactoryPublish gradle task and collect the build info } stage ( 'Publish Build Info' ) { //collect the environment variables to build info //publish the build info } } I don't like scripted, so I opted for Declarative with Jenkins in Kubernetes with the Jenkins Kubernetes plugin. Steps I took: get a trial license from the JFrog website install Artifactory and copy in the license when prompted change admin password create local maven repo 'libs-snapshot-local' create remote maven repo 'jcenter' (default remote value is jcenter, so only have to set the name) install Jenkins Artifactory plugin Kubernetes plugin add Artifactory username/password as credential in Jenkins create a gradle application (Spring boot via start.spring.io) which you can find here create a Jenkinsfile","title":"Jenkins Challenge"},{"location":"jenkins-pipeline/artifactory-integration/#installing-artifactory","text":"I installed Artifactory via Helm. JFrog has their own Helm repository - of course, would weird otherwise tbh - and you have to add that first. helm repo add jfrog https://charts.jfrog.io helm install --name artifactory stable/artifactory","title":"Installing Artifactory"},{"location":"jenkins-pipeline/artifactory-integration/#jenkinsfile","text":"This uses the Gradle wrapper - as per instructions in the challenge. So we can use the standard JNLP container, which is default, so agent any will do. pipeline { agent any environment { rtServer = '' rtGradle = '' buildInfo = '' artifactoryServerAddress = 'http://..../artifactory' } stages { stage ( 'Test Container' ) { steps { container ( 'gradle' ) { sh 'which gradle' sh 'uname -a' sh 'gradle -version' } } } stage ( 'Checkout' ){ steps { git 'https://github.com/demomon/gradle-jenkins-challenge.git' } } stage ( 'Preparation' ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: artifactoryServerAddress , credentialsId: 'art-admin' // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: 'jcenter' , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: 'libs-snapshot-local' , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( 'Build' ) { steps { script { //run the artifactoryPublish gradle task and collect the build info buildInfo = rtGradle . run buildFile: 'build.gradle' , tasks: 'clean build artifactoryPublish' } } } stage ( 'Publish Build Info' ) { steps { script { //collect the environment variables to build info buildInfo . env . capture = true //publish the build info rtServer . publishBuildInfo buildInfo } } } } }","title":"Jenkinsfile"},{"location":"jenkins-pipeline/artifactory-integration/#jenkinsfile-without-gradle-wrapper","text":"I'd rather not install the Gradle tool if I can just use a pre-build container with it. Unfortunately, to use it correctly with the Artifactory plugin and a Jenkins Kubernetes plugin, we need to do two things. create a Gradle Tool in the Jenkins master because the Artifactory plugin expects a Jenkins Tool object, not a location Manage Jenkins -> Global Tool Configuration -> Gradle -> Add As value supply /usr , the Artifactory build will add /gradle/bin to it automatically set the user of build Pod to id 1000 explicitly else the build will not be allowed to touch files in /home/jenkins/workspace pipeline { agent { kubernetes { label 'mypod' yaml \"\"\"apiVersion: v1 kind: Pod spec: securityContext: runAsUser: 1000 fsGroup: 1000 containers: - name: gradle image: gradle:4.10-jdk-alpine command: ['cat'] tty: true \"\"\" } } environment { rtServer = '' rtGradle = '' buildInfo = '' CONTAINER_GRADLE_TOOL = '/usr/bin/gradle' } stages { stage ( 'Test Container' ) { steps { container ( 'gradle' ) { sh 'which gradle' sh 'uname -a' sh 'gradle -version' } } } stage ( 'Checkout' ){ steps { // git 'https://github.com/demomon/gradle-jenkins-challenge.git' checkout scm } } stage ( 'Preparation' ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: 'http://35.204.238.14/artifactory' , credentialsId: 'art-admin' // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: 'jcenter' , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: 'libs-snapshot-local' , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( 'Build' ) { //run the artifactoryPublish gradle task and collect the build info steps { script { buildInfo = rtGradle . run buildFile: 'build.gradle' , tasks: 'clean build artifactoryPublish' } } } stage ( 'Publish Build Info' ) { //collect the environment variables to build info //publish the build info steps { script { buildInfo . env . capture = true rtServer . publishBuildInfo buildInfo } } } } }","title":"Jenkinsfile without Gradle Wrapper"},{"location":"jenkins-pipeline/core-concepts/","text":"Core Concepts \u00b6 Below are some core concepts to understand before building pipelines in Jenkins. Pipeline as Code Step Master vs Nodes Checkout Workspace Stage Sandbox and Script Security Java vs. Groovy Env (object) Stash & archive Credentials Tools & Build Environment Pipeline Syntax Page Terminology \u00b6 The terminology used in this page is based upon the terms used by Cloudbees as related to Jenkins. If in doubt, please consult the Jenkins Glossary . Pipeline as Code \u00b6 Step \u00b6 A single task; fundamentally steps tell Jenkins what to do inside of a Pipeline or Project. Consider the following piece of pipeline code: node { timestamps { stage ( ' My FIrst Stage ' ) { if ( isUnix ()) { sh ' echo \"this is Unix!\" ' } else { bat ' echo \"this is windows\" ' } } } } The only execution that happens (almost) exclusively on the node (or build slave) are the isUnix() , sh and bat shell commands. Those specific tasks are the steps in pipeline code. Master vs Nodes \u00b6 There are many things to keep in mind about Pipelines in Jenkins. By far the most important are those related to the distinction between Masters and Nodes. Aside from the points below, the key thing to keep in mind: Nodes (build slaves) are designed to executes task, Masters are not. Except for the steps themselves, all of the Pipeline logic, the Groovy conditionals, loops, etc execute on the master . Whether simple or complex! Even inside a node block ! Steps may use executors to do work where appropriate, but each step has a small on-master overhead too. Pipeline code is written as Groovy but the execution model is radically transformed at compile-time to Continuation Passing Style (CPS). This transformation provides valuable safety and durability guarantees for Pipelines, but it comes with trade-offs: Steps can invoke Java and execute fast and efficiently, but Groovy is much slower to run than normal. Groovy logic requires far more memory, because an object-based syntax/block tree is kept in memory. Pipelines persist the program and its state frequently to be able to survive failure of the master. Source: Sam van Oort , Cloudbees Engineer Node \u00b6 A machine which is part of the Jenkins environment and capable of executing Pipelines or Projects. Both the Master and Agents are considered to be Nodes. Master \u00b6 The central, coordinating process which stores configuration, loads plugins, and renders the various user interfaces for Jenkins. What to do? \u00b6 So, if Pipeline code can cause big loads on Master, what should we do than? Try to limit the use of logic in your groovy code Avoid blocking or I/O calls unless explicitly done on a slave via a Step If you need heavy processing, and there isn't a Step, create either a plugin Shared Library Or use a CLI tool via a platform independent language, such as Java or Go Tip If need to do any I/O, use a plugin or anything related to a workspace, you need a node. If you only need to interact with variables, for example for an input form, do this outside of a node block. See Pipeline Input for how that works. Workspace \u00b6 A disposable directory on the file system of a Node where work can be done by a Pipeline or Project. Workspaces are typically left in place after a Build or Pipeline run completes unless specific Workspace cleanup policies have been put in place on the Jenkins Master. The key part of the glossary entry there is disposable directory . There are absolutely no guarantees about Workspaces in pipeline jobs. That said, what you should take care of: always clean your workspace before you start, you don't know the state of the folder you get always clean your workspace after you finish, this way you're less likely to run into problems in subsequent builds a workspace is a temporary folder on a single node's filesystem: so every time you use node{} you have a new workspace after your build is finish or leaving the node otherwise, your workspace should be considered gone: need something from? stash or archive it! Checkout \u00b6 There are several ways to do a checkout in the Jenkins pipeline code. In the groovy DSL you can use the Checkout dsl command, svn shorthand or the git shorthand. node { stage ( 'scm' ) { git 'https://github.com/joostvdg/jishi' } } Danger If you use a pipeline from SCM, multi-branch pipeline or a derived job type, beware! Only the Jenkinsfile gets checked out. You still need to checkout the rest of your files yourself! Tip However, when using pipeline from SCM, multi-branch pipeline or a derived job type. You can use a shorthand: checkout scm . This checks out the scm defined in your job (where the Jenkinsfile came from). node { stage ( 'scm' ) { checkout scm } } Stage \u00b6 Stage is a step for defining a conceptually distinct subset of the entire Pipeline, for example: \"Build\", \"Test\", and \"Deploy\", which is used by many plugins to visualize or present Jenkins Pipeline status/progress. The stage \"step\" has a primary function and a secondary function. Its primary function is to define the visual boundaries between logically separable parts of the pipeline. For example, you can define SCM, Build, QA, Deploy as stages to tell you where the build currently is or where it failed. The secondary function is to provided a scope for variables. Just like most programming languages, code blocks are a more than just syntactic sugar, they also limit the scope of variables. node { stage ( ' SCM ' ) { def myVar = ' abc ' checkout scm } stage ( ' Build ' ) { sh ' mvn clean install ' echo myVar # will fail because the variable doesn ' t exist here } } Stages in classic view \u00b6 Stages in Blue Ocean view \u00b6 Sandbox and Script Security \u00b6 In Jenkins some plugins - such as the pipeline plugin - allow you to write groovy code that gets executed on the master. This means you could run code on the master that accesses the host machine with the same rights as Jenkins. As is unsafe, Jenkins has some guards against this in the form the sandbox mode and the script security . When you create a pipeline job, you get a inline code editor by default. If you're an administrator you get the option to turn the \"sandbox\" mode of. If you use a pipeline from SCM or any of the higher abstraction pipeline job types (Multibranch Pipeline, BitBucket Team) you are always running in sandbox mode. When you're in sandbox mode, your script will run past the script security. This uses a whitelisting technique to block dangerous or undesired methods, but is does so in a very restrictive manner. It could be you're doing something that is safe but still gets blocked. An administrator can then go to the script approval page (under Jenkins Administration) and approve your script. For more details, please consult Script Security plugin page. Example error \u00b6 org . jenkinsci . plugins . scriptsecurity . sandbox . RejectedAccessException : unclassified staticMethod org . tmatesoft . svn . core . internal . io . dav . DAVRepositoryFactory create org . tmatesoft . svn . core . SVNURL at org . jenkinsci . plugins . scriptsecurity . sandbox . groovy . SandboxInterceptor . onStaticCall ( SandboxInterceptor . java : 138 ) at org . kohsuke . groovy . sandbox . impl . Checker$2 . call ( Checker . java : 180 ) at org . kohsuke . groovy . sandbox . impl . Checker . checkedStaticCall ( Checker . java : 177 ) at org . kohsuke . groovy . sandbox . impl . Checker . checkedCall ( Checker . java : 91 ) at com . cloudbees . groovy . cps . sandbox . SandboxInvoker . methodCall ( SandboxInvoker . java : 16 ) at WorkflowScript . run ( WorkflowScript : 12 ) at ___cps . transform___ ( Native Method ) Tip There are three ways to deal with these errors. go to manage jenkins \u2192 script approval and approve the script use a Shared Library use a CLI tool/script via a shell command to do what you need to do Java vs. Groovy \u00b6 The pipeline code has to be written in groovy and therefor can also use java code. Two big difference to note: the usage of double quoted string (gstring, interpreted) and single quoted strings (literal) def abc = 'xyz' # is a literal echo '$abc' # prints $abc echo \"$abc\" # prints xyz no use of ; Unfortunately, due to the way the Pipeline code is processed, many of the groovy features don't work or don't work as expected. Things like the lambda's and for-each loops don't work well and are best avoided. In these situations, it is best to keep to the standard syntax of Java. For more information on how the groovy is being processed, it is best to read the technical-design . Env (object) \u00b6 The env object is an object that is available to use in any pipeline script. The env object allows you to store objects and variables to be used anywhere during the script. So things can be shared between nodes, the master and nodes and code blocks. Why would you want to use it? As in general, global variables are a bad practice. But if you need to have variables to be available through the execution on different machines (master, nodes) it is good to use this. Also the env object contains context variables, such as BRANCH_NAME, JOB_NAME and so one. For a complete overview, view the pipeline syntax page. Don't use the env object in functions, always feed them the parameters directly. Only use it in the \"pipeline flow\" and use it for the parameters of the methods. node { stage ( 'SCM' ) { checkout scm } stage ( 'Echo' ){ echo \"Branch=$env.BRANCH_NAME\" // will print Branch=master } } Stash & archive \u00b6 If you need to store files for keeping for later, there are two options available stash and archive . Both should be avoided as they cause heavy I/O traffic, usually between the Node and Master. For more specific information, please consult the Pipeline Syntax Page. Stash \u00b6 Stash allows you to copy files from the current workspace to a temp folder in the workspace in the master. If you're currently on a different machine it will copy them one by one over the network, keep this in mind. The files can only be retrieved during the pipeline execution and you can do so via the unstash command. node ( 'Machine1' ) { stage ( 'A' ) { // generate some files stash excludes: 'secret.txt' , includes: '*.txt' , name: 'abc' } } node ( 'Machine2' ) { stage ( 'B' ) { unstash 'abc' } } Saves a set of files for use later in the same build, generally on another node/workspace. Stashed files are not otherwise available and are generally discarded at the end of the build. Note that the stash and unstash steps are designed for use with small files. For large data transfers, use the External Workspace Manager plugin, or use an external repository manager such as Nexus or Artifactory. Archive & archiveArtifacts \u00b6 Archives build output artifacts for later use. As of Jenkins 2.x, you may use the more configurable archiveArtifacts. With archive you can store a file semi-permanently in your job. Semi as the files will be overridden by the latest build. The files you archive will be stored in the Job folder on the master. One usecase is to save a log file from a build tool. node { stage ( 'A' ) { try { // do some build } finally { // This step should not normally be used in your script. Consult the inline help for details. archive excludes: 'useless.log' , includes: '*.log' // Use this instead, but only for permanent files, or external logfiles archiveArtifacts allowEmptyArchive: true , artifacts: '*.log' , excludes: 'useless.log' , fingerprint: true , onlyIfSuccessful: true } } } Credentials \u00b6 In many pipelines you will have to deal with external systems, requiring credentials. Jenkins has the Credentials API which you can also utilize in the pipeline. You can use do this via the Credentials and Credentials Binding plugins, the first is the core plugin the second provides the integration for the pipeline. The best way to generate the required code snippet, is to go to the pipeline syntax page, select withCredentials and configure what you need. node { stage ( 'someRemoteCall' ) { withCredentials ([ usernameColonPassword ( credentialsId: 'someCredentialsId' , variable: 'USRPASS' )]) { sh \"curl -u $env.USRPASS $URL\" } } } For more examples, please consult Cloudbees' Injecting-Secrets-into-Jenkins-Build-Jobs blog post. Tools & Build Environment \u00b6 Jenkins would not be Jenkins without the direct support for the build tools, such as JDK's, SDK's, Maven, Ant what have you not. So, how do you use them in the pipeline? Unfortunately, this is a bit more cumbersome than it is in a freestyle (or legacy ) job. You have to do two things: retrieve the tool's location via the tool DSL method set the environment variables to suit the tool node { stage ( 'Maven' ) { String jdk = tool name: 'jdk_8' , type: 'jdk' String maven = tool name: 'maven_3.5.0' , type: 'maven' withEnv ([ \"JAVA_HOME=$jdk\" , \"PATH+MAVEN=${jdk}/bin:${maven}/bin\" ]) { sh 'mvn clean install' } // or in one go withEnv ([ \"JAVA_HOME=${ tool 'jdk_8' }\" , \"PATH+MAVEN=${tool 'maven_3.5.0'}/bin:${env.JAVA_HOME}/bin\" ]) { sh 'mvn clean install' } } } Pipeline Syntax Page \u00b6 Soooo, do I always have to figure out how to write these code snippets? No, don't worry. You don't have to. At every pipeline job type there is a link called \"Pipeline Syntax\". This gives you a page with a drop down menu, from where you can select all the available steps. Once you select a step, you can use the UI to setup the step and then use the generate button to give you the correct syntax.","title":"Core Concepts"},{"location":"jenkins-pipeline/core-concepts/#core-concepts","text":"Below are some core concepts to understand before building pipelines in Jenkins. Pipeline as Code Step Master vs Nodes Checkout Workspace Stage Sandbox and Script Security Java vs. Groovy Env (object) Stash & archive Credentials Tools & Build Environment Pipeline Syntax Page","title":"Core Concepts"},{"location":"jenkins-pipeline/core-concepts/#terminology","text":"The terminology used in this page is based upon the terms used by Cloudbees as related to Jenkins. If in doubt, please consult the Jenkins Glossary .","title":"Terminology"},{"location":"jenkins-pipeline/core-concepts/#pipeline-as-code","text":"","title":"Pipeline as Code"},{"location":"jenkins-pipeline/core-concepts/#step","text":"A single task; fundamentally steps tell Jenkins what to do inside of a Pipeline or Project. Consider the following piece of pipeline code: node { timestamps { stage ( ' My FIrst Stage ' ) { if ( isUnix ()) { sh ' echo \"this is Unix!\" ' } else { bat ' echo \"this is windows\" ' } } } } The only execution that happens (almost) exclusively on the node (or build slave) are the isUnix() , sh and bat shell commands. Those specific tasks are the steps in pipeline code.","title":"Step"},{"location":"jenkins-pipeline/core-concepts/#master-vs-nodes","text":"There are many things to keep in mind about Pipelines in Jenkins. By far the most important are those related to the distinction between Masters and Nodes. Aside from the points below, the key thing to keep in mind: Nodes (build slaves) are designed to executes task, Masters are not. Except for the steps themselves, all of the Pipeline logic, the Groovy conditionals, loops, etc execute on the master . Whether simple or complex! Even inside a node block ! Steps may use executors to do work where appropriate, but each step has a small on-master overhead too. Pipeline code is written as Groovy but the execution model is radically transformed at compile-time to Continuation Passing Style (CPS). This transformation provides valuable safety and durability guarantees for Pipelines, but it comes with trade-offs: Steps can invoke Java and execute fast and efficiently, but Groovy is much slower to run than normal. Groovy logic requires far more memory, because an object-based syntax/block tree is kept in memory. Pipelines persist the program and its state frequently to be able to survive failure of the master. Source: Sam van Oort , Cloudbees Engineer","title":"Master vs Nodes"},{"location":"jenkins-pipeline/core-concepts/#node","text":"A machine which is part of the Jenkins environment and capable of executing Pipelines or Projects. Both the Master and Agents are considered to be Nodes.","title":"Node"},{"location":"jenkins-pipeline/core-concepts/#master","text":"The central, coordinating process which stores configuration, loads plugins, and renders the various user interfaces for Jenkins.","title":"Master"},{"location":"jenkins-pipeline/core-concepts/#what-to-do","text":"So, if Pipeline code can cause big loads on Master, what should we do than? Try to limit the use of logic in your groovy code Avoid blocking or I/O calls unless explicitly done on a slave via a Step If you need heavy processing, and there isn't a Step, create either a plugin Shared Library Or use a CLI tool via a platform independent language, such as Java or Go Tip If need to do any I/O, use a plugin or anything related to a workspace, you need a node. If you only need to interact with variables, for example for an input form, do this outside of a node block. See Pipeline Input for how that works.","title":"What to do?"},{"location":"jenkins-pipeline/core-concepts/#workspace","text":"A disposable directory on the file system of a Node where work can be done by a Pipeline or Project. Workspaces are typically left in place after a Build or Pipeline run completes unless specific Workspace cleanup policies have been put in place on the Jenkins Master. The key part of the glossary entry there is disposable directory . There are absolutely no guarantees about Workspaces in pipeline jobs. That said, what you should take care of: always clean your workspace before you start, you don't know the state of the folder you get always clean your workspace after you finish, this way you're less likely to run into problems in subsequent builds a workspace is a temporary folder on a single node's filesystem: so every time you use node{} you have a new workspace after your build is finish or leaving the node otherwise, your workspace should be considered gone: need something from? stash or archive it!","title":"Workspace"},{"location":"jenkins-pipeline/core-concepts/#checkout","text":"There are several ways to do a checkout in the Jenkins pipeline code. In the groovy DSL you can use the Checkout dsl command, svn shorthand or the git shorthand. node { stage ( 'scm' ) { git 'https://github.com/joostvdg/jishi' } } Danger If you use a pipeline from SCM, multi-branch pipeline or a derived job type, beware! Only the Jenkinsfile gets checked out. You still need to checkout the rest of your files yourself! Tip However, when using pipeline from SCM, multi-branch pipeline or a derived job type. You can use a shorthand: checkout scm . This checks out the scm defined in your job (where the Jenkinsfile came from). node { stage ( 'scm' ) { checkout scm } }","title":"Checkout"},{"location":"jenkins-pipeline/core-concepts/#stage","text":"Stage is a step for defining a conceptually distinct subset of the entire Pipeline, for example: \"Build\", \"Test\", and \"Deploy\", which is used by many plugins to visualize or present Jenkins Pipeline status/progress. The stage \"step\" has a primary function and a secondary function. Its primary function is to define the visual boundaries between logically separable parts of the pipeline. For example, you can define SCM, Build, QA, Deploy as stages to tell you where the build currently is or where it failed. The secondary function is to provided a scope for variables. Just like most programming languages, code blocks are a more than just syntactic sugar, they also limit the scope of variables. node { stage ( ' SCM ' ) { def myVar = ' abc ' checkout scm } stage ( ' Build ' ) { sh ' mvn clean install ' echo myVar # will fail because the variable doesn ' t exist here } }","title":"Stage"},{"location":"jenkins-pipeline/core-concepts/#stages-in-classic-view","text":"","title":"Stages in classic view"},{"location":"jenkins-pipeline/core-concepts/#stages-in-blue-ocean-view","text":"","title":"Stages in Blue Ocean view"},{"location":"jenkins-pipeline/core-concepts/#sandbox-and-script-security","text":"In Jenkins some plugins - such as the pipeline plugin - allow you to write groovy code that gets executed on the master. This means you could run code on the master that accesses the host machine with the same rights as Jenkins. As is unsafe, Jenkins has some guards against this in the form the sandbox mode and the script security . When you create a pipeline job, you get a inline code editor by default. If you're an administrator you get the option to turn the \"sandbox\" mode of. If you use a pipeline from SCM or any of the higher abstraction pipeline job types (Multibranch Pipeline, BitBucket Team) you are always running in sandbox mode. When you're in sandbox mode, your script will run past the script security. This uses a whitelisting technique to block dangerous or undesired methods, but is does so in a very restrictive manner. It could be you're doing something that is safe but still gets blocked. An administrator can then go to the script approval page (under Jenkins Administration) and approve your script. For more details, please consult Script Security plugin page.","title":"Sandbox and Script Security"},{"location":"jenkins-pipeline/core-concepts/#example-error","text":"org . jenkinsci . plugins . scriptsecurity . sandbox . RejectedAccessException : unclassified staticMethod org . tmatesoft . svn . core . internal . io . dav . DAVRepositoryFactory create org . tmatesoft . svn . core . SVNURL at org . jenkinsci . plugins . scriptsecurity . sandbox . groovy . SandboxInterceptor . onStaticCall ( SandboxInterceptor . java : 138 ) at org . kohsuke . groovy . sandbox . impl . Checker$2 . call ( Checker . java : 180 ) at org . kohsuke . groovy . sandbox . impl . Checker . checkedStaticCall ( Checker . java : 177 ) at org . kohsuke . groovy . sandbox . impl . Checker . checkedCall ( Checker . java : 91 ) at com . cloudbees . groovy . cps . sandbox . SandboxInvoker . methodCall ( SandboxInvoker . java : 16 ) at WorkflowScript . run ( WorkflowScript : 12 ) at ___cps . transform___ ( Native Method ) Tip There are three ways to deal with these errors. go to manage jenkins \u2192 script approval and approve the script use a Shared Library use a CLI tool/script via a shell command to do what you need to do","title":"Example error"},{"location":"jenkins-pipeline/core-concepts/#java-vs-groovy","text":"The pipeline code has to be written in groovy and therefor can also use java code. Two big difference to note: the usage of double quoted string (gstring, interpreted) and single quoted strings (literal) def abc = 'xyz' # is a literal echo '$abc' # prints $abc echo \"$abc\" # prints xyz no use of ; Unfortunately, due to the way the Pipeline code is processed, many of the groovy features don't work or don't work as expected. Things like the lambda's and for-each loops don't work well and are best avoided. In these situations, it is best to keep to the standard syntax of Java. For more information on how the groovy is being processed, it is best to read the technical-design .","title":"Java vs. Groovy"},{"location":"jenkins-pipeline/core-concepts/#env-object","text":"The env object is an object that is available to use in any pipeline script. The env object allows you to store objects and variables to be used anywhere during the script. So things can be shared between nodes, the master and nodes and code blocks. Why would you want to use it? As in general, global variables are a bad practice. But if you need to have variables to be available through the execution on different machines (master, nodes) it is good to use this. Also the env object contains context variables, such as BRANCH_NAME, JOB_NAME and so one. For a complete overview, view the pipeline syntax page. Don't use the env object in functions, always feed them the parameters directly. Only use it in the \"pipeline flow\" and use it for the parameters of the methods. node { stage ( 'SCM' ) { checkout scm } stage ( 'Echo' ){ echo \"Branch=$env.BRANCH_NAME\" // will print Branch=master } }","title":"Env (object)"},{"location":"jenkins-pipeline/core-concepts/#stash-archive","text":"If you need to store files for keeping for later, there are two options available stash and archive . Both should be avoided as they cause heavy I/O traffic, usually between the Node and Master. For more specific information, please consult the Pipeline Syntax Page.","title":"Stash &amp; archive"},{"location":"jenkins-pipeline/core-concepts/#stash","text":"Stash allows you to copy files from the current workspace to a temp folder in the workspace in the master. If you're currently on a different machine it will copy them one by one over the network, keep this in mind. The files can only be retrieved during the pipeline execution and you can do so via the unstash command. node ( 'Machine1' ) { stage ( 'A' ) { // generate some files stash excludes: 'secret.txt' , includes: '*.txt' , name: 'abc' } } node ( 'Machine2' ) { stage ( 'B' ) { unstash 'abc' } } Saves a set of files for use later in the same build, generally on another node/workspace. Stashed files are not otherwise available and are generally discarded at the end of the build. Note that the stash and unstash steps are designed for use with small files. For large data transfers, use the External Workspace Manager plugin, or use an external repository manager such as Nexus or Artifactory.","title":"Stash"},{"location":"jenkins-pipeline/core-concepts/#archive-archiveartifacts","text":"Archives build output artifacts for later use. As of Jenkins 2.x, you may use the more configurable archiveArtifacts. With archive you can store a file semi-permanently in your job. Semi as the files will be overridden by the latest build. The files you archive will be stored in the Job folder on the master. One usecase is to save a log file from a build tool. node { stage ( 'A' ) { try { // do some build } finally { // This step should not normally be used in your script. Consult the inline help for details. archive excludes: 'useless.log' , includes: '*.log' // Use this instead, but only for permanent files, or external logfiles archiveArtifacts allowEmptyArchive: true , artifacts: '*.log' , excludes: 'useless.log' , fingerprint: true , onlyIfSuccessful: true } } }","title":"Archive &amp; archiveArtifacts"},{"location":"jenkins-pipeline/core-concepts/#credentials","text":"In many pipelines you will have to deal with external systems, requiring credentials. Jenkins has the Credentials API which you can also utilize in the pipeline. You can use do this via the Credentials and Credentials Binding plugins, the first is the core plugin the second provides the integration for the pipeline. The best way to generate the required code snippet, is to go to the pipeline syntax page, select withCredentials and configure what you need. node { stage ( 'someRemoteCall' ) { withCredentials ([ usernameColonPassword ( credentialsId: 'someCredentialsId' , variable: 'USRPASS' )]) { sh \"curl -u $env.USRPASS $URL\" } } } For more examples, please consult Cloudbees' Injecting-Secrets-into-Jenkins-Build-Jobs blog post.","title":"Credentials"},{"location":"jenkins-pipeline/core-concepts/#tools-build-environment","text":"Jenkins would not be Jenkins without the direct support for the build tools, such as JDK's, SDK's, Maven, Ant what have you not. So, how do you use them in the pipeline? Unfortunately, this is a bit more cumbersome than it is in a freestyle (or legacy ) job. You have to do two things: retrieve the tool's location via the tool DSL method set the environment variables to suit the tool node { stage ( 'Maven' ) { String jdk = tool name: 'jdk_8' , type: 'jdk' String maven = tool name: 'maven_3.5.0' , type: 'maven' withEnv ([ \"JAVA_HOME=$jdk\" , \"PATH+MAVEN=${jdk}/bin:${maven}/bin\" ]) { sh 'mvn clean install' } // or in one go withEnv ([ \"JAVA_HOME=${ tool 'jdk_8' }\" , \"PATH+MAVEN=${tool 'maven_3.5.0'}/bin:${env.JAVA_HOME}/bin\" ]) { sh 'mvn clean install' } } }","title":"Tools &amp; Build Environment"},{"location":"jenkins-pipeline/core-concepts/#pipeline-syntax-page","text":"Soooo, do I always have to figure out how to write these code snippets? No, don't worry. You don't have to. At every pipeline job type there is a link called \"Pipeline Syntax\". This gives you a page with a drop down menu, from where you can select all the available steps. Once you select a step, you can use the UI to setup the step and then use the generate button to give you the correct syntax.","title":"Pipeline Syntax Page"},{"location":"jenkins-pipeline/declarative-pipeline/","text":"Declarative Pipeline \u00b6 Declarative Pipeline is a relatively recent addition to Jenkins Pipeline [1] which presents a more simplified and opinionated syntax on top of the Pipeline sub-systems. All valid Declarative Pipelines must be enclosed within a pipeline block, for example: pipeline { /* insert Declarative Pipeline here */ } Hello World Example \u00b6 pipeline { agent { docker 'python:3.5.1' } stages { stage ( 'build' ) { steps { sh 'python --version' } } } } MKDocs Build Example \u00b6 pipeline { agent none options { timeout ( time: 10 , unit: 'MINUTES' ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: '5' )) } stages { stage ( 'Prepare' ){ agent { label 'docker' } steps { parallel ( Clean: { deleteDir () }, NotifySlack: { slackSend channel: 'cicd' , color: '#FFFF00' , message: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" } ) } } stage ( 'Checkout' ){ agent { label 'docker' } steps { git credentialsId: '355df378-e726-4abd-90fa-e723c5c21ad5' , url: 'git@gitlab.flusso.nl:CICD/ci-cd-docs.git' script { env . GIT_COMMIT_HASH = sh returnStdout: true , script: 'git rev-parse --verify HEAD' } } } stage ( 'Build Docs' ) { agent { docker { image \"caladreas/mkdocs-docker-build-container\" label \"docker\" } } steps { sh 'mkdocs build' } } stage ( 'Prepare Docker Image' ){ agent { label 'docker' } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true , script: 'docker run --rm -i lukasmartinelli/hadolint < Dockerfile' if ( lintResult . trim () == '' ) { println 'Lint finished with no errors' } else { println 'Error found in Lint' println \"${lintResult}\" currentBuild . result = 'UNSTABLE' } } }, // end test dockerfile BuildImage: { sh 'chmod +x build.sh' sh './build.sh' } ) } post { success { sh 'chmod +x push.sh' sh './push.sh' } } } stage ( 'Update Docker Container' ) { agent { label 'docker' } steps { sh 'chmod +x container-update.sh' sh \"./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH}\" } } } post { success { slackSend channel: 'cicd' , color: '#00FF00' , message: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" } failure { slackSend channel: 'cicd' , color: '#FF0000' , message: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" } } } Resources \u00b6 Syntax Reference Getting started Notifications","title":"Pipeline (declarative)"},{"location":"jenkins-pipeline/declarative-pipeline/#declarative-pipeline","text":"Declarative Pipeline is a relatively recent addition to Jenkins Pipeline [1] which presents a more simplified and opinionated syntax on top of the Pipeline sub-systems. All valid Declarative Pipelines must be enclosed within a pipeline block, for example: pipeline { /* insert Declarative Pipeline here */ }","title":"Declarative Pipeline"},{"location":"jenkins-pipeline/declarative-pipeline/#hello-world-example","text":"pipeline { agent { docker 'python:3.5.1' } stages { stage ( 'build' ) { steps { sh 'python --version' } } } }","title":"Hello World Example"},{"location":"jenkins-pipeline/declarative-pipeline/#mkdocs-build-example","text":"pipeline { agent none options { timeout ( time: 10 , unit: 'MINUTES' ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: '5' )) } stages { stage ( 'Prepare' ){ agent { label 'docker' } steps { parallel ( Clean: { deleteDir () }, NotifySlack: { slackSend channel: 'cicd' , color: '#FFFF00' , message: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" } ) } } stage ( 'Checkout' ){ agent { label 'docker' } steps { git credentialsId: '355df378-e726-4abd-90fa-e723c5c21ad5' , url: 'git@gitlab.flusso.nl:CICD/ci-cd-docs.git' script { env . GIT_COMMIT_HASH = sh returnStdout: true , script: 'git rev-parse --verify HEAD' } } } stage ( 'Build Docs' ) { agent { docker { image \"caladreas/mkdocs-docker-build-container\" label \"docker\" } } steps { sh 'mkdocs build' } } stage ( 'Prepare Docker Image' ){ agent { label 'docker' } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true , script: 'docker run --rm -i lukasmartinelli/hadolint < Dockerfile' if ( lintResult . trim () == '' ) { println 'Lint finished with no errors' } else { println 'Error found in Lint' println \"${lintResult}\" currentBuild . result = 'UNSTABLE' } } }, // end test dockerfile BuildImage: { sh 'chmod +x build.sh' sh './build.sh' } ) } post { success { sh 'chmod +x push.sh' sh './push.sh' } } } stage ( 'Update Docker Container' ) { agent { label 'docker' } steps { sh 'chmod +x container-update.sh' sh \"./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH}\" } } } post { success { slackSend channel: 'cicd' , color: '#00FF00' , message: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" } failure { slackSend channel: 'cicd' , color: '#FF0000' , message: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" } } }","title":"MKDocs Build Example"},{"location":"jenkins-pipeline/declarative-pipeline/#resources","text":"Syntax Reference Getting started Notifications","title":"Resources"},{"location":"jenkins-pipeline/github-org/","text":"GitHub Organization Pipeline \u00b6 Add Webhook \u00b6 https://dzone.com/articles/adding-a-github-webhook-in-your-jenkins-pipeline ${JENKINS_URL}/github-webhook/","title":"GitHub Organization Pipeline"},{"location":"jenkins-pipeline/github-org/#github-organization-pipeline","text":"","title":"GitHub Organization Pipeline"},{"location":"jenkins-pipeline/github-org/#add-webhook","text":"https://dzone.com/articles/adding-a-github-webhook-in-your-jenkins-pipeline ${JENKINS_URL}/github-webhook/","title":"Add Webhook"},{"location":"jenkins-pipeline/global-shared-library/","text":"Global Shared Library \u00b6 https://jenkins.io/doc/book/pipeline/shared-libraries/ When you're making pipelines on Jenkins you will run into the situation that you will want to stay DRY . To share pipeline code there are several ways. SCM: Have a pipeline dsl script in a SCM and load it from there Plugin: A Jenkins plugin that you can call via the pipeline dsl Global Workflow Library: There is a global library for pipeline dsl scripts in the Jekins master Preferred solution Please read the documentation to get a basic idea. Danger When using a Global Library you will always have to import something from this library. This doesn't make sense when you online use functions (via the vars folder). In this case, you have to import nothing, which you do via: \"_\" @Library ( ' FlussoGlobal ' ) import nl.flusso.Utilities @Library ( ' FlussoGlobal ' ) _ Library Directory structure \u00b6 The directory structure of a shared library repository is as follows: 1 2 3 4 5 6 7 8 9 10 11 12 (root) +- src # Groovy source files | +- org | +- foo | +- Bar.groovy # for org.foo.Bar class +- vars | +- foo.groovy # for global 'foo' variable/function | +- foo.txt # help for 'foo' variable/function +- resources # resource files (external libraries only) | +- org | +- foo | +- bar.json # static helper data for org.foo.Bar The src directory should look like standard Java source directory structure. This directory is added to the classpath when executing Pipelines. The vars directory hosts scripts that define global variables accessible from Pipeline scripts. The basename of each *.groovy file should be a Groovy (~ Java) identifier, conventionally camelCased . The matching *.txt , if present, can contain documentation, processed through the system\u2019s configured markup formatter (so may really be HTML, Markdown, etc., though the txt extension is required). The Groovy source files in these directories get the same \u201cCPS transformation\u201d as your Pipeline scripts. A resources directory allows the libraryResource step to be used from an external library to load associated non-Groovy files. Currently this feature is not supported for internal libraries. Other directories under the root are reserved for future enhancements. Configure libraries in Jenkins \u00b6 The a Jenkins Master you can configure the Global Pipeline Libraries. You can find this in: Manage Jenkins -> Configure System -> Global Pipeline Libraries You can configure multiple libraries, where the there is a preference for Git repositories. You can select a default version (for example: the master branch), and either allow or disallow overrides to this. To be able to use a different version, you would use the @ in case of Git. @Library ( ' FlussoGlobal @my - feature - branch ' ) HelloWorld Example \u00b6 Create Git repository (see below for structure) Configure this Git repository as an \"Global Pipeline Libraries\" entry Name: FlussoGlobal Default Version: master Modern SCM: git Project repository: git@gitlab.flusso.nl :CICD/jenkins-pipeline-library.git Create the resources you want in the git repository Use the library in a pipeline Util Class (class) Example \u00b6 #!/usr/bin/groovy # /src/ nl /flusso/ Utilities . groovy package nl . flusso import java.io.Serializable class Utilities implements Serializable { def steps Utilities ( steps ) { this . steps = steps } def sayHello ( String name ) { steps . sh \"echo $name\" } } @Library ( 'FlussoGlobal' ) import nl.flusso.Utilities def utils = new Utilities ( steps ) node { String name = 'Joost' utils . sayHello ( name ) } Util method (var) Example \u00b6 #!/usr/bin/groovy # /vars/ sayHello . groovy def call ( name ) { // you can call any valid step functions from your code, just like you can from Pipeline scripts echo \"Hello world, ${name}\" } @Library ( 'FlussoGlobal' ) _ node { String name = 'Joost' sayHello name } Combining libraries \u00b6 Lets say you want to want to have a core library and multiple specific libraries that utilize these. There are several to do this, we will show two. Import both \u00b6 One way is to explicitly import both libraries in the Jenkinsfile. @Library ([ 'github.com/joostvdg/jenkins-pipeline-lib' , 'github.com/joostvdg/jenkins-pipeline-go' ]) _ Con: you have to import all the required libraries yourself Pro: you can specify the versions of each Implicit Import + Explicit Import \u00b6 You can also configure the core (in this case jenkins-pipeline-lib) as \"loaded implicitly\". This will make anything from this library available by default. Be careful with the naming of the vars though! The resulting Jenkinsfile would then be. @Library ( 'github.com/joostvdg/jenkins-pipeline-go' ) _ Resources \u00b6 implement-reusable-function-call","title":"Pipeline Libraries"},{"location":"jenkins-pipeline/global-shared-library/#global-shared-library","text":"https://jenkins.io/doc/book/pipeline/shared-libraries/ When you're making pipelines on Jenkins you will run into the situation that you will want to stay DRY . To share pipeline code there are several ways. SCM: Have a pipeline dsl script in a SCM and load it from there Plugin: A Jenkins plugin that you can call via the pipeline dsl Global Workflow Library: There is a global library for pipeline dsl scripts in the Jekins master Preferred solution Please read the documentation to get a basic idea. Danger When using a Global Library you will always have to import something from this library. This doesn't make sense when you online use functions (via the vars folder). In this case, you have to import nothing, which you do via: \"_\" @Library ( ' FlussoGlobal ' ) import nl.flusso.Utilities @Library ( ' FlussoGlobal ' ) _","title":"Global Shared Library"},{"location":"jenkins-pipeline/global-shared-library/#library-directory-structure","text":"The directory structure of a shared library repository is as follows: 1 2 3 4 5 6 7 8 9 10 11 12 (root) +- src # Groovy source files | +- org | +- foo | +- Bar.groovy # for org.foo.Bar class +- vars | +- foo.groovy # for global 'foo' variable/function | +- foo.txt # help for 'foo' variable/function +- resources # resource files (external libraries only) | +- org | +- foo | +- bar.json # static helper data for org.foo.Bar The src directory should look like standard Java source directory structure. This directory is added to the classpath when executing Pipelines. The vars directory hosts scripts that define global variables accessible from Pipeline scripts. The basename of each *.groovy file should be a Groovy (~ Java) identifier, conventionally camelCased . The matching *.txt , if present, can contain documentation, processed through the system\u2019s configured markup formatter (so may really be HTML, Markdown, etc., though the txt extension is required). The Groovy source files in these directories get the same \u201cCPS transformation\u201d as your Pipeline scripts. A resources directory allows the libraryResource step to be used from an external library to load associated non-Groovy files. Currently this feature is not supported for internal libraries. Other directories under the root are reserved for future enhancements.","title":"Library Directory structure"},{"location":"jenkins-pipeline/global-shared-library/#configure-libraries-in-jenkins","text":"The a Jenkins Master you can configure the Global Pipeline Libraries. You can find this in: Manage Jenkins -> Configure System -> Global Pipeline Libraries You can configure multiple libraries, where the there is a preference for Git repositories. You can select a default version (for example: the master branch), and either allow or disallow overrides to this. To be able to use a different version, you would use the @ in case of Git. @Library ( ' FlussoGlobal @my - feature - branch ' )","title":"Configure libraries in Jenkins"},{"location":"jenkins-pipeline/global-shared-library/#helloworld-example","text":"Create Git repository (see below for structure) Configure this Git repository as an \"Global Pipeline Libraries\" entry Name: FlussoGlobal Default Version: master Modern SCM: git Project repository: git@gitlab.flusso.nl :CICD/jenkins-pipeline-library.git Create the resources you want in the git repository Use the library in a pipeline","title":"HelloWorld Example"},{"location":"jenkins-pipeline/global-shared-library/#util-class-class-example","text":"#!/usr/bin/groovy # /src/ nl /flusso/ Utilities . groovy package nl . flusso import java.io.Serializable class Utilities implements Serializable { def steps Utilities ( steps ) { this . steps = steps } def sayHello ( String name ) { steps . sh \"echo $name\" } } @Library ( 'FlussoGlobal' ) import nl.flusso.Utilities def utils = new Utilities ( steps ) node { String name = 'Joost' utils . sayHello ( name ) }","title":"Util Class (class) Example"},{"location":"jenkins-pipeline/global-shared-library/#util-method-var-example","text":"#!/usr/bin/groovy # /vars/ sayHello . groovy def call ( name ) { // you can call any valid step functions from your code, just like you can from Pipeline scripts echo \"Hello world, ${name}\" } @Library ( 'FlussoGlobal' ) _ node { String name = 'Joost' sayHello name }","title":"Util method (var) Example"},{"location":"jenkins-pipeline/global-shared-library/#combining-libraries","text":"Lets say you want to want to have a core library and multiple specific libraries that utilize these. There are several to do this, we will show two.","title":"Combining libraries"},{"location":"jenkins-pipeline/global-shared-library/#import-both","text":"One way is to explicitly import both libraries in the Jenkinsfile. @Library ([ 'github.com/joostvdg/jenkins-pipeline-lib' , 'github.com/joostvdg/jenkins-pipeline-go' ]) _ Con: you have to import all the required libraries yourself Pro: you can specify the versions of each","title":"Import both"},{"location":"jenkins-pipeline/global-shared-library/#implicit-import-explicit-import","text":"You can also configure the core (in this case jenkins-pipeline-lib) as \"loaded implicitly\". This will make anything from this library available by default. Be careful with the naming of the vars though! The resulting Jenkinsfile would then be. @Library ( 'github.com/joostvdg/jenkins-pipeline-go' ) _","title":"Implicit Import + Explicit Import"},{"location":"jenkins-pipeline/global-shared-library/#resources","text":"implement-reusable-function-call","title":"Resources"},{"location":"jenkins-pipeline/groovy-pipeline/","text":"Jenkins Pipelines \u00b6 Warning This style of pipeline definition is deprecated. When possible, please use the declarative version. Jenkins Pipeline is a suite of plugins which supports implementing and integrating continuous delivery pipelines into Jenkins. Pipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines \"as code\" via the Pipeline DSL. There are two ways to create pipelines in Jenkins. Either via the Groovy DSL or via the Declarative pipeline . For more information about the declarative pipeline, read the next page . Hello World Example \u00b6 node { timestamps { stage ( 'My FIrst Stage' ) { if ( isUnix ()) { sh 'echo \"this is Unix!\"' } else { bat 'echo \"this is windows\"' } } } } Resources \u00b6 Getting started Best practices Best practices for scaling Possible Steps","title":"Groovy DSL Pipeline"},{"location":"jenkins-pipeline/groovy-pipeline/#jenkins-pipelines","text":"Warning This style of pipeline definition is deprecated. When possible, please use the declarative version. Jenkins Pipeline is a suite of plugins which supports implementing and integrating continuous delivery pipelines into Jenkins. Pipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines \"as code\" via the Pipeline DSL. There are two ways to create pipelines in Jenkins. Either via the Groovy DSL or via the Declarative pipeline . For more information about the declarative pipeline, read the next page .","title":"Jenkins Pipelines"},{"location":"jenkins-pipeline/groovy-pipeline/#hello-world-example","text":"node { timestamps { stage ( 'My FIrst Stage' ) { if ( isUnix ()) { sh 'echo \"this is Unix!\"' } else { bat 'echo \"this is windows\"' } } } }","title":"Hello World Example"},{"location":"jenkins-pipeline/groovy-pipeline/#resources","text":"Getting started Best practices Best practices for scaling Possible Steps","title":"Resources"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/","text":"IDE Integration for Jenkins Pipeline DSL \u00b6 Supported IDE's \u00b6 Currently only Jetbrain's Intelli J's IDEA is supported . This via a Groovy DSL file (.gdsl). Configure Intelli J IDEA \u00b6 Go to a Jenkins Pipeline job and open the Pipeline Syntax page. On the page in the left hand menu, you will see a link to download a Jenkins Master specific Groovy DSL file. Download this and save it into your project's workspace. It will have to be part of your classpath, the easiest way to do this is to add the file as pipeline.gdsl in a/the src folder. For more information, you can read Steffen Gerbert 's blog. Remarks from Kohsuke Kawaguchi \u00b6 More effort in this space will be taken by Cloudbees. But the priority is low compared to other initiatives. Integration of Pipeline Library \u00b6 If you're using the Global Shared Libraries for sharing generic pipeline building blocks, it would be nice to have this awareness in your editor as well. One of the ways to do this, is to checkout the source code of this library and make sure it is compiled. In your editor (assuming Intelli J IDEA) you can then add the compiled classes as dependency (type: classes). This way, at least every class defined in your library is usable as a normal dependency would be. Final configuration Intelli J IDEA \u00b6","title":"DSL IDE Integration"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#ide-integration-for-jenkins-pipeline-dsl","text":"","title":"IDE Integration for Jenkins Pipeline DSL"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#supported-ides","text":"Currently only Jetbrain's Intelli J's IDEA is supported . This via a Groovy DSL file (.gdsl).","title":"Supported IDE's"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#configure-intelli-j-idea","text":"Go to a Jenkins Pipeline job and open the Pipeline Syntax page. On the page in the left hand menu, you will see a link to download a Jenkins Master specific Groovy DSL file. Download this and save it into your project's workspace. It will have to be part of your classpath, the easiest way to do this is to add the file as pipeline.gdsl in a/the src folder. For more information, you can read Steffen Gerbert 's blog.","title":"Configure Intelli J IDEA"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#remarks-from-kohsuke-kawaguchi","text":"More effort in this space will be taken by Cloudbees. But the priority is low compared to other initiatives.","title":"Remarks from Kohsuke Kawaguchi"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#integration-of-pipeline-library","text":"If you're using the Global Shared Libraries for sharing generic pipeline building blocks, it would be nice to have this awareness in your editor as well. One of the ways to do this, is to checkout the source code of this library and make sure it is compiled. In your editor (assuming Intelli J IDEA) you can then add the compiled classes as dependency (type: classes). This way, at least every class defined in your library is usable as a normal dependency would be.","title":"Integration of Pipeline Library"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#final-configuration-intelli-j-idea","text":"","title":"Final configuration Intelli J IDEA"},{"location":"jenkins-pipeline/input/","text":"Jenkins Pipeline - Input \u00b6 The Jenkins Pipeline has a plugin for dealing with external input. Generally it is used to gather user input (values or approval), but it also has a REST API for this. General Info \u00b6 The Pipeline Input Step allows you to The plugin allows you to capture input in a variety of ways, but there are some gotcha's. If you have a single parameter, it will be returned as a single value If you have multiple parameters, it will be returned as a map The choices for the Choice parameter should be a single line, where values are separated with /n Don't use input within a node {}, as this will block an executor slot .. Examples \u00b6 Single Parameter \u00b6 def hello = input id: 'CustomId' , message: 'Want to continue?' , ok: 'Yes' , parameters: [ string ( defaultValue: 'world' , description: '' , name: 'hello' )] node { println \"echo $hello\" } Multiple Parameters \u00b6 def userInput = input id: 'CustomId' , message: 'Want to continue?' , ok: 'Yes' , parameters: [ string ( defaultValue: 'world' , description: '' , name: 'hello' ), string ( defaultValue: '' , description: '' , name: 'token' )] node { def hello = userInput [ 'hello' ] def token = userInput [ 'token' ] println \"hello=$hello, token=$token\" } Timeout on Input \u00b6 def userInput timeout ( time: 10 , unit: 'SECONDS' ) { println 'Waiting for input' userInput = input id: 'CustomId' , message: 'Want to continue?' , ok: 'Yes' , parameters: [ string ( defaultValue: 'world' , description: '' , name: 'hello' ), string ( defaultValue: '' , description: '' , name: 'token' )] } REST API \u00b6 There's a rest API for sending the input to a waiting input step. The format of the url: {JenkinsURL}/ {JenkinsURL}/ {JobURL}/ {Build#}/input/ {Build#}/input/ {InputID}/submit. There are some things to keep in mind: If Jenkins has CSRF protection enabled, you need a Crumb (see below) for the requests Requests are send via POST For supplying values you need to have a JSON with the parameters with as json param You need to supply the proceed value: the value of the ok button, as proceed param You will have to fill in the input id , so it is best to configure a unique input id for the input steps you want to connect to from outside Examples \u00b6 { \"parameter\" : [ { \"name\" : \"hello\" , \"value\" : \"joost\" }, { \"name\" : \"token\" , \"value\" : \"not a token\" } ] } # single parameter curl --user $USER : $PASS -X POST -H \"Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33\" -d json = '{\"parameter\": {\"name\": \"hello\", \"value\": \"joost\"}}' -d proceed = 'Yes' 'https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit' # Multiple Parameters curl --user $USER : $PASS -X POST -H \"Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33\" -d json = '{\"parameter\": [{\"name\": \"hello\", \"value\": \"joost\"},{\"name\": \"token\", \"value\": \"not a token\"}]}' -d proceed = 'Yes' 'https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit' Crumb (secured Jenkins) \u00b6 If Jenkins is secured against CSRF (via Global Security: Prevent Cross Site Request Forgery exploits), any API call requires a Crumb. You can read more about it here . To get a valid crumb you have to send a crumb request as authenticated user. JSON: https://ci.flusso.nl/jenkins/crumbIssuer/api/json XML (parsed): https://ci.flusso.nl/jenkins/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,%22:%22,//crumb )","title":"Input"},{"location":"jenkins-pipeline/input/#jenkins-pipeline-input","text":"The Jenkins Pipeline has a plugin for dealing with external input. Generally it is used to gather user input (values or approval), but it also has a REST API for this.","title":"Jenkins Pipeline - Input"},{"location":"jenkins-pipeline/input/#general-info","text":"The Pipeline Input Step allows you to The plugin allows you to capture input in a variety of ways, but there are some gotcha's. If you have a single parameter, it will be returned as a single value If you have multiple parameters, it will be returned as a map The choices for the Choice parameter should be a single line, where values are separated with /n Don't use input within a node {}, as this will block an executor slot ..","title":"General Info"},{"location":"jenkins-pipeline/input/#examples","text":"","title":"Examples"},{"location":"jenkins-pipeline/input/#single-parameter","text":"def hello = input id: 'CustomId' , message: 'Want to continue?' , ok: 'Yes' , parameters: [ string ( defaultValue: 'world' , description: '' , name: 'hello' )] node { println \"echo $hello\" }","title":"Single Parameter"},{"location":"jenkins-pipeline/input/#multiple-parameters","text":"def userInput = input id: 'CustomId' , message: 'Want to continue?' , ok: 'Yes' , parameters: [ string ( defaultValue: 'world' , description: '' , name: 'hello' ), string ( defaultValue: '' , description: '' , name: 'token' )] node { def hello = userInput [ 'hello' ] def token = userInput [ 'token' ] println \"hello=$hello, token=$token\" }","title":"Multiple Parameters"},{"location":"jenkins-pipeline/input/#timeout-on-input","text":"def userInput timeout ( time: 10 , unit: 'SECONDS' ) { println 'Waiting for input' userInput = input id: 'CustomId' , message: 'Want to continue?' , ok: 'Yes' , parameters: [ string ( defaultValue: 'world' , description: '' , name: 'hello' ), string ( defaultValue: '' , description: '' , name: 'token' )] }","title":"Timeout on Input"},{"location":"jenkins-pipeline/input/#rest-api","text":"There's a rest API for sending the input to a waiting input step. The format of the url: {JenkinsURL}/ {JenkinsURL}/ {JobURL}/ {Build#}/input/ {Build#}/input/ {InputID}/submit. There are some things to keep in mind: If Jenkins has CSRF protection enabled, you need a Crumb (see below) for the requests Requests are send via POST For supplying values you need to have a JSON with the parameters with as json param You need to supply the proceed value: the value of the ok button, as proceed param You will have to fill in the input id , so it is best to configure a unique input id for the input steps you want to connect to from outside","title":"REST API"},{"location":"jenkins-pipeline/input/#examples_1","text":"{ \"parameter\" : [ { \"name\" : \"hello\" , \"value\" : \"joost\" }, { \"name\" : \"token\" , \"value\" : \"not a token\" } ] } # single parameter curl --user $USER : $PASS -X POST -H \"Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33\" -d json = '{\"parameter\": {\"name\": \"hello\", \"value\": \"joost\"}}' -d proceed = 'Yes' 'https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit' # Multiple Parameters curl --user $USER : $PASS -X POST -H \"Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33\" -d json = '{\"parameter\": [{\"name\": \"hello\", \"value\": \"joost\"},{\"name\": \"token\", \"value\": \"not a token\"}]}' -d proceed = 'Yes' 'https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit'","title":"Examples"},{"location":"jenkins-pipeline/input/#crumb-secured-jenkins","text":"If Jenkins is secured against CSRF (via Global Security: Prevent Cross Site Request Forgery exploits), any API call requires a Crumb. You can read more about it here . To get a valid crumb you have to send a crumb request as authenticated user. JSON: https://ci.flusso.nl/jenkins/crumbIssuer/api/json XML (parsed): https://ci.flusso.nl/jenkins/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,%22:%22,//crumb )","title":"Crumb (secured Jenkins)"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/","text":"Parallel Pipeline \u00b6 Building applications can be fun, but it can also cause a lot of wait time 3 . There are many ways to speed up builds, do fewer tests, get bigger and better hardware, or run some tasks in parallel. Jenkins Pipelines can do parallel stages for a while, even in the Declarative format 1 . Although doing parallel pipelines, Jenkins didn't become awesome until Sequential Stages 2 . We will dive into the magic of Sequential Stages , but first, let's start with building in parallel. Parallel Stages \u00b6 This is an elementary example. We have an application we want to build in Java 11 - latest LTS - and the latest version of Java - now Java 13. As both are running in their own containers, each can leverage its own resources - provided the underlying VM has them available. pipeline { agent { kubernetes { label 'jx-maven-lib' yaml \"\"\" apiVersion: v1 kind: Pod spec: containers: - name: maven11 image: maven:3-jdk-11 command: ['cat'] tty: true - name: maven13 image: maven:3-jdk-13 command: ['cat'] tty: true \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/jx-maven-lib.git' } } stage ( 'Run Tests' ) { parallel { stage ( 'Java 11' ) { steps { container ( 'maven11' ) { sh 'mvn -V -e -C verify' } } } stage ( 'Java 13' ) { steps { container ( 'maven13' ) { sh 'mvn -V -e -C -Pjava13 verify' } } } } } } } Visualization \u00b6 This will then look like this: Sequential \u00b6 Before we dive into all the benefits of Sequential Stages , let's look at how the syntax changes. With Parallel Stages, we can execute some steps in parallel, but with regards to visualizing individual steps, it is inferior. Sequential Stages allows us to add Stages in sequence within a Parallel step, which is why I usually call them Parallel Sequential stages. In summary, the syntax now becomes: stages { stage ( 'Checkout' ) { stage ( 'Run Tests' ) { parallel { stage ( 'Java 11' ) { stages { stage ( 'Build' ) { steps {} } } } } } } } The full example: pipeline { agent { kubernetes { label 'jx-maven-lib' yaml \"\"\" apiVersion: v1 kind: Pod spec: containers: - name: maven11 image: maven:3-jdk-11 command: ['cat'] tty: true - name: maven13 image: maven:3-jdk-13 command: ['cat'] tty: true \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/jx-maven-lib.git' } } stage ( 'Run Tests' ) { parallel { stage ( 'Java 11' ) { stages { stage ( 'Build' ) { steps { container ( 'maven11' ) { sh 'mvn -V -e -C package' } } } stage ( 'Test' ) { steps { container ( 'maven11' ) { sh 'mvn -V -e -C test' } } } } } stage ( 'Java 13' ) { stages { stage ( 'Build' ) { steps { container ( 'maven13' ) { sh 'mvn -V -e -C -Pjava13 package' } } } stage ( 'Test' ) { steps { container ( 'maven13' ) { sh 'mvn -V -e -C -Pjava13 test' } } } } } } } } } Visualization \u00b6 The first thing we notice is that we get more indenting and more { } . But, we can also visualize independent stages within the parallel \"streams\". If you're thinking, but can't I now do much more with stage individual stages? You're right, and we'll dive into that next. Sequential With Separate Pods \u00b6 The biggest downside of the previous examples is that the Kubernetes Pod is always there, including all the containers we need. But what if some parallel tasks take much longer than others? It would be great if the other containers would be removed as soon if we're done with them. With Sequential stages, we can achieve this. We first set agent none , to make sure we don't have a Pod running from start to finish. This comes with a price, though, now every stage will need to have its own agent defined. Luckily, combining Parallel and Sequential stages, we can give each parallel \"stream\" an agent - a Pod - and have each sequential stage use this. In summary, we do this: Pipeline: no agent parallel: Build stream java 11: agent maven11 stream java 13: agent maven13 parallel: Test stream java 11: agent maven11 Functional Tests API Contract Tests Performance Tests stream java 13: agent maven13 Functional Tests API Contract Tests Performance Tests Deploy Another benefit is that each stage can leverage every Declarative Directive 4 , such as when { } 5 . In this example, we've used when { branch 'master' } to avoid executing steps when we're not on branch master . To extend this even further, we can now leverage both the dynamic Pod allocation and the When Directive. When combined with beforeAgent true , we won't even spin up the Pod, avoiding unnecessary resource consumption and waiting. stage ( 'Deploy' ) { agent { ... } when { branch 'master' beforeAgent true } steps { echo \"hello\" } } The complete example now looks like this. pipeline { agent none stages { stage ( 'Build' ) { parallel { stage ( 'Java 11' ) { agent { kubernetes { label 'jxmavenlib-jdk11_build' containerTemplate { name 'maven11' image 'maven:3-jdk-11' ttyEnabled true command 'cat' } } } steps { container ( 'maven11' ) { sh 'mvn -v' } } } stage ( 'Java 13' ) { agent { kubernetes { label 'jxmavenlib-jdk13-build' containerTemplate { name 'maven13' image 'maven:3-jdk-13' ttyEnabled true command 'cat' } } } steps { container ( 'maven13' ) { sh 'mvn -v' } } } } } stage ( 'Test' ) { parallel { stage ( 'Java 11' ) { agent { kubernetes { label 'jxmavenlib-jdk11-test' containerTemplate { name 'maven' image 'maven:3-jdk-11' ttyEnabled true command 'cat' } } } stages { stage ( 'Functional Tests' ) { steps { echo 'Hello' } } stage ( 'API Contract Tests' ) { steps { echo 'Hello' } } stage ( 'Performance Tests' ) { when { branch 'master' } steps { echo 'Hello' } } } } stage ( 'Java 13' ) { agent { kubernetes { label 'jxmavenlib-jdk13-test' containerTemplate { name 'mavenjdk11' image 'maven:3-jdk-13' ttyEnabled true command 'cat' } } } stages { stage ( 'Functional Tests' ) { steps { echo 'Hello' } } stage ( 'API Contract Tests' ) { steps { echo 'Hello' } } stage ( 'Performance Tests' ) { when { branch 'master' } steps { echo 'Hello' } } } } } } stage ( 'Deploy' ) { agent { kubernetes { label 'jxmavenlib-deployment' containerTemplate { name 'pl_deployment' image 'cloudbees/docker-java-with-docker-client' ttyEnabled true command 'cat' } } } when { branch 'master' beforeAgent true } steps { echo \"hello\" } } } } Visualization \u00b6 When visualized, we can spot which stages are skipped due to the when {} Directives. Sequential With Optional & Reusable Pods \u00b6 We're not done yet \u2014 one more step to take. While dynamically allocating Pods with containers sounds excellent, it comes at a cost. If you do something in the build phase and you need the data in the latest stages, you've now lost it - new Pod = new workspace. There are a few things you can do, but none of them pretty. You can use stash 6 and unstash [\u02c67], but these can be very costly in terms of I/O performance and time. Alternatively, you can either look for externalizing your workspace or keep the Pod around for reuse. Important If you can run more than one build at the same time - concurrent builds - you run the risk of having builds claim Pods another build has done work in. To avoid builds reusing Pods from other runs, you can disable concurrent builds. options { disableConcurrentBuilds () } Alternatively, you can encode the build number into the name: agent { kubernetes { label \"jxmavenlib-jdk11-b${BUILD_NUMBER}\" yaml ... } } Pod Reuse \u00b6 To reuse a Pod, we have to override some default values in the PodTemplate 10 . idleMinutes : Allows the Pod to remain active for reuse until the configured number of minutes has passed since the last step was executed on it. The configuration below means the Pod can be idle for about 5 minutes before it gets deleted. Additionally, we changed the label not to include the phase name. Otherwise, we cannot get the same Pod. agent { kubernetes { idleMinutes 5 label 'jxmavenlib-jdk11' yaml \"\"\" spec: containers: - name: maven11 image: maven:3-jdk-11 command: ['cat'] tty: true \"\"\" } } Volume For Externalizing Workspace \u00b6 There are various ways to externalize your workspace, you can leverage NFS 8 or Jenkins workspace related plugins 9 . One way we'll look at here is to leverage a PersistedVolume in Kubernetes - which could be of any kind, incl NFS 8 . We add a volume of type persistentVolumeClaim and point to an existing one by claimName . volumeMounts : - name : build-cache mountPath : /tmp/cache volumes : - name : build-cache persistentVolumeClaim : claimName : azure-managed-disk azure-managed-disk-pvc.yaml apiVersion : v1 kind : PersistentVolumeClaim metadata : name : azure-managed-disk spec : accessModes : - ReadWriteOnce storageClassName : managed-premium resources : requests : storage : 5Gi Full Example \u00b6 pipeline { agent none options { disableConcurrentBuilds () } stages { stage ( 'Build' ) { parallel { stage ( 'Java 11' ) { agent { kubernetes { idleMinutes 5 label \"jxmavenlib-jdk11-b${BUILD_NUMBER}\" yaml \"\"\" spec: containers: - name: maven11 image: maven:3-jdk-11 command: ['cat'] tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2/repository - name: build-cache mountPath: /tmp/cache volumes: - name: maven-cache hostPath: path: /tmp type: Directory - name: build-cache persistentVolumeClaim: claimName: azure-managed-disk \"\"\" } } steps { git 'https://github.com/joostvdg/jx-maven-lib.git' container ( 'maven11' ) { sh 'mvn -V -e -C verify' sh 'cp -R target/ /tmp/cache/' sh 'ls -lath /tmp/cache/' } } } stage ( 'Java 13' ) { agent { kubernetes { idleMinutes 5 label \"jxmavenlib-jdk13-b${BUILD_NUMBER}\" yaml \"\"\" spec: containers: - name: maven13 image: maven:3-jdk-13 command: ['cat'] tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2/repository volumes: - name: maven-cache hostPath: path: /tmp type: Directory \"\"\" } } steps { git 'https://github.com/joostvdg/jx-maven-lib.git' container ( 'maven13' ) { sh 'mvn -V -e -C -Pjava13 verify' } } } } } stage ( 'Test' ) { parallel { stage ( 'Java 11' ) { agent { kubernetes { idleMinutes 5 label \"jxmavenlib-jdk11-b${BUILD_NUMBER}\" yaml \"\"\" spec: containers: - name: maven11 image: maven:3-jdk-11 command: ['cat'] tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2/repository - name: build-cache mountPath: /tmp/cache volumes: - name: maven-cache hostPath: path: /tmp type: Directory - name: build-cache persistentVolumeClaim: claimName: azure-managed-disk \"\"\" } } stages { stage ( 'Functional Tests' ) { steps { container ( 'maven11' ) { sh 'ls -lath /tmp/cache' sh 'cp -R /tmp/cache/ .' sh 'ls -lath' } } } stage ( 'API Contract Tests' ) { steps { echo 'Hello' } } stage ( 'Performance Tests' ) { when { branch 'master' } steps { echo 'Hello' } } } } stage ( 'Java 13' ) { agent { kubernetes { idleMinutes 5 label \"jxmavenlib-jdk13-b${BUILD_NUMBER}\" yaml \"\"\" spec: containers: - name: maven13 image: maven:3-jdk-13 command: ['cat'] tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2/repository volumes: - name: maven-cache hostPath: path: /tmp type: Directory \"\"\" } } stages { stage ( 'Functional Tests' ) { steps { container ( 'maven13' ) { sh 'ls -lath' } } } stage ( 'API Contract Tests' ) { steps { echo 'Hello' } } stage ( 'Performance Tests' ) { when { branch 'master' } steps { echo 'Hello' } } } } } } stage ( 'Deploy' ) { agent { kubernetes { label 'jxmavenlib-deployment' containerTemplate { name 'pl_deployment' image 'cloudbees/docker-java-with-docker-client' ttyEnabled true command 'cat' } } } when { branch 'master' beforeAgent true } steps { echo \"hello\" } } } } Matrix \u00b6 In late 2019, Jenkins released the Matrix Build feature. It extends the Parallel and Sequential features and allows you to create a Matrix of options for parallel execution. If you want to get a more in-depth look at this feature, I recommend reading Liam Newman's 11 excellent introductory blog post. When you've read this and Liam's posts, I recommend you go through the 12 test cases of the Matrix build feature on GitHub. They provide a wealth of information and worked-out examples. What Is It \u00b6 The Matrix build feature extends the Parallel and Sequential features mentioned earlier in this article. What it allows you to do is to specify multiple sets of values ( axis ), and then execute each entry of the cartesian product 13 . Matrix acts as a special stage type, and can include an agent{} and stages{} definition. matrix { axes { } excludes {} agent {} stages {} } Axis \u00b6 The mechanism for specifying the sets of values is called axis . You start by specifying the matrix build within a stage. pipeline { agent none stages { stage ( 'matrix build' ) { matrix { } } } } Within matrix you specify the axis with one or more axis . matrix { axes { axis { name 'JDK_VERSION' values '8' , '11' , '13' } axis { name 'JDK_TYPE' values 'ibmjava' , 'amazoncorretto' , 'jdk' } } } In this case, we will get these nine combinations: jdk + 8, jdk + 11, jdk + 13 ibmjava + 8, ibmjava + 11, ibmjava + 13 amazoncorretto + 8, amazoncorretto + 11, amazoncorretto + 13 Exclusions \u00b6 Unfortunately, several of these are not valid combinations. The ibmjava only has a JDK 8 version, and the default (open)JDK is the only one with a JDK 13 version (at this time). You can filter these out with a when directive, but that would pollute the pipeline view. Wouldn't it be better to ensure the invalid combinations aren't even considered? You can tell the matrix build feature just that, by setting exclusions . You start with an exclusions {} followed by one or more exclusion{} blocks. Let's exclude JDK 11 from IBM's java. excludes { exclude { axis { name 'JDK_VERSION' values '11' } axis { name 'JDK_TYPE' values \"ibmjava\" } } } Sometimes, you want to limit a combination to a single axis . Meaning, exclude every combination of x , except for y . We can do just that, by specifying notValues in the exclusion. Let's ensure only jdk will be paired with 13 . exclude { axis { name 'JDK_VERSION' values '13' } axis { name 'JDK_TYPE' notValues 'jdk' // double negative, make sure we only do 13 with jdk } } Together With PodTemplates \u00b6 As with previous examples of the Parallel and Sequential features, Matrix adds an extra dimension to using PodTemplates. Remember the structure of a Matrix ? matrix { axes { } excludes {} agent {} stages {} } It means we can structure of the Sequential stages, by adding a stages {} block and a single agent{} block for those stages. Because the PodTemplate properties are variables, the values we put in there can be dynamic. This means we can leverage the axis values as input for which container images to use for our PodTemplate. matrix { axes {} agent { kubernetes { label \"maven-${JDK_TYPE}-${JDK_VERSION}-test\" containerTemplate { name 'maven' image \"maven:3-${JDK_TYPE}-${JDK_VERSION}\" ttyEnabled true command 'cat' } } } stages {} } Visual Representation \u00b6 In the visual representation, we can the benefits of the Sequential stages with Matrix. Each axis combination will have its own set of stages, which can also have their own when{} directives. Complete Example \u00b6 pipeline { agent none stages { stage ( 'Prepare' ) { agent { kubernetes { label \"maven\" containerTemplate { name 'maven' image \"maven\" ttyEnabled true command 'cat' } } } steps { println 'hello world!' container ( 'maven' ) { sh 'env' } } } stage ( 'Test' ) { matrix { axes { axis { name 'JDK_VERSION' values '8' , '11' , '13' } axis { name 'JDK_TYPE' values 'ibmjava' , 'amazoncorretto' , 'jdk' } } excludes { exclude { axis { name 'JDK_VERSION' values '13' } axis { name 'JDK_TYPE' notValues 'jdk' // double negative, make sure we only do 13 with jdk } } exclude { axis { name 'JDK_VERSION' values '11' } axis { name 'JDK_TYPE' values \"ibmjava\" } } } agent { kubernetes { label \"maven-${JDK_TYPE}-${JDK_VERSION}-test\" containerTemplate { name 'maven' image \"maven:3-${JDK_TYPE}-${JDK_VERSION}\" ttyEnabled true command 'cat' } } } stages { stage ( 'Test Image' ) { steps { println \"Using Image: maven:3-${JDK_TYPE}-${JDK_VERSION}\" } } stage ( \"Build\" ) { steps { sh 'uname -a' git 'https://github.com/joostvdg/jx-maven-lib.git' container ( 'maven' ) { sh 'mvn clean verify --show-version --strict-checksums -e' } } } stage ( \"IT Test\" ) { when { branch 'master' } steps { echo \"WE SHOULD NEVER GET HERE\" } } } } } stage ( 'Publish' ) { agent { kubernetes { label \"maven\" containerTemplate { name 'maven' image \"maven\" ttyEnabled true command 'cat' } } } steps { println 'hello world!' container ( 'maven' ) { sh 'env' } } } } } References \u00b6 Jenkins Pipeline - Parallel Stages \u21a9 Jenkins Pipeline - Introducing Sequential Stages \u21a9 XKCD - Code's Compiling \u21a9 Jenkins Pipeline Syntax \u21a9 Jenkins Declarative Pipeline - When Directive \u21a9 Jenkins Pipeline - Step Stash \u21a9 Jenkins Pipeline - Step Unstash \u21a9 Kubernetes NFS Storage Provisioner \u21a9 \u21a9 Jenkins External Workspace Manager Plugin \u21a9 Jenkins Kubernetes Plugin - template values explained \u21a9 Jenkins Blog Introducing Matrix Build Feature \u21a9 GitHub Repository containing test examples of the Matrix Build Feature \u21a9 Wikipedia - Cartesian Product \u21a9","title":"Parallel Pipelines"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#parallel-pipeline","text":"Building applications can be fun, but it can also cause a lot of wait time 3 . There are many ways to speed up builds, do fewer tests, get bigger and better hardware, or run some tasks in parallel. Jenkins Pipelines can do parallel stages for a while, even in the Declarative format 1 . Although doing parallel pipelines, Jenkins didn't become awesome until Sequential Stages 2 . We will dive into the magic of Sequential Stages , but first, let's start with building in parallel.","title":"Parallel Pipeline"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#parallel-stages","text":"This is an elementary example. We have an application we want to build in Java 11 - latest LTS - and the latest version of Java - now Java 13. As both are running in their own containers, each can leverage its own resources - provided the underlying VM has them available. pipeline { agent { kubernetes { label 'jx-maven-lib' yaml \"\"\" apiVersion: v1 kind: Pod spec: containers: - name: maven11 image: maven:3-jdk-11 command: ['cat'] tty: true - name: maven13 image: maven:3-jdk-13 command: ['cat'] tty: true \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/jx-maven-lib.git' } } stage ( 'Run Tests' ) { parallel { stage ( 'Java 11' ) { steps { container ( 'maven11' ) { sh 'mvn -V -e -C verify' } } } stage ( 'Java 13' ) { steps { container ( 'maven13' ) { sh 'mvn -V -e -C -Pjava13 verify' } } } } } } }","title":"Parallel Stages"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#visualization","text":"This will then look like this:","title":"Visualization"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#sequential","text":"Before we dive into all the benefits of Sequential Stages , let's look at how the syntax changes. With Parallel Stages, we can execute some steps in parallel, but with regards to visualizing individual steps, it is inferior. Sequential Stages allows us to add Stages in sequence within a Parallel step, which is why I usually call them Parallel Sequential stages. In summary, the syntax now becomes: stages { stage ( 'Checkout' ) { stage ( 'Run Tests' ) { parallel { stage ( 'Java 11' ) { stages { stage ( 'Build' ) { steps {} } } } } } } } The full example: pipeline { agent { kubernetes { label 'jx-maven-lib' yaml \"\"\" apiVersion: v1 kind: Pod spec: containers: - name: maven11 image: maven:3-jdk-11 command: ['cat'] tty: true - name: maven13 image: maven:3-jdk-13 command: ['cat'] tty: true \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/jx-maven-lib.git' } } stage ( 'Run Tests' ) { parallel { stage ( 'Java 11' ) { stages { stage ( 'Build' ) { steps { container ( 'maven11' ) { sh 'mvn -V -e -C package' } } } stage ( 'Test' ) { steps { container ( 'maven11' ) { sh 'mvn -V -e -C test' } } } } } stage ( 'Java 13' ) { stages { stage ( 'Build' ) { steps { container ( 'maven13' ) { sh 'mvn -V -e -C -Pjava13 package' } } } stage ( 'Test' ) { steps { container ( 'maven13' ) { sh 'mvn -V -e -C -Pjava13 test' } } } } } } } } }","title":"Sequential"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#visualization_1","text":"The first thing we notice is that we get more indenting and more { } . But, we can also visualize independent stages within the parallel \"streams\". If you're thinking, but can't I now do much more with stage individual stages? You're right, and we'll dive into that next.","title":"Visualization"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#sequential-with-separate-pods","text":"The biggest downside of the previous examples is that the Kubernetes Pod is always there, including all the containers we need. But what if some parallel tasks take much longer than others? It would be great if the other containers would be removed as soon if we're done with them. With Sequential stages, we can achieve this. We first set agent none , to make sure we don't have a Pod running from start to finish. This comes with a price, though, now every stage will need to have its own agent defined. Luckily, combining Parallel and Sequential stages, we can give each parallel \"stream\" an agent - a Pod - and have each sequential stage use this. In summary, we do this: Pipeline: no agent parallel: Build stream java 11: agent maven11 stream java 13: agent maven13 parallel: Test stream java 11: agent maven11 Functional Tests API Contract Tests Performance Tests stream java 13: agent maven13 Functional Tests API Contract Tests Performance Tests Deploy Another benefit is that each stage can leverage every Declarative Directive 4 , such as when { } 5 . In this example, we've used when { branch 'master' } to avoid executing steps when we're not on branch master . To extend this even further, we can now leverage both the dynamic Pod allocation and the When Directive. When combined with beforeAgent true , we won't even spin up the Pod, avoiding unnecessary resource consumption and waiting. stage ( 'Deploy' ) { agent { ... } when { branch 'master' beforeAgent true } steps { echo \"hello\" } } The complete example now looks like this. pipeline { agent none stages { stage ( 'Build' ) { parallel { stage ( 'Java 11' ) { agent { kubernetes { label 'jxmavenlib-jdk11_build' containerTemplate { name 'maven11' image 'maven:3-jdk-11' ttyEnabled true command 'cat' } } } steps { container ( 'maven11' ) { sh 'mvn -v' } } } stage ( 'Java 13' ) { agent { kubernetes { label 'jxmavenlib-jdk13-build' containerTemplate { name 'maven13' image 'maven:3-jdk-13' ttyEnabled true command 'cat' } } } steps { container ( 'maven13' ) { sh 'mvn -v' } } } } } stage ( 'Test' ) { parallel { stage ( 'Java 11' ) { agent { kubernetes { label 'jxmavenlib-jdk11-test' containerTemplate { name 'maven' image 'maven:3-jdk-11' ttyEnabled true command 'cat' } } } stages { stage ( 'Functional Tests' ) { steps { echo 'Hello' } } stage ( 'API Contract Tests' ) { steps { echo 'Hello' } } stage ( 'Performance Tests' ) { when { branch 'master' } steps { echo 'Hello' } } } } stage ( 'Java 13' ) { agent { kubernetes { label 'jxmavenlib-jdk13-test' containerTemplate { name 'mavenjdk11' image 'maven:3-jdk-13' ttyEnabled true command 'cat' } } } stages { stage ( 'Functional Tests' ) { steps { echo 'Hello' } } stage ( 'API Contract Tests' ) { steps { echo 'Hello' } } stage ( 'Performance Tests' ) { when { branch 'master' } steps { echo 'Hello' } } } } } } stage ( 'Deploy' ) { agent { kubernetes { label 'jxmavenlib-deployment' containerTemplate { name 'pl_deployment' image 'cloudbees/docker-java-with-docker-client' ttyEnabled true command 'cat' } } } when { branch 'master' beforeAgent true } steps { echo \"hello\" } } } }","title":"Sequential With Separate Pods"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#visualization_2","text":"When visualized, we can spot which stages are skipped due to the when {} Directives.","title":"Visualization"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#sequential-with-optional-reusable-pods","text":"We're not done yet \u2014 one more step to take. While dynamically allocating Pods with containers sounds excellent, it comes at a cost. If you do something in the build phase and you need the data in the latest stages, you've now lost it - new Pod = new workspace. There are a few things you can do, but none of them pretty. You can use stash 6 and unstash [\u02c67], but these can be very costly in terms of I/O performance and time. Alternatively, you can either look for externalizing your workspace or keep the Pod around for reuse. Important If you can run more than one build at the same time - concurrent builds - you run the risk of having builds claim Pods another build has done work in. To avoid builds reusing Pods from other runs, you can disable concurrent builds. options { disableConcurrentBuilds () } Alternatively, you can encode the build number into the name: agent { kubernetes { label \"jxmavenlib-jdk11-b${BUILD_NUMBER}\" yaml ... } }","title":"Sequential With Optional &amp; Reusable Pods"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#pod-reuse","text":"To reuse a Pod, we have to override some default values in the PodTemplate 10 . idleMinutes : Allows the Pod to remain active for reuse until the configured number of minutes has passed since the last step was executed on it. The configuration below means the Pod can be idle for about 5 minutes before it gets deleted. Additionally, we changed the label not to include the phase name. Otherwise, we cannot get the same Pod. agent { kubernetes { idleMinutes 5 label 'jxmavenlib-jdk11' yaml \"\"\" spec: containers: - name: maven11 image: maven:3-jdk-11 command: ['cat'] tty: true \"\"\" } }","title":"Pod Reuse"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#volume-for-externalizing-workspace","text":"There are various ways to externalize your workspace, you can leverage NFS 8 or Jenkins workspace related plugins 9 . One way we'll look at here is to leverage a PersistedVolume in Kubernetes - which could be of any kind, incl NFS 8 . We add a volume of type persistentVolumeClaim and point to an existing one by claimName . volumeMounts : - name : build-cache mountPath : /tmp/cache volumes : - name : build-cache persistentVolumeClaim : claimName : azure-managed-disk azure-managed-disk-pvc.yaml apiVersion : v1 kind : PersistentVolumeClaim metadata : name : azure-managed-disk spec : accessModes : - ReadWriteOnce storageClassName : managed-premium resources : requests : storage : 5Gi","title":"Volume For Externalizing Workspace"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#full-example","text":"pipeline { agent none options { disableConcurrentBuilds () } stages { stage ( 'Build' ) { parallel { stage ( 'Java 11' ) { agent { kubernetes { idleMinutes 5 label \"jxmavenlib-jdk11-b${BUILD_NUMBER}\" yaml \"\"\" spec: containers: - name: maven11 image: maven:3-jdk-11 command: ['cat'] tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2/repository - name: build-cache mountPath: /tmp/cache volumes: - name: maven-cache hostPath: path: /tmp type: Directory - name: build-cache persistentVolumeClaim: claimName: azure-managed-disk \"\"\" } } steps { git 'https://github.com/joostvdg/jx-maven-lib.git' container ( 'maven11' ) { sh 'mvn -V -e -C verify' sh 'cp -R target/ /tmp/cache/' sh 'ls -lath /tmp/cache/' } } } stage ( 'Java 13' ) { agent { kubernetes { idleMinutes 5 label \"jxmavenlib-jdk13-b${BUILD_NUMBER}\" yaml \"\"\" spec: containers: - name: maven13 image: maven:3-jdk-13 command: ['cat'] tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2/repository volumes: - name: maven-cache hostPath: path: /tmp type: Directory \"\"\" } } steps { git 'https://github.com/joostvdg/jx-maven-lib.git' container ( 'maven13' ) { sh 'mvn -V -e -C -Pjava13 verify' } } } } } stage ( 'Test' ) { parallel { stage ( 'Java 11' ) { agent { kubernetes { idleMinutes 5 label \"jxmavenlib-jdk11-b${BUILD_NUMBER}\" yaml \"\"\" spec: containers: - name: maven11 image: maven:3-jdk-11 command: ['cat'] tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2/repository - name: build-cache mountPath: /tmp/cache volumes: - name: maven-cache hostPath: path: /tmp type: Directory - name: build-cache persistentVolumeClaim: claimName: azure-managed-disk \"\"\" } } stages { stage ( 'Functional Tests' ) { steps { container ( 'maven11' ) { sh 'ls -lath /tmp/cache' sh 'cp -R /tmp/cache/ .' sh 'ls -lath' } } } stage ( 'API Contract Tests' ) { steps { echo 'Hello' } } stage ( 'Performance Tests' ) { when { branch 'master' } steps { echo 'Hello' } } } } stage ( 'Java 13' ) { agent { kubernetes { idleMinutes 5 label \"jxmavenlib-jdk13-b${BUILD_NUMBER}\" yaml \"\"\" spec: containers: - name: maven13 image: maven:3-jdk-13 command: ['cat'] tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2/repository volumes: - name: maven-cache hostPath: path: /tmp type: Directory \"\"\" } } stages { stage ( 'Functional Tests' ) { steps { container ( 'maven13' ) { sh 'ls -lath' } } } stage ( 'API Contract Tests' ) { steps { echo 'Hello' } } stage ( 'Performance Tests' ) { when { branch 'master' } steps { echo 'Hello' } } } } } } stage ( 'Deploy' ) { agent { kubernetes { label 'jxmavenlib-deployment' containerTemplate { name 'pl_deployment' image 'cloudbees/docker-java-with-docker-client' ttyEnabled true command 'cat' } } } when { branch 'master' beforeAgent true } steps { echo \"hello\" } } } }","title":"Full Example"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#matrix","text":"In late 2019, Jenkins released the Matrix Build feature. It extends the Parallel and Sequential features and allows you to create a Matrix of options for parallel execution. If you want to get a more in-depth look at this feature, I recommend reading Liam Newman's 11 excellent introductory blog post. When you've read this and Liam's posts, I recommend you go through the 12 test cases of the Matrix build feature on GitHub. They provide a wealth of information and worked-out examples.","title":"Matrix"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#what-is-it","text":"The Matrix build feature extends the Parallel and Sequential features mentioned earlier in this article. What it allows you to do is to specify multiple sets of values ( axis ), and then execute each entry of the cartesian product 13 . Matrix acts as a special stage type, and can include an agent{} and stages{} definition. matrix { axes { } excludes {} agent {} stages {} }","title":"What Is It"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#axis","text":"The mechanism for specifying the sets of values is called axis . You start by specifying the matrix build within a stage. pipeline { agent none stages { stage ( 'matrix build' ) { matrix { } } } } Within matrix you specify the axis with one or more axis . matrix { axes { axis { name 'JDK_VERSION' values '8' , '11' , '13' } axis { name 'JDK_TYPE' values 'ibmjava' , 'amazoncorretto' , 'jdk' } } } In this case, we will get these nine combinations: jdk + 8, jdk + 11, jdk + 13 ibmjava + 8, ibmjava + 11, ibmjava + 13 amazoncorretto + 8, amazoncorretto + 11, amazoncorretto + 13","title":"Axis"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#exclusions","text":"Unfortunately, several of these are not valid combinations. The ibmjava only has a JDK 8 version, and the default (open)JDK is the only one with a JDK 13 version (at this time). You can filter these out with a when directive, but that would pollute the pipeline view. Wouldn't it be better to ensure the invalid combinations aren't even considered? You can tell the matrix build feature just that, by setting exclusions . You start with an exclusions {} followed by one or more exclusion{} blocks. Let's exclude JDK 11 from IBM's java. excludes { exclude { axis { name 'JDK_VERSION' values '11' } axis { name 'JDK_TYPE' values \"ibmjava\" } } } Sometimes, you want to limit a combination to a single axis . Meaning, exclude every combination of x , except for y . We can do just that, by specifying notValues in the exclusion. Let's ensure only jdk will be paired with 13 . exclude { axis { name 'JDK_VERSION' values '13' } axis { name 'JDK_TYPE' notValues 'jdk' // double negative, make sure we only do 13 with jdk } }","title":"Exclusions"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#together-with-podtemplates","text":"As with previous examples of the Parallel and Sequential features, Matrix adds an extra dimension to using PodTemplates. Remember the structure of a Matrix ? matrix { axes { } excludes {} agent {} stages {} } It means we can structure of the Sequential stages, by adding a stages {} block and a single agent{} block for those stages. Because the PodTemplate properties are variables, the values we put in there can be dynamic. This means we can leverage the axis values as input for which container images to use for our PodTemplate. matrix { axes {} agent { kubernetes { label \"maven-${JDK_TYPE}-${JDK_VERSION}-test\" containerTemplate { name 'maven' image \"maven:3-${JDK_TYPE}-${JDK_VERSION}\" ttyEnabled true command 'cat' } } } stages {} }","title":"Together With PodTemplates"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#visual-representation","text":"In the visual representation, we can the benefits of the Sequential stages with Matrix. Each axis combination will have its own set of stages, which can also have their own when{} directives.","title":"Visual Representation"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#complete-example","text":"pipeline { agent none stages { stage ( 'Prepare' ) { agent { kubernetes { label \"maven\" containerTemplate { name 'maven' image \"maven\" ttyEnabled true command 'cat' } } } steps { println 'hello world!' container ( 'maven' ) { sh 'env' } } } stage ( 'Test' ) { matrix { axes { axis { name 'JDK_VERSION' values '8' , '11' , '13' } axis { name 'JDK_TYPE' values 'ibmjava' , 'amazoncorretto' , 'jdk' } } excludes { exclude { axis { name 'JDK_VERSION' values '13' } axis { name 'JDK_TYPE' notValues 'jdk' // double negative, make sure we only do 13 with jdk } } exclude { axis { name 'JDK_VERSION' values '11' } axis { name 'JDK_TYPE' values \"ibmjava\" } } } agent { kubernetes { label \"maven-${JDK_TYPE}-${JDK_VERSION}-test\" containerTemplate { name 'maven' image \"maven:3-${JDK_TYPE}-${JDK_VERSION}\" ttyEnabled true command 'cat' } } } stages { stage ( 'Test Image' ) { steps { println \"Using Image: maven:3-${JDK_TYPE}-${JDK_VERSION}\" } } stage ( \"Build\" ) { steps { sh 'uname -a' git 'https://github.com/joostvdg/jx-maven-lib.git' container ( 'maven' ) { sh 'mvn clean verify --show-version --strict-checksums -e' } } } stage ( \"IT Test\" ) { when { branch 'master' } steps { echo \"WE SHOULD NEVER GET HERE\" } } } } } stage ( 'Publish' ) { agent { kubernetes { label \"maven\" containerTemplate { name 'maven' image \"maven\" ttyEnabled true command 'cat' } } } steps { println 'hello world!' container ( 'maven' ) { sh 'env' } } } } }","title":"Complete Example"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#references","text":"Jenkins Pipeline - Parallel Stages \u21a9 Jenkins Pipeline - Introducing Sequential Stages \u21a9 XKCD - Code's Compiling \u21a9 Jenkins Pipeline Syntax \u21a9 Jenkins Declarative Pipeline - When Directive \u21a9 Jenkins Pipeline - Step Stash \u21a9 Jenkins Pipeline - Step Unstash \u21a9 Kubernetes NFS Storage Provisioner \u21a9 \u21a9 Jenkins External Workspace Manager Plugin \u21a9 Jenkins Kubernetes Plugin - template values explained \u21a9 Jenkins Blog Introducing Matrix Build Feature \u21a9 GitHub Repository containing test examples of the Matrix Build Feature \u21a9 Wikipedia - Cartesian Product \u21a9","title":"References"},{"location":"jenkins-pipeline/job-types/","text":"Jenkins Pipeline Job Types \u00b6 Jenkins Pipelines are written as a Jenkinsfile and can have either a Declarative 1 or Scripted 2 format. Whenever possible for as much as possible, always write your pipelines in Declarative format. How they are configured and run is a classic case of it depends... . This page goes into the job types available and when to use which and why. Caution This piece is full of personal opinions. While based on years of experience with writing Jenkins Pipelines and helping customers and clients with reviewing theirs, they're still just an opinion. Job Types \u00b6 We classify the job types as follows: Something that isn't a pipeline job and you should never ever use (I'm looking at you Freestyle ) Pipeline Multibranch Pipeline Organization Job (named after the most commonly used GitHub Organization Job) Pipeline \u00b6 A Pipeline job 9 is the most fundamental building block available. You can create a New Item in Jenkins and choose the type Pipeline . Here you can choose two different configurations, either you write the Pipeline inline or from SCM . Inline means that you write the Pipeline directly in the Job configuration itself. Where from SCM let's check out a Jenkinsfile from an SCM (such as GitHub). Aside from testing Snippets, you should never use this Job type. It is very limited in what it offers beyond running a Pipeline. Tip I use this Job type for testing sections of a more extensive Pipeline or new features. Once it works, I will commit it to an SCM source and use one of the other Job types. Multibranch Pipeline \u00b6 Now, this is starting to look like something we can use! The Multibranch Pipeline 3 allows you to map a Pipeline to an SCM Repository. Where a Pipeline Job maps to a specific Branch (or Branch scheme) of a Repository, a Multibranch Pipeline Job targets the entire repository. This means that it scans the repository for its Branches. For each branch that meets the criteria you set, it creates a Pipeline Job targeting this specific branch. By default, the only criteria configured is the existence of a file called Jenkinsfile . However, you can add other criteria such as a Branch Naming Scheme - either based on Regex or Wildcards. Benefits Beyond Pipeline Job \u00b6 Aside from not having to manage Job configurations for branches, as Multibranch handles both creation and deletion, this job type has additional benefits. It automatically populates environment variables such as BRANCH_NAME and many other related to the Git Commit. This allows you to take full leverage of the When 10 Directive. pipeline { stages { stage ( 'Only Run If Master Branch' ) { when { branch 'master' } steps { echo 'This only runs iff we are in the master branch' } } } } To leverage the Multibranch Pipeline well, I recommend going through this excellent end-to-end tutorial 4 . Branch Sources \u00b6 The Multibranch Pipeline Job lets you configure to wich SCM Repository it should map to. This can be merely pointing to a Git or SVN (please don't) repository, but more importantly, you can map it to specific Branch Sources . What are Branch Sources ? These are specific Git providers that 1) give Jenkins more information, and 2) have their own unique handler plugins providing better integration with the Git provider. In most cases, this means you can also build Pull Requests and deal with Tags in a way that leverages Jenkins Pipeline Directives such as the already mentioned When 10 directive. As of this writing - August 2019 - there are four such Branch Source plugins available; GitHub 5 (most mature), GitLab 8 (latest addition), Bitbucket 6 , and Gitea 7 . Organization Job \u00b6 I name these types of jobs after the first one that worked very well: GitHub Organization Job. This plugin is replaced by GitHub Branch Source , now the common name for the plugins providing this kind of integration. The goal of Organization Pipeline Job is to scan an entire namespace of Git repositories for branches containing Jenkinsfile . In the case of GitHub, this is called an Organization , in the case of Bitbucket cloud, this is a Team and so forth. Creating several benefits, the essential being, you create a single job within Jenkins and manage the Pipeline for every application in that Organization/Team/Whatever. Moreover, don't worry, there are ample settings to configure if you want to limit the job to specific branches or repositories. Benefits \u00b6 reduced maintenance : a single job maintained in Jenkins takes care of all applications in an Organization/Team/... including creation and deletion of the repositories and branches! reduced complexity : due to the configuration working as a standardization, there are no special snowflakes, there's just one way a job gets configured, and that includes the naming visible coupling : a Pipeline is coupled to a repository, the job clearly shows this connection with links and icons manages status reporting : although this does depend on the specific integration, in general, every Pipeline automatically reports the build status of each commit to your SCM without requiring configuration Models \u00b6 What people often fail to realize, is that the mentioned Pipeline Job Types are not on their own. They are part of a single model, where they build on top of each other. The Pipeline job being the smallest building block, then Multibranch , and last but not least, Organization job extends Multibranch . I've created some models to try and visualize this. Abstract \u00b6 At the very abstract level, we see this babushka doll type of layering. Basic \u00b6 Starting from the outer layer, we have a GitHub Organization job. In essence, it is a folder that contains Multbranch Pipeline jobs. A Multibranch Pipeline job is also a folder, and it contains Pipeline jobs. Mapping \u00b6 This layering is created by mapping the different types of jobs to corresponding resources in the SCM you are talking to. In the example of a GitHub Organization job, we're talking to a GitHub Organization. Jenkins scans each repository in that Organization, and per repository, scan each branch. For each branch that meets the criteria - by default the presence of a Jenkinsfile - it creates the Multibranch Pipeline job for that repository. The Multibranch Pipeline job creates a Pipeline job for each branch that met the criteria. So as summary: GitHub Organization Job -> GitHub Organization / Bitbucket Team / ... Multibranch Pipeline Job -> Repository Pipeline Job -> Branch References \u00b6 Jenkins Declarative Pipeline Fundamentals \u21a9 Jenkins Scripted Pipeline Fundamentals \u21a9 Jenkins Pipeline Multibranch Plugin \u21a9 Tutorial On Creating a Multibranch Pipeline \u21a9 GitHub Branch Source Plugin \u21a9 Bitbucket Branch Source Plugin \u21a9 Integrate Jenkins And Gitea \u21a9 GitLab Branch Source Plugin \u21a9 Create Pipeline Job Via UI \u21a9 Pipeline Syntax - When Directive \u21a9 \u21a9","title":"Job Types"},{"location":"jenkins-pipeline/job-types/#jenkins-pipeline-job-types","text":"Jenkins Pipelines are written as a Jenkinsfile and can have either a Declarative 1 or Scripted 2 format. Whenever possible for as much as possible, always write your pipelines in Declarative format. How they are configured and run is a classic case of it depends... . This page goes into the job types available and when to use which and why. Caution This piece is full of personal opinions. While based on years of experience with writing Jenkins Pipelines and helping customers and clients with reviewing theirs, they're still just an opinion.","title":"Jenkins Pipeline Job Types"},{"location":"jenkins-pipeline/job-types/#job-types","text":"We classify the job types as follows: Something that isn't a pipeline job and you should never ever use (I'm looking at you Freestyle ) Pipeline Multibranch Pipeline Organization Job (named after the most commonly used GitHub Organization Job)","title":"Job Types"},{"location":"jenkins-pipeline/job-types/#pipeline","text":"A Pipeline job 9 is the most fundamental building block available. You can create a New Item in Jenkins and choose the type Pipeline . Here you can choose two different configurations, either you write the Pipeline inline or from SCM . Inline means that you write the Pipeline directly in the Job configuration itself. Where from SCM let's check out a Jenkinsfile from an SCM (such as GitHub). Aside from testing Snippets, you should never use this Job type. It is very limited in what it offers beyond running a Pipeline. Tip I use this Job type for testing sections of a more extensive Pipeline or new features. Once it works, I will commit it to an SCM source and use one of the other Job types.","title":"Pipeline"},{"location":"jenkins-pipeline/job-types/#multibranch-pipeline","text":"Now, this is starting to look like something we can use! The Multibranch Pipeline 3 allows you to map a Pipeline to an SCM Repository. Where a Pipeline Job maps to a specific Branch (or Branch scheme) of a Repository, a Multibranch Pipeline Job targets the entire repository. This means that it scans the repository for its Branches. For each branch that meets the criteria you set, it creates a Pipeline Job targeting this specific branch. By default, the only criteria configured is the existence of a file called Jenkinsfile . However, you can add other criteria such as a Branch Naming Scheme - either based on Regex or Wildcards.","title":"Multibranch Pipeline"},{"location":"jenkins-pipeline/job-types/#benefits-beyond-pipeline-job","text":"Aside from not having to manage Job configurations for branches, as Multibranch handles both creation and deletion, this job type has additional benefits. It automatically populates environment variables such as BRANCH_NAME and many other related to the Git Commit. This allows you to take full leverage of the When 10 Directive. pipeline { stages { stage ( 'Only Run If Master Branch' ) { when { branch 'master' } steps { echo 'This only runs iff we are in the master branch' } } } } To leverage the Multibranch Pipeline well, I recommend going through this excellent end-to-end tutorial 4 .","title":"Benefits Beyond Pipeline Job"},{"location":"jenkins-pipeline/job-types/#branch-sources","text":"The Multibranch Pipeline Job lets you configure to wich SCM Repository it should map to. This can be merely pointing to a Git or SVN (please don't) repository, but more importantly, you can map it to specific Branch Sources . What are Branch Sources ? These are specific Git providers that 1) give Jenkins more information, and 2) have their own unique handler plugins providing better integration with the Git provider. In most cases, this means you can also build Pull Requests and deal with Tags in a way that leverages Jenkins Pipeline Directives such as the already mentioned When 10 directive. As of this writing - August 2019 - there are four such Branch Source plugins available; GitHub 5 (most mature), GitLab 8 (latest addition), Bitbucket 6 , and Gitea 7 .","title":"Branch Sources"},{"location":"jenkins-pipeline/job-types/#organization-job","text":"I name these types of jobs after the first one that worked very well: GitHub Organization Job. This plugin is replaced by GitHub Branch Source , now the common name for the plugins providing this kind of integration. The goal of Organization Pipeline Job is to scan an entire namespace of Git repositories for branches containing Jenkinsfile . In the case of GitHub, this is called an Organization , in the case of Bitbucket cloud, this is a Team and so forth. Creating several benefits, the essential being, you create a single job within Jenkins and manage the Pipeline for every application in that Organization/Team/Whatever. Moreover, don't worry, there are ample settings to configure if you want to limit the job to specific branches or repositories.","title":"Organization Job"},{"location":"jenkins-pipeline/job-types/#benefits","text":"reduced maintenance : a single job maintained in Jenkins takes care of all applications in an Organization/Team/... including creation and deletion of the repositories and branches! reduced complexity : due to the configuration working as a standardization, there are no special snowflakes, there's just one way a job gets configured, and that includes the naming visible coupling : a Pipeline is coupled to a repository, the job clearly shows this connection with links and icons manages status reporting : although this does depend on the specific integration, in general, every Pipeline automatically reports the build status of each commit to your SCM without requiring configuration","title":"Benefits"},{"location":"jenkins-pipeline/job-types/#models","text":"What people often fail to realize, is that the mentioned Pipeline Job Types are not on their own. They are part of a single model, where they build on top of each other. The Pipeline job being the smallest building block, then Multibranch , and last but not least, Organization job extends Multibranch . I've created some models to try and visualize this.","title":"Models"},{"location":"jenkins-pipeline/job-types/#abstract","text":"At the very abstract level, we see this babushka doll type of layering.","title":"Abstract"},{"location":"jenkins-pipeline/job-types/#basic","text":"Starting from the outer layer, we have a GitHub Organization job. In essence, it is a folder that contains Multbranch Pipeline jobs. A Multibranch Pipeline job is also a folder, and it contains Pipeline jobs.","title":"Basic"},{"location":"jenkins-pipeline/job-types/#mapping","text":"This layering is created by mapping the different types of jobs to corresponding resources in the SCM you are talking to. In the example of a GitHub Organization job, we're talking to a GitHub Organization. Jenkins scans each repository in that Organization, and per repository, scan each branch. For each branch that meets the criteria - by default the presence of a Jenkinsfile - it creates the Multibranch Pipeline job for that repository. The Multibranch Pipeline job creates a Pipeline job for each branch that met the criteria. So as summary: GitHub Organization Job -> GitHub Organization / Bitbucket Team / ... Multibranch Pipeline Job -> Repository Pipeline Job -> Branch","title":"Mapping"},{"location":"jenkins-pipeline/job-types/#references","text":"Jenkins Declarative Pipeline Fundamentals \u21a9 Jenkins Scripted Pipeline Fundamentals \u21a9 Jenkins Pipeline Multibranch Plugin \u21a9 Tutorial On Creating a Multibranch Pipeline \u21a9 GitHub Branch Source Plugin \u21a9 Bitbucket Branch Source Plugin \u21a9 Integrate Jenkins And Gitea \u21a9 GitLab Branch Source Plugin \u21a9 Create Pipeline Job Via UI \u21a9 Pipeline Syntax - When Directive \u21a9 \u21a9","title":"References"},{"location":"jenkins-pipeline/kaniko-pipelines/","text":"Kaniko Pipelines \u00b6 Kaniko 1 is one of the recommended tools for building Docker images within Kubernetes, especially when you build them as part of a Jenkins Pipeline. I've written about why you should use Kaniko (or similar) tools, the rest assumes you want to use Kaniko within your pipeline. Quote Kaniko is a tool to build container images from a Dockerfile, inside a container or Kubernetes cluster. kaniko doesn't depend on a Docker daemon and executes each command within a Dockerfile completely in userspace. This enables building container images in environments that can't easily or securely run a Docker daemon, such as a standard Kubernetes cluster. For more examples for leveraging Kaniko when using Jenkins in Kubernetes, you can look at the documentation from CloudBees Core 2 . Pipeline Example \u00b6 Note The Kaniko logger uses ANSI Colors, which can be represented via the Jenkins ANSI Color Plugin . If you have the plugin installed, you can do something like the snipped below to render the colors. container ( name: 'kaniko' , shell: '/busybox/sh' ) { ansiColor ( 'xterm' ) { sh '''#!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=${REGISTRY}/${REPOSITORY}/${IMAGE} ''' } } pipeline { agent { kubernetes { label 'kaniko' yaml \"\"\" kind: Pod metadata: name: kaniko spec: containers: - name: golang image: golang:1.12 command: - cat tty: true - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /kaniko/.docker volumes: - name: jenkins-docker-cfg projected: sources: - secret: name: registry-credentials items: - key: .dockerconfigjson path: config.json \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/cat.git' } } stage ( 'Build' ) { steps { container ( 'golang' ) { sh './build-go-bin.sh' } } } stage ( 'Make Image' ) { environment { PATH = \"/busybox: $PATH \" REGISTRY = 'index.docker.io' // Configure your own registry REPOSITORY = 'caladreas' IMAGE = 'cat' } steps { container ( name: 'kaniko' , shell: '/busybox/sh' ) { sh '''#!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=${REGISTRY}/${REPOSITORY}/${IMAGE} ''' } } } } } Configuration \u00b6 Kaniko relies on a docker secret for directly communicating to a Docker Registry. This can be supplied in various ways, but the most common is to create a Kubernetes Secret of type docker-registry . kubectl create secret docker-registry registry-credentials \\ --docker-username = <username> \\ --docker-password = <password> \\ --docker-email = <email-address> We can then mount it in the pod via Volumes (PodSpec level) and volumeMounts (Container level). volumes : - name : jenkins-docker-cfg projected : sources : - secret : name : docker-credentials items : - key : .dockerconfigjson path : config.json Azure & ACR \u00b6 Of course, there always have to be difference between the Public Cloud Providers (AWS, Azure, Alibaba, GCP). In the case of Kaniko, its Azure that does things differently. Assuming you want to leverage Azure Container Registry (ACR), you're in Azure after all, you will have to do a few things differently. Create ACR \u00b6 You can use the Azure CLI 5 or an Configuration-As-Code Tool such as Terraform 6 . Azure CLI \u00b6 First, create a resource group. az group create --name myResourceGroup --location eastus And then create the ACR. az acr create --resource-group myResourceGroup --name myContainerRegistry007 --sku Basic Terraform \u00b6 We leverage the azurerm backend of terraform 9 10 . resource \"azurerm_resource_group\" \"acr\" { name = \"${var.resource_group_name}-acr\" location = \"${var.location}\" } resource \"azurerm_container_registry\" \"acr\" { name = \"${var.container_registry_name}\" resource_group_name = \"${azurerm_resource_group.acr.name}\" location = \"${azurerm_resource_group.k8s.location}\" sku = \"Premium\" admin_enabled = false } Configure Access to ACR \u00b6 Now that we have an ACR, we need to be able to pull and images from and to the registry. This requires access credentials, which we can create in several ways, we'll explore via ServicePrinciple. Via ServicePrinciple Credentials \u00b6 The commands below are taken from the Azure Container Registry documentation about authentication 7 . First, lets setup some values that are not derived from something. EMAIL = me@example.com SERVICE_PRINCIPAL_NAME = acr-service-principal ACR_NAME = myacrinstance Second, we fetch the basic information about the registry we have. We need this information for the other commands. ACR_LOGIN_SERVER = $( az acr show --name $ACR_NAME --query loginServer --output tsv ) ACR_REGISTRY_ID = $( az acr show --name $ACR_NAME --query id --output tsv ) Now we can create a ServicePrinciple with just the rights we need 8 . In the case of Kaniko, we need Push and Pull rights, which are both captured in the role acrpush . SP_PASSWD = $( az ad sp create-for-rbac --name http:// $SERVICE_PRINCIPAL_NAME --role acrpush --scopes $ACR_REGISTRY_ID --query password --output tsv ) CLIENT_ID = $( az ad sp show --id http:// $SERVICE_PRINCIPAL_NAME --query appId --output tsv ) kubectl create secret docker-registry registry-credentials --docker-server ${ ACR_LOGIN_SERVER } --docker-username ${ CLIENT_ID } --docker-password ${ SP_PASSWD } --docker-email ${ EMAIL } References \u00b6 Kaniko GitHub \u21a9 CloudBees Guide On Using Kaniko With CloudBees Core \u21a9 Sail CI On Kaniko With Azure Container Registry \u21a9 Create Azure Container Registry With Azure CLI \u21a9 Azure CLI \u21a9 Terraform \u21a9 Azure Container Registry Authentication Documentation \u21a9 Azure Container Registry Roles and Permissions \u21a9 Terraform AzureRM Backend \u21a9 Create AKS Cluster Via Terraform \u21a9 Azure Container Registry Credentials Management \u21a9","title":"Build Docker Image Kaniko"},{"location":"jenkins-pipeline/kaniko-pipelines/#kaniko-pipelines","text":"Kaniko 1 is one of the recommended tools for building Docker images within Kubernetes, especially when you build them as part of a Jenkins Pipeline. I've written about why you should use Kaniko (or similar) tools, the rest assumes you want to use Kaniko within your pipeline. Quote Kaniko is a tool to build container images from a Dockerfile, inside a container or Kubernetes cluster. kaniko doesn't depend on a Docker daemon and executes each command within a Dockerfile completely in userspace. This enables building container images in environments that can't easily or securely run a Docker daemon, such as a standard Kubernetes cluster. For more examples for leveraging Kaniko when using Jenkins in Kubernetes, you can look at the documentation from CloudBees Core 2 .","title":"Kaniko Pipelines"},{"location":"jenkins-pipeline/kaniko-pipelines/#pipeline-example","text":"Note The Kaniko logger uses ANSI Colors, which can be represented via the Jenkins ANSI Color Plugin . If you have the plugin installed, you can do something like the snipped below to render the colors. container ( name: 'kaniko' , shell: '/busybox/sh' ) { ansiColor ( 'xterm' ) { sh '''#!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=${REGISTRY}/${REPOSITORY}/${IMAGE} ''' } } pipeline { agent { kubernetes { label 'kaniko' yaml \"\"\" kind: Pod metadata: name: kaniko spec: containers: - name: golang image: golang:1.12 command: - cat tty: true - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /kaniko/.docker volumes: - name: jenkins-docker-cfg projected: sources: - secret: name: registry-credentials items: - key: .dockerconfigjson path: config.json \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/cat.git' } } stage ( 'Build' ) { steps { container ( 'golang' ) { sh './build-go-bin.sh' } } } stage ( 'Make Image' ) { environment { PATH = \"/busybox: $PATH \" REGISTRY = 'index.docker.io' // Configure your own registry REPOSITORY = 'caladreas' IMAGE = 'cat' } steps { container ( name: 'kaniko' , shell: '/busybox/sh' ) { sh '''#!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=${REGISTRY}/${REPOSITORY}/${IMAGE} ''' } } } } }","title":"Pipeline Example"},{"location":"jenkins-pipeline/kaniko-pipelines/#configuration","text":"Kaniko relies on a docker secret for directly communicating to a Docker Registry. This can be supplied in various ways, but the most common is to create a Kubernetes Secret of type docker-registry . kubectl create secret docker-registry registry-credentials \\ --docker-username = <username> \\ --docker-password = <password> \\ --docker-email = <email-address> We can then mount it in the pod via Volumes (PodSpec level) and volumeMounts (Container level). volumes : - name : jenkins-docker-cfg projected : sources : - secret : name : docker-credentials items : - key : .dockerconfigjson path : config.json","title":"Configuration"},{"location":"jenkins-pipeline/kaniko-pipelines/#azure-acr","text":"Of course, there always have to be difference between the Public Cloud Providers (AWS, Azure, Alibaba, GCP). In the case of Kaniko, its Azure that does things differently. Assuming you want to leverage Azure Container Registry (ACR), you're in Azure after all, you will have to do a few things differently.","title":"Azure &amp; ACR"},{"location":"jenkins-pipeline/kaniko-pipelines/#create-acr","text":"You can use the Azure CLI 5 or an Configuration-As-Code Tool such as Terraform 6 .","title":"Create ACR"},{"location":"jenkins-pipeline/kaniko-pipelines/#azure-cli","text":"First, create a resource group. az group create --name myResourceGroup --location eastus And then create the ACR. az acr create --resource-group myResourceGroup --name myContainerRegistry007 --sku Basic","title":"Azure CLI"},{"location":"jenkins-pipeline/kaniko-pipelines/#terraform","text":"We leverage the azurerm backend of terraform 9 10 . resource \"azurerm_resource_group\" \"acr\" { name = \"${var.resource_group_name}-acr\" location = \"${var.location}\" } resource \"azurerm_container_registry\" \"acr\" { name = \"${var.container_registry_name}\" resource_group_name = \"${azurerm_resource_group.acr.name}\" location = \"${azurerm_resource_group.k8s.location}\" sku = \"Premium\" admin_enabled = false }","title":"Terraform"},{"location":"jenkins-pipeline/kaniko-pipelines/#configure-access-to-acr","text":"Now that we have an ACR, we need to be able to pull and images from and to the registry. This requires access credentials, which we can create in several ways, we'll explore via ServicePrinciple.","title":"Configure Access to ACR"},{"location":"jenkins-pipeline/kaniko-pipelines/#via-serviceprinciple-credentials","text":"The commands below are taken from the Azure Container Registry documentation about authentication 7 . First, lets setup some values that are not derived from something. EMAIL = me@example.com SERVICE_PRINCIPAL_NAME = acr-service-principal ACR_NAME = myacrinstance Second, we fetch the basic information about the registry we have. We need this information for the other commands. ACR_LOGIN_SERVER = $( az acr show --name $ACR_NAME --query loginServer --output tsv ) ACR_REGISTRY_ID = $( az acr show --name $ACR_NAME --query id --output tsv ) Now we can create a ServicePrinciple with just the rights we need 8 . In the case of Kaniko, we need Push and Pull rights, which are both captured in the role acrpush . SP_PASSWD = $( az ad sp create-for-rbac --name http:// $SERVICE_PRINCIPAL_NAME --role acrpush --scopes $ACR_REGISTRY_ID --query password --output tsv ) CLIENT_ID = $( az ad sp show --id http:// $SERVICE_PRINCIPAL_NAME --query appId --output tsv ) kubectl create secret docker-registry registry-credentials --docker-server ${ ACR_LOGIN_SERVER } --docker-username ${ CLIENT_ID } --docker-password ${ SP_PASSWD } --docker-email ${ EMAIL }","title":"Via ServicePrinciple Credentials"},{"location":"jenkins-pipeline/kaniko-pipelines/#references","text":"Kaniko GitHub \u21a9 CloudBees Guide On Using Kaniko With CloudBees Core \u21a9 Sail CI On Kaniko With Azure Container Registry \u21a9 Create Azure Container Registry With Azure CLI \u21a9 Azure CLI \u21a9 Terraform \u21a9 Azure Container Registry Authentication Documentation \u21a9 Azure Container Registry Roles and Permissions \u21a9 Terraform AzureRM Backend \u21a9 Create AKS Cluster Via Terraform \u21a9 Azure Container Registry Credentials Management \u21a9","title":"References"},{"location":"jenkins-pipeline/podtemplate-dind/","text":"Docker-in-Docker With PodTemplates \u00b6 First of all, doing Docker-In-Docker is a controversial practice to begin with 1 , creating just as many problems as it solves. Second, if you're in Kubernetes, one should not use Docker directly . Ok, all caveats aside, there can be legitimate reasons for wanting to run Docker containers directly even in Kubernetes. Scenario: Pipeline Framework Based On Docker \u00b6 I created the solution in this article for a client. This client has created a internal framework around Jenkins Pipeline 3 . This pipeline processes the requirements off the pipeline run and spins up the appropriate containers in parallel. Running parallel container can also be done with PodTemplates via the Kubernetes Plugin , which was deemed the way forward. However, as one can expect, you do not rewrite a framework used by dozens of applications unless you're reasonably sure you can get everything to run as it should. In order to bridge this period of running the pipelines as is, while evaluating Jenkins on Kubernetes, we got to work on achieving Docker-in-Docker with Jenkins Kubernetes Plugin 4 . Goal \u00b6 The goal of the exercise, is to prove we can do a single git clone, and spin up multiple containers re-using the original namespace. Docker Socket \u00b6 There's many ways to spin up Docker containers, and I would say the most common one is know as Docker-in-Docker or dind . I find this misleading, because most of the time the second container doesn't run inside the original container, but parallel to it. This is often achieved by mounting the docker socket as a volume - /var/run/docker.sock . I call this: Docker-On-Docker . Because I want to stay close to the term Docker-in-Docker, but also signify it is different. What Docker-In-Docker should be, is that we host a docker daemon in a docker container, which then hosts the new containers in itself. Before we go into the differences, let's discuss why it matters. Remember, the goal is to be able to re-use the git clone in the original container. The default container with Jenkins Kubernetes Plugin is the jnpl , which acts as an ephemeral agent. It mounts a emptyDir{} as temporary volume as its workspace, which means this folder only exists within the Pod (practically speaking). Docker-On-Docker \u00b6 Classic Docker-On-Docker, we have a VM which has a Docker Daemon. We have a Docker client container which mounts the docker socket. The docker client now pretends to talk to a local Docker Daemon, but this redirects to the VM's daemon instead. When we do a docker run , the daemon will spawn a new container next to us. While this is nice, and stays clear from virtulization inception, this new container - let's say, a maven container - cannot access our workspace. Any volume flag we give to our container with -v {origin}:{target} maps a Host directory to our new container, but not the workspace volume (purple) in our Pod. This is because the new container cannot access any volume inside the Pod. For this, we need true Docker-In-Docker. Jenkinsfile Example \u00b6 podTemplate ( yaml: \"\"\" apiVersion: v1 kind: Pod spec: containers: - name: docker image: docker:1.11 command: ['cat'] tty: true volumeMounts: - name: dockersock mountPath: /var/run/docker.sock volumes: - name: dockersock hostPath: path: /var/run/docker.sock \"\"\" ) { def image = \"jenkins/jnlp-slave\" node ( POD_LABEL ) { stage ( 'Build Docker image' ) { git 'https://github.com/jenkinsci/docker-jnlp-slave.git' container ( 'docker' ) { sh \"docker build -t ${image} .\" } } } } Docker-In-Docker \u00b6 True Docker-In-Docker means spinning up another container inside an existing container - as illustrated below. This has a big benefit. The container now has access to all the volumes inside the Pod. We can now do a git clone and use docker build on this workspace. In addition, we can also spin up a new container and give it the workspace as a volume and have it build with it. Pod Configuration \u00b6 The examples below contain some specific configuration elements required to make the magic happen. So let's explore each configuration item to see what it does and why we set it. Client Container \u00b6 The Docker Client has to know where the Docker Daemon is. We do this by setting the environment variable DOCKER_HOST 2 to tcp://localhost:2375 . If we put the Daemon container within the same Pod, we can use localhost and then the default port of the daemon, being 2375 . env : - name : DOCKER_HOST value : tcp://localhost:2375 The client container will directly terminate unless we give it something to do. If we put it to sleep for a significant amount of time, it should be there to execute our every command! command : [ 'sleep' , '99d' ] Daemon Container \u00b6 As of Docker 18.09 the Docker Daemon container can use TLS, as of 19.03 it is configured by default. 2 So either you work around this or get the certificates sorted. The easiest way around it, is to set the environment variable DOCKER_TLS_CERTDIR to \"\" , which will disable TLS. Caution Disabling TLS is at your OWN risk. Read the docs carefully for what this means 2 . env : - name : DOCKER_TLS_CERTDIR value : \"\" In order for the Docker Daemon to create containers, it needs the privileged flag set to true. This should be another warning to you, to be careful of what you do with it! securityContext : privileged : true Last but not least, we would not want to store all the Docker Daemon data in the pods. We might as well leverage local storage or a some specific volume. The volume and volumeMounts configuration of the Daemon container ensures we can leverage the Docker build and image caches. volumeMounts : - name : cache mountPath : /var/lib/docker volumes : - name : cache hostPath : path : /tmp type : Directory Docker Build Now that we can directly use Docker to build, we can also leverage buildkit 7 . Because we have the workspace available to us, we can directly start our docker build. pipeline { options { disableConcurrentBuilds () } agent { kubernetes { label 'docker-in-docker-maven' yaml \"\"\" apiVersion: v1 kind: Pod spec: containers: - name: docker-client image: docker:19.03.1 command: ['sleep', '99d'] env: - name: DOCKER_HOST value: tcp://localhost:2375 - name: docker-daemon image: docker:19.03.1-dind env: - name: DOCKER_TLS_CERTDIR value: \"\" securityContext: privileged: true volumeMounts: - name: cache mountPath: /var/lib/docker volumes: - name: cache hostPath: path: /tmp type: Directory \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/jenkinsci/docker-jnlp-slave.git' } } stage ( 'Docker Build' ) { steps { container ( 'docker-client' ) { sh 'docker version && DOCKER_BUILDKIT=1 docker build --progress plain -t testing .' } } } } } Docker Run With Workspace In this example we're going to spin up another container to run our build with maven. This means it needs our workspace - see below - but it will also download a lot of Maven depencencies. We want to make sure we can leverage a local Maven Repository in order to speed up builds. We can do so by mounting a volume - or a hostPath to our Docker Client container. volumeMounts : - name : cache mountPath : /tmp/repository We can then mount this into our new container via the Docker volume flag ( -v ). -v /tmp/repository:/root/.m2/repository Here we mount the workspace from our Jenkins Build. Notice the double quotes - in the pipeline below - this means we interpolate our Jenkins Environment variables. -v ${WORKSPACE}:/usr/src/mymaven The -w flag means work directory , it makes sure our container works in the directory container our workspace. -w /usr/src/mymaven pipeline { options { disableConcurrentBuilds () } agent { kubernetes { label 'docker-in-docker-maven' yaml \"\"\" apiVersion: v1 kind: Pod spec: containers: - name: docker-client image: docker:19.03.1 command: ['sleep', '99d'] env: - name: DOCKER_HOST value: tcp://localhost:2375 volumeMounts: - name: cache mountPath: /tmp/repository - name: docker-daemon image: docker:19.03.1-dind env: - name: DOCKER_TLS_CERTDIR value: \"\" securityContext: privileged: true volumeMounts: - name: cache mountPath: /var/lib/docker volumes: - name: cache hostPath: path: /tmp type: Directory \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/jx-maven-lib.git' } } stage ( 'Build' ) { steps { container ( 'docker-client' ) { sh \"docker run -v ${WORKSPACE}:/usr/src/mymaven -v /tmp/repository:/root/.m2/repository -w /usr/src/mymaven maven:3-jdk-11-slim mvn clean verify\" } } } } } References \u00b6 Using Docker-in-Docker - Jerome Petazzo \u21a9 Docker images on Dockerhub \u21a9 \u21a9 \u21a9 Jenkins Pipeline \u21a9 Jenkins Kubernetes Plugin \u21a9 Jenkins Kubernetes Plugin - Docker In Docker Example \u21a9 Jenkins Kubernetes Plugin - Docker On Docker Example \u21a9 Docker Build Enchancements - Buildkit \u21a9","title":"PodTemplate With Docker-In-Docker"},{"location":"jenkins-pipeline/podtemplate-dind/#docker-in-docker-with-podtemplates","text":"First of all, doing Docker-In-Docker is a controversial practice to begin with 1 , creating just as many problems as it solves. Second, if you're in Kubernetes, one should not use Docker directly . Ok, all caveats aside, there can be legitimate reasons for wanting to run Docker containers directly even in Kubernetes.","title":"Docker-in-Docker With PodTemplates"},{"location":"jenkins-pipeline/podtemplate-dind/#scenario-pipeline-framework-based-on-docker","text":"I created the solution in this article for a client. This client has created a internal framework around Jenkins Pipeline 3 . This pipeline processes the requirements off the pipeline run and spins up the appropriate containers in parallel. Running parallel container can also be done with PodTemplates via the Kubernetes Plugin , which was deemed the way forward. However, as one can expect, you do not rewrite a framework used by dozens of applications unless you're reasonably sure you can get everything to run as it should. In order to bridge this period of running the pipelines as is, while evaluating Jenkins on Kubernetes, we got to work on achieving Docker-in-Docker with Jenkins Kubernetes Plugin 4 .","title":"Scenario: Pipeline Framework Based On Docker"},{"location":"jenkins-pipeline/podtemplate-dind/#goal","text":"The goal of the exercise, is to prove we can do a single git clone, and spin up multiple containers re-using the original namespace.","title":"Goal"},{"location":"jenkins-pipeline/podtemplate-dind/#docker-socket","text":"There's many ways to spin up Docker containers, and I would say the most common one is know as Docker-in-Docker or dind . I find this misleading, because most of the time the second container doesn't run inside the original container, but parallel to it. This is often achieved by mounting the docker socket as a volume - /var/run/docker.sock . I call this: Docker-On-Docker . Because I want to stay close to the term Docker-in-Docker, but also signify it is different. What Docker-In-Docker should be, is that we host a docker daemon in a docker container, which then hosts the new containers in itself. Before we go into the differences, let's discuss why it matters. Remember, the goal is to be able to re-use the git clone in the original container. The default container with Jenkins Kubernetes Plugin is the jnpl , which acts as an ephemeral agent. It mounts a emptyDir{} as temporary volume as its workspace, which means this folder only exists within the Pod (practically speaking).","title":"Docker Socket"},{"location":"jenkins-pipeline/podtemplate-dind/#docker-on-docker","text":"Classic Docker-On-Docker, we have a VM which has a Docker Daemon. We have a Docker client container which mounts the docker socket. The docker client now pretends to talk to a local Docker Daemon, but this redirects to the VM's daemon instead. When we do a docker run , the daemon will spawn a new container next to us. While this is nice, and stays clear from virtulization inception, this new container - let's say, a maven container - cannot access our workspace. Any volume flag we give to our container with -v {origin}:{target} maps a Host directory to our new container, but not the workspace volume (purple) in our Pod. This is because the new container cannot access any volume inside the Pod. For this, we need true Docker-In-Docker.","title":"Docker-On-Docker"},{"location":"jenkins-pipeline/podtemplate-dind/#jenkinsfile-example","text":"podTemplate ( yaml: \"\"\" apiVersion: v1 kind: Pod spec: containers: - name: docker image: docker:1.11 command: ['cat'] tty: true volumeMounts: - name: dockersock mountPath: /var/run/docker.sock volumes: - name: dockersock hostPath: path: /var/run/docker.sock \"\"\" ) { def image = \"jenkins/jnlp-slave\" node ( POD_LABEL ) { stage ( 'Build Docker image' ) { git 'https://github.com/jenkinsci/docker-jnlp-slave.git' container ( 'docker' ) { sh \"docker build -t ${image} .\" } } } }","title":"Jenkinsfile Example"},{"location":"jenkins-pipeline/podtemplate-dind/#docker-in-docker","text":"True Docker-In-Docker means spinning up another container inside an existing container - as illustrated below. This has a big benefit. The container now has access to all the volumes inside the Pod. We can now do a git clone and use docker build on this workspace. In addition, we can also spin up a new container and give it the workspace as a volume and have it build with it.","title":"Docker-In-Docker"},{"location":"jenkins-pipeline/podtemplate-dind/#pod-configuration","text":"The examples below contain some specific configuration elements required to make the magic happen. So let's explore each configuration item to see what it does and why we set it.","title":"Pod Configuration"},{"location":"jenkins-pipeline/podtemplate-dind/#client-container","text":"The Docker Client has to know where the Docker Daemon is. We do this by setting the environment variable DOCKER_HOST 2 to tcp://localhost:2375 . If we put the Daemon container within the same Pod, we can use localhost and then the default port of the daemon, being 2375 . env : - name : DOCKER_HOST value : tcp://localhost:2375 The client container will directly terminate unless we give it something to do. If we put it to sleep for a significant amount of time, it should be there to execute our every command! command : [ 'sleep' , '99d' ]","title":"Client Container"},{"location":"jenkins-pipeline/podtemplate-dind/#daemon-container","text":"As of Docker 18.09 the Docker Daemon container can use TLS, as of 19.03 it is configured by default. 2 So either you work around this or get the certificates sorted. The easiest way around it, is to set the environment variable DOCKER_TLS_CERTDIR to \"\" , which will disable TLS. Caution Disabling TLS is at your OWN risk. Read the docs carefully for what this means 2 . env : - name : DOCKER_TLS_CERTDIR value : \"\" In order for the Docker Daemon to create containers, it needs the privileged flag set to true. This should be another warning to you, to be careful of what you do with it! securityContext : privileged : true Last but not least, we would not want to store all the Docker Daemon data in the pods. We might as well leverage local storage or a some specific volume. The volume and volumeMounts configuration of the Daemon container ensures we can leverage the Docker build and image caches. volumeMounts : - name : cache mountPath : /var/lib/docker volumes : - name : cache hostPath : path : /tmp type : Directory Docker Build Now that we can directly use Docker to build, we can also leverage buildkit 7 . Because we have the workspace available to us, we can directly start our docker build. pipeline { options { disableConcurrentBuilds () } agent { kubernetes { label 'docker-in-docker-maven' yaml \"\"\" apiVersion: v1 kind: Pod spec: containers: - name: docker-client image: docker:19.03.1 command: ['sleep', '99d'] env: - name: DOCKER_HOST value: tcp://localhost:2375 - name: docker-daemon image: docker:19.03.1-dind env: - name: DOCKER_TLS_CERTDIR value: \"\" securityContext: privileged: true volumeMounts: - name: cache mountPath: /var/lib/docker volumes: - name: cache hostPath: path: /tmp type: Directory \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/jenkinsci/docker-jnlp-slave.git' } } stage ( 'Docker Build' ) { steps { container ( 'docker-client' ) { sh 'docker version && DOCKER_BUILDKIT=1 docker build --progress plain -t testing .' } } } } } Docker Run With Workspace In this example we're going to spin up another container to run our build with maven. This means it needs our workspace - see below - but it will also download a lot of Maven depencencies. We want to make sure we can leverage a local Maven Repository in order to speed up builds. We can do so by mounting a volume - or a hostPath to our Docker Client container. volumeMounts : - name : cache mountPath : /tmp/repository We can then mount this into our new container via the Docker volume flag ( -v ). -v /tmp/repository:/root/.m2/repository Here we mount the workspace from our Jenkins Build. Notice the double quotes - in the pipeline below - this means we interpolate our Jenkins Environment variables. -v ${WORKSPACE}:/usr/src/mymaven The -w flag means work directory , it makes sure our container works in the directory container our workspace. -w /usr/src/mymaven pipeline { options { disableConcurrentBuilds () } agent { kubernetes { label 'docker-in-docker-maven' yaml \"\"\" apiVersion: v1 kind: Pod spec: containers: - name: docker-client image: docker:19.03.1 command: ['sleep', '99d'] env: - name: DOCKER_HOST value: tcp://localhost:2375 volumeMounts: - name: cache mountPath: /tmp/repository - name: docker-daemon image: docker:19.03.1-dind env: - name: DOCKER_TLS_CERTDIR value: \"\" securityContext: privileged: true volumeMounts: - name: cache mountPath: /var/lib/docker volumes: - name: cache hostPath: path: /tmp type: Directory \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/jx-maven-lib.git' } } stage ( 'Build' ) { steps { container ( 'docker-client' ) { sh \"docker run -v ${WORKSPACE}:/usr/src/mymaven -v /tmp/repository:/root/.m2/repository -w /usr/src/mymaven maven:3-jdk-11-slim mvn clean verify\" } } } } }","title":"Daemon Container"},{"location":"jenkins-pipeline/podtemplate-dind/#references","text":"Using Docker-in-Docker - Jerome Petazzo \u21a9 Docker images on Dockerhub \u21a9 \u21a9 \u21a9 Jenkins Pipeline \u21a9 Jenkins Kubernetes Plugin \u21a9 Jenkins Kubernetes Plugin - Docker In Docker Example \u21a9 Jenkins Kubernetes Plugin - Docker On Docker Example \u21a9 Docker Build Enchancements - Buildkit \u21a9","title":"References"},{"location":"jenkins-pipeline-examples/","text":"Jenkins Pipeline Examples \u00b6 Please mind that these examples all assume the following: you have Jenkins 2.32+ you have a recent set of pipeline plugins Jenkins Pipeline Model Jenkins Blue Ocean Jenkins Pipeline Maven Build timeout plugin Credentials Binding Credentials Pipeline Multi-Branch SonarQube Timestamper Pipeline Supporting APIs Pipeline Shared Groovy Libraries","title":"Index"},{"location":"jenkins-pipeline-examples/#jenkins-pipeline-examples","text":"Please mind that these examples all assume the following: you have Jenkins 2.32+ you have a recent set of pipeline plugins Jenkins Pipeline Model Jenkins Blue Ocean Jenkins Pipeline Maven Build timeout plugin Credentials Binding Credentials Pipeline Multi-Branch SonarQube Timestamper Pipeline Supporting APIs Pipeline Shared Groovy Libraries","title":"Jenkins Pipeline Examples"},{"location":"jenkins-pipeline-examples/docker-alternatives/","text":"Pipelines With Docker Alternatives \u00b6 Building pipelines with Jenkins on Docker has been common for a while. But accessing the docker socket has always been a bit tricky. The easiest solution is directly mounting the docker socket of the host into your build container. However, this is a big security risk and something that is undesirable at best and, pretty dangerous at worst. When you're using an orchestrator such as Kubernetes - which is where Jenkins is currently best put to work - the problem gets worse. Not only do you open up security holes, you also mess up the schedular's insight into available resources, and mess with it's ability to keep your cluster in a correct state. In some sense, using docker directly in a Kubernetes cluster, is likened to running with scissors . Potential Alternatives \u00b6 So, directly running docker containers via a docker engine inside your cluster is out. Let's look at some alternatives. Kubernetes Pod and External Node : the simple way out, use a cloud (such as EC2 Cloud) to provision a classic VM with docker engine as an agent and build there JIB : tool from Google, only works for Java applications and is supported directly from Gradle and Maven - Link Kaniko : tool from Google, works as 1-on-1 replacement for docker image build (except Windows Containers ) - Link IMG : tool from Jess Frazelle to avoid building docker images with a root user involved Link Kubernetes Pod and External Node \u00b6 One of the most used cloud environments is AWS, so I created this solution with AWS's Amazon EC2 Plugin . Warning Unfortunately, you cannot combine the Kubernetes plugin with external nodes (none pod container nodes) in a Declarative pipeline. So you have to use Scripted . This can be done with various different cloud providers such as Digital Ocean, Google, AWS or Azure. This guide will use AWS, as it has the most mature Jenkins Cloud plugin. Prerequisites \u00b6 AWS Account with rights to create AMI's and run EC2 instances Packer Jenkins with Amazon EC2 Plugin installed Steps \u00b6 create AMI with Packer install and configure Amazon EC2 plugin create a test pipeline Create AMI with Packer \u00b6 Packer needs to be able to access EC2 API's and be able to spin up an EC2 instance and create an AMI out of it. AWS setup for Packer \u00b6 You need to configure two things: account details for Packer to use security group where your EC2 instances will be running with this security group needs to open port 22 both Packer and Jenkins will use this for their connection export AWS_DEFAULT_REGION = eu-west-1 export AWS_ACCESS_KEY_ID = XXX export AWS_SECRET_ACCESS_KEY = XXX aws ec2 --profile myAwsProfile create-security-group \\ --description \"For building Docker images\" \\ --group-name docker { \"GroupId\" : \"sg-08079f78cXXXXXXX\" } Export the security group ID. export SG_ID = sg-08079f78cXXXXXXX echo $SG_ID Enable port 22 \u00b6 aws ec2 \\ --profile myAwsProfile \\ authorize-security-group-ingress \\ --group-name docker \\ --protocol tcp \\ --port 22 \\ --cidr 0 .0.0.0/0 Packer AMI definition \u00b6 Here's an example definition for Packer for a Ubuntu 18.04 LTS base image with JDK 8 (required by Jenkins) and Docker. { \"builders\" : [{ \"type\" : \"amazon-ebs\" , \"region\" : \"eu-west-1\" , \"source_ami_filter\" : { \"filters\" : { \"virtualization-type\" : \"hvm\" , \"name\" : \"*ubuntu-bionic-18.04-amd64-server-*\" , \"root-device-type\" : \"ebs\" }, \"owners\" : [ \"679593333241\" ], \"most_recent\" : true }, \"instance_type\" : \"t2.micro\" , \"ssh_username\" : \"ubuntu\" , \"ami_name\" : \"docker\" , \"force_deregister\" : true }], \"provisioners\" : [{ \"type\" : \"shell\" , \"inline\" : [ \"sleep 15\" , \"sudo apt-get clean\" , \"sudo apt-get update\" , \"sudo apt-get install -y apt-transport-https ca-certificates nfs-common\" , \"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\" , \"sudo add-apt-repository \\\"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\\"\" , \"sudo add-apt-repository -y ppa:openjdk-r/ppa\" , \"sudo apt-get update\" , \"sudo apt-get install -y docker-ce\" , \"sudo usermod -aG docker ubuntu\" , \"sudo apt-get install -y openjdk-8-jdk\" , \"java -version\" , \"docker version\" ] }] } Build the new AMI with packer. packer build docker-ami.json export AMI = ami-0212ab37f84e418f4 EC2 Key Pair \u00b6 Create EC2 key pair, this will be used by Jenkins to connect to the instances via ssh. aws ec2 --profile myAwsProfile create-key-pair \\ --key-name jenkinsec2 \\ | jq -r '.KeyMaterial' \\ >jenkins-ec2-proton.pem EC2 Cloud Configuration \u00b6 In a Jenkins' master main configuration, you add a new cloud . In this case, we will use a ec2-cloud so we can instantiate our EC2 VM's with docker. use EC2 credentials for initial connection use key ( .pem ) for VM connection (jenkins <> agent) configure the following: AMI: ami-0212ab37f84e418f4 availability zone: eu-west-1a VPC SubnetID: subnet-aa54XXXX Remote user: ubuntu labels: docker ubuntu linux SecurityGroup Name: (the id) sg-08079f78cXXXXXXX public ip = true connect via public ip = true Pipeline \u00b6 @Library ( 'jenkins-pipeline-library@master' ) _ def scmVars def label = \"jenkins-slave-${UUID.randomUUID().toString()}\" podTemplate ( label: label , yaml: \"\"\" apiVersion: v1 kind: Pod spec: containers: - name: kubectl image: vfarcic/kubectl command: [\"cat\"] tty: true \"\"\" ) { node ( label ) { node ( \"docker\" ) { stage ( 'SCM & Prepare' ) { scmVars = checkout scm } stage ( 'Lint' ) { dockerfileLint () } stage ( 'Build Docker' ) { sh \"docker image build -t demo:rc-1 .\" } stage ( 'Tag & Push Docker' ) { IMAGE = \"${DOCKER_IMAGE_NAME}\" TAG = \"${DOCKER_IMAGE_TAG}\" FULL_NAME = \"${FULL_IMAGE_NAME}\" withCredentials ([ usernamePassword ( credentialsId: \"dockerhub\" , usernameVariable: \"USER\" , passwordVariable: \"PASS\" )]) { sh \"docker login -u $USER -p $PASS\" } sh \"docker image tag ${IMAGE}:${TAG} ${FULL_NAME}\" sh \"docker image push ${FULL_NAME}\" } } // end node docker stage ( 'Prepare Pod' ) { // have to checkout on our kubernetes pod aswell checkout scm } stage ( 'Check version' ) { container ( 'kubectl' ) { sh 'kubectl version' } } } // end node random label } // end pod def Maven JIB \u00b6 If you use Java with either Gradle or Maven , you can use JIB to create docker image without requiring a docker client or docker engine. JIB will communicate with DockerHub and Google's container registry for sending image layers back and forth. The last layer will be created by the JIB plugin itself, adding your self-executing Jar to the layer and a Entrypoint with the correct flags. For more information about the runtime options, see either jib-maven-plugin documentation or jib-gradle-plugin . Prerequisites \u00b6 Java project build with Gradle or Maven Java project that can start it self, such as Spring Boot or Thorntail (previously Wildfly Swarm, from the JBoss family) able to build either gradle or maven applications The project used can be found at github.com/demomon/maven-spring-boot-demo . Steps \u00b6 configure the plugin for either Gradle or Maven build using an official docker image via the kubernetes pod template Pipeline \u00b6 Using a bit more elaborate pipeline example here. Using SonarCloud for static code analysis and then JIB to create a docker image. We could then use that image either directly in anything that runs docker or in the same cluster via a Helm Chart. def scmVars def tag pipeline { options { buildDiscarder logRotator ( artifactDaysToKeepStr: '5' , artifactNumToKeepStr: '5' , daysToKeepStr: '5' , numToKeepStr: '5' ) } libraries { lib ( 'core@master' ) lib ( 'maven@master' ) } agent { kubernetes { label 'mypod' defaultContainer 'jnlp' yaml \"\"\" apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:3-jdk-11-slim command: - cat tty: true \"\"\" } } stages { stage ( 'Test versions' ) { steps { container ( 'maven' ) { sh 'uname -a' sh 'mvn -version' } } } stage ( 'Checkout' ) { steps { script { scmVars = checkout scm } gitRemoteConfigByUrl ( scmVars . GIT_URL , 'githubtoken' ) sh ''' git config --global user.email \"jenkins@jenkins.io\" git config --global user.name \"Jenkins\" ''' } } stage ( 'Build' ) { steps { container ( 'maven' ) { sh 'mvn clean verify -B -e' } } } stage ( 'Version & Analysis' ) { parallel { stage ( 'Version Bump' ) { when { branch 'master' } environment { NEW_VERSION = gitNextSemverTagMaven ( 'pom.xml' ) } steps { script { tag = \"${NEW_VERSION}\" } container ( 'maven' ) { sh 'mvn versions:set -DnewVersion=${NEW_VERSION}' } gitTag ( \"v${NEW_VERSION}\" ) } } stage ( 'Sonar Analysis' ) { when { branch 'master' } environment { SONAR_HOST = 'https://sonarcloud.io' KEY = 'spring-maven-demo' ORG = 'demomon' SONAR_TOKEN = credentials ( 'sonarcloud' ) } steps { container ( 'maven' ) { sh '''mvn sonar:sonar \\ -Dsonar.projectKey=${KEY} \\ -Dsonar.organization=${ORG} \\ -Dsonar.host.url=${SONAR_HOST} \\ -Dsonar.login=${SONAR_TOKEN} ''' } } } } } stage ( 'Publish Artifact' ) { when { branch 'master' } environment { DHUB = credentials ( 'dockerhub' ) } steps { container ( 'maven' ) { // we should never come here if the tests have not run, as we run verify before sh 'mvn clean compile -B -e jib:build -Djib.to.auth.username=${DHUB_USR} -Djib.to.auth.password=${DHUB_PSW} -DskipTests' } } } } post { always { cleanWs () } } } Kaniko \u00b6 Google loves Kubernetes and Google prefers people building docker images in Kubernetes without Docker. As JIB is only available for Java projects, there's needs to be an alternative for any other usecase/programming language. The answer to that is Kaniko , a specialized Docker image to create Docker images. Kaniko isn't the most secure way to create docker images, it barely beats mounting a Docker Socket, or might even be worse if you ask others (such as Jess Frazelle ). That said, it is gaining some traction being used by JenkinsX and having an example in Jenkins' Kubernetes Plugin . Info When building more than one image inside the kaniko container, make sure to use the --cleanup flag. So it cleans its temporary cache data before building the next image, as discussed in this google group . Prerequisites \u00b6 Steps \u00b6 Create docker registry secret Configure pod container template Configure stage Create docker registry secret \u00b6 This is an example for DockerHub inside the build namespace. kubectl create secret docker-registry -n build regcred \\ --docker-server = index.docker.io \\ --docker-username = myDockerHubAccount \\ --docker-password = myDockerHubPassword \\ --docker-email = myDockerHub@Email.com Example Ppeline \u00b6 Warning Although multi-stage Dockerfile 's are supported, it did fail in my case. So I created a second Dockerfile which is only for running the application ( Dockerfile.run ). pipeline { agent { kubernetes { //cloud 'kubernetes' label 'kaniko' yaml \"\"\" kind: Pod metadata: name: kaniko spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root - name: go-build-cache mountPath: /root/.cache/go-build - name: img-build-cache mountPath: /root/.local volumes: - name: go-build-cache emptyDir: {} - name: img-build-cache emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/cat.git' } } stage ( 'Build' ) { steps { container ( 'golang' ) { sh './build-go-bin.sh' } } } stage ( 'Make Image' ) { environment { PATH = \"/busybox:$PATH\" } steps { container ( name: 'kaniko' , shell: '/busybox/sh' ) { sh '''#!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=index.docker.io/caladreas/cat ''' } } } } } IMG \u00b6 img is the brainchild of Jess Frazelle, a prominent figure in the container space. The goal is to be the safest and best way to build OCI compliant images, as outlined in her blog building container images securely on kubernetes . Not working (for me) yet \u00b6 It does not seem to work for me on AWS's EKS. To many little details with relation to runc, file permissions and other configuration. For those who want to give it a spin, here are some resources to take a look at. https://blog.jessfraz.com/post/building-container-images-securely-on-kubernetes/ https://github.com/genuinetools/img https://github.com/opencontainers/runc https://git.j3ss.co/genuinetools/img/+/d05b3e4e10cd0e3c074ffb03dc22d7bb6cde1e78 Pipeline Example \u00b6 pipeline { agent { kubernetes { label 'img' yaml \"\"\" kind: Pod metadata: name: img annotations: container.apparmor.security.beta.kubernetes.io/img: unconfined spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: img workingDir: /home/jenkins image: caladreas/img:0.5.1 imagePullPolicy: Always securityContext: rawProc: true privileged: true command: - cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: temp emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/cat.git' } } stage ( 'Build' ) { steps { container ( 'golang' ) { sh './build-go-bin.sh' } } } stage ( 'Make Image' ) { steps { container ( 'img' ) { sh 'mkdir cache' sh 'img build -s ./cache -f Dockerfile.run -t caladreas/cat .' } } } } }","title":"Docker Alternatives"},{"location":"jenkins-pipeline-examples/docker-alternatives/#pipelines-with-docker-alternatives","text":"Building pipelines with Jenkins on Docker has been common for a while. But accessing the docker socket has always been a bit tricky. The easiest solution is directly mounting the docker socket of the host into your build container. However, this is a big security risk and something that is undesirable at best and, pretty dangerous at worst. When you're using an orchestrator such as Kubernetes - which is where Jenkins is currently best put to work - the problem gets worse. Not only do you open up security holes, you also mess up the schedular's insight into available resources, and mess with it's ability to keep your cluster in a correct state. In some sense, using docker directly in a Kubernetes cluster, is likened to running with scissors .","title":"Pipelines With Docker Alternatives"},{"location":"jenkins-pipeline-examples/docker-alternatives/#potential-alternatives","text":"So, directly running docker containers via a docker engine inside your cluster is out. Let's look at some alternatives. Kubernetes Pod and External Node : the simple way out, use a cloud (such as EC2 Cloud) to provision a classic VM with docker engine as an agent and build there JIB : tool from Google, only works for Java applications and is supported directly from Gradle and Maven - Link Kaniko : tool from Google, works as 1-on-1 replacement for docker image build (except Windows Containers ) - Link IMG : tool from Jess Frazelle to avoid building docker images with a root user involved Link","title":"Potential Alternatives"},{"location":"jenkins-pipeline-examples/docker-alternatives/#kubernetes-pod-and-external-node","text":"One of the most used cloud environments is AWS, so I created this solution with AWS's Amazon EC2 Plugin . Warning Unfortunately, you cannot combine the Kubernetes plugin with external nodes (none pod container nodes) in a Declarative pipeline. So you have to use Scripted . This can be done with various different cloud providers such as Digital Ocean, Google, AWS or Azure. This guide will use AWS, as it has the most mature Jenkins Cloud plugin.","title":"Kubernetes Pod and External Node"},{"location":"jenkins-pipeline-examples/docker-alternatives/#prerequisites","text":"AWS Account with rights to create AMI's and run EC2 instances Packer Jenkins with Amazon EC2 Plugin installed","title":"Prerequisites"},{"location":"jenkins-pipeline-examples/docker-alternatives/#steps","text":"create AMI with Packer install and configure Amazon EC2 plugin create a test pipeline","title":"Steps"},{"location":"jenkins-pipeline-examples/docker-alternatives/#create-ami-with-packer","text":"Packer needs to be able to access EC2 API's and be able to spin up an EC2 instance and create an AMI out of it.","title":"Create AMI with Packer"},{"location":"jenkins-pipeline-examples/docker-alternatives/#aws-setup-for-packer","text":"You need to configure two things: account details for Packer to use security group where your EC2 instances will be running with this security group needs to open port 22 both Packer and Jenkins will use this for their connection export AWS_DEFAULT_REGION = eu-west-1 export AWS_ACCESS_KEY_ID = XXX export AWS_SECRET_ACCESS_KEY = XXX aws ec2 --profile myAwsProfile create-security-group \\ --description \"For building Docker images\" \\ --group-name docker { \"GroupId\" : \"sg-08079f78cXXXXXXX\" } Export the security group ID. export SG_ID = sg-08079f78cXXXXXXX echo $SG_ID","title":"AWS setup for Packer"},{"location":"jenkins-pipeline-examples/docker-alternatives/#enable-port-22","text":"aws ec2 \\ --profile myAwsProfile \\ authorize-security-group-ingress \\ --group-name docker \\ --protocol tcp \\ --port 22 \\ --cidr 0 .0.0.0/0","title":"Enable port 22"},{"location":"jenkins-pipeline-examples/docker-alternatives/#packer-ami-definition","text":"Here's an example definition for Packer for a Ubuntu 18.04 LTS base image with JDK 8 (required by Jenkins) and Docker. { \"builders\" : [{ \"type\" : \"amazon-ebs\" , \"region\" : \"eu-west-1\" , \"source_ami_filter\" : { \"filters\" : { \"virtualization-type\" : \"hvm\" , \"name\" : \"*ubuntu-bionic-18.04-amd64-server-*\" , \"root-device-type\" : \"ebs\" }, \"owners\" : [ \"679593333241\" ], \"most_recent\" : true }, \"instance_type\" : \"t2.micro\" , \"ssh_username\" : \"ubuntu\" , \"ami_name\" : \"docker\" , \"force_deregister\" : true }], \"provisioners\" : [{ \"type\" : \"shell\" , \"inline\" : [ \"sleep 15\" , \"sudo apt-get clean\" , \"sudo apt-get update\" , \"sudo apt-get install -y apt-transport-https ca-certificates nfs-common\" , \"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\" , \"sudo add-apt-repository \\\"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\\"\" , \"sudo add-apt-repository -y ppa:openjdk-r/ppa\" , \"sudo apt-get update\" , \"sudo apt-get install -y docker-ce\" , \"sudo usermod -aG docker ubuntu\" , \"sudo apt-get install -y openjdk-8-jdk\" , \"java -version\" , \"docker version\" ] }] } Build the new AMI with packer. packer build docker-ami.json export AMI = ami-0212ab37f84e418f4","title":"Packer AMI definition"},{"location":"jenkins-pipeline-examples/docker-alternatives/#ec2-key-pair","text":"Create EC2 key pair, this will be used by Jenkins to connect to the instances via ssh. aws ec2 --profile myAwsProfile create-key-pair \\ --key-name jenkinsec2 \\ | jq -r '.KeyMaterial' \\ >jenkins-ec2-proton.pem","title":"EC2 Key Pair"},{"location":"jenkins-pipeline-examples/docker-alternatives/#ec2-cloud-configuration","text":"In a Jenkins' master main configuration, you add a new cloud . In this case, we will use a ec2-cloud so we can instantiate our EC2 VM's with docker. use EC2 credentials for initial connection use key ( .pem ) for VM connection (jenkins <> agent) configure the following: AMI: ami-0212ab37f84e418f4 availability zone: eu-west-1a VPC SubnetID: subnet-aa54XXXX Remote user: ubuntu labels: docker ubuntu linux SecurityGroup Name: (the id) sg-08079f78cXXXXXXX public ip = true connect via public ip = true","title":"EC2 Cloud Configuration"},{"location":"jenkins-pipeline-examples/docker-alternatives/#pipeline","text":"@Library ( 'jenkins-pipeline-library@master' ) _ def scmVars def label = \"jenkins-slave-${UUID.randomUUID().toString()}\" podTemplate ( label: label , yaml: \"\"\" apiVersion: v1 kind: Pod spec: containers: - name: kubectl image: vfarcic/kubectl command: [\"cat\"] tty: true \"\"\" ) { node ( label ) { node ( \"docker\" ) { stage ( 'SCM & Prepare' ) { scmVars = checkout scm } stage ( 'Lint' ) { dockerfileLint () } stage ( 'Build Docker' ) { sh \"docker image build -t demo:rc-1 .\" } stage ( 'Tag & Push Docker' ) { IMAGE = \"${DOCKER_IMAGE_NAME}\" TAG = \"${DOCKER_IMAGE_TAG}\" FULL_NAME = \"${FULL_IMAGE_NAME}\" withCredentials ([ usernamePassword ( credentialsId: \"dockerhub\" , usernameVariable: \"USER\" , passwordVariable: \"PASS\" )]) { sh \"docker login -u $USER -p $PASS\" } sh \"docker image tag ${IMAGE}:${TAG} ${FULL_NAME}\" sh \"docker image push ${FULL_NAME}\" } } // end node docker stage ( 'Prepare Pod' ) { // have to checkout on our kubernetes pod aswell checkout scm } stage ( 'Check version' ) { container ( 'kubectl' ) { sh 'kubectl version' } } } // end node random label } // end pod def","title":"Pipeline"},{"location":"jenkins-pipeline-examples/docker-alternatives/#maven-jib","text":"If you use Java with either Gradle or Maven , you can use JIB to create docker image without requiring a docker client or docker engine. JIB will communicate with DockerHub and Google's container registry for sending image layers back and forth. The last layer will be created by the JIB plugin itself, adding your self-executing Jar to the layer and a Entrypoint with the correct flags. For more information about the runtime options, see either jib-maven-plugin documentation or jib-gradle-plugin .","title":"Maven JIB"},{"location":"jenkins-pipeline-examples/docker-alternatives/#prerequisites_1","text":"Java project build with Gradle or Maven Java project that can start it self, such as Spring Boot or Thorntail (previously Wildfly Swarm, from the JBoss family) able to build either gradle or maven applications The project used can be found at github.com/demomon/maven-spring-boot-demo .","title":"Prerequisites"},{"location":"jenkins-pipeline-examples/docker-alternatives/#steps_1","text":"configure the plugin for either Gradle or Maven build using an official docker image via the kubernetes pod template","title":"Steps"},{"location":"jenkins-pipeline-examples/docker-alternatives/#pipeline_1","text":"Using a bit more elaborate pipeline example here. Using SonarCloud for static code analysis and then JIB to create a docker image. We could then use that image either directly in anything that runs docker or in the same cluster via a Helm Chart. def scmVars def tag pipeline { options { buildDiscarder logRotator ( artifactDaysToKeepStr: '5' , artifactNumToKeepStr: '5' , daysToKeepStr: '5' , numToKeepStr: '5' ) } libraries { lib ( 'core@master' ) lib ( 'maven@master' ) } agent { kubernetes { label 'mypod' defaultContainer 'jnlp' yaml \"\"\" apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:3-jdk-11-slim command: - cat tty: true \"\"\" } } stages { stage ( 'Test versions' ) { steps { container ( 'maven' ) { sh 'uname -a' sh 'mvn -version' } } } stage ( 'Checkout' ) { steps { script { scmVars = checkout scm } gitRemoteConfigByUrl ( scmVars . GIT_URL , 'githubtoken' ) sh ''' git config --global user.email \"jenkins@jenkins.io\" git config --global user.name \"Jenkins\" ''' } } stage ( 'Build' ) { steps { container ( 'maven' ) { sh 'mvn clean verify -B -e' } } } stage ( 'Version & Analysis' ) { parallel { stage ( 'Version Bump' ) { when { branch 'master' } environment { NEW_VERSION = gitNextSemverTagMaven ( 'pom.xml' ) } steps { script { tag = \"${NEW_VERSION}\" } container ( 'maven' ) { sh 'mvn versions:set -DnewVersion=${NEW_VERSION}' } gitTag ( \"v${NEW_VERSION}\" ) } } stage ( 'Sonar Analysis' ) { when { branch 'master' } environment { SONAR_HOST = 'https://sonarcloud.io' KEY = 'spring-maven-demo' ORG = 'demomon' SONAR_TOKEN = credentials ( 'sonarcloud' ) } steps { container ( 'maven' ) { sh '''mvn sonar:sonar \\ -Dsonar.projectKey=${KEY} \\ -Dsonar.organization=${ORG} \\ -Dsonar.host.url=${SONAR_HOST} \\ -Dsonar.login=${SONAR_TOKEN} ''' } } } } } stage ( 'Publish Artifact' ) { when { branch 'master' } environment { DHUB = credentials ( 'dockerhub' ) } steps { container ( 'maven' ) { // we should never come here if the tests have not run, as we run verify before sh 'mvn clean compile -B -e jib:build -Djib.to.auth.username=${DHUB_USR} -Djib.to.auth.password=${DHUB_PSW} -DskipTests' } } } } post { always { cleanWs () } } }","title":"Pipeline"},{"location":"jenkins-pipeline-examples/docker-alternatives/#kaniko","text":"Google loves Kubernetes and Google prefers people building docker images in Kubernetes without Docker. As JIB is only available for Java projects, there's needs to be an alternative for any other usecase/programming language. The answer to that is Kaniko , a specialized Docker image to create Docker images. Kaniko isn't the most secure way to create docker images, it barely beats mounting a Docker Socket, or might even be worse if you ask others (such as Jess Frazelle ). That said, it is gaining some traction being used by JenkinsX and having an example in Jenkins' Kubernetes Plugin . Info When building more than one image inside the kaniko container, make sure to use the --cleanup flag. So it cleans its temporary cache data before building the next image, as discussed in this google group .","title":"Kaniko"},{"location":"jenkins-pipeline-examples/docker-alternatives/#prerequisites_2","text":"","title":"Prerequisites"},{"location":"jenkins-pipeline-examples/docker-alternatives/#steps_2","text":"Create docker registry secret Configure pod container template Configure stage","title":"Steps"},{"location":"jenkins-pipeline-examples/docker-alternatives/#create-docker-registry-secret","text":"This is an example for DockerHub inside the build namespace. kubectl create secret docker-registry -n build regcred \\ --docker-server = index.docker.io \\ --docker-username = myDockerHubAccount \\ --docker-password = myDockerHubPassword \\ --docker-email = myDockerHub@Email.com","title":"Create docker registry secret"},{"location":"jenkins-pipeline-examples/docker-alternatives/#example-ppeline","text":"Warning Although multi-stage Dockerfile 's are supported, it did fail in my case. So I created a second Dockerfile which is only for running the application ( Dockerfile.run ). pipeline { agent { kubernetes { //cloud 'kubernetes' label 'kaniko' yaml \"\"\" kind: Pod metadata: name: kaniko spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root - name: go-build-cache mountPath: /root/.cache/go-build - name: img-build-cache mountPath: /root/.local volumes: - name: go-build-cache emptyDir: {} - name: img-build-cache emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/cat.git' } } stage ( 'Build' ) { steps { container ( 'golang' ) { sh './build-go-bin.sh' } } } stage ( 'Make Image' ) { environment { PATH = \"/busybox:$PATH\" } steps { container ( name: 'kaniko' , shell: '/busybox/sh' ) { sh '''#!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=index.docker.io/caladreas/cat ''' } } } } }","title":"Example Ppeline"},{"location":"jenkins-pipeline-examples/docker-alternatives/#img","text":"img is the brainchild of Jess Frazelle, a prominent figure in the container space. The goal is to be the safest and best way to build OCI compliant images, as outlined in her blog building container images securely on kubernetes .","title":"IMG"},{"location":"jenkins-pipeline-examples/docker-alternatives/#not-working-for-me-yet","text":"It does not seem to work for me on AWS's EKS. To many little details with relation to runc, file permissions and other configuration. For those who want to give it a spin, here are some resources to take a look at. https://blog.jessfraz.com/post/building-container-images-securely-on-kubernetes/ https://github.com/genuinetools/img https://github.com/opencontainers/runc https://git.j3ss.co/genuinetools/img/+/d05b3e4e10cd0e3c074ffb03dc22d7bb6cde1e78","title":"Not working (for me) yet"},{"location":"jenkins-pipeline-examples/docker-alternatives/#pipeline-example","text":"pipeline { agent { kubernetes { label 'img' yaml \"\"\" kind: Pod metadata: name: img annotations: container.apparmor.security.beta.kubernetes.io/img: unconfined spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: img workingDir: /home/jenkins image: caladreas/img:0.5.1 imagePullPolicy: Always securityContext: rawProc: true privileged: true command: - cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: temp emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json \"\"\" } } stages { stage ( 'Checkout' ) { steps { git 'https://github.com/joostvdg/cat.git' } } stage ( 'Build' ) { steps { container ( 'golang' ) { sh './build-go-bin.sh' } } } stage ( 'Make Image' ) { steps { container ( 'img' ) { sh 'mkdir cache' sh 'img build -s ./cache -f Dockerfile.run -t caladreas/cat .' } } } } }","title":"Pipeline Example"},{"location":"jenkins-pipeline-examples/docker-declarative/","text":"Docker Declarative Examples \u00b6 pipeline { agent none options { timeout ( time: 10 , unit: 'MINUTES' ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: '5' )) } stages { stage ( 'Prepare' ){ agent { label 'docker' } steps { parallel ( Clean: { deleteDir () }, NotifySlack: { slackSend channel: 'cicd' , color: '#FFFF00' , message: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" } ) } } stage ( 'Checkout' ){ agent { label 'docker' } steps { git credentialsId: '355df378-e726-4abd-90fa-e723c5c21ad5' , url: 'git@gitlab.flusso.nl:CICD/ci-cd-docs.git' script { env . GIT_COMMIT_HASH = sh returnStdout: true , script: 'git rev-parse --verify HEAD' } } } stage ( 'Build Docs' ) { agent { docker { image \"caladreas/mkdocs-docker-build-container\" label \"docker\" } } steps { sh 'mkdocs build' } } stage ( 'Prepare Docker Image' ){ agent { label 'docker' } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true , script: 'docker run --rm -i lukasmartinelli/hadolint < Dockerfile' if ( lintResult . trim () == '' ) { println 'Lint finished with no errors' } else { println 'Error found in Lint' println \"${lintResult}\" currentBuild . result = 'UNSTABLE' } } }, // end test dockerfile BuildImage: { sh 'chmod +x build.sh' sh './build.sh' } ) } post { success { sh 'chmod +x push.sh' sh './push.sh' } } } stage ( 'Update Docker Container' ) { agent { label 'docker' } steps { sh 'chmod +x container-update.sh' sh \"./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH}\" } } } post { success { slackSend channel: 'cicd' , color: '#00FF00' , message: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" } failure { slackSend channel: 'cicd' , color: '#FF0000' , message: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" } } }","title":"Docker Declarative"},{"location":"jenkins-pipeline-examples/docker-declarative/#docker-declarative-examples","text":"pipeline { agent none options { timeout ( time: 10 , unit: 'MINUTES' ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: '5' )) } stages { stage ( 'Prepare' ){ agent { label 'docker' } steps { parallel ( Clean: { deleteDir () }, NotifySlack: { slackSend channel: 'cicd' , color: '#FFFF00' , message: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" } ) } } stage ( 'Checkout' ){ agent { label 'docker' } steps { git credentialsId: '355df378-e726-4abd-90fa-e723c5c21ad5' , url: 'git@gitlab.flusso.nl:CICD/ci-cd-docs.git' script { env . GIT_COMMIT_HASH = sh returnStdout: true , script: 'git rev-parse --verify HEAD' } } } stage ( 'Build Docs' ) { agent { docker { image \"caladreas/mkdocs-docker-build-container\" label \"docker\" } } steps { sh 'mkdocs build' } } stage ( 'Prepare Docker Image' ){ agent { label 'docker' } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true , script: 'docker run --rm -i lukasmartinelli/hadolint < Dockerfile' if ( lintResult . trim () == '' ) { println 'Lint finished with no errors' } else { println 'Error found in Lint' println \"${lintResult}\" currentBuild . result = 'UNSTABLE' } } }, // end test dockerfile BuildImage: { sh 'chmod +x build.sh' sh './build.sh' } ) } post { success { sh 'chmod +x push.sh' sh './push.sh' } } } stage ( 'Update Docker Container' ) { agent { label 'docker' } steps { sh 'chmod +x container-update.sh' sh \"./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH}\" } } } post { success { slackSend channel: 'cicd' , color: '#00FF00' , message: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" } failure { slackSend channel: 'cicd' , color: '#FF0000' , message: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" } } }","title":"Docker Declarative Examples"},{"location":"jenkins-pipeline-examples/maven-declarative/","text":"Maven Declarative Examples \u00b6 Basics \u00b6 We have to wrap the entire script in pipeline { } , for it to be marked a declarative script. As we will be using different agents for different stages, we select none as the default. For house keeping, we add the options{} block, where we configure the following: timeout: make sure this jobs succeeds in 10 minutes, else just cancel it timestamps(): to make sure we have timestamps in our logs buildDiscarder(): this will make sure we will only keep the latest 5 builds pipeline { agent none options { timeout ( time: 10 , unit: 'MINUTES' ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: '5' )) } } Checkout \u00b6 There are several ways to checkout the code. Let's assume our code is somewhere in a git repository. Full Checkout command \u00b6 The main command for checking out is the Checkout command. It will look like this. stage ( 'SCM' ) { checkout ([ $class : 'GitSCM' , branches: [[ name: '*/master' ]], doGenerateSubmoduleConfigurations: false , extensions: [], submoduleCfg: [], userRemoteConfigs: [[ credentialsId: 'MyCredentialsId' , url: 'https://github.com/joostvdg/keep-watching' ]] ]) } Git shorthand \u00b6 Thats a lot of configuration for a simple checkout. So what if I'm just using the master branch of a publicly accessible repository (as is the case with GitHub)? stage ( 'SCM' ) { git 'https://github.com/joostvdg/keep-watching' } Or with a different branch and credentials: stage ( 'SCM' ) { git credentialsId: 'MyCredentialsId' , url: 'https://github.com/joostvdg/keep-watching' } That's much better, but we can do even better. SCM shorthand \u00b6 If you're starting this pipeline job via a SCM, you've already configured the SCM. So assuming you've configured a pipeline job with 'Jenkinsfile from SCM' or an abstraction job - such as Multibranch-Pipeline, GitHub Organization or BitBucket Team/Project - you can do this. stage ( 'SCM' ) { checkout scm } The checkout scm line will use the checkout command we've used in the first example together with the object scm . This scm object, will contain the SCM configuration of the Job and will be reused for checking out. Warning A pipeline job from SCM or abstraction, will only checkout your Jenkinsfile. You will always need to checkout the rest of your code if you want to build it. For that, just use checkout scm Different Agent per Stage \u00b6 As you could see on the top, we've set agent to none. So for every stage we now need to tell it which agent to use - without it, the stage will fail. Agent any \u00b6 If you don't care what node it comes on, you specify any. stage ( 'Checkout' ) { agent any steps { git 'https://github.com/joostvdg/keep-watching' } } Agent via Label \u00b6 If you want build on a node with a specific label - here docker - you do so with agent { label '<LABEL>' } . stage ( 'Checkout' ) { agent { label 'docker' } steps { git 'https://github.com/joostvdg/keep-watching' } } Docker Container as Agent \u00b6 Many developers are using docker for their CI/CD builds. So being able to use docker containers as build agents is a requirement these days. stage ( 'Maven Build' ) { agent { docker { image 'maven:3-alpine' label 'docker' args '-v /home/joost/.m2:/root/.m2' } } steps { sh 'mvn -B clean package' } } Cache Maven repo \u00b6 When you're using a docker build container, it will be clean every time. So if you want to avoid downloading the maven dependencies every build, you have to cache them. One way to do this, is to map a volume into the container so the container will use that folder instead. stage ( 'Maven Build' ) { agent { docker { image 'maven:3-alpine' label 'docker' args '-v /home/joost/.m2:/root/.m2' } } steps { sh 'mvn -B clean package' } } Post stage/build \u00b6 The declarative pipeline allows for Post actions, on both stage and complete build level. For both types there are different post hooks you can use, such as success, failure. stage ( 'Maven Build' ) { agent { docker { image 'maven:3-alpine' label 'docker' args '-v /home/joost/.m2:/root/.m2' } } steps { sh 'mvn -B clean package' } post { success { junit 'target/surefire-reports/**/*.xml' } } } stages { stage ( 'Example' ) { steps { echo 'Hello World' } } } post { always { echo 'This will always run' } success { echo 'SUCCESS!' } failure { echo \"We Failed\" } unstable { echo \"We're unstable\" } changed { echo \"Status Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result]\" } } Entire example \u00b6 pipeline { agent none options { timeout ( time: 10 , unit: 'MINUTES' ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: '5' )) } stages { stage ( 'Example' ) { steps { echo 'Hello World' } } stage ( 'Checkout' ) { agent { label 'docker' } steps { git 'https://github.com/joostvdg/keep-watching' } } stage ( 'Maven Build' ) { agent { docker { image 'maven:3-alpine' label 'docker' args '-v /home/joost/.m2:/root/.m2' } } steps { sh 'mvn -B clean package' } post { success { junit 'target/surefire-reports/**/*.xml' } } } stage ( 'Docker Build' ) { agent { label 'docker' } steps { sh 'docker build --tag=keep-watching-be .' } } } post { always { echo 'This will always run' } success { echo 'SUCCESS!' } failure { echo \"We Failed\" } unstable { echo \"We're unstable\" } changed { echo \"Status Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result]\" } } }","title":"Maven Declarative"},{"location":"jenkins-pipeline-examples/maven-declarative/#maven-declarative-examples","text":"","title":"Maven Declarative Examples"},{"location":"jenkins-pipeline-examples/maven-declarative/#basics","text":"We have to wrap the entire script in pipeline { } , for it to be marked a declarative script. As we will be using different agents for different stages, we select none as the default. For house keeping, we add the options{} block, where we configure the following: timeout: make sure this jobs succeeds in 10 minutes, else just cancel it timestamps(): to make sure we have timestamps in our logs buildDiscarder(): this will make sure we will only keep the latest 5 builds pipeline { agent none options { timeout ( time: 10 , unit: 'MINUTES' ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: '5' )) } }","title":"Basics"},{"location":"jenkins-pipeline-examples/maven-declarative/#checkout","text":"There are several ways to checkout the code. Let's assume our code is somewhere in a git repository.","title":"Checkout"},{"location":"jenkins-pipeline-examples/maven-declarative/#full-checkout-command","text":"The main command for checking out is the Checkout command. It will look like this. stage ( 'SCM' ) { checkout ([ $class : 'GitSCM' , branches: [[ name: '*/master' ]], doGenerateSubmoduleConfigurations: false , extensions: [], submoduleCfg: [], userRemoteConfigs: [[ credentialsId: 'MyCredentialsId' , url: 'https://github.com/joostvdg/keep-watching' ]] ]) }","title":"Full Checkout command"},{"location":"jenkins-pipeline-examples/maven-declarative/#git-shorthand","text":"Thats a lot of configuration for a simple checkout. So what if I'm just using the master branch of a publicly accessible repository (as is the case with GitHub)? stage ( 'SCM' ) { git 'https://github.com/joostvdg/keep-watching' } Or with a different branch and credentials: stage ( 'SCM' ) { git credentialsId: 'MyCredentialsId' , url: 'https://github.com/joostvdg/keep-watching' } That's much better, but we can do even better.","title":"Git shorthand"},{"location":"jenkins-pipeline-examples/maven-declarative/#scm-shorthand","text":"If you're starting this pipeline job via a SCM, you've already configured the SCM. So assuming you've configured a pipeline job with 'Jenkinsfile from SCM' or an abstraction job - such as Multibranch-Pipeline, GitHub Organization or BitBucket Team/Project - you can do this. stage ( 'SCM' ) { checkout scm } The checkout scm line will use the checkout command we've used in the first example together with the object scm . This scm object, will contain the SCM configuration of the Job and will be reused for checking out. Warning A pipeline job from SCM or abstraction, will only checkout your Jenkinsfile. You will always need to checkout the rest of your code if you want to build it. For that, just use checkout scm","title":"SCM shorthand"},{"location":"jenkins-pipeline-examples/maven-declarative/#different-agent-per-stage","text":"As you could see on the top, we've set agent to none. So for every stage we now need to tell it which agent to use - without it, the stage will fail.","title":"Different Agent per Stage"},{"location":"jenkins-pipeline-examples/maven-declarative/#agent-any","text":"If you don't care what node it comes on, you specify any. stage ( 'Checkout' ) { agent any steps { git 'https://github.com/joostvdg/keep-watching' } }","title":"Agent any"},{"location":"jenkins-pipeline-examples/maven-declarative/#agent-via-label","text":"If you want build on a node with a specific label - here docker - you do so with agent { label '<LABEL>' } . stage ( 'Checkout' ) { agent { label 'docker' } steps { git 'https://github.com/joostvdg/keep-watching' } }","title":"Agent via Label"},{"location":"jenkins-pipeline-examples/maven-declarative/#docker-container-as-agent","text":"Many developers are using docker for their CI/CD builds. So being able to use docker containers as build agents is a requirement these days. stage ( 'Maven Build' ) { agent { docker { image 'maven:3-alpine' label 'docker' args '-v /home/joost/.m2:/root/.m2' } } steps { sh 'mvn -B clean package' } }","title":"Docker Container as Agent"},{"location":"jenkins-pipeline-examples/maven-declarative/#cache-maven-repo","text":"When you're using a docker build container, it will be clean every time. So if you want to avoid downloading the maven dependencies every build, you have to cache them. One way to do this, is to map a volume into the container so the container will use that folder instead. stage ( 'Maven Build' ) { agent { docker { image 'maven:3-alpine' label 'docker' args '-v /home/joost/.m2:/root/.m2' } } steps { sh 'mvn -B clean package' } }","title":"Cache Maven repo"},{"location":"jenkins-pipeline-examples/maven-declarative/#post-stagebuild","text":"The declarative pipeline allows for Post actions, on both stage and complete build level. For both types there are different post hooks you can use, such as success, failure. stage ( 'Maven Build' ) { agent { docker { image 'maven:3-alpine' label 'docker' args '-v /home/joost/.m2:/root/.m2' } } steps { sh 'mvn -B clean package' } post { success { junit 'target/surefire-reports/**/*.xml' } } } stages { stage ( 'Example' ) { steps { echo 'Hello World' } } } post { always { echo 'This will always run' } success { echo 'SUCCESS!' } failure { echo \"We Failed\" } unstable { echo \"We're unstable\" } changed { echo \"Status Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result]\" } }","title":"Post stage/build"},{"location":"jenkins-pipeline-examples/maven-declarative/#entire-example","text":"pipeline { agent none options { timeout ( time: 10 , unit: 'MINUTES' ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: '5' )) } stages { stage ( 'Example' ) { steps { echo 'Hello World' } } stage ( 'Checkout' ) { agent { label 'docker' } steps { git 'https://github.com/joostvdg/keep-watching' } } stage ( 'Maven Build' ) { agent { docker { image 'maven:3-alpine' label 'docker' args '-v /home/joost/.m2:/root/.m2' } } steps { sh 'mvn -B clean package' } post { success { junit 'target/surefire-reports/**/*.xml' } } } stage ( 'Docker Build' ) { agent { label 'docker' } steps { sh 'docker build --tag=keep-watching-be .' } } } post { always { echo 'This will always run' } success { echo 'SUCCESS!' } failure { echo \"We Failed\" } unstable { echo \"We're unstable\" } changed { echo \"Status Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result]\" } } }","title":"Entire example"},{"location":"jenkins-pipeline-examples/maven-groovy-dsl/","text":"Maven Groovy DSL Example \u00b6 node { timestamps { timeout ( time: 15 , unit: 'MINUTES' ) { deleteDir () stage 'SCM' //git branch: 'master', credentialsId: 'flusso-gitlab', url: 'https://gitlab.flusso.nl/keep/keep-backend-spring.git' checkout scm env . JAVA_HOME = \"${tool 'JDK 8 Latest'}\" env . PATH = \"${env.JAVA_HOME}/bin:${env.PATH}\" sh 'java -version' try { def gradleHome = tool name: 'Gradle Latest' , type: 'hudson.plugins.gradle.GradleInstallation' stage 'Build' sh \"${gradleHome}/bin/gradle clean build javadoc\" step ([ $class : 'CheckStylePublisher' , canComputeNew: false , defaultEncoding: '' , healthy: '' , pattern: 'build/reports/checkstyle/main.xml' , unHealthy: '' ]) step ([ $class : 'JUnitResultArchiver' , testResults: 'build/test-results/*.xml' ]) step ([ $class : 'JavadocArchiver' , javadocDir: 'build/docs/javadoc' ]) stage 'SonarQube' sh \"${gradleHome}/bin/gradle sonarqube -Dsonar.host.url=http://sonarqube5-instance:9000\" stash 'workspace' } catch ( err ) { archive 'build/**/*.html' echo \"Caught: ${err}\" currentBuild . result = 'FAILURE' } } } } node ( 'docker' ) { timestamps { timeout ( time: 15 , unit: 'MINUTES' ) { deleteDir () unstash 'workspace' stage 'Build Docker image' sh './build.sh' def image = docker . image ( 'keep-backend-spring-img' ) stage 'Push Docker image' try { sh 'docker tag keep-backend-spring-img nexus.docker:18443/flusso/keep-backend-spring-img:latest' sh 'docker push nexus.docker:18443/flusso/keep-backend-spring-img:latest' } catch ( err ) { archive 'build/**/*.html' echo \"Caught: ${err}\" currentBuild . result = 'FAILURE' } } } }","title":"Maven Groovy DSL Example"},{"location":"jenkins-pipeline-examples/maven-groovy-dsl/#maven-groovy-dsl-example","text":"node { timestamps { timeout ( time: 15 , unit: 'MINUTES' ) { deleteDir () stage 'SCM' //git branch: 'master', credentialsId: 'flusso-gitlab', url: 'https://gitlab.flusso.nl/keep/keep-backend-spring.git' checkout scm env . JAVA_HOME = \"${tool 'JDK 8 Latest'}\" env . PATH = \"${env.JAVA_HOME}/bin:${env.PATH}\" sh 'java -version' try { def gradleHome = tool name: 'Gradle Latest' , type: 'hudson.plugins.gradle.GradleInstallation' stage 'Build' sh \"${gradleHome}/bin/gradle clean build javadoc\" step ([ $class : 'CheckStylePublisher' , canComputeNew: false , defaultEncoding: '' , healthy: '' , pattern: 'build/reports/checkstyle/main.xml' , unHealthy: '' ]) step ([ $class : 'JUnitResultArchiver' , testResults: 'build/test-results/*.xml' ]) step ([ $class : 'JavadocArchiver' , javadocDir: 'build/docs/javadoc' ]) stage 'SonarQube' sh \"${gradleHome}/bin/gradle sonarqube -Dsonar.host.url=http://sonarqube5-instance:9000\" stash 'workspace' } catch ( err ) { archive 'build/**/*.html' echo \"Caught: ${err}\" currentBuild . result = 'FAILURE' } } } } node ( 'docker' ) { timestamps { timeout ( time: 15 , unit: 'MINUTES' ) { deleteDir () unstash 'workspace' stage 'Build Docker image' sh './build.sh' def image = docker . image ( 'keep-backend-spring-img' ) stage 'Push Docker image' try { sh 'docker tag keep-backend-spring-img nexus.docker:18443/flusso/keep-backend-spring-img:latest' sh 'docker push nexus.docker:18443/flusso/keep-backend-spring-img:latest' } catch ( err ) { archive 'build/**/*.html' echo \"Caught: ${err}\" currentBuild . result = 'FAILURE' } } } }","title":"Maven Groovy DSL Example"},{"location":"jenkinsx/aks-boot-core/","text":"Jenkins X On AKS With JX Boot & CloudBees Core \u00b6 The goal of the guide is the following: manage CloudBees Core on Modern via Jenkins X in its own environment/namespace. To make it more interesting, we add more variables in the mix in the form of \"requirements\". cluster must NOT run on GKE, Jenkins X works pretty well there and doesn't teach us much every exposed service MUST use TLS, no excuses we do not want to create a certificate for every service that uses TLS as much as possible must be Configuration-as-Code In conclusion: We use Terraform to manage the Kubernetes Cluster on AKS JX Boot to manage Jenkins X We use Google CloudDNS to manage the DNS this enables us to validate an entire subdomain via Let's Encrypt in one go Note Unfortunately, these are already quite a lot of requirements. The Vault integration on anywhere but GKE is not stable. So we cheat and use local storage for credentials, meaning we need to use jx boot every time to upgrade the cluster. We will come back to this! Install Jenkins X \u00b6 First, install Jenkins X with jx boot on AKS . Install CloudBees Core \u00b6 In order to install CloudBees Core with TLS, we need the following: TLS configuration for the environment Core is landing in (see above on how) add CloudBees Core as a requirement to the env/requirements.yaml add configuration for CloudBees Core to the env/values.yaml requirements.yaml \u00b6 dependencies : - name : exposecontroller version : 2.3.89 repository : http://chartmuseum.jenkins-x.io alias : expose - name : exposecontroller version : 2.3.89 repository : http://chartmuseum.jenkins-x.io alias : cleanup - name : cloudbees-core version : 2.176.203 repository : https://charts.cloudbees.com/public/cloudbees alias : cbcore values.yaml \u00b6 Important The value you've set for the alias in the requirements, is your entrypoint for the configuration in the values.yaml ! Also, take care to change the following values to reflect your environment! * OperationsCenter.HostName * OperationsCenter.Ingress.tls.Host * OperationsCenter.Ingress.tls.SecretName cbcore : OperationsCenter : CSRF : ProxyCompatibility : true HostName : cbcore.staging.aks.example.com Ingress : Annotations : kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : \"true\" nginx.ingress.kubernetes.io/app-root : https://$best_http_host/cjoc/teams-check/ nginx.ingress.kubernetes.io/proxy-body-size : 50m nginx.ingress.kubernetes.io/proxy-request-buffering : \"off\" nginx.ingress.kubernetes.io/ssl-redirect : \"true\" tls : Enable : true Host : cbcore.staging.aks.example.com SecretName : tls-staging-aks-example-com-p ServiceType : ClusterIP nginx-ingress : Enabled : false Resources \u00b6 https://cloud.google.com/iam/docs/creating-managing-service-account-keys#iam-service-account-keys-create-console https://medium.com/google-cloud/kubernetes-w-lets-encrypt-cloud-dns-c888b2ff8c0e https://support.google.com/domains/answer/3290309?hl=en-GB&ref_topic=9018335 https://thorsten-hans.com/how-to-use-private-azure-container-registry-with-kubernetes https://cloud.google.com/dns/docs/migrating","title":"JX Boot & CloudBees Core"},{"location":"jenkinsx/aks-boot-core/#jenkins-x-on-aks-with-jx-boot-cloudbees-core","text":"The goal of the guide is the following: manage CloudBees Core on Modern via Jenkins X in its own environment/namespace. To make it more interesting, we add more variables in the mix in the form of \"requirements\". cluster must NOT run on GKE, Jenkins X works pretty well there and doesn't teach us much every exposed service MUST use TLS, no excuses we do not want to create a certificate for every service that uses TLS as much as possible must be Configuration-as-Code In conclusion: We use Terraform to manage the Kubernetes Cluster on AKS JX Boot to manage Jenkins X We use Google CloudDNS to manage the DNS this enables us to validate an entire subdomain via Let's Encrypt in one go Note Unfortunately, these are already quite a lot of requirements. The Vault integration on anywhere but GKE is not stable. So we cheat and use local storage for credentials, meaning we need to use jx boot every time to upgrade the cluster. We will come back to this!","title":"Jenkins X On AKS With JX Boot &amp; CloudBees Core"},{"location":"jenkinsx/aks-boot-core/#install-jenkins-x","text":"First, install Jenkins X with jx boot on AKS .","title":"Install Jenkins X"},{"location":"jenkinsx/aks-boot-core/#install-cloudbees-core","text":"In order to install CloudBees Core with TLS, we need the following: TLS configuration for the environment Core is landing in (see above on how) add CloudBees Core as a requirement to the env/requirements.yaml add configuration for CloudBees Core to the env/values.yaml","title":"Install CloudBees Core"},{"location":"jenkinsx/aks-boot-core/#requirementsyaml","text":"dependencies : - name : exposecontroller version : 2.3.89 repository : http://chartmuseum.jenkins-x.io alias : expose - name : exposecontroller version : 2.3.89 repository : http://chartmuseum.jenkins-x.io alias : cleanup - name : cloudbees-core version : 2.176.203 repository : https://charts.cloudbees.com/public/cloudbees alias : cbcore","title":"requirements.yaml"},{"location":"jenkinsx/aks-boot-core/#valuesyaml","text":"Important The value you've set for the alias in the requirements, is your entrypoint for the configuration in the values.yaml ! Also, take care to change the following values to reflect your environment! * OperationsCenter.HostName * OperationsCenter.Ingress.tls.Host * OperationsCenter.Ingress.tls.SecretName cbcore : OperationsCenter : CSRF : ProxyCompatibility : true HostName : cbcore.staging.aks.example.com Ingress : Annotations : kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : \"true\" nginx.ingress.kubernetes.io/app-root : https://$best_http_host/cjoc/teams-check/ nginx.ingress.kubernetes.io/proxy-body-size : 50m nginx.ingress.kubernetes.io/proxy-request-buffering : \"off\" nginx.ingress.kubernetes.io/ssl-redirect : \"true\" tls : Enable : true Host : cbcore.staging.aks.example.com SecretName : tls-staging-aks-example-com-p ServiceType : ClusterIP nginx-ingress : Enabled : false","title":"values.yaml"},{"location":"jenkinsx/aks-boot-core/#resources","text":"https://cloud.google.com/iam/docs/creating-managing-service-account-keys#iam-service-account-keys-create-console https://medium.com/google-cloud/kubernetes-w-lets-encrypt-cloud-dns-c888b2ff8c0e https://support.google.com/domains/answer/3290309?hl=en-GB&ref_topic=9018335 https://thorsten-hans.com/how-to-use-private-azure-container-registry-with-kubernetes https://cloud.google.com/dns/docs/migrating","title":"Resources"},{"location":"jenkinsx/build-packs/","text":"Build Packs \u00b6 In this guide we will look at what a Jenkins X Build Pack is, what you can use it for and how you can extend it. What is a Build Pack \u00b6 A build pack is a collection of resources that helps you build and deploy an application in Kubernetes with Jenkins X. There's three ways you can use a build pack: when you create a new project ( jx create quickstart ) when you import an existing project ( jx import ) referencing it in a jenkins-x.yaml pipeline file A build pack contains resources related to the pipeline, both a Jenkinsfile for static Jenkins and a jenkins-x.yanl for Jenkins X pipelines. In addition, it contains everything else required to get an application of a specific technology and framework to build, versioned, containerized, and deployed with Helm. This means it contains a Dockerfile and a Helm Chart at least. When importing, what you already have doesn't get replaced, so they're safe to apply to any existing applications. As with any technology, you can never capture the whole world. This means there are times when you need to extend the default Build Packs. Ways To Extend \u00b6 Roughly speaking, there are three ways to extend the Build Packs functionality within Jenkins X. Customize : you can customize your Build Packs by replacing the local reference of Jenkins X to different repository this makes especially sense for jx create quickstart and jx import , as these commands run locally Extend Locally : you can locally (in your own repository) extend the build pack by changing the generated files (such as jenkins-x.yaml ) Extend Globally : for the Jenkins X pipeline ( jenkins-x.yaml ), you can also extend it globally by chaining Build Pack references Important The focus of this guide is on extending globally ! Customize \u00b6 To customize the Build Packs we have to do the following: fork the default Jenkins X Build Packs make your changes to this fork repository update your Jenkins X's local Build Pack reference to your fork To fork the repository, you can go here https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes . The changes you might want to make I cannot predict, so you're on your own there. To create a similar Build Pack, you can copy a whole folder, and only change the things that need to be different. And, last but not least, to tell Jenkins X to use a different Build Pack repositort, use the command below: Tip In order to make this a bit easier to use, I've assumed the following: you do a direct fork, and do not rename the repository you fork it on GitHub Set the GH_USER variable and then the next few commands should be easier to use. GH_USER = jx edit buildpack \\ -u https://github.com/ $GH_USER /jenkins-x-kubernetes \\ -r master \\ -b Once you've use it to create a new quickstart or import an existing application, you should be able to see your Build Repository checked out by Jenkins X locally. ls -1 ~/.jx/draft/packs/github.com/ $GH_USER /jenkins-x-kubernetes/packs For a whole tutorial, you can look at Viktor Farcic's blog technologyconversations.com. Extend Locally \u00b6 To extend locally, we simply have to alter anything in your application's repository. We can be a bit more helpful, there are ways to extend the Jenkins X pipeline locally, not by writing your own, but by the special override syntax in your jenkins-x.yaml file. This would look like this: pipelineConfig: pipelines: overrides: Go to the Jenkins X Pipelines page for further details. Extend Globally \u00b6 Extending the Build Pack globally has a very limited scope, it is only for the Jenkins X pipeline. However, as the pipeline is one of the - if not the - most important parts of Jenkins X. So I'd argue that it is very powerful despite its limited scope. So what we're going to do is the following: create a new repository set up the required structure in the repository create a pipeline extension configure an existing Jenkins X application to leverage our Build Pack Info Why would you want to globally extend the Build Packs? Because this allows you to store your extensions to the Jenkins X default pipelines in a way every application can reuse them, or even extend those. It allows you to define standard steps that every pipeline in your organization needs to execute, once and only once. Create Build Pack Repository \u00b6 The minimal amount we need, is a folder called packs , inside which we need a few more things. an imports.yaml importing any and all Build Pack repositories we want to use for example, the default kubernetes and classic packs from the Jenkins X Authors themselves a folder, the name of your Build Pack, containing a pipeline.yaml The structure will then look like this. . \u251c\u2500\u2500 README.md \u2514\u2500\u2500 packs \u251c\u2500\u2500 imports.yaml \u2514\u2500\u2500 maven-joost \u2514\u2500\u2500 pipeline.yaml packs/imports.yaml Here we import the default Build Pack repositories, so our new pipelines can extends them using the Pipeline Extensions syntax. modules : - name : classic gitUrl : https://github.com/jenkins-x-buildpacks/jenkins-x-classic.git gitRef : master - name : kubernetes gitUrl : https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes.git gitRef : master Create Pipeline Extension \u00b6 In order to extend the existing pipelines coming from other Build Packs, we have to set the extends configuration in the Build Pack's pipeline.yaml . This file would reside in packs/<name-of-your-pack>/pipeline.yaml . In this case, we want to extend the kubernetes Build Pack repository's maven-java11 syntax. We do this by filling import field with the name field from the repository listed in the packs/imports.yaml file. We then select a Build Pack's pipeline file by pointing to a pipeline.yaml file from the relative path of packs/ . Lets say we want to use a different docker container as build agent, we would end up with this. packs/maven-joost/pipeline.yaml extends : import : kubernetes file : maven-java11/pipeline.yaml agent : label : jenkins-maven-java11 image : maven-java11 container : maven Extending The Pipeline Further \u00b6 If you want to further extend the pipeline, you can leverage the Jenkins X Pipeline syntax. For example, say you want to make sure your Pull Request builds run a SonarQube scan. You can add the step sonar-scan-pr to the pullRequest Pipeline, under the stage build as below. packs/maven-joost/pipeline.yaml pipelines : pullRequest : build : steps : - name : sonar-scan-pr command : sonar-scanner image : newtmitch/sonar-scanner:3.0 dir : /workspace/source/ args : - -Dsonar.projectName=... - -Dsonar.projectKey=... - -Dsonar.organization=... - -Dsonar.sources=./src/main/java/ - -Dsonar.language=java - -Dsonar.java.binaries=./target/classes - -Dsonar.host.url=https://sonarcloud.io - -Dsonar.login=${SONARCLOUD_TOKEN} Go to the Jenkins X Pipelines page for further details. Use New Build Pack \u00b6 Once you have defined the pipeline in your Build Pack, you then specify the Build Pack you want to use in your application in the jenkins-x.yaml file. Important This section refers to your application's, not the Build Back's repository. You need to specify three parameters in order for Jenkins X to pick up your Build Pack and build up the effective pipeline from your Build Pack hierarchy. buildPack : the name of your Build Pack, e.g. the folder name in your Build Pack repository's packs folder that contains the pipeline.yaml buildPackGitRef : the Git ref, e.g. tag, commit or branch name buildPackGitURL : the http(s) git URL jenkins-x.yaml buildPack: maven-joost buildPackGitRef: master buildPackGitURL: https://github.com/joostvdg/jx-buildpacks.git","title":"Build Packs"},{"location":"jenkinsx/build-packs/#build-packs","text":"In this guide we will look at what a Jenkins X Build Pack is, what you can use it for and how you can extend it.","title":"Build Packs"},{"location":"jenkinsx/build-packs/#what-is-a-build-pack","text":"A build pack is a collection of resources that helps you build and deploy an application in Kubernetes with Jenkins X. There's three ways you can use a build pack: when you create a new project ( jx create quickstart ) when you import an existing project ( jx import ) referencing it in a jenkins-x.yaml pipeline file A build pack contains resources related to the pipeline, both a Jenkinsfile for static Jenkins and a jenkins-x.yanl for Jenkins X pipelines. In addition, it contains everything else required to get an application of a specific technology and framework to build, versioned, containerized, and deployed with Helm. This means it contains a Dockerfile and a Helm Chart at least. When importing, what you already have doesn't get replaced, so they're safe to apply to any existing applications. As with any technology, you can never capture the whole world. This means there are times when you need to extend the default Build Packs.","title":"What is a Build Pack"},{"location":"jenkinsx/build-packs/#ways-to-extend","text":"Roughly speaking, there are three ways to extend the Build Packs functionality within Jenkins X. Customize : you can customize your Build Packs by replacing the local reference of Jenkins X to different repository this makes especially sense for jx create quickstart and jx import , as these commands run locally Extend Locally : you can locally (in your own repository) extend the build pack by changing the generated files (such as jenkins-x.yaml ) Extend Globally : for the Jenkins X pipeline ( jenkins-x.yaml ), you can also extend it globally by chaining Build Pack references Important The focus of this guide is on extending globally !","title":"Ways To Extend"},{"location":"jenkinsx/build-packs/#customize","text":"To customize the Build Packs we have to do the following: fork the default Jenkins X Build Packs make your changes to this fork repository update your Jenkins X's local Build Pack reference to your fork To fork the repository, you can go here https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes . The changes you might want to make I cannot predict, so you're on your own there. To create a similar Build Pack, you can copy a whole folder, and only change the things that need to be different. And, last but not least, to tell Jenkins X to use a different Build Pack repositort, use the command below: Tip In order to make this a bit easier to use, I've assumed the following: you do a direct fork, and do not rename the repository you fork it on GitHub Set the GH_USER variable and then the next few commands should be easier to use. GH_USER = jx edit buildpack \\ -u https://github.com/ $GH_USER /jenkins-x-kubernetes \\ -r master \\ -b Once you've use it to create a new quickstart or import an existing application, you should be able to see your Build Repository checked out by Jenkins X locally. ls -1 ~/.jx/draft/packs/github.com/ $GH_USER /jenkins-x-kubernetes/packs For a whole tutorial, you can look at Viktor Farcic's blog technologyconversations.com.","title":"Customize"},{"location":"jenkinsx/build-packs/#extend-locally","text":"To extend locally, we simply have to alter anything in your application's repository. We can be a bit more helpful, there are ways to extend the Jenkins X pipeline locally, not by writing your own, but by the special override syntax in your jenkins-x.yaml file. This would look like this: pipelineConfig: pipelines: overrides: Go to the Jenkins X Pipelines page for further details.","title":"Extend Locally"},{"location":"jenkinsx/build-packs/#extend-globally","text":"Extending the Build Pack globally has a very limited scope, it is only for the Jenkins X pipeline. However, as the pipeline is one of the - if not the - most important parts of Jenkins X. So I'd argue that it is very powerful despite its limited scope. So what we're going to do is the following: create a new repository set up the required structure in the repository create a pipeline extension configure an existing Jenkins X application to leverage our Build Pack Info Why would you want to globally extend the Build Packs? Because this allows you to store your extensions to the Jenkins X default pipelines in a way every application can reuse them, or even extend those. It allows you to define standard steps that every pipeline in your organization needs to execute, once and only once.","title":"Extend Globally"},{"location":"jenkinsx/build-packs/#create-build-pack-repository","text":"The minimal amount we need, is a folder called packs , inside which we need a few more things. an imports.yaml importing any and all Build Pack repositories we want to use for example, the default kubernetes and classic packs from the Jenkins X Authors themselves a folder, the name of your Build Pack, containing a pipeline.yaml The structure will then look like this. . \u251c\u2500\u2500 README.md \u2514\u2500\u2500 packs \u251c\u2500\u2500 imports.yaml \u2514\u2500\u2500 maven-joost \u2514\u2500\u2500 pipeline.yaml packs/imports.yaml Here we import the default Build Pack repositories, so our new pipelines can extends them using the Pipeline Extensions syntax. modules : - name : classic gitUrl : https://github.com/jenkins-x-buildpacks/jenkins-x-classic.git gitRef : master - name : kubernetes gitUrl : https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes.git gitRef : master","title":"Create Build Pack Repository"},{"location":"jenkinsx/build-packs/#create-pipeline-extension","text":"In order to extend the existing pipelines coming from other Build Packs, we have to set the extends configuration in the Build Pack's pipeline.yaml . This file would reside in packs/<name-of-your-pack>/pipeline.yaml . In this case, we want to extend the kubernetes Build Pack repository's maven-java11 syntax. We do this by filling import field with the name field from the repository listed in the packs/imports.yaml file. We then select a Build Pack's pipeline file by pointing to a pipeline.yaml file from the relative path of packs/ . Lets say we want to use a different docker container as build agent, we would end up with this. packs/maven-joost/pipeline.yaml extends : import : kubernetes file : maven-java11/pipeline.yaml agent : label : jenkins-maven-java11 image : maven-java11 container : maven","title":"Create Pipeline Extension"},{"location":"jenkinsx/build-packs/#extending-the-pipeline-further","text":"If you want to further extend the pipeline, you can leverage the Jenkins X Pipeline syntax. For example, say you want to make sure your Pull Request builds run a SonarQube scan. You can add the step sonar-scan-pr to the pullRequest Pipeline, under the stage build as below. packs/maven-joost/pipeline.yaml pipelines : pullRequest : build : steps : - name : sonar-scan-pr command : sonar-scanner image : newtmitch/sonar-scanner:3.0 dir : /workspace/source/ args : - -Dsonar.projectName=... - -Dsonar.projectKey=... - -Dsonar.organization=... - -Dsonar.sources=./src/main/java/ - -Dsonar.language=java - -Dsonar.java.binaries=./target/classes - -Dsonar.host.url=https://sonarcloud.io - -Dsonar.login=${SONARCLOUD_TOKEN} Go to the Jenkins X Pipelines page for further details.","title":"Extending The Pipeline Further"},{"location":"jenkinsx/build-packs/#use-new-build-pack","text":"Once you have defined the pipeline in your Build Pack, you then specify the Build Pack you want to use in your application in the jenkins-x.yaml file. Important This section refers to your application's, not the Build Back's repository. You need to specify three parameters in order for Jenkins X to pick up your Build Pack and build up the effective pipeline from your Build Pack hierarchy. buildPack : the name of your Build Pack, e.g. the folder name in your Build Pack repository's packs folder that contains the pipeline.yaml buildPackGitRef : the Git ref, e.g. tag, commit or branch name buildPackGitURL : the http(s) git URL jenkins-x.yaml buildPack: maven-joost buildPackGitRef: master buildPackGitURL: https://github.com/joostvdg/jx-buildpacks.git","title":"Use New Build Pack"},{"location":"jenkinsx/buildpack/","text":"Build Packs \u00b6 There are multiple ways to create your own buildpack for Jenkins X. Start from a working example : either create a quickstart project or import your existing application. Make the build and promotions work and then create a new buildpack by making the same changes (parameterized where applicable) to a copy of the buildpack you started from. Start from a working example \u00b6 We're going to build a buildpack for the following application: Micronaut framework build with Gradle with a Redis datastore with a TLS certificate for the ingress (https) Create Micronaut application \u00b6 create application via Micronaut CLI add a controller enable default healthendpoint import application with Jenkins X update helm chart: change healtcheck endpoint update helm chart: add dependency on Redis update values: set redis to not use a password mn create-app example.micronaut.complete --features = kotlin,spek,tracing-jaeger,redis-lettuce jx import Secrets \u00b6 helm repo add soluto https://charts.soluto.io helm repo update helm upgrade --install kamus soluto/kamus","title":"Jenkins X - BuildPack"},{"location":"jenkinsx/buildpack/#build-packs","text":"There are multiple ways to create your own buildpack for Jenkins X. Start from a working example : either create a quickstart project or import your existing application. Make the build and promotions work and then create a new buildpack by making the same changes (parameterized where applicable) to a copy of the buildpack you started from.","title":"Build Packs"},{"location":"jenkinsx/buildpack/#start-from-a-working-example","text":"We're going to build a buildpack for the following application: Micronaut framework build with Gradle with a Redis datastore with a TLS certificate for the ingress (https)","title":"Start from a working example"},{"location":"jenkinsx/buildpack/#create-micronaut-application","text":"create application via Micronaut CLI add a controller enable default healthendpoint import application with Jenkins X update helm chart: change healtcheck endpoint update helm chart: add dependency on Redis update values: set redis to not use a password mn create-app example.micronaut.complete --features = kotlin,spek,tracing-jaeger,redis-lettuce jx import","title":"Create Micronaut application"},{"location":"jenkinsx/buildpack/#secrets","text":"helm repo add soluto https://charts.soluto.io helm repo update helm upgrade --install kamus soluto/kamus","title":"Secrets"},{"location":"jenkinsx/custom-domain/","text":"Custom Domain \u00b6 At Creation Time \u00b6 After Creation \u00b6 Changing Domain \u00b6","title":"Custom Domain"},{"location":"jenkinsx/custom-domain/#custom-domain","text":"","title":"Custom Domain"},{"location":"jenkinsx/custom-domain/#at-creation-time","text":"","title":"At Creation Time"},{"location":"jenkinsx/custom-domain/#after-creation","text":"","title":"After Creation"},{"location":"jenkinsx/custom-domain/#changing-domain","text":"","title":"Changing Domain"},{"location":"jenkinsx/hello-world/","text":"Hello World Demo \u00b6 Create GKE + Jenkins X cluster \u00b6 export JX_CLUSTER_NAME = joostvdg export JX_ENV_PREFIX = joostvdg export JX_ADMIN_PSS = vXDzpiaVAthneXJR355J7PBT export JX_DOMAIN = jx.kearos.net export JX_GIT_USER = joostvdg export JX_API_TOKEN = 61edcbf6507d31b3f2fe811baa82aa6de33db001 export JX_ORG = demomon export JX_GCE_PROJECT = ps-dev-201405 export JX_K8S_REGION = europe-west4 export JX_K8S_ZONE = europe-west4-a export GKE_NODE_LOCATIONS = ${ REGION } -a, ${ REGION } -b export JX_K8S_VERSION = Get supported K8S versions \u00b6 Zonal gcloud container get-server-config --zone ${ JX_K8S_ZONE } Regional gcloud container get-server-config --region ${ JX_K8S_REGION } export JX_K8S_VERSION = 1 .11.7-gke.4 Create regional cluster w/ Domain \u00b6 Currently only possible if you create a regional cluster first and then install jx. gcloud container clusters create ${ JX_CLUSTER_NAME } \\ --region ${ JX_K8S_REGION } --node-locations ${ GKE_NODE_LOCATIONS } \\ --cluster-version ${ JX_K8S_VERSION } \\ --enable-pod-security-policy \\ --enable-network-policy \\ --num-nodes 2 --machine-type n1-standard-2 \\ --min-nodes 2 --max-nodes 3 \\ --enable-autoupgrade \\ --enable-autoscaling \\ --labels = owner = jvandergriendt,purpose = practice jx install Create zonal cluster w/ Domain \u00b6 jx create cluster gke \\ --cluster-name = \" ${ JX_CLUSTER_NAME } \" \\ --default-admin-password = \" ${ JX_ADMIN_PSS } \" \\ --domain = \" ${ JX_DOMAIN } \" \\ --kubernetes-version = \" ${ JX_K8S_VERSION } \" \\ --machine-type = 'n1-standard-2' \\ --max-num-nodes = 3 \\ --min-num-nodes = 2 \\ --project-id = ${ JX_GCE_PROJECT } \\ --zone = \" ${ JX_K8S_ZONE } \" \\ --kaniko = true \\ --skip-login Reinstall \u00b6 jx install \\ --default-environment-prefix = $JX_ENV_PREFIX \\ --git-api-token = $JX_API_TOKEN \\ --git-username = $JX_GIT_USER \\ --environment-git-owner = $JX_GIT_USER \\ --default-admin-password = \" ${ JX_ADMIN_PSS } \" \\ --domain = \" ${ JX_DOMAIN } \" Configure Domain \u00b6 Once the cluster is up and the Jenkins X basics are installed, jx will prompt us about missing an ingress controller. ? No existing ingress controller found in the kube-system namespace, shall we install one? Yes Reply yes, and in a little while, you will see the following message: You can now configure your wildcard DNS jx.kearos.net to point to 35 .204.0.182 nginx ingress controller installed and configured We can now go to our Domain configuration and set *.jx.${DomainName} to the ip listed. If you're using Google Domains by any chance, you create an A class record for *.jx with ip 35.204.0.182 . Unfortunately, that's not enough, as the ingress resources created by jx after will have a different IP address. So we have to add a second IP address to your Class A record. Still assuming GKE, you can retrieve the second IP address as follows: INGRESS_IP = $( kubectl get ing chartmuseum -o jsonpath = \"{.status.loadBalancer.ingress[0].ip}\" ) echo $INGRESS_IP To test the domain, you can do the following: CM_URL = $( kubectl get ing chartmuseum -o jsonpath = \"{.spec.rules[0].host}\" ) Curl curl $CM_URL Httpie http $CM_URL Configure TLS \u00b6 If we have a proper domain configured and working, we can also enable TLS. As we've not done so at the start, we will have to update the Ingress configuration. For all the options for updating the Ingress configuration, use the command below. jx upgrade ingress --help To configure TLS for our ingresses, we need TLS certificates. Jenkins X does this via Let's Encrypt, which in Kubernetes is easily done via Certmanager . The command we will issue will ask us if we want to install CertManager , and then delete all existing ingress resources and recreate them with the certificate. Unfortunately, when in Batch mode ( -b ) it does not install CertManager nor is there an option to force it in this case. When asked if we want to delete and recreate the existing ingress rules, say yes ( y ). Select the expose type , which should be Ingress (route is for OpenShift). Confirm your domain -> do not change it, as this upgrade does NOT change your domain configuration everywhere and you will end up with a broken system Say yes to cluster wide TLS If you're certain your Domain works, select the production LetEncrypt configuration, else choose staging for tests Confirm your email address and the summary Agree with installing CertManager Do Not agree with updating the webhooks (see below) jx upgrade ingress --cluster --verbose Warning There's currently a bug with changing the webhooks via this command; see issue #3115 It somehow can only select a different GitHub user than the current one, which makes no sense for an UPDATE. So we must update the webhooks manually ourselves! Manually update webhooks \u00b6 Due to issue #3115 we need to manually update our webhooks for the environment repositories. If you're not sure where your environment repositories are, you can retrieve them with the command below: js get env Open each environment repository, go to the settings tabs (top right), open the webhooks menu (on the left), and edit the webhook. Simply change the http:// to https:// and save. Warning If you've selected the staging configuration for Let's Encrypt, you have set the SSL configuration to Disable (not recommended) . Create options \u00b6 --buildpack='': The name of the build pack to use for the Team --vault --helm3=false: Use helm3 to install Jenkins X which does not use Tiller --kaniko=false --urltemplate='': For ingress; exposers can set the urltemplate to expose Addons \u00b6 create addon ambassador Create an ambassador addon create addon anchore Create the Anchore addon for verifying container images create addon cloudbees Create the CloudBees app for Kubernetes ( a web console for working with CI/CD, Environments and GitOps ) create addon flagger Create the Flagger addon for Canary deployments create addon gitea Create a Gitea addon for hosting Git repositories create addon istio Create the Istio addon for service mesh create addon knative-build Create the knative build addon create addon kubeless Create a kubeless addon for hosting Git repositories create addon owasp-zap Create the OWASP Zed Attack Proxy addon for dynamic security checks against running apps create addon pipeline-events Create the pipeline events addon create addon prometheus Creates a prometheus addon create addon prow Create a Prow addon create addon sso Create a SSO addon for Single Sign-On create addon vault-operator Create an vault-operator addon for Hashicorp Vault Upgrade \u00b6 jx upgrade --help upgrade addons Upgrades any Addons added to Jenkins X if there are any new releases available upgrade apps Upgrades any Apps to the latest release upgrade binaries Upgrades the command line binaries ( like helm or eksctl ) - if there are new versions available upgrade cli Upgrades the command line applications - if there are new versions available upgrade cluster Upgrades the Kubernetes master to the specified version upgrade extensions Upgrades the Jenkins X extensions available to this Jenkins X install if there are new versions available upgrade ingress Upgrades Ingress rules upgrade platform Upgrades the Jenkins X platform if there is a new release available Go lang example \u00b6 jx create quickstartjc select golang-http Promote \u00b6 APP = jx-go-demo-5 VERSION = 0 .0.2 jx promote ${ APP } --version $VERSION --env production -b jx get apps Spring Boot Example \u00b6 jx create spring -d web -d actuator Serverless \u00b6 jx create terraform gke \\ --vault = 'true' \\ --cluster = \" ${ JX_CLUSTER_NAME } \" = gke \\ --gke-project-id = ${ JX_GCE_PROJECT } \\ --prow \\ --skip-login Demo - Show JX Stuff \u00b6 GitOps \u00b6 Get environments: jx get environments Watch pipeline activity via: jx get activity -f golang-http -w Browse the pipeline log via: jx get build logs demomon/golang-http/master Open the Jenkins console via jx console You can list the pipelines via: jx get pipelines When the pipeline is complete: jx get applications Build It \u00b6 explain build packs jx create quickstart show Jenkinsfile open application jx get applications create new branch git checkout -b wip change main.go commit git -a -m \"better message\" push git push remote wip open PR page explain tide add cat picture: /meow test it add comment /test this open logs jx logs -k open PR environment approve the change /approve or add approved label open logs for next step promote: jx promote myapp --version 1.2.3 --env production promote: jx promote ${APP} --version ${VERSION} --env production","title":"HelloWorld"},{"location":"jenkinsx/hello-world/#hello-world-demo","text":"","title":"Hello World Demo"},{"location":"jenkinsx/hello-world/#create-gke-jenkins-x-cluster","text":"export JX_CLUSTER_NAME = joostvdg export JX_ENV_PREFIX = joostvdg export JX_ADMIN_PSS = vXDzpiaVAthneXJR355J7PBT export JX_DOMAIN = jx.kearos.net export JX_GIT_USER = joostvdg export JX_API_TOKEN = 61edcbf6507d31b3f2fe811baa82aa6de33db001 export JX_ORG = demomon export JX_GCE_PROJECT = ps-dev-201405 export JX_K8S_REGION = europe-west4 export JX_K8S_ZONE = europe-west4-a export GKE_NODE_LOCATIONS = ${ REGION } -a, ${ REGION } -b export JX_K8S_VERSION =","title":"Create GKE + Jenkins X cluster"},{"location":"jenkinsx/hello-world/#get-supported-k8s-versions","text":"Zonal gcloud container get-server-config --zone ${ JX_K8S_ZONE } Regional gcloud container get-server-config --region ${ JX_K8S_REGION } export JX_K8S_VERSION = 1 .11.7-gke.4","title":"Get supported K8S versions"},{"location":"jenkinsx/hello-world/#create-regional-cluster-w-domain","text":"Currently only possible if you create a regional cluster first and then install jx. gcloud container clusters create ${ JX_CLUSTER_NAME } \\ --region ${ JX_K8S_REGION } --node-locations ${ GKE_NODE_LOCATIONS } \\ --cluster-version ${ JX_K8S_VERSION } \\ --enable-pod-security-policy \\ --enable-network-policy \\ --num-nodes 2 --machine-type n1-standard-2 \\ --min-nodes 2 --max-nodes 3 \\ --enable-autoupgrade \\ --enable-autoscaling \\ --labels = owner = jvandergriendt,purpose = practice jx install","title":"Create regional cluster w/ Domain"},{"location":"jenkinsx/hello-world/#create-zonal-cluster-w-domain","text":"jx create cluster gke \\ --cluster-name = \" ${ JX_CLUSTER_NAME } \" \\ --default-admin-password = \" ${ JX_ADMIN_PSS } \" \\ --domain = \" ${ JX_DOMAIN } \" \\ --kubernetes-version = \" ${ JX_K8S_VERSION } \" \\ --machine-type = 'n1-standard-2' \\ --max-num-nodes = 3 \\ --min-num-nodes = 2 \\ --project-id = ${ JX_GCE_PROJECT } \\ --zone = \" ${ JX_K8S_ZONE } \" \\ --kaniko = true \\ --skip-login","title":"Create zonal cluster w/ Domain"},{"location":"jenkinsx/hello-world/#reinstall","text":"jx install \\ --default-environment-prefix = $JX_ENV_PREFIX \\ --git-api-token = $JX_API_TOKEN \\ --git-username = $JX_GIT_USER \\ --environment-git-owner = $JX_GIT_USER \\ --default-admin-password = \" ${ JX_ADMIN_PSS } \" \\ --domain = \" ${ JX_DOMAIN } \"","title":"Reinstall"},{"location":"jenkinsx/hello-world/#configure-domain","text":"Once the cluster is up and the Jenkins X basics are installed, jx will prompt us about missing an ingress controller. ? No existing ingress controller found in the kube-system namespace, shall we install one? Yes Reply yes, and in a little while, you will see the following message: You can now configure your wildcard DNS jx.kearos.net to point to 35 .204.0.182 nginx ingress controller installed and configured We can now go to our Domain configuration and set *.jx.${DomainName} to the ip listed. If you're using Google Domains by any chance, you create an A class record for *.jx with ip 35.204.0.182 . Unfortunately, that's not enough, as the ingress resources created by jx after will have a different IP address. So we have to add a second IP address to your Class A record. Still assuming GKE, you can retrieve the second IP address as follows: INGRESS_IP = $( kubectl get ing chartmuseum -o jsonpath = \"{.status.loadBalancer.ingress[0].ip}\" ) echo $INGRESS_IP To test the domain, you can do the following: CM_URL = $( kubectl get ing chartmuseum -o jsonpath = \"{.spec.rules[0].host}\" ) Curl curl $CM_URL Httpie http $CM_URL","title":"Configure Domain"},{"location":"jenkinsx/hello-world/#configure-tls","text":"If we have a proper domain configured and working, we can also enable TLS. As we've not done so at the start, we will have to update the Ingress configuration. For all the options for updating the Ingress configuration, use the command below. jx upgrade ingress --help To configure TLS for our ingresses, we need TLS certificates. Jenkins X does this via Let's Encrypt, which in Kubernetes is easily done via Certmanager . The command we will issue will ask us if we want to install CertManager , and then delete all existing ingress resources and recreate them with the certificate. Unfortunately, when in Batch mode ( -b ) it does not install CertManager nor is there an option to force it in this case. When asked if we want to delete and recreate the existing ingress rules, say yes ( y ). Select the expose type , which should be Ingress (route is for OpenShift). Confirm your domain -> do not change it, as this upgrade does NOT change your domain configuration everywhere and you will end up with a broken system Say yes to cluster wide TLS If you're certain your Domain works, select the production LetEncrypt configuration, else choose staging for tests Confirm your email address and the summary Agree with installing CertManager Do Not agree with updating the webhooks (see below) jx upgrade ingress --cluster --verbose Warning There's currently a bug with changing the webhooks via this command; see issue #3115 It somehow can only select a different GitHub user than the current one, which makes no sense for an UPDATE. So we must update the webhooks manually ourselves!","title":"Configure TLS"},{"location":"jenkinsx/hello-world/#manually-update-webhooks","text":"Due to issue #3115 we need to manually update our webhooks for the environment repositories. If you're not sure where your environment repositories are, you can retrieve them with the command below: js get env Open each environment repository, go to the settings tabs (top right), open the webhooks menu (on the left), and edit the webhook. Simply change the http:// to https:// and save. Warning If you've selected the staging configuration for Let's Encrypt, you have set the SSL configuration to Disable (not recommended) .","title":"Manually update webhooks"},{"location":"jenkinsx/hello-world/#create-options","text":"--buildpack='': The name of the build pack to use for the Team --vault --helm3=false: Use helm3 to install Jenkins X which does not use Tiller --kaniko=false --urltemplate='': For ingress; exposers can set the urltemplate to expose","title":"Create options"},{"location":"jenkinsx/hello-world/#addons","text":"create addon ambassador Create an ambassador addon create addon anchore Create the Anchore addon for verifying container images create addon cloudbees Create the CloudBees app for Kubernetes ( a web console for working with CI/CD, Environments and GitOps ) create addon flagger Create the Flagger addon for Canary deployments create addon gitea Create a Gitea addon for hosting Git repositories create addon istio Create the Istio addon for service mesh create addon knative-build Create the knative build addon create addon kubeless Create a kubeless addon for hosting Git repositories create addon owasp-zap Create the OWASP Zed Attack Proxy addon for dynamic security checks against running apps create addon pipeline-events Create the pipeline events addon create addon prometheus Creates a prometheus addon create addon prow Create a Prow addon create addon sso Create a SSO addon for Single Sign-On create addon vault-operator Create an vault-operator addon for Hashicorp Vault","title":"Addons"},{"location":"jenkinsx/hello-world/#upgrade","text":"jx upgrade --help upgrade addons Upgrades any Addons added to Jenkins X if there are any new releases available upgrade apps Upgrades any Apps to the latest release upgrade binaries Upgrades the command line binaries ( like helm or eksctl ) - if there are new versions available upgrade cli Upgrades the command line applications - if there are new versions available upgrade cluster Upgrades the Kubernetes master to the specified version upgrade extensions Upgrades the Jenkins X extensions available to this Jenkins X install if there are new versions available upgrade ingress Upgrades Ingress rules upgrade platform Upgrades the Jenkins X platform if there is a new release available","title":"Upgrade"},{"location":"jenkinsx/hello-world/#go-lang-example","text":"jx create quickstartjc select golang-http","title":"Go lang example"},{"location":"jenkinsx/hello-world/#promote","text":"APP = jx-go-demo-5 VERSION = 0 .0.2 jx promote ${ APP } --version $VERSION --env production -b jx get apps","title":"Promote"},{"location":"jenkinsx/hello-world/#spring-boot-example","text":"jx create spring -d web -d actuator","title":"Spring Boot Example"},{"location":"jenkinsx/hello-world/#serverless","text":"jx create terraform gke \\ --vault = 'true' \\ --cluster = \" ${ JX_CLUSTER_NAME } \" = gke \\ --gke-project-id = ${ JX_GCE_PROJECT } \\ --prow \\ --skip-login","title":"Serverless"},{"location":"jenkinsx/hello-world/#demo-show-jx-stuff","text":"","title":"Demo - Show JX Stuff"},{"location":"jenkinsx/hello-world/#gitops","text":"Get environments: jx get environments Watch pipeline activity via: jx get activity -f golang-http -w Browse the pipeline log via: jx get build logs demomon/golang-http/master Open the Jenkins console via jx console You can list the pipelines via: jx get pipelines When the pipeline is complete: jx get applications","title":"GitOps"},{"location":"jenkinsx/hello-world/#build-it","text":"explain build packs jx create quickstart show Jenkinsfile open application jx get applications create new branch git checkout -b wip change main.go commit git -a -m \"better message\" push git push remote wip open PR page explain tide add cat picture: /meow test it add comment /test this open logs jx logs -k open PR environment approve the change /approve or add approved label open logs for next step promote: jx promote myapp --version 1.2.3 --env production promote: jx promote ${APP} --version ${VERSION} --env production","title":"Build It"},{"location":"jenkinsx/hybrid/","text":"Jenkins X Hybrid TLS \u00b6 Jenkins X Hybrid TLS is a configuration of Jenkins X using both Static Jenkins and Jenkins X Serverless with Tekton within the same cluster. As the TLS suffix hints at, it also uses TLS for both installations to make sure all the services and your applications are accessible via https with a valid certificate. Pre-requisites \u00b6 GCP account with active subscription with an active project with which you are authenticated gcloud CLI Jenkins X CLI jx httpie or curl Steps \u00b6 create JX cluster in GKE with static Jenkins without Nexus create Go (lang) quickstart configure TLS install Serverless Jenkins X in the same cluster create Spring Boot Quickstart configure TLS for Serverless namespaces only re-install Jenkins X with Nexus Static \u00b6 Prepare \u00b6 Variables \u00b6 CLUSTER_NAME = #name of your cluster PROJECT = #name of your GCP project REGION = #GCP region to install cluster in GITHUB_USER = #your GitHub Username GITHUB_TOKEN = #GitHub apitoken myvalues.yaml \u00b6 We're going to use a demo application based on Go, so we don't need Nexus. To configure Jenkins X to skip Nexus' installation, create the file myvalues.yaml with the following contents: nexus : enabled : false docker-registry : enabled : true Install JX \u00b6 Make sure you execute this command where you have the myvalues.yaml file. jx create cluster gke \\ --cluster-name ${ CLUSTER_NAME } \\ --project-id ${ PROJECT } \\ --region ${ REGION } \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --git-username ${ GITHUB_USER } \\ --git-provider-kind github \\ --git-api-token ${ GITHUB_TOKEN } \\ --batch-mode Go Quickstart \u00b6 jx create quickstart \\ -l go --org ${ GITHUB_USER } \\ --project-name jx-static-go \\ --import-mode = Jenkinsfile \\ --deploy-kind default \\ -b Watch activity \u00b6 You can either go to Jenkins and watch the job there: jx console or watch in your console via jx get activity . jx get activity -f jx-static-go -w Once the build completes, you should see something like the line below, you can test the application. Promoted 28m5s 1m41s Succeeded Application is at: http://jx-static-go.jx-staging.34.90.105.15.nip.io Test application \u00b6 To confirm the application is running in the staging environment: jx get applications Which should show something like this: APPLICATION STAGING PODS URL jx-static-go 0 .0.1 1 /1 http://jx-static-go.jx-staging. ${ LIB_IP } .nip.io LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = \"{.status.loadBalancer.ingress[0].ip}\" ) http jx-static-go.jx-staging. ${ LB_IP } .nip.io Which should show the following: HTTP/1.1 200 OK Connection: keep-alive Content-Length: 43 Content-Type: text/plain ; charset = utf-8 Date: Thu, 13 Jun 2019 12 :17:39 GMT Server: nginx/1.15.8 Hello from: Jenkins X golang http example Configure TLS \u00b6 Make sure you have two things: the address of your LoadBalancer (see below how to retrieve this) a Domain name with a quick and easy DNS configuration (incl. wildcard support) Retrieve LoadBalancer address \u00b6 LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = \"{.status.loadBalancer.ingress[0].ip}\" ) Configure DNS \u00b6 Go to your Domain provider of choice, if you don't have one, consider Google Domains for 12 Euro per year. They might no be the cheapest, but the service is great and works quick - changes like we're about to do, take a few minutes to be effectuated. Configure the following wildcards to direct to your LoadBalancer's IP address: *.jx - type A *.jx-staging - type A *.jx-production - type A *.serverless - type A (for the serverless section) Upgrade Ingress \u00b6 To configure TLS inside Jenkins X, we make use of Let's Encrypt and cert-manager . To get Jenkins X to configure TLS, we use the jx upgrade ingress command. DOMAIN = #your domain name jx upgrade ingress \\ --cluster true \\ --domain $DOMAIN Info To be sure, the Domain name above should the base hostname only. Any resource within your JX installation will automatically get the following domain name: {name}.{namespace}.{DOMAIN} . For example, if your domain is example.com Jenkins will become jenkins.jx.example.com . Test applications \u00b6 Confirm your application now has a https protocol. jx get applications http https://jx-static-go.jx-staging. ${ DOMAIN } Serverless \u00b6 Prepare \u00b6 The values for INGRESS_NS and INGRESS_DEP are the default based on the static install created above. If your ingress controller namespace and/or deployment have different names, replace the values. For the LB_IP , we're also assuming default names and namespaces. PROVIDER = gke LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = \"{.status.loadBalancer.ingress[0].ip}\" ) DOMAIN_SUFFIX = #your domain name DOMAIN = serverless. ${ DOMAIN_SUFFIX } INGRESS_NS = kube-system INGRESS_DEP = jxing-nginx-ingress-controller INSTALL_NS = cdx PROJECT = #your GCP project Info We're going to use the cdx namespace, this will create namespaces such as cdx and cdx-staging . In order to avoid having to register every environment in at our DNS provider, we will use an additional domain prefix serverless . Making the domain serverless.{DOMAIN} and the JX components {name}.cdx.serverless.{DOMAIN} . Install Serverless JX \u00b6 jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b Spring Boot Quickstart \u00b6 Create quickstart \u00b6 jx create spring -d web -d actuator \\ --group com.example \\ --artifact jx-spring-boot-demo \\ -b cd jx-spring-boot-demo Add controller \u00b6 Assuming you kept the group the same, you should find a folder src/main/java/com/example/jxspringbootdemo containing a file, DemoApplication.java . We're going to have to add two files to the same folder: Greeting.java GreetingController.java Greeting \u00b6 package com.example.jxspringbootdemo ; public class Greeting { private final long id ; private final String content ; public Greeting ( long id , String content ) { this . id = id ; this . content = content ; } public long getId () { return id ; } public String getContent () { return content ; } } GreetingController \u00b6 package com.example.jxspringbootdemo ; import java.util.concurrent.atomic.AtomicLong ; import org.springframework.web.bind.annotation.RequestMapping ; import org.springframework.web.bind.annotation.RequestParam ; import org.springframework.web.bind.annotation.RestController ; @RestController public class GreetingController { private static final String template = \"Hello, %s!\" ; private final AtomicLong counter = new AtomicLong (); @RequestMapping ( \"/greeting\" ) public Greeting greeting ( @RequestParam ( value = \"name\" , defaultValue = \"World\" ) String name ) { return new Greeting ( counter . incrementAndGet (), String . format ( template , name )); } } Test application \u00b6 jx get activity -f jx-cdx-spring-boot-demo-1 -w Re-Install with Nexus \u00b6 myvalues.yaml \u00b6 Our application didn't work because now we have an application that depends on a Maven repository. We have to \"re-install\" Jenkins X, to have it install Nexus for us in the cdx namespace. nexus : enabled : true docker-registry : enabled : true Install \u00b6 Make sure you execute this command where you have the myvalues.yaml file. jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain serverless. $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b Test Application \u00b6 To trigger a new build, make a change - for example to the README.md and push it. jx get activity -f jx-cdx-spring-boot-demo-1 -w http jx-cdx-spring-boot-demo-1.cdx-staging.serverless. ${ DOMAIN } /greeting Configure TLS \u00b6 jx upgrade ingress --domain $DOMAIN --namespaces cdx,cdx-staging Re-test application \u00b6 ORG = #the GitHub user or organisation your application is in jx update webhooks --repo = jx-cdx-spring-boot-demo-1 --org = ${ ORG } jx get applications jx get activity -f jx-cdx-spring-boot-demo-1 -w http https://jx-cdx-spring-boot-demo-1.cdx-staging.serverless. ${ DOMAIN } /greeting","title":"Hybrid"},{"location":"jenkinsx/hybrid/#jenkins-x-hybrid-tls","text":"Jenkins X Hybrid TLS is a configuration of Jenkins X using both Static Jenkins and Jenkins X Serverless with Tekton within the same cluster. As the TLS suffix hints at, it also uses TLS for both installations to make sure all the services and your applications are accessible via https with a valid certificate.","title":"Jenkins X Hybrid TLS"},{"location":"jenkinsx/hybrid/#pre-requisites","text":"GCP account with active subscription with an active project with which you are authenticated gcloud CLI Jenkins X CLI jx httpie or curl","title":"Pre-requisites"},{"location":"jenkinsx/hybrid/#steps","text":"create JX cluster in GKE with static Jenkins without Nexus create Go (lang) quickstart configure TLS install Serverless Jenkins X in the same cluster create Spring Boot Quickstart configure TLS for Serverless namespaces only re-install Jenkins X with Nexus","title":"Steps"},{"location":"jenkinsx/hybrid/#static","text":"","title":"Static"},{"location":"jenkinsx/hybrid/#prepare","text":"","title":"Prepare"},{"location":"jenkinsx/hybrid/#variables","text":"CLUSTER_NAME = #name of your cluster PROJECT = #name of your GCP project REGION = #GCP region to install cluster in GITHUB_USER = #your GitHub Username GITHUB_TOKEN = #GitHub apitoken","title":"Variables"},{"location":"jenkinsx/hybrid/#myvaluesyaml","text":"We're going to use a demo application based on Go, so we don't need Nexus. To configure Jenkins X to skip Nexus' installation, create the file myvalues.yaml with the following contents: nexus : enabled : false docker-registry : enabled : true","title":"myvalues.yaml"},{"location":"jenkinsx/hybrid/#install-jx","text":"Make sure you execute this command where you have the myvalues.yaml file. jx create cluster gke \\ --cluster-name ${ CLUSTER_NAME } \\ --project-id ${ PROJECT } \\ --region ${ REGION } \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --git-username ${ GITHUB_USER } \\ --git-provider-kind github \\ --git-api-token ${ GITHUB_TOKEN } \\ --batch-mode","title":"Install JX"},{"location":"jenkinsx/hybrid/#go-quickstart","text":"jx create quickstart \\ -l go --org ${ GITHUB_USER } \\ --project-name jx-static-go \\ --import-mode = Jenkinsfile \\ --deploy-kind default \\ -b","title":"Go Quickstart"},{"location":"jenkinsx/hybrid/#watch-activity","text":"You can either go to Jenkins and watch the job there: jx console or watch in your console via jx get activity . jx get activity -f jx-static-go -w Once the build completes, you should see something like the line below, you can test the application. Promoted 28m5s 1m41s Succeeded Application is at: http://jx-static-go.jx-staging.34.90.105.15.nip.io","title":"Watch activity"},{"location":"jenkinsx/hybrid/#test-application","text":"To confirm the application is running in the staging environment: jx get applications Which should show something like this: APPLICATION STAGING PODS URL jx-static-go 0 .0.1 1 /1 http://jx-static-go.jx-staging. ${ LIB_IP } .nip.io LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = \"{.status.loadBalancer.ingress[0].ip}\" ) http jx-static-go.jx-staging. ${ LB_IP } .nip.io Which should show the following: HTTP/1.1 200 OK Connection: keep-alive Content-Length: 43 Content-Type: text/plain ; charset = utf-8 Date: Thu, 13 Jun 2019 12 :17:39 GMT Server: nginx/1.15.8 Hello from: Jenkins X golang http example","title":"Test application"},{"location":"jenkinsx/hybrid/#configure-tls","text":"Make sure you have two things: the address of your LoadBalancer (see below how to retrieve this) a Domain name with a quick and easy DNS configuration (incl. wildcard support)","title":"Configure TLS"},{"location":"jenkinsx/hybrid/#retrieve-loadbalancer-address","text":"LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = \"{.status.loadBalancer.ingress[0].ip}\" )","title":"Retrieve LoadBalancer address"},{"location":"jenkinsx/hybrid/#configure-dns","text":"Go to your Domain provider of choice, if you don't have one, consider Google Domains for 12 Euro per year. They might no be the cheapest, but the service is great and works quick - changes like we're about to do, take a few minutes to be effectuated. Configure the following wildcards to direct to your LoadBalancer's IP address: *.jx - type A *.jx-staging - type A *.jx-production - type A *.serverless - type A (for the serverless section)","title":"Configure DNS"},{"location":"jenkinsx/hybrid/#upgrade-ingress","text":"To configure TLS inside Jenkins X, we make use of Let's Encrypt and cert-manager . To get Jenkins X to configure TLS, we use the jx upgrade ingress command. DOMAIN = #your domain name jx upgrade ingress \\ --cluster true \\ --domain $DOMAIN Info To be sure, the Domain name above should the base hostname only. Any resource within your JX installation will automatically get the following domain name: {name}.{namespace}.{DOMAIN} . For example, if your domain is example.com Jenkins will become jenkins.jx.example.com .","title":"Upgrade Ingress"},{"location":"jenkinsx/hybrid/#test-applications","text":"Confirm your application now has a https protocol. jx get applications http https://jx-static-go.jx-staging. ${ DOMAIN }","title":"Test applications"},{"location":"jenkinsx/hybrid/#serverless","text":"","title":"Serverless"},{"location":"jenkinsx/hybrid/#prepare_1","text":"The values for INGRESS_NS and INGRESS_DEP are the default based on the static install created above. If your ingress controller namespace and/or deployment have different names, replace the values. For the LB_IP , we're also assuming default names and namespaces. PROVIDER = gke LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = \"{.status.loadBalancer.ingress[0].ip}\" ) DOMAIN_SUFFIX = #your domain name DOMAIN = serverless. ${ DOMAIN_SUFFIX } INGRESS_NS = kube-system INGRESS_DEP = jxing-nginx-ingress-controller INSTALL_NS = cdx PROJECT = #your GCP project Info We're going to use the cdx namespace, this will create namespaces such as cdx and cdx-staging . In order to avoid having to register every environment in at our DNS provider, we will use an additional domain prefix serverless . Making the domain serverless.{DOMAIN} and the JX components {name}.cdx.serverless.{DOMAIN} .","title":"Prepare"},{"location":"jenkinsx/hybrid/#install-serverless-jx","text":"jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b","title":"Install Serverless JX"},{"location":"jenkinsx/hybrid/#spring-boot-quickstart","text":"","title":"Spring Boot Quickstart"},{"location":"jenkinsx/hybrid/#create-quickstart","text":"jx create spring -d web -d actuator \\ --group com.example \\ --artifact jx-spring-boot-demo \\ -b cd jx-spring-boot-demo","title":"Create quickstart"},{"location":"jenkinsx/hybrid/#add-controller","text":"Assuming you kept the group the same, you should find a folder src/main/java/com/example/jxspringbootdemo containing a file, DemoApplication.java . We're going to have to add two files to the same folder: Greeting.java GreetingController.java","title":"Add controller"},{"location":"jenkinsx/hybrid/#greeting","text":"package com.example.jxspringbootdemo ; public class Greeting { private final long id ; private final String content ; public Greeting ( long id , String content ) { this . id = id ; this . content = content ; } public long getId () { return id ; } public String getContent () { return content ; } }","title":"Greeting"},{"location":"jenkinsx/hybrid/#greetingcontroller","text":"package com.example.jxspringbootdemo ; import java.util.concurrent.atomic.AtomicLong ; import org.springframework.web.bind.annotation.RequestMapping ; import org.springframework.web.bind.annotation.RequestParam ; import org.springframework.web.bind.annotation.RestController ; @RestController public class GreetingController { private static final String template = \"Hello, %s!\" ; private final AtomicLong counter = new AtomicLong (); @RequestMapping ( \"/greeting\" ) public Greeting greeting ( @RequestParam ( value = \"name\" , defaultValue = \"World\" ) String name ) { return new Greeting ( counter . incrementAndGet (), String . format ( template , name )); } }","title":"GreetingController"},{"location":"jenkinsx/hybrid/#test-application_1","text":"jx get activity -f jx-cdx-spring-boot-demo-1 -w","title":"Test application"},{"location":"jenkinsx/hybrid/#re-install-with-nexus","text":"","title":"Re-Install with Nexus"},{"location":"jenkinsx/hybrid/#myvaluesyaml_1","text":"Our application didn't work because now we have an application that depends on a Maven repository. We have to \"re-install\" Jenkins X, to have it install Nexus for us in the cdx namespace. nexus : enabled : true docker-registry : enabled : true","title":"myvalues.yaml"},{"location":"jenkinsx/hybrid/#install","text":"Make sure you execute this command where you have the myvalues.yaml file. jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain serverless. $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b","title":"Install"},{"location":"jenkinsx/hybrid/#test-application_2","text":"To trigger a new build, make a change - for example to the README.md and push it. jx get activity -f jx-cdx-spring-boot-demo-1 -w http jx-cdx-spring-boot-demo-1.cdx-staging.serverless. ${ DOMAIN } /greeting","title":"Test Application"},{"location":"jenkinsx/hybrid/#configure-tls_1","text":"jx upgrade ingress --domain $DOMAIN --namespaces cdx,cdx-staging","title":"Configure TLS"},{"location":"jenkinsx/hybrid/#re-test-application","text":"ORG = #the GitHub user or organisation your application is in jx update webhooks --repo = jx-cdx-spring-boot-demo-1 --org = ${ ORG } jx get applications jx get activity -f jx-cdx-spring-boot-demo-1 -w http https://jx-cdx-spring-boot-demo-1.cdx-staging.serverless. ${ DOMAIN } /greeting","title":"Re-test application"},{"location":"jenkinsx/intro/","text":"Introduction to Jenkins X \u00b6","title":"Introduction to Jenkins X"},{"location":"jenkinsx/intro/#introduction-to-jenkins-x","text":"","title":"Introduction to Jenkins X"},{"location":"jenkinsx/jx-pipelines/","text":"Jenkins X Pipelines \u00b6 Verify Changes \u00b6 jx step syntax effective Adding Steps \u00b6 Add Step Default \u00b6 Info By default, adding a step to a Pipeline's stage will cause it to end up at the end of the stage. buildPack : maven-java11 pipelineConfig : pipelines : pullRequest : build : steps : - command : sonar-scanner image : fabiopotame/sonar-scanner-cli # newtmitch/sonar-scanner for JDK 10+? dir : /workspace/source/ args : - -Dsonar.projectName=... - -Dsonar.projectKey=... - -Dsonar.organization=... - -Dsonar.sources=./src/main/java/ - -Dsonar.language=java - -Dsonar.java.binaries=./target/classes - -Dsonar.host.url=https://sonarcloud.io - -Dsonar.login=... Add Step At Specific Place \u00b6 If we want it explicity after or before a specific step, we have to \"select\" the step within a stage of a pipeline first. We do that as follows: pipelineConfig : pipelines : overrides : - pipeline : [ pipeline name : release , feature , pullRequest ] stage : [ stage name ] name : [ step name ] And then state of we want to replace the step (via type: replace ), or execute it before or after the \"selected\" step, via the field type . pipelineConfig : pipelines : overrides : - name : mvn-deploy pipeline : release stage : build step : name : sonar command : sonar-scanner image : fabiopotame/sonar-scanner-cli # newtmitch/sonar-scanner for JDK 10+? dir : /workspace/source/ args : - -Dsonar.projectName=... - -Dsonar.projectKey=... - -Dsonar.organization=... - -Dsonar.sources=./src/main/java/ - -Dsonar.language=java - -Dsonar.java.binaries=./target/classes - -Dsonar.host.url=https://sonarcloud.io - -Dsonar.login=... type : after Overriding Steps \u00b6 pipelineConfig : pipelines : release : setup : preSteps : - sh : echo BEFORE BASE SETUP steps : - sh : echo AFTER BASE SETUP build : replace : true steps : - sh : mvn clean deploy -Pmyprofile comment : this command is overridden from the base pipeline Meta pipeline/run always for every pipeline \u00b6 JX Pipeline Converter \u00b6 JX Pipeline Converter plugin for Jenkins X to assist in converting from the legacy Jenkinsfile-based pipelines to the modern jenkins-x.yml-based pipelines. Install \u00b6 macos curl -L https://github.com/jenkins-x/jx-convert-jenkinsfile/releases/download/ $( curl --silent https://api.github.com/repos/jenkins-x/jx-convert-jenkinsfile/releases/latest | jq -r '.tag_name' ) /jx-convert-jenkinsfile-darwin-amd64.tar.gz | tar xzv sudo mv jx-convert-jenkinsfile /usr/local/bin linux curl -L https://github.com/jenkins-x/jx-convert-jenkinsfile/releases/download/ $( curl --silent https://api.github.com/repos/jenkins-x/jx-convert-jenkinsfile/releases/latest | jq -r '.tag_name' ) /jx-convert-jenkinsfile-linux-amd64.tar.gz | tar xzv sudo mv jx-convert-jenkinsfile /usr/local/bin Caution It seems some shells will escape the ( in the above command. Make sure the command reads .../download/$(curl --silent... . Usage \u00b6 jx convert jenkinsfile Loops \u00b6 buildPack : go pipelineConfig : pipelines : overrides : - pipeline : release # This is new stage : build name : make-build steps : - loop : variable : GOOS values : - darwin - linux - windows steps : - name : build command : CGO_ENABLED=0 GOOS=\\${GOOS} GOARCH=amd64 go build -o bin/jx-go-loops_\\${GOOS} main.go Borrowed from Viktor Farcic's blog on Jenkins X Pipelines . Replace Whole Pipeline \u00b6 If you want to write your pipeline from scratch, you can specify you do not use a Build Pack. buildPack : none pipelineConfig : pipelines : release : pipeline : agent : image : busybox stages : - name : ci steps : - name : echo-version image : mvn command : mvn version Parallelization \u00b6 pipelineConfig : pipelines : release : pipeline : stages : - name : \"Parallelsss\" agent : image : maven parallel : - name : \"Parallel1\" agent : image : maven steps : - command : echo args : - test one a - command : sleep args : - \"60\" - name : \"Parallel2\" agent : image : maven steps : - command : echo args : - test two a - command : sleep args : - \"60\" Caution Each parallel stage will have its own Pod and thus they do not share the same workspace! Which in the activity log will look like this: joostvdg/jx-spring-boot-11/master #13 1m58s Running meta pipeline 1m58s 49s Succeeded Credential Initializer T6bkh 1m58s 0s Succeeded Working Dir Initializer 7qqhg 1m58s 0s Succeeded Place Tools 1m58s 1s Succeeded Git Source Meta Joostvdg Jx Spring Boot 11 Bj2lh 1m57s 7s Succeeded Git Merge 1m50s 0s Succeeded Merge Pull Refs 1m50s 0s Succeeded Create Effective Pipeline 1m50s 8s Succeeded Create Tekton Crds 1m42s 33s Succeeded Parallelsss 1m8s Running Parallelsss / Parallel1 1m8s 1m7s Succeeded Credential Initializer H2lh9 1m8s 0s Succeeded Working Dir Initializer Nlk6b 1m8s 0s Succeeded Place Tools 1m8s 2s Succeeded Git Source Joostvdg Jx Spring Boot 11 Mast 96j7m 1m6s 4s Succeeded Git Merge 1m2s 1s Succeeded Step2 1m1s 0s Succeeded Step3 1m1s 1m0s Succeeded Parallelsss / Parallel2 1m8s Running Credential Initializer Shc5t 1m8s 0s Succeeded Working Dir Initializer Tsx5j 1m8s 2s Succeeded Place Tools 1m6s 1s Succeeded Git Source Joostvdg Jx Spring Boot 11 Mast 8dbcl 1m5s 5s Succeeded Git Merge 1m0s 1s Succeeded Step2 59s 0s Succeeded Other Resources \u00b6 https://jenkins-x.io/docs/concepts/jenkins-x-pipelines/ https://jenkins-x.io/docs/reference/pipeline-syntax-reference/ https://jenkins-x.io/docs/reference/components/build-packs//#pipeline-extension-model https://technologyconversations.com/2019/06/30/overriding-pipelines-stages-and-steps-and-implementing-loops-in-jenkins-x-pipelines/ https://docs.cloudbees.com/docs/cloudbees-jenkins-x-distribution/latest/pipelines/#_extending_pipelines","title":"Jenkins X Pipelines"},{"location":"jenkinsx/jx-pipelines/#jenkins-x-pipelines","text":"","title":"Jenkins X Pipelines"},{"location":"jenkinsx/jx-pipelines/#verify-changes","text":"jx step syntax effective","title":"Verify Changes"},{"location":"jenkinsx/jx-pipelines/#adding-steps","text":"","title":"Adding Steps"},{"location":"jenkinsx/jx-pipelines/#add-step-default","text":"Info By default, adding a step to a Pipeline's stage will cause it to end up at the end of the stage. buildPack : maven-java11 pipelineConfig : pipelines : pullRequest : build : steps : - command : sonar-scanner image : fabiopotame/sonar-scanner-cli # newtmitch/sonar-scanner for JDK 10+? dir : /workspace/source/ args : - -Dsonar.projectName=... - -Dsonar.projectKey=... - -Dsonar.organization=... - -Dsonar.sources=./src/main/java/ - -Dsonar.language=java - -Dsonar.java.binaries=./target/classes - -Dsonar.host.url=https://sonarcloud.io - -Dsonar.login=...","title":"Add Step Default"},{"location":"jenkinsx/jx-pipelines/#add-step-at-specific-place","text":"If we want it explicity after or before a specific step, we have to \"select\" the step within a stage of a pipeline first. We do that as follows: pipelineConfig : pipelines : overrides : - pipeline : [ pipeline name : release , feature , pullRequest ] stage : [ stage name ] name : [ step name ] And then state of we want to replace the step (via type: replace ), or execute it before or after the \"selected\" step, via the field type . pipelineConfig : pipelines : overrides : - name : mvn-deploy pipeline : release stage : build step : name : sonar command : sonar-scanner image : fabiopotame/sonar-scanner-cli # newtmitch/sonar-scanner for JDK 10+? dir : /workspace/source/ args : - -Dsonar.projectName=... - -Dsonar.projectKey=... - -Dsonar.organization=... - -Dsonar.sources=./src/main/java/ - -Dsonar.language=java - -Dsonar.java.binaries=./target/classes - -Dsonar.host.url=https://sonarcloud.io - -Dsonar.login=... type : after","title":"Add Step At Specific Place"},{"location":"jenkinsx/jx-pipelines/#overriding-steps","text":"pipelineConfig : pipelines : release : setup : preSteps : - sh : echo BEFORE BASE SETUP steps : - sh : echo AFTER BASE SETUP build : replace : true steps : - sh : mvn clean deploy -Pmyprofile comment : this command is overridden from the base pipeline","title":"Overriding Steps"},{"location":"jenkinsx/jx-pipelines/#meta-pipelinerun-always-for-every-pipeline","text":"","title":"Meta pipeline/run always for every pipeline"},{"location":"jenkinsx/jx-pipelines/#jx-pipeline-converter","text":"JX Pipeline Converter plugin for Jenkins X to assist in converting from the legacy Jenkinsfile-based pipelines to the modern jenkins-x.yml-based pipelines.","title":"JX Pipeline Converter"},{"location":"jenkinsx/jx-pipelines/#install","text":"macos curl -L https://github.com/jenkins-x/jx-convert-jenkinsfile/releases/download/ $( curl --silent https://api.github.com/repos/jenkins-x/jx-convert-jenkinsfile/releases/latest | jq -r '.tag_name' ) /jx-convert-jenkinsfile-darwin-amd64.tar.gz | tar xzv sudo mv jx-convert-jenkinsfile /usr/local/bin linux curl -L https://github.com/jenkins-x/jx-convert-jenkinsfile/releases/download/ $( curl --silent https://api.github.com/repos/jenkins-x/jx-convert-jenkinsfile/releases/latest | jq -r '.tag_name' ) /jx-convert-jenkinsfile-linux-amd64.tar.gz | tar xzv sudo mv jx-convert-jenkinsfile /usr/local/bin Caution It seems some shells will escape the ( in the above command. Make sure the command reads .../download/$(curl --silent... .","title":"Install"},{"location":"jenkinsx/jx-pipelines/#usage","text":"jx convert jenkinsfile","title":"Usage"},{"location":"jenkinsx/jx-pipelines/#loops","text":"buildPack : go pipelineConfig : pipelines : overrides : - pipeline : release # This is new stage : build name : make-build steps : - loop : variable : GOOS values : - darwin - linux - windows steps : - name : build command : CGO_ENABLED=0 GOOS=\\${GOOS} GOARCH=amd64 go build -o bin/jx-go-loops_\\${GOOS} main.go Borrowed from Viktor Farcic's blog on Jenkins X Pipelines .","title":"Loops"},{"location":"jenkinsx/jx-pipelines/#replace-whole-pipeline","text":"If you want to write your pipeline from scratch, you can specify you do not use a Build Pack. buildPack : none pipelineConfig : pipelines : release : pipeline : agent : image : busybox stages : - name : ci steps : - name : echo-version image : mvn command : mvn version","title":"Replace Whole Pipeline"},{"location":"jenkinsx/jx-pipelines/#parallelization","text":"pipelineConfig : pipelines : release : pipeline : stages : - name : \"Parallelsss\" agent : image : maven parallel : - name : \"Parallel1\" agent : image : maven steps : - command : echo args : - test one a - command : sleep args : - \"60\" - name : \"Parallel2\" agent : image : maven steps : - command : echo args : - test two a - command : sleep args : - \"60\" Caution Each parallel stage will have its own Pod and thus they do not share the same workspace! Which in the activity log will look like this: joostvdg/jx-spring-boot-11/master #13 1m58s Running meta pipeline 1m58s 49s Succeeded Credential Initializer T6bkh 1m58s 0s Succeeded Working Dir Initializer 7qqhg 1m58s 0s Succeeded Place Tools 1m58s 1s Succeeded Git Source Meta Joostvdg Jx Spring Boot 11 Bj2lh 1m57s 7s Succeeded Git Merge 1m50s 0s Succeeded Merge Pull Refs 1m50s 0s Succeeded Create Effective Pipeline 1m50s 8s Succeeded Create Tekton Crds 1m42s 33s Succeeded Parallelsss 1m8s Running Parallelsss / Parallel1 1m8s 1m7s Succeeded Credential Initializer H2lh9 1m8s 0s Succeeded Working Dir Initializer Nlk6b 1m8s 0s Succeeded Place Tools 1m8s 2s Succeeded Git Source Joostvdg Jx Spring Boot 11 Mast 96j7m 1m6s 4s Succeeded Git Merge 1m2s 1s Succeeded Step2 1m1s 0s Succeeded Step3 1m1s 1m0s Succeeded Parallelsss / Parallel2 1m8s Running Credential Initializer Shc5t 1m8s 0s Succeeded Working Dir Initializer Tsx5j 1m8s 2s Succeeded Place Tools 1m6s 1s Succeeded Git Source Joostvdg Jx Spring Boot 11 Mast 8dbcl 1m5s 5s Succeeded Git Merge 1m0s 1s Succeeded Step2 59s 0s Succeeded","title":"Parallelization"},{"location":"jenkinsx/jx-pipelines/#other-resources","text":"https://jenkins-x.io/docs/concepts/jenkins-x-pipelines/ https://jenkins-x.io/docs/reference/pipeline-syntax-reference/ https://jenkins-x.io/docs/reference/components/build-packs//#pipeline-extension-model https://technologyconversations.com/2019/06/30/overriding-pipelines-stages-and-steps-and-implementing-loops-in-jenkins-x-pipelines/ https://docs.cloudbees.com/docs/cloudbees-jenkins-x-distribution/latest/pipelines/#_extending_pipelines","title":"Other Resources"},{"location":"jenkinsx/jxboot-aks/","text":"Jenkins X with Boot on AKS \u00b6 This guide is about installing Jenkins X via jx boot on AKS, including leveraging Google's CloudDNS for dynamic DNS configuration (for ingress). Create AKS Cluster \u00b6 Either create a cluster via AKS Terraform (recommended) or via AKS CLI . Install Jenkins X \u00b6 Boot Config \u00b6 Make a fork of the jenkins-x-boot-config repository and clone it. GH_USER = git clone https://github.com/ ${ GH_USER } /jenkins-x-boot-config.git cd jenkins-x-boot-config Changes to make: provider from gke to aks set domain set clustername set external dns (see below) set repository value for each environments (not dev) as below - key : staging repository : environment-jx-aks-staging External DNS \u00b6 Using Google CloudDNS: login to the GCP account you want to use enable CloudDNS API by going to it create a CloudDNS zone for your subdomain if the main domain is example.com -> aks.example.com once created, you get NS entries, copy these (usualy in the form ns-cloud-X{1-4}.googledomains.com in your Domain's DNS configuration, map your subdomain to these NS entries create a service account that can use CloudDNS API add the Google Project to which the Service Account belongs to: jx-requirements.yaml and values.yaml export the json configuration file rename the file to credentials.json create secret a secret in Kubernetes kubectl create secret generic external-dns-gcp-sa --from-file=credentials.json fix external dns values template -> systems/external-dns/values.tmpl.yaml add project: \"{{ .Requirements.cluster.project }}\" to external-dns . google Important You have to create the secret external-dns-gcp-sa in every namespace you set up TLS via the dns01 challenge. jx-requirements.yaml We're omitting the default values as much as possible, such as the dev and production environments. cluster : environmentGitOwner : <YOUR GITHUB ACCOUNT> gitKind : github gitName : github gitServer : https://github.com namespace : jx project : your-google-project provider : aks environments : - ingress : domain : staging.aks.example.com externalDNS : true namespaceSubDomain : \"\" tls : email : <YOUR EMAIL ADDRESS> enabled : true production : true key : staging repository : environment-jx-aks-staging gitops : true ingress : domain : aks.example.com externalDNS : true namespaceSubDomain : -jx. tls : email : <YOUR EMAIL ADDRESS> enabled : true production : true kaniko : true secretStorage : local values.yaml cluster : projectID : your-google-project TLS Config \u00b6 Update the jx-requirements.yaml , make sure ingress configuration is correct: ingress : domain : aks.example.com externalDNS : true namespaceSubDomain : -jx. tls : email : admin@example.com enabled : true production : true If all is done correctly with the CloudDNS configuration, the external dns will contain all the entries of the jx services (such as hook, chartmuseum) and certmanager will be able to verify the domain with Let's Encrypt. Docker Registry Config \u00b6 values.yaml jenkins-x-platform : dockerRegistry : myacr.azurecr.io This was not enough, added it to the values template: env/jenkins-x-platform/values.tmpl.yaml dockerRegistry : myacr.azurecr.io TLS For Application In Environment \u00b6 create issuer create certificate Note This implies you need to run jx boot at least once before working on your environment configuration! Easiest way I found, was to copy the yaml from the issuer and certificate in the jx namespace. You then remove the unnecesary elements, those generated by Kubernetes itself (such as creation date, status, etc). You have to change the domain name and hosts values, as they should now point to the subdomain corresponding to this environment (unless its production). Once the files are good, you add them to your environment. You do so, by adding them to the templates folder -> env/templates . . \u251c\u2500\u2500 Jenkinsfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 env \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 requirements.yaml \u2502 \u251c\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 certificate.yaml \u2502 \u2502 \u2514\u2500\u2500 issuer.yaml \u2502 \u2514\u2500\u2500 values.yaml \u2514\u2500\u2500 jenkins-x.yml kubectl -n jx get issuer letsencrypt-prod -o yaml kubectl -n jx get certificate tls-<unique to your cluster>-p -o yaml issuer.yaml The end result should look like this: apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : letsencrypt-prod spec : acme : email : admin@example.com privateKeySecretRef : name : letsencrypt-prod server : https://acme-v02.api.letsencrypt.org/directory solvers : - dns01 : clouddns : project : your-google-project serviceAccountSecretRef : key : credentials.json name : external-dns-gcp-sa selector : dnsNames : - '*.staging.aks.example.com' - staging.aks.example.com certificate.yaml apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : tls-staging-aks-example-com-p spec : commonName : '*.staging.aks.example.com' dnsNames : - '*.staging.aks.example.com' issuerRef : name : letsencrypt-prod secretName : tls-staging-aks-example-com-p","title":"JX Boot AKS"},{"location":"jenkinsx/jxboot-aks/#jenkins-x-with-boot-on-aks","text":"This guide is about installing Jenkins X via jx boot on AKS, including leveraging Google's CloudDNS for dynamic DNS configuration (for ingress).","title":"Jenkins X with Boot on AKS"},{"location":"jenkinsx/jxboot-aks/#create-aks-cluster","text":"Either create a cluster via AKS Terraform (recommended) or via AKS CLI .","title":"Create AKS Cluster"},{"location":"jenkinsx/jxboot-aks/#install-jenkins-x","text":"","title":"Install Jenkins X"},{"location":"jenkinsx/jxboot-aks/#boot-config","text":"Make a fork of the jenkins-x-boot-config repository and clone it. GH_USER = git clone https://github.com/ ${ GH_USER } /jenkins-x-boot-config.git cd jenkins-x-boot-config Changes to make: provider from gke to aks set domain set clustername set external dns (see below) set repository value for each environments (not dev) as below - key : staging repository : environment-jx-aks-staging","title":"Boot Config"},{"location":"jenkinsx/jxboot-aks/#external-dns","text":"Using Google CloudDNS: login to the GCP account you want to use enable CloudDNS API by going to it create a CloudDNS zone for your subdomain if the main domain is example.com -> aks.example.com once created, you get NS entries, copy these (usualy in the form ns-cloud-X{1-4}.googledomains.com in your Domain's DNS configuration, map your subdomain to these NS entries create a service account that can use CloudDNS API add the Google Project to which the Service Account belongs to: jx-requirements.yaml and values.yaml export the json configuration file rename the file to credentials.json create secret a secret in Kubernetes kubectl create secret generic external-dns-gcp-sa --from-file=credentials.json fix external dns values template -> systems/external-dns/values.tmpl.yaml add project: \"{{ .Requirements.cluster.project }}\" to external-dns . google Important You have to create the secret external-dns-gcp-sa in every namespace you set up TLS via the dns01 challenge. jx-requirements.yaml We're omitting the default values as much as possible, such as the dev and production environments. cluster : environmentGitOwner : <YOUR GITHUB ACCOUNT> gitKind : github gitName : github gitServer : https://github.com namespace : jx project : your-google-project provider : aks environments : - ingress : domain : staging.aks.example.com externalDNS : true namespaceSubDomain : \"\" tls : email : <YOUR EMAIL ADDRESS> enabled : true production : true key : staging repository : environment-jx-aks-staging gitops : true ingress : domain : aks.example.com externalDNS : true namespaceSubDomain : -jx. tls : email : <YOUR EMAIL ADDRESS> enabled : true production : true kaniko : true secretStorage : local values.yaml cluster : projectID : your-google-project","title":"External DNS"},{"location":"jenkinsx/jxboot-aks/#tls-config","text":"Update the jx-requirements.yaml , make sure ingress configuration is correct: ingress : domain : aks.example.com externalDNS : true namespaceSubDomain : -jx. tls : email : admin@example.com enabled : true production : true If all is done correctly with the CloudDNS configuration, the external dns will contain all the entries of the jx services (such as hook, chartmuseum) and certmanager will be able to verify the domain with Let's Encrypt.","title":"TLS Config"},{"location":"jenkinsx/jxboot-aks/#docker-registry-config","text":"values.yaml jenkins-x-platform : dockerRegistry : myacr.azurecr.io This was not enough, added it to the values template: env/jenkins-x-platform/values.tmpl.yaml dockerRegistry : myacr.azurecr.io","title":"Docker Registry Config"},{"location":"jenkinsx/jxboot-aks/#tls-for-application-in-environment","text":"create issuer create certificate Note This implies you need to run jx boot at least once before working on your environment configuration! Easiest way I found, was to copy the yaml from the issuer and certificate in the jx namespace. You then remove the unnecesary elements, those generated by Kubernetes itself (such as creation date, status, etc). You have to change the domain name and hosts values, as they should now point to the subdomain corresponding to this environment (unless its production). Once the files are good, you add them to your environment. You do so, by adding them to the templates folder -> env/templates . . \u251c\u2500\u2500 Jenkinsfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 env \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 requirements.yaml \u2502 \u251c\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 certificate.yaml \u2502 \u2502 \u2514\u2500\u2500 issuer.yaml \u2502 \u2514\u2500\u2500 values.yaml \u2514\u2500\u2500 jenkins-x.yml kubectl -n jx get issuer letsencrypt-prod -o yaml kubectl -n jx get certificate tls-<unique to your cluster>-p -o yaml issuer.yaml The end result should look like this: apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : letsencrypt-prod spec : acme : email : admin@example.com privateKeySecretRef : name : letsencrypt-prod server : https://acme-v02.api.letsencrypt.org/directory solvers : - dns01 : clouddns : project : your-google-project serviceAccountSecretRef : key : credentials.json name : external-dns-gcp-sa selector : dnsNames : - '*.staging.aks.example.com' - staging.aks.example.com certificate.yaml apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : tls-staging-aks-example-com-p spec : commonName : '*.staging.aks.example.com' dnsNames : - '*.staging.aks.example.com' issuerRef : name : letsencrypt-prod secretName : tls-staging-aks-example-com-p","title":"TLS For Application In Environment"},{"location":"jenkinsx/jxboot/","text":"Jenkins X Boot \u00b6 What Is Jenkins X Boot \u00b6 Jenkins X Boot is a way to install Jenkins X via the principles of Configuration-as-Code and GitOps . What it does, is to run an initial pipeline that creates a dev environment repository. This repository contains all the information in order to install Jenkins X. Once the dev environment (in Kubernetes) and it's backing git repository are created, it can run a pipeline to install the rest of Jenkins X. Process \u00b6 You start with a configuration repository (boot config) if you don't have one, Jenkins X will first create one for you From there, Jenkins X will create the dev environment repository Next Jenkins X leverages the repository to create the dev environment in your Kubernetes cluster The pipeline of this dev environment will checkout the dev repository and install Jenkins X Architecture \u00b6 CloudBees Jenkins X Distribution \u00b6 CloudBees Jenkins X Distribution (or CJXD ) is a version of Jenkins X maintained and supported by CloudBees . It is often abreviated to CJXD for convenience. There are three main differences between regular Jenkins X and CloudBees Jenkins X: there is more focus on stabilty and less on features, the release cadence is therefor once a month this distribution allows you to get paid support from CloudBees it contains a Web UI As Jenkins X, this is also free (as in beer) software. Get CJXD \u00b6 To use CJXD, you have to download the CloudBees distribution rather than relying on installes such as Homebrew or Chocolatey. Before you can then leverage the additional features included - such as the UI - you have to set the profile to CloudBees. To do so, run jx profile cloudbees . Initialize \u00b6 install jx cli binary create a Kubernetes cluster initialize jx via jx boot -> jx boot update jx-requirements.yml of your boot configuration repository Install On GKE \u00b6 First, install a Kubernetes Cluster on Google Cloud GKE. Either directly via Google's gcloud CLI or via Terraform . Things To Know \u00b6 Single Service Account \u00b6 Currently - as of November 2019 - Jenkins X with jx boot on GKE still uses the default service account of the account used to create the cluster. This is not the ideal situation, and is being worked upon. You can track the progress via GitHub issue #5856 . Useful Commands \u00b6 Restart From Specific Step If you do not want to redo the entire jx boot process, you can start from a specific step. For example, if you changed the envionments configuration, you can start with the step install-env . jx boot --start-step install-env Confirm Jenkins X Status jx status Add CJXD UI \u00b6 If you use the CloudBees Jenkins X Distribution (CJXD), you can also install a web UI. jx add app jx-app-ui --version=0.1.2 This will make a PR, which, when merged launches a Master Promotion build. jx get activities --watch --filter environment-dev jx get build log jx ui -p 8081 Add Environment \u00b6 As of this writing - November 2019 - adding a new environment to Jenkins X with Config as Code is a bit tedious. https://jenkins-x.io/docs/managing-jx/faq/boot/#how-do-i-add-new-environments Create Environment \u00b6 copy existing resources into new ( env/templates/ ) see Get Resources update values to suit new environment execute jx boot configure tls cloud dns zone external dns secret dns domain forward add cert and issuer to env repo if the environment has a separate domain, edit exdns-external-dns deployment kubectl edit deployment -n jx exdns-external-dns Get Resources \u00b6 kubectl get env staging -oyaml > env/templates/cb.yaml kubectl get sr joostvdg-env-cjxd-staging -oyaml > env/templates/cb-sr.yaml Configure TLS \u00b6 Configure Certificate & Issuer \u00b6 If you want TLS certificates for your new Environment, you will configure this yourself. The easiest way is to copy the current CertificateIssuer and Certificate from jx namespace and modify the data. kubectl get issuer -n jx letsencrypt-prod -oyaml > env/templates/issuer.yaml kubectl get cert -n jx tls-dev-cjxd-kearos-net-p -oyaml > env/templates/certificate.yaml Rename the namespace to your namespace Rename domain and host names to the new (sub-)domain Remove status segment Remove kubernetes managed fields ( uuid , timestamps, etc) git add env/ git commit -m \"add certs\" git push jx get activities -w Create Cloud DNS Zone \u00b6 If you leverage Cloud DNS and your new environment has a unique sub-domain, you have to create a new zone. A zone in Google's CloudDNS has three values: name : the name of the configuration item in GCP description : the description of the configuration item DNS Name : the actual DNS name ZONE_NAME=cb-cjxd-kearos-net DESCRIPTION=\"joostvdg - cb env for cjxd\" DNS_NAME=cb.cjxd.kearos.net gcloud dns managed-zones create ${ZONE_NAME} --description=${DESCRIPTION} --dns-name=${DNS_NAME} Once you've created the new zone, you will have to update your Domain provider to point the subdomain to the DNS servers of this new Zone. Configure CloudDNS Secrets \u00b6 If you leverage CloudDNS, you will also have to copy over the CloudDNS configuration from the jx namespace to your new Environment's namespace. kubectl get secret -n jx exdns-external-dns-token-cq5mv -oyaml > exdns-external-dns-token-env-cb.yaml kubectl get secret -n jx external-dns-gcp-sa -oyaml > external-dns-gcp-sa-env-cb.yaml Rename the namespace to your namespace remove status segment remove kubernetes managed fields ( uuid , timestamps, etc) kubectl apply -f exdns-external-dns-token-env-cb.yaml kubectl apply -f external-dns-gcp-sa-env-cb.yaml Confirm Certificate Works \u00b6 kubectl get cert -n cloudbees Common Issues \u00b6 ImagePullBackup \u00b6 events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18m default-scheduler Successfully assigned jx-staging/jx-jx-qs-spring-boot-6-58b75446b4-pkd7x to gke-joost-cjxd-pool2-54e21b2f-hlhd Normal Pulling 16m (x4 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Pulling image \"gcr.io/ps-dev-201405/jx-qs-spring-boot-6:0.0.1\" Warning Failed 16m (x4 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Failed to pull image \"gcr.io/ps-dev-201405/jx-qs-spring-boot-6:0.0.1\": rpc error: code = Unknown desc = Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication Warning Failed 16m (x4 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Error: ErrImagePull Normal BackOff 8m24s (x42 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Back-off pulling image \"gcr.io/ps-dev-201405/jx-qs-spring-boot-6:0.0.1\" Warning Failed 3m18s (x64 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Error: ImagePullBackOff Then you're missing scopes in your GKE Node's. resource \"google_container_node_pool\" \"nodepool2\" { ... node_config { machine_type = \"n2-standard-2\" oauth_scopes = [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" , ] } ... } Unable To Enable DNS API \u00b6 valid: there is a Secret: external-dns-gcp-sa in namespace: jx error: unable to enable 'dns' api: failed to run 'gcloud services list --enabled --project XXXXXX' command in directory '', output: 'ERROR: (gcloud.services.list) User [XXXXXX857-compute@developer.gserviceaccount.com] does not have permission to access project [XXXXXX] (or it may not exist): Request had insufficient authentication scopes.' valid: there is a Secret: external-dns-gcp-sa in namespace: jx error: unable to enable 'dns' api: failed to run 'gcloud services list --enabled --project GCP PROJECT B' command in directory '', output: 'ERROR: (gcloud.services.list) User [389413650857-compute@developer.gserviceaccount.com] does not have permission to access project [GCP PROJECT B] (or it may not exist): Request had insufficient authentication scopes.' Pipeline failed on stage 'release' : container 'step-create-install-values'. The execution of the pipeline has stopped. Have to enable \"Service Usage API\": https://console.developers.google.com/apis/api/serviceusage.googleapis.com/overview?project= node pools (for example in terraform) need access to the security scope https://www.googleapis.com/auth/cloud-platform oauth_scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ] Certmanager complaining about the wrong domain \u00b6 In case Cert-Manager is complaining that while validating x.y.example.com it cannot find example.com . See: https://github.com/jetstack/cert-manager/issues/1507 I1104 09:13:33.884549 1 base_controller.go:187] cert-manager/controller/challenges \"level\"=0 \"msg\"=\"syncing item\" \"key\"=\"cloudbees/tls-cb-cjxd-kearos-net-p-2383487961-0\" I1104 09:13:33.884966 1 dns.go:104] cert-manager/controller/challenges/Present \"level\"=0 \"msg\"=\"presenting DNS01 challenge for domain\" \"dnsName\"=\"cloudbees.cjxd.kearos.net\" \"domain\"=\"cloudbees.cjxd.kearos.net\" \"resource_kind\"=\"Challenge\" \"resource_name\"=\"tls-cb-cjxd-kearos-net-p-2383487961-0\" \"resource_namespace\"=\"cloudbees\" \"type\"=\"dns-01\" E1104 09:13:34.141122 1 base_controller.go:189] cert-manager/controller/challenges \"msg\"=\"re-queuing item due to error processing\" \"error\"=\"No matching GoogleCloud domain found for domain kearos.net.\" \"key\"=\"cloudbees/tls-cb-cjxd-kearos-net-p-2383487961-0\" Reset Installation \u00b6 Remove Environment Repos \u00b6 go to git and remove environment repo clear jx boot config folder clear cloud dns config (if used) clear requirements from env repo's if re-used Recreate Git Token \u00b6 If for some reason the git token is invalid, you can recreate it with the commands below . jx delete git token -n github <yourUserName> jx create git token -n github <yourUserName> References \u00b6 Jenkins X - Managing FAQ Jenkins X - Using FAQ Jenkins X - Pipeline FAQ Jenkins X - Boot FAQ Jenkins X - Configure CloudDNS Jenkins X - Add New Environment","title":"JX Boot GKE"},{"location":"jenkinsx/jxboot/#jenkins-x-boot","text":"","title":"Jenkins X Boot"},{"location":"jenkinsx/jxboot/#what-is-jenkins-x-boot","text":"Jenkins X Boot is a way to install Jenkins X via the principles of Configuration-as-Code and GitOps . What it does, is to run an initial pipeline that creates a dev environment repository. This repository contains all the information in order to install Jenkins X. Once the dev environment (in Kubernetes) and it's backing git repository are created, it can run a pipeline to install the rest of Jenkins X.","title":"What Is Jenkins X Boot"},{"location":"jenkinsx/jxboot/#process","text":"You start with a configuration repository (boot config) if you don't have one, Jenkins X will first create one for you From there, Jenkins X will create the dev environment repository Next Jenkins X leverages the repository to create the dev environment in your Kubernetes cluster The pipeline of this dev environment will checkout the dev repository and install Jenkins X","title":"Process"},{"location":"jenkinsx/jxboot/#architecture","text":"","title":"Architecture"},{"location":"jenkinsx/jxboot/#cloudbees-jenkins-x-distribution","text":"CloudBees Jenkins X Distribution (or CJXD ) is a version of Jenkins X maintained and supported by CloudBees . It is often abreviated to CJXD for convenience. There are three main differences between regular Jenkins X and CloudBees Jenkins X: there is more focus on stabilty and less on features, the release cadence is therefor once a month this distribution allows you to get paid support from CloudBees it contains a Web UI As Jenkins X, this is also free (as in beer) software.","title":"CloudBees Jenkins X Distribution"},{"location":"jenkinsx/jxboot/#get-cjxd","text":"To use CJXD, you have to download the CloudBees distribution rather than relying on installes such as Homebrew or Chocolatey. Before you can then leverage the additional features included - such as the UI - you have to set the profile to CloudBees. To do so, run jx profile cloudbees .","title":"Get CJXD"},{"location":"jenkinsx/jxboot/#initialize","text":"install jx cli binary create a Kubernetes cluster initialize jx via jx boot -> jx boot update jx-requirements.yml of your boot configuration repository","title":"Initialize"},{"location":"jenkinsx/jxboot/#install-on-gke","text":"First, install a Kubernetes Cluster on Google Cloud GKE. Either directly via Google's gcloud CLI or via Terraform .","title":"Install On GKE"},{"location":"jenkinsx/jxboot/#things-to-know","text":"","title":"Things To Know"},{"location":"jenkinsx/jxboot/#single-service-account","text":"Currently - as of November 2019 - Jenkins X with jx boot on GKE still uses the default service account of the account used to create the cluster. This is not the ideal situation, and is being worked upon. You can track the progress via GitHub issue #5856 .","title":"Single Service Account"},{"location":"jenkinsx/jxboot/#useful-commands","text":"Restart From Specific Step If you do not want to redo the entire jx boot process, you can start from a specific step. For example, if you changed the envionments configuration, you can start with the step install-env . jx boot --start-step install-env Confirm Jenkins X Status jx status","title":"Useful Commands"},{"location":"jenkinsx/jxboot/#add-cjxd-ui","text":"If you use the CloudBees Jenkins X Distribution (CJXD), you can also install a web UI. jx add app jx-app-ui --version=0.1.2 This will make a PR, which, when merged launches a Master Promotion build. jx get activities --watch --filter environment-dev jx get build log jx ui -p 8081","title":"Add CJXD UI"},{"location":"jenkinsx/jxboot/#add-environment","text":"As of this writing - November 2019 - adding a new environment to Jenkins X with Config as Code is a bit tedious. https://jenkins-x.io/docs/managing-jx/faq/boot/#how-do-i-add-new-environments","title":"Add Environment"},{"location":"jenkinsx/jxboot/#create-environment","text":"copy existing resources into new ( env/templates/ ) see Get Resources update values to suit new environment execute jx boot configure tls cloud dns zone external dns secret dns domain forward add cert and issuer to env repo if the environment has a separate domain, edit exdns-external-dns deployment kubectl edit deployment -n jx exdns-external-dns","title":"Create Environment"},{"location":"jenkinsx/jxboot/#get-resources","text":"kubectl get env staging -oyaml > env/templates/cb.yaml kubectl get sr joostvdg-env-cjxd-staging -oyaml > env/templates/cb-sr.yaml","title":"Get Resources"},{"location":"jenkinsx/jxboot/#configure-tls","text":"","title":"Configure TLS"},{"location":"jenkinsx/jxboot/#configure-certificate-issuer","text":"If you want TLS certificates for your new Environment, you will configure this yourself. The easiest way is to copy the current CertificateIssuer and Certificate from jx namespace and modify the data. kubectl get issuer -n jx letsencrypt-prod -oyaml > env/templates/issuer.yaml kubectl get cert -n jx tls-dev-cjxd-kearos-net-p -oyaml > env/templates/certificate.yaml Rename the namespace to your namespace Rename domain and host names to the new (sub-)domain Remove status segment Remove kubernetes managed fields ( uuid , timestamps, etc) git add env/ git commit -m \"add certs\" git push jx get activities -w","title":"Configure Certificate &amp; Issuer"},{"location":"jenkinsx/jxboot/#create-cloud-dns-zone","text":"If you leverage Cloud DNS and your new environment has a unique sub-domain, you have to create a new zone. A zone in Google's CloudDNS has three values: name : the name of the configuration item in GCP description : the description of the configuration item DNS Name : the actual DNS name ZONE_NAME=cb-cjxd-kearos-net DESCRIPTION=\"joostvdg - cb env for cjxd\" DNS_NAME=cb.cjxd.kearos.net gcloud dns managed-zones create ${ZONE_NAME} --description=${DESCRIPTION} --dns-name=${DNS_NAME} Once you've created the new zone, you will have to update your Domain provider to point the subdomain to the DNS servers of this new Zone.","title":"Create Cloud DNS Zone"},{"location":"jenkinsx/jxboot/#configure-clouddns-secrets","text":"If you leverage CloudDNS, you will also have to copy over the CloudDNS configuration from the jx namespace to your new Environment's namespace. kubectl get secret -n jx exdns-external-dns-token-cq5mv -oyaml > exdns-external-dns-token-env-cb.yaml kubectl get secret -n jx external-dns-gcp-sa -oyaml > external-dns-gcp-sa-env-cb.yaml Rename the namespace to your namespace remove status segment remove kubernetes managed fields ( uuid , timestamps, etc) kubectl apply -f exdns-external-dns-token-env-cb.yaml kubectl apply -f external-dns-gcp-sa-env-cb.yaml","title":"Configure CloudDNS Secrets"},{"location":"jenkinsx/jxboot/#confirm-certificate-works","text":"kubectl get cert -n cloudbees","title":"Confirm Certificate Works"},{"location":"jenkinsx/jxboot/#common-issues","text":"","title":"Common Issues"},{"location":"jenkinsx/jxboot/#imagepullbackup","text":"events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18m default-scheduler Successfully assigned jx-staging/jx-jx-qs-spring-boot-6-58b75446b4-pkd7x to gke-joost-cjxd-pool2-54e21b2f-hlhd Normal Pulling 16m (x4 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Pulling image \"gcr.io/ps-dev-201405/jx-qs-spring-boot-6:0.0.1\" Warning Failed 16m (x4 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Failed to pull image \"gcr.io/ps-dev-201405/jx-qs-spring-boot-6:0.0.1\": rpc error: code = Unknown desc = Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication Warning Failed 16m (x4 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Error: ErrImagePull Normal BackOff 8m24s (x42 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Back-off pulling image \"gcr.io/ps-dev-201405/jx-qs-spring-boot-6:0.0.1\" Warning Failed 3m18s (x64 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Error: ImagePullBackOff Then you're missing scopes in your GKE Node's. resource \"google_container_node_pool\" \"nodepool2\" { ... node_config { machine_type = \"n2-standard-2\" oauth_scopes = [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" , ] } ... }","title":"ImagePullBackup"},{"location":"jenkinsx/jxboot/#unable-to-enable-dns-api","text":"valid: there is a Secret: external-dns-gcp-sa in namespace: jx error: unable to enable 'dns' api: failed to run 'gcloud services list --enabled --project XXXXXX' command in directory '', output: 'ERROR: (gcloud.services.list) User [XXXXXX857-compute@developer.gserviceaccount.com] does not have permission to access project [XXXXXX] (or it may not exist): Request had insufficient authentication scopes.' valid: there is a Secret: external-dns-gcp-sa in namespace: jx error: unable to enable 'dns' api: failed to run 'gcloud services list --enabled --project GCP PROJECT B' command in directory '', output: 'ERROR: (gcloud.services.list) User [389413650857-compute@developer.gserviceaccount.com] does not have permission to access project [GCP PROJECT B] (or it may not exist): Request had insufficient authentication scopes.' Pipeline failed on stage 'release' : container 'step-create-install-values'. The execution of the pipeline has stopped. Have to enable \"Service Usage API\": https://console.developers.google.com/apis/api/serviceusage.googleapis.com/overview?project= node pools (for example in terraform) need access to the security scope https://www.googleapis.com/auth/cloud-platform oauth_scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ]","title":"Unable To Enable DNS API"},{"location":"jenkinsx/jxboot/#certmanager-complaining-about-the-wrong-domain","text":"In case Cert-Manager is complaining that while validating x.y.example.com it cannot find example.com . See: https://github.com/jetstack/cert-manager/issues/1507 I1104 09:13:33.884549 1 base_controller.go:187] cert-manager/controller/challenges \"level\"=0 \"msg\"=\"syncing item\" \"key\"=\"cloudbees/tls-cb-cjxd-kearos-net-p-2383487961-0\" I1104 09:13:33.884966 1 dns.go:104] cert-manager/controller/challenges/Present \"level\"=0 \"msg\"=\"presenting DNS01 challenge for domain\" \"dnsName\"=\"cloudbees.cjxd.kearos.net\" \"domain\"=\"cloudbees.cjxd.kearos.net\" \"resource_kind\"=\"Challenge\" \"resource_name\"=\"tls-cb-cjxd-kearos-net-p-2383487961-0\" \"resource_namespace\"=\"cloudbees\" \"type\"=\"dns-01\" E1104 09:13:34.141122 1 base_controller.go:189] cert-manager/controller/challenges \"msg\"=\"re-queuing item due to error processing\" \"error\"=\"No matching GoogleCloud domain found for domain kearos.net.\" \"key\"=\"cloudbees/tls-cb-cjxd-kearos-net-p-2383487961-0\"","title":"Certmanager complaining about the wrong domain"},{"location":"jenkinsx/jxboot/#reset-installation","text":"","title":"Reset Installation"},{"location":"jenkinsx/jxboot/#remove-environment-repos","text":"go to git and remove environment repo clear jx boot config folder clear cloud dns config (if used) clear requirements from env repo's if re-used","title":"Remove Environment Repos"},{"location":"jenkinsx/jxboot/#recreate-git-token","text":"If for some reason the git token is invalid, you can recreate it with the commands below . jx delete git token -n github <yourUserName> jx create git token -n github <yourUserName>","title":"Recreate Git Token"},{"location":"jenkinsx/jxboot/#references","text":"Jenkins X - Managing FAQ Jenkins X - Using FAQ Jenkins X - Pipeline FAQ Jenkins X - Boot FAQ Jenkins X - Configure CloudDNS Jenkins X - Add New Environment","title":"References"},{"location":"jenkinsx/kubernetes-days/","text":"Jenkins X - Kubernetes Days \u00b6 Prepare \u00b6 asciinema rec first.cast asciinema play -i 2 first.cast -i -> play back with max of 2 seconds of idleness -s -> play back with double speed Process \u00b6 Create cluster Create quickstart Gitops Promotion Pr jx boot Commands \u00b6 asciinema rec jx-k8s-days-00-logo.cast ______ __ ______ __ __ _______ .__ __. ___ .___________. __ ____ ____ _______ / || | / __ \\ | | | | | \\ | \\ | | / \\ | || | \\ \\ / / | ____ | | ,---- '| | | | | | | | | | | .--. |______| \\| | / ^ \\ `---| |----`| | \\ \\/ / | |__ | | | | | | | | | | | | | | | |______| . ` | / /_\\ \\ | | | | \\ / | __| | `----.| `----.| `--' | | ` -- ' | | ' -- ' | | |\\ | / _____ \\ | | | | \\ / | |____ \\______||_______| \\______/ \\______/ |_______/ |__| \\__| /__/ \\__\\ |__| |__| \\__/ |_______| ______ __ ___ ______ _______ / || | / // || \\ | ,----' | | / / | ,---- '| .--. | | | | | / / | | | | | | | `----.| | / / | `----.| ' -- ' | \\______||__| /__/ \\______||_______/ ____ __ ____ __ .___________. __ __ __ _______ .__ __. __ ___ __ .__ __. _______. ___ ___ \\ \\ / \\ / / | | | || | | | | | | ____|| \\ | | | |/ / | | | \\ | | / | \\ \\ / / \\ \\/ \\/ / | | `---| |----`| |__| | | | | |__ | \\| | | ' / | | | \\| | | ( ---- ` \\ V / \\ / | | | | | __ | .--. | | | __ | | . ` | | < | | | . ` | \\ \\ > < \\ / \\ / | | | | | | | | | ` -- ' | | | ____ | | \\ | | . \\ | | | | \\ | .---- ) | / . \\ \\_ _/ \\_ _/ | __ | | __ | | __ | | __ | \\_ _____/ | _______ || __ | \\_ _ | | __ | \\_ _ \\ | __ | | __ | \\_ _ | | _______/ /__/ \\_ _ \\ asciinema play jx-k8s-days-00-logo.cast Create Cluster \u00b6 export NAMESPACE = cd export PROJECT = asciinema rec jx-k8s-days-01-create.cast jx create cluster gke -n jx-rocks -p $PROJECT -r us-east1 \\ -m n1-standard-4 --min-num-nodes 1 --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks --git-provider-kind github \\ --namespace $NAMESPACE --prow --tekton asciinema play -i 1 -s 4 jx-k8s-days-01-create.cast Create QuickStart Go \u00b6 asciinema rec jx-k8s-days-02-quickstart.cast export APP_NAME = jx-k8s-days-go-02 jx create quickstart --filter golang-http --project-name ${ APP_NAME } --batch-mode ls -l ${ APP_NAME } jx get activity -f ${ APP_NAME } -w jx get pipelines jx get applications -e staging http \"http:// ${ APP_NAME } .cd-staging.35.185.41.106.nip.io\" jx get build logs -f ${ APP_NAME } # Cancel with ctrl+c cd ${ APP_NAME } vim main.go jx get activity -f ${ APP_NAME } -w jx get applications -e staging http \"http:// ${ APP_NAME } .cd-staging.35.185.41.106.nip.io\" asciinema play -i 2 -s 2 jx-k8s-days-02-quickstart.cast Import Project \u00b6 __ .___ ___. .______ ______ .______ .___________. __________ ___ __ _______.___________..__ __. _______ | | | \\/ | | _ \\ / __ \\ | _ \\ | | | ____ \\ \\ / / | | / | || \\ | | / _____ | | | | \\ / | | | _ ) | | | | | | | _ ) | ` --- | | ---- ` | | __ \\ V / | | | ( ---- ` --- | | ---- ` | \\| | | | __ | | | | \\/ | | | ___/ | | | | | / | | | __ | > < | | \\ \\ | | | . ` | | | | _ | | | | | | | | | | ` -- ' | | |\\ \\----. | | | |____ / . \\ | | .----) | | | | |\\ | | |__| | |__| |__| |__| | _| \\______/ | _| `._____| |__| |_______/__/ \\__\\ |__| |_______/ |__| |__| \\__| \\______| ___ .______ .______ __ __ ______ ___ .___________. __ ______ .__ __. / \\ | _ \\ | _ \\ | | | | / | / \\ | || | / __ \\ | \\ | | / ^ \\ | |_) | | |_) | | | | | | ,----' / ^ \\ ` --- | | ---- ` | | | | | | | \\| | / /_ \\ \\ | ___/ | ___/ | | | | | | / /_ \\ \\ | | | | | | | | | . ` | / _____ \\ | | | | | ` ----. | | | ` ----./ _____ \\ | | | | | ` -- ' | | | \\ | /__/ \\_ _ \\ | _ | | _ | | _______ || __ | \\_ _____/__/ \\_ _ \\ | __ | | __ | \\_ _____/ | __ | \\_ _ | asciinema rec jx-k8s-days-03-import.cast git clone https://github.com/joostvdg/go-demo-6.git cd go-demo-6 git checkout orig git merge -s ours master --no-edit git checkout master git merge orig rm -rf charts ls -lath git push jx import --batch-mode jx get activities --filter go-demo-6 --watch jx get applications kubectl --namespace cd-staging logs -l app = jx-go-demo-6 echo \"dependencies: - name: mongodb alias: go-demo-6-db version: 5.3.0 repository: https://kubernetes-charts.storage.googleapis.com condition: db.enabled\" > charts/go-demo-6/requirements.yaml cat charts/go-demo-6/requirements.yaml vim charts/go-demo-6/templates/deployment.yaml env: - name: DB value: {{ template \"fullname\" . }} -db vim charts/go-demo-6/values.yaml probePath: /demo/hello?health = true git status git add charts/ git commit -m \"add db dependency\" git push jx get activities --filter go-demo-6 --watch jx get applications http http://go-demo-6.cd-staging.35.185.41.106.nip.io/demo/hello jx delete application rm -rf go-demo-6 jx get applications asciinema play -i 2 -s 2 jx-k8s-days-03-import.cast Preview Environments \u00b6 APP_NAME = jx-k8s-days-go-01 asciinema rec jx-k8s-days-04-preview.cast jx get applications cd ${ APP_NAME } git checkout -b my-new-pr-3 vim main.go git status git add main.go git commit -m \"change message\" git push --set-upstream origin my-new-pr-3 jx create pullrequest \\ --title \"My PR\" \\ --body \"This is the text that describes the PR\" \\ --batch-mode open pr link jx get previews http .. add /lgtm to pr merge pr jx get activity --filter jx-k8s-day-go-01 --watch jx get applications git checkout master git pull cd .. jx get previews jx gc previews jx get previews asciinema play -i 2 -s 2 jx-k8s-days-04-preview.cast Env \u00b6 jx create environment Reel \u00b6 for VARIABLE in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 do asciinema play jx-k8s-days-00-logo.cast asciinema play -i 2 -s 3 jx-k8s-days-01-create.cast asciinema play -i 2 -s 1 jx-k8s-days-02-quickstart.cast asciinema play -i 2 -s 1 jx-k8s-days-03-import.cast asciinema play -i 2 -s 2 jx-k8s-days-04-preview.cast done","title":"Example For Console Reel"},{"location":"jenkinsx/kubernetes-days/#jenkins-x-kubernetes-days","text":"","title":"Jenkins X - Kubernetes Days"},{"location":"jenkinsx/kubernetes-days/#prepare","text":"asciinema rec first.cast asciinema play -i 2 first.cast -i -> play back with max of 2 seconds of idleness -s -> play back with double speed","title":"Prepare"},{"location":"jenkinsx/kubernetes-days/#process","text":"Create cluster Create quickstart Gitops Promotion Pr jx boot","title":"Process"},{"location":"jenkinsx/kubernetes-days/#commands","text":"asciinema rec jx-k8s-days-00-logo.cast ______ __ ______ __ __ _______ .__ __. ___ .___________. __ ____ ____ _______ / || | / __ \\ | | | | | \\ | \\ | | / \\ | || | \\ \\ / / | ____ | | ,---- '| | | | | | | | | | | .--. |______| \\| | / ^ \\ `---| |----`| | \\ \\/ / | |__ | | | | | | | | | | | | | | | |______| . ` | / /_\\ \\ | | | | \\ / | __| | `----.| `----.| `--' | | ` -- ' | | ' -- ' | | |\\ | / _____ \\ | | | | \\ / | |____ \\______||_______| \\______/ \\______/ |_______/ |__| \\__| /__/ \\__\\ |__| |__| \\__/ |_______| ______ __ ___ ______ _______ / || | / // || \\ | ,----' | | / / | ,---- '| .--. | | | | | / / | | | | | | | `----.| | / / | `----.| ' -- ' | \\______||__| /__/ \\______||_______/ ____ __ ____ __ .___________. __ __ __ _______ .__ __. __ ___ __ .__ __. _______. ___ ___ \\ \\ / \\ / / | | | || | | | | | | ____|| \\ | | | |/ / | | | \\ | | / | \\ \\ / / \\ \\/ \\/ / | | `---| |----`| |__| | | | | |__ | \\| | | ' / | | | \\| | | ( ---- ` \\ V / \\ / | | | | | __ | .--. | | | __ | | . ` | | < | | | . ` | \\ \\ > < \\ / \\ / | | | | | | | | | ` -- ' | | | ____ | | \\ | | . \\ | | | | \\ | .---- ) | / . \\ \\_ _/ \\_ _/ | __ | | __ | | __ | | __ | \\_ _____/ | _______ || __ | \\_ _ | | __ | \\_ _ \\ | __ | | __ | \\_ _ | | _______/ /__/ \\_ _ \\ asciinema play jx-k8s-days-00-logo.cast","title":"Commands"},{"location":"jenkinsx/kubernetes-days/#create-cluster","text":"export NAMESPACE = cd export PROJECT = asciinema rec jx-k8s-days-01-create.cast jx create cluster gke -n jx-rocks -p $PROJECT -r us-east1 \\ -m n1-standard-4 --min-num-nodes 1 --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks --git-provider-kind github \\ --namespace $NAMESPACE --prow --tekton asciinema play -i 1 -s 4 jx-k8s-days-01-create.cast","title":"Create Cluster"},{"location":"jenkinsx/kubernetes-days/#create-quickstart-go","text":"asciinema rec jx-k8s-days-02-quickstart.cast export APP_NAME = jx-k8s-days-go-02 jx create quickstart --filter golang-http --project-name ${ APP_NAME } --batch-mode ls -l ${ APP_NAME } jx get activity -f ${ APP_NAME } -w jx get pipelines jx get applications -e staging http \"http:// ${ APP_NAME } .cd-staging.35.185.41.106.nip.io\" jx get build logs -f ${ APP_NAME } # Cancel with ctrl+c cd ${ APP_NAME } vim main.go jx get activity -f ${ APP_NAME } -w jx get applications -e staging http \"http:// ${ APP_NAME } .cd-staging.35.185.41.106.nip.io\" asciinema play -i 2 -s 2 jx-k8s-days-02-quickstart.cast","title":"Create QuickStart Go"},{"location":"jenkinsx/kubernetes-days/#import-project","text":"__ .___ ___. .______ ______ .______ .___________. __________ ___ __ _______.___________..__ __. _______ | | | \\/ | | _ \\ / __ \\ | _ \\ | | | ____ \\ \\ / / | | / | || \\ | | / _____ | | | | \\ / | | | _ ) | | | | | | | _ ) | ` --- | | ---- ` | | __ \\ V / | | | ( ---- ` --- | | ---- ` | \\| | | | __ | | | | \\/ | | | ___/ | | | | | / | | | __ | > < | | \\ \\ | | | . ` | | | | _ | | | | | | | | | | ` -- ' | | |\\ \\----. | | | |____ / . \\ | | .----) | | | | |\\ | | |__| | |__| |__| |__| | _| \\______/ | _| `._____| |__| |_______/__/ \\__\\ |__| |_______/ |__| |__| \\__| \\______| ___ .______ .______ __ __ ______ ___ .___________. __ ______ .__ __. / \\ | _ \\ | _ \\ | | | | / | / \\ | || | / __ \\ | \\ | | / ^ \\ | |_) | | |_) | | | | | | ,----' / ^ \\ ` --- | | ---- ` | | | | | | | \\| | / /_ \\ \\ | ___/ | ___/ | | | | | | / /_ \\ \\ | | | | | | | | | . ` | / _____ \\ | | | | | ` ----. | | | ` ----./ _____ \\ | | | | | ` -- ' | | | \\ | /__/ \\_ _ \\ | _ | | _ | | _______ || __ | \\_ _____/__/ \\_ _ \\ | __ | | __ | \\_ _____/ | __ | \\_ _ | asciinema rec jx-k8s-days-03-import.cast git clone https://github.com/joostvdg/go-demo-6.git cd go-demo-6 git checkout orig git merge -s ours master --no-edit git checkout master git merge orig rm -rf charts ls -lath git push jx import --batch-mode jx get activities --filter go-demo-6 --watch jx get applications kubectl --namespace cd-staging logs -l app = jx-go-demo-6 echo \"dependencies: - name: mongodb alias: go-demo-6-db version: 5.3.0 repository: https://kubernetes-charts.storage.googleapis.com condition: db.enabled\" > charts/go-demo-6/requirements.yaml cat charts/go-demo-6/requirements.yaml vim charts/go-demo-6/templates/deployment.yaml env: - name: DB value: {{ template \"fullname\" . }} -db vim charts/go-demo-6/values.yaml probePath: /demo/hello?health = true git status git add charts/ git commit -m \"add db dependency\" git push jx get activities --filter go-demo-6 --watch jx get applications http http://go-demo-6.cd-staging.35.185.41.106.nip.io/demo/hello jx delete application rm -rf go-demo-6 jx get applications asciinema play -i 2 -s 2 jx-k8s-days-03-import.cast","title":"Import Project"},{"location":"jenkinsx/kubernetes-days/#preview-environments","text":"APP_NAME = jx-k8s-days-go-01 asciinema rec jx-k8s-days-04-preview.cast jx get applications cd ${ APP_NAME } git checkout -b my-new-pr-3 vim main.go git status git add main.go git commit -m \"change message\" git push --set-upstream origin my-new-pr-3 jx create pullrequest \\ --title \"My PR\" \\ --body \"This is the text that describes the PR\" \\ --batch-mode open pr link jx get previews http .. add /lgtm to pr merge pr jx get activity --filter jx-k8s-day-go-01 --watch jx get applications git checkout master git pull cd .. jx get previews jx gc previews jx get previews asciinema play -i 2 -s 2 jx-k8s-days-04-preview.cast","title":"Preview Environments"},{"location":"jenkinsx/kubernetes-days/#env","text":"jx create environment","title":"Env"},{"location":"jenkinsx/kubernetes-days/#reel","text":"for VARIABLE in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 do asciinema play jx-k8s-days-00-logo.cast asciinema play -i 2 -s 3 jx-k8s-days-01-create.cast asciinema play -i 2 -s 1 jx-k8s-days-02-quickstart.cast asciinema play -i 2 -s 1 jx-k8s-days-03-import.cast asciinema play -i 2 -s 2 jx-k8s-days-04-preview.cast done","title":"Reel"},{"location":"jenkinsx/lighthouse-bitbucket/","text":"Jenkins X - Lighthouse & Bitbucket \u00b6 This guide is about using Jenkins X with Lighthouse 1 as webhook manager and Bitbucket for the environment repositories 2 . Run Bitbucket on Kubernetes \u00b6 Unfortunately, Atlassian doesn't have an officially supported Bitbucket for Kubernetes. So I've taken the courtesy of creating my own basic configuration - read, not production ready. service.yaml apiVersion : v1 kind : Service metadata : labels : app : bitbucket name : bitbucket namespace : default spec : ports : - name : http port : 80 protocol : TCP targetPort : http selector : app : bitbucket sessionAffinity : None type : ClusterIP ingress.yaml I've taken the assumption that your cluster supports Ingress resources (even if its an OpenShift cluster). apiVersion : extensions/v1beta1 kind : Ingress metadata : name : bitbucket namespace : default spec : rules : - host : bitbucket.openshift.example.com http : paths : - backend : serviceName : bitbucket servicePort : 80 stateful-set.yaml apiVersion : apps/v1 kind : StatefulSet metadata : name : bitbucket namespace : default spec : serviceName : \"bitbucket\" replicas : 1 selector : matchLabels : app : bitbucket template : metadata : labels : app : bitbucket spec : containers : - name : bitbucket image : atlassian/bitbucket-server:7.0.0 ports : - containerPort : 7990 name : http - containerPort : 7999 name : web volumeMounts : - name : data mountPath : /var/atlassian/application-data/bitbucket volumeClaimTemplates : - metadata : name : data spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 5Gi JX Boot Configuration \u00b6 We use jx boot 3 to install Jenkins X. If we want to use Bitbucket for the environment repositories, we have to use Lighthouse 1 4 . In order to jx to install correctly, we have configure several parameters in the jx-requirements.yml with specific values. See the docs for all the possible values 5 . webhook: lighthouse : we have to set the webhook manager to lighthouse , as Prow only works with GitHub environmentGitOwner: jx : the project in Bitbucket where the repositories need to be created gitKind: bitbucketserver : the kind of git server, in this case bitbucketserver , because bitbucket refers to Bitbucket Cloud gitName: bs : the name for our gitserver configuration gitServer: http://bitbucket.openshift.example.com : the url to our Bitbucket Server registry: docker.io : when not using a Public Cloud provider, you have to specify the docker registry URL, in this case, Dockerhub (which is docker.io ) dockerRegistryOrg: caladreas : when the docker registry owner - in my case, caladreas - is different from the git repository owner, you have to specify this via dockerRegistryOrg We also have to set the storage for at least the logs. If we do not configure the storage for our logs, they will be assumed to be written to github pages of our application. That is, regardless of where our application resides. So, if you use anything other than GitHub (cloud), you have to configure the logs storage. The easiest solution, is to create a seperate repository for the build logs in your Bitbucket Server project. storage : logs : enabled : true url : \"http://bitbucket.openshift.example.com/scm/jx/build-logs.git\" If you have forgotten to set the storage before the installation, you can rectify this afterwards via the jx edit storage command. jx edit storage -c logs --git-url http://bitbucket.openshift.kearos.net/scm/jx/build-logs.git --git-branch master jx-requirements.yml bootConfigURL : https://github.com/jenkins-x/jenkins-x-boot-config.git cluster : clusterName : rhos11 devEnvApprovers : - jvandergriendt environmentGitOwner : jx gitKind : bitbucketserver gitName : bs gitServer : http://bitbucket.openshift.example.com namespace : jx provider : kubernetes registry : docker.io dockerRegistryOrg : caladreas environments : - ingress : domain : openshift.example.com namespaceSubDomain : -jx. key : dev repository : environment-rhos11-dev - ingress : domain : staging.openshift.example.com namespaceSubDomain : \"\" key : staging repository : env-rhos311-staging - key : production repository : env-rhos311-prod gitops : true ingress : domain : openshift.example.com namespaceSubDomain : -jx. kaniko : true repository : nexus secretStorage : local storage : logs : enabled : true url : \"http://bitbucket.openshift.example.com/scm/jx/build-logs.git\" versionStream : ref : v1.0.361 url : https://github.com/jenkins-x/jenkins-x-versions.git webhook : lighthouse Bitbucket API Token \u00b6 To authenticate with Bitbucket server, Jenkins X needs a API token of a user that has admin permissions. First, create this user API token in Bitbucket. You can do so, via Manage Account (top right menu) -> Personal access tokens -> Create a token (top right). Then use the jx create token addon 6 command to create the API token for Bitbucket server. Make sure to use the same --name <NAME> , as the gitName in your jx-requirements.yml file. Creates a new User Token for an Addon service For example, lets create the token for my configuration: jx create token addon --name bs --url http://bitbucket.openshift.example.com --api-token <API_TOKEN> <USER> This should give the following response. Created user <USER> API Token for addon server bs at http://bitbucket.openshift.example.com Installation \u00b6 Before running the Jenkins X installation with jx boot , make sure you meet the pre-requisites. Pre-requisites \u00b6 Kubernetes cluster cluster admin access to Kubernetes cluster Bitbucket server Project in Bitbucket server API token in Bitbucket server API token for Jenkins X in the Kubernetes cluster Once these are met, we can install Jenkins X via jx boot 3 . Issue with controllerbuild \u00b6 A potential issue you can run into, is that the deployment jenkins-x-controllerbuild fails to come up. could not lock config file //.gitconfig: Permission denied: failed to run 'git config --global --add user.name jenkins-x-bot' command in directory '' , The issue here, seems to be some missing configuration, as the the two / 's in //.gitconfig , give the idea there's supposed to be some folder defined. A way to solve this, is to ensure we have a home folder git can write into, and tell git where this home folder is. The image seems to set its working directory to /home/jenkins , so lets use that. In order to tell git where to write its configuration to, we can set the HOME environment variable. So in the jenkins-x-controllerbuild deployment, set the HOME environment variable to /home/jenkins . - name : HOME value : /home/jenkins Add folder for home/jenkins via volume and volumeMount. volumeMounts : - mountPath : /home/jenkins name : jenkinshome volumes : - name : jenkinshome emptyDir : {} Errata \u00b6 Import & Quickstarts Source Repositories Always HTTPS \u00b6 When you add applications to Jenkins X, either via the jx import or jx create quickstart processes, a SourceRepository CRD gets created. This resource will contain the the value spec.httpCloneURL . This is used in the Tekton pipelines for cloning the repository. This httpCloneURL is always set to https:// , even if the repository url is http:// . To retrieve the existing source repositories, you can do the following: kubectl get sourcerepository You can edit a specific source repository via: kubectl edit sourcerepository jx-jx-go And if required, change the https:// into a http:// . PullRequest Updates \u00b6 Bitbucket Server does not send a specific webhook when there's an update to a branch participating in a PullRequest. It only sends a generic Push event, which does not give Jenkins X the information required to trigger a new build for the specific PullRequest. Atlassian has recently add this feature in Bitbucket Server 7.0.0 , confirmed by the March 5 th update in this Jira ticket . As of March 2020, this is not yet supported by Jenkins X, nor is it expected at this point in time to find its way into earlier releases (such as 6.x) of Bitbucket server. References \u00b6 https://jenkins-x.io/docs/reference/components/lighthouse/ \u21a9 \u21a9 https://jenkins-x.io/docs/reference/components/lighthouse/#bitbucket-server \u21a9 https://jenkins-x.io/docs/getting-started/setup/boot/how-it-works/ \u21a9 \u21a9 https://jenkins-x.io/docs/getting-started/setup/boot/#bitbucket-server \u21a9 https://jenkins-x.io/docs/reference/config/config/#config.jenkins.io/v1.ClusterConfig \u21a9 https://jenkins-x.io/commands/jx_create_token_addon/ \u21a9","title":"Lighthouse & Bitbucket"},{"location":"jenkinsx/lighthouse-bitbucket/#jenkins-x-lighthouse-bitbucket","text":"This guide is about using Jenkins X with Lighthouse 1 as webhook manager and Bitbucket for the environment repositories 2 .","title":"Jenkins X - Lighthouse &amp; Bitbucket"},{"location":"jenkinsx/lighthouse-bitbucket/#run-bitbucket-on-kubernetes","text":"Unfortunately, Atlassian doesn't have an officially supported Bitbucket for Kubernetes. So I've taken the courtesy of creating my own basic configuration - read, not production ready. service.yaml apiVersion : v1 kind : Service metadata : labels : app : bitbucket name : bitbucket namespace : default spec : ports : - name : http port : 80 protocol : TCP targetPort : http selector : app : bitbucket sessionAffinity : None type : ClusterIP ingress.yaml I've taken the assumption that your cluster supports Ingress resources (even if its an OpenShift cluster). apiVersion : extensions/v1beta1 kind : Ingress metadata : name : bitbucket namespace : default spec : rules : - host : bitbucket.openshift.example.com http : paths : - backend : serviceName : bitbucket servicePort : 80 stateful-set.yaml apiVersion : apps/v1 kind : StatefulSet metadata : name : bitbucket namespace : default spec : serviceName : \"bitbucket\" replicas : 1 selector : matchLabels : app : bitbucket template : metadata : labels : app : bitbucket spec : containers : - name : bitbucket image : atlassian/bitbucket-server:7.0.0 ports : - containerPort : 7990 name : http - containerPort : 7999 name : web volumeMounts : - name : data mountPath : /var/atlassian/application-data/bitbucket volumeClaimTemplates : - metadata : name : data spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 5Gi","title":"Run Bitbucket on Kubernetes"},{"location":"jenkinsx/lighthouse-bitbucket/#jx-boot-configuration","text":"We use jx boot 3 to install Jenkins X. If we want to use Bitbucket for the environment repositories, we have to use Lighthouse 1 4 . In order to jx to install correctly, we have configure several parameters in the jx-requirements.yml with specific values. See the docs for all the possible values 5 . webhook: lighthouse : we have to set the webhook manager to lighthouse , as Prow only works with GitHub environmentGitOwner: jx : the project in Bitbucket where the repositories need to be created gitKind: bitbucketserver : the kind of git server, in this case bitbucketserver , because bitbucket refers to Bitbucket Cloud gitName: bs : the name for our gitserver configuration gitServer: http://bitbucket.openshift.example.com : the url to our Bitbucket Server registry: docker.io : when not using a Public Cloud provider, you have to specify the docker registry URL, in this case, Dockerhub (which is docker.io ) dockerRegistryOrg: caladreas : when the docker registry owner - in my case, caladreas - is different from the git repository owner, you have to specify this via dockerRegistryOrg We also have to set the storage for at least the logs. If we do not configure the storage for our logs, they will be assumed to be written to github pages of our application. That is, regardless of where our application resides. So, if you use anything other than GitHub (cloud), you have to configure the logs storage. The easiest solution, is to create a seperate repository for the build logs in your Bitbucket Server project. storage : logs : enabled : true url : \"http://bitbucket.openshift.example.com/scm/jx/build-logs.git\" If you have forgotten to set the storage before the installation, you can rectify this afterwards via the jx edit storage command. jx edit storage -c logs --git-url http://bitbucket.openshift.kearos.net/scm/jx/build-logs.git --git-branch master jx-requirements.yml bootConfigURL : https://github.com/jenkins-x/jenkins-x-boot-config.git cluster : clusterName : rhos11 devEnvApprovers : - jvandergriendt environmentGitOwner : jx gitKind : bitbucketserver gitName : bs gitServer : http://bitbucket.openshift.example.com namespace : jx provider : kubernetes registry : docker.io dockerRegistryOrg : caladreas environments : - ingress : domain : openshift.example.com namespaceSubDomain : -jx. key : dev repository : environment-rhos11-dev - ingress : domain : staging.openshift.example.com namespaceSubDomain : \"\" key : staging repository : env-rhos311-staging - key : production repository : env-rhos311-prod gitops : true ingress : domain : openshift.example.com namespaceSubDomain : -jx. kaniko : true repository : nexus secretStorage : local storage : logs : enabled : true url : \"http://bitbucket.openshift.example.com/scm/jx/build-logs.git\" versionStream : ref : v1.0.361 url : https://github.com/jenkins-x/jenkins-x-versions.git webhook : lighthouse","title":"JX Boot Configuration"},{"location":"jenkinsx/lighthouse-bitbucket/#bitbucket-api-token","text":"To authenticate with Bitbucket server, Jenkins X needs a API token of a user that has admin permissions. First, create this user API token in Bitbucket. You can do so, via Manage Account (top right menu) -> Personal access tokens -> Create a token (top right). Then use the jx create token addon 6 command to create the API token for Bitbucket server. Make sure to use the same --name <NAME> , as the gitName in your jx-requirements.yml file. Creates a new User Token for an Addon service For example, lets create the token for my configuration: jx create token addon --name bs --url http://bitbucket.openshift.example.com --api-token <API_TOKEN> <USER> This should give the following response. Created user <USER> API Token for addon server bs at http://bitbucket.openshift.example.com","title":"Bitbucket API Token"},{"location":"jenkinsx/lighthouse-bitbucket/#installation","text":"Before running the Jenkins X installation with jx boot , make sure you meet the pre-requisites.","title":"Installation"},{"location":"jenkinsx/lighthouse-bitbucket/#pre-requisites","text":"Kubernetes cluster cluster admin access to Kubernetes cluster Bitbucket server Project in Bitbucket server API token in Bitbucket server API token for Jenkins X in the Kubernetes cluster Once these are met, we can install Jenkins X via jx boot 3 .","title":"Pre-requisites"},{"location":"jenkinsx/lighthouse-bitbucket/#issue-with-controllerbuild","text":"A potential issue you can run into, is that the deployment jenkins-x-controllerbuild fails to come up. could not lock config file //.gitconfig: Permission denied: failed to run 'git config --global --add user.name jenkins-x-bot' command in directory '' , The issue here, seems to be some missing configuration, as the the two / 's in //.gitconfig , give the idea there's supposed to be some folder defined. A way to solve this, is to ensure we have a home folder git can write into, and tell git where this home folder is. The image seems to set its working directory to /home/jenkins , so lets use that. In order to tell git where to write its configuration to, we can set the HOME environment variable. So in the jenkins-x-controllerbuild deployment, set the HOME environment variable to /home/jenkins . - name : HOME value : /home/jenkins Add folder for home/jenkins via volume and volumeMount. volumeMounts : - mountPath : /home/jenkins name : jenkinshome volumes : - name : jenkinshome emptyDir : {}","title":"Issue with controllerbuild"},{"location":"jenkinsx/lighthouse-bitbucket/#errata","text":"","title":"Errata"},{"location":"jenkinsx/lighthouse-bitbucket/#import-quickstarts-source-repositories-always-https","text":"When you add applications to Jenkins X, either via the jx import or jx create quickstart processes, a SourceRepository CRD gets created. This resource will contain the the value spec.httpCloneURL . This is used in the Tekton pipelines for cloning the repository. This httpCloneURL is always set to https:// , even if the repository url is http:// . To retrieve the existing source repositories, you can do the following: kubectl get sourcerepository You can edit a specific source repository via: kubectl edit sourcerepository jx-jx-go And if required, change the https:// into a http:// .","title":"Import &amp; Quickstarts Source Repositories Always HTTPS"},{"location":"jenkinsx/lighthouse-bitbucket/#pullrequest-updates","text":"Bitbucket Server does not send a specific webhook when there's an update to a branch participating in a PullRequest. It only sends a generic Push event, which does not give Jenkins X the information required to trigger a new build for the specific PullRequest. Atlassian has recently add this feature in Bitbucket Server 7.0.0 , confirmed by the March 5 th update in this Jira ticket . As of March 2020, this is not yet supported by Jenkins X, nor is it expected at this point in time to find its way into earlier releases (such as 6.x) of Bitbucket server.","title":"PullRequest Updates"},{"location":"jenkinsx/lighthouse-bitbucket/#references","text":"https://jenkins-x.io/docs/reference/components/lighthouse/ \u21a9 \u21a9 https://jenkins-x.io/docs/reference/components/lighthouse/#bitbucket-server \u21a9 https://jenkins-x.io/docs/getting-started/setup/boot/how-it-works/ \u21a9 \u21a9 https://jenkins-x.io/docs/getting-started/setup/boot/#bitbucket-server \u21a9 https://jenkins-x.io/docs/reference/config/config/#config.jenkins.io/v1.ClusterConfig \u21a9 https://jenkins-x.io/commands/jx_create_token_addon/ \u21a9","title":"References"},{"location":"jenkinsx/maven/","text":"Jenkins X + Maven + Nexus \u00b6 The goal of this article it to demonstrate how Jenkins X works with Maven and Sonatype Nexus . Unless you configure otherwise, Jenkins X comes with a Nexus instance pre-configure out-of-the-box. Create Jenkins X Cluster \u00b6 Static \u00b6 See: Example \u00b6 Here's an example for creating a standard Jenkins X installation in Google Cloud with GKE. This example uses Google Cloud and it's CLI, gcloud . Where: CLUSTER_NAME : the name of your GKE cluster PROJECT : the project ID of your Google Project/account ( gcloud config list ) REGION : the region in Google Cloud where you want to run this cluster, if you don't know, use us-east1 jx create cluster gke \\ --cluster-name ${ CLUSTER_NAME } \\ --project-id ${ PROJECT } \\ --region ${ REGION } \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --skip-login \\ --batch-mode Serverless \u00b6 See: Nexus \u00b6 Open \u00b6 To open Nexus' Web UI, you can use jx open to see it's URL. By default, the URL will be nexus.jx.<domain> , for example http://nexus.jx.${LB_IP}.nip.io . I recommend using a proper Domain name and use TLS via Let's Encrypt. Jenkins X has built in support for this, via the jx upgrade ingress command. This is food for another article though. Credentials \u00b6 The username will be admin , the password depends on you. If you specified the --default-admin-password , it will be that. If you didn't specify the password, you can find it in a Kubernetes secret. kubectl get secret nexus -o yaml Which should look like this: apiVersion : v1 data : password : YWRtaW4= kind : Secret To retrieve the password, we have to decode the value of password with Base64. On Mac or Linux, this should be as easy as the command below. echo \"YWRtaW4=\" | base64 -D Use \u00b6 Log in as Administrator and you get two views. Either browse, which allows you to discover and inspect packages. Or, Administrate (the Gear Icon) which allows you to manage the repositories. For more information, read the Nexus 3 documentation . Use Nexus with Maven in Jenkins X \u00b6 Maven Library \u00b6 Steps \u00b6 create new Jenkins X buildpack create new maven application import application into Jenkins X (with the Build Pack) double check job in Jenkins double check webhook in GitHub build the application in Jenkins verify package in Nexus Maven Application \u00b6 Steps \u00b6 create new maven application add repository for local dev add dependency on library build locally import application into Jenkins X build application in Jenkins How the magic works \u00b6 build image let's dig to see whats in it kubernetes secret with settings.xml maven repo maven distribution management maven mirror How would you do this yourself \u00b6 Options \u00b6 adjust Jenkins X's solution bridge Jenkins X's solution to your existing repo's create something yourself Adjust Jenkins X solution \u00b6 ? Bridge to existing \u00b6 Only to external \u00b6 Library \u00b6 buildpack https://github.com/jenkins-x-buildpacks/jenkins-x-classic/blob/master/packs/maven/pipeline.yaml Create new application \u00b6 mvn archetype:generate -DarchetypeGroupId = org.apache.maven.archetypes -DarchetypeArtifactId = maven-archetype-quickstart -DarchetypeVersion = 1 .4 Import JX \u00b6 jx import --pack maven-lib -b Do we need it? \u00b6 Seems to work without it as well. Perhaps its inside the build image? Edit secret \u00b6 secret: jenkins-maven-settings add labels: jenkins.io/credentials-type: secretFile https://jenkinsci.github.io/kubernetes-credentials-provider-plugin/examples/ https://jenkinsci.github.io/kubernetes-credentials-provider-plugin/ Pom xml config \u00b6 In order to publish stuff, we need to make sure we have our distribution config setup. <profiles> <profile> <id> jx-nexus </id> <distributionManagement> <repository> <id> nexus </id> <name> nexus </name> <url> ${altReleaseDeploymentRepository} </url> </repository> </distributionManagement> </profile> </profiles> Pipeline example \u00b6 pipeline { agent { label \"jenkins-maven-java11\" } stages { stage ( 'Test' ) { environment { SETTINGS = credentials ( 'another-test-file2' ) } steps { sh \"echo ${SETTINGS}\" sh 'cat ${SETTINGS}' container ( 'maven' ) { sh 'mvn clean javadoc:aggregate verify -C -e' sh \"mvn deploy --show-version --errors --activate-profiles jx-nexus --strict-checksums --settings ${SETTINGS}\" } } } } } Config example \u00b6 apiVersion : v1 kind : Secret metadata : # this is the jenkins id. name : \"another-test-file2\" labels : # so we know what type it is. \"jenkins.io/credentials-type\" : \"secretFile\" annotations : # description - can not be a label as spaces are not allowed \"jenkins.io/credentials-description\" : \"secret file credential from Kubernetes\" type : Opaque stringData : filename : mySecret.txt data : # base64 encoded bytes data : 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 Create App \u00b6 create new java application with maven or gradle add dependency add repo: https://nexus.jx.kearos.net/repository/maven-public/ Know Issues \u00b6 Jenkins X doesn't have a kubernetes buildpack for Maven libraries, so I'm not sure how to import that directly which is why, for now, we create a new build pack first Jenkins X cannot import more than one application into static Jenkins within the same folder requires GitHub issue + PR","title":"Maven"},{"location":"jenkinsx/maven/#jenkins-x-maven-nexus","text":"The goal of this article it to demonstrate how Jenkins X works with Maven and Sonatype Nexus . Unless you configure otherwise, Jenkins X comes with a Nexus instance pre-configure out-of-the-box.","title":"Jenkins X + Maven + Nexus"},{"location":"jenkinsx/maven/#create-jenkins-x-cluster","text":"","title":"Create Jenkins X Cluster"},{"location":"jenkinsx/maven/#static","text":"See:","title":"Static"},{"location":"jenkinsx/maven/#example","text":"Here's an example for creating a standard Jenkins X installation in Google Cloud with GKE. This example uses Google Cloud and it's CLI, gcloud . Where: CLUSTER_NAME : the name of your GKE cluster PROJECT : the project ID of your Google Project/account ( gcloud config list ) REGION : the region in Google Cloud where you want to run this cluster, if you don't know, use us-east1 jx create cluster gke \\ --cluster-name ${ CLUSTER_NAME } \\ --project-id ${ PROJECT } \\ --region ${ REGION } \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --skip-login \\ --batch-mode","title":"Example"},{"location":"jenkinsx/maven/#serverless","text":"See:","title":"Serverless"},{"location":"jenkinsx/maven/#nexus","text":"","title":"Nexus"},{"location":"jenkinsx/maven/#open","text":"To open Nexus' Web UI, you can use jx open to see it's URL. By default, the URL will be nexus.jx.<domain> , for example http://nexus.jx.${LB_IP}.nip.io . I recommend using a proper Domain name and use TLS via Let's Encrypt. Jenkins X has built in support for this, via the jx upgrade ingress command. This is food for another article though.","title":"Open"},{"location":"jenkinsx/maven/#credentials","text":"The username will be admin , the password depends on you. If you specified the --default-admin-password , it will be that. If you didn't specify the password, you can find it in a Kubernetes secret. kubectl get secret nexus -o yaml Which should look like this: apiVersion : v1 data : password : YWRtaW4= kind : Secret To retrieve the password, we have to decode the value of password with Base64. On Mac or Linux, this should be as easy as the command below. echo \"YWRtaW4=\" | base64 -D","title":"Credentials"},{"location":"jenkinsx/maven/#use","text":"Log in as Administrator and you get two views. Either browse, which allows you to discover and inspect packages. Or, Administrate (the Gear Icon) which allows you to manage the repositories. For more information, read the Nexus 3 documentation .","title":"Use"},{"location":"jenkinsx/maven/#use-nexus-with-maven-in-jenkins-x","text":"","title":"Use Nexus with Maven in Jenkins X"},{"location":"jenkinsx/maven/#maven-library","text":"","title":"Maven Library"},{"location":"jenkinsx/maven/#steps","text":"create new Jenkins X buildpack create new maven application import application into Jenkins X (with the Build Pack) double check job in Jenkins double check webhook in GitHub build the application in Jenkins verify package in Nexus","title":"Steps"},{"location":"jenkinsx/maven/#maven-application","text":"","title":"Maven Application"},{"location":"jenkinsx/maven/#steps_1","text":"create new maven application add repository for local dev add dependency on library build locally import application into Jenkins X build application in Jenkins","title":"Steps"},{"location":"jenkinsx/maven/#how-the-magic-works","text":"build image let's dig to see whats in it kubernetes secret with settings.xml maven repo maven distribution management maven mirror","title":"How the magic works"},{"location":"jenkinsx/maven/#how-would-you-do-this-yourself","text":"","title":"How would you do this yourself"},{"location":"jenkinsx/maven/#options","text":"adjust Jenkins X's solution bridge Jenkins X's solution to your existing repo's create something yourself","title":"Options"},{"location":"jenkinsx/maven/#adjust-jenkins-x-solution","text":"?","title":"Adjust Jenkins X solution"},{"location":"jenkinsx/maven/#bridge-to-existing","text":"","title":"Bridge to existing"},{"location":"jenkinsx/maven/#only-to-external","text":"","title":"Only to external"},{"location":"jenkinsx/maven/#library","text":"buildpack https://github.com/jenkins-x-buildpacks/jenkins-x-classic/blob/master/packs/maven/pipeline.yaml","title":"Library"},{"location":"jenkinsx/maven/#create-new-application","text":"mvn archetype:generate -DarchetypeGroupId = org.apache.maven.archetypes -DarchetypeArtifactId = maven-archetype-quickstart -DarchetypeVersion = 1 .4","title":"Create new application"},{"location":"jenkinsx/maven/#import-jx","text":"jx import --pack maven-lib -b","title":"Import JX"},{"location":"jenkinsx/maven/#do-we-need-it","text":"Seems to work without it as well. Perhaps its inside the build image?","title":"Do we need it?"},{"location":"jenkinsx/maven/#edit-secret","text":"secret: jenkins-maven-settings add labels: jenkins.io/credentials-type: secretFile https://jenkinsci.github.io/kubernetes-credentials-provider-plugin/examples/ https://jenkinsci.github.io/kubernetes-credentials-provider-plugin/","title":"Edit secret"},{"location":"jenkinsx/maven/#pom-xml-config","text":"In order to publish stuff, we need to make sure we have our distribution config setup. <profiles> <profile> <id> jx-nexus </id> <distributionManagement> <repository> <id> nexus </id> <name> nexus </name> <url> ${altReleaseDeploymentRepository} </url> </repository> </distributionManagement> </profile> </profiles>","title":"Pom xml config"},{"location":"jenkinsx/maven/#pipeline-example","text":"pipeline { agent { label \"jenkins-maven-java11\" } stages { stage ( 'Test' ) { environment { SETTINGS = credentials ( 'another-test-file2' ) } steps { sh \"echo ${SETTINGS}\" sh 'cat ${SETTINGS}' container ( 'maven' ) { sh 'mvn clean javadoc:aggregate verify -C -e' sh \"mvn deploy --show-version --errors --activate-profiles jx-nexus --strict-checksums --settings ${SETTINGS}\" } } } } }","title":"Pipeline example"},{"location":"jenkinsx/maven/#config-example","text":"apiVersion : v1 kind : Secret metadata : # this is the jenkins id. name : \"another-test-file2\" labels : # so we know what type it is. \"jenkins.io/credentials-type\" : \"secretFile\" annotations : # description - can not be a label as spaces are not allowed \"jenkins.io/credentials-description\" : \"secret file credential from Kubernetes\" type : Opaque stringData : filename : mySecret.txt data : # base64 encoded bytes data : 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","title":"Config example"},{"location":"jenkinsx/maven/#create-app","text":"create new java application with maven or gradle add dependency add repo: https://nexus.jx.kearos.net/repository/maven-public/","title":"Create App"},{"location":"jenkinsx/maven/#know-issues","text":"Jenkins X doesn't have a kubernetes buildpack for Maven libraries, so I'm not sure how to import that directly which is why, for now, we create a new build pack first Jenkins X cannot import more than one application into static Jenkins within the same folder requires GitHub issue + PR","title":"Know Issues"},{"location":"jenkinsx/multi-cluster/","text":"Multi Cluster \u00b6 Namespace & TLS \u00b6 get DNS tokens from jx get cert/issuer from jx kubectl label namespace jx-staging certmanager.k8s.io/disable-validation=\"true\"","title":"Multi cluster"},{"location":"jenkinsx/multi-cluster/#multi-cluster","text":"","title":"Multi Cluster"},{"location":"jenkinsx/multi-cluster/#namespace-tls","text":"get DNS tokens from jx get cert/issuer from jx kubectl label namespace jx-staging certmanager.k8s.io/disable-validation=\"true\"","title":"Namespace &amp; TLS"},{"location":"jenkinsx/rhos-311-minimal/","text":"Jenkins X on RedHat OpenShift 3.11 \u00b6 Why Jenkins X on RedHat OpenShift 3.11? Well, not everyone can use public cloud solutions. So, in order to help out those running OpenShift 3.11 and want to leverage Jenkins X, read along. Note This guide is written early March 2020, using jx version 2.0.1212 and OpenShift version v3.11.170 . The OpenShift used is installed on GCP in a minimal fashion , so some shortcuts are taken. For example, there's only one user, the Cluster Admin. This isn't likely in a production cluster, but it is a start. Pre-requisites \u00b6 jx binary kubectl is 1.16.x or less Helm v2 running OpenShift cluster with cluster admin access (will update how to avoid this) GitHub account If you're like me, you're likely managing your packages via a package manager such as Homebrew or Chocolatey. This means you might run newer versions of Helm and kubectl and need to downgrade them. See below how! Caution If you run this in an on-premises solution or otherwise cannot contact GitHub, you have to use Lighthouse for managing the webhooks. As of March 2020, the support for Bitbucket Server is missing some features read here on what you can about that . Meanwhile, we suggest you either use GitHub Enterprise or GitLab as alternatives with better support. Temporarily set Helm V2 \u00b6 Download Helm v2 release from Helms GitHub Releases page . Place the binary somewhere, for example $HOME/Resource/helm2 . Then set your path with the location of Helm v2 first, before including the whole path to ensure Helm v2 is found first. PATH = $HOME /Resources/helm2: $PATH Ensure you're now running helm 2 by the command below: helm version --client It should show this: Client: & version.Version { SemVer: \"v2.16.1\" , GitCommit: \"bbdfe5e7803a12bbdf97e94cd847859890cf4050\" , GitTreeState: \"clean\" } Downgrade Kubctl \u00b6 Downgrade kubectl (need lower than 1.17): curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.16.7/bin/darwin/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl To confirm your kubectl version is as expected, run the command below: kubectl version --client The output should be as follows: Client Version: version.Info { Major: \"1\" , Minor: \"16\" , GitVersion: \"v1.16.7\" , GitCommit: \"be3d344ed06bff7a4fc60656200a93c74f31f9a4\" , GitTreeState: \"clean\" , BuildDate: \"2020-02-11T19:34:02Z\" , GoVersion: \"go1.13.6\" , Compiler: \"gc\" , Platform: \"darwin/amd64\" } Install Via Boot \u00b6 The current (as of March 2020) recommended way of installing Jenkins X, is via jx boot . Boot Configuration \u00b6 provider: kubernetes : Normally, this is set to your cloud provider. in order to stay close to Kubernetes itself and thus OpenShift, we set this to kubernetes registry: docker.io : If you're on a public cloud vender, jx boot creates a docker registry for you (GCR on GCP, ACR on AWS, and so on), in this example we leverage Docker Hub ( docker.io ). This should be indicative for any self-hosted registry as well! dockerRegistryOrg: caladreas : when the docker registry owner - in my case, caladreas - is different from the git repository owner, you have to specify this via dockerRegistryOrg secretStorage: local : Thre recommended approach is to use the HashiCorp Vault integration, but that isn't supported on OpenShift webhook: prow : This uses Prow for webhook management. In March 2020 the best option to use with GitHub. If you want to use Bitbucket read my guide on jx with lighthouse & bitbucket . jx-requirements.yaml autoUpdate : enabled : false bootConfigURL : https://github.com/jenkins-x/jenkins-x-boot-config.git cluster : clusterName : rhos11 environmentGitOwner : <GitHub User> gitKind : github gitName : github gitServer : https://github.com namespace : jx provider : kubernetes registry : docker.io dockerRegistryOrg : caladreas environments : - ingress : domain : openshift.kearos.net externalDNS : false ignoreLoadBalancer : true namespaceSubDomain : -jx. key : dev repository : environment-rhos11-dev - ingress : domain : \"staging.openshift.kearos.net\" namespaceSubDomain : \"\" key : staging repository : env-rhos311-staging - ingress : domain : \"openshift.kearos.net\" namespaceSubDomain : \"\" key : production repository : env-rhos311-prod gitops : true ingress : domain : openshift.example.com externalDNS : false ignoreLoadBalancer : true namespaceSubDomain : -jx. kaniko : true repository : nexus secretStorage : local versionStream : ref : v1.0.361 url : https://github.com/jenkins-x/jenkins-x-versions.git webhook : prow Jx Boot \u00b6 Go to a directory where you want to clone the development environment repository. Create the initial configuration file, jx-requirements.yml , and run the initial jx boot iteration. jx boot It will ask you if you want to clone the jenkins x boot config repository: ? Do you want to clone the Jenkins X Boot Git repository? [ ? for help ] ( Y/n ) Say yes, and it will clone the configuration repository and start the jx boot pipeline. It will fail, because not all values are copied from your jx-requirements.yml into the new cloned repository. To resolve this, go into the new cloned repository and replace the values of jx-requirements.yml with your configuration. Once done, restart the installation. jx boot Failed to install certmanager \u00b6 Jenkins X will fail to install Certmanager, because it relies on newer API components from Kubernetes than are available in OpenShift 3.11. The 11 of 3.11 refers to Kubernetes 1.11 . Certmanager requires 1.12 +. To disable the installation of certmanager, we edit the jenkins-x.yml , which is the pipeline executed by jx boot . We have to remove the step , that tries to install certmanager; install-cert-manager-crds . The block of code we have to remove, is as follows: - args : - apply - --wait - --validate=false - -f - https://raw.githubusercontent.com/jetstack/cert-manager/release-0.11/deploy/manifests/00-crds.yaml command : kubectl dir : /workspace/source env : - name : DEPLOY_NAMESPACE value : cert-manager name : install-cert-manager-crds Once done, we can run jx boot again. Pipeline Runner faillure \u00b6 For me, the pipeline runner deployment failed, failing the jx boot process - when it validates if everything came up. pipelinerunner-74897865f5-2k4vb 0 /1 CrashLoopBackOff 55 4h error: unable to clone version dir: unable to create temp dir for version stream: mkdir /tmp/jx-version-repo-083799486: permission denied A solution, is add a volume to the pipelinerunner deployment, which mounts an emptyDir 1 to at /tmp . volumeMounts : - mountPath : /tmp name : cache-volume volumes : - name : cache-volume emptyDir : {} Once the pipelinerunner pod is running, rerun the jx boot installation. jx boot It should now succeed with cluster ok . Create Quickstart \u00b6 To validate Jenkins X works as it should, the first step is to create a quickstart 2 3 . For simplicity, lets stick to a Go (lang) project. jx create quickstart --filter golang-http --project-name jx-go-rhos311 --batch-mode This creates a new repository based on the quickstart for Go (lang) 4 and the build pack for Go (lang) 5 . I ran into two issues: Tekton is not mounting my Docker registry credentials, thus the Kaniko build fails with 401: not authenticated the expose controller 6 is using Ingress resources by default, but doesn't want to create those on OpenShift 7 Once the issues below are solved, the application is runing in the staging environment. You can view the applications in your cluster as follows: jx get application Which should look something like this: APPLICATION STAGING PODS URL jx-go 0 .0.1 1 /1 http://jx-go-jx-staging.staging.openshift.example.com Missing Docker Credentials \u00b6 I used Docker hub as my Docker registry, but this applies to any other self-hosted Docker registry. We have to do the following: create a docker-registry secret in Kubernetes, with the credentials to our Docker registry (dockerhub or otherwise) mount this secret in a location Kaniko picks it up kubectl create secret docker-registry kaniko-secret --docker-username = <username> --docker-password = <password> --docker-email = <email-address> Mount docker hub secret as json in classic Kaniko style. pipelineConfig : env : - name : DOCKER_CONFIG value : /root/.docker/ pipelines : overrides : - pipeline : release stage : build name : container-build volumes : - name : kaniko-secret secret : defaultMode : 420 secretName : kaniko-secret items : - key : .dockerconfigjson path : config.json containerOptions : volumeMounts : - mountPath : /root/.docker name : kaniko-secret This should be enough. But if Kaniko still runs into a 401 unathenticated error, you have to change the ConfigMap for the builder PodTemplate. For example, if you use Go with the go build pack, your build container will use the jenkins-x-pod-template-go config map. This contains some environment variables related to Docker. If you still have issues, remove these environment variables. those pod template configmaps are to support traditional jenkins servers so we don't really need much from them anymore with tekton, though if we delete them things fail for now so need to keep them, but just try and remove all the DOCKER related stuff from the configmap kubectl edit cm jenkins-x-pod-template-go Expose Controller Options \u00b6 When the build succeeds, Jenkins X makes a PullRequest to your environment repository. By default, the first one is Staging , which will automatically promote 10 and run the application. This fails, because the default setting of the staging environment, is to expose the applications via the Expose Controller with an Ingress resource. Currently (March 2020), the Expose Controller assumes OpenShift cannot handle Ingress resources 7 . So there's two options here: configure the Expose Controller to use a Route to expose an application 11 customize the Expose Controller to only issue a warning when using exposer: Ingress on OpenShift environment If you choose option one, change the value of exposer from Ingress to Route of the env/values.yaml . env/values.yaml expose : Annotations : helm.sh/hook : post-install,post-upgrade helm.sh/hook-delete-policy : hook-succeeded Args : - --v - 4 config : domain : staging.openshift.staging.example.com exposer : Route http : \"true\" tlsacme : \"false\" urltemplate : '{{.Service}}.{{.Domain}}' If you choose option 2, fork the Expose Controller repository and change the line that stops it from creating Ingress resources 7 . As can be seen here: https://github.com/jenkins-x/exposecontroller/blob/master/exposestrategy/ingress.go#L48 if t == openShift { return nil , errors . New ( \"ingress strategy is not supported on OpenShift, please use Route strategy\" ) } And the following steps: new local build create and push Docker image to a registry accessable in the cluster create a new helm package, in charts directory, execute helm package exposecontroller upload Helm chart somewhere, for example, a GitHub repository update the env/requirements.yaml to use your helm chart instead of the Jenkins X one for the Expose Controller For example: env/requirements.yaml dependencies : # - alias: expose # name: exposecontroller # repository: http://chartmuseum.jenkins-x.io # version: 2.3.118 - alias : expose name : exposecontroller version : 2.3.109 repository : https://raw.githubusercontent.com/joostvdg/helm-repo/master/ Promote To Production \u00b6 To promote an application to the Production environment, we have to instruct Jenkins X to do it for us 10 . For example: jx promote jx-go-rhos311-1 --version 0 .0.34 --env production --batch-mode Aside from the Expose Controller issues, there's nothing else to be done. Just be sure to make those changes in your production environment repository. Preview Environments \u00b6 The only thing required to generate a Preview Environment in Jenkins X 12 , is to create a PullRequest to the master branch from a other branch. Wether the preview environment succeeds, depends on two things. One, does the Jenkins X service account - tekton-bot - have enough permissions to create the namespace unique to the pull request - default naming scheme is jx-<user>-<app>-pr-<prNumber> . Two, because each Preview Environment has its one Expose Controller 13 , the Expose Controller needs to be configured to either use Route or accept creating Ingress when on OpenShift. If all is done, you can retrieve the current preview environments as follows: jx get preview Which should yield something like this: PULL REQUEST NAMESPACE APPLICATION https://bitbucket.openshift.kearos.net/jx/jx-go/pull/1 jx-jx-jx-go-pr-1 http://jx-go.jx-jx-jx-go-pr-1.openshift.example.com Errata \u00b6 Registry Owner Mismatch \u00b6 It can happen that the docker registry owner is not the same for every application. If this is the case, the application will have to make a workaround after it is imported into Jenkins X (via jx import or jx create quickstart ). In order to resolve the mismatch between the default Jenkins X installation Docker registry owner and the application's owner, we need to change two things in our Jenkins X pipeline ( jenkins-x.yml ) 8 . add an override for the Docker registry owner in the jenkins-x.yml , the pipeline of your application. add an override for the container-build step of the build stage, for both the release and pullrequest pipelines. Overriding the pipeline is done by specifying the stage to override under pipelineConfig.overides 8 9 . When you set dockerRegistryOwner , it overrides the value generated elsewhere. dockerRegistryOwner : caladreas The only exception is where the image gets uploaded to via Kaniko . - --destination=docker.io/caladreas/jx-go-rhos311-1:${inputs.params.version} The end result will look like this. jenkins-x.yml dockerRegistryOwner : caladreas buildPack : go pipelineConfig : overrides : - pipeline : release stage : build name : container-build steps : - name : container-build dir : /workspace/source image : gcr.io/kaniko-project/executor:9912ccbf8d22bbafbf971124600fbb0b13b9cbd6 command : /kaniko/executor args : - --cache=true - --cache-dir=/workspace - --context=/workspace/source - --dockerfile=/workspace/source/Dockerfile - --destination=docker.io/caladreas/jx-go-rhos311-1:${inputs.params.version} - --cache-repo=docker.io/todo/cache - --skip-tls-verify-registry=docker.io - --verbosity=debug - pipeline : pullrequest stage : build name : container-build steps : - name : container-build dir : /workspace/source image : gcr.io/kaniko-project/executor:9912ccbf8d22bbafbf971124600fbb0b13b9cbd6 command : /kaniko/executor args : - --cache=true - --cache-dir=/workspace - --context=/workspace/source - --dockerfile=/workspace/source/Dockerfile - --destination=docker.io/caladreas/jx-go-rhos311-1:${inputs.params.version} - --cache-repo=docker.io/todo/cache - --skip-tls-verify-registry=docker.io - --verbosity=debug References \u00b6 https://kubernetes.io/docs/concepts/storage/volumes/#emptydir \u21a9 https://jenkins-x.io/docs/getting-started/first-project/create-quickstart/ \u21a9 https://github.com/jenkins-x-quickstarts \u21a9 https://github.com/jenkins-x-quickstarts/golang-http \u21a9 https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs \u21a9 https://jenkins-x.io/docs/concepts/technology/#whats-is-exposecontroller \u21a9 https://github.com/jenkins-x/exposecontroller/blob/master/exposestrategy/ingress.go#L48 \u21a9 \u21a9 \u21a9 https://jenkins-x.io/docs/reference/pipeline-syntax-reference/ \u21a9 \u21a9 https://technologyconversations.com/2019/06/30/overriding-pipelines-stages-and-steps-and-implementing-loops-in-jenkins-x-pipelines/ \u21a9 https://jenkins-x.io/docs/getting-started/promotion/ \u21a9 \u21a9 https://github.com/jenkins-x/exposecontroller#exposer-types \u21a9 https://jenkins-x.io/docs/getting-started/build-test-preview/#generating-a-preview-environment \u21a9 https://jenkins-x.io/docs/reference/preview/#charts \u21a9","title":"JX On OpenShift 3.11"},{"location":"jenkinsx/rhos-311-minimal/#jenkins-x-on-redhat-openshift-311","text":"Why Jenkins X on RedHat OpenShift 3.11? Well, not everyone can use public cloud solutions. So, in order to help out those running OpenShift 3.11 and want to leverage Jenkins X, read along. Note This guide is written early March 2020, using jx version 2.0.1212 and OpenShift version v3.11.170 . The OpenShift used is installed on GCP in a minimal fashion , so some shortcuts are taken. For example, there's only one user, the Cluster Admin. This isn't likely in a production cluster, but it is a start.","title":"Jenkins X on RedHat OpenShift 3.11"},{"location":"jenkinsx/rhos-311-minimal/#pre-requisites","text":"jx binary kubectl is 1.16.x or less Helm v2 running OpenShift cluster with cluster admin access (will update how to avoid this) GitHub account If you're like me, you're likely managing your packages via a package manager such as Homebrew or Chocolatey. This means you might run newer versions of Helm and kubectl and need to downgrade them. See below how! Caution If you run this in an on-premises solution or otherwise cannot contact GitHub, you have to use Lighthouse for managing the webhooks. As of March 2020, the support for Bitbucket Server is missing some features read here on what you can about that . Meanwhile, we suggest you either use GitHub Enterprise or GitLab as alternatives with better support.","title":"Pre-requisites"},{"location":"jenkinsx/rhos-311-minimal/#temporarily-set-helm-v2","text":"Download Helm v2 release from Helms GitHub Releases page . Place the binary somewhere, for example $HOME/Resource/helm2 . Then set your path with the location of Helm v2 first, before including the whole path to ensure Helm v2 is found first. PATH = $HOME /Resources/helm2: $PATH Ensure you're now running helm 2 by the command below: helm version --client It should show this: Client: & version.Version { SemVer: \"v2.16.1\" , GitCommit: \"bbdfe5e7803a12bbdf97e94cd847859890cf4050\" , GitTreeState: \"clean\" }","title":"Temporarily set Helm V2"},{"location":"jenkinsx/rhos-311-minimal/#downgrade-kubctl","text":"Downgrade kubectl (need lower than 1.17): curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.16.7/bin/darwin/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl To confirm your kubectl version is as expected, run the command below: kubectl version --client The output should be as follows: Client Version: version.Info { Major: \"1\" , Minor: \"16\" , GitVersion: \"v1.16.7\" , GitCommit: \"be3d344ed06bff7a4fc60656200a93c74f31f9a4\" , GitTreeState: \"clean\" , BuildDate: \"2020-02-11T19:34:02Z\" , GoVersion: \"go1.13.6\" , Compiler: \"gc\" , Platform: \"darwin/amd64\" }","title":"Downgrade Kubctl"},{"location":"jenkinsx/rhos-311-minimal/#install-via-boot","text":"The current (as of March 2020) recommended way of installing Jenkins X, is via jx boot .","title":"Install Via Boot"},{"location":"jenkinsx/rhos-311-minimal/#boot-configuration","text":"provider: kubernetes : Normally, this is set to your cloud provider. in order to stay close to Kubernetes itself and thus OpenShift, we set this to kubernetes registry: docker.io : If you're on a public cloud vender, jx boot creates a docker registry for you (GCR on GCP, ACR on AWS, and so on), in this example we leverage Docker Hub ( docker.io ). This should be indicative for any self-hosted registry as well! dockerRegistryOrg: caladreas : when the docker registry owner - in my case, caladreas - is different from the git repository owner, you have to specify this via dockerRegistryOrg secretStorage: local : Thre recommended approach is to use the HashiCorp Vault integration, but that isn't supported on OpenShift webhook: prow : This uses Prow for webhook management. In March 2020 the best option to use with GitHub. If you want to use Bitbucket read my guide on jx with lighthouse & bitbucket . jx-requirements.yaml autoUpdate : enabled : false bootConfigURL : https://github.com/jenkins-x/jenkins-x-boot-config.git cluster : clusterName : rhos11 environmentGitOwner : <GitHub User> gitKind : github gitName : github gitServer : https://github.com namespace : jx provider : kubernetes registry : docker.io dockerRegistryOrg : caladreas environments : - ingress : domain : openshift.kearos.net externalDNS : false ignoreLoadBalancer : true namespaceSubDomain : -jx. key : dev repository : environment-rhos11-dev - ingress : domain : \"staging.openshift.kearos.net\" namespaceSubDomain : \"\" key : staging repository : env-rhos311-staging - ingress : domain : \"openshift.kearos.net\" namespaceSubDomain : \"\" key : production repository : env-rhos311-prod gitops : true ingress : domain : openshift.example.com externalDNS : false ignoreLoadBalancer : true namespaceSubDomain : -jx. kaniko : true repository : nexus secretStorage : local versionStream : ref : v1.0.361 url : https://github.com/jenkins-x/jenkins-x-versions.git webhook : prow","title":"Boot Configuration"},{"location":"jenkinsx/rhos-311-minimal/#jx-boot","text":"Go to a directory where you want to clone the development environment repository. Create the initial configuration file, jx-requirements.yml , and run the initial jx boot iteration. jx boot It will ask you if you want to clone the jenkins x boot config repository: ? Do you want to clone the Jenkins X Boot Git repository? [ ? for help ] ( Y/n ) Say yes, and it will clone the configuration repository and start the jx boot pipeline. It will fail, because not all values are copied from your jx-requirements.yml into the new cloned repository. To resolve this, go into the new cloned repository and replace the values of jx-requirements.yml with your configuration. Once done, restart the installation. jx boot","title":"Jx Boot"},{"location":"jenkinsx/rhos-311-minimal/#failed-to-install-certmanager","text":"Jenkins X will fail to install Certmanager, because it relies on newer API components from Kubernetes than are available in OpenShift 3.11. The 11 of 3.11 refers to Kubernetes 1.11 . Certmanager requires 1.12 +. To disable the installation of certmanager, we edit the jenkins-x.yml , which is the pipeline executed by jx boot . We have to remove the step , that tries to install certmanager; install-cert-manager-crds . The block of code we have to remove, is as follows: - args : - apply - --wait - --validate=false - -f - https://raw.githubusercontent.com/jetstack/cert-manager/release-0.11/deploy/manifests/00-crds.yaml command : kubectl dir : /workspace/source env : - name : DEPLOY_NAMESPACE value : cert-manager name : install-cert-manager-crds Once done, we can run jx boot again.","title":"Failed to install certmanager"},{"location":"jenkinsx/rhos-311-minimal/#pipeline-runner-faillure","text":"For me, the pipeline runner deployment failed, failing the jx boot process - when it validates if everything came up. pipelinerunner-74897865f5-2k4vb 0 /1 CrashLoopBackOff 55 4h error: unable to clone version dir: unable to create temp dir for version stream: mkdir /tmp/jx-version-repo-083799486: permission denied A solution, is add a volume to the pipelinerunner deployment, which mounts an emptyDir 1 to at /tmp . volumeMounts : - mountPath : /tmp name : cache-volume volumes : - name : cache-volume emptyDir : {} Once the pipelinerunner pod is running, rerun the jx boot installation. jx boot It should now succeed with cluster ok .","title":"Pipeline Runner faillure"},{"location":"jenkinsx/rhos-311-minimal/#create-quickstart","text":"To validate Jenkins X works as it should, the first step is to create a quickstart 2 3 . For simplicity, lets stick to a Go (lang) project. jx create quickstart --filter golang-http --project-name jx-go-rhos311 --batch-mode This creates a new repository based on the quickstart for Go (lang) 4 and the build pack for Go (lang) 5 . I ran into two issues: Tekton is not mounting my Docker registry credentials, thus the Kaniko build fails with 401: not authenticated the expose controller 6 is using Ingress resources by default, but doesn't want to create those on OpenShift 7 Once the issues below are solved, the application is runing in the staging environment. You can view the applications in your cluster as follows: jx get application Which should look something like this: APPLICATION STAGING PODS URL jx-go 0 .0.1 1 /1 http://jx-go-jx-staging.staging.openshift.example.com","title":"Create Quickstart"},{"location":"jenkinsx/rhos-311-minimal/#missing-docker-credentials","text":"I used Docker hub as my Docker registry, but this applies to any other self-hosted Docker registry. We have to do the following: create a docker-registry secret in Kubernetes, with the credentials to our Docker registry (dockerhub or otherwise) mount this secret in a location Kaniko picks it up kubectl create secret docker-registry kaniko-secret --docker-username = <username> --docker-password = <password> --docker-email = <email-address> Mount docker hub secret as json in classic Kaniko style. pipelineConfig : env : - name : DOCKER_CONFIG value : /root/.docker/ pipelines : overrides : - pipeline : release stage : build name : container-build volumes : - name : kaniko-secret secret : defaultMode : 420 secretName : kaniko-secret items : - key : .dockerconfigjson path : config.json containerOptions : volumeMounts : - mountPath : /root/.docker name : kaniko-secret This should be enough. But if Kaniko still runs into a 401 unathenticated error, you have to change the ConfigMap for the builder PodTemplate. For example, if you use Go with the go build pack, your build container will use the jenkins-x-pod-template-go config map. This contains some environment variables related to Docker. If you still have issues, remove these environment variables. those pod template configmaps are to support traditional jenkins servers so we don't really need much from them anymore with tekton, though if we delete them things fail for now so need to keep them, but just try and remove all the DOCKER related stuff from the configmap kubectl edit cm jenkins-x-pod-template-go","title":"Missing Docker Credentials"},{"location":"jenkinsx/rhos-311-minimal/#expose-controller-options","text":"When the build succeeds, Jenkins X makes a PullRequest to your environment repository. By default, the first one is Staging , which will automatically promote 10 and run the application. This fails, because the default setting of the staging environment, is to expose the applications via the Expose Controller with an Ingress resource. Currently (March 2020), the Expose Controller assumes OpenShift cannot handle Ingress resources 7 . So there's two options here: configure the Expose Controller to use a Route to expose an application 11 customize the Expose Controller to only issue a warning when using exposer: Ingress on OpenShift environment If you choose option one, change the value of exposer from Ingress to Route of the env/values.yaml . env/values.yaml expose : Annotations : helm.sh/hook : post-install,post-upgrade helm.sh/hook-delete-policy : hook-succeeded Args : - --v - 4 config : domain : staging.openshift.staging.example.com exposer : Route http : \"true\" tlsacme : \"false\" urltemplate : '{{.Service}}.{{.Domain}}' If you choose option 2, fork the Expose Controller repository and change the line that stops it from creating Ingress resources 7 . As can be seen here: https://github.com/jenkins-x/exposecontroller/blob/master/exposestrategy/ingress.go#L48 if t == openShift { return nil , errors . New ( \"ingress strategy is not supported on OpenShift, please use Route strategy\" ) } And the following steps: new local build create and push Docker image to a registry accessable in the cluster create a new helm package, in charts directory, execute helm package exposecontroller upload Helm chart somewhere, for example, a GitHub repository update the env/requirements.yaml to use your helm chart instead of the Jenkins X one for the Expose Controller For example: env/requirements.yaml dependencies : # - alias: expose # name: exposecontroller # repository: http://chartmuseum.jenkins-x.io # version: 2.3.118 - alias : expose name : exposecontroller version : 2.3.109 repository : https://raw.githubusercontent.com/joostvdg/helm-repo/master/","title":"Expose Controller Options"},{"location":"jenkinsx/rhos-311-minimal/#promote-to-production","text":"To promote an application to the Production environment, we have to instruct Jenkins X to do it for us 10 . For example: jx promote jx-go-rhos311-1 --version 0 .0.34 --env production --batch-mode Aside from the Expose Controller issues, there's nothing else to be done. Just be sure to make those changes in your production environment repository.","title":"Promote To Production"},{"location":"jenkinsx/rhos-311-minimal/#preview-environments","text":"The only thing required to generate a Preview Environment in Jenkins X 12 , is to create a PullRequest to the master branch from a other branch. Wether the preview environment succeeds, depends on two things. One, does the Jenkins X service account - tekton-bot - have enough permissions to create the namespace unique to the pull request - default naming scheme is jx-<user>-<app>-pr-<prNumber> . Two, because each Preview Environment has its one Expose Controller 13 , the Expose Controller needs to be configured to either use Route or accept creating Ingress when on OpenShift. If all is done, you can retrieve the current preview environments as follows: jx get preview Which should yield something like this: PULL REQUEST NAMESPACE APPLICATION https://bitbucket.openshift.kearos.net/jx/jx-go/pull/1 jx-jx-jx-go-pr-1 http://jx-go.jx-jx-jx-go-pr-1.openshift.example.com","title":"Preview Environments"},{"location":"jenkinsx/rhos-311-minimal/#errata","text":"","title":"Errata"},{"location":"jenkinsx/rhos-311-minimal/#registry-owner-mismatch","text":"It can happen that the docker registry owner is not the same for every application. If this is the case, the application will have to make a workaround after it is imported into Jenkins X (via jx import or jx create quickstart ). In order to resolve the mismatch between the default Jenkins X installation Docker registry owner and the application's owner, we need to change two things in our Jenkins X pipeline ( jenkins-x.yml ) 8 . add an override for the Docker registry owner in the jenkins-x.yml , the pipeline of your application. add an override for the container-build step of the build stage, for both the release and pullrequest pipelines. Overriding the pipeline is done by specifying the stage to override under pipelineConfig.overides 8 9 . When you set dockerRegistryOwner , it overrides the value generated elsewhere. dockerRegistryOwner : caladreas The only exception is where the image gets uploaded to via Kaniko . - --destination=docker.io/caladreas/jx-go-rhos311-1:${inputs.params.version} The end result will look like this. jenkins-x.yml dockerRegistryOwner : caladreas buildPack : go pipelineConfig : overrides : - pipeline : release stage : build name : container-build steps : - name : container-build dir : /workspace/source image : gcr.io/kaniko-project/executor:9912ccbf8d22bbafbf971124600fbb0b13b9cbd6 command : /kaniko/executor args : - --cache=true - --cache-dir=/workspace - --context=/workspace/source - --dockerfile=/workspace/source/Dockerfile - --destination=docker.io/caladreas/jx-go-rhos311-1:${inputs.params.version} - --cache-repo=docker.io/todo/cache - --skip-tls-verify-registry=docker.io - --verbosity=debug - pipeline : pullrequest stage : build name : container-build steps : - name : container-build dir : /workspace/source image : gcr.io/kaniko-project/executor:9912ccbf8d22bbafbf971124600fbb0b13b9cbd6 command : /kaniko/executor args : - --cache=true - --cache-dir=/workspace - --context=/workspace/source - --dockerfile=/workspace/source/Dockerfile - --destination=docker.io/caladreas/jx-go-rhos311-1:${inputs.params.version} - --cache-repo=docker.io/todo/cache - --skip-tls-verify-registry=docker.io - --verbosity=debug","title":"Registry Owner Mismatch"},{"location":"jenkinsx/rhos-311-minimal/#references","text":"https://kubernetes.io/docs/concepts/storage/volumes/#emptydir \u21a9 https://jenkins-x.io/docs/getting-started/first-project/create-quickstart/ \u21a9 https://github.com/jenkins-x-quickstarts \u21a9 https://github.com/jenkins-x-quickstarts/golang-http \u21a9 https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs \u21a9 https://jenkins-x.io/docs/concepts/technology/#whats-is-exposecontroller \u21a9 https://github.com/jenkins-x/exposecontroller/blob/master/exposestrategy/ingress.go#L48 \u21a9 \u21a9 \u21a9 https://jenkins-x.io/docs/reference/pipeline-syntax-reference/ \u21a9 \u21a9 https://technologyconversations.com/2019/06/30/overriding-pipelines-stages-and-steps-and-implementing-loops-in-jenkins-x-pipelines/ \u21a9 https://jenkins-x.io/docs/getting-started/promotion/ \u21a9 \u21a9 https://github.com/jenkins-x/exposecontroller#exposer-types \u21a9 https://jenkins-x.io/docs/getting-started/build-test-preview/#generating-a-preview-environment \u21a9 https://jenkins-x.io/docs/reference/preview/#charts \u21a9","title":"References"},{"location":"jenkinsx/serverless/","text":"Jenkins X Serverless \u00b6 What \u00b6 Tekton Jenkins X Serverless Jenkins X Pipelines Commands \u00b6 Create Cluster \u00b6 jx create cluster gke \\ --cluster-name jx-rocks \\ --project-id $PROJECT \\ --region us-east1 \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --batch-mode Install JX \u00b6 Where Project, is Gcloud Project ID. Requires docker-registry gcr.io , else it doesn't work. jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b Notes \u00b6 jx create cluster gke cannot use kubernetes server version flag it somehow sets an empty --machineType flag instead jx install --tekton --prow requires --dockerRegistry to be set jx install --tekton --prow can be install multiple times in the same cluster to differentiate, set different namespace (which becomes part of the domain) might need to update webhooks incase env's already existed two jx serverless installs, and now jx get build logs doesn't work error: no Tekton pipelines have been triggered which match the current filter","title":"Jenkins X Serverless"},{"location":"jenkinsx/serverless/#jenkins-x-serverless","text":"","title":"Jenkins X Serverless"},{"location":"jenkinsx/serverless/#what","text":"Tekton Jenkins X Serverless Jenkins X Pipelines","title":"What"},{"location":"jenkinsx/serverless/#commands","text":"","title":"Commands"},{"location":"jenkinsx/serverless/#create-cluster","text":"jx create cluster gke \\ --cluster-name jx-rocks \\ --project-id $PROJECT \\ --region us-east1 \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --batch-mode","title":"Create Cluster"},{"location":"jenkinsx/serverless/#install-jx","text":"Where Project, is Gcloud Project ID. Requires docker-registry gcr.io , else it doesn't work. jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b","title":"Install JX"},{"location":"jenkinsx/serverless/#notes","text":"jx create cluster gke cannot use kubernetes server version flag it somehow sets an empty --machineType flag instead jx install --tekton --prow requires --dockerRegistry to be set jx install --tekton --prow can be install multiple times in the same cluster to differentiate, set different namespace (which becomes part of the domain) might need to update webhooks incase env's already existed two jx serverless installs, and now jx get build logs doesn't work error: no Tekton pipelines have been triggered which match the current filter","title":"Notes"},{"location":"jenkinsx/workshop/","text":"Jenkins X Workshop \u00b6 Create Cluster \u00b6 PROJECT = NAME = ws-feb ZONE = europe-west4 MACHINE = n1-standard-2 MIN_NODES = 3 MAX_NODES = 5 PASS = admin PREFIX = ws Warning You might want to do this: (not sure why) echo \"nexus: enabled: false \" | tee myvalues.yaml jx create cluster gke -n $NAME -p $PROJECT -z $ZONE -m $MACHINE \\ --min-num-nodes $MIN_NODES --max-num-nodes $MAX_NODES \\ --default-admin-password = $PASS \\ --default-environment-prefix $NAME Alternatively \u00b6 Info Domain will get .jx as a prefix anyway. JX_CLUSTER_NAME = joostvdg JX_DOMAIN = kearos.net JX_GIT_USER = joostvdg JX_ORG = joostvdg JX_K8S_REGION = europe-west4 JX_NAME = jx-joostvdg JX_API_TOKEN = JX_ADMIN_PSS = JX_GCE_PROJECT = jx create cluster gke \\ -n ${ JX_NAME } \\ --exposer = 'Ingress' \\ --preemptible = false \\ --cluster-name = \" ${ JX_CLUSTER_NAME } \" \\ --default-admin-password = \" ${ JX_ADMIN_PSS } \" \\ --domain = \" ${ JX_DOMAIN } \" \\ --machine-type = 'n1-standard-2' \\ --max-num-nodes = 3 \\ --min-num-nodes = 2 \\ --project-id = ${ JX_GCE_PROJECT } \\ --default-environment-prefix ${ JX_NAME } \\ --zone = \" ${ JX_ZONE } \" \\ --http = 'false' \\ --tls-acme = 'true' \\ --skip-login --prow \\ --no-tiller = 'true' \\ --vault = 'true' \\ Issues \u00b6 -b doesn't work with --vault as the config is empty --vault='true' doesn't work with https (cert-manager) because sync.go:64] Not syncing ingress jx/cm-acme-http-solver-stx47 as it does not contain necessary annotations ** also, it seems its TLS config isn't correct for some reason does TLS with cert-manager actually work? now it doesn't actually install cert-manager ? Whats up with that. Install Certmanager \u00b6 Via JX \u00b6 Updates the entire ingress configuration, installs cert-mananger, certificates, replaces ingress definitions, updates webhooks, and allows you to set a different domain name. jx upgrade ingress --cluster Manually \u00b6 This does not create certificates nor does it update the ingress defintions. kubectl create namespace cert-manager kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.6/deploy/manifests/00-crds.yaml helm install --name cert-manager --namespace cert-manager stable/cert-manager Demo App \u00b6 Post creation \u00b6 Creating GitHub webhook for joostvdg/cmg for url https://hook.jx.jx.kearos.net/hook Watch pipeline activity via: jx get activity -f cmg -w Browse the pipeline log via: jx get build logs joostvdg/cmg/master Open the Jenkins console via jx console You can list the pipelines via: jx get pipelines When the pipeline is complete: jx get applications For more help on available commands see: https://jenkins-x.io/developing/browsing/ Promote \u00b6 jx promote ${ APP } --version $VERSION --env production -b Compliance \u00b6 jx compliance run jx compliance status jx compliance logs -f jx compliance delete Chartmuseum auth faillure \u00b6 uses kubernetes secret relies on kubernetes-secret (Jenkins) plugin can have trouble with special charactes to fix, update the kubernetes secret (used by chartmuseum and the pipeline) Workshop responses \u00b6 send to Alyssa & Juni address for sending Responses \u00b6 Extra requirements \u00b6 billing needs to be enabled, else you cannot create a cluster of that size we need to test more with windows ** without admin ** different python versions","title":"Jenkins X Workshop"},{"location":"jenkinsx/workshop/#jenkins-x-workshop","text":"","title":"Jenkins X Workshop"},{"location":"jenkinsx/workshop/#create-cluster","text":"PROJECT = NAME = ws-feb ZONE = europe-west4 MACHINE = n1-standard-2 MIN_NODES = 3 MAX_NODES = 5 PASS = admin PREFIX = ws Warning You might want to do this: (not sure why) echo \"nexus: enabled: false \" | tee myvalues.yaml jx create cluster gke -n $NAME -p $PROJECT -z $ZONE -m $MACHINE \\ --min-num-nodes $MIN_NODES --max-num-nodes $MAX_NODES \\ --default-admin-password = $PASS \\ --default-environment-prefix $NAME","title":"Create Cluster"},{"location":"jenkinsx/workshop/#alternatively","text":"Info Domain will get .jx as a prefix anyway. JX_CLUSTER_NAME = joostvdg JX_DOMAIN = kearos.net JX_GIT_USER = joostvdg JX_ORG = joostvdg JX_K8S_REGION = europe-west4 JX_NAME = jx-joostvdg JX_API_TOKEN = JX_ADMIN_PSS = JX_GCE_PROJECT = jx create cluster gke \\ -n ${ JX_NAME } \\ --exposer = 'Ingress' \\ --preemptible = false \\ --cluster-name = \" ${ JX_CLUSTER_NAME } \" \\ --default-admin-password = \" ${ JX_ADMIN_PSS } \" \\ --domain = \" ${ JX_DOMAIN } \" \\ --machine-type = 'n1-standard-2' \\ --max-num-nodes = 3 \\ --min-num-nodes = 2 \\ --project-id = ${ JX_GCE_PROJECT } \\ --default-environment-prefix ${ JX_NAME } \\ --zone = \" ${ JX_ZONE } \" \\ --http = 'false' \\ --tls-acme = 'true' \\ --skip-login --prow \\ --no-tiller = 'true' \\ --vault = 'true' \\","title":"Alternatively"},{"location":"jenkinsx/workshop/#issues","text":"-b doesn't work with --vault as the config is empty --vault='true' doesn't work with https (cert-manager) because sync.go:64] Not syncing ingress jx/cm-acme-http-solver-stx47 as it does not contain necessary annotations ** also, it seems its TLS config isn't correct for some reason does TLS with cert-manager actually work? now it doesn't actually install cert-manager ? Whats up with that.","title":"Issues"},{"location":"jenkinsx/workshop/#install-certmanager","text":"","title":"Install Certmanager"},{"location":"jenkinsx/workshop/#via-jx","text":"Updates the entire ingress configuration, installs cert-mananger, certificates, replaces ingress definitions, updates webhooks, and allows you to set a different domain name. jx upgrade ingress --cluster","title":"Via JX"},{"location":"jenkinsx/workshop/#manually","text":"This does not create certificates nor does it update the ingress defintions. kubectl create namespace cert-manager kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.6/deploy/manifests/00-crds.yaml helm install --name cert-manager --namespace cert-manager stable/cert-manager","title":"Manually"},{"location":"jenkinsx/workshop/#demo-app","text":"","title":"Demo App"},{"location":"jenkinsx/workshop/#post-creation","text":"Creating GitHub webhook for joostvdg/cmg for url https://hook.jx.jx.kearos.net/hook Watch pipeline activity via: jx get activity -f cmg -w Browse the pipeline log via: jx get build logs joostvdg/cmg/master Open the Jenkins console via jx console You can list the pipelines via: jx get pipelines When the pipeline is complete: jx get applications For more help on available commands see: https://jenkins-x.io/developing/browsing/","title":"Post creation"},{"location":"jenkinsx/workshop/#promote","text":"jx promote ${ APP } --version $VERSION --env production -b","title":"Promote"},{"location":"jenkinsx/workshop/#compliance","text":"jx compliance run jx compliance status jx compliance logs -f jx compliance delete","title":"Compliance"},{"location":"jenkinsx/workshop/#chartmuseum-auth-faillure","text":"uses kubernetes secret relies on kubernetes-secret (Jenkins) plugin can have trouble with special charactes to fix, update the kubernetes secret (used by chartmuseum and the pipeline)","title":"Chartmuseum auth faillure"},{"location":"jenkinsx/workshop/#workshop-responses","text":"send to Alyssa & Juni address for sending","title":"Workshop responses"},{"location":"jenkinsx/workshop/#responses","text":"","title":"Responses"},{"location":"jenkinsx/workshop/#extra-requirements","text":"billing needs to be enabled, else you cannot create a cluster of that size we need to test more with windows ** without admin ** different python versions","title":"Extra requirements"},{"location":"kubernetes/","text":"Kubernetes \u00b6 Some resources on how to start with Kubernetes. Workshops \u00b6 My Own Jerome Petazzoni Viktor Farcic Play With Kubernetes Katacoda VMWare Slide Decks \u00b6 My Own Viktor Farcic Books \u00b6 DevOps Toolkit 2.3 - Viktor Farcic DevOps Toolkit 2.4 - Viktor Farcic DevOps Toolkit 2.5 - Viktor Farcic Kubernetes Up And Running - Joe Beda, Brendan Burns, Kelsey Hightower The Kubernetes Book - Nigel Poulton Articles \u00b6 Article Looking At Kubernetes' Reconciliation In-depth Look At Pods https://medium.com/@vikram.fugro/container-networking-interface-aka-cni-bdfe23f865cf https://itnext.io/benchmark-results-of-kubernetes-network-plugins-cni-over-10gbit-s-network-36475925a560 ? Linux basics \u00b6 Namespaces & CGroups \u00b6 https://jvns.ca/blog/2016/10/10/what-even-is-a-container/ https://www.youtube.com/watch?v=sK5i-N34im8 https://www.ianlewis.org/en/what-are-kubernetes-pods-anyway Networking \u00b6 https://itnext.io/kubernetes-networking-behind-the-scenes-39a1ab1792bb https://github.com/nleiva/kubernetes-networking-links IP Tables CIDR Explanation video Packets & Frames introduction Traefik on AWS Metrics \u00b6 https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-66936addedae Secrets \u00b6 Hashicorp Vault - Kelsey Hightower https://medium.com/qubit-engineering/kubernetes-up-integrated-secrets-configuration-5a15b9f5a6c6 Security \u00b6 RBAC 11 ways not to get hacked on Kubernetes https://www.youtube.com/channel/UCiqnRXPAAk6iv2m47odUFzw Tools to use \u00b6 Microscanner Dex - OpenID Connect solution Sonubuoy Helm Learn LUA book , lua's required for Helm 3.0 ChartMuseum Skaffold KSynch Traefik Istio Falco Prometheus Grafana Jenkins Kaniko Prow Tarmak Kube-Lego Knative Rook Stern - aggregate log rendering tool Linkerd 2","title":"Introduction"},{"location":"kubernetes/#kubernetes","text":"Some resources on how to start with Kubernetes.","title":"Kubernetes"},{"location":"kubernetes/#workshops","text":"My Own Jerome Petazzoni Viktor Farcic Play With Kubernetes Katacoda VMWare","title":"Workshops"},{"location":"kubernetes/#slide-decks","text":"My Own Viktor Farcic","title":"Slide Decks"},{"location":"kubernetes/#books","text":"DevOps Toolkit 2.3 - Viktor Farcic DevOps Toolkit 2.4 - Viktor Farcic DevOps Toolkit 2.5 - Viktor Farcic Kubernetes Up And Running - Joe Beda, Brendan Burns, Kelsey Hightower The Kubernetes Book - Nigel Poulton","title":"Books"},{"location":"kubernetes/#articles","text":"Article Looking At Kubernetes' Reconciliation In-depth Look At Pods https://medium.com/@vikram.fugro/container-networking-interface-aka-cni-bdfe23f865cf https://itnext.io/benchmark-results-of-kubernetes-network-plugins-cni-over-10gbit-s-network-36475925a560 ?","title":"Articles"},{"location":"kubernetes/#linux-basics","text":"","title":"Linux basics"},{"location":"kubernetes/#namespaces-cgroups","text":"https://jvns.ca/blog/2016/10/10/what-even-is-a-container/ https://www.youtube.com/watch?v=sK5i-N34im8 https://www.ianlewis.org/en/what-are-kubernetes-pods-anyway","title":"Namespaces &amp; CGroups"},{"location":"kubernetes/#networking","text":"https://itnext.io/kubernetes-networking-behind-the-scenes-39a1ab1792bb https://github.com/nleiva/kubernetes-networking-links IP Tables CIDR Explanation video Packets & Frames introduction Traefik on AWS","title":"Networking"},{"location":"kubernetes/#metrics","text":"https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-66936addedae","title":"Metrics"},{"location":"kubernetes/#secrets","text":"Hashicorp Vault - Kelsey Hightower https://medium.com/qubit-engineering/kubernetes-up-integrated-secrets-configuration-5a15b9f5a6c6","title":"Secrets"},{"location":"kubernetes/#security","text":"RBAC 11 ways not to get hacked on Kubernetes https://www.youtube.com/channel/UCiqnRXPAAk6iv2m47odUFzw","title":"Security"},{"location":"kubernetes/#tools-to-use","text":"Microscanner Dex - OpenID Connect solution Sonubuoy Helm Learn LUA book , lua's required for Helm 3.0 ChartMuseum Skaffold KSynch Traefik Istio Falco Prometheus Grafana Jenkins Kaniko Prow Tarmak Kube-Lego Knative Rook Stern - aggregate log rendering tool Linkerd 2","title":"Tools to use"},{"location":"kubernetes/cka-exam-prep/","text":"CKA Exam Prep \u00b6 https://github.com/dgkanatsios/CKAD-exercises https://github.com/kelseyhightower/kubernetes-the-hard-way https://github.com/walidshaari/Kubernetes-Certified-Administrator https://github.com/kubernetes/community/blob/master/contributors/devel/e2e-tests.md https://www.cncf.io/certification/cka/ https://oscon2018.container.training https://github.com/ahmetb/kubernetes-network-policy-recipes https://github.com/ramitsurana/awesome-kubernetes https://sysdig.com/blog/kubernetes-security-guide/ https://severalnines.com/blog/installing-kubernetes-cluster-minions-centos7-manage-pods-services https://docs.google.com/presentation/d/1Gp-2blk5WExI_QR59EUZdwfO2BWLJqa626mK2ej-huo/edit#slide=id.g27a78b354c_0_0 https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html Some basic commands \u00b6 kubectl -n kube-public get secrets Test network policy \u00b6 For some common recipes, look at Ahmet's recipe repository . Warning Make sure you have CNI enabled and you have a network plugin that enforces the policies. Note You can check current existing policies like this: kubectl get netpol --all-namespaces Example Ingress Policy \u00b6 kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : name : dui-network-policy namespace : dui spec : podSelector : matchLabels : app : dui distribution : server ingress : [] Run test pod \u00b6 Apply above network policy, and then test in the same dui namespace, and in the default namespace. Note Use alpine:3.6 because telnet was dropped starting 3.7. kubectl -n dui get pods -l app = dui -o wide kubectl run --rm -i -t --image = alpine:3.6 -n dui test -- sh telnet 10 .32.0.7 8888 This should now fail - timeout - due the packages being dropped. Egress \u00b6 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : dui-network-policy-egress namespace : dui spec : podSelector : matchLabels : app : dui policyTypes : - Egress egress : - ports : - port : 7777 protocol : TCP - to : - podSelector : matchLabels : app : dui Warning This should in theory, block our test pod from reading this. As it doesn't have the label app=dui . But it seems it is working just fine. Allow DNS \u00b6 If it should also be able to do DNS calls, we have to enable port 53. - ports : - port : 53 protocol : UDP - port : 53 protocol : TCP - port : 7777 protocol : TCP - to : - namespaceSelector : {} Create a test pod with curl \u00b6 kubectl run --rm -i -t --image = alpine:3.6 -n dui test -- sh apk --no-cache add curl curl 10 .32.0.11:7777/servers Run minikube cluster \u00b6 ###################### # Create The Cluster # ###################### # Make sure that your minikube version is v0.25 or higher # WARNING!!! # Some users experienced problems starting the cluster with minikuber v0.26 and v0.27. # A few of the reported issues are https://github.com/kubernetes/minikube/issues/2707 and https://github.com/kubernetes/minikube/issues/2703 # If you are experiencing problems creating a cluster, please consider downgrading to minikube v0.25. minikube start \\ --vm-driver virtualbox \\ --cpus 4 \\ --memory 12228 \\ --network-plugin = cni \\ --extra-config = kubelet.network-plugin = cni ############################### # Install Ingress and Storage # ############################### minikube addons enable ingress minikube addons enable storage-provisioner minikube addons enable default-storageclass ################## # Install Tiller # ################## kubectl create \\ -f https://raw.githubusercontent.com/vfarcic/k8s-specs/master/helm/tiller-rbac.yml \\ --record --save-config helm init --service-account tiller kubectl -n kube-system \\ rollout status deploy tiller-deploy ################## # Get Cluster IP # ################## export LB_IP = $( minikube ip ) ####################### # Install ChartMuseum # ####################### CM_ADDR = \"cm. $LB_IP .nip.io\" echo $CM_ADDR CM_ADDR_ESC = $( echo $CM_ADDR \\ | sed -e \"s@\\.@\\\\\\.@g\" ) echo $CM_ADDR_ESC helm install stable/chartmuseum \\ --namespace charts \\ --name cm \\ --values helm/chartmuseum-values.yml \\ --set ingress.hosts. \" $CM_ADDR_ESC \" ={ \"/\" } \\ --set env.secret.BASIC_AUTH_USER = admin \\ --set env.secret.BASIC_AUTH_PASS = admin kubectl -n charts \\ rollout status deploy \\ cm-chartmuseum # http \"http://$CM_ADDR/health\" # It should return `{\"healthy\":true} ###################### # Install Weave Net ## ###################### kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) \" kubectl -n kube-system rollout status daemonset weave-net Weave Net \u00b6 On minikube \u00b6 To run Weave Net on minikube, after upgrading minikube, you need to overwrite the default CNI config shipped with minikube: mkdir -p ~/.minikube/files/etc/cni/net.d/ && touch ~/.minikube/files/etc/cni.net.d/k8s.conf and then to start minikube with CNI enabled: minikube start --network-plugin=cni --extra-config=kubelet.network-plugin=cni. Afterwards, you can install Weave Net. kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) \" Install stern \u00b6 Stern - aggregate log rendering tool via brew \u00b6 brew install stern Binary release \u00b6 sudo curl -L -o /usr/local/bin/stern \\ https://github.com/wercker/stern/releases/download/1.6.0/stern_linux_amd64 sudo chmod +x /usr/local/bin/stern Sysdig \u00b6 Install Sysdig \u00b6 Run Sysdig for Kubernetes \u00b6 collect API server address collect client cert + key https://www.digitalocean.com/community/tutorials/how-to-monitor-your-ubuntu-16-04-system-with-sysdig certificate-authority: /home/joostvdg/.minikube/ca.crt server: https://192.168.99.100:8443 client-certificate: /home/joostvdg/.minikube/client.crt client-key: /home/joostvdg/.minikube/client.key sysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key sysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key syslog.severity.str = info CSysdig \u00b6 sudo csysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key From Udemy Course \u00b6","title":"CKA Exam prep"},{"location":"kubernetes/cka-exam-prep/#cka-exam-prep","text":"https://github.com/dgkanatsios/CKAD-exercises https://github.com/kelseyhightower/kubernetes-the-hard-way https://github.com/walidshaari/Kubernetes-Certified-Administrator https://github.com/kubernetes/community/blob/master/contributors/devel/e2e-tests.md https://www.cncf.io/certification/cka/ https://oscon2018.container.training https://github.com/ahmetb/kubernetes-network-policy-recipes https://github.com/ramitsurana/awesome-kubernetes https://sysdig.com/blog/kubernetes-security-guide/ https://severalnines.com/blog/installing-kubernetes-cluster-minions-centos7-manage-pods-services https://docs.google.com/presentation/d/1Gp-2blk5WExI_QR59EUZdwfO2BWLJqa626mK2ej-huo/edit#slide=id.g27a78b354c_0_0 https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html","title":"CKA Exam Prep"},{"location":"kubernetes/cka-exam-prep/#some-basic-commands","text":"kubectl -n kube-public get secrets","title":"Some basic commands"},{"location":"kubernetes/cka-exam-prep/#test-network-policy","text":"For some common recipes, look at Ahmet's recipe repository . Warning Make sure you have CNI enabled and you have a network plugin that enforces the policies. Note You can check current existing policies like this: kubectl get netpol --all-namespaces","title":"Test network policy"},{"location":"kubernetes/cka-exam-prep/#example-ingress-policy","text":"kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : name : dui-network-policy namespace : dui spec : podSelector : matchLabels : app : dui distribution : server ingress : []","title":"Example Ingress Policy"},{"location":"kubernetes/cka-exam-prep/#run-test-pod","text":"Apply above network policy, and then test in the same dui namespace, and in the default namespace. Note Use alpine:3.6 because telnet was dropped starting 3.7. kubectl -n dui get pods -l app = dui -o wide kubectl run --rm -i -t --image = alpine:3.6 -n dui test -- sh telnet 10 .32.0.7 8888 This should now fail - timeout - due the packages being dropped.","title":"Run test pod"},{"location":"kubernetes/cka-exam-prep/#egress","text":"apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : dui-network-policy-egress namespace : dui spec : podSelector : matchLabels : app : dui policyTypes : - Egress egress : - ports : - port : 7777 protocol : TCP - to : - podSelector : matchLabels : app : dui Warning This should in theory, block our test pod from reading this. As it doesn't have the label app=dui . But it seems it is working just fine.","title":"Egress"},{"location":"kubernetes/cka-exam-prep/#allow-dns","text":"If it should also be able to do DNS calls, we have to enable port 53. - ports : - port : 53 protocol : UDP - port : 53 protocol : TCP - port : 7777 protocol : TCP - to : - namespaceSelector : {}","title":"Allow DNS"},{"location":"kubernetes/cka-exam-prep/#create-a-test-pod-with-curl","text":"kubectl run --rm -i -t --image = alpine:3.6 -n dui test -- sh apk --no-cache add curl curl 10 .32.0.11:7777/servers","title":"Create a test pod with curl"},{"location":"kubernetes/cka-exam-prep/#run-minikube-cluster","text":"###################### # Create The Cluster # ###################### # Make sure that your minikube version is v0.25 or higher # WARNING!!! # Some users experienced problems starting the cluster with minikuber v0.26 and v0.27. # A few of the reported issues are https://github.com/kubernetes/minikube/issues/2707 and https://github.com/kubernetes/minikube/issues/2703 # If you are experiencing problems creating a cluster, please consider downgrading to minikube v0.25. minikube start \\ --vm-driver virtualbox \\ --cpus 4 \\ --memory 12228 \\ --network-plugin = cni \\ --extra-config = kubelet.network-plugin = cni ############################### # Install Ingress and Storage # ############################### minikube addons enable ingress minikube addons enable storage-provisioner minikube addons enable default-storageclass ################## # Install Tiller # ################## kubectl create \\ -f https://raw.githubusercontent.com/vfarcic/k8s-specs/master/helm/tiller-rbac.yml \\ --record --save-config helm init --service-account tiller kubectl -n kube-system \\ rollout status deploy tiller-deploy ################## # Get Cluster IP # ################## export LB_IP = $( minikube ip ) ####################### # Install ChartMuseum # ####################### CM_ADDR = \"cm. $LB_IP .nip.io\" echo $CM_ADDR CM_ADDR_ESC = $( echo $CM_ADDR \\ | sed -e \"s@\\.@\\\\\\.@g\" ) echo $CM_ADDR_ESC helm install stable/chartmuseum \\ --namespace charts \\ --name cm \\ --values helm/chartmuseum-values.yml \\ --set ingress.hosts. \" $CM_ADDR_ESC \" ={ \"/\" } \\ --set env.secret.BASIC_AUTH_USER = admin \\ --set env.secret.BASIC_AUTH_PASS = admin kubectl -n charts \\ rollout status deploy \\ cm-chartmuseum # http \"http://$CM_ADDR/health\" # It should return `{\"healthy\":true} ###################### # Install Weave Net ## ###################### kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) \" kubectl -n kube-system rollout status daemonset weave-net","title":"Run minikube cluster"},{"location":"kubernetes/cka-exam-prep/#weave-net","text":"","title":"Weave Net"},{"location":"kubernetes/cka-exam-prep/#on-minikube","text":"To run Weave Net on minikube, after upgrading minikube, you need to overwrite the default CNI config shipped with minikube: mkdir -p ~/.minikube/files/etc/cni/net.d/ && touch ~/.minikube/files/etc/cni.net.d/k8s.conf and then to start minikube with CNI enabled: minikube start --network-plugin=cni --extra-config=kubelet.network-plugin=cni. Afterwards, you can install Weave Net. kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) \"","title":"On minikube"},{"location":"kubernetes/cka-exam-prep/#install-stern","text":"Stern - aggregate log rendering tool","title":"Install stern"},{"location":"kubernetes/cka-exam-prep/#via-brew","text":"brew install stern","title":"via brew"},{"location":"kubernetes/cka-exam-prep/#binary-release","text":"sudo curl -L -o /usr/local/bin/stern \\ https://github.com/wercker/stern/releases/download/1.6.0/stern_linux_amd64 sudo chmod +x /usr/local/bin/stern","title":"Binary release"},{"location":"kubernetes/cka-exam-prep/#sysdig","text":"","title":"Sysdig"},{"location":"kubernetes/cka-exam-prep/#install-sysdig","text":"","title":"Install Sysdig"},{"location":"kubernetes/cka-exam-prep/#run-sysdig-for-kubernetes","text":"collect API server address collect client cert + key https://www.digitalocean.com/community/tutorials/how-to-monitor-your-ubuntu-16-04-system-with-sysdig certificate-authority: /home/joostvdg/.minikube/ca.crt server: https://192.168.99.100:8443 client-certificate: /home/joostvdg/.minikube/client.crt client-key: /home/joostvdg/.minikube/client.key sysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key sysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key syslog.severity.str = info","title":"Run Sysdig for Kubernetes"},{"location":"kubernetes/cka-exam-prep/#csysdig","text":"sudo csysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key","title":"CSysdig"},{"location":"kubernetes/cka-exam-prep/#from-udemy-course","text":"","title":"From Udemy Course"},{"location":"kubernetes/cka-exam/","text":"Certified Kubernetes Administrator Exam \u00b6","title":"Certified Kubernetes Administrator Exam"},{"location":"kubernetes/cka-exam/#certified-kubernetes-administrator-exam","text":"","title":"Certified Kubernetes Administrator Exam"},{"location":"kubernetes/dev-platform/","text":"Kubernetes As Developer Platform \u00b6 Resources \u00b6 https://medium.com/@jpcontad/a-year-of-running-kubernetes-as-a-product-7eed1204eecd","title":"Kubernetes As Developer Platform"},{"location":"kubernetes/dev-platform/#kubernetes-as-developer-platform","text":"","title":"Kubernetes As Developer Platform"},{"location":"kubernetes/dev-platform/#resources","text":"https://medium.com/@jpcontad/a-year-of-running-kubernetes-as-a-product-7eed1204eecd","title":"Resources"},{"location":"kubernetes/observability/","text":"Kubernetes Observability \u00b6 Monitoring \u00b6 Metrics Server \u00b6 Helm chart: https://github.com/helm/charts/tree/master/stable/metrics-server Home: https://github.com/kubernetes-incubator/metrics-server helm install stable/metrics-server \\ --name metrics-server \\ --version 2 .0.3 \\ --namespace metrics kubectl -n metrics \\ rollout status \\ deployment metrics-server Prometheus & Alert Manager \u00b6 Prometheus Helm Values \u00b6 server: ingress: enabled: true annotations: ingress.kubernetes.io/ssl-redirect: \"false\" nginx.ingress.kubernetes.io/ssl-redirect: \"false\" resources: limits: cpu: 100m memory: 1000Mi requests: cpu: 10m memory: 500Mi alertmanager: ingress: enabled: true annotations: ingress.kubernetes.io/ssl-redirect: \"false\" nginx.ingress.kubernetes.io/ssl-redirect: \"false\" resources: limits: cpu: 10m memory: 20Mi requests: cpu: 5m memory: 10Mi kubeStateMetrics: resources: limits: cpu: 10m memory: 50Mi requests: cpu: 5m memory: 25Mi nodeExporter: resources: limits: cpu: 10m memory: 20Mi requests: cpu: 5m memory: 10Mi pushgateway: resources: limits: cpu: 10m memory: 20Mi requests: cpu: 5m memory: 10Mi Grafana \u00b6 Application Metrics \u00b6 Resources \u00b6 https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-b190cc97f0f6 https://brancz.com/2018/01/05/prometheus-vs-heapster-vs-kubernetes-metrics-apis/ https://rancher.com/blog/2018/2018-06-26-measuring-metrics-that-matter-in-kubernetes-clusters/","title":""},{"location":"kubernetes/observability/#kubernetes-observability","text":"","title":"Kubernetes Observability"},{"location":"kubernetes/observability/#monitoring","text":"","title":"Monitoring"},{"location":"kubernetes/observability/#metrics-server","text":"Helm chart: https://github.com/helm/charts/tree/master/stable/metrics-server Home: https://github.com/kubernetes-incubator/metrics-server helm install stable/metrics-server \\ --name metrics-server \\ --version 2 .0.3 \\ --namespace metrics kubectl -n metrics \\ rollout status \\ deployment metrics-server","title":"Metrics Server"},{"location":"kubernetes/observability/#prometheus-alert-manager","text":"","title":"Prometheus &amp; Alert Manager"},{"location":"kubernetes/observability/#prometheus-helm-values","text":"server: ingress: enabled: true annotations: ingress.kubernetes.io/ssl-redirect: \"false\" nginx.ingress.kubernetes.io/ssl-redirect: \"false\" resources: limits: cpu: 100m memory: 1000Mi requests: cpu: 10m memory: 500Mi alertmanager: ingress: enabled: true annotations: ingress.kubernetes.io/ssl-redirect: \"false\" nginx.ingress.kubernetes.io/ssl-redirect: \"false\" resources: limits: cpu: 10m memory: 20Mi requests: cpu: 5m memory: 10Mi kubeStateMetrics: resources: limits: cpu: 10m memory: 50Mi requests: cpu: 5m memory: 25Mi nodeExporter: resources: limits: cpu: 10m memory: 20Mi requests: cpu: 5m memory: 10Mi pushgateway: resources: limits: cpu: 10m memory: 20Mi requests: cpu: 5m memory: 10Mi","title":"Prometheus Helm Values"},{"location":"kubernetes/observability/#grafana","text":"","title":"Grafana"},{"location":"kubernetes/observability/#application-metrics","text":"","title":"Application Metrics"},{"location":"kubernetes/observability/#resources","text":"https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-b190cc97f0f6 https://brancz.com/2018/01/05/prometheus-vs-heapster-vs-kubernetes-metrics-apis/ https://rancher.com/blog/2018/2018-06-26-measuring-metrics-that-matter-in-kubernetes-clusters/","title":"Resources"},{"location":"kubernetes/tools/","text":"Kubernetes Tools \u00b6 Helm \u00b6 We use Helm as a package manager to more easily install other tools on Kubernetes. There's several repositories with a large number of mature charts - the name of the Helm packages. One being Helm/Stable another being Helm Hub . Install \u00b6 MacOS/Homebrew brew install kubernetes-helm Windows/Chocolatey hoco install kubernetes-helm Ubuntu/Snap sudo snap install helm --classic Sccop scoop install helm GoFish gofish install helm Usage \u00b6 helm install stable/jenkins Kubecontext \u00b6 Kubectx is a utility to manage and switch between Kubernetes ( kubectl ) contexts and namespaces (via kubens , see below). Install \u00b6 MacOS/Homebrew brew install kubectx Ubuntu sudo apt install kubectx Usage \u00b6 Kubectx \u00b6 kubectx minikube Switched to context \"minikube\" . $ kubectx - Switched to context \"oregon\" . $ kubectx - Switched to context \"minikube\" . $ kubectx dublin = gke_ahmetb_europe-west1-b_dublin Context \"dublin\" set. Aliased \"gke_ahmetb_europe-west1-b_dublin\" as \"dublin\" . Kubens \u00b6 Kubens (part of Kubectx) helps you manage your current Kubernetes namespace. $ kubens kube-system Context \"test\" set. Active namespace is \"kube-system\" . $ kubens - Context \"test\" set. Active namespace is \"default\" . Kuard \u00b6 Kuard is a small demo application to show your cluster works. Also exposes some info you might want to see. kubectl run --restart = Never --image = gcr.io/kuar-demo/kuard-amd64:blue kuard kubectl port-forward kuard 8080 :8080 Open your browser to http://localhost:8080 . Stern \u00b6 Stern allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging. brew install stern Usage \u00b6 Imagine a build in Jenkins using more than one container in the Pod. You want to tail the logs of all containers... you can with stern. stern maven- Kube Capacity \u00b6 Kube Capacity is a simple CLI that provides an overview of the resource requests, limits, and utilization in a Kubernetes cluster. brew tap robscott/tap brew install robscott/tap/kube-capacity kube-capacity NODE CPU REQUESTS CPU LIMITS MEMORY REQUESTS MEMORY LIMITS * 560m ( 28 % ) 130m ( 7 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) example-node-1 220m ( 22 % ) 10m ( 1 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) example-node-2 340m ( 34 % ) 120m ( 12 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) kube-capacity --pods NODE NAMESPACE POD CPU REQUESTS CPU LIMITS MEMORY REQUESTS MEMORY LIMITS * * * 560m ( 28 % ) 780m ( 38 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) example-node-1 * * 220m ( 22 % ) 320m ( 32 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) example-node-1 kube-system metrics-server-lwc6z 100m ( 10 % ) 200m ( 20 % ) 100Mi ( 3 % ) 200Mi ( 7 % ) example-node-1 kube-system coredns-7b5bcb98f8 120m ( 12 % ) 120m ( 12 % ) 92Mi ( 3 % ) 160Mi ( 5 % ) example-node-2 * * 340m ( 34 % ) 460m ( 46 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) example-node-2 kube-system kube-proxy-3ki7 200m ( 20 % ) 280m ( 28 % ) 210Mi ( 7 % ) 210Mi ( 7 % ) example-node-2 tiller tiller-deploy 140m ( 14 % ) 180m ( 18 % ) 170Mi ( 5 % ) 200Mi ( 7 % ) kube-capacity --util NODE CPU REQUESTS CPU LIMITS CPU UTIL MEMORY REQUESTS MEMORY LIMITS MEMORY UTIL * 560m ( 28 % ) 130m ( 7 % ) 40m ( 2 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) 470Mi ( 8 % ) example-node-1 220m ( 22 % ) 10m ( 1 % ) 10m ( 1 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) 210Mi ( 7 % ) example-node-2 340m ( 34 % ) 120m ( 12 % ) 30m ( 3 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) 260Mi ( 9 % ) kube-capacity --pods --util Velero \u00b6 Velero RBAC Lookup \u00b6 RBAC Lookup Install \u00b6 bash brew install reactiveops/tap/rbac-lookup Krew kubectl krew install rbac-lookup Lookup user \u00b6 rbac-lookup jvandergriendt -owide Lookup GKE user \u00b6 rbac-lookup jvandergriendt --gke K9S \u00b6 K9S is a tool that gives you a console UI on your kubernetes cluster/namespace. Install \u00b6 brew tap derailed/k9s && brew install k9s Use \u00b6 By default is looks at a single namespace, and allows you to view elements of the pods running. k9s -n cje K9S \u00b6 K9S is a tool that gives you a console UI on your kubernetes cluster/namespace. Install \u00b6 brew tap derailed/k9s && brew install k9s Use \u00b6 By default is looks at a single namespace, and allows you to view elements of the pods running. k9s -n cje Dive \u00b6 A tool for exploring a docker image, layer contents, and discovering ways to shrink your Docker image size. Dive is a tool for analyzing Docker images. Install \u00b6 Debian based wget https://github.com/wagoodman/dive/releases/download/v0.7.1/dive_0.7.1_linux_amd64.deb sudo apt install ./dive_0.7.1_linux_amd64.deb RHEL based curl -OL https://github.com/wagoodman/dive/releases/download/v0.7.1/dive_0.7.1_linux_amd64.rpm rpm -i dive_0.7.1_linux_amd64.rpm Homebrew brew tap wagoodman/dive brew install dive Windows go get github.com/wagoodman/dive Use \u00b6 Existing image dive <your-image-tag> To be build image dive build -t <some-tag> . For CI builds CI = true dive <your-image> Kiali \u00b6 https://www.kiali.io/ Telepresence \u00b6 https://www.telepresence.io/","title":"Tools"},{"location":"kubernetes/tools/#kubernetes-tools","text":"","title":"Kubernetes Tools"},{"location":"kubernetes/tools/#helm","text":"We use Helm as a package manager to more easily install other tools on Kubernetes. There's several repositories with a large number of mature charts - the name of the Helm packages. One being Helm/Stable another being Helm Hub .","title":"Helm"},{"location":"kubernetes/tools/#install","text":"MacOS/Homebrew brew install kubernetes-helm Windows/Chocolatey hoco install kubernetes-helm Ubuntu/Snap sudo snap install helm --classic Sccop scoop install helm GoFish gofish install helm","title":"Install"},{"location":"kubernetes/tools/#usage","text":"helm install stable/jenkins","title":"Usage"},{"location":"kubernetes/tools/#kubecontext","text":"Kubectx is a utility to manage and switch between Kubernetes ( kubectl ) contexts and namespaces (via kubens , see below).","title":"Kubecontext"},{"location":"kubernetes/tools/#install_1","text":"MacOS/Homebrew brew install kubectx Ubuntu sudo apt install kubectx","title":"Install"},{"location":"kubernetes/tools/#usage_1","text":"","title":"Usage"},{"location":"kubernetes/tools/#kubectx","text":"kubectx minikube Switched to context \"minikube\" . $ kubectx - Switched to context \"oregon\" . $ kubectx - Switched to context \"minikube\" . $ kubectx dublin = gke_ahmetb_europe-west1-b_dublin Context \"dublin\" set. Aliased \"gke_ahmetb_europe-west1-b_dublin\" as \"dublin\" .","title":"Kubectx"},{"location":"kubernetes/tools/#kubens","text":"Kubens (part of Kubectx) helps you manage your current Kubernetes namespace. $ kubens kube-system Context \"test\" set. Active namespace is \"kube-system\" . $ kubens - Context \"test\" set. Active namespace is \"default\" .","title":"Kubens"},{"location":"kubernetes/tools/#kuard","text":"Kuard is a small demo application to show your cluster works. Also exposes some info you might want to see. kubectl run --restart = Never --image = gcr.io/kuar-demo/kuard-amd64:blue kuard kubectl port-forward kuard 8080 :8080 Open your browser to http://localhost:8080 .","title":"Kuard"},{"location":"kubernetes/tools/#stern","text":"Stern allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging. brew install stern","title":"Stern"},{"location":"kubernetes/tools/#usage_2","text":"Imagine a build in Jenkins using more than one container in the Pod. You want to tail the logs of all containers... you can with stern. stern maven-","title":"Usage"},{"location":"kubernetes/tools/#kube-capacity","text":"Kube Capacity is a simple CLI that provides an overview of the resource requests, limits, and utilization in a Kubernetes cluster. brew tap robscott/tap brew install robscott/tap/kube-capacity kube-capacity NODE CPU REQUESTS CPU LIMITS MEMORY REQUESTS MEMORY LIMITS * 560m ( 28 % ) 130m ( 7 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) example-node-1 220m ( 22 % ) 10m ( 1 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) example-node-2 340m ( 34 % ) 120m ( 12 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) kube-capacity --pods NODE NAMESPACE POD CPU REQUESTS CPU LIMITS MEMORY REQUESTS MEMORY LIMITS * * * 560m ( 28 % ) 780m ( 38 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) example-node-1 * * 220m ( 22 % ) 320m ( 32 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) example-node-1 kube-system metrics-server-lwc6z 100m ( 10 % ) 200m ( 20 % ) 100Mi ( 3 % ) 200Mi ( 7 % ) example-node-1 kube-system coredns-7b5bcb98f8 120m ( 12 % ) 120m ( 12 % ) 92Mi ( 3 % ) 160Mi ( 5 % ) example-node-2 * * 340m ( 34 % ) 460m ( 46 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) example-node-2 kube-system kube-proxy-3ki7 200m ( 20 % ) 280m ( 28 % ) 210Mi ( 7 % ) 210Mi ( 7 % ) example-node-2 tiller tiller-deploy 140m ( 14 % ) 180m ( 18 % ) 170Mi ( 5 % ) 200Mi ( 7 % ) kube-capacity --util NODE CPU REQUESTS CPU LIMITS CPU UTIL MEMORY REQUESTS MEMORY LIMITS MEMORY UTIL * 560m ( 28 % ) 130m ( 7 % ) 40m ( 2 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) 470Mi ( 8 % ) example-node-1 220m ( 22 % ) 10m ( 1 % ) 10m ( 1 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) 210Mi ( 7 % ) example-node-2 340m ( 34 % ) 120m ( 12 % ) 30m ( 3 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) 260Mi ( 9 % ) kube-capacity --pods --util","title":"Kube Capacity"},{"location":"kubernetes/tools/#velero","text":"Velero","title":"Velero"},{"location":"kubernetes/tools/#rbac-lookup","text":"RBAC Lookup","title":"RBAC Lookup"},{"location":"kubernetes/tools/#install_2","text":"bash brew install reactiveops/tap/rbac-lookup Krew kubectl krew install rbac-lookup","title":"Install"},{"location":"kubernetes/tools/#lookup-user","text":"rbac-lookup jvandergriendt -owide","title":"Lookup user"},{"location":"kubernetes/tools/#lookup-gke-user","text":"rbac-lookup jvandergriendt --gke","title":"Lookup GKE user"},{"location":"kubernetes/tools/#k9s","text":"K9S is a tool that gives you a console UI on your kubernetes cluster/namespace.","title":"K9S"},{"location":"kubernetes/tools/#install_3","text":"brew tap derailed/k9s && brew install k9s","title":"Install"},{"location":"kubernetes/tools/#use","text":"By default is looks at a single namespace, and allows you to view elements of the pods running. k9s -n cje","title":"Use"},{"location":"kubernetes/tools/#k9s_1","text":"K9S is a tool that gives you a console UI on your kubernetes cluster/namespace.","title":"K9S"},{"location":"kubernetes/tools/#install_4","text":"brew tap derailed/k9s && brew install k9s","title":"Install"},{"location":"kubernetes/tools/#use_1","text":"By default is looks at a single namespace, and allows you to view elements of the pods running. k9s -n cje","title":"Use"},{"location":"kubernetes/tools/#dive","text":"A tool for exploring a docker image, layer contents, and discovering ways to shrink your Docker image size. Dive is a tool for analyzing Docker images.","title":"Dive"},{"location":"kubernetes/tools/#install_5","text":"Debian based wget https://github.com/wagoodman/dive/releases/download/v0.7.1/dive_0.7.1_linux_amd64.deb sudo apt install ./dive_0.7.1_linux_amd64.deb RHEL based curl -OL https://github.com/wagoodman/dive/releases/download/v0.7.1/dive_0.7.1_linux_amd64.rpm rpm -i dive_0.7.1_linux_amd64.rpm Homebrew brew tap wagoodman/dive brew install dive Windows go get github.com/wagoodman/dive","title":"Install"},{"location":"kubernetes/tools/#use_2","text":"Existing image dive <your-image-tag> To be build image dive build -t <some-tag> . For CI builds CI = true dive <your-image>","title":"Use"},{"location":"kubernetes/tools/#kiali","text":"https://www.kiali.io/","title":"Kiali"},{"location":"kubernetes/tools/#telepresence","text":"https://www.telepresence.io/","title":"Telepresence"},{"location":"kubernetes/cicd/cdp-jenkins-helm/","text":"CD Pipeline with Jenkins & Helm \u00b6 Prerequisites \u00b6 Kubernetes 1.9.x+ cluster Valid domain names Jenkins 2.x+ with pipeline plugins below Helm/Tiller Tools \u00b6 Jenkins 2.x Install Helm \u00b6 For more information, checkout the github page . Helm's current version (as of October 2018) - version 2 - consists of two parts. One is a local client - Helm - which you should install on your own machine, see here for how. The other is a server component part - Tiller - that should be installed in your Kubernetes cluster. Install Tiller \u00b6 kubectl create serviceaccount tiller --namespace kube-system create rbac config: rbac-config.yaml apiVersion : v1 kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : tiller-role-binding roleRef : kind : ClusterRole name : cluster-admin apiGroup : rbac.authorization.k8s.io subjects : - kind : ServiceAccount name : tiller namespace : kube-system kubectl apply -f rbac-config.yaml helm init --service-account tiller install nginx helm chart \u00b6 helm install stable/nginx-ingress Jenkins Plugins \u00b6 Warnings Plugin: https://github.com/jenkinsci/warnings-plugin/blob/master/doc/Documentation.md Anchore: https://jenkins.io/blog/2018/06/20/anchore-image-scanning/ Anchore \u00b6 https://github.com/anchore/anchore-engine https://github.com/helm/charts/tree/master/stable/anchore-engine https://wiki.jenkins.io/display/JENKINS/Anchore+Container+Image+Scanner+Plugin","title":"CD Pipeline with Jenkins & Helm"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#cd-pipeline-with-jenkins-helm","text":"","title":"CD Pipeline with Jenkins &amp; Helm"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#prerequisites","text":"Kubernetes 1.9.x+ cluster Valid domain names Jenkins 2.x+ with pipeline plugins below Helm/Tiller","title":"Prerequisites"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#tools","text":"Jenkins 2.x","title":"Tools"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#install-helm","text":"For more information, checkout the github page . Helm's current version (as of October 2018) - version 2 - consists of two parts. One is a local client - Helm - which you should install on your own machine, see here for how. The other is a server component part - Tiller - that should be installed in your Kubernetes cluster.","title":"Install Helm"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#install-tiller","text":"kubectl create serviceaccount tiller --namespace kube-system create rbac config: rbac-config.yaml apiVersion : v1 kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : tiller-role-binding roleRef : kind : ClusterRole name : cluster-admin apiGroup : rbac.authorization.k8s.io subjects : - kind : ServiceAccount name : tiller namespace : kube-system kubectl apply -f rbac-config.yaml helm init --service-account tiller","title":"Install Tiller"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#install-nginx-helm-chart","text":"helm install stable/nginx-ingress","title":"install nginx helm chart"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#jenkins-plugins","text":"Warnings Plugin: https://github.com/jenkinsci/warnings-plugin/blob/master/doc/Documentation.md Anchore: https://jenkins.io/blog/2018/06/20/anchore-image-scanning/","title":"Jenkins Plugins"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#anchore","text":"https://github.com/anchore/anchore-engine https://github.com/helm/charts/tree/master/stable/anchore-engine https://wiki.jenkins.io/display/JENKINS/Anchore+Container+Image+Scanner+Plugin","title":"Anchore"},{"location":"kubernetes/distributions/","text":"Kubernetes \u00b6 What is kubernetes \u00b6 Kubernetes Objects \u00b6 Kubernetes tutorials \u00b6 Kubernetes Guides \u00b6 Linux basics \u00b6 Namespaces & CGroups \u00b6 https://jvns.ca/blog/2016/10/10/what-even-is-a-container/ https://www.youtube.com/watch?v=sK5i-N34im8 https://www.ianlewis.org/en/what-are-kubernetes-pods-anyway Networking \u00b6 https://itnext.io/kubernetes-networking-behind-the-scenes-39a1ab1792bb https://github.com/nleiva/kubernetes-networking-links IP Tables CIDR Explanation video Packets & Frames introduction Ingress \u00b6 Traefik on AWS Metrics \u00b6 https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-66936addedae Secrets \u00b6 Hashicorp Vault - Kelsey Hightower https://medium.com/qubit-engineering/kubernetes-up-integrated-secrets-configuration-5a15b9f5a6c6 Security \u00b6 RBAC 11 ways not to get hacked on Kubernetes https://www.youtube.com/channel/UCiqnRXPAAk6iv2m47odUFzw Tools to use \u00b6 Microscanner Dex - OpenID Connect solution Sonubuoy Helm Learn LUA book , lua's required for Helm 3.0 ChartMuseum Skaffold KSynch Traefik Istio Falco Prometheus Grafana Jenkins Kaniko Prow Tarmak Kube-Lego Knative Rook Stern - aggregate log rendering tool Linkerd 2","title":"Kubernetes"},{"location":"kubernetes/distributions/#kubernetes","text":"","title":"Kubernetes"},{"location":"kubernetes/distributions/#what-is-kubernetes","text":"","title":"What is kubernetes"},{"location":"kubernetes/distributions/#kubernetes-objects","text":"","title":"Kubernetes Objects"},{"location":"kubernetes/distributions/#kubernetes-tutorials","text":"","title":"Kubernetes tutorials"},{"location":"kubernetes/distributions/#kubernetes-guides","text":"","title":"Kubernetes Guides"},{"location":"kubernetes/distributions/#linux-basics","text":"","title":"Linux basics"},{"location":"kubernetes/distributions/#namespaces-cgroups","text":"https://jvns.ca/blog/2016/10/10/what-even-is-a-container/ https://www.youtube.com/watch?v=sK5i-N34im8 https://www.ianlewis.org/en/what-are-kubernetes-pods-anyway","title":"Namespaces &amp; CGroups"},{"location":"kubernetes/distributions/#networking","text":"https://itnext.io/kubernetes-networking-behind-the-scenes-39a1ab1792bb https://github.com/nleiva/kubernetes-networking-links IP Tables CIDR Explanation video Packets & Frames introduction","title":"Networking"},{"location":"kubernetes/distributions/#ingress","text":"Traefik on AWS","title":"Ingress"},{"location":"kubernetes/distributions/#metrics","text":"https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-66936addedae","title":"Metrics"},{"location":"kubernetes/distributions/#secrets","text":"Hashicorp Vault - Kelsey Hightower https://medium.com/qubit-engineering/kubernetes-up-integrated-secrets-configuration-5a15b9f5a6c6","title":"Secrets"},{"location":"kubernetes/distributions/#security","text":"RBAC 11 ways not to get hacked on Kubernetes https://www.youtube.com/channel/UCiqnRXPAAk6iv2m47odUFzw","title":"Security"},{"location":"kubernetes/distributions/#tools-to-use","text":"Microscanner Dex - OpenID Connect solution Sonubuoy Helm Learn LUA book , lua's required for Helm 3.0 ChartMuseum Skaffold KSynch Traefik Istio Falco Prometheus Grafana Jenkins Kaniko Prow Tarmak Kube-Lego Knative Rook Stern - aggregate log rendering tool Linkerd 2","title":"Tools to use"},{"location":"kubernetes/distributions/aks-cli/","text":"Azure CLI \u00b6 Configure AZ CLI \u00b6 az login az account show --query \"{subscriptionId:id, tenantId:tenantId}\" export SUBSCRIPTION_ID = SUBSCRIPTION_ID = ... az account set --subscription = \" ${ SUBSCRIPTION_ID } \" az ad sp create-for-rbac --role = \"Owner\" --scopes = \"/subscriptions/ ${ SUBSCRIPTION_ID } \" Should be owner \u00b6 Should be owner, else it cannot create a LoadBalancer via the nginx-ingress . See: https://weidongzhou.wordpress.com/2018/06/27/could-not-get-external-ip-for-load-balancer-on-azure-aks/ And: https://github.com/Azure/AKS/issues/427 Configure Kubecontext \u00b6 az aks get-credentials --resource-group cbcore --name cbcore Configure Cluster Autoscaler \u00b6 az extension add --name aks-preview az feature register --name VMSSPreview --namespace Microsoft.ContainerService az feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService/VMSSPreview')].{Name:name,State:properties.state}\" az provider register --namespace Microsoft.ContainerService az feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService')].{Name:name,State:properties.state}\" Configure multi-node pool \u00b6 az feature register --name MultiAgentpoolPreview --namespace Microsoft.ContainerService az feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService/MultiAgentpoolPreview')].{Name:name,State:properties.state}\" az provider register --namespace Microsoft.ContainerService Create AKS cluster via CLI \u00b6 Resources \u00b6 https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/ Get available versions \u00b6 az aks get-versions --location westeurope Create initial cluster \u00b6 Prepare variables \u00b6 RESOURCE_GROUP_NAME = CLUSTER_NAME = LOCATION = eastus NODE_POOL_MASTERS = masters NODE_POOL_BUILDS = builds VM_SIZE_MASTERS_NP = Standard_DS2_v2 VM_SIZE_BUILDS_NP = ? Create Resource Group \u00b6 az group create --name ${ RESOURCE_GROUP_NAME } --location ${ LOCATION } Create AKS Cluster \u00b6 az aks create \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --name ${ CLUSTER_NAME } \\ --enable-vmss \\ --node-count 1 \\ --nodepool-name ${ NODE_POOL_MASTERS } \\ --node-vm-size ${ VM_SIZE_MASTERS_NP } \\ --enable-cluster-autoscaler \\ --enable-vmss \\ --generate-ssh-keys PodSecurityPolicy \u00b6 --enable-cluster-autoscaler : Enable cluster autoscaler, default value is false. If specified, please make sure the kubernetes version is larger than 1 .10.6. Networking \u00b6 --network-plugin : The Kubernetes network plugin to use. Specify \"azure\" for advanced networking configurations. Defaults to \"kubenet\" . --network-policy : ( PREVIEW ) The Kubernetes network policy to use. Using together with \"azure\" network plugin. Specify \"azure\" for Azure network policy manager and \"calico\" for calico network policy controller. Defaults to \"\" ( network policy disabled ) . Retrieve credentials \u00b6 az aks get-credentials --resource-group ${ RESOURCE_GROUP_NAME } --name ${ CLUSTER_NAME } Add second node pool \u00b6 az aks nodepool add \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --cluster-name ${ CLUSTER_NAME } \\ --name ${ NODE_POOL_BUILDS } \\ --node-count 3","title":"AKS CLI"},{"location":"kubernetes/distributions/aks-cli/#azure-cli","text":"","title":"Azure CLI"},{"location":"kubernetes/distributions/aks-cli/#configure-az-cli","text":"az login az account show --query \"{subscriptionId:id, tenantId:tenantId}\" export SUBSCRIPTION_ID = SUBSCRIPTION_ID = ... az account set --subscription = \" ${ SUBSCRIPTION_ID } \" az ad sp create-for-rbac --role = \"Owner\" --scopes = \"/subscriptions/ ${ SUBSCRIPTION_ID } \"","title":"Configure AZ CLI"},{"location":"kubernetes/distributions/aks-cli/#should-be-owner","text":"Should be owner, else it cannot create a LoadBalancer via the nginx-ingress . See: https://weidongzhou.wordpress.com/2018/06/27/could-not-get-external-ip-for-load-balancer-on-azure-aks/ And: https://github.com/Azure/AKS/issues/427","title":"Should be owner"},{"location":"kubernetes/distributions/aks-cli/#configure-kubecontext","text":"az aks get-credentials --resource-group cbcore --name cbcore","title":"Configure Kubecontext"},{"location":"kubernetes/distributions/aks-cli/#configure-cluster-autoscaler","text":"az extension add --name aks-preview az feature register --name VMSSPreview --namespace Microsoft.ContainerService az feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService/VMSSPreview')].{Name:name,State:properties.state}\" az provider register --namespace Microsoft.ContainerService az feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService')].{Name:name,State:properties.state}\"","title":"Configure Cluster Autoscaler"},{"location":"kubernetes/distributions/aks-cli/#configure-multi-node-pool","text":"az feature register --name MultiAgentpoolPreview --namespace Microsoft.ContainerService az feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService/MultiAgentpoolPreview')].{Name:name,State:properties.state}\" az provider register --namespace Microsoft.ContainerService","title":"Configure multi-node pool"},{"location":"kubernetes/distributions/aks-cli/#create-aks-cluster-via-cli","text":"","title":"Create AKS cluster via CLI"},{"location":"kubernetes/distributions/aks-cli/#resources","text":"https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/","title":"Resources"},{"location":"kubernetes/distributions/aks-cli/#get-available-versions","text":"az aks get-versions --location westeurope","title":"Get available versions"},{"location":"kubernetes/distributions/aks-cli/#create-initial-cluster","text":"","title":"Create initial cluster"},{"location":"kubernetes/distributions/aks-cli/#prepare-variables","text":"RESOURCE_GROUP_NAME = CLUSTER_NAME = LOCATION = eastus NODE_POOL_MASTERS = masters NODE_POOL_BUILDS = builds VM_SIZE_MASTERS_NP = Standard_DS2_v2 VM_SIZE_BUILDS_NP = ?","title":"Prepare variables"},{"location":"kubernetes/distributions/aks-cli/#create-resource-group","text":"az group create --name ${ RESOURCE_GROUP_NAME } --location ${ LOCATION }","title":"Create Resource Group"},{"location":"kubernetes/distributions/aks-cli/#create-aks-cluster","text":"az aks create \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --name ${ CLUSTER_NAME } \\ --enable-vmss \\ --node-count 1 \\ --nodepool-name ${ NODE_POOL_MASTERS } \\ --node-vm-size ${ VM_SIZE_MASTERS_NP } \\ --enable-cluster-autoscaler \\ --enable-vmss \\ --generate-ssh-keys","title":"Create AKS Cluster"},{"location":"kubernetes/distributions/aks-cli/#podsecuritypolicy","text":"--enable-cluster-autoscaler : Enable cluster autoscaler, default value is false. If specified, please make sure the kubernetes version is larger than 1 .10.6.","title":"PodSecurityPolicy"},{"location":"kubernetes/distributions/aks-cli/#networking","text":"--network-plugin : The Kubernetes network plugin to use. Specify \"azure\" for advanced networking configurations. Defaults to \"kubenet\" . --network-policy : ( PREVIEW ) The Kubernetes network policy to use. Using together with \"azure\" network plugin. Specify \"azure\" for Azure network policy manager and \"calico\" for calico network policy controller. Defaults to \"\" ( network policy disabled ) .","title":"Networking"},{"location":"kubernetes/distributions/aks-cli/#retrieve-credentials","text":"az aks get-credentials --resource-group ${ RESOURCE_GROUP_NAME } --name ${ CLUSTER_NAME }","title":"Retrieve credentials"},{"location":"kubernetes/distributions/aks-cli/#add-second-node-pool","text":"az aks nodepool add \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --cluster-name ${ CLUSTER_NAME } \\ --name ${ NODE_POOL_BUILDS } \\ --node-count 3","title":"Add second node pool"},{"location":"kubernetes/distributions/aks-terraform/","text":"AKS Terraform \u00b6 Resources \u00b6 https://docs.microsoft.com/en-us/azure/terraform/terraform-create-k8s-cluster-with-tf-and-aks https://www.terraform.io/docs/providers/azurerm/r/kubernetes_cluster.html Pre-Requisites \u00b6 Create Service Principle \u00b6 It comes from this guide . az account show --query \"{subscriptionId:id, tenantId:tenantId}\" az ad sp create-for-rbac --role = \"Owner\" --scopes = \"/subscriptions/ ${ SUBSCRIPTION_ID } \" Retrieve current Kubernetes Versions \u00b6 az aks get-versions --location westeurope --output table Terraform Config \u00b6 ARM_SUBSCRIPTION_ID = ARM_CLIENT_ID = ARM_CLIENT_SECRET = ARM_TENANT_ID = ARM_ENVIRONMENT = Create storage account for TF State \u00b6 LOCATION = westeurope RESOURCE_GROUP_NAME = joostvdg-cb-ext-storage STORAGE_ACCOUNT_NAME = joostvdgcbtfstate CONTAINER_NAME = tfstate List locations \u00b6 az account list-locations \\ --query \"[].{Region:name}\" \\ --out table Create resource group \u00b6 az group create \\ --name ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION } Create storage account \u00b6 az storage account create \\ --name ${ STORAGE_ACCOUNT_NAME } \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION } \\ --sku Standard_ZRS \\ --kind StorageV2 Retrieve storage account login \u00b6 Apparently, no CLI commands available? Use the Azure Blog on AKS via Terraform for how via the UI. STORAGE_ACCOUNT_KEY = Create TF Storage \u00b6 az storage container create -n ${ CONTAINER_NAME } --account-name ${ STORAGE_ACCOUNT_NAME } --account-key ${ STORAGE_ACCOUNT_KEY } Init Terraform backend \u00b6 terraform init -backend-config = \"storage_account_name= ${ STORAGE_ACCOUNT_NAME } \" \\ -backend-config = \"container_name= ${ CONTAINER_NAME } \" \\ -backend-config = \"access_key= ${ STORAGE_ACCOUNT_KEY } \" \\ -backend-config = \"key=codelab.microsoft.tfstate\" Expose temp variables \u00b6 These are from your Service Principle we created earlier. Where client_id = appId, and client_secret the password. export TF_VAR_client_id = <your-client-id> export TF_VAR_client_secret = <your-client-secret> Rollout \u00b6 Set variables \u00b6 source ../export-variables.sh Validate \u00b6 terraform validate Plan \u00b6 terraform plan -out out.plan Apply the plan \u00b6 terraform apply out.plan Get kubectl config \u00b6 AKS_RESOURCE_GROUP = joostvdg-cbcore AKS_CLUSTER_NAME = acctestaks1 az aks get-credentials --resource-group ${ AKS_RESOURCE_GROUP } --name ${ AKS_CLUSTER_NAME } Enable Preview Features \u00b6 Currently having cluster autoscalers requires enabling of a Preview Feature in Azure. The same holds true for enabling multiple node pools, which I think is a best practice for using Kubernetes. Enable Multi Node Pool Enable Cluster Autoscaler - via VMScaleSets Terraform Code \u00b6 Important When using Terraform for AKS and you want to use Multiple Node Pools and/or the Cluster Autoscaler, you need to use the minimum of 1.32.0 of the azurerm provider. main.tf provider \"azurerm\" { # whilst the `version` attribute is optional, we recommend pinning to a given version of the Provider version = \"~> 1.32.0\" } terraform { backend \"azurerm\" {} } k8s.tf resource \"azurerm_kubernetes_cluster\" \"k8s\" { name = \"acctestaks1\" location = \"${azurerm_resource_group.k8s.location}\" resource_group_name = \"${azurerm_resource_group.k8s.name}\" dns_prefix = \"jvdg\" kubernetes_version = \"${var.kubernetes_version}\" agent_pool_profile { name = \"default\" vm_size = \"Standard_D2s_v3\" os_type = \"Linux\" os_disk_size_gb = 30 enable_auto_scaling = true count = 2 min_count = 2 max_count = 3 type = \"VirtualMachineScaleSets\" node_taints = [\"mytaint = true :NoSchedule\" ] } agent_pool_profile { name = \"pool1\" vm_size = \"Standard_D2s_v3\" os_type = \"Linux\" os_disk_size_gb = 30 enable_auto_scaling = true min_count = 1 max_count = 3 type = \"VirtualMachineScaleSets\" } agent_pool_profile { name = \"pool2\" vm_size = \"Standard_D4s_v3\" os_type = \"Linux\" os_disk_size_gb = 30 enable_auto_scaling = true min_count = 1 max_count = 3 type = \"VirtualMachineScaleSets\" } role_based_access_control { enabled = true } service_principal { client_id = \"${var.client_id}\" client_secret = \"${var.client_secret}\" } tags = { Environment = \"Development\" CreatedBy = \"Joostvdg\" } } output \"client_certificate\" { value = \"${azurerm_kubernetes_cluster.k8s.kube_config.0.client_certificate}\" } output \"kube_config\" { value = \"${azurerm_kubernetes_cluster.k8s.kube_config_raw}\" } variables.tf variable \"client_id\" {} variable \"client_secret\" {} variable \"kubernetes_version\" { default = \"1.14.6\" } variable \"agent_count\" { default = 3 } variable \"ssh_public_key\" { default = \"~/.ssh/id_rsa.pub\" } variable \"dns_prefix\" { default = \"jvdg\" } variable cluster_name { default = \"cbcore\" } variable resource_group_name { default = \"joostvdg-cbcore\" } variable container_registry_name { default = \"joostvdgacr\" } variable location { default = \"westeurope\" } acr.tf resource \"azurerm_resource_group\" \"ecr\" { name = \"${var.resource_group_name}-acr\" location = \"${var.location}\" } resource \"azurerm_container_registry\" \"acr\" { name = \"${var.container_registry_name}\" resource_group_name = \"${azurerm_resource_group.ecr.name}\" location = \"${azurerm_resource_group.k8s.location}\" sku = \"Premium\" admin_enabled = false } resource-group.tf resource \"azurerm_resource_group\" \"k8s\" { name = \"${var.resource_group_name}\" location = \"${var.location}\" }","title":"AKS Terraform"},{"location":"kubernetes/distributions/aks-terraform/#aks-terraform","text":"","title":"AKS Terraform"},{"location":"kubernetes/distributions/aks-terraform/#resources","text":"https://docs.microsoft.com/en-us/azure/terraform/terraform-create-k8s-cluster-with-tf-and-aks https://www.terraform.io/docs/providers/azurerm/r/kubernetes_cluster.html","title":"Resources"},{"location":"kubernetes/distributions/aks-terraform/#pre-requisites","text":"","title":"Pre-Requisites"},{"location":"kubernetes/distributions/aks-terraform/#create-service-principle","text":"It comes from this guide . az account show --query \"{subscriptionId:id, tenantId:tenantId}\" az ad sp create-for-rbac --role = \"Owner\" --scopes = \"/subscriptions/ ${ SUBSCRIPTION_ID } \"","title":"Create Service Principle"},{"location":"kubernetes/distributions/aks-terraform/#retrieve-current-kubernetes-versions","text":"az aks get-versions --location westeurope --output table","title":"Retrieve current Kubernetes Versions"},{"location":"kubernetes/distributions/aks-terraform/#terraform-config","text":"ARM_SUBSCRIPTION_ID = ARM_CLIENT_ID = ARM_CLIENT_SECRET = ARM_TENANT_ID = ARM_ENVIRONMENT =","title":"Terraform Config"},{"location":"kubernetes/distributions/aks-terraform/#create-storage-account-for-tf-state","text":"LOCATION = westeurope RESOURCE_GROUP_NAME = joostvdg-cb-ext-storage STORAGE_ACCOUNT_NAME = joostvdgcbtfstate CONTAINER_NAME = tfstate","title":"Create storage account for TF State"},{"location":"kubernetes/distributions/aks-terraform/#list-locations","text":"az account list-locations \\ --query \"[].{Region:name}\" \\ --out table","title":"List locations"},{"location":"kubernetes/distributions/aks-terraform/#create-resource-group","text":"az group create \\ --name ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION }","title":"Create resource group"},{"location":"kubernetes/distributions/aks-terraform/#create-storage-account","text":"az storage account create \\ --name ${ STORAGE_ACCOUNT_NAME } \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION } \\ --sku Standard_ZRS \\ --kind StorageV2","title":"Create storage account"},{"location":"kubernetes/distributions/aks-terraform/#retrieve-storage-account-login","text":"Apparently, no CLI commands available? Use the Azure Blog on AKS via Terraform for how via the UI. STORAGE_ACCOUNT_KEY =","title":"Retrieve storage account login"},{"location":"kubernetes/distributions/aks-terraform/#create-tf-storage","text":"az storage container create -n ${ CONTAINER_NAME } --account-name ${ STORAGE_ACCOUNT_NAME } --account-key ${ STORAGE_ACCOUNT_KEY }","title":"Create TF Storage"},{"location":"kubernetes/distributions/aks-terraform/#init-terraform-backend","text":"terraform init -backend-config = \"storage_account_name= ${ STORAGE_ACCOUNT_NAME } \" \\ -backend-config = \"container_name= ${ CONTAINER_NAME } \" \\ -backend-config = \"access_key= ${ STORAGE_ACCOUNT_KEY } \" \\ -backend-config = \"key=codelab.microsoft.tfstate\"","title":"Init Terraform backend"},{"location":"kubernetes/distributions/aks-terraform/#expose-temp-variables","text":"These are from your Service Principle we created earlier. Where client_id = appId, and client_secret the password. export TF_VAR_client_id = <your-client-id> export TF_VAR_client_secret = <your-client-secret>","title":"Expose temp variables"},{"location":"kubernetes/distributions/aks-terraform/#rollout","text":"","title":"Rollout"},{"location":"kubernetes/distributions/aks-terraform/#set-variables","text":"source ../export-variables.sh","title":"Set variables"},{"location":"kubernetes/distributions/aks-terraform/#validate","text":"terraform validate","title":"Validate"},{"location":"kubernetes/distributions/aks-terraform/#plan","text":"terraform plan -out out.plan","title":"Plan"},{"location":"kubernetes/distributions/aks-terraform/#apply-the-plan","text":"terraform apply out.plan","title":"Apply the plan"},{"location":"kubernetes/distributions/aks-terraform/#get-kubectl-config","text":"AKS_RESOURCE_GROUP = joostvdg-cbcore AKS_CLUSTER_NAME = acctestaks1 az aks get-credentials --resource-group ${ AKS_RESOURCE_GROUP } --name ${ AKS_CLUSTER_NAME }","title":"Get kubectl config"},{"location":"kubernetes/distributions/aks-terraform/#enable-preview-features","text":"Currently having cluster autoscalers requires enabling of a Preview Feature in Azure. The same holds true for enabling multiple node pools, which I think is a best practice for using Kubernetes. Enable Multi Node Pool Enable Cluster Autoscaler - via VMScaleSets","title":"Enable Preview Features"},{"location":"kubernetes/distributions/aks-terraform/#terraform-code","text":"Important When using Terraform for AKS and you want to use Multiple Node Pools and/or the Cluster Autoscaler, you need to use the minimum of 1.32.0 of the azurerm provider. main.tf provider \"azurerm\" { # whilst the `version` attribute is optional, we recommend pinning to a given version of the Provider version = \"~> 1.32.0\" } terraform { backend \"azurerm\" {} } k8s.tf resource \"azurerm_kubernetes_cluster\" \"k8s\" { name = \"acctestaks1\" location = \"${azurerm_resource_group.k8s.location}\" resource_group_name = \"${azurerm_resource_group.k8s.name}\" dns_prefix = \"jvdg\" kubernetes_version = \"${var.kubernetes_version}\" agent_pool_profile { name = \"default\" vm_size = \"Standard_D2s_v3\" os_type = \"Linux\" os_disk_size_gb = 30 enable_auto_scaling = true count = 2 min_count = 2 max_count = 3 type = \"VirtualMachineScaleSets\" node_taints = [\"mytaint = true :NoSchedule\" ] } agent_pool_profile { name = \"pool1\" vm_size = \"Standard_D2s_v3\" os_type = \"Linux\" os_disk_size_gb = 30 enable_auto_scaling = true min_count = 1 max_count = 3 type = \"VirtualMachineScaleSets\" } agent_pool_profile { name = \"pool2\" vm_size = \"Standard_D4s_v3\" os_type = \"Linux\" os_disk_size_gb = 30 enable_auto_scaling = true min_count = 1 max_count = 3 type = \"VirtualMachineScaleSets\" } role_based_access_control { enabled = true } service_principal { client_id = \"${var.client_id}\" client_secret = \"${var.client_secret}\" } tags = { Environment = \"Development\" CreatedBy = \"Joostvdg\" } } output \"client_certificate\" { value = \"${azurerm_kubernetes_cluster.k8s.kube_config.0.client_certificate}\" } output \"kube_config\" { value = \"${azurerm_kubernetes_cluster.k8s.kube_config_raw}\" } variables.tf variable \"client_id\" {} variable \"client_secret\" {} variable \"kubernetes_version\" { default = \"1.14.6\" } variable \"agent_count\" { default = 3 } variable \"ssh_public_key\" { default = \"~/.ssh/id_rsa.pub\" } variable \"dns_prefix\" { default = \"jvdg\" } variable cluster_name { default = \"cbcore\" } variable resource_group_name { default = \"joostvdg-cbcore\" } variable container_registry_name { default = \"joostvdgacr\" } variable location { default = \"westeurope\" } acr.tf resource \"azurerm_resource_group\" \"ecr\" { name = \"${var.resource_group_name}-acr\" location = \"${var.location}\" } resource \"azurerm_container_registry\" \"acr\" { name = \"${var.container_registry_name}\" resource_group_name = \"${azurerm_resource_group.ecr.name}\" location = \"${azurerm_resource_group.k8s.location}\" sku = \"Premium\" admin_enabled = false } resource-group.tf resource \"azurerm_resource_group\" \"k8s\" { name = \"${var.resource_group_name}\" location = \"${var.location}\" }","title":"Terraform Code"},{"location":"kubernetes/distributions/aks/","text":"Azure Kubernetes Service \u00b6 Resources \u00b6 https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/ https://www.cloudbees.com/blog/securing-jenkins-role-based-access-control-and-azure-active-directory https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/aks-install/# https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/kubernetes-helm-install/#_additional_information_creating_a_tls_certificate Configure AZ CLI \u00b6 az login az account show --query \"{subscriptionId:id, tenantId:tenantId}\" export SUBSCRIPTION_ID = SUBSCRIPTION_ID = ... az account set --subscription = \" ${ SUBSCRIPTION_ID } \" az ad sp create-for-rbac --role = \"Owner\" --scopes = \"/subscriptions/ ${ SUBSCRIPTION_ID } \" Should be owner \u00b6 Should be owner, else it cannot create a LoadBalancer via the nginx-ingress . See: https://weidongzhou.wordpress.com/2018/06/27/could-not-get-external-ip-for-load-balancer-on-azure-aks/ And: https://github.com/Azure/AKS/issues/427 Terraform Config \u00b6 ARM_SUBSCRIPTION_ID ARM_CLIENT_ID ARM_CLIENT_SECRET ARM_TENANT_ID ARM_ENVIRONMENT Create storage account for TF State \u00b6 LOCATION = westeurope RESOURCE_GROUP_NAME = joostvdg-cb-ext-storage STORAGE_ACCOUNT_NAME = joostvdgcbtfstate CONTAINER_NAME = tfstate List locations \u00b6 az account list-locations \\ --query \"[].{Region:name}\" \\ --out table Create resource group \u00b6 az group create \\ --name ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION } Create storage account \u00b6 az storage account create \\ --name ${ STORAGE_ACCOUNT_NAME } \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION } \\ --sku Standard_ZRS \\ --kind StorageV2 Retrieve storage account login \u00b6 Apparently, no CLI commands available? Use the Azure Blog on AKS via Terraform for how via the UI. STORAGE_ACCOUNT_KEY = Create TF Storage \u00b6 az storage container create -n ${ CONTAINER_NAME } --account-name ${ STORAGE_ACCOUNT_NAME } --account-key ${ STORAGE_ACCOUNT_KEY } Init Terraform backend \u00b6 terraform init -backend-config = \"storage_account_name= ${ STORAGE_ACCOUNT_NAME } \" \\ -backend-config = \"container_name= ${ CONTAINER_NAME } \" \\ -backend-config = \"access_key= ${ STORAGE_ACCOUNT_KEY } \" \\ -backend-config = \"key=codelab.microsoft.tfstate\" Expose temp variables \u00b6 These are from your Service Principle we created earlier. Where client_id = appId, and client_secret the password. export TF_VAR_client_id = <your-client-id> export TF_VAR_client_secret = <your-client-secret> Rollout \u00b6 Set variables \u00b6 source ../export-variables.sh Validate \u00b6 terraform validate Plan \u00b6 terraform plan -out out.plan Apply the plan \u00b6 terraform apply out.plan Configure Kubecontext \u00b6 az aks get-credentials --resource-group cbcore --name cbcore Configure Cluster Autoscaler \u00b6 az extension add --name aks-preview az feature register --name VMSSPreview --namespace Microsoft.ContainerService az feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService/VMSSPreview')].{Name:name,State:properties.state}\" az provider register --namespace Microsoft.ContainerService az feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService')].{Name:name,State:properties.state}\" Configure multi-node pool \u00b6 az feature register --name MultiAgentpoolPreview --namespace Microsoft.ContainerService az feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService/MultiAgentpoolPreview')].{Name:name,State:properties.state}\" az provider register --namespace Microsoft.ContainerService Create AKS cluster via CLI \u00b6 Resources \u00b6 https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/ Get available versions \u00b6 az aks get-versions --location westeurope Create initial cluster \u00b6 # Create a resource group in East US az group create --name myResourceGroup --location eastus # Create a basic single-node AKS cluster az aks create \\ --resource-group myResourceGroup \\ --name myAKSCluster \\ --enable-vmss \\ --node-count 1 \\ --nodepool-name masters \\ --node-vm-size Standard_DS2_v2 \\ --enable-cluster-autoscaler \\ --enable-vmss \\ --generate-ssh-keys PodSecurityPolicy \u00b6 --enable-cluster-autoscaler : Enable cluster autoscaler, default value is false. If specified, please make sure the kubernetes version is larger than 1 .10.6. Networking \u00b6 --network-plugin : The Kubernetes network plugin to use. Specify \"azure\" for advanced networking configurations. Defaults to \"kubenet\" . --network-policy : ( PREVIEW ) The Kubernetes network policy to use. Using together with \"azure\" network plugin. Specify \"azure\" for Azure network policy manager and \"calico\" for calico network policy controller. Defaults to \"\" ( network policy disabled ) . Retrieve credentials \u00b6 az aks get-credentials --resource-group myResourceGroup --name myAKSCluster Add second node pool \u00b6 az aks nodepool add \\ --resource-group myResourceGroup \\ --cluster-name myAKSCluster \\ --name mynodepool \\ --node-count 3","title":"Azure Kubernetes Service"},{"location":"kubernetes/distributions/aks/#azure-kubernetes-service","text":"","title":"Azure Kubernetes Service"},{"location":"kubernetes/distributions/aks/#resources","text":"https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/ https://www.cloudbees.com/blog/securing-jenkins-role-based-access-control-and-azure-active-directory https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/aks-install/# https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/kubernetes-helm-install/#_additional_information_creating_a_tls_certificate","title":"Resources"},{"location":"kubernetes/distributions/aks/#configure-az-cli","text":"az login az account show --query \"{subscriptionId:id, tenantId:tenantId}\" export SUBSCRIPTION_ID = SUBSCRIPTION_ID = ... az account set --subscription = \" ${ SUBSCRIPTION_ID } \" az ad sp create-for-rbac --role = \"Owner\" --scopes = \"/subscriptions/ ${ SUBSCRIPTION_ID } \"","title":"Configure AZ CLI"},{"location":"kubernetes/distributions/aks/#should-be-owner","text":"Should be owner, else it cannot create a LoadBalancer via the nginx-ingress . See: https://weidongzhou.wordpress.com/2018/06/27/could-not-get-external-ip-for-load-balancer-on-azure-aks/ And: https://github.com/Azure/AKS/issues/427","title":"Should be owner"},{"location":"kubernetes/distributions/aks/#terraform-config","text":"ARM_SUBSCRIPTION_ID ARM_CLIENT_ID ARM_CLIENT_SECRET ARM_TENANT_ID ARM_ENVIRONMENT","title":"Terraform Config"},{"location":"kubernetes/distributions/aks/#create-storage-account-for-tf-state","text":"LOCATION = westeurope RESOURCE_GROUP_NAME = joostvdg-cb-ext-storage STORAGE_ACCOUNT_NAME = joostvdgcbtfstate CONTAINER_NAME = tfstate","title":"Create storage account for TF State"},{"location":"kubernetes/distributions/aks/#list-locations","text":"az account list-locations \\ --query \"[].{Region:name}\" \\ --out table","title":"List locations"},{"location":"kubernetes/distributions/aks/#create-resource-group","text":"az group create \\ --name ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION }","title":"Create resource group"},{"location":"kubernetes/distributions/aks/#create-storage-account","text":"az storage account create \\ --name ${ STORAGE_ACCOUNT_NAME } \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION } \\ --sku Standard_ZRS \\ --kind StorageV2","title":"Create storage account"},{"location":"kubernetes/distributions/aks/#retrieve-storage-account-login","text":"Apparently, no CLI commands available? Use the Azure Blog on AKS via Terraform for how via the UI. STORAGE_ACCOUNT_KEY =","title":"Retrieve storage account login"},{"location":"kubernetes/distributions/aks/#create-tf-storage","text":"az storage container create -n ${ CONTAINER_NAME } --account-name ${ STORAGE_ACCOUNT_NAME } --account-key ${ STORAGE_ACCOUNT_KEY }","title":"Create TF Storage"},{"location":"kubernetes/distributions/aks/#init-terraform-backend","text":"terraform init -backend-config = \"storage_account_name= ${ STORAGE_ACCOUNT_NAME } \" \\ -backend-config = \"container_name= ${ CONTAINER_NAME } \" \\ -backend-config = \"access_key= ${ STORAGE_ACCOUNT_KEY } \" \\ -backend-config = \"key=codelab.microsoft.tfstate\"","title":"Init Terraform backend"},{"location":"kubernetes/distributions/aks/#expose-temp-variables","text":"These are from your Service Principle we created earlier. Where client_id = appId, and client_secret the password. export TF_VAR_client_id = <your-client-id> export TF_VAR_client_secret = <your-client-secret>","title":"Expose temp variables"},{"location":"kubernetes/distributions/aks/#rollout","text":"","title":"Rollout"},{"location":"kubernetes/distributions/aks/#set-variables","text":"source ../export-variables.sh","title":"Set variables"},{"location":"kubernetes/distributions/aks/#validate","text":"terraform validate","title":"Validate"},{"location":"kubernetes/distributions/aks/#plan","text":"terraform plan -out out.plan","title":"Plan"},{"location":"kubernetes/distributions/aks/#apply-the-plan","text":"terraform apply out.plan","title":"Apply the plan"},{"location":"kubernetes/distributions/aks/#configure-kubecontext","text":"az aks get-credentials --resource-group cbcore --name cbcore","title":"Configure Kubecontext"},{"location":"kubernetes/distributions/aks/#configure-cluster-autoscaler","text":"az extension add --name aks-preview az feature register --name VMSSPreview --namespace Microsoft.ContainerService az feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService/VMSSPreview')].{Name:name,State:properties.state}\" az provider register --namespace Microsoft.ContainerService az feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService')].{Name:name,State:properties.state}\"","title":"Configure Cluster Autoscaler"},{"location":"kubernetes/distributions/aks/#configure-multi-node-pool","text":"az feature register --name MultiAgentpoolPreview --namespace Microsoft.ContainerService az feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService/MultiAgentpoolPreview')].{Name:name,State:properties.state}\" az provider register --namespace Microsoft.ContainerService","title":"Configure multi-node pool"},{"location":"kubernetes/distributions/aks/#create-aks-cluster-via-cli","text":"","title":"Create AKS cluster via CLI"},{"location":"kubernetes/distributions/aks/#resources_1","text":"https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/","title":"Resources"},{"location":"kubernetes/distributions/aks/#get-available-versions","text":"az aks get-versions --location westeurope","title":"Get available versions"},{"location":"kubernetes/distributions/aks/#create-initial-cluster","text":"# Create a resource group in East US az group create --name myResourceGroup --location eastus # Create a basic single-node AKS cluster az aks create \\ --resource-group myResourceGroup \\ --name myAKSCluster \\ --enable-vmss \\ --node-count 1 \\ --nodepool-name masters \\ --node-vm-size Standard_DS2_v2 \\ --enable-cluster-autoscaler \\ --enable-vmss \\ --generate-ssh-keys","title":"Create initial cluster"},{"location":"kubernetes/distributions/aks/#podsecuritypolicy","text":"--enable-cluster-autoscaler : Enable cluster autoscaler, default value is false. If specified, please make sure the kubernetes version is larger than 1 .10.6.","title":"PodSecurityPolicy"},{"location":"kubernetes/distributions/aks/#networking","text":"--network-plugin : The Kubernetes network plugin to use. Specify \"azure\" for advanced networking configurations. Defaults to \"kubenet\" . --network-policy : ( PREVIEW ) The Kubernetes network policy to use. Using together with \"azure\" network plugin. Specify \"azure\" for Azure network policy manager and \"calico\" for calico network policy controller. Defaults to \"\" ( network policy disabled ) .","title":"Networking"},{"location":"kubernetes/distributions/aks/#retrieve-credentials","text":"az aks get-credentials --resource-group myResourceGroup --name myAKSCluster","title":"Retrieve credentials"},{"location":"kubernetes/distributions/aks/#add-second-node-pool","text":"az aks nodepool add \\ --resource-group myResourceGroup \\ --cluster-name myAKSCluster \\ --name mynodepool \\ --node-count 3","title":"Add second node pool"},{"location":"kubernetes/distributions/eks-eksctl/","text":"AWS EKS via eksctl \u00b6 EKS Access Configuration \u00b6 Some reference configuration, this is assuming you need temporary access tokens based on a assume role while having a MFA device configured. I seemed to have to create a new token every X minutes. If you don't run into this, ignore the configuration below and go straight to creating the cluster. EKS Keys Config \u00b6 [ cloudbees-eks ] aws_access_key_id = ASI... aws_secret_access_key = NMAX... aws_session_token = FQoGZXIvYXdzEJr//////////wE..................... // one long ass token Generate Temporary Access Tokens With MFA \u00b6 keys =( $( aws sts assume-role --profile default --role-arn arn:aws:iam::<ROLE_ARN>:role/<ROLE_NAME> \\ --role-session-name MyEKSCTLSession \\ --serial-number arn:aws:iam::<MFA_ARN>:mfa/<USER> \\ --token-code <REPLACE_THIS_WITH_MFA_TOKEN> \\ --query 'Credentials.[AccessKeyId,SecretAccessKey,SessionToken]' --output text ) ) Cluster Create \u00b6 EKS_CLUSTER_NAME = mycluster AWS_PROFILE = cloudbees-eks AWS_REGION = us-east-1 AWS_SSH_KEY_LOCATION = \"~/.ssh/id_rsa.pub\" EKS_NUM_NODES = 4 eksctl create cluster \\ --asg-access \\ --auto-kubeconfig \\ --full-ecr-access \\ --name ${ EKS_CLUSTER_NAME } \\ --profile ${ AWS_PROFILE } \\ --region ${ AWS_REGION } \\ --set-kubeconfig-context \\ --ssh-public-key ${ AWS_SSH_KEY_LOCATION } \\ --nodes = ${ EKS_NUM_NODES } \\ --verbose 4 alias eks = \"kubectl --kubeconfig=~/.kube/eksctl/clusters/mycluster\" Encrypted Network With Weavenet \u00b6 If you want your network to be encrypted, you can use Weavenet. Warning The price of the encrypted network is high. So you're probably better off with a Network Policy. WEAVENET_PASS = vjStsrzC4q7xDnb1wZkYacnk IPALLOC_RANGE = 10 .10.0.0/24 echo \" ${ WEAVENET_PASS } \" > weave-passwd eks create secret -n kube-system generic weave-passwd --from-file = weave-passwd eks apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) &password-secret=weave-passwd&env.IPALLOC_RANGE= ${ IPALLOC_RANGE } \" Helm & Tiller \u00b6 alias helmks = \"helm --kubeconfig=~/.kube/eksctl/clusters/mycluster\" eks create serviceaccount --namespace kube-system tiller eks create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller helmks init --service-account tiller --upgrade Nginx \u00b6 Nginx Ingress Docs, How to install on AWS CloudBees AWS Docs eks apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml eks apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/aws/service-l4.yaml eks apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/aws/patch-configmap-l4.yaml eks patch service ingress-nginx -p '{\"spec\":{\"externalTrafficPolicy\":\"Local\"}}' -n ingress-nginx Certmanager \u00b6 helmks install --name cert-manager --namespace default stable/cert-manager echo \"apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: letsencrypt-prd spec: acme: # The ACME server URL server: https://acme-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: yourname@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-prd # Enable the HTTP-01 challenge provider http01: {}\" > cluster-issuer.yml eks apply -f cluster-issuer.yml Storage class \u00b6 Create a gp2 storage class and set it as default. echo \"kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gp2 provisioner: kubernetes.io/aws-ebs parameters: type: gp2 encrypted: \\\"true\\\"\" > gp2-storage.yaml eks patch storageclass gp2 -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' eks patch storageclass default -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}' Confirm \u00b6 Confirm the storage class is create and set as default. kubectl get sc Expected result. NAME PROVISIONER AGE gp2 ( default ) kubernetes.io/aws-ebs 59","title":"AWS EKS"},{"location":"kubernetes/distributions/eks-eksctl/#aws-eks-via-eksctl","text":"","title":"AWS EKS via eksctl"},{"location":"kubernetes/distributions/eks-eksctl/#eks-access-configuration","text":"Some reference configuration, this is assuming you need temporary access tokens based on a assume role while having a MFA device configured. I seemed to have to create a new token every X minutes. If you don't run into this, ignore the configuration below and go straight to creating the cluster.","title":"EKS Access Configuration"},{"location":"kubernetes/distributions/eks-eksctl/#eks-keys-config","text":"[ cloudbees-eks ] aws_access_key_id = ASI... aws_secret_access_key = NMAX... aws_session_token = FQoGZXIvYXdzEJr//////////wE..................... // one long ass token","title":"EKS Keys Config"},{"location":"kubernetes/distributions/eks-eksctl/#generate-temporary-access-tokens-with-mfa","text":"keys =( $( aws sts assume-role --profile default --role-arn arn:aws:iam::<ROLE_ARN>:role/<ROLE_NAME> \\ --role-session-name MyEKSCTLSession \\ --serial-number arn:aws:iam::<MFA_ARN>:mfa/<USER> \\ --token-code <REPLACE_THIS_WITH_MFA_TOKEN> \\ --query 'Credentials.[AccessKeyId,SecretAccessKey,SessionToken]' --output text ) )","title":"Generate Temporary Access Tokens With MFA"},{"location":"kubernetes/distributions/eks-eksctl/#cluster-create","text":"EKS_CLUSTER_NAME = mycluster AWS_PROFILE = cloudbees-eks AWS_REGION = us-east-1 AWS_SSH_KEY_LOCATION = \"~/.ssh/id_rsa.pub\" EKS_NUM_NODES = 4 eksctl create cluster \\ --asg-access \\ --auto-kubeconfig \\ --full-ecr-access \\ --name ${ EKS_CLUSTER_NAME } \\ --profile ${ AWS_PROFILE } \\ --region ${ AWS_REGION } \\ --set-kubeconfig-context \\ --ssh-public-key ${ AWS_SSH_KEY_LOCATION } \\ --nodes = ${ EKS_NUM_NODES } \\ --verbose 4 alias eks = \"kubectl --kubeconfig=~/.kube/eksctl/clusters/mycluster\"","title":"Cluster Create"},{"location":"kubernetes/distributions/eks-eksctl/#encrypted-network-with-weavenet","text":"If you want your network to be encrypted, you can use Weavenet. Warning The price of the encrypted network is high. So you're probably better off with a Network Policy. WEAVENET_PASS = vjStsrzC4q7xDnb1wZkYacnk IPALLOC_RANGE = 10 .10.0.0/24 echo \" ${ WEAVENET_PASS } \" > weave-passwd eks create secret -n kube-system generic weave-passwd --from-file = weave-passwd eks apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) &password-secret=weave-passwd&env.IPALLOC_RANGE= ${ IPALLOC_RANGE } \"","title":"Encrypted Network With Weavenet"},{"location":"kubernetes/distributions/eks-eksctl/#helm-tiller","text":"alias helmks = \"helm --kubeconfig=~/.kube/eksctl/clusters/mycluster\" eks create serviceaccount --namespace kube-system tiller eks create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller helmks init --service-account tiller --upgrade","title":"Helm &amp; Tiller"},{"location":"kubernetes/distributions/eks-eksctl/#nginx","text":"Nginx Ingress Docs, How to install on AWS CloudBees AWS Docs eks apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml eks apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/aws/service-l4.yaml eks apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/aws/patch-configmap-l4.yaml eks patch service ingress-nginx -p '{\"spec\":{\"externalTrafficPolicy\":\"Local\"}}' -n ingress-nginx","title":"Nginx"},{"location":"kubernetes/distributions/eks-eksctl/#certmanager","text":"helmks install --name cert-manager --namespace default stable/cert-manager echo \"apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: letsencrypt-prd spec: acme: # The ACME server URL server: https://acme-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: yourname@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-prd # Enable the HTTP-01 challenge provider http01: {}\" > cluster-issuer.yml eks apply -f cluster-issuer.yml","title":"Certmanager"},{"location":"kubernetes/distributions/eks-eksctl/#storage-class","text":"Create a gp2 storage class and set it as default. echo \"kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gp2 provisioner: kubernetes.io/aws-ebs parameters: type: gp2 encrypted: \\\"true\\\"\" > gp2-storage.yaml eks patch storageclass gp2 -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' eks patch storageclass default -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}'","title":"Storage class"},{"location":"kubernetes/distributions/eks-eksctl/#confirm","text":"Confirm the storage class is create and set as default. kubectl get sc Expected result. NAME PROVISIONER AGE gp2 ( default ) kubernetes.io/aws-ebs 59","title":"Confirm"},{"location":"kubernetes/distributions/gke-terraform/","text":"GKE Terraform \u00b6 Resources \u00b6 Terraform Google Cloud Container Cluster (GKE) Resource Un-official GKE Terraform Module Jetstack GKE Terraform Module Pre-Requisites \u00b6 Terraform Configuration \u00b6 The idea behind the Terraform configuration is as follows: Use Configuration-as-Code to create the GKE Cluster Have separate Node Pools for workload isolation / specialization Each Node Pool has a Cluster Autoscaler to make the cluster size dynamic Variables \u00b6 variable \"project\" { } variable \"name\" { description = \"The name of the cluster (required)\" default = \"my-awesome-jx-cluster\" } variable \"description\" { description = \"The description of the cluster\" default = \"Jenkins X Environment for ...\" } variable \"location\" { description = \"The location to host the cluster\" default = \"europe-west4\" } variable \"cluster_master_version\" { description = \"The minimum kubernetes version for the master nodes\" default = \"1.14.7-gke.10\" } Main \u00b6 terraform { required_version = \"~> 0.12\" } # https://www.terraform.io/docs/providers/google/index.html provider \"google\" { version = \"~> 2.18.1\" project = \"${var.project}\" region = \"europe-west4\" zone = \"europe-west4-b\" } Cluster \u00b6 resource \"google_container_cluster\" \"primary\" { name = \"${var.name}\" location = \"${var.location}\" # We can't create a cluster with no node pool defined, but we want to only use # separately managed node pools. So we create the smallest possible default # node pool and immediately delete it. remove_default_node_pool = true initial_node_count = 1 min_master_version = \"${var.cluster_master_version}\" resource_labels = { environment = \"development\" created-by = \"terraform\" owner = \"joostvdg\" } # Configuration options for the NetworkPolicy feature. network_policy { # Whether network policy is enabled on the cluster. Defaults to false. # In GKE this also enables the ip masquerade agent # https://cloud.google.com/kubernetes-engine/docs/how-to/ip-masquerade-agent enabled = true # The selected network policy provider. Defaults to PROVIDER_UNSPECIFIED. provider = \"CALICO\" } } Node Pools \u00b6 resource \"google_container_node_pool\" \"nodepool1\" { name = \"pool1\" location = \"${var.location}\" cluster = \"${google_container_cluster.primary.name}\" node_count = 1 management { auto_repair = true auto_upgrade = true } autoscaling { min_node_count = 1 max_node_count = 4 } node_config { machine_type = \"n1-standard-2\" oauth_scopes = [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" , \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" , \"https://www.googleapis.com/auth/cloud-platform\" ] } } resource \"google_container_node_pool\" \"nodepool2\" { name = \"pool2\" location = \"europe-west4\" cluster = \"${google_container_cluster.primary.name}\" node_count = 1 autoscaling { min_node_count = 1 max_node_count = 3 } node_config { machine_type = \"n2-standard-2\" oauth_scopes = [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" , \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" , \"https://www.googleapis.com/auth/cloud-platform\" ] } }","title":"GKE Terraform"},{"location":"kubernetes/distributions/gke-terraform/#gke-terraform","text":"","title":"GKE Terraform"},{"location":"kubernetes/distributions/gke-terraform/#resources","text":"Terraform Google Cloud Container Cluster (GKE) Resource Un-official GKE Terraform Module Jetstack GKE Terraform Module","title":"Resources"},{"location":"kubernetes/distributions/gke-terraform/#pre-requisites","text":"","title":"Pre-Requisites"},{"location":"kubernetes/distributions/gke-terraform/#terraform-configuration","text":"The idea behind the Terraform configuration is as follows: Use Configuration-as-Code to create the GKE Cluster Have separate Node Pools for workload isolation / specialization Each Node Pool has a Cluster Autoscaler to make the cluster size dynamic","title":"Terraform Configuration"},{"location":"kubernetes/distributions/gke-terraform/#variables","text":"variable \"project\" { } variable \"name\" { description = \"The name of the cluster (required)\" default = \"my-awesome-jx-cluster\" } variable \"description\" { description = \"The description of the cluster\" default = \"Jenkins X Environment for ...\" } variable \"location\" { description = \"The location to host the cluster\" default = \"europe-west4\" } variable \"cluster_master_version\" { description = \"The minimum kubernetes version for the master nodes\" default = \"1.14.7-gke.10\" }","title":"Variables"},{"location":"kubernetes/distributions/gke-terraform/#main","text":"terraform { required_version = \"~> 0.12\" } # https://www.terraform.io/docs/providers/google/index.html provider \"google\" { version = \"~> 2.18.1\" project = \"${var.project}\" region = \"europe-west4\" zone = \"europe-west4-b\" }","title":"Main"},{"location":"kubernetes/distributions/gke-terraform/#cluster","text":"resource \"google_container_cluster\" \"primary\" { name = \"${var.name}\" location = \"${var.location}\" # We can't create a cluster with no node pool defined, but we want to only use # separately managed node pools. So we create the smallest possible default # node pool and immediately delete it. remove_default_node_pool = true initial_node_count = 1 min_master_version = \"${var.cluster_master_version}\" resource_labels = { environment = \"development\" created-by = \"terraform\" owner = \"joostvdg\" } # Configuration options for the NetworkPolicy feature. network_policy { # Whether network policy is enabled on the cluster. Defaults to false. # In GKE this also enables the ip masquerade agent # https://cloud.google.com/kubernetes-engine/docs/how-to/ip-masquerade-agent enabled = true # The selected network policy provider. Defaults to PROVIDER_UNSPECIFIED. provider = \"CALICO\" } }","title":"Cluster"},{"location":"kubernetes/distributions/gke-terraform/#node-pools","text":"resource \"google_container_node_pool\" \"nodepool1\" { name = \"pool1\" location = \"${var.location}\" cluster = \"${google_container_cluster.primary.name}\" node_count = 1 management { auto_repair = true auto_upgrade = true } autoscaling { min_node_count = 1 max_node_count = 4 } node_config { machine_type = \"n1-standard-2\" oauth_scopes = [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" , \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" , \"https://www.googleapis.com/auth/cloud-platform\" ] } } resource \"google_container_node_pool\" \"nodepool2\" { name = \"pool2\" location = \"europe-west4\" cluster = \"${google_container_cluster.primary.name}\" node_count = 1 autoscaling { min_node_count = 1 max_node_count = 3 } node_config { machine_type = \"n2-standard-2\" oauth_scopes = [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" , \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" , \"https://www.googleapis.com/auth/cloud-platform\" ] } }","title":"Node Pools"},{"location":"kubernetes/distributions/install-gke/","text":"GKE with Helm \u00b6 Env Variables \u00b6 CLUSTER_NAME = MyGKECluster REGION = europe-west4 NODE_LOCATIONS = ${ REGION } -a, ${ REGION } -b ZONE = europe-west4-a K8S_VERSION = 1 .11.5-gke.4 PROJECT_ID = Get Kubernetes versions \u00b6 gcloud container get-server-config --region $REGION Create Cluster \u00b6 gcloud container clusters create ${ CLUSTER_NAME } \\ --region ${ REGION } --node-locations ${ NODE_LOCATIONS } \\ --cluster-version ${ K8S_VERSION } \\ --num-nodes 2 --machine-type n1-standard-2 \\ --addons = HorizontalPodAutoscaling \\ --min-nodes 2 --max-nodes 3 \\ --enable-autoupgrade \\ --enable-autoscaling \\ --enable-network-policy \\ --labels = purpose = practice Post Install \u00b6 kubectl create clusterrolebinding cluster-admin-binding \\ --clusterrole cluster-admin \\ --user $( gcloud config get-value account ) Delete Cluster \u00b6 gcloud container clusters delete $CLUSTER_NAME --region $REGION Configure kubeconfig \u00b6 gcloud container clusters get-credentials ${ CLUSTER_NAME } --region ${ REGION } Install Cluster Tools \u00b6 Helm \u00b6 We use Helm as a package manager to more easily install other tools on Kubernetes. There's several repositories with a large number of mature charts - the name of the Helm packages. One being Helm/Stable another being Helm Hub . Create service account \u00b6 kubectl create serviceaccount --namespace kube-system tiller Warning Tiller is deemed not safe for production, at least not in its default configuration. Either enable its TLS configuration and take other measures (such as namespace limitation) or use alternative solutions. Such as Kustomize, Pulumi, Jenkins X or raw Yaml. Create cluster role binding \u00b6 kubectl create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller helm init \u00b6 helm init --service-account tiller Test version \u00b6 helm version Warning Currently, nginx ingress controller has an issue with Helm 2.14. So if you 2.14, either downgrade to 2.13.1 or install the Ingress Controller via an alternative solution (such as Kustomize). Ingress Controller \u00b6 helm install --namespace ingress-nginx --name nginx-ingress stable/nginx-ingress \\ --set controller.service.externalTrafficPolicy = Local \\ --set controller.replicaCount = 3 \\ --set rbac.create = true Get LoadBalancer IP \u00b6 export LB_IP = $( kubectl get svc -n ingress-nginx nginx-ingress-controller -o jsonpath = \"{.status.loadBalancer.ingress[0].ip}\" ) echo $LB_IP Warning Now is the time to configure your DNS to use whatever LB_IP 's value is. Cert Manager \u00b6 Cert Manager is the recommended approach for managing TLS certificates in Kubernetes. If you do not want to manage certificates yourself, please use this. The certificates it uses are real and valid certificates, provided by Let's Encrypt . Install CRD's \u00b6 kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8/deploy/manifests/00-crds.yaml Create Namespace \u00b6 kubectl create namespace cert-manager Label namespace \u00b6 kubectl label namespace cert-manager certmanager.k8s.io/disable-validation = true Add Helm Repo \u00b6 helm repo add jetstack https://charts.jetstack.io helm repo update Install \u00b6 helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.8.0 \\ jetstack/cert-manager","title":"GKE"},{"location":"kubernetes/distributions/install-gke/#gke-with-helm","text":"","title":"GKE with Helm"},{"location":"kubernetes/distributions/install-gke/#env-variables","text":"CLUSTER_NAME = MyGKECluster REGION = europe-west4 NODE_LOCATIONS = ${ REGION } -a, ${ REGION } -b ZONE = europe-west4-a K8S_VERSION = 1 .11.5-gke.4 PROJECT_ID =","title":"Env Variables"},{"location":"kubernetes/distributions/install-gke/#get-kubernetes-versions","text":"gcloud container get-server-config --region $REGION","title":"Get Kubernetes versions"},{"location":"kubernetes/distributions/install-gke/#create-cluster","text":"gcloud container clusters create ${ CLUSTER_NAME } \\ --region ${ REGION } --node-locations ${ NODE_LOCATIONS } \\ --cluster-version ${ K8S_VERSION } \\ --num-nodes 2 --machine-type n1-standard-2 \\ --addons = HorizontalPodAutoscaling \\ --min-nodes 2 --max-nodes 3 \\ --enable-autoupgrade \\ --enable-autoscaling \\ --enable-network-policy \\ --labels = purpose = practice","title":"Create Cluster"},{"location":"kubernetes/distributions/install-gke/#post-install","text":"kubectl create clusterrolebinding cluster-admin-binding \\ --clusterrole cluster-admin \\ --user $( gcloud config get-value account )","title":"Post Install"},{"location":"kubernetes/distributions/install-gke/#delete-cluster","text":"gcloud container clusters delete $CLUSTER_NAME --region $REGION","title":"Delete Cluster"},{"location":"kubernetes/distributions/install-gke/#configure-kubeconfig","text":"gcloud container clusters get-credentials ${ CLUSTER_NAME } --region ${ REGION }","title":"Configure kubeconfig"},{"location":"kubernetes/distributions/install-gke/#install-cluster-tools","text":"","title":"Install Cluster Tools"},{"location":"kubernetes/distributions/install-gke/#helm","text":"We use Helm as a package manager to more easily install other tools on Kubernetes. There's several repositories with a large number of mature charts - the name of the Helm packages. One being Helm/Stable another being Helm Hub .","title":"Helm"},{"location":"kubernetes/distributions/install-gke/#create-service-account","text":"kubectl create serviceaccount --namespace kube-system tiller Warning Tiller is deemed not safe for production, at least not in its default configuration. Either enable its TLS configuration and take other measures (such as namespace limitation) or use alternative solutions. Such as Kustomize, Pulumi, Jenkins X or raw Yaml.","title":"Create service account"},{"location":"kubernetes/distributions/install-gke/#create-cluster-role-binding","text":"kubectl create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller","title":"Create cluster role binding"},{"location":"kubernetes/distributions/install-gke/#helm-init","text":"helm init --service-account tiller","title":"helm init"},{"location":"kubernetes/distributions/install-gke/#test-version","text":"helm version Warning Currently, nginx ingress controller has an issue with Helm 2.14. So if you 2.14, either downgrade to 2.13.1 or install the Ingress Controller via an alternative solution (such as Kustomize).","title":"Test version"},{"location":"kubernetes/distributions/install-gke/#ingress-controller","text":"helm install --namespace ingress-nginx --name nginx-ingress stable/nginx-ingress \\ --set controller.service.externalTrafficPolicy = Local \\ --set controller.replicaCount = 3 \\ --set rbac.create = true","title":"Ingress Controller"},{"location":"kubernetes/distributions/install-gke/#get-loadbalancer-ip","text":"export LB_IP = $( kubectl get svc -n ingress-nginx nginx-ingress-controller -o jsonpath = \"{.status.loadBalancer.ingress[0].ip}\" ) echo $LB_IP Warning Now is the time to configure your DNS to use whatever LB_IP 's value is.","title":"Get LoadBalancer IP"},{"location":"kubernetes/distributions/install-gke/#cert-manager","text":"Cert Manager is the recommended approach for managing TLS certificates in Kubernetes. If you do not want to manage certificates yourself, please use this. The certificates it uses are real and valid certificates, provided by Let's Encrypt .","title":"Cert Manager"},{"location":"kubernetes/distributions/install-gke/#install-crds","text":"kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8/deploy/manifests/00-crds.yaml","title":"Install CRD's"},{"location":"kubernetes/distributions/install-gke/#create-namespace","text":"kubectl create namespace cert-manager","title":"Create Namespace"},{"location":"kubernetes/distributions/install-gke/#label-namespace","text":"kubectl label namespace cert-manager certmanager.k8s.io/disable-validation = true","title":"Label namespace"},{"location":"kubernetes/distributions/install-gke/#add-helm-repo","text":"helm repo add jetstack https://charts.jetstack.io helm repo update","title":"Add Helm Repo"},{"location":"kubernetes/distributions/install-gke/#install","text":"helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.8.0 \\ jetstack/cert-manager","title":"Install"},{"location":"kubernetes/khw-gce/","text":"Kubernetes the Hard Way - GCE \u00b6 This assumes OSX and GCE. Goal \u00b6 The goal is to setup up HA Kubernetes cluster on GCE from it's most basic parts. That means we will install and configure the basic components ourselves, such as the API server and Kubelets. Setup \u00b6 As to limit the scope to doing the setup of the Kubernetes cluster ourselves, we will make it static. That means we will create and configure the network and compute resources to be fit for 3 Control Plane VM's and 3 worker VM's. We will not be able to recover a failing node or accomidate additional resources. Resources in GCE \u00b6 Public IP address, as front-end for the three API servers 3 VM's for the Control Plance 3 VM's as workers VPC Network Routes: from POD CIDR blocks to the host VM (for workers) Firewall configuration: allow health checks, dns, internal communication and connection to API server Kubernetes Resources \u00b6 Control Plane \u00b6 etcd : stores cluster state kube-api server : entry point for interacting with the cluster by exposing the api kube-scheduler : makes sure pods get scheduled kube-controller-manager : aggregate of required controllers Node Controller : > Responsible for noticing and responding when nodes go down. Replication Controller : > Responsible for maintaining the correct number of pods for every replication controller object in the system. Endpoints Controller : > Populates the Endpoints object (that is, joins Services & Pods). Service Account & Token Controller : > Create default accounts and API access tokens for new namespaces. Worker nodes \u00b6 kubelet : > An agent that runs on each node in the cluster. It makes sure that containers are running in a pod. kube-proxy : > kube-proxy enables the Kubernetes service abstraction by maintaining network rules on the host and performing connection forwarding A container runtime: this can be Docker , rkt or as in our case containerd Network \u00b6 https://blog.csnet.me/k8s-thw/part1/ https://github.com/kelseyhightower/kubernetes-the-hard-way We will be using the network components - with Weave-Net and CoreDNS - as described in the csnet blog. But we will use the CIDR blocks as stated in the Kelsey Hightower's Kubernetes the Hard Way ( KHW ). Kelsey's KHW \u00b6 Range Use 10.240.0.10/24 LAN (GCE VMS) 10.200.0.0/16 k8s Pod network 10.32.0.0/24 k8s Service network 10.32.0.1 k8s API server 10.32.0.10 k8s dns API Server: https://127.0.0.1:6443 service-cluster-ip-range=10.32.0.0/24 cluster-cidr=10.200.0.0/1 CSNETs \u00b6 Range Use 10.32.2.0/24 LAN (csnet.me) 10.16.0.0/16 k8s Pod network 10.10.0.0/22 k8s Service network 10.10.0.1 k8s API server 10.10.0.10 k8s dns API Server: https://10.32.2.97:6443 service-cluster-ip-range=10.10.0.0/22 cluster-cidr=10.16.0.0/16 Install tools \u00b6 On the machine doing the installation, we will need some tools installed. We will use the following tools: kubectl : for communicating with the API server cfssl : for creating the certificates and sign them helm : for installing additional tools later stern : for viewing logs of multiple pods at once (for example, all kube-dns pods) terraform : for managing our resources in GCE brew install kubernetes-cli brew install cfssl brew install kubernetes-helm brew install stern brew install terraform Check versions \u00b6 kubectl version -c -o yaml cfssl version helm version -c --short stern --version terraform version Terraform remote storage \u00b6 The help with problems of local storage and potential loss of data when local OS problems occur, we will use an S3 bucket as Terraform state storage. create s3 bucket configure Terraform to use this as remote state storage see how to this here read more about this, in Terraform's docs export AWS_ACCESS_KEY_ID = \"anaccesskey\" export AWS_SECRET_ACCESS_KEY = \"asecretkey\" export AWS_DEFAULT_REGION = \"eu-central-1\" terraform { backend \"s3\" { bucket = \"euros-terraform-state\" key = \"terraform.tfstate\" region = \"eu-central-1\" encrypt = \"true\" } } GKE Service Account \u00b6 Create a new GKE service account, and export it's json credentials file for use with Terraform. See GKE Tutorial page for how you can do this.","title":"Installer Preparation"},{"location":"kubernetes/khw-gce/#kubernetes-the-hard-way-gce","text":"This assumes OSX and GCE.","title":"Kubernetes the Hard Way - GCE"},{"location":"kubernetes/khw-gce/#goal","text":"The goal is to setup up HA Kubernetes cluster on GCE from it's most basic parts. That means we will install and configure the basic components ourselves, such as the API server and Kubelets.","title":"Goal"},{"location":"kubernetes/khw-gce/#setup","text":"As to limit the scope to doing the setup of the Kubernetes cluster ourselves, we will make it static. That means we will create and configure the network and compute resources to be fit for 3 Control Plane VM's and 3 worker VM's. We will not be able to recover a failing node or accomidate additional resources.","title":"Setup"},{"location":"kubernetes/khw-gce/#resources-in-gce","text":"Public IP address, as front-end for the three API servers 3 VM's for the Control Plance 3 VM's as workers VPC Network Routes: from POD CIDR blocks to the host VM (for workers) Firewall configuration: allow health checks, dns, internal communication and connection to API server","title":"Resources in GCE"},{"location":"kubernetes/khw-gce/#kubernetes-resources","text":"","title":"Kubernetes Resources"},{"location":"kubernetes/khw-gce/#control-plane","text":"etcd : stores cluster state kube-api server : entry point for interacting with the cluster by exposing the api kube-scheduler : makes sure pods get scheduled kube-controller-manager : aggregate of required controllers Node Controller : > Responsible for noticing and responding when nodes go down. Replication Controller : > Responsible for maintaining the correct number of pods for every replication controller object in the system. Endpoints Controller : > Populates the Endpoints object (that is, joins Services & Pods). Service Account & Token Controller : > Create default accounts and API access tokens for new namespaces.","title":"Control Plane"},{"location":"kubernetes/khw-gce/#worker-nodes","text":"kubelet : > An agent that runs on each node in the cluster. It makes sure that containers are running in a pod. kube-proxy : > kube-proxy enables the Kubernetes service abstraction by maintaining network rules on the host and performing connection forwarding A container runtime: this can be Docker , rkt or as in our case containerd","title":"Worker nodes"},{"location":"kubernetes/khw-gce/#network","text":"https://blog.csnet.me/k8s-thw/part1/ https://github.com/kelseyhightower/kubernetes-the-hard-way We will be using the network components - with Weave-Net and CoreDNS - as described in the csnet blog. But we will use the CIDR blocks as stated in the Kelsey Hightower's Kubernetes the Hard Way ( KHW ).","title":"Network"},{"location":"kubernetes/khw-gce/#kelseys-khw","text":"Range Use 10.240.0.10/24 LAN (GCE VMS) 10.200.0.0/16 k8s Pod network 10.32.0.0/24 k8s Service network 10.32.0.1 k8s API server 10.32.0.10 k8s dns API Server: https://127.0.0.1:6443 service-cluster-ip-range=10.32.0.0/24 cluster-cidr=10.200.0.0/1","title":"Kelsey's KHW"},{"location":"kubernetes/khw-gce/#csnets","text":"Range Use 10.32.2.0/24 LAN (csnet.me) 10.16.0.0/16 k8s Pod network 10.10.0.0/22 k8s Service network 10.10.0.1 k8s API server 10.10.0.10 k8s dns API Server: https://10.32.2.97:6443 service-cluster-ip-range=10.10.0.0/22 cluster-cidr=10.16.0.0/16","title":"CSNETs"},{"location":"kubernetes/khw-gce/#install-tools","text":"On the machine doing the installation, we will need some tools installed. We will use the following tools: kubectl : for communicating with the API server cfssl : for creating the certificates and sign them helm : for installing additional tools later stern : for viewing logs of multiple pods at once (for example, all kube-dns pods) terraform : for managing our resources in GCE brew install kubernetes-cli brew install cfssl brew install kubernetes-helm brew install stern brew install terraform","title":"Install tools"},{"location":"kubernetes/khw-gce/#check-versions","text":"kubectl version -c -o yaml cfssl version helm version -c --short stern --version terraform version","title":"Check versions"},{"location":"kubernetes/khw-gce/#terraform-remote-storage","text":"The help with problems of local storage and potential loss of data when local OS problems occur, we will use an S3 bucket as Terraform state storage. create s3 bucket configure Terraform to use this as remote state storage see how to this here read more about this, in Terraform's docs export AWS_ACCESS_KEY_ID = \"anaccesskey\" export AWS_SECRET_ACCESS_KEY = \"asecretkey\" export AWS_DEFAULT_REGION = \"eu-central-1\" terraform { backend \"s3\" { bucket = \"euros-terraform-state\" key = \"terraform.tfstate\" region = \"eu-central-1\" encrypt = \"true\" } }","title":"Terraform remote storage"},{"location":"kubernetes/khw-gce/#gke-service-account","text":"Create a new GKE service account, and export it's json credentials file for use with Terraform. See GKE Tutorial page for how you can do this.","title":"GKE Service Account"},{"location":"kubernetes/khw-gce/certificates/","text":"Certificates \u00b6 Note Before we can continue here, we need to have our nodes up and running with their external ip addresses and our fixed public ip address. This is because some certificates require these external ip addresses! gcloud compute instances list gcloud compute addresses list --filter = \"name=('kubernetes-the-hard-way')\" We need to create a whole lot of certificates, listed below, with the help of cfssl . A tool from CDN provider CloudFlare. Required certificates \u00b6 CA (or Certificate Authority): will be the root certificate of our trust chain result: ca.pem & ca-key.pem Admin : the admin of our cluster (you!) result: admin-key.pem & admin.pem Kubelet : the certificates of the kubelet processes on the worker nodes result: worker-0.pem worker-1-key.pem worker-1.pem worker-2-key.pem worker-2.pem Controller Manager result: kube-controller-manager-key.pem & kube-controller-manager.pem Scheduler result: kube-scheduler-key.pem & kube-scheduler.pem API Server result kubernetes-key.pem & kubernetes.pem Service Account : ??? result: service-account-key.pem & service-account.pem Certificate example \u00b6 Because we will use the cfssl tool from CloudFlare, we will define our certificate signing request (CSR's) in json. { \"CN\" : \"service-accounts\" , \"key\" : { \"algo\" : \"rsa\" , \"size\" : 2048 }, \"names\" : [ { \"C\" : \"NL\" , \"L\" : \"Utrecht\" , \"O\" : \"Kubernetes\" , \"OU\" : \"Kubernetes The Hard Way\" , \"ST\" : \"Utrecht\" } ] } Install scripts \u00b6 Make sure you're in k8s-the-hard-way/scripts ./certs.sh","title":"Prepare Certificates"},{"location":"kubernetes/khw-gce/certificates/#certificates","text":"Note Before we can continue here, we need to have our nodes up and running with their external ip addresses and our fixed public ip address. This is because some certificates require these external ip addresses! gcloud compute instances list gcloud compute addresses list --filter = \"name=('kubernetes-the-hard-way')\" We need to create a whole lot of certificates, listed below, with the help of cfssl . A tool from CDN provider CloudFlare.","title":"Certificates"},{"location":"kubernetes/khw-gce/certificates/#required-certificates","text":"CA (or Certificate Authority): will be the root certificate of our trust chain result: ca.pem & ca-key.pem Admin : the admin of our cluster (you!) result: admin-key.pem & admin.pem Kubelet : the certificates of the kubelet processes on the worker nodes result: worker-0.pem worker-1-key.pem worker-1.pem worker-2-key.pem worker-2.pem Controller Manager result: kube-controller-manager-key.pem & kube-controller-manager.pem Scheduler result: kube-scheduler-key.pem & kube-scheduler.pem API Server result kubernetes-key.pem & kubernetes.pem Service Account : ??? result: service-account-key.pem & service-account.pem","title":"Required certificates"},{"location":"kubernetes/khw-gce/certificates/#certificate-example","text":"Because we will use the cfssl tool from CloudFlare, we will define our certificate signing request (CSR's) in json. { \"CN\" : \"service-accounts\" , \"key\" : { \"algo\" : \"rsa\" , \"size\" : 2048 }, \"names\" : [ { \"C\" : \"NL\" , \"L\" : \"Utrecht\" , \"O\" : \"Kubernetes\" , \"OU\" : \"Kubernetes The Hard Way\" , \"ST\" : \"Utrecht\" } ] }","title":"Certificate example"},{"location":"kubernetes/khw-gce/certificates/#install-scripts","text":"Make sure you're in k8s-the-hard-way/scripts ./certs.sh","title":"Install scripts"},{"location":"kubernetes/khw-gce/controller/","text":"Controller Config \u00b6 We have to configure the following: move certificates to the correct location move encryption configuration to /var/lib/kubernetes download and install binaries kubectl kube-apiserver kube-scheduler kube-controller-manager configure API server systemd service configure Controller Manager systemd service configure Scheduler systemd service kubernetes configuration yaml kind: KubeSchedulerConfiguration create nginx reverse proxy to enable GCE's health checks to reach each API Server instance configure RBAC configuration in the API server via ClusterRole and ClusterRoleBinding Install \u00b6 We have an installer script, controller-local.sh , which should be executed on each controller VM. To do so, use the controller.sh script to upload this file to the VM's. ./controller.sh","title":"Controller Config"},{"location":"kubernetes/khw-gce/controller/#controller-config","text":"We have to configure the following: move certificates to the correct location move encryption configuration to /var/lib/kubernetes download and install binaries kubectl kube-apiserver kube-scheduler kube-controller-manager configure API server systemd service configure Controller Manager systemd service configure Scheduler systemd service kubernetes configuration yaml kind: KubeSchedulerConfiguration create nginx reverse proxy to enable GCE's health checks to reach each API Server instance configure RBAC configuration in the API server via ClusterRole and ClusterRoleBinding","title":"Controller Config"},{"location":"kubernetes/khw-gce/controller/#install","text":"We have an installer script, controller-local.sh , which should be executed on each controller VM. To do so, use the controller.sh script to upload this file to the VM's. ./controller.sh","title":"Install"},{"location":"kubernetes/khw-gce/debug/","text":"Debug \u00b6 Kubernetes components not healthy \u00b6 Check for healthy status \u00b6 On a control plane node, check etcd . sudo ETCDCTL_API = 3 etcdctl member list \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem 3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379 f98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379 ffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379 On a control plan node, check control plane components. kubectl get componentstatuses --kubeconfig admin.kubeconfig Should look like this: NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy { \"health\" : \"true\" } etcd-0 Healthy { \"health\" : \"true\" } etcd-1 Healthy { \"health\" : \"true\" } On a control plane node, check API server status (via nginx reverse proxy). curl -H \"Host: kubernetes.default.svc.cluster.local\" -i http://127.0.0.1/healthz HTTP/1.1 200 OK Server: nginx/1.14.0 ( Ubuntu ) Date: Mon, 14 May 2018 13 :45:39 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 2 Connection: keep-alive ok On an external system, you can check if the API server is working and reachable via routing. curl --cacert ca.pem https:// ${ KUBERNETES_PUBLIC_ADDRESS } :6443/version Assuming that GCE is used. KUBERNETES_PUBLIC_ADDRESS = $( gcloud compute addresses describe kubernetes-the-hard-way \\ --region $( gcloud config get-value compute/region ) \\ --format 'value(address)' ) Check for errors \u00b6 journalctl Or for specific components. journalctl -u kube-scheduler Weave-Net pods Blocked \u00b6 Sometimes when installing weave-net as the CNI plugin, the pods are blocked. NAME READY STATUS RESTARTS AGE weave-net-fwvsr 0 /2 Blocked 0 3m weave-net-v9z9n 0 /2 Blocked 0 3m weave-net-zfghq 0 /2 Blocked 0 3m Usually this means something went wrong with the CNI configuration. Ideally, Weave-Net will generate this when installed, but sometimes this doesn't happen. This is easily found when checking the journalctl on the worker nodes ( journalctl -u kubelet ). There are three things to be done before installing weave-net again. Ensure ip4 forwarding is enabled \u00b6 sysctl net.ipv4.ip_forward = 1 sysctl -p /etc/sysctl.conf See Kubernetes Docs for GCE routing or Michael Champagne 's blog on KHW. Ensure all weave-net resources are gone \u00b6 I've noticed that when this problem occurs, deleting the weave-net resources with kubectl delete -f <weaveNet resource> leaves the pods. The pods are terminated (they never started) but are not removed. To remove them, use the line below, as explained on stackoverflow . kubectl delete pod NAME --grace-period = 0 --force Restart Kubelet \u00b6 I'm not sure if this is 100% required, but I've had better luck with restarting the kubelet before reinstalling weave-net. So, login to each worker node, gcloud compute ssh worker-? and issue the following commands. sudo systemctl daemon-reload sudo systemctl restart kubelet DNS on GCE not working \u00b6 It seemed something has changed in GCE after Kelsey Hightower's Kubernetes The Hardway was written/updated. This means that if you follow through the documentation, you will run into this: kubectl exec -ti $POD_NAME -- nslookup kubernetes ;; connection timed out ; no servers could be reached command terminated with exit code 1 The cure seems to be to add additional resolve.conf file configuration to the kubelet's systemd service definition. cat <<EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description = Kubernetes Kubelet Documentation = https://github.com/kubernetes/kubernetes After = containerd.service Requires = containerd.service [Service] ExecStart = /usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --container-runtime=remote \\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\ --resolv-conf=/run/systemd/resolve/resolv.conf \\ --image-pull-progress-deadline=2m \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --network-plugin=cni \\ --register-node=true \\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF In addition, one should also use at least busybox 1.28 to do the dns check. For more information, read this issue .","title":"Debug"},{"location":"kubernetes/khw-gce/debug/#debug","text":"","title":"Debug"},{"location":"kubernetes/khw-gce/debug/#kubernetes-components-not-healthy","text":"","title":"Kubernetes components not healthy"},{"location":"kubernetes/khw-gce/debug/#check-for-healthy-status","text":"On a control plane node, check etcd . sudo ETCDCTL_API = 3 etcdctl member list \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem 3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379 f98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379 ffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379 On a control plan node, check control plane components. kubectl get componentstatuses --kubeconfig admin.kubeconfig Should look like this: NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy { \"health\" : \"true\" } etcd-0 Healthy { \"health\" : \"true\" } etcd-1 Healthy { \"health\" : \"true\" } On a control plane node, check API server status (via nginx reverse proxy). curl -H \"Host: kubernetes.default.svc.cluster.local\" -i http://127.0.0.1/healthz HTTP/1.1 200 OK Server: nginx/1.14.0 ( Ubuntu ) Date: Mon, 14 May 2018 13 :45:39 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 2 Connection: keep-alive ok On an external system, you can check if the API server is working and reachable via routing. curl --cacert ca.pem https:// ${ KUBERNETES_PUBLIC_ADDRESS } :6443/version Assuming that GCE is used. KUBERNETES_PUBLIC_ADDRESS = $( gcloud compute addresses describe kubernetes-the-hard-way \\ --region $( gcloud config get-value compute/region ) \\ --format 'value(address)' )","title":"Check for healthy status"},{"location":"kubernetes/khw-gce/debug/#check-for-errors","text":"journalctl Or for specific components. journalctl -u kube-scheduler","title":"Check for errors"},{"location":"kubernetes/khw-gce/debug/#weave-net-pods-blocked","text":"Sometimes when installing weave-net as the CNI plugin, the pods are blocked. NAME READY STATUS RESTARTS AGE weave-net-fwvsr 0 /2 Blocked 0 3m weave-net-v9z9n 0 /2 Blocked 0 3m weave-net-zfghq 0 /2 Blocked 0 3m Usually this means something went wrong with the CNI configuration. Ideally, Weave-Net will generate this when installed, but sometimes this doesn't happen. This is easily found when checking the journalctl on the worker nodes ( journalctl -u kubelet ). There are three things to be done before installing weave-net again.","title":"Weave-Net pods Blocked"},{"location":"kubernetes/khw-gce/debug/#ensure-ip4-forwarding-is-enabled","text":"sysctl net.ipv4.ip_forward = 1 sysctl -p /etc/sysctl.conf See Kubernetes Docs for GCE routing or Michael Champagne 's blog on KHW.","title":"Ensure ip4 forwarding is enabled"},{"location":"kubernetes/khw-gce/debug/#ensure-all-weave-net-resources-are-gone","text":"I've noticed that when this problem occurs, deleting the weave-net resources with kubectl delete -f <weaveNet resource> leaves the pods. The pods are terminated (they never started) but are not removed. To remove them, use the line below, as explained on stackoverflow . kubectl delete pod NAME --grace-period = 0 --force","title":"Ensure all weave-net resources are gone"},{"location":"kubernetes/khw-gce/debug/#restart-kubelet","text":"I'm not sure if this is 100% required, but I've had better luck with restarting the kubelet before reinstalling weave-net. So, login to each worker node, gcloud compute ssh worker-? and issue the following commands. sudo systemctl daemon-reload sudo systemctl restart kubelet","title":"Restart Kubelet"},{"location":"kubernetes/khw-gce/debug/#dns-on-gce-not-working","text":"It seemed something has changed in GCE after Kelsey Hightower's Kubernetes The Hardway was written/updated. This means that if you follow through the documentation, you will run into this: kubectl exec -ti $POD_NAME -- nslookup kubernetes ;; connection timed out ; no servers could be reached command terminated with exit code 1 The cure seems to be to add additional resolve.conf file configuration to the kubelet's systemd service definition. cat <<EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description = Kubernetes Kubelet Documentation = https://github.com/kubernetes/kubernetes After = containerd.service Requires = containerd.service [Service] ExecStart = /usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --container-runtime=remote \\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\ --resolv-conf=/run/systemd/resolve/resolv.conf \\ --image-pull-progress-deadline=2m \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --network-plugin=cni \\ --register-node=true \\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF In addition, one should also use at least busybox 1.28 to do the dns check. For more information, read this issue .","title":"DNS on GCE not working"},{"location":"kubernetes/khw-gce/encryption/","text":"Encryption \u00b6 Kubernetes stores a variety of data including cluster state, application configurations, and secrets. Kubernetes supports the ability to encrypt cluster data at rest. In order to use this ability to encrypt data at rest, each member of the control plane has to know the encryption key. So we will have to create one. Encryption configuration \u00b6 We have to create a encryption key first. For the sake of embedding it into a yaml file, we will have to encode it to base64 . ENCRYPTION_KEY = $( head -c 32 /dev/urandom | base64 ) Install scripts \u00b6 Make sure you're in k8s-the-hard-way/scripts ./encryption.sh","title":"Encryption configuration"},{"location":"kubernetes/khw-gce/encryption/#encryption","text":"Kubernetes stores a variety of data including cluster state, application configurations, and secrets. Kubernetes supports the ability to encrypt cluster data at rest. In order to use this ability to encrypt data at rest, each member of the control plane has to know the encryption key. So we will have to create one.","title":"Encryption"},{"location":"kubernetes/khw-gce/encryption/#encryption-configuration","text":"We have to create a encryption key first. For the sake of embedding it into a yaml file, we will have to encode it to base64 . ENCRYPTION_KEY = $( head -c 32 /dev/urandom | base64 )","title":"Encryption configuration"},{"location":"kubernetes/khw-gce/encryption/#install-scripts","text":"Make sure you're in k8s-the-hard-way/scripts ./encryption.sh","title":"Install scripts"},{"location":"kubernetes/khw-gce/etcd/","text":"ETCD \u00b6 Kubernetes components are stateless and store cluster state in etcd. In this lab you will bootstrap a three node etcd cluster and configure it for high availability and secure remote access. The bare minimum is to have a single etcd instance running. But for production purposes it is best to run etcd in HA mode. This means we need to have three instances running that know eachother. Again, this is not a production ready setup, as the static nature prevents automatic recovery if a node fails. Steps to take \u00b6 download & install etcd binary prepare required certificates create systemd service definition reload systemd configuration, enable & start the service Install script \u00b6 Make sure that the local install script is on every server, you can use the etcd.sh script for this. Then, make sure you're connect to all three controller VM's at the same time, for example via tmux or iterm. For iterm: use ctrl + shift + d to open three horizontal windows use ctrl + shift + i to write output to all three windows at once login to each controller gcloud compute ssh controller-? ./etcd-local.sh Verification \u00b6 sudo ETCDCTL_API = 3 etcdctl member list \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem Expected Output \u00b6 3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379 f98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379 ffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379","title":"ETCD configuration"},{"location":"kubernetes/khw-gce/etcd/#etcd","text":"Kubernetes components are stateless and store cluster state in etcd. In this lab you will bootstrap a three node etcd cluster and configure it for high availability and secure remote access. The bare minimum is to have a single etcd instance running. But for production purposes it is best to run etcd in HA mode. This means we need to have three instances running that know eachother. Again, this is not a production ready setup, as the static nature prevents automatic recovery if a node fails.","title":"ETCD"},{"location":"kubernetes/khw-gce/etcd/#steps-to-take","text":"download & install etcd binary prepare required certificates create systemd service definition reload systemd configuration, enable & start the service","title":"Steps to take"},{"location":"kubernetes/khw-gce/etcd/#install-script","text":"Make sure that the local install script is on every server, you can use the etcd.sh script for this. Then, make sure you're connect to all three controller VM's at the same time, for example via tmux or iterm. For iterm: use ctrl + shift + d to open three horizontal windows use ctrl + shift + i to write output to all three windows at once login to each controller gcloud compute ssh controller-? ./etcd-local.sh","title":"Install script"},{"location":"kubernetes/khw-gce/etcd/#verification","text":"sudo ETCDCTL_API = 3 etcdctl member list \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem","title":"Verification"},{"location":"kubernetes/khw-gce/etcd/#expected-output","text":"3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379 f98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379 ffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379","title":"Expected Output"},{"location":"kubernetes/khw-gce/kubeconfigs/","text":"Kubeconfigs \u00b6 Now that we have certificates we have to make sure we have configurations that the Kubernetes parts can actually use - certificates themselves are not enough. This is where we will use kubernetes configuration files, or kubeconfigs . We will have to create the following kubeconfigs : controller manager kubelet kube-proxy kube-scheduler admin user Create & Test kubeconfig file \u00b6 Here's an example script: kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate = kube-controller-manager.pem \\ --client-key = kube-controller-manager-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-controller-manager \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig = kube-controller-manager.kubeconfig The steps we execute in order are the following: create a kubeconfig entry for our kubernetes-the-hard-way cluster and export this into a .kubeconfig file add credentials to this config file, in the form of our kubernetes component's certificate set the default config of this config file to namespace default and user to the component we're configuring test the configuration file by using it Install scripts \u00b6 Make sure you're in k8s-the-hard-way/scripts ./kube-configs.sh","title":"Prepare Kubeconfigs"},{"location":"kubernetes/khw-gce/kubeconfigs/#kubeconfigs","text":"Now that we have certificates we have to make sure we have configurations that the Kubernetes parts can actually use - certificates themselves are not enough. This is where we will use kubernetes configuration files, or kubeconfigs . We will have to create the following kubeconfigs : controller manager kubelet kube-proxy kube-scheduler admin user","title":"Kubeconfigs"},{"location":"kubernetes/khw-gce/kubeconfigs/#create-test-kubeconfig-file","text":"Here's an example script: kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate = kube-controller-manager.pem \\ --client-key = kube-controller-manager-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-controller-manager \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig = kube-controller-manager.kubeconfig The steps we execute in order are the following: create a kubeconfig entry for our kubernetes-the-hard-way cluster and export this into a .kubeconfig file add credentials to this config file, in the form of our kubernetes component's certificate set the default config of this config file to namespace default and user to the component we're configuring test the configuration file by using it","title":"Create &amp; Test kubeconfig file"},{"location":"kubernetes/khw-gce/kubeconfigs/#install-scripts","text":"Make sure you're in k8s-the-hard-way/scripts ./kube-configs.sh","title":"Install scripts"},{"location":"kubernetes/khw-gce/network/","text":"Networking \u00b6 First, configure external access so we can run kubectl commands from our own machine. Confirm the you can now call the following: kubectl get nodes -o wide Configure WeaveNet \u00b6 kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) &env.IPALLOC_RANGE=10.200.0.0/16\" Confirm WeaveNet works \u00b6 kubectl get pod --namespace = kube-system -l name = weave-net It should look like this: NAME READY STATUS RESTARTS AGE weave-net-fwvsr 2 /2 Running 1 4h weave-net-v9z9n 2 /2 Running 1 4h weave-net-zfghq 2 /2 Running 1 4h Configure CoreDNS \u00b6 Before installing CoreDNS , please confirm networking is in order. kubectl get nodes -o wide Warning If nodes are not Ready , something is wrong and needs to be fixed before you continue. kubectl apply -f ../configs/core-dns-config.yaml Confirm CoreDNS pods \u00b6 kubectl get pod --all-namespaces -l k8s-app = coredns -o wide Confirm DNS works \u00b6 kubectl run busybox --image = busybox:1.28 --command -- sleep 3600 POD_NAME = $( kubectl get pods -l run = busybox -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl exec -ti $POD_NAME -- nslookup kubernetes Note It should look like this: Server: 10 .10.0.10 Address 1 : 10 .10.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1 : 10 .10.0.1 kubernetes.default.svc.cluster.local","title":"Network config"},{"location":"kubernetes/khw-gce/network/#networking","text":"First, configure external access so we can run kubectl commands from our own machine. Confirm the you can now call the following: kubectl get nodes -o wide","title":"Networking"},{"location":"kubernetes/khw-gce/network/#configure-weavenet","text":"kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) &env.IPALLOC_RANGE=10.200.0.0/16\"","title":"Configure WeaveNet"},{"location":"kubernetes/khw-gce/network/#confirm-weavenet-works","text":"kubectl get pod --namespace = kube-system -l name = weave-net It should look like this: NAME READY STATUS RESTARTS AGE weave-net-fwvsr 2 /2 Running 1 4h weave-net-v9z9n 2 /2 Running 1 4h weave-net-zfghq 2 /2 Running 1 4h","title":"Confirm WeaveNet works"},{"location":"kubernetes/khw-gce/network/#configure-coredns","text":"Before installing CoreDNS , please confirm networking is in order. kubectl get nodes -o wide Warning If nodes are not Ready , something is wrong and needs to be fixed before you continue. kubectl apply -f ../configs/core-dns-config.yaml","title":"Configure CoreDNS"},{"location":"kubernetes/khw-gce/network/#confirm-coredns-pods","text":"kubectl get pod --all-namespaces -l k8s-app = coredns -o wide","title":"Confirm CoreDNS pods"},{"location":"kubernetes/khw-gce/network/#confirm-dns-works","text":"kubectl run busybox --image = busybox:1.28 --command -- sleep 3600 POD_NAME = $( kubectl get pods -l run = busybox -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl exec -ti $POD_NAME -- nslookup kubernetes Note It should look like this: Server: 10 .10.0.10 Address 1 : 10 .10.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1 : 10 .10.0.1 kubernetes.default.svc.cluster.local","title":"Confirm DNS works"},{"location":"kubernetes/khw-gce/remote-access/","text":"Remote Access \u00b6 KUBERNETES_PUBLIC_ADDRESS = $( gcloud compute addresses describe kubernetes-the-hard-way \\ --region $( gcloud config get-value compute/region ) \\ --format 'value(address)' ) echo \"KUBERNETES_PUBLIC_ADDRESS= ${ KUBERNETES_PUBLIC_ADDRESS } \" kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https:// ${ KUBERNETES_PUBLIC_ADDRESS } :6443 kubectl config set-credentials admin \\ --client-certificate = admin.pem \\ --client-key = admin-key.pem kubectl config set-context kubernetes-the-hard-way \\ --cluster = kubernetes-the-hard-way \\ --user = admin kubectl config use-context kubernetes-the-hard-way Confirm \u00b6 kubectl get nodes -o wide","title":"Remote access"},{"location":"kubernetes/khw-gce/remote-access/#remote-access","text":"KUBERNETES_PUBLIC_ADDRESS = $( gcloud compute addresses describe kubernetes-the-hard-way \\ --region $( gcloud config get-value compute/region ) \\ --format 'value(address)' ) echo \"KUBERNETES_PUBLIC_ADDRESS= ${ KUBERNETES_PUBLIC_ADDRESS } \" kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https:// ${ KUBERNETES_PUBLIC_ADDRESS } :6443 kubectl config set-credentials admin \\ --client-certificate = admin.pem \\ --client-key = admin-key.pem kubectl config set-context kubernetes-the-hard-way \\ --cluster = kubernetes-the-hard-way \\ --user = admin kubectl config use-context kubernetes-the-hard-way","title":"Remote Access"},{"location":"kubernetes/khw-gce/remote-access/#confirm","text":"kubectl get nodes -o wide","title":"Confirm"},{"location":"kubernetes/khw-gce/terraform-compute/","text":"Compute resources \u00b6 Create network \u00b6 VPC with Firewall rules \u00b6 provider \"google\" { credentials = \"${file(\"${var.credentials_file_path}\")}\" project = \"${var.project_name}\" region = \"${var.region}\" } resource \"google_compute_network\" \"khw\" { name = \"kubernetes-the-hard-way\" auto_create_subnetworks = \"false\" } resource \"google_compute_subnetwork\" \"khw-kubernetes\" { name = \"kubernetes\" ip_cidr_range = \"10.240.0.0/24\" region = \"${var.region}\" network = \"${google_compute_network.khw.self_link}\" } resource \"google_compute_firewall\" \"khw-allow-internal\" { name = \"kubernetes-the-hard-way-allow-internal\" network = \"${google_compute_network.khw.name}\" source_ranges = [ \"10.240.0.0/24\", \"10.200.0.0/16\" ] allow { protocol = \"tcp\" } allow { protocol = \"udp\" } allow { protocol = \"icmp\" } } resource \"google_compute_firewall\" \"khw-allow-external\" { name = \"kubernetes-the-hard-way-allow-external\" network = \"${google_compute_network.khw.name}\" allow { protocol = \"icmp\" } allow { protocol = \"tcp\" ports = [ \"22\", \"6443\" ] } source_ranges = [ \"0.0.0.0/0\" ] } resource \"google_compute_firewall\" \"khw-allow-dns\" { name = \"kubernetes-the-hard-way-allow-dns\" network = \"${google_compute_network.khw.name}\" source_ranges = [ \"0.0.0.0\" ] allow { protocol = \"tcp\" ports = [ \"53\", \"443\" ] } allow { protocol = \"udp\" ports = [ \"53\" ] } } resource \"google_compute_firewall\" \"khw-allow-health-check\" { name = \"kubernetes-the-hard-way-allow-health-check\" network = \"${google_compute_network.khw.name}\" allow { protocol = \"tcp\" } source_ranges = [ \"209.85.152.0/22\", \"209.85.204.0/22\", \"35.191.0.0/16\" ] } Confirm network \u00b6 gcloud compute firewall-rules list --filter = \"network:kubernetes-the-hard-way\" Should look like: NAME NETWORK DIRECTION PRIORITY ALLOW DENY kubernetes-the-hard-way-allow-external kubernetes-the-hard-way INGRESS 1000 icmp,tcp:22,tcp:6443 kubernetes-the-hard-way-allow-internal kubernetes-the-hard-way INGRESS 1000 icmp,udp,tcp Public IP \u00b6 resource \"google_compute_address\" \"khw-lb-public-ip\" { name = \"kubernetes-the-hard-way\" } Confirm: gcloud compute addresses list --filter = \"name=('kubernetes-the-hard-way')\" Output: NAME REGION ADDRESS STATUS kubernetes-the-hard-way europe-west4 35 .204.134.219 RESERVED VM Definitions with Terraform modules \u00b6 We're going to need to create 6 VM's. 3 Controller nodes and 3 worker nodes. Within each of the two categories, all the three VM's will be the same. So it would be a waste to define them more than once. This can be achieved via Terraform's Module system (read more here . Define a module \u00b6 For the sake of naming convention, we'll put all of our modules in a modules subfolder. We'll start with the controller module, but you can do the same for the worker. mkdir -p modules/controller ls -lath drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 . drwxr-xr-x 20 joostvdg staff 640B Aug 22 14 :47 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 modules ls -lath modules drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 . drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :03 controller Inside modules/controller we create two files, main.tf and variables.tf . We have to create an additional variables file, as the module cannot use the main folder's variables. Then, in our main folder we'll create a tf file for using these modules, called nodes.tf . As stated above, we pass along any variable from our main variables.tf to the module. module \"controller\" { source = \"modules/controller\" machine_type = \"${var.machine_type_controllers}\" num = \"${var.num_controllers}\" zone = \"${var.region_default_zone}\" subnet = \"${var.subnet_name}\" } module \"worker\" { source = \"modules/worker\" machine_type = \"${var.machine_type_workers}\" num = \"${var.num_workers}\" zone = \"${var.region_default_zone}\" network = \"${google_compute_network.khw.name}\" subnet = \"${var.subnet_name}\" } Controller config \u00b6 data \"google_compute_image\" \"khw-ubuntu\" { family = \"ubuntu-1804-lts\" project = \"ubuntu-os-cloud\" } resource \"google_compute_instance\" \"khw-controller\" { count = \"${var.num}\" name = \"controller-${count.index}\" machine_type = \"${var.machine_type}\" zone = \"${var.zone}\" can_ip_forward = \"true\" tags = [ \"kubernetes-the-hard-way\", \"controller\" ] boot_disk { initialize_params { image = \"${data.google_compute_image.khw-ubuntu.self_link}\" size = 200 // in GB } } network_interface { subnetwork = \"${var.subnet}\" address = \"10.240.0.1${count.index}\" access_config { // Ephemeral External IP } } # compute-rw,storage-ro,service-management,service-control,logging-write,monitoring service_account { scopes = [ \"compute-rw\" , \"storage-ro\" , \"service-management\" , \"service-control\" , \"logging-write\" , \"monitoring\" , ] } } Variables \u00b6 variable \"num\" { description = \"The number of controller VMs\" } variable \"machine_type\" { description = \"The type of VM for controllers\" } variable \"zone\" { description = \"The zone to create the controllers in\" } variable \"subnet\" { description = \"The subnet to create the nic in\" } Worker config \u00b6 Extra config for the worker are the routes, to aid the pods going out of the node. data \"google_compute_image\" \"khw-ubuntu\" { family = \"ubuntu-1804-lts\" project = \"ubuntu-os-cloud\" } resource \"google_compute_instance\" \"khw-worker\" { count = \"${var.num}\" name = \"worker-${count.index}\" machine_type = \"${var.machine_type}\" zone = \"${var.zone}\" can_ip_forward = \"true\" tags = [ \"kubernetes-the-hard-way\", \"worker\" ] metadata { pod-cidr = \"10.200.${count.index}.0/24\" } boot_disk { initialize_params { image = \"${data.google_compute_image.khw-ubuntu.self_link}\" size = 200 // in GB } } network_interface { subnetwork = \"${var.subnet}\" address = \"10.240.0.2${count.index}\" access_config { // Ephemeral External IP } } service_account { scopes = [ \"compute-rw\" , \"storage-ro\" , \"service-management\" , \"service-control\" , \"logging-write\" , \"monitoring\" , ] } } resource \"google_compute_route\" \"khw-worker-route\" { count = \"${var.num}\" name = \"kubernetes-route-10-200-${count.index}-0-24\" network = \"${var.network}\" next_hop_ip = \"10.240.0.2${count.index}\" dest_range = \"10.200.${count.index}.0/24\" } Variables \u00b6 variable \"num\" { description = \"The number of controller VMs\" } variable \"machine_type\" { description = \"The type of VM for controllers\" } variable \"zone\" { description = \"The zone to create the controllers in\" } variable \"network\" { description = \"The network to use for routes\" } variable \"subnet\" { description = \"The subnet to create the nic in\" } Health check \u00b6 Because we will have three controllers, we have to make sure that GKE forwards Kubernetes API requests to each of them via our public IP address. We do this via a http health check, wich involves a forwarding rule and a target pool. Target pool being the group of controller VM's for which the forwarding rule is active. resource \"google_compute_target_pool\" \"khw-hc-target-pool\" { name = \"instance-pool\" # TODO: fixed set for now, maybe we can make this dynamic some day instances = [ \"${var.region_default_zone}/controller-0\" , \"${var.region_default_zone}/controller-1\" , \"${var.region_default_zone}/controller-2\" , ] health_checks = [ \"${google_compute_http_health_check.khw-health-check.name}\" , ] } resource \"google_compute_http_health_check\" \"khw-health-check\" { name = \"kubernetes\" request_path = \"/healthz\" description = \"The health check for Kubernetes API server\" host = \"${var.kubernetes-cluster-dns}\" } resource \"google_compute_forwarding_rule\" \"khw-hc-forward\" { name = \"kubernetes-forwarding-rule\" target = \"${google_compute_target_pool.khw-hc-target-pool.self_link}\" region = \"${var.region}\" port_range = \"6443\" ip_address = \"${google_compute_address.khw-lb-public-ip.self_link}\" } Apply Terraform state \u00b6 In the end, our configuration should consist out of several .tf files and look something like this. ls -lath drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 . drwxr-xr-x 20 joostvdg staff 640B Aug 22 14 :47 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 modules -rw-r--r-- 1 joostvdg staff 1 .5K Aug 26 12 :50 variables.tf -rw-r--r-- 1 joostvdg staff 1 .3K Aug 17 16 :03 firewall.tf -rw-r--r-- 1 joostvdg staff 4 .4K Aug 17 12 :06 worker-config.md -rw-r--r-- 1 joostvdg staff 1 .6K Aug 17 09 :35 healthcheck.tf -rw-r--r-- 1 joostvdg staff 517B Aug 16 17 :09 nodes.tf -rw-r--r-- 1 joostvdg staff 92B Aug 16 13 :52 publicip.tf -rw-r--r-- 1 joostvdg staff 365B Aug 7 22 :07 vpc.tf -rw-r--r-- 1 joostvdg staff 189B Aug 7 16 :51 base.tf drwxr-xr-x 5 joostvdg staff 160B Aug 7 21 :52 .terraform -rw-r--r-- 1 joostvdg staff 0B Aug 7 18 :28 terraform.tfstate We're now going to plan and then apply our Terraform configuration to create the resources in GCE. terraform plan terraform apply","title":"Create GCE resources"},{"location":"kubernetes/khw-gce/terraform-compute/#compute-resources","text":"","title":"Compute resources"},{"location":"kubernetes/khw-gce/terraform-compute/#create-network","text":"","title":"Create network"},{"location":"kubernetes/khw-gce/terraform-compute/#vpc-with-firewall-rules","text":"provider \"google\" { credentials = \"${file(\"${var.credentials_file_path}\")}\" project = \"${var.project_name}\" region = \"${var.region}\" } resource \"google_compute_network\" \"khw\" { name = \"kubernetes-the-hard-way\" auto_create_subnetworks = \"false\" } resource \"google_compute_subnetwork\" \"khw-kubernetes\" { name = \"kubernetes\" ip_cidr_range = \"10.240.0.0/24\" region = \"${var.region}\" network = \"${google_compute_network.khw.self_link}\" } resource \"google_compute_firewall\" \"khw-allow-internal\" { name = \"kubernetes-the-hard-way-allow-internal\" network = \"${google_compute_network.khw.name}\" source_ranges = [ \"10.240.0.0/24\", \"10.200.0.0/16\" ] allow { protocol = \"tcp\" } allow { protocol = \"udp\" } allow { protocol = \"icmp\" } } resource \"google_compute_firewall\" \"khw-allow-external\" { name = \"kubernetes-the-hard-way-allow-external\" network = \"${google_compute_network.khw.name}\" allow { protocol = \"icmp\" } allow { protocol = \"tcp\" ports = [ \"22\", \"6443\" ] } source_ranges = [ \"0.0.0.0/0\" ] } resource \"google_compute_firewall\" \"khw-allow-dns\" { name = \"kubernetes-the-hard-way-allow-dns\" network = \"${google_compute_network.khw.name}\" source_ranges = [ \"0.0.0.0\" ] allow { protocol = \"tcp\" ports = [ \"53\", \"443\" ] } allow { protocol = \"udp\" ports = [ \"53\" ] } } resource \"google_compute_firewall\" \"khw-allow-health-check\" { name = \"kubernetes-the-hard-way-allow-health-check\" network = \"${google_compute_network.khw.name}\" allow { protocol = \"tcp\" } source_ranges = [ \"209.85.152.0/22\", \"209.85.204.0/22\", \"35.191.0.0/16\" ] }","title":"VPC with Firewall rules"},{"location":"kubernetes/khw-gce/terraform-compute/#confirm-network","text":"gcloud compute firewall-rules list --filter = \"network:kubernetes-the-hard-way\" Should look like: NAME NETWORK DIRECTION PRIORITY ALLOW DENY kubernetes-the-hard-way-allow-external kubernetes-the-hard-way INGRESS 1000 icmp,tcp:22,tcp:6443 kubernetes-the-hard-way-allow-internal kubernetes-the-hard-way INGRESS 1000 icmp,udp,tcp","title":"Confirm network"},{"location":"kubernetes/khw-gce/terraform-compute/#public-ip","text":"resource \"google_compute_address\" \"khw-lb-public-ip\" { name = \"kubernetes-the-hard-way\" } Confirm: gcloud compute addresses list --filter = \"name=('kubernetes-the-hard-way')\" Output: NAME REGION ADDRESS STATUS kubernetes-the-hard-way europe-west4 35 .204.134.219 RESERVED","title":"Public IP"},{"location":"kubernetes/khw-gce/terraform-compute/#vm-definitions-with-terraform-modules","text":"We're going to need to create 6 VM's. 3 Controller nodes and 3 worker nodes. Within each of the two categories, all the three VM's will be the same. So it would be a waste to define them more than once. This can be achieved via Terraform's Module system (read more here .","title":"VM Definitions with Terraform modules"},{"location":"kubernetes/khw-gce/terraform-compute/#define-a-module","text":"For the sake of naming convention, we'll put all of our modules in a modules subfolder. We'll start with the controller module, but you can do the same for the worker. mkdir -p modules/controller ls -lath drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 . drwxr-xr-x 20 joostvdg staff 640B Aug 22 14 :47 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 modules ls -lath modules drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 . drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :03 controller Inside modules/controller we create two files, main.tf and variables.tf . We have to create an additional variables file, as the module cannot use the main folder's variables. Then, in our main folder we'll create a tf file for using these modules, called nodes.tf . As stated above, we pass along any variable from our main variables.tf to the module. module \"controller\" { source = \"modules/controller\" machine_type = \"${var.machine_type_controllers}\" num = \"${var.num_controllers}\" zone = \"${var.region_default_zone}\" subnet = \"${var.subnet_name}\" } module \"worker\" { source = \"modules/worker\" machine_type = \"${var.machine_type_workers}\" num = \"${var.num_workers}\" zone = \"${var.region_default_zone}\" network = \"${google_compute_network.khw.name}\" subnet = \"${var.subnet_name}\" }","title":"Define a module"},{"location":"kubernetes/khw-gce/terraform-compute/#controller-config","text":"data \"google_compute_image\" \"khw-ubuntu\" { family = \"ubuntu-1804-lts\" project = \"ubuntu-os-cloud\" } resource \"google_compute_instance\" \"khw-controller\" { count = \"${var.num}\" name = \"controller-${count.index}\" machine_type = \"${var.machine_type}\" zone = \"${var.zone}\" can_ip_forward = \"true\" tags = [ \"kubernetes-the-hard-way\", \"controller\" ] boot_disk { initialize_params { image = \"${data.google_compute_image.khw-ubuntu.self_link}\" size = 200 // in GB } } network_interface { subnetwork = \"${var.subnet}\" address = \"10.240.0.1${count.index}\" access_config { // Ephemeral External IP } } # compute-rw,storage-ro,service-management,service-control,logging-write,monitoring service_account { scopes = [ \"compute-rw\" , \"storage-ro\" , \"service-management\" , \"service-control\" , \"logging-write\" , \"monitoring\" , ] } }","title":"Controller config"},{"location":"kubernetes/khw-gce/terraform-compute/#variables","text":"variable \"num\" { description = \"The number of controller VMs\" } variable \"machine_type\" { description = \"The type of VM for controllers\" } variable \"zone\" { description = \"The zone to create the controllers in\" } variable \"subnet\" { description = \"The subnet to create the nic in\" }","title":"Variables"},{"location":"kubernetes/khw-gce/terraform-compute/#worker-config","text":"Extra config for the worker are the routes, to aid the pods going out of the node. data \"google_compute_image\" \"khw-ubuntu\" { family = \"ubuntu-1804-lts\" project = \"ubuntu-os-cloud\" } resource \"google_compute_instance\" \"khw-worker\" { count = \"${var.num}\" name = \"worker-${count.index}\" machine_type = \"${var.machine_type}\" zone = \"${var.zone}\" can_ip_forward = \"true\" tags = [ \"kubernetes-the-hard-way\", \"worker\" ] metadata { pod-cidr = \"10.200.${count.index}.0/24\" } boot_disk { initialize_params { image = \"${data.google_compute_image.khw-ubuntu.self_link}\" size = 200 // in GB } } network_interface { subnetwork = \"${var.subnet}\" address = \"10.240.0.2${count.index}\" access_config { // Ephemeral External IP } } service_account { scopes = [ \"compute-rw\" , \"storage-ro\" , \"service-management\" , \"service-control\" , \"logging-write\" , \"monitoring\" , ] } } resource \"google_compute_route\" \"khw-worker-route\" { count = \"${var.num}\" name = \"kubernetes-route-10-200-${count.index}-0-24\" network = \"${var.network}\" next_hop_ip = \"10.240.0.2${count.index}\" dest_range = \"10.200.${count.index}.0/24\" }","title":"Worker config"},{"location":"kubernetes/khw-gce/terraform-compute/#variables_1","text":"variable \"num\" { description = \"The number of controller VMs\" } variable \"machine_type\" { description = \"The type of VM for controllers\" } variable \"zone\" { description = \"The zone to create the controllers in\" } variable \"network\" { description = \"The network to use for routes\" } variable \"subnet\" { description = \"The subnet to create the nic in\" }","title":"Variables"},{"location":"kubernetes/khw-gce/terraform-compute/#health-check","text":"Because we will have three controllers, we have to make sure that GKE forwards Kubernetes API requests to each of them via our public IP address. We do this via a http health check, wich involves a forwarding rule and a target pool. Target pool being the group of controller VM's for which the forwarding rule is active. resource \"google_compute_target_pool\" \"khw-hc-target-pool\" { name = \"instance-pool\" # TODO: fixed set for now, maybe we can make this dynamic some day instances = [ \"${var.region_default_zone}/controller-0\" , \"${var.region_default_zone}/controller-1\" , \"${var.region_default_zone}/controller-2\" , ] health_checks = [ \"${google_compute_http_health_check.khw-health-check.name}\" , ] } resource \"google_compute_http_health_check\" \"khw-health-check\" { name = \"kubernetes\" request_path = \"/healthz\" description = \"The health check for Kubernetes API server\" host = \"${var.kubernetes-cluster-dns}\" } resource \"google_compute_forwarding_rule\" \"khw-hc-forward\" { name = \"kubernetes-forwarding-rule\" target = \"${google_compute_target_pool.khw-hc-target-pool.self_link}\" region = \"${var.region}\" port_range = \"6443\" ip_address = \"${google_compute_address.khw-lb-public-ip.self_link}\" }","title":"Health check"},{"location":"kubernetes/khw-gce/terraform-compute/#apply-terraform-state","text":"In the end, our configuration should consist out of several .tf files and look something like this. ls -lath drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 . drwxr-xr-x 20 joostvdg staff 640B Aug 22 14 :47 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 modules -rw-r--r-- 1 joostvdg staff 1 .5K Aug 26 12 :50 variables.tf -rw-r--r-- 1 joostvdg staff 1 .3K Aug 17 16 :03 firewall.tf -rw-r--r-- 1 joostvdg staff 4 .4K Aug 17 12 :06 worker-config.md -rw-r--r-- 1 joostvdg staff 1 .6K Aug 17 09 :35 healthcheck.tf -rw-r--r-- 1 joostvdg staff 517B Aug 16 17 :09 nodes.tf -rw-r--r-- 1 joostvdg staff 92B Aug 16 13 :52 publicip.tf -rw-r--r-- 1 joostvdg staff 365B Aug 7 22 :07 vpc.tf -rw-r--r-- 1 joostvdg staff 189B Aug 7 16 :51 base.tf drwxr-xr-x 5 joostvdg staff 160B Aug 7 21 :52 .terraform -rw-r--r-- 1 joostvdg staff 0B Aug 7 18 :28 terraform.tfstate We're now going to plan and then apply our Terraform configuration to create the resources in GCE. terraform plan terraform apply","title":"Apply Terraform state"},{"location":"kubernetes/khw-gce/worker/","text":"Worker Installation \u00b6 Install base components \u00b6 Download \u00b6 wget -q --show-progress --https-only --timestamping \\ https://github.com/kubernetes-incubator/cri-tools/releases/download/v1.0.0-beta.0/crictl-v1.0.0-beta.0-linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-the-hard-way/runsc \\ https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \\ https://github.com/containernetworking/plugins/releases/download/v0.7.1/cni-plugins-amd64-v0.7.1.tgz \\ https://github.com/containerd/containerd/releases/download/v1.1.2/containerd-1.1.2.linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-proxy \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubelet Prepare landing folders \u00b6 sudo mkdir -p \\ /etc/cni/net.d \\ /opt/cni/bin \\ /var/lib/kubelet \\ /var/lib/kube-proxy \\ /var/lib/kubernetes \\ /var/run/kubernetes \\ /etc/containerd/ Unpack to folders \u00b6 chmod +x kubectl kube-proxy kubelet runc.amd64 runsc sudo mv runc.amd64 runc sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/ sudo tar -xvf crictl-v1.0.0-beta.0-linux-amd64.tar.gz -C /usr/local/bin/ sudo tar -xvf cni-plugins-amd64-v0.7.1.tgz -C /opt/cni/bin/ sudo tar -xvf containerd-1.1.2.linux-amd64.tar.gz -C / List variables \u00b6 POD_CIDR = $( curl -s -H \"Metadata-Flavor: Google\" \\ http://metadata.google.internal/computeMetadata/v1/instance/attributes/pod-cidr ) echo HOSTNAME = $HOSTNAME echo POD_CIDR = $POD_CIDR Configure ContainerD \u00b6 Runtime configuration file \u00b6 cat << EOF | sudo tee /etc/containerd/config.toml [plugins] [plugins.cri.containerd] snapshotter = \"overlayfs\" [plugins.cri.containerd.default_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runc\" runtime_root = \"\" [plugins.cri.containerd.untrusted_workload_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runsc\" runtime_root = \"/run/containerd/runsc\" EOF SystemD service configuration file \u00b6 cat <<EOF | sudo tee /etc/systemd/system/containerd.service [Unit] Description = containerd container runtime Documentation = https://containerd.io After = network.target [Service] ExecStartPre = /sbin/modprobe overlay ExecStart = /bin/containerd Restart = always RestartSec = 5 Delegate = yes KillMode = process OOMScoreAdjust = -999 LimitNOFILE = 1048576 LimitNPROC = infinity LimitCORE = infinity [Install] WantedBy = multi-user.target EOF Configure CNI \u00b6 Warning We do not need to configure cni as we will setup Weave and it will do the necessary setup automagically. Configure Kubelet \u00b6 Move certificates to correct places \u00b6 sudo mv ${ HOSTNAME } -key.pem ${ HOSTNAME } .pem /var/lib/kubelet/ sudo mv ${ HOSTNAME } .kubeconfig /var/lib/kubelet/kubeconfig sudo mv ca.pem /var/lib/kubernetes/ Create k8s yaml configuration \u00b6 cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml kind : KubeletConfiguration apiVersion : kubelet.config.k8s.io/v1beta1 authentication : anonymous : enabled : false webhook : enabled : true x509 : clientCAFile : \"/var/lib/kubernetes/ca.pem\" authorization : mode : Webhook clusterDomain : \"cluster.local\" clusterDNS : - \"10.32.0.10\" podCIDR : \"${POD_CIDR}\" runtimeRequestTimeout : \"15m\" tlsCertFile : \"/var/lib/kubelet/${HOSTNAME}.pem\" tlsPrivateKeyFile : \"/var/lib/kubelet/${HOSTNAME}-key.pem\" EOF SystemD service configuration file \u00b6 Warning One thing I see missing from your kubelet configuration is --non-masquerade-cidr flag. Kubelet needs to be run with this option for traffic to outside clusterIP range. Refer here - kubenet 1 Kubelet should also be run with the `--non-masquerade-cidr=<clusterCidr>` argument to ensure traffic to IPs outside this range will use IP masquerade. Not sure, if this is the cause, but looks like this is a requirement and is missing from the Kubelet config. cat <<EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description = Kubernetes Kubelet Documentation = https://github.com/kubernetes/kubernetes After = containerd.service Requires = containerd.service [Service] ExecStart = /usr/local/bin/kubelet \\\\ --allow-privileged=true \\\\ --config=/var/lib/kubelet/kubelet-config.yaml \\\\ --container-runtime=remote \\\\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\ --image-pull-progress-deadline=2m \\\\ --kubeconfig=/var/lib/kubelet/kubeconfig \\\\ --network-plugin=cni \\\\ --register-node=true \\\\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF Kube-Proxy \u00b6 Move kubeconfig \u00b6 sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig Create k8s yaml config \u00b6 cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml kind : KubeProxyConfiguration apiVersion : kubeproxy.config.k8s.io/v1alpha1 clientConnection : kubeconfig : \"/var/lib/kube-proxy/kubeconfig\" mode : \"iptables\" clusterCIDR : \"10.200.0.0/16\" EOF Create SystemD service \u00b6 cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description = Kubernetes Kube Proxy Documentation = https://github.com/kubernetes/kubernetes [Service] ExecStart = /usr/local/bin/kube-proxy \\\\ --config=/var/lib/kube-proxy/kube-proxy-config.yaml \\\\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF Configure and start the SystemD services \u00b6 sudo systemctl daemon-reload sudo systemctl enable containerd kubelet kube-proxy sudo systemctl start containerd kubelet kube-proxy Validate \u00b6 Note Run this from a machine outside the cluster, with access to the admin kubeconfig. gcloud compute ssh controller-0 --command \"kubectl get nodes --kubeconfig admin.kubeconfig\" Note As we didn't configure networking yet, the nodes should be shown as NotReady status.","title":"Worker Config"},{"location":"kubernetes/khw-gce/worker/#worker-installation","text":"","title":"Worker Installation"},{"location":"kubernetes/khw-gce/worker/#install-base-components","text":"","title":"Install base components"},{"location":"kubernetes/khw-gce/worker/#download","text":"wget -q --show-progress --https-only --timestamping \\ https://github.com/kubernetes-incubator/cri-tools/releases/download/v1.0.0-beta.0/crictl-v1.0.0-beta.0-linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-the-hard-way/runsc \\ https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \\ https://github.com/containernetworking/plugins/releases/download/v0.7.1/cni-plugins-amd64-v0.7.1.tgz \\ https://github.com/containerd/containerd/releases/download/v1.1.2/containerd-1.1.2.linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-proxy \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubelet","title":"Download"},{"location":"kubernetes/khw-gce/worker/#prepare-landing-folders","text":"sudo mkdir -p \\ /etc/cni/net.d \\ /opt/cni/bin \\ /var/lib/kubelet \\ /var/lib/kube-proxy \\ /var/lib/kubernetes \\ /var/run/kubernetes \\ /etc/containerd/","title":"Prepare landing folders"},{"location":"kubernetes/khw-gce/worker/#unpack-to-folders","text":"chmod +x kubectl kube-proxy kubelet runc.amd64 runsc sudo mv runc.amd64 runc sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/ sudo tar -xvf crictl-v1.0.0-beta.0-linux-amd64.tar.gz -C /usr/local/bin/ sudo tar -xvf cni-plugins-amd64-v0.7.1.tgz -C /opt/cni/bin/ sudo tar -xvf containerd-1.1.2.linux-amd64.tar.gz -C /","title":"Unpack to folders"},{"location":"kubernetes/khw-gce/worker/#list-variables","text":"POD_CIDR = $( curl -s -H \"Metadata-Flavor: Google\" \\ http://metadata.google.internal/computeMetadata/v1/instance/attributes/pod-cidr ) echo HOSTNAME = $HOSTNAME echo POD_CIDR = $POD_CIDR","title":"List variables"},{"location":"kubernetes/khw-gce/worker/#configure-containerd","text":"","title":"Configure ContainerD"},{"location":"kubernetes/khw-gce/worker/#runtime-configuration-file","text":"cat << EOF | sudo tee /etc/containerd/config.toml [plugins] [plugins.cri.containerd] snapshotter = \"overlayfs\" [plugins.cri.containerd.default_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runc\" runtime_root = \"\" [plugins.cri.containerd.untrusted_workload_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runsc\" runtime_root = \"/run/containerd/runsc\" EOF","title":"Runtime configuration file"},{"location":"kubernetes/khw-gce/worker/#systemd-service-configuration-file","text":"cat <<EOF | sudo tee /etc/systemd/system/containerd.service [Unit] Description = containerd container runtime Documentation = https://containerd.io After = network.target [Service] ExecStartPre = /sbin/modprobe overlay ExecStart = /bin/containerd Restart = always RestartSec = 5 Delegate = yes KillMode = process OOMScoreAdjust = -999 LimitNOFILE = 1048576 LimitNPROC = infinity LimitCORE = infinity [Install] WantedBy = multi-user.target EOF","title":"SystemD service configuration file"},{"location":"kubernetes/khw-gce/worker/#configure-cni","text":"Warning We do not need to configure cni as we will setup Weave and it will do the necessary setup automagically.","title":"Configure CNI"},{"location":"kubernetes/khw-gce/worker/#configure-kubelet","text":"","title":"Configure Kubelet"},{"location":"kubernetes/khw-gce/worker/#move-certificates-to-correct-places","text":"sudo mv ${ HOSTNAME } -key.pem ${ HOSTNAME } .pem /var/lib/kubelet/ sudo mv ${ HOSTNAME } .kubeconfig /var/lib/kubelet/kubeconfig sudo mv ca.pem /var/lib/kubernetes/","title":"Move certificates to correct places"},{"location":"kubernetes/khw-gce/worker/#create-k8s-yaml-configuration","text":"cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml kind : KubeletConfiguration apiVersion : kubelet.config.k8s.io/v1beta1 authentication : anonymous : enabled : false webhook : enabled : true x509 : clientCAFile : \"/var/lib/kubernetes/ca.pem\" authorization : mode : Webhook clusterDomain : \"cluster.local\" clusterDNS : - \"10.32.0.10\" podCIDR : \"${POD_CIDR}\" runtimeRequestTimeout : \"15m\" tlsCertFile : \"/var/lib/kubelet/${HOSTNAME}.pem\" tlsPrivateKeyFile : \"/var/lib/kubelet/${HOSTNAME}-key.pem\" EOF","title":"Create k8s yaml configuration"},{"location":"kubernetes/khw-gce/worker/#systemd-service-configuration-file_1","text":"Warning One thing I see missing from your kubelet configuration is --non-masquerade-cidr flag. Kubelet needs to be run with this option for traffic to outside clusterIP range. Refer here - kubenet 1 Kubelet should also be run with the `--non-masquerade-cidr=<clusterCidr>` argument to ensure traffic to IPs outside this range will use IP masquerade. Not sure, if this is the cause, but looks like this is a requirement and is missing from the Kubelet config. cat <<EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description = Kubernetes Kubelet Documentation = https://github.com/kubernetes/kubernetes After = containerd.service Requires = containerd.service [Service] ExecStart = /usr/local/bin/kubelet \\\\ --allow-privileged=true \\\\ --config=/var/lib/kubelet/kubelet-config.yaml \\\\ --container-runtime=remote \\\\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\ --image-pull-progress-deadline=2m \\\\ --kubeconfig=/var/lib/kubelet/kubeconfig \\\\ --network-plugin=cni \\\\ --register-node=true \\\\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF","title":"SystemD service configuration file"},{"location":"kubernetes/khw-gce/worker/#kube-proxy","text":"","title":"Kube-Proxy"},{"location":"kubernetes/khw-gce/worker/#move-kubeconfig","text":"sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig","title":"Move kubeconfig"},{"location":"kubernetes/khw-gce/worker/#create-k8s-yaml-config","text":"cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml kind : KubeProxyConfiguration apiVersion : kubeproxy.config.k8s.io/v1alpha1 clientConnection : kubeconfig : \"/var/lib/kube-proxy/kubeconfig\" mode : \"iptables\" clusterCIDR : \"10.200.0.0/16\" EOF","title":"Create k8s yaml config"},{"location":"kubernetes/khw-gce/worker/#create-systemd-service","text":"cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description = Kubernetes Kube Proxy Documentation = https://github.com/kubernetes/kubernetes [Service] ExecStart = /usr/local/bin/kube-proxy \\\\ --config=/var/lib/kube-proxy/kube-proxy-config.yaml \\\\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF","title":"Create SystemD service"},{"location":"kubernetes/khw-gce/worker/#configure-and-start-the-systemd-services","text":"sudo systemctl daemon-reload sudo systemctl enable containerd kubelet kube-proxy sudo systemctl start containerd kubelet kube-proxy","title":"Configure and start the SystemD services"},{"location":"kubernetes/khw-gce/worker/#validate","text":"Note Run this from a machine outside the cluster, with access to the admin kubeconfig. gcloud compute ssh controller-0 --command \"kubectl get nodes --kubeconfig admin.kubeconfig\" Note As we didn't configure networking yet, the nodes should be shown as NotReady status.","title":"Validate"},{"location":"linux/","text":"Linux \u00b6","title":"Linux"},{"location":"linux/#linux","text":"","title":"Linux"},{"location":"linux/iptables/","text":"iptables \u00b6","title":"iptables"},{"location":"linux/iptables/#iptables","text":"","title":"iptables"},{"location":"linux/networking/","text":"Networking \u00b6","title":"Networking"},{"location":"linux/networking/#networking","text":"","title":"Networking"},{"location":"linux/systemd/","text":"SystemD \u00b6 Increasingly, Linux distributions are adopting or planning to adopt the systemd init system. This powerful suite of software can manage many aspects of your server, from services to mounted devices and system states. 1 Concepts \u00b6 Unit \u00b6 In systemd , a unit refers to any resource that the system knows how to operate on and manage. This is the primary object that the systemd tools know how to deal with. These resources are defined using configuration files called unit files . 1 Path \u00b6 A path unit defines a filesystem path that systmed can monitor for changes. Another unit must exist that will be be activated when certain activity is detected at the path location. Path activity is determined through inotify events . My idea, you can use this for those services that should trigger on file uploads or backup dumps. Although I wonder if the Unit's main service knows which path was triggered? If it does, than it's easy, else you still need a \"file walker\". Example \u00b6 [Unit] Description = Timezone Helper Service After = network.target StartLimitIntervalSec = 0 [Service] Type = simple Restart = always RestartSec = 3 User = joostvdg ExecStart = /usr/bin/timezone_helper_service [Install] WantedBy = multi-user.target Resources \u00b6 https://www.linuxjournal.com/content/linux-filesystem-events-inotify References \u00b6 Introduction to systemd from Digital Ocean \u21a9 \u21a9","title":"SystemD"},{"location":"linux/systemd/#systemd","text":"Increasingly, Linux distributions are adopting or planning to adopt the systemd init system. This powerful suite of software can manage many aspects of your server, from services to mounted devices and system states. 1","title":"SystemD"},{"location":"linux/systemd/#concepts","text":"","title":"Concepts"},{"location":"linux/systemd/#unit","text":"In systemd , a unit refers to any resource that the system knows how to operate on and manage. This is the primary object that the systemd tools know how to deal with. These resources are defined using configuration files called unit files . 1","title":"Unit"},{"location":"linux/systemd/#path","text":"A path unit defines a filesystem path that systmed can monitor for changes. Another unit must exist that will be be activated when certain activity is detected at the path location. Path activity is determined through inotify events . My idea, you can use this for those services that should trigger on file uploads or backup dumps. Although I wonder if the Unit's main service knows which path was triggered? If it does, than it's easy, else you still need a \"file walker\".","title":"Path"},{"location":"linux/systemd/#example","text":"[Unit] Description = Timezone Helper Service After = network.target StartLimitIntervalSec = 0 [Service] Type = simple Restart = always RestartSec = 3 User = joostvdg ExecStart = /usr/bin/timezone_helper_service [Install] WantedBy = multi-user.target","title":"Example"},{"location":"linux/systemd/#resources","text":"https://www.linuxjournal.com/content/linux-filesystem-events-inotify","title":"Resources"},{"location":"linux/systemd/#references","text":"Introduction to systemd from Digital Ocean \u21a9 \u21a9","title":"References"},{"location":"openshift/rhos311-gcp-medium/","text":"RedHat OpenShift 3.11 on GCP - Medium \u00b6 Initially, I wrote a guide to get RedHat OpenShift 3.11 running on Google Cloud (GCP) with minimal effort . It is based on the GCP guide from RedHat 1 , but limiting the configuration options to the bare minimum. So, why another guide? Well, I wanted to go a few steps further. Creating a separated network to make the cluster more secure. I also want to ensure the Domain Name, IP, and load balancing are automated to improve the OpenShift installation. I will guide you in creating a secured network, with an automated DNS configuration, and automated TLS for our endpoints. Note This guide is written early March 2020, using jx version 2.0.1212 and OpenShift version v3.11.170 . Pre-requisites \u00b6 What do we need to get started? active GCP account with billing enabled GCP project gcloud tool installed terraform 0.12 + installed active RedHat account which hasn't used its 60 day trial license of OpenShift yet domain name registration which can forward subdomains to name servers (class CNAME ) Important This doc contains a prefix for the VM's created in GCP. This prefix is set to <prefix> . Next to that, the user used, is set to my_user . You probably want to change that, so please take care with copy-pasting! Process \u00b6 create Terraform configuration for GCP VM's create the VM's in GCP with RedHat Enterprise Linux v7 configure dns install OpenShift pre-requisites on each VM create OpenShift Ansible configuration install OpenShift via Ansible create initial users GCP Terraform \u00b6 What Do We Need \u00b6 Having gone through the process of installing RHOS 3.11 once, I ran into an issue. The documentation makes it seems you only need master nodes, compute nodes and VM's for etcd (can be the same as Master ). However, you also need at least one infra node. You can opt for a HA cluster, with three master nodes, or a single master node for a test cluster. I'm going with the latter. The master node will house the Kubernetes Control Plane, the infra node will house the OpenShift infra. As we won't have cluster autoscaling - a bit fancy for a manual test cluster - we have to make sure the machines are beefy to take the entire workload. Another thing we need for OpenShift, is having DNS that works between the nodes. For example, you should be able to say node1 and end up at the correct machine. Due to GCP networking, this internal DNS works out-of-the-box for any machine with our network/project. Important Our machines need to have unique names! So I ended up with the following: 1x master node -> n1-standard-8 : 8 cores, 30gb mem 1x infra node -> n1-standard-8 : 8 cores, 30gb mem 4x compute node -> n1-standard-4 : 4 cores, 15gb mem (each) VM Image \u00b6 Of course, if you want to run RedHat OpenShift Enterprise (RHOS), your VM's need to run a RedHat Enterprise Linux distribution(RHEL). In order to figure out which vm images are currently available, you can issue the following command via gcloud . gcloud compute images list --project rhel-cloud Which should give a response like this: NAME PROJECT FAMILY DEPRECATED STATUS rhel-6-v20200205 rhel-cloud rhel-6 READY rhel-7-v20200205 rhel-cloud rhel-7 READY rhel-8-v20200205 rhel-cloud rhel-8 READY For the VM image in our Terraform configuration, we will use the NAME of the image. For RHOS 3.11, RedHat strongly recommends using RHEL 7, so we use rhel-7-v20200205 . Terraform Configuration \u00b6 Google Cloud networking can get a bit complicated. In order to make the configuration easier to digest, I've created the Terraform configuration in the form of Terraform Modules 2 . There are four modules: Instance : VM image definition IP : static IP's and CloudDNS configuration LB : load balancers, port forwards and health checks VPC : vpc , subnet, and firewalls There are two main benefits to using the modules; the modules means we can reuse the terraform code - especially nice for the instances it is easier to port the outcome of one resource - for example an IP address - as input to another For this case, each module - except IP - will have three files: main.tf : contains the resource definitions of this module variables.tf : contains the input for the module outputs.tf : export values from created resources, to be used by other modules/resources In the main folder we have: main.tf : creating the resources via the modules variables.tf : input variables storage.tf : Google Bucket 3 as backing storage for the OpenShift (Docker) Registry 4 Model \u00b6 Terraform Module IP \u00b6 main.tf resource \"google_compute_global_address\" \"master\" { name = \"master\" } resource \"google_compute_address\" \"nodes\" { name = \"nodes\" } resource \"google_dns_record_set\" \"ocp\" { name = \"api.${google_dns_managed_zone.my_ocp_domain.dns_name}\" type = \"A\" ttl = 300 managed_zone = google_dns_managed_zone . my_ocp_domain . name rrdatas = [ google_compute_global_address . master . address ] } resource \"google_dns_record_set\" \"apps\" { name = \"*.apps.${google_dns_managed_zone.my_ocp_domain.dns_name}\" type = \"A\" ttl = 300 managed_zone = google_dns_managed_zone . my_ocp_domain . name rrdatas = [ google_compute_address . nodes . address ] } resource \"google_dns_managed_zone\" \"my_ocp_domain\" { name = \"my-ocp-domain\" dns_name = \"my.ocp.domain.\" } outputs.tf output \"master_ip\" { value = google_compute_global_address . master . address } output \"nodes_ip\" { value = google_compute_address . nodes . address } Terraform Module VPC \u00b6 main.tf resource \"google_compute_network\" \"vpc_network\" { name = var . vpc_name auto_create_subnetworks = \"false\" routing_mode = \"GLOBAL\" } resource \"google_compute_subnetwork\" \"vpc_subnet_int\" { name = var . network_name_int ip_cidr_range = var . network_name_int_range network = google_compute_network . vpc_network . self_link region = var . region private_ip_google_access = true } resource \"google_compute_firewall\" \"fw_access\" { name = \"my-rhos311-fw-ext\" network = google_compute_network . vpc_network . name allow { protocol = \"icmp\" } allow { protocol = \"tcp\" ports = [ \"22\", \"80\", \"443\" ] } source_ranges = [ \"0.0.0.0/0\" ] } resource \"google_compute_firewall\" \"fw_gcp_health_checks\" { name = \"my-rhos311-fw-gcp-health-checks\" network = google_compute_network . vpc_network . name allow { protocol = \"tcp\" ports = [ \"0-65535\" ] } source_ranges = [ \"35.191.0.0/16\",\"130.211.0.0/22\" ] } resource \"google_compute_firewall\" \"allow-internal\" { name = \"my-rhos311-fw-int\" network = google_compute_network . vpc_network . name allow { protocol = \"icmp\" } allow { protocol = \"tcp\" ports = [ \"0-65535\" ] } allow { protocol = \"udp\" ports = [ \"0-65535\" ] } source_ranges = [ var . network_name_int_range ] } variables.tf variable \"region\" {} variable \"vpc_name\" {} variable \"network_name_int\" {} variable \"network_name_int_range\" { default = \"10.26.1.0/24\" } outputs.tf output \"vpc_link\" { value = \"${google_compute_network.vpc_network.self_link}\" } output \"vpc_subnet_int\" { value = \"${google_compute_subnetwork.vpc_subnet_int.self_link}\" } Terraform Module LB \u00b6 main.tf resource \"google_compute_health_check\" \"master\" { name = \"https\" healthy_threshold = 3 unhealthy_threshold = 3 timeout_sec = 10 check_interval_sec = 10 https_health_check { request_path = \"/healthz\" port = \"443\" } } resource \"google_compute_http_health_check\" \"nodes\" { name = \"nodes\" request_path = \"/healthz\" port = 1936 healthy_threshold = 3 unhealthy_threshold = 3 timeout_sec = 10 check_interval_sec = 10 } resource \"google_compute_backend_service\" \"master\" { name = \"backend-service-master\" health_checks = [ \"${google_compute_health_check.master.self_link}\" ] # load_balancing_scheme = \"EXTERNAL\" protocol = \"TCP\" session_affinity = \"CLIENT_IP\" port_name = \"ocp-api\" backend { group = var . instance_group_master } } resource \"google_compute_target_tcp_proxy\" \"master_tcp_proxy\" { name = \"master-tcp-proxy\" backend_service = google_compute_backend_service . master . self_link } resource \"google_compute_global_forwarding_rule\" \"master_forwarding\" { name = \"master-forwarding\" ip_address = var . master_ip target = google_compute_target_tcp_proxy . master_tcp_proxy . self_link port_range = 443 } resource \"google_compute_target_pool\" \"nodes\" { name = \"nodes\" instances = var . instance_group_nodes health_checks = [ google_compute_http_health_check . nodes . name ] } resource \"google_compute_forwarding_rule\" \"network-load-balancer-http\" { name = \"network-load-balancer-http\" ip_address = var . nodes_ip target = google_compute_target_pool . nodes . self_link port_range = \"80\" ip_protocol = \"TCP\" } resource \"google_compute_forwarding_rule\" \"network-load-balancer-https\" { name = \"network-load-balancer-https\" ip_address = var . nodes_ip target = google_compute_target_pool . nodes . self_link port_range = \"443\" ip_protocol = \"TCP\" } variables.tf variable \"region\" {} variable \"instance_group_master\" {} variable \"instance_group_nodes\" {} variable \"master_ip\" {} variable \"nodes_ip\" {} outputs.tf output \"health_check_master\" { value = \"${google_compute_health_check.master.self_link}\" } output \"health_check_nodes\" { value = \"${google_compute_http_health_check.nodes.self_link}\" } Terraform Module Instance \u00b6 main.tf resource \"google_compute_instance\" \"instance\" { name = \"${var.prefix}-${var.instance_name}\" machine_type = var . machine_type zone = var . zone allow_stopping_for_update = true boot_disk { initialize_params { image = var . vm_image size = var . disk_size type = \"pd-ssd\" } } network_interface { network = var . network subnetwork = var . vpc_subnet_int access_config { # gives the node an external IP address # not sure if this is still required? } } metadata = { ssh-keys = var . ssh_key } service_account { scopes = [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" , \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" , \"https://www.googleapis.com/auth/cloud-platform\" ] } } variables.tf variable \"machine_type\" { default = \"n1-standard-4\" } variable \"vm_image\" { default = \"rhel-7-v20200205\" } variable \"disk_size\" { default = 100 } variable \"ssh_key\" {} variable \"prefix\" {} variable \"instance_name\" { } variable \"zone\" {} variable \"network\" {} variable \"vpc_subnet_ext\" {} variable \"vpc_subnet_int\" {} outputs.tf output \"link\" { value = \"${google_compute_instance.instance.self_link}\" } Terraform Main \u00b6 main.tf terraform { required_version = \"~> 0.12\" } # https://www.terraform.io/docs/providers/google/index.html provider \"google\" { version = \"~> 2.18.1\" project = var . project region = var . region zone = \"europe-west4-b\" } module \"vpc\" { source = \"./modules/vpc\" region = var . region vpc_name = \"vpc-my-rhos311\" network_name_int = \"network-my-rhos311-int\" network_name_ext = \"network-my-rhos311-ext\" } module \"master1\" { source = \"./modules/instance\" machine_type = \"n1-standard-8\" instance_name = \"master1\" prefix = var . instance_prefix network = module . vpc . vpc_link vpc_subnet_int = module . vpc . vpc_subnet_int vpc_subnet_ext = module . vpc . vpc_subnet_ext zone = var . zone ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\" } module \"infra1\" { source = \"./modules/instance\" machine_type = \"n1-standard-8\" instance_name = \"infra1\" prefix = var . instance_prefix network = module . vpc . vpc_link vpc_subnet_int = module . vpc . vpc_subnet_int vpc_subnet_ext = module . vpc . vpc_subnet_ext zone = var . zone ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\" } module \"node1\" { source = \"./modules/instance\" instance_name = \"node1\" prefix = var . instance_prefix network = module . vpc . vpc_link vpc_subnet_int = module . vpc . vpc_subnet_int vpc_subnet_ext = module . vpc . vpc_subnet_ext zone = var . zone ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\" } module \"node2\" { source = \"./modules/instance\" instance_name = \"node2\" prefix = var . instance_prefix network = module . vpc . vpc_link vpc_subnet_int = module . vpc . vpc_subnet_int vpc_subnet_ext = module . vpc . vpc_subnet_ext zone = var . zone ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\" } module \"node3\" { source = \"./modules/instance\" instance_name = \"node3\" prefix = var . instance_prefix network = module . vpc . vpc_link vpc_subnet_int = module . vpc . vpc_subnet_int vpc_subnet_ext = module . vpc . vpc_subnet_ext zone = var . zone ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\" } module \"node4\" { source = \"./modules/instance\" instance_name = \"node4\" prefix = var . instance_prefix network = module . vpc . vpc_link vpc_subnet_int = module . vpc . vpc_subnet_int vpc_subnet_ext = module . vpc . vpc_subnet_ext zone = var . zone ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\" } resource \"google_compute_instance_group\" \"master\" { name = \"my-rhos311-master\" zone = var . master_zone instances = [ module . master1 . link , ] named_port { name = \"ocp-api\" port = \"443\" } } resource \"google_compute_instance_group\" \"nodes\" { name = \"my-rhos311-nodes\" zone = var . master_zone instances = [ module . infra1 . link , module . node1 . link , module . node2 . link , module . node3 . link , module . node4 . link ] named_port { name = \"ocp-api\" port = \"1936\" } } module \"ips\" { source = \"./modules/ip\" } module \"lb\" { source = \"./modules/lb\" region = var . region instance_group_master = \"${google_compute_instance_group.master.self_link}\" instance_group_nodes = [ \"${var.zone}/${var.instance_prefix}-infra1\" , \"${var.zone}/${var.instance_prefix}-node1\" , \"${var.zone}/${var.instance_prefix}-node2\" , \"${var.zone}/${var.instance_prefix}-node3\" , \"${var.zone}/${var.instance_prefix}-node4\" ] master_ip = module . ips . master_ip nodes_ip = module . ips . nodes_ip } variables.tf variable \"project\" { } variable \"region\" { default = \"europe-west4\" } variable \"name\" { description = \"The name of the cluster (required)\" default = \"jx-openshift-311\" } variable \"compute_machine_type\" { default = \"n1-standard-4\" } variable \"master_machine_type\" { default = \"n1-standard-8\" } variable \"instance_prefix\" { default = \"my-rhos311\" } variable \"vm_image\" { default = \"rhel-7-v20200205\" } variable \"master_zone\" { description = \"Zone in which the Master Node will be created\" default = \"europe-west4-a\" } variable \"zone\" { default = \"europe-west4-a\" } storage.tf resource \"google_storage_bucket\" \"ocp-registry\" { name = \"my-rhos311-ocp-registry\" location = \"EU\" } Create VMs with Terraform \u00b6 terraform init terraform validate terraform plan -out plan.out terraform apply \"plan.out\" We should end up with six nodes: <prefix>-master1 <prefix>-infra1 <prefix>-node1 <prefix>-node2 <prefix>-node3 <prefix>-node4 Verify VMs \u00b6 Before we can install the OpenShift pre-requisites, we verify if the VMs are ready to use. To verify the VMs, we will do the following: confirm we can ssh into each VM confirm we can use sudo in each VM confirm the infra node can call each VM by name ( <prefix>-infra1 , <prefix>-master1 , <prefix>-node1 , <prefix>-node2 , <prefix>-node3 , <prefix>-node4 ) confirm the infra node can ssh into all VMs (including itself!) SSH into VMs \u00b6 There are several ways to ssh into the VMs. You can do so via ssh installed on your machine, you can do so via the GCP console. I will use another option, using the gcloud CLI, using the ssh key I've configured in Terrafom ( ssh-keys = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\" ). Why am I using this form? Well, it makes it easier to reason about which machine I ssh into, as I can use the VM name . # your google project id PROJECT_ID = # the google zone the vm is in, for example: europe-west-4a VM_ZONE = gcloud beta compute --project $PROJECT_ID ssh --zone $VM_ZONE \"<prefix>-infra1\" Confirm you can ssh into each VM by changing the zone/name accordingly. Confirm Sudo \u00b6 Our ssh user isn't root - as it should be - so we need to use sudo for some tasks. Confirm sudo works; sudo cat /etc/locale.conf Confirm Local DNS \u00b6 The OpenShift installation process and later OpenShift itself, relies on local dns . This means, it assumes if there's a node called <prefix>-master1 , it can do ssh <prefix>-master1 and it works. In GCP, DNS works within a Project by default. So assuming all the VMs are within the same project this works out-of-the-box. But, to avoid any surprises later, confirm it. [ my_user@master ~ ] $ ping <prefix>-master1 PING master.c.MY_PROJECT_ID.internal ( 10 .164.0.49 ) 56 ( 84 ) bytes of data. 64 bytes from <prefix>-master1.c.MY_PROJECT_ID.internal ( 10 .164.0.49 ) : icmp_seq = 1 ttl = 64 time = 0 .041 ms 64 bytes from <prefix>-master1.c.MY_PROJECT_ID.internal ( 10 .164.0.49 ) : icmp_seq = 2 ttl = 64 time = 0 .094 ms Note As you might expect, MY_PROJECT_ID will be the Google project Id where your VMs are. I've hidden that as a safety precaution, confirm it looks correct! Infra Node can SSH into others \u00b6 For the OpenShift installation, our installation VM has to be able to ssh into every other VM 3 . This doesn't work out of the box. Warning I used my own keys here directly, because this is a temporary project only used by me. If your usecase is different, and you're not sure how to proceed, consult a security professional! We have to create the ssh public key on every node for our ssh user (in my case, my_user ) and the private also for our installation host (for example, <prefix>-infra1 ). This might not be a security best practice, but I did this by copying over my ~/.ssh/id_rsa and ~/.ssh/id_rsa.pub to each node's user home ( /home/my_user/.ssh/ ). Important Once you've done this, ssh into the infra node, and confirm it can ssh to every other node. ssh my_user@<prefix>-node1 ssh my_user@<prefix>-node2 ssh my_user@<prefix>-node3 ssh my_user@<prefix>-node4 ssh my_user@<prefix>-master1 ssh my_user@<prefix>-infra1 -> YES, you have to ssh into yourself! This is important, because through this step, you can accept the prompt so the installation process can run unattended ! Make sure to set the correct permissions to the id_rsa file via sudo chmod 0400 ~/.ssh/id_rsa ! Fix Locale \u00b6 I kept running into a locale warning, about using one that didn't exist on the VM. If you want to get rid of this, you can change the /etc/locale.conf file. sudo vim /etc/locale.conf Make sure it looks like this. LANG = \"en_US.UTF-8\" LC_CTYPE = \"en_US.UTF-8\" LC_ALL = en_US.UTF-8 OpenShift Installation Pre-requisites \u00b6 Before we can install OpenShift, we have to bring our nodes into a certain state. We will do the following: register our VMs to RedHat register our VMs as part of our OpenShift Enterprise license configure yum for the installation process install and configure docker for the installation process login to the RedHat docker registry Register VMs \u00b6 Please note, these steps have to be done on every VM! If you use something like iterm2 5 , you can save yourself some time by having four parallel sessions for each VM. You do this by creating a split window ( control + command + D ), and once logged in, create a shared cursor via command + shift + i . We start by installing the subscription manager. sudo yum install subscription-manager -y We then register our instance with our RedHat account. sudo subscription-manager register --username = <user_name> --password = <password> sudo subscription-manager refresh Find the OpenShift subscription and you should get a single option. Use the id as the --pool in the next command. sudo subscription-manager list --available --matches '*OpenShift*' sudo subscription-manager attach --pool = ? Configure Yum Repos \u00b6 There's commands to disable each individual repository, but I found it easier to disable all, and then add those we need after. sudo subscription-manager repos --disable = \"*\" sudo yum repolist sudo yum-config-manager --disable \\* Install Default Packages \u00b6 As we've disable all of our yum repositories, we first add the once we need. sudo subscription-manager repos \\ --enable = \"rhel-7-server-rpms\" \\ --enable = \"rhel-7-server-extras-rpms\" \\ --enable = \"rhel-7-server-ose-3.11-rpms\" \\ --enable = \"rhel-7-server-ansible-2.8-rpms\" Once we have a set of usable yum repositories, we can then install all the packages we need. sudo yum install wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct openshift-ansible atomic python-docker-py docker device-mapper-libs device-mapper-event-libs -y Note There have been some bugs in the past, related to docker versions. If, for some reason, you have to downgrade to a known working version of docker, this is a way of doing that. sudo yum downgrade docker-rhel-push-plugin-1.13.1-75.git8633870.el7_5 docker-client-1.13.1-75.git8633870.el7_5 docker-common-1.13.1-75.git8633870.el7_5 docker-1.13.1-75.git8633870.el7_5 Once we have all the packages installed, make sure they're updated and then we reboot our machines. sudo yum update -y sudo reboot Install Docker \u00b6 I sneaked the docker packages into the previous installation command already, so we only have to enable/configure docker at this point. If you want to configure more details, such as where docker stores its volumes/data, please take a look at RedHat's installation guide 6 . sudo systemctl start docker.service sudo systemctl enable docker.service To confirm docker works: sudo systemctl status docker.service Make sure that on each node, your default user can use docker. sudo setfacl --modify user:my_user:rw /var/run/docker.sock Setup Registry Authentication \u00b6 The images for OpenShift come from RedHat's own docker registry. We have to login, before we can use those images 7 . So use your RedHat account credentials. docker login https://registry.redhat.io -u <USER> -p <PASS> Create Ansible Inventory File \u00b6 OpenShift comes with two ways of installing, via docker or via Ansible. The fun part, the docker container will use Ansible to install anyway. So no matter which way you will install OpenShift, you need to create a InventoryFile. RedHat has a couple of example files 8 , but these aren't complate - you need infra nodes as well! Important Configuration Items \u00b6 Bellow follow some variables I recommend configuring, for information, consult the RedHat documentation[^9]. OSEv3:children : the types of nodes to be configured OSEv3:vars : variables for the installation process ansible_become : set the True if Ansible can not run as root ansible_ssh_user : if Ansible cannot run as root, as which user should it ssh into the other nodes oreg_url : template for the docker images used by OpenShift, this should be registry.access.redhat.com/openshift3/ose-${component}:${version} , it will be used by components such as ETCD, Kubelet and so on oreg_auth_user : your RedHat account username oreg_auth_password : your RedHat account password openshift_cloudprovider_kind : the kind of cloud provider where RHOS is installed on, in the case of GCP its gce (don't ask me) openshift_gcp_project : is required to allow OpenShift the ability to create local disks in GCP for PersistentVolumes, should be your Google Project ID openshift_gcp_prefix : prefix used for all the GCP resources, makes it easier to correlate, and reduces the chance of naming conflicts openshift_gcp_network_name : the name of the vpc created with Terraform openshift_gcp_multizone : set this to False , unless you have more than one master, in more than one gcp zone openshift_master_api_port=443 : by default OpenShift uses 8443 , but this is not supported by GCP loadbalancers openshift_master_console_port=443 : by default OpenShift uses 8443 , but this is not supported by GCP loadbalancers os_firewall_use_firewalld : use firewalld instead of iptables, seems to work better and is recommended by the RHOS 3.11 install guide (as of 2018+ I believe openshift_master_cluster_public_hostname : Node definitions ( etcd , masters , nodes ): instructs Ansible which machine should be configured and with what Info If you want to use the internal OpenShift (Docker) Registry, you have to configure it here 9 . You can add these variables under OSEv3:vars . openshift_hosted_registry_replicas : how many replicas, docs recommend 1 openshift_hosted_registry_storage_kind : type of storage, if using the GCP Bucket, set this to object openshift_hosted_registry_storage_provider : if using the GCP Bucket, set this to gcs openshift_hosted_registry_storage_gcs_bucket : the full name of the GCP Bucket openshift_gcp_network_name : the network the GCP Bucket is part of, irrelevant if the Bucket is Global 10 Inventory File \u00b6 # Create an OSEv3 group that contains the masters, nodes, and etcd groups [ OSEv3 : children ] masters nodes etcd # Set variables common for all OSEv3 hosts [OSEv3:vars] # SSH user, this user should allow ssh based auth without requiring a password ansible_ssh_user=my_user # If ansible_ssh_user is not root, ansible_become must be set to true ansible_become=true openshift_deployment_type=openshift-enterprise oreg_url=registry.access.redhat.com/openshift3/ose-${component}:${version} oreg_auth_user=\"YOUR_RED_HAT_USERNAME\" oreg_auth_password=\"YOUR_RED_HAT_PASSWORD\" openshift_cloudprovider_kind=gce openshift_gcp_project=YOUR_GOOGLE_PROJECT_ID openshift_gcp_prefix=YOUR_PREFIX # If deploying single zone cluster set to \"False\" openshift_gcp_multizone=\"False\" openshift_hosted_registry_replicas=1 openshift_hosted_registry_storage_kind=object openshift_hosted_registry_storage_provider=gcs openshift_hosted_registry_storage_gcs_bucket=<prefix>-ocp-registry openshift_gcp_network_name=vpc-<prefix> #openshift_master_cluster_hostname=api.ocp.kearos.net openshift_master_cluster_public_hostname=api.ocp.example.com openshift_master_default_subdomain=apps.ocp.example.com openshift_master_api_port=443 openshift_master_console_port=443 os_firewall_use_firewalld=True # https://docs.openshift.com/container-platform/3.11/install_config/configuring_authentication.html#identity-providers_parameters openshift_master_identity_providers=[{'name' : 'htpasswd_auth' , 'login' : 'true' , 'challenge' : 'true' , 'kind' : 'HTPasswdPasswordIdentityProvider' }] openshift_master_htpasswd_users={'administrator' : 'password' } # host group for masters [ masters ] <prefix>-master1.c.ps-dev-201405.internal # host group for etcd [ etcd ] <prefix>-master1.c.ps-dev-201405.internal # host group for nodes, includes region info [ nodes ] <prefix>-master1.c.ps-dev-201405.internal openshift_node_group_name='node-config-master' <prefix>-node1.c.ps-dev-201405.internal openshift_node_group_name='node-config-compute' <prefix>-node2.c.ps-dev-201405.internal openshift_node_group_name='node-config-compute' <prefix>-node3.c.ps-dev-201405.internal openshift_node_group_name='node-config-compute' <prefix>-node4.c.ps-dev-201405.internal openshift_node_group_name='node-config-compute' <prefix>-infra1.c.ps-dev-201405.internal openshift_node_group_name='node-config-infra' Important Make sure to replace the YOUR_... placeholder values with your actual values. oreg_auth_user (YOUR_RED_HAT_USERNAME) oreg_auth_password (YOUR_RED_HAT_PASSWORD) YOUR_GOOGLE_PROJECT_ID Install RHOS 3.11 with Ansible \u00b6 There are two ways to install RHOS 3.11. Via Ansible directly 8 , or via Ansible in a container[^9]. As our nodes are configured according to what the Ansible installation requires, there's no need to rely on the container. Additionally, if you want to use the container way, you have to make sure the container can use the same DNS configuration as the nodes can themselves. I've not done this, so this would be on you! Final Preparations \u00b6 Ansible creates a fact file. It does so at a location a non-root user doesn't have access to. So it is best to create this file upfront - on every node - and chown it to the user that will do the ssh/Ansible install. sudo mkdir -p /etc/ansible/facts.d sudo chown -R my_user /etc/ansible/facts.d Install OpenShift \u00b6 We install OpenShift via two scripts, playbooks/prerequisites.yml and playbooks/deploy_cluster.yml . When we install openshift-ansible atomic via yum, we also get the Ansible playbooks for OpenShift. Either go into the directory of those files, or use the entire path; cd /usr/share/ansible/openshift-ansible Execute OpenShift Pre-requisites script: ansible-playbook -i /home/my_user/inventoryFile /usr/share/ansible/openshift-ansible/playbooks/prerequisites.yml If all is successful, it will end with all actions in green and finished successfully (or similar) . Once this is the case, execute OpenShift Installation: ansible-playbook -i /home/my_user/inventoryFile /usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml Now you should be able to run oc get nodes on the installation node. Create Users \u00b6 In the inventoryFile , I've configured the OpenShift user management as follows: openshift_master_identity_providers=[{'name' : 'htpasswd_auth' , 'login' : 'true' , 'challenge' : 'true' , 'kind' : 'HTPasswdPasswordIdentityProvider' }] openshift_master_htpasswd_users={'administrator' : 'password' } Caution You probably do NOT want to do this for a production environment. RedHat has a guide on this 10 , I suggest you consult it for an overview of the options. This configuration means that you have to create the users on the <prefix>-master1 node, via htpasswd . Create user via htpasswd \u00b6 sudo htpasswd /etc/origin/master/htpasswd <user> Mind you, this user does not have any rights yet, nor a token with which they can login to the Kubernetes API! Give him admin rights \u00b6 oc create clusterrolebinding registry-controller --clusterrole = cluster-admin --user = <user> Get Config \u00b6 First, login with oc login , this will prompt you for a username and then password. Fill in the values for the user you've created above with htpasswd . Only once you've logged in via oc login , do you get a token. Use the password for the console, but the token for kubeconfig/docker registry. To view the token, first confirm you're logged in with the right user: oc whoami Then retrieve the token as follows: oc whoami -t To retrieve the config file for Kubernetes's API, you can issue the following command. oc config view Login to OpenShift Registry \u00b6 To retrieve the Registry URL: kubectl get route --namespace default Which should give you something like this: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD docker-registry docker-registry-default.apps.ocp.example.com docker-registry <all> passthrough None registry-console registry-console-default.apps.ocp..example.com registry-console <all> passthrough None Unless you have configured a proper SSL certificate, docker won't let you login. See the next chapter for more information on that. If you do have a SSL certificate, this is how you can login: docker login -u <user> -p <token> registry-console-default.apps.ocp..example.com SSL Via Certmanager \u00b6 There are many ways to get a valid SSL certificate for your domain. Here I guide you through the following: install old certmanager - the latest requires Kubernetes 1.12+ Confgure DNS Verification to let Certmanager use CloudDNS 12 generate cert for *.apps.ocp.kearos.net add cert to the OpenShift Router and Docker Registry resources Install Compatible Certmanager \u00b6 Luckily, Certmanager has documentation on how to install itself on OpenShift 3.11 11 . Below are the steps, yes it is that simple. oc create namespace cert-manager oc apply --validate = false -f https://github.com/jetstack/cert-manager/releases/download/v0.14.0/cert-manager-legacy.yaml Confgure DNS Verification \u00b6 First we need to setup a Google cloud service account 13 , and retrieve the json key. Save this file as credentials.json , and then create a kubernetes secret . kubectl create secret generic external-dns-gcp-sa --from-file = credentials.json Certificate Issuer \u00b6 apiVersion : cert-manager.io/v1alpha2 kind : Issuer metadata : name : letsencrypt-prod spec : acme : email : <INSERT_YOUR_EMAIL> privateKeySecretRef : name : letsencrypt-prod server : https://acme-v02.api.letsencrypt.org/directory solvers : - dns01 : clouddns : project : <INSERT_YOUR_GOOGLE_PROJECT_ID> serviceAccountSecretRef : key : credentials.json name : external-dns-gcp-sa Wildcard Certificate \u00b6 apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : acme-crt spec : secretName : acme-crt-secret dnsNames : - apps.ocp.example.com - \"*.apps.ocp.example.com\" issuerRef : name : letsencrypt-prod # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind : Issuer group : cert-manager.io Caution Don't forget to change the dnsNames ! Configure Certificate In Router \u00b6 HA Proxy expect a crt file with the key and cert, while the Certmanager secret's tls.crt file only contains the certificate - how unexpected. So, what we have to do is the following: * get tls.crt raw data (the actual certificate) * get tls.key raw data * add the data from tls.key to a file tls.key * add the data from both tls.key and tls.crt to a file tls.crt * create a new secret * edit the Router to use our new secret * kill the router pod to force it to load our new certificate kubectl get secret acme-crt-secret -o yaml echo \"<base64 secret value> | base64 -D\" Or, if you have jq 14 installed, you can do something like this. kubectl get secret -n default acme-crt-secret -o json | jq -r '.data | map_values(@base64d)' Add them both the PEM certificate and the key to the file tls.crt , and the key to the file tls.key . Create a new secret kubectl create secret tls letsencrypt-copy --cert = tls.crt --key = tls.key And then edit the Route, via the following command: kubectl edit -n default replicationcontrollers router-1 Change the secret.secretName value of the volume named server-certificate from router-certs to letsencrypt-copy . It should then look like this: volumes : - name : metrics-server-certificate secret : defaultMode : 420 secretName : router-metrics-tls - name : server-certificate secret : defaultMode : 420 secretName : letsencrypt-copy Configure Certificate in Docker Registry \u00b6 Edit docker registry as well: kubectl edit replicationcontrollers docker-registry-1 Make sure to overide the naming convention of registry.crt and registry.key to the sensible tls.key and tls.crt . - name : REGISTRY_HTTP_TLS_KEY value : /etc/secrets/tls.key - name : OPENSHIFT_DEFAULT_REGISTRY value : docker-registry.default.svc:5000 - name : REGISTRY_CONFIGURATION_PATH value : /etc/registry/config.yml - name : REGISTRY_OPENSHIFT_SERVER_ADDR value : docker-registry.default.svc:5000 - name : REGISTRY_HTTP_TLS_CERTIFICATE value : /etc/secrets/tls.crt References \u00b6 https://access.redhat.com/documentation/en-us/reference_architectures/2018/html/deploying_and_managing_openshift_3.9_on_google_cloud_platform/ \u21a9 https://www.terraform.io/docs/modules/index.html \u21a9 https://cloud.google.com/storage/docs/key-terms#buckets \u21a9 \u21a9 https://docs.openshift.com/container-platform/3.11/install_config/registry/index.html \u21a9 https://iterm2.com/ \u21a9 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-install-host-preparation#configuring-docker-storage \u21a9 https://access.redhat.com/RegistryAuthentication \u21a9 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html-single/installing_clusters/index#install-config-example-inventories [ ^9]: https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-configuring-inventory-file#configuring-cluster-variables \u21a9 \u21a9 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-configuring-inventory-file#advanced-install-registry-storage \u21a9 https://docs.openshift.com/container-platform/3.11/install_config/configuring_authentication.html#identity-providers_parameters \u21a9 \u21a9 https://cert-manager.io/docs/installation/openshift/ \u21a9 https://cloud.google.com/dns/ \u21a9 https://access.redhat.com/documentation/en-us/reference_architectures/2018/html-single/deploying_and_managing_openshift_3.9_on_google_cloud_platform/index#google_cloud_platform_service_account \u21a9 https://stedolan.github.io/jq/ \u21a9","title":"OpenShift 3.11 GCP(medium)"},{"location":"openshift/rhos311-gcp-medium/#redhat-openshift-311-on-gcp-medium","text":"Initially, I wrote a guide to get RedHat OpenShift 3.11 running on Google Cloud (GCP) with minimal effort . It is based on the GCP guide from RedHat 1 , but limiting the configuration options to the bare minimum. So, why another guide? Well, I wanted to go a few steps further. Creating a separated network to make the cluster more secure. I also want to ensure the Domain Name, IP, and load balancing are automated to improve the OpenShift installation. I will guide you in creating a secured network, with an automated DNS configuration, and automated TLS for our endpoints. Note This guide is written early March 2020, using jx version 2.0.1212 and OpenShift version v3.11.170 .","title":"RedHat OpenShift 3.11 on GCP - Medium"},{"location":"openshift/rhos311-gcp-medium/#pre-requisites","text":"What do we need to get started? active GCP account with billing enabled GCP project gcloud tool installed terraform 0.12 + installed active RedHat account which hasn't used its 60 day trial license of OpenShift yet domain name registration which can forward subdomains to name servers (class CNAME ) Important This doc contains a prefix for the VM's created in GCP. This prefix is set to <prefix> . Next to that, the user used, is set to my_user . You probably want to change that, so please take care with copy-pasting!","title":"Pre-requisites"},{"location":"openshift/rhos311-gcp-medium/#process","text":"create Terraform configuration for GCP VM's create the VM's in GCP with RedHat Enterprise Linux v7 configure dns install OpenShift pre-requisites on each VM create OpenShift Ansible configuration install OpenShift via Ansible create initial users","title":"Process"},{"location":"openshift/rhos311-gcp-medium/#gcp-terraform","text":"","title":"GCP Terraform"},{"location":"openshift/rhos311-gcp-medium/#what-do-we-need","text":"Having gone through the process of installing RHOS 3.11 once, I ran into an issue. The documentation makes it seems you only need master nodes, compute nodes and VM's for etcd (can be the same as Master ). However, you also need at least one infra node. You can opt for a HA cluster, with three master nodes, or a single master node for a test cluster. I'm going with the latter. The master node will house the Kubernetes Control Plane, the infra node will house the OpenShift infra. As we won't have cluster autoscaling - a bit fancy for a manual test cluster - we have to make sure the machines are beefy to take the entire workload. Another thing we need for OpenShift, is having DNS that works between the nodes. For example, you should be able to say node1 and end up at the correct machine. Due to GCP networking, this internal DNS works out-of-the-box for any machine with our network/project. Important Our machines need to have unique names! So I ended up with the following: 1x master node -> n1-standard-8 : 8 cores, 30gb mem 1x infra node -> n1-standard-8 : 8 cores, 30gb mem 4x compute node -> n1-standard-4 : 4 cores, 15gb mem (each)","title":"What Do We Need"},{"location":"openshift/rhos311-gcp-medium/#vm-image","text":"Of course, if you want to run RedHat OpenShift Enterprise (RHOS), your VM's need to run a RedHat Enterprise Linux distribution(RHEL). In order to figure out which vm images are currently available, you can issue the following command via gcloud . gcloud compute images list --project rhel-cloud Which should give a response like this: NAME PROJECT FAMILY DEPRECATED STATUS rhel-6-v20200205 rhel-cloud rhel-6 READY rhel-7-v20200205 rhel-cloud rhel-7 READY rhel-8-v20200205 rhel-cloud rhel-8 READY For the VM image in our Terraform configuration, we will use the NAME of the image. For RHOS 3.11, RedHat strongly recommends using RHEL 7, so we use rhel-7-v20200205 .","title":"VM Image"},{"location":"openshift/rhos311-gcp-medium/#terraform-configuration","text":"Google Cloud networking can get a bit complicated. In order to make the configuration easier to digest, I've created the Terraform configuration in the form of Terraform Modules 2 . There are four modules: Instance : VM image definition IP : static IP's and CloudDNS configuration LB : load balancers, port forwards and health checks VPC : vpc , subnet, and firewalls There are two main benefits to using the modules; the modules means we can reuse the terraform code - especially nice for the instances it is easier to port the outcome of one resource - for example an IP address - as input to another For this case, each module - except IP - will have three files: main.tf : contains the resource definitions of this module variables.tf : contains the input for the module outputs.tf : export values from created resources, to be used by other modules/resources In the main folder we have: main.tf : creating the resources via the modules variables.tf : input variables storage.tf : Google Bucket 3 as backing storage for the OpenShift (Docker) Registry 4","title":"Terraform Configuration"},{"location":"openshift/rhos311-gcp-medium/#model","text":"","title":"Model"},{"location":"openshift/rhos311-gcp-medium/#terraform-module-ip","text":"main.tf resource \"google_compute_global_address\" \"master\" { name = \"master\" } resource \"google_compute_address\" \"nodes\" { name = \"nodes\" } resource \"google_dns_record_set\" \"ocp\" { name = \"api.${google_dns_managed_zone.my_ocp_domain.dns_name}\" type = \"A\" ttl = 300 managed_zone = google_dns_managed_zone . my_ocp_domain . name rrdatas = [ google_compute_global_address . master . address ] } resource \"google_dns_record_set\" \"apps\" { name = \"*.apps.${google_dns_managed_zone.my_ocp_domain.dns_name}\" type = \"A\" ttl = 300 managed_zone = google_dns_managed_zone . my_ocp_domain . name rrdatas = [ google_compute_address . nodes . address ] } resource \"google_dns_managed_zone\" \"my_ocp_domain\" { name = \"my-ocp-domain\" dns_name = \"my.ocp.domain.\" } outputs.tf output \"master_ip\" { value = google_compute_global_address . master . address } output \"nodes_ip\" { value = google_compute_address . nodes . address }","title":"Terraform Module IP"},{"location":"openshift/rhos311-gcp-medium/#terraform-module-vpc","text":"main.tf resource \"google_compute_network\" \"vpc_network\" { name = var . vpc_name auto_create_subnetworks = \"false\" routing_mode = \"GLOBAL\" } resource \"google_compute_subnetwork\" \"vpc_subnet_int\" { name = var . network_name_int ip_cidr_range = var . network_name_int_range network = google_compute_network . vpc_network . self_link region = var . region private_ip_google_access = true } resource \"google_compute_firewall\" \"fw_access\" { name = \"my-rhos311-fw-ext\" network = google_compute_network . vpc_network . name allow { protocol = \"icmp\" } allow { protocol = \"tcp\" ports = [ \"22\", \"80\", \"443\" ] } source_ranges = [ \"0.0.0.0/0\" ] } resource \"google_compute_firewall\" \"fw_gcp_health_checks\" { name = \"my-rhos311-fw-gcp-health-checks\" network = google_compute_network . vpc_network . name allow { protocol = \"tcp\" ports = [ \"0-65535\" ] } source_ranges = [ \"35.191.0.0/16\",\"130.211.0.0/22\" ] } resource \"google_compute_firewall\" \"allow-internal\" { name = \"my-rhos311-fw-int\" network = google_compute_network . vpc_network . name allow { protocol = \"icmp\" } allow { protocol = \"tcp\" ports = [ \"0-65535\" ] } allow { protocol = \"udp\" ports = [ \"0-65535\" ] } source_ranges = [ var . network_name_int_range ] } variables.tf variable \"region\" {} variable \"vpc_name\" {} variable \"network_name_int\" {} variable \"network_name_int_range\" { default = \"10.26.1.0/24\" } outputs.tf output \"vpc_link\" { value = \"${google_compute_network.vpc_network.self_link}\" } output \"vpc_subnet_int\" { value = \"${google_compute_subnetwork.vpc_subnet_int.self_link}\" }","title":"Terraform Module VPC"},{"location":"openshift/rhos311-gcp-medium/#terraform-module-lb","text":"main.tf resource \"google_compute_health_check\" \"master\" { name = \"https\" healthy_threshold = 3 unhealthy_threshold = 3 timeout_sec = 10 check_interval_sec = 10 https_health_check { request_path = \"/healthz\" port = \"443\" } } resource \"google_compute_http_health_check\" \"nodes\" { name = \"nodes\" request_path = \"/healthz\" port = 1936 healthy_threshold = 3 unhealthy_threshold = 3 timeout_sec = 10 check_interval_sec = 10 } resource \"google_compute_backend_service\" \"master\" { name = \"backend-service-master\" health_checks = [ \"${google_compute_health_check.master.self_link}\" ] # load_balancing_scheme = \"EXTERNAL\" protocol = \"TCP\" session_affinity = \"CLIENT_IP\" port_name = \"ocp-api\" backend { group = var . instance_group_master } } resource \"google_compute_target_tcp_proxy\" \"master_tcp_proxy\" { name = \"master-tcp-proxy\" backend_service = google_compute_backend_service . master . self_link } resource \"google_compute_global_forwarding_rule\" \"master_forwarding\" { name = \"master-forwarding\" ip_address = var . master_ip target = google_compute_target_tcp_proxy . master_tcp_proxy . self_link port_range = 443 } resource \"google_compute_target_pool\" \"nodes\" { name = \"nodes\" instances = var . instance_group_nodes health_checks = [ google_compute_http_health_check . nodes . name ] } resource \"google_compute_forwarding_rule\" \"network-load-balancer-http\" { name = \"network-load-balancer-http\" ip_address = var . nodes_ip target = google_compute_target_pool . nodes . self_link port_range = \"80\" ip_protocol = \"TCP\" } resource \"google_compute_forwarding_rule\" \"network-load-balancer-https\" { name = \"network-load-balancer-https\" ip_address = var . nodes_ip target = google_compute_target_pool . nodes . self_link port_range = \"443\" ip_protocol = \"TCP\" } variables.tf variable \"region\" {} variable \"instance_group_master\" {} variable \"instance_group_nodes\" {} variable \"master_ip\" {} variable \"nodes_ip\" {} outputs.tf output \"health_check_master\" { value = \"${google_compute_health_check.master.self_link}\" } output \"health_check_nodes\" { value = \"${google_compute_http_health_check.nodes.self_link}\" }","title":"Terraform Module LB"},{"location":"openshift/rhos311-gcp-medium/#terraform-module-instance","text":"main.tf resource \"google_compute_instance\" \"instance\" { name = \"${var.prefix}-${var.instance_name}\" machine_type = var . machine_type zone = var . zone allow_stopping_for_update = true boot_disk { initialize_params { image = var . vm_image size = var . disk_size type = \"pd-ssd\" } } network_interface { network = var . network subnetwork = var . vpc_subnet_int access_config { # gives the node an external IP address # not sure if this is still required? } } metadata = { ssh-keys = var . ssh_key } service_account { scopes = [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" , \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" , \"https://www.googleapis.com/auth/cloud-platform\" ] } } variables.tf variable \"machine_type\" { default = \"n1-standard-4\" } variable \"vm_image\" { default = \"rhel-7-v20200205\" } variable \"disk_size\" { default = 100 } variable \"ssh_key\" {} variable \"prefix\" {} variable \"instance_name\" { } variable \"zone\" {} variable \"network\" {} variable \"vpc_subnet_ext\" {} variable \"vpc_subnet_int\" {} outputs.tf output \"link\" { value = \"${google_compute_instance.instance.self_link}\" }","title":"Terraform Module Instance"},{"location":"openshift/rhos311-gcp-medium/#terraform-main","text":"main.tf terraform { required_version = \"~> 0.12\" } # https://www.terraform.io/docs/providers/google/index.html provider \"google\" { version = \"~> 2.18.1\" project = var . project region = var . region zone = \"europe-west4-b\" } module \"vpc\" { source = \"./modules/vpc\" region = var . region vpc_name = \"vpc-my-rhos311\" network_name_int = \"network-my-rhos311-int\" network_name_ext = \"network-my-rhos311-ext\" } module \"master1\" { source = \"./modules/instance\" machine_type = \"n1-standard-8\" instance_name = \"master1\" prefix = var . instance_prefix network = module . vpc . vpc_link vpc_subnet_int = module . vpc . vpc_subnet_int vpc_subnet_ext = module . vpc . vpc_subnet_ext zone = var . zone ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\" } module \"infra1\" { source = \"./modules/instance\" machine_type = \"n1-standard-8\" instance_name = \"infra1\" prefix = var . instance_prefix network = module . vpc . vpc_link vpc_subnet_int = module . vpc . vpc_subnet_int vpc_subnet_ext = module . vpc . vpc_subnet_ext zone = var . zone ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\" } module \"node1\" { source = \"./modules/instance\" instance_name = \"node1\" prefix = var . instance_prefix network = module . vpc . vpc_link vpc_subnet_int = module . vpc . vpc_subnet_int vpc_subnet_ext = module . vpc . vpc_subnet_ext zone = var . zone ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\" } module \"node2\" { source = \"./modules/instance\" instance_name = \"node2\" prefix = var . instance_prefix network = module . vpc . vpc_link vpc_subnet_int = module . vpc . vpc_subnet_int vpc_subnet_ext = module . vpc . vpc_subnet_ext zone = var . zone ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\" } module \"node3\" { source = \"./modules/instance\" instance_name = \"node3\" prefix = var . instance_prefix network = module . vpc . vpc_link vpc_subnet_int = module . vpc . vpc_subnet_int vpc_subnet_ext = module . vpc . vpc_subnet_ext zone = var . zone ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\" } module \"node4\" { source = \"./modules/instance\" instance_name = \"node4\" prefix = var . instance_prefix network = module . vpc . vpc_link vpc_subnet_int = module . vpc . vpc_subnet_int vpc_subnet_ext = module . vpc . vpc_subnet_ext zone = var . zone ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\" } resource \"google_compute_instance_group\" \"master\" { name = \"my-rhos311-master\" zone = var . master_zone instances = [ module . master1 . link , ] named_port { name = \"ocp-api\" port = \"443\" } } resource \"google_compute_instance_group\" \"nodes\" { name = \"my-rhos311-nodes\" zone = var . master_zone instances = [ module . infra1 . link , module . node1 . link , module . node2 . link , module . node3 . link , module . node4 . link ] named_port { name = \"ocp-api\" port = \"1936\" } } module \"ips\" { source = \"./modules/ip\" } module \"lb\" { source = \"./modules/lb\" region = var . region instance_group_master = \"${google_compute_instance_group.master.self_link}\" instance_group_nodes = [ \"${var.zone}/${var.instance_prefix}-infra1\" , \"${var.zone}/${var.instance_prefix}-node1\" , \"${var.zone}/${var.instance_prefix}-node2\" , \"${var.zone}/${var.instance_prefix}-node3\" , \"${var.zone}/${var.instance_prefix}-node4\" ] master_ip = module . ips . master_ip nodes_ip = module . ips . nodes_ip } variables.tf variable \"project\" { } variable \"region\" { default = \"europe-west4\" } variable \"name\" { description = \"The name of the cluster (required)\" default = \"jx-openshift-311\" } variable \"compute_machine_type\" { default = \"n1-standard-4\" } variable \"master_machine_type\" { default = \"n1-standard-8\" } variable \"instance_prefix\" { default = \"my-rhos311\" } variable \"vm_image\" { default = \"rhel-7-v20200205\" } variable \"master_zone\" { description = \"Zone in which the Master Node will be created\" default = \"europe-west4-a\" } variable \"zone\" { default = \"europe-west4-a\" } storage.tf resource \"google_storage_bucket\" \"ocp-registry\" { name = \"my-rhos311-ocp-registry\" location = \"EU\" }","title":"Terraform Main"},{"location":"openshift/rhos311-gcp-medium/#create-vms-with-terraform","text":"terraform init terraform validate terraform plan -out plan.out terraform apply \"plan.out\" We should end up with six nodes: <prefix>-master1 <prefix>-infra1 <prefix>-node1 <prefix>-node2 <prefix>-node3 <prefix>-node4","title":"Create VMs with Terraform"},{"location":"openshift/rhos311-gcp-medium/#verify-vms","text":"Before we can install the OpenShift pre-requisites, we verify if the VMs are ready to use. To verify the VMs, we will do the following: confirm we can ssh into each VM confirm we can use sudo in each VM confirm the infra node can call each VM by name ( <prefix>-infra1 , <prefix>-master1 , <prefix>-node1 , <prefix>-node2 , <prefix>-node3 , <prefix>-node4 ) confirm the infra node can ssh into all VMs (including itself!)","title":"Verify VMs"},{"location":"openshift/rhos311-gcp-medium/#ssh-into-vms","text":"There are several ways to ssh into the VMs. You can do so via ssh installed on your machine, you can do so via the GCP console. I will use another option, using the gcloud CLI, using the ssh key I've configured in Terrafom ( ssh-keys = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\" ). Why am I using this form? Well, it makes it easier to reason about which machine I ssh into, as I can use the VM name . # your google project id PROJECT_ID = # the google zone the vm is in, for example: europe-west-4a VM_ZONE = gcloud beta compute --project $PROJECT_ID ssh --zone $VM_ZONE \"<prefix>-infra1\" Confirm you can ssh into each VM by changing the zone/name accordingly.","title":"SSH into VMs"},{"location":"openshift/rhos311-gcp-medium/#confirm-sudo","text":"Our ssh user isn't root - as it should be - so we need to use sudo for some tasks. Confirm sudo works; sudo cat /etc/locale.conf","title":"Confirm Sudo"},{"location":"openshift/rhos311-gcp-medium/#confirm-local-dns","text":"The OpenShift installation process and later OpenShift itself, relies on local dns . This means, it assumes if there's a node called <prefix>-master1 , it can do ssh <prefix>-master1 and it works. In GCP, DNS works within a Project by default. So assuming all the VMs are within the same project this works out-of-the-box. But, to avoid any surprises later, confirm it. [ my_user@master ~ ] $ ping <prefix>-master1 PING master.c.MY_PROJECT_ID.internal ( 10 .164.0.49 ) 56 ( 84 ) bytes of data. 64 bytes from <prefix>-master1.c.MY_PROJECT_ID.internal ( 10 .164.0.49 ) : icmp_seq = 1 ttl = 64 time = 0 .041 ms 64 bytes from <prefix>-master1.c.MY_PROJECT_ID.internal ( 10 .164.0.49 ) : icmp_seq = 2 ttl = 64 time = 0 .094 ms Note As you might expect, MY_PROJECT_ID will be the Google project Id where your VMs are. I've hidden that as a safety precaution, confirm it looks correct!","title":"Confirm Local DNS"},{"location":"openshift/rhos311-gcp-medium/#infra-node-can-ssh-into-others","text":"For the OpenShift installation, our installation VM has to be able to ssh into every other VM 3 . This doesn't work out of the box. Warning I used my own keys here directly, because this is a temporary project only used by me. If your usecase is different, and you're not sure how to proceed, consult a security professional! We have to create the ssh public key on every node for our ssh user (in my case, my_user ) and the private also for our installation host (for example, <prefix>-infra1 ). This might not be a security best practice, but I did this by copying over my ~/.ssh/id_rsa and ~/.ssh/id_rsa.pub to each node's user home ( /home/my_user/.ssh/ ). Important Once you've done this, ssh into the infra node, and confirm it can ssh to every other node. ssh my_user@<prefix>-node1 ssh my_user@<prefix>-node2 ssh my_user@<prefix>-node3 ssh my_user@<prefix>-node4 ssh my_user@<prefix>-master1 ssh my_user@<prefix>-infra1 -> YES, you have to ssh into yourself! This is important, because through this step, you can accept the prompt so the installation process can run unattended ! Make sure to set the correct permissions to the id_rsa file via sudo chmod 0400 ~/.ssh/id_rsa !","title":"Infra Node can SSH into others"},{"location":"openshift/rhos311-gcp-medium/#fix-locale","text":"I kept running into a locale warning, about using one that didn't exist on the VM. If you want to get rid of this, you can change the /etc/locale.conf file. sudo vim /etc/locale.conf Make sure it looks like this. LANG = \"en_US.UTF-8\" LC_CTYPE = \"en_US.UTF-8\" LC_ALL = en_US.UTF-8","title":"Fix Locale"},{"location":"openshift/rhos311-gcp-medium/#openshift-installation-pre-requisites","text":"Before we can install OpenShift, we have to bring our nodes into a certain state. We will do the following: register our VMs to RedHat register our VMs as part of our OpenShift Enterprise license configure yum for the installation process install and configure docker for the installation process login to the RedHat docker registry","title":"OpenShift Installation Pre-requisites"},{"location":"openshift/rhos311-gcp-medium/#register-vms","text":"Please note, these steps have to be done on every VM! If you use something like iterm2 5 , you can save yourself some time by having four parallel sessions for each VM. You do this by creating a split window ( control + command + D ), and once logged in, create a shared cursor via command + shift + i . We start by installing the subscription manager. sudo yum install subscription-manager -y We then register our instance with our RedHat account. sudo subscription-manager register --username = <user_name> --password = <password> sudo subscription-manager refresh Find the OpenShift subscription and you should get a single option. Use the id as the --pool in the next command. sudo subscription-manager list --available --matches '*OpenShift*' sudo subscription-manager attach --pool = ?","title":"Register VMs"},{"location":"openshift/rhos311-gcp-medium/#configure-yum-repos","text":"There's commands to disable each individual repository, but I found it easier to disable all, and then add those we need after. sudo subscription-manager repos --disable = \"*\" sudo yum repolist sudo yum-config-manager --disable \\*","title":"Configure Yum Repos"},{"location":"openshift/rhos311-gcp-medium/#install-default-packages","text":"As we've disable all of our yum repositories, we first add the once we need. sudo subscription-manager repos \\ --enable = \"rhel-7-server-rpms\" \\ --enable = \"rhel-7-server-extras-rpms\" \\ --enable = \"rhel-7-server-ose-3.11-rpms\" \\ --enable = \"rhel-7-server-ansible-2.8-rpms\" Once we have a set of usable yum repositories, we can then install all the packages we need. sudo yum install wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct openshift-ansible atomic python-docker-py docker device-mapper-libs device-mapper-event-libs -y Note There have been some bugs in the past, related to docker versions. If, for some reason, you have to downgrade to a known working version of docker, this is a way of doing that. sudo yum downgrade docker-rhel-push-plugin-1.13.1-75.git8633870.el7_5 docker-client-1.13.1-75.git8633870.el7_5 docker-common-1.13.1-75.git8633870.el7_5 docker-1.13.1-75.git8633870.el7_5 Once we have all the packages installed, make sure they're updated and then we reboot our machines. sudo yum update -y sudo reboot","title":"Install Default Packages"},{"location":"openshift/rhos311-gcp-medium/#install-docker","text":"I sneaked the docker packages into the previous installation command already, so we only have to enable/configure docker at this point. If you want to configure more details, such as where docker stores its volumes/data, please take a look at RedHat's installation guide 6 . sudo systemctl start docker.service sudo systemctl enable docker.service To confirm docker works: sudo systemctl status docker.service Make sure that on each node, your default user can use docker. sudo setfacl --modify user:my_user:rw /var/run/docker.sock","title":"Install Docker"},{"location":"openshift/rhos311-gcp-medium/#setup-registry-authentication","text":"The images for OpenShift come from RedHat's own docker registry. We have to login, before we can use those images 7 . So use your RedHat account credentials. docker login https://registry.redhat.io -u <USER> -p <PASS>","title":"Setup Registry Authentication"},{"location":"openshift/rhos311-gcp-medium/#create-ansible-inventory-file","text":"OpenShift comes with two ways of installing, via docker or via Ansible. The fun part, the docker container will use Ansible to install anyway. So no matter which way you will install OpenShift, you need to create a InventoryFile. RedHat has a couple of example files 8 , but these aren't complate - you need infra nodes as well!","title":"Create Ansible Inventory File"},{"location":"openshift/rhos311-gcp-medium/#important-configuration-items","text":"Bellow follow some variables I recommend configuring, for information, consult the RedHat documentation[^9]. OSEv3:children : the types of nodes to be configured OSEv3:vars : variables for the installation process ansible_become : set the True if Ansible can not run as root ansible_ssh_user : if Ansible cannot run as root, as which user should it ssh into the other nodes oreg_url : template for the docker images used by OpenShift, this should be registry.access.redhat.com/openshift3/ose-${component}:${version} , it will be used by components such as ETCD, Kubelet and so on oreg_auth_user : your RedHat account username oreg_auth_password : your RedHat account password openshift_cloudprovider_kind : the kind of cloud provider where RHOS is installed on, in the case of GCP its gce (don't ask me) openshift_gcp_project : is required to allow OpenShift the ability to create local disks in GCP for PersistentVolumes, should be your Google Project ID openshift_gcp_prefix : prefix used for all the GCP resources, makes it easier to correlate, and reduces the chance of naming conflicts openshift_gcp_network_name : the name of the vpc created with Terraform openshift_gcp_multizone : set this to False , unless you have more than one master, in more than one gcp zone openshift_master_api_port=443 : by default OpenShift uses 8443 , but this is not supported by GCP loadbalancers openshift_master_console_port=443 : by default OpenShift uses 8443 , but this is not supported by GCP loadbalancers os_firewall_use_firewalld : use firewalld instead of iptables, seems to work better and is recommended by the RHOS 3.11 install guide (as of 2018+ I believe openshift_master_cluster_public_hostname : Node definitions ( etcd , masters , nodes ): instructs Ansible which machine should be configured and with what Info If you want to use the internal OpenShift (Docker) Registry, you have to configure it here 9 . You can add these variables under OSEv3:vars . openshift_hosted_registry_replicas : how many replicas, docs recommend 1 openshift_hosted_registry_storage_kind : type of storage, if using the GCP Bucket, set this to object openshift_hosted_registry_storage_provider : if using the GCP Bucket, set this to gcs openshift_hosted_registry_storage_gcs_bucket : the full name of the GCP Bucket openshift_gcp_network_name : the network the GCP Bucket is part of, irrelevant if the Bucket is Global 10","title":"Important Configuration Items"},{"location":"openshift/rhos311-gcp-medium/#inventory-file","text":"# Create an OSEv3 group that contains the masters, nodes, and etcd groups [ OSEv3 : children ] masters nodes etcd # Set variables common for all OSEv3 hosts [OSEv3:vars] # SSH user, this user should allow ssh based auth without requiring a password ansible_ssh_user=my_user # If ansible_ssh_user is not root, ansible_become must be set to true ansible_become=true openshift_deployment_type=openshift-enterprise oreg_url=registry.access.redhat.com/openshift3/ose-${component}:${version} oreg_auth_user=\"YOUR_RED_HAT_USERNAME\" oreg_auth_password=\"YOUR_RED_HAT_PASSWORD\" openshift_cloudprovider_kind=gce openshift_gcp_project=YOUR_GOOGLE_PROJECT_ID openshift_gcp_prefix=YOUR_PREFIX # If deploying single zone cluster set to \"False\" openshift_gcp_multizone=\"False\" openshift_hosted_registry_replicas=1 openshift_hosted_registry_storage_kind=object openshift_hosted_registry_storage_provider=gcs openshift_hosted_registry_storage_gcs_bucket=<prefix>-ocp-registry openshift_gcp_network_name=vpc-<prefix> #openshift_master_cluster_hostname=api.ocp.kearos.net openshift_master_cluster_public_hostname=api.ocp.example.com openshift_master_default_subdomain=apps.ocp.example.com openshift_master_api_port=443 openshift_master_console_port=443 os_firewall_use_firewalld=True # https://docs.openshift.com/container-platform/3.11/install_config/configuring_authentication.html#identity-providers_parameters openshift_master_identity_providers=[{'name' : 'htpasswd_auth' , 'login' : 'true' , 'challenge' : 'true' , 'kind' : 'HTPasswdPasswordIdentityProvider' }] openshift_master_htpasswd_users={'administrator' : 'password' } # host group for masters [ masters ] <prefix>-master1.c.ps-dev-201405.internal # host group for etcd [ etcd ] <prefix>-master1.c.ps-dev-201405.internal # host group for nodes, includes region info [ nodes ] <prefix>-master1.c.ps-dev-201405.internal openshift_node_group_name='node-config-master' <prefix>-node1.c.ps-dev-201405.internal openshift_node_group_name='node-config-compute' <prefix>-node2.c.ps-dev-201405.internal openshift_node_group_name='node-config-compute' <prefix>-node3.c.ps-dev-201405.internal openshift_node_group_name='node-config-compute' <prefix>-node4.c.ps-dev-201405.internal openshift_node_group_name='node-config-compute' <prefix>-infra1.c.ps-dev-201405.internal openshift_node_group_name='node-config-infra' Important Make sure to replace the YOUR_... placeholder values with your actual values. oreg_auth_user (YOUR_RED_HAT_USERNAME) oreg_auth_password (YOUR_RED_HAT_PASSWORD) YOUR_GOOGLE_PROJECT_ID","title":"Inventory File"},{"location":"openshift/rhos311-gcp-medium/#install-rhos-311-with-ansible","text":"There are two ways to install RHOS 3.11. Via Ansible directly 8 , or via Ansible in a container[^9]. As our nodes are configured according to what the Ansible installation requires, there's no need to rely on the container. Additionally, if you want to use the container way, you have to make sure the container can use the same DNS configuration as the nodes can themselves. I've not done this, so this would be on you!","title":"Install RHOS 3.11 with Ansible"},{"location":"openshift/rhos311-gcp-medium/#final-preparations","text":"Ansible creates a fact file. It does so at a location a non-root user doesn't have access to. So it is best to create this file upfront - on every node - and chown it to the user that will do the ssh/Ansible install. sudo mkdir -p /etc/ansible/facts.d sudo chown -R my_user /etc/ansible/facts.d","title":"Final Preparations"},{"location":"openshift/rhos311-gcp-medium/#install-openshift","text":"We install OpenShift via two scripts, playbooks/prerequisites.yml and playbooks/deploy_cluster.yml . When we install openshift-ansible atomic via yum, we also get the Ansible playbooks for OpenShift. Either go into the directory of those files, or use the entire path; cd /usr/share/ansible/openshift-ansible Execute OpenShift Pre-requisites script: ansible-playbook -i /home/my_user/inventoryFile /usr/share/ansible/openshift-ansible/playbooks/prerequisites.yml If all is successful, it will end with all actions in green and finished successfully (or similar) . Once this is the case, execute OpenShift Installation: ansible-playbook -i /home/my_user/inventoryFile /usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml Now you should be able to run oc get nodes on the installation node.","title":"Install OpenShift"},{"location":"openshift/rhos311-gcp-medium/#create-users","text":"In the inventoryFile , I've configured the OpenShift user management as follows: openshift_master_identity_providers=[{'name' : 'htpasswd_auth' , 'login' : 'true' , 'challenge' : 'true' , 'kind' : 'HTPasswdPasswordIdentityProvider' }] openshift_master_htpasswd_users={'administrator' : 'password' } Caution You probably do NOT want to do this for a production environment. RedHat has a guide on this 10 , I suggest you consult it for an overview of the options. This configuration means that you have to create the users on the <prefix>-master1 node, via htpasswd .","title":"Create Users"},{"location":"openshift/rhos311-gcp-medium/#create-user-via-htpasswd","text":"sudo htpasswd /etc/origin/master/htpasswd <user> Mind you, this user does not have any rights yet, nor a token with which they can login to the Kubernetes API!","title":"Create user via htpasswd"},{"location":"openshift/rhos311-gcp-medium/#give-him-admin-rights","text":"oc create clusterrolebinding registry-controller --clusterrole = cluster-admin --user = <user>","title":"Give him admin rights"},{"location":"openshift/rhos311-gcp-medium/#get-config","text":"First, login with oc login , this will prompt you for a username and then password. Fill in the values for the user you've created above with htpasswd . Only once you've logged in via oc login , do you get a token. Use the password for the console, but the token for kubeconfig/docker registry. To view the token, first confirm you're logged in with the right user: oc whoami Then retrieve the token as follows: oc whoami -t To retrieve the config file for Kubernetes's API, you can issue the following command. oc config view","title":"Get Config"},{"location":"openshift/rhos311-gcp-medium/#login-to-openshift-registry","text":"To retrieve the Registry URL: kubectl get route --namespace default Which should give you something like this: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD docker-registry docker-registry-default.apps.ocp.example.com docker-registry <all> passthrough None registry-console registry-console-default.apps.ocp..example.com registry-console <all> passthrough None Unless you have configured a proper SSL certificate, docker won't let you login. See the next chapter for more information on that. If you do have a SSL certificate, this is how you can login: docker login -u <user> -p <token> registry-console-default.apps.ocp..example.com","title":"Login to OpenShift Registry"},{"location":"openshift/rhos311-gcp-medium/#ssl-via-certmanager","text":"There are many ways to get a valid SSL certificate for your domain. Here I guide you through the following: install old certmanager - the latest requires Kubernetes 1.12+ Confgure DNS Verification to let Certmanager use CloudDNS 12 generate cert for *.apps.ocp.kearos.net add cert to the OpenShift Router and Docker Registry resources","title":"SSL Via Certmanager"},{"location":"openshift/rhos311-gcp-medium/#install-compatible-certmanager","text":"Luckily, Certmanager has documentation on how to install itself on OpenShift 3.11 11 . Below are the steps, yes it is that simple. oc create namespace cert-manager oc apply --validate = false -f https://github.com/jetstack/cert-manager/releases/download/v0.14.0/cert-manager-legacy.yaml","title":"Install Compatible Certmanager"},{"location":"openshift/rhos311-gcp-medium/#confgure-dns-verification","text":"First we need to setup a Google cloud service account 13 , and retrieve the json key. Save this file as credentials.json , and then create a kubernetes secret . kubectl create secret generic external-dns-gcp-sa --from-file = credentials.json","title":"Confgure DNS Verification"},{"location":"openshift/rhos311-gcp-medium/#certificate-issuer","text":"apiVersion : cert-manager.io/v1alpha2 kind : Issuer metadata : name : letsencrypt-prod spec : acme : email : <INSERT_YOUR_EMAIL> privateKeySecretRef : name : letsencrypt-prod server : https://acme-v02.api.letsencrypt.org/directory solvers : - dns01 : clouddns : project : <INSERT_YOUR_GOOGLE_PROJECT_ID> serviceAccountSecretRef : key : credentials.json name : external-dns-gcp-sa","title":"Certificate Issuer"},{"location":"openshift/rhos311-gcp-medium/#wildcard-certificate","text":"apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : acme-crt spec : secretName : acme-crt-secret dnsNames : - apps.ocp.example.com - \"*.apps.ocp.example.com\" issuerRef : name : letsencrypt-prod # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind : Issuer group : cert-manager.io Caution Don't forget to change the dnsNames !","title":"Wildcard Certificate"},{"location":"openshift/rhos311-gcp-medium/#configure-certificate-in-router","text":"HA Proxy expect a crt file with the key and cert, while the Certmanager secret's tls.crt file only contains the certificate - how unexpected. So, what we have to do is the following: * get tls.crt raw data (the actual certificate) * get tls.key raw data * add the data from tls.key to a file tls.key * add the data from both tls.key and tls.crt to a file tls.crt * create a new secret * edit the Router to use our new secret * kill the router pod to force it to load our new certificate kubectl get secret acme-crt-secret -o yaml echo \"<base64 secret value> | base64 -D\" Or, if you have jq 14 installed, you can do something like this. kubectl get secret -n default acme-crt-secret -o json | jq -r '.data | map_values(@base64d)' Add them both the PEM certificate and the key to the file tls.crt , and the key to the file tls.key . Create a new secret kubectl create secret tls letsencrypt-copy --cert = tls.crt --key = tls.key And then edit the Route, via the following command: kubectl edit -n default replicationcontrollers router-1 Change the secret.secretName value of the volume named server-certificate from router-certs to letsencrypt-copy . It should then look like this: volumes : - name : metrics-server-certificate secret : defaultMode : 420 secretName : router-metrics-tls - name : server-certificate secret : defaultMode : 420 secretName : letsencrypt-copy","title":"Configure Certificate In Router"},{"location":"openshift/rhos311-gcp-medium/#configure-certificate-in-docker-registry","text":"Edit docker registry as well: kubectl edit replicationcontrollers docker-registry-1 Make sure to overide the naming convention of registry.crt and registry.key to the sensible tls.key and tls.crt . - name : REGISTRY_HTTP_TLS_KEY value : /etc/secrets/tls.key - name : OPENSHIFT_DEFAULT_REGISTRY value : docker-registry.default.svc:5000 - name : REGISTRY_CONFIGURATION_PATH value : /etc/registry/config.yml - name : REGISTRY_OPENSHIFT_SERVER_ADDR value : docker-registry.default.svc:5000 - name : REGISTRY_HTTP_TLS_CERTIFICATE value : /etc/secrets/tls.crt","title":"Configure Certificate in Docker Registry"},{"location":"openshift/rhos311-gcp-medium/#references","text":"https://access.redhat.com/documentation/en-us/reference_architectures/2018/html/deploying_and_managing_openshift_3.9_on_google_cloud_platform/ \u21a9 https://www.terraform.io/docs/modules/index.html \u21a9 https://cloud.google.com/storage/docs/key-terms#buckets \u21a9 \u21a9 https://docs.openshift.com/container-platform/3.11/install_config/registry/index.html \u21a9 https://iterm2.com/ \u21a9 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-install-host-preparation#configuring-docker-storage \u21a9 https://access.redhat.com/RegistryAuthentication \u21a9 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html-single/installing_clusters/index#install-config-example-inventories [ ^9]: https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-configuring-inventory-file#configuring-cluster-variables \u21a9 \u21a9 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-configuring-inventory-file#advanced-install-registry-storage \u21a9 https://docs.openshift.com/container-platform/3.11/install_config/configuring_authentication.html#identity-providers_parameters \u21a9 \u21a9 https://cert-manager.io/docs/installation/openshift/ \u21a9 https://cloud.google.com/dns/ \u21a9 https://access.redhat.com/documentation/en-us/reference_architectures/2018/html-single/deploying_and_managing_openshift_3.9_on_google_cloud_platform/index#google_cloud_platform_service_account \u21a9 https://stedolan.github.io/jq/ \u21a9","title":"References"},{"location":"openshift/rhos311-gcp-minimal/","text":"RedHat OpenShift 3.11 on GCP - Minimal \u00b6 Why am I writing this guide? Well, to document my own steps taken. Also, I think the guide from Google 1 is to abstract to be useful, and the guide from RedHat 2 contains such number of options and digressions its hard to focus on what you require. Note This guide is written early March 2020, using jx version 2.0.1212 and OpenShift version v3.11.170 . Pre-requisites \u00b6 What do we need to get started? active GCP account with billing enabled GCP project gcloud tool installed terraform 0.12 + installed active RedHat account which hasn't used its 60 day trial license of OpenShift yet Process \u00b6 create Terraform configuration for GCP VM's create the VM's in GCP with RedHat Enterprise Linux v7 install OpenShift pre-requisites on each VM create OpenShift Ansible configuration install OpenShift via Ansible GCP Terraform \u00b6 What Do We Need \u00b6 Having gone through the process of installing RHOS 3.11 once, I ran into an issue. The documentation makes it seems you only need master nodes, compute nodes and VM's for etcd (can be the same as Master ). However, you also need at least one infra node. You can opt for a HA cluster, with three master nodes, or a single master node for a test cluster. I'm going with the latter. The master node will house the Kubernetes Control Plane, the infra node will house the OpenShift infra. As we won't have cluster autoscaling - a bit fancy for a manual test cluster - we have to make sure the machines are beefy to take the entire workload. Another thing we need for OpenShift, is having DNS that works between the nodes. For example, you should be able to say node1 and end up at the correct machine. Due to GCP networking, this internal DNS works out-of-the-box for any machine with our network/project. !!!! important Our machines need to have unique names! So I ended up with the following: 1x master node -> n1-standard-8 : 8 cores, 30gb mem 1x infra node -> n1-standard-8 : 8 cores, 30gb mem 2x compute node -> n1-standard-4 : 4 cores, 15gb mem (each) Caution For a first iteration, I've ignored creating a separate VPC and networking configuration. This to avoid learning too many things at once. You probably do want that for a more secure cluster. Read the medium effort guide in case you want to. VM Image \u00b6 Of course, if you want to run RedHat OpenShift Enterprise (RHOS), your VM's need to run a RedHat Enterprise Linux distribution(RHEL). In order to figure out which vm images are currently available, you can issue the following command via gcloud . gcloud compute images list --project rhel-cloud Which should give a response like this: NAME PROJECT FAMILY DEPRECATED STATUS rhel-6-v20200205 rhel-cloud rhel-6 READY rhel-7-v20200205 rhel-cloud rhel-7 READY rhel-8-v20200205 rhel-cloud rhel-8 READY For the VM image in our Terraform configuration, we will use the NAME of the image. For RHOS 3.11, RedHat strongly recommends using RHEL 7, so we use rhel-7-v20200205 . Terraform Configuration \u00b6 We have the following files: main.tf : contains the main configuration for the provider, in this case google variables.tf : the variables and their defaults master-node.tf : the master node configuration infra-node.tf : the infra node configuration compute-nodes.tf : the two compute node configurations Important We need to ssh into the VMs. To make this easy, I'm using a local ssh key and make sure it is configured on the VMs. See ssh-keys = \"joostvdg:${file(\"~/.ssh/id_rsa.pub\")}\" in the metadata block. The first part of the value joostvdg is my desired username. Change this if you want. main.tf terraform { required_version = \"~> 0.12\" } provider \"google\" { version = \"~> 2.18.1\" project = var.project region = var.region zone = var.main_zone } variables.tf variable \"project\" { } variable \"name\" { default = \"jx-openshift-311\" } variable \"compute_machine_type\" { default = \"n1-standard-4\" } variable \"master_machine_type\" { default = \"n1-standard-8\" } variable \"vm_image\" { default = \"rhel-7-v20200205\" } variable \"master_zone\" { default = \"europe-west4-a\" } master-node.tf resource \"google_compute_instance\" \"master\" { name = \"master\" machine_type = var.master_machine_type zone = var.master_zone allow_stopping_for_update = true boot_disk { initialize_params { image = var.vm_image size = 100 } } // Local SSD disk scratch_disk { interface = \"SCSI\" } network_interface { network = \"default\" # network_ip = google_compute_address.masterip.address access_config { # external address } } metadata = { ssh-keys = \"joostvdg:${file(\" ~/.ssh/id_rsa.pub \")}\" } service_account { scopes = [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" , \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" , \"https://www.googleapis.com/auth/cloud-platform\" ] } } infra-node.tf resource \"google_compute_instance\" \"infra1\" { name = \"infra1\" machine_type = var.master_machine_type zone = var.master_zone allow_stopping_for_update = true boot_disk { initialize_params { image = var.vm_image size = 100 } } // Local SSD disk scratch_disk { interface = \"SCSI\" } network_interface { network = \"default\" # network_ip = google_compute_address.node 2 ip.address access_config { # external address } } metadata = { ssh-keys = \"joostvdg:${file(\" ~/.ssh/id_rsa.pub \")}\" } service_account { scopes = [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" , \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" , \"https://www.googleapis.com/auth/cloud-platform\" ] } } compute-node.tf resource \"google_compute_instance\" \"node1\" { name = \"node1\" machine_type = var.compute_machine_type zone = var.master_zone allow_stopping_for_update = true boot_disk { initialize_params { image = var.vm_image size = 100 } } // Local SSD disk scratch_disk { interface = \"SCSI\" } network_interface { network = \"default\" # network_ip = google_compute_address.node 1 ip.address access_config { # external address } } metadata = { ssh-keys = \"joostvdg:${file(\" ~/.ssh/id_rsa.pub \")}\" } service_account { scopes = [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" , \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" , \"https://www.googleapis.com/auth/cloud-platform\" ] } } resource \"google_compute_instance\" \"node2\" { name = \"node2\" machine_type = \"n1-standard-4\" zone = var.master_zone allow_stopping_for_update = true boot_disk { initialize_params { image = var.vm_image size = 100 } } // Local SSD disk scratch_disk { interface = \"SCSI\" } network_interface { network = \"default\" # network_ip = google_compute_address.node 2 ip.address access_config { # external address } } metadata = { ssh-keys = \"joostvdg:${file(\" ~/.ssh/id_rsa.pub \")}\" } service_account { scopes = [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" , \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" , \"https://www.googleapis.com/auth/cloud-platform\" ] } } We should end up with four nodes: master infra node1 node2 Create VMs with Terraform \u00b6 terraform init terraform validate terraform plan -out plan.out terraform apply \"plan.out\" Verify VMs \u00b6 Before we can install the OpenShift pre-requisites, we verify if the VMs are ready to use. To verify the VMs, we will do the following: confirm we can ssh into each VM confirm we can use sudo in each VM confirm the infra node can call each VM by name ( infra , master , node1 , node2 ) confirm the infra node can ssh into all VMs (including itself!) SSH into VMs \u00b6 There are several ways to ssh into the VMs. You can do so via ssh installed on your machine, you can do so via the GCP console. I will use another option, using the gcloud CLI, using the ssh key I've configured in Terrafom ( ssh-keys = \"joostvdg:${file(\"~/.ssh/id_rsa.pub\")}\" ). Why am I using this form? Well, it makes it easier to reason about which machine I ssh into, as I can use the VM name . # your google project id PROJECT_ID = # the google zone the vm is in, for example: europe-west-4a VM_ZONE = gcloud beta compute --project $PROJECT_ID ssh --zone $VM_ZONE \"node1\" Confirm you can ssh into each VM by changing the zone/name accordingly. Confirm Sudo \u00b6 Our ssh user isn't root - as it should be - so we need to use sudo for some tasks. Confirm sudo works; sudo cat /etc/locale.conf Confirm Local DNS \u00b6 The OpenShift installation process and later OpenShift itself, relies on local dns . This means, it assumes if there's a node called master , it can do ssh master and it works. In GCP, DNS works within a Project by default. So assuming all the VMs are within the same project this works out-of-the-box. But, to avoid any surprises later, confirm it. [ joostvdg@master ~ ] $ ping master PING master.c.MY_PROJECT_ID.internal ( 10 .164.0.49 ) 56 ( 84 ) bytes of data. 64 bytes from master.c.MY_PROJECT_ID.internal ( 10 .164.0.49 ) : icmp_seq = 1 ttl = 64 time = 0 .041 ms 64 bytes from master.c.MY_PROJECT_ID.internal ( 10 .164.0.49 ) : icmp_seq = 2 ttl = 64 time = 0 .094 ms [ joostvdg@master ~ ] $ ping node1 PING node1.c.MY_PROJECT_ID.internal ( 10 .164.0.50 ) 56 ( 84 ) bytes of data. 64 bytes from node1.c.MY_PROJECT_ID.internal ( 10 .164.0.50 ) : icmp_seq = 1 ttl = 64 time = 1 .13 ms 64 bytes from node1.c.MY_PROJECT_ID.internal ( 10 .164.0.50 ) : icmp_seq = 2 ttl = 64 time = 0 .343 ms Note As you might expect, MY_PROJECT_ID will be the Google project Id where your VMs are. I've hidden that as a safety precaution, confirm it looks correct! Infra Node can SSH into others \u00b6 For the OpenShift installation, our installation VM has to be able to ssh into every other VM 3 . This doesn't work out of the box. Warning I used my own keys here directly, because this is a temporary project only used by me. If your usecase is different, and you're not sure how to proceed, consult a security professional! We have to create the ssh public key on every node for our ssh user (in my case, joostvdg ) and the private also for our installation host (for example, infra ). This might not be a security best practice, but I did this by copying over my ~/.ssh/id_rsa and ~/.ssh/id_rsa.pub to each node's user home ( /home/joostvdg/.ssh/ ). Important Once you've done this, ssh into the infra node, and confirm it can ssh to every other node. ssh joostvdg@node1 ssh joostvdg@node2 ssh joostvdg@master ssh joostvdg@infra -> YES, you have to ssh into yourself! This is important, because through this step, you can accept the prompt so the installation process can run unattended ! Make sure to set the correct permissions to the id_rsa file via sudo chmod 0400 ~/.ssh/id_rsa ! Fix Locale \u00b6 I kept running into a locale warning, about using one that didn't exist on the VM. If you want to get rid of this, you can change the /etc/locale.conf file. sudo vim /etc/locale.conf Make sure it looks like this. LANG = \"en_US.UTF-8\" LC_CTYPE = \"en_US.UTF-8\" LC_ALL = en_US.UTF-8 OpenShift Installation Pre-requisites \u00b6 Before we can install OpenShift, we have to bring our nodes into a certain state. We will do the following: register our VMs to RedHat register our VMs as part of our OpenShift Enterprise license configure yum for the installation process install and configure docker for the installation process login to the RedHat docker registry Register VMs \u00b6 Please note, these steps have to be done on every VM! If you use something like iterm2 , you can save yourself some time by having four parallel sessions for each VM. You do this by creating a split window ( control + command + D ), and once logged in, create a shared cursor via command + shift + i . We start by installing the subscription manager. sudo yum install subscription-manager -y We then register our instance with our RedHat account. sudo subscription-manager register --username = <user_name> --password = <password> sudo subscription-manager refresh Find the OpenShift subscription and you should get a single option. Use the id as the --pool in the next command. sudo subscription-manager list --available --matches '*OpenShift*' sudo subscription-manager attach --pool = ? Configure Yum Repos \u00b6 There's commands to disable each individual repository, but I found it easier to disable all, and then add those we need after. sudo subscription-manager repos --disable = \"*\" sudo yum repolist sudo yum-config-manager --disable \\* Install Default Packages \u00b6 As we've disable all of our yum repositories, we first add the once we need. sudo subscription-manager repos \\ --enable = \"rhel-7-server-rpms\" \\ --enable = \"rhel-7-server-extras-rpms\" \\ --enable = \"rhel-7-server-ose-3.11-rpms\" \\ --enable = \"rhel-7-server-ansible-2.8-rpms\" Once we have a set of usable yum repositories, we can then install all the packages we need. sudo yum install wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct openshift-ansible atomic python-docker-py docker device-mapper-libs device-mapper-event-libs -y Note There have been some bugs in the past, related to docker versions. If, for some reason, you have to downgrade to a known working version of docker, this is a way of doing that. sudo yum downgrade docker-rhel-push-plugin-1.13.1-75.git8633870.el7_5 docker-client-1.13.1-75.git8633870.el7_5 docker-common-1.13.1-75.git8633870.el7_5 docker-1.13.1-75.git8633870.el7_5 Once we have all the packages installed, make sure they're updated and then we reboot our machines. sudo yum update -y sudo reboot Install Docker \u00b6 I sneaked the docker packages into the previous installation command already, so we only have to enable/configure docker at this point. If you want to configure more details, such as where docker stores its volumes/data, please take a look at RedHat's installation guide 4 . sudo systemctl start docker.service sudo systemctl enable docker.service To confirm docker works: sudo systemctl status docker.service Make sure that on each node, your default user can use docker. sudo setfacl --modify user:joostvdg:rw /var/run/docker.sock Setup Registry Authentication \u00b6 The images for OpenShift come from RedHat's own docker registry. We have to login, before we can use those images 5 . So use your RedHat account credentials. docker login https://registry.redhat.io -u <USER> -p <PASS> Create Ansible Inventory File \u00b6 OpenShift comes with two ways of installing, via docker or via Ansible. The fun part, the docker container will use Ansible to install anyway. So no matter which way you will install OpenShift, you need to create a InventoryFile. RedHat has a couple of example files 6 , but these aren't complate - you need infra nodes as well! Important Configuration Items \u00b6 Bellow follow some variables I recommend configuring, for information, consult the RedHat documentation[^7]. OSEv3:children : the types of nodes to be configured OSEv3:vars : variables for the installation process ansible_become : set the True if Ansible can not run as root ansible_ssh_user : if Ansible cannot run as root, as which user should it ssh into the other nodes oreg_url : template for the docker images used by OpenShift, this should be registry.access.redhat.com/openshift3/ose-${component}:${version} , it will be used by components such as ETCD, Kubelet and so on oreg_auth_user : your RedHat account username oreg_auth_password : your RedHat account password openshift_cloudprovider_kind : the kind of cloud provider where RHOS is installed on, in the case of GCP its gce (don't ask me) openshift_gcp_project : is required to allow OpenShift the ability to create local disks in GCP for PersistentVolumes, should be your Google Project ID os_firewall_use_firewalld : use firewalld instead of iptables, seems to work better and is recommended by the RHOS 3.11 install guide (as of 2018+ I believe) Node definitions ( etcd , masters , nodes ): instructs Ansible which machine should be configured and with what Important If you use an external LoadBalancer, also set openshift_master_cluster_public_hostname . This variable overrides the public host name for the cluster, which defaults to the host name of the master. If you use an external load balancer, specify the address of the external load balancer. Example Inventory File \u00b6 # Create an OSEv3 group that contains the masters, nodes, and etcd groups [ OSEv3 : children ] masters nodes etcd # Set variables common for all OSEv3 hosts [OSEv3:vars] # SSH user, this user should allow ssh based auth without requiring a password ansible_ssh_user=joostvdg # If ansible_ssh_user is not root, ansible_become must be set to true ansible_become=true openshift_deployment_type=openshift-enterprise # This is supposed to be a template, do not change! oreg_url=registry.access.redhat.com/openshift3/ose-${component}:${version} oreg_auth_user=\"YOUR_RED_HAT_USERNAME\" oreg_auth_password=\"YOUR_RED_HAT_PASSWORD\" openshift_cloudprovider_kind=gce openshift_gcp_project=\"YOUR_GOOGLE_PROJECT_ID\" openshift_gcp_prefix=joostvdgrhos # If deploying single zone cluster set to \"False\" openshift_gcp_multizone=\"False\" openshift_master_api_port=443 openshift_master_console_port=443 os_firewall_use_firewalld=True # Enable if you want to use httpd for managing additional users # openshift_master_identity_providers=[{'name' : 'htpasswd_auth' , 'login' : 'true' , 'challenge' : 'true' , 'kind' : 'HTPasswdPasswordIdentityProvider' }] # openshift_master_htpasswd_users={'administrator': 'password'} # host group for masters [ masters ] master.c.<YOUR_GOOGLE_PROJECT_ID>.internal # host group for etcd [ etcd ] master.c.<YOUR_GOOGLE_PROJECT_ID>.internal # host group for nodes, includes region info [ nodes ] master.c.<YOUR_GOOGLE_PROJECT_ID>.internal openshift_node_group_name='node-config-master' node1.c.<YOUR_GOOGLE_PROJECT_ID>.internal openshift_node_group_name='node-config-compute' node2.c.<YOUR_GOOGLE_PROJECT_ID>.internal openshift_node_group_name='node-config-compute' infra1.c.<YOUR_GOOGLE_PROJECT_ID>.internal openshift_node_group_name='node-config-infra' Important Make sure to replace the YOUR_... placeholder values with your actual values. oreg_auth_user (YOUR_RED_HAT_USERNAME) oreg_auth_password (YOUR_RED_HAT_PASSWORD) YOUR_GOOGLE_PROJECT_ID Install RHOS 3.11 with Ansible \u00b6 There are two ways to install RHOS 3.11. Via Ansible directly 7 , or via Ansible in a container 8 . As our nodes are configured according to what the Ansible installation requires, there's no need to rely on the container. Additionally, if you want to use the container way, you have to make sure the container can use the same DNS configuration as the nodes can themselves. I've not done this, so this would be on you! Final Preparations \u00b6 Ansible creates a fact file. It does so at a location a non-root user doesn't have access to. So it is best to create this file upfront - on every node - and chown it to the user that will do the ssh/Ansible install. sudo mkdir -p /etc/ansible/facts.d sudo chown -R joostvdg /etc/ansible/facts.d Install OpenShift \u00b6 We install OpenShift via two scripts, playbooks/prerequisites.yml and playbooks/deploy_cluster.yml . When we install openshift-ansible atomic via yum, we also get the Ansible playbooks for OpenShift. Either go into the directory of those files, or use the entire path; cd /usr/share/ansible/openshift-ansible Execute OpenShift Pre-requisites script: ansible-playbook -i /home/joostvdg/inventoryFile /usr/share/ansible/openshift-ansible/playbooks/prerequisites.yml If all is successful, it will end with all actions in green and finished successfully (or similar) . Once this is the case, execute OpenShift Installation: ansible-playbook -i /home/joostvdg/inventoryFile /usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml Now you should be able to run oc get nodes on the installation node. Read More \u00b6 https://access.redhat.com/documentation/en-us/reference_architectures/2018/html-single/deploying_and_managing_openshift_3.9_on_google_cloud_platform/index#google_cloud_platform_networking https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html-single/installing_clusters/index#what-s-next https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-install-prerequisites#prereq-network-access http://crunchtools.com/hackers-guide-to-installing-openshift-container-platform-3-11/ https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html-single/installing_clusters/index#install-config-example-inventories https://docs.openshift.com/container-platform/3.11/admin_guide/manage_users.html https://itnext.io/explore-different-methods-to-build-and-push-image-to-private-registry-with-tekton-pipelines-5cad9dec1ddc References \u00b6 https://cloud.google.com/solutions/partners/openshift-on-gcp \u21a9 https://access.redhat.com/documentation/en-us/reference_architectures/2018/html/deploying_and_managing_openshift_3.9_on_google_cloud_platform/ \u21a9 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-install-host-preparation#ensuring-host-access \u21a9 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-install-host-preparation#configuring-docker-storage \u21a9 https://access.redhat.com/RegistryAuthentication \u21a9 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html-single/installing_clusters/index#install-config-example-inventories [ ^7]: https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-configuring-inventory-file#configuring-cluster-variables \u21a9 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-running-installation-playbooks#running-the-advanced-installation-rpm \u21a9 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-running-installation-playbooks#running-the-advanced-installation-containerized \u21a9","title":"OpenShift 3.11 GCP(minimal)"},{"location":"openshift/rhos311-gcp-minimal/#redhat-openshift-311-on-gcp-minimal","text":"Why am I writing this guide? Well, to document my own steps taken. Also, I think the guide from Google 1 is to abstract to be useful, and the guide from RedHat 2 contains such number of options and digressions its hard to focus on what you require. Note This guide is written early March 2020, using jx version 2.0.1212 and OpenShift version v3.11.170 .","title":"RedHat OpenShift 3.11 on GCP - Minimal"},{"location":"openshift/rhos311-gcp-minimal/#pre-requisites","text":"What do we need to get started? active GCP account with billing enabled GCP project gcloud tool installed terraform 0.12 + installed active RedHat account which hasn't used its 60 day trial license of OpenShift yet","title":"Pre-requisites"},{"location":"openshift/rhos311-gcp-minimal/#process","text":"create Terraform configuration for GCP VM's create the VM's in GCP with RedHat Enterprise Linux v7 install OpenShift pre-requisites on each VM create OpenShift Ansible configuration install OpenShift via Ansible","title":"Process"},{"location":"openshift/rhos311-gcp-minimal/#gcp-terraform","text":"","title":"GCP Terraform"},{"location":"openshift/rhos311-gcp-minimal/#what-do-we-need","text":"Having gone through the process of installing RHOS 3.11 once, I ran into an issue. The documentation makes it seems you only need master nodes, compute nodes and VM's for etcd (can be the same as Master ). However, you also need at least one infra node. You can opt for a HA cluster, with three master nodes, or a single master node for a test cluster. I'm going with the latter. The master node will house the Kubernetes Control Plane, the infra node will house the OpenShift infra. As we won't have cluster autoscaling - a bit fancy for a manual test cluster - we have to make sure the machines are beefy to take the entire workload. Another thing we need for OpenShift, is having DNS that works between the nodes. For example, you should be able to say node1 and end up at the correct machine. Due to GCP networking, this internal DNS works out-of-the-box for any machine with our network/project. !!!! important Our machines need to have unique names! So I ended up with the following: 1x master node -> n1-standard-8 : 8 cores, 30gb mem 1x infra node -> n1-standard-8 : 8 cores, 30gb mem 2x compute node -> n1-standard-4 : 4 cores, 15gb mem (each) Caution For a first iteration, I've ignored creating a separate VPC and networking configuration. This to avoid learning too many things at once. You probably do want that for a more secure cluster. Read the medium effort guide in case you want to.","title":"What Do We Need"},{"location":"openshift/rhos311-gcp-minimal/#vm-image","text":"Of course, if you want to run RedHat OpenShift Enterprise (RHOS), your VM's need to run a RedHat Enterprise Linux distribution(RHEL). In order to figure out which vm images are currently available, you can issue the following command via gcloud . gcloud compute images list --project rhel-cloud Which should give a response like this: NAME PROJECT FAMILY DEPRECATED STATUS rhel-6-v20200205 rhel-cloud rhel-6 READY rhel-7-v20200205 rhel-cloud rhel-7 READY rhel-8-v20200205 rhel-cloud rhel-8 READY For the VM image in our Terraform configuration, we will use the NAME of the image. For RHOS 3.11, RedHat strongly recommends using RHEL 7, so we use rhel-7-v20200205 .","title":"VM Image"},{"location":"openshift/rhos311-gcp-minimal/#terraform-configuration","text":"We have the following files: main.tf : contains the main configuration for the provider, in this case google variables.tf : the variables and their defaults master-node.tf : the master node configuration infra-node.tf : the infra node configuration compute-nodes.tf : the two compute node configurations Important We need to ssh into the VMs. To make this easy, I'm using a local ssh key and make sure it is configured on the VMs. See ssh-keys = \"joostvdg:${file(\"~/.ssh/id_rsa.pub\")}\" in the metadata block. The first part of the value joostvdg is my desired username. Change this if you want. main.tf terraform { required_version = \"~> 0.12\" } provider \"google\" { version = \"~> 2.18.1\" project = var.project region = var.region zone = var.main_zone } variables.tf variable \"project\" { } variable \"name\" { default = \"jx-openshift-311\" } variable \"compute_machine_type\" { default = \"n1-standard-4\" } variable \"master_machine_type\" { default = \"n1-standard-8\" } variable \"vm_image\" { default = \"rhel-7-v20200205\" } variable \"master_zone\" { default = \"europe-west4-a\" } master-node.tf resource \"google_compute_instance\" \"master\" { name = \"master\" machine_type = var.master_machine_type zone = var.master_zone allow_stopping_for_update = true boot_disk { initialize_params { image = var.vm_image size = 100 } } // Local SSD disk scratch_disk { interface = \"SCSI\" } network_interface { network = \"default\" # network_ip = google_compute_address.masterip.address access_config { # external address } } metadata = { ssh-keys = \"joostvdg:${file(\" ~/.ssh/id_rsa.pub \")}\" } service_account { scopes = [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" , \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" , \"https://www.googleapis.com/auth/cloud-platform\" ] } } infra-node.tf resource \"google_compute_instance\" \"infra1\" { name = \"infra1\" machine_type = var.master_machine_type zone = var.master_zone allow_stopping_for_update = true boot_disk { initialize_params { image = var.vm_image size = 100 } } // Local SSD disk scratch_disk { interface = \"SCSI\" } network_interface { network = \"default\" # network_ip = google_compute_address.node 2 ip.address access_config { # external address } } metadata = { ssh-keys = \"joostvdg:${file(\" ~/.ssh/id_rsa.pub \")}\" } service_account { scopes = [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" , \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" , \"https://www.googleapis.com/auth/cloud-platform\" ] } } compute-node.tf resource \"google_compute_instance\" \"node1\" { name = \"node1\" machine_type = var.compute_machine_type zone = var.master_zone allow_stopping_for_update = true boot_disk { initialize_params { image = var.vm_image size = 100 } } // Local SSD disk scratch_disk { interface = \"SCSI\" } network_interface { network = \"default\" # network_ip = google_compute_address.node 1 ip.address access_config { # external address } } metadata = { ssh-keys = \"joostvdg:${file(\" ~/.ssh/id_rsa.pub \")}\" } service_account { scopes = [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" , \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" , \"https://www.googleapis.com/auth/cloud-platform\" ] } } resource \"google_compute_instance\" \"node2\" { name = \"node2\" machine_type = \"n1-standard-4\" zone = var.master_zone allow_stopping_for_update = true boot_disk { initialize_params { image = var.vm_image size = 100 } } // Local SSD disk scratch_disk { interface = \"SCSI\" } network_interface { network = \"default\" # network_ip = google_compute_address.node 2 ip.address access_config { # external address } } metadata = { ssh-keys = \"joostvdg:${file(\" ~/.ssh/id_rsa.pub \")}\" } service_account { scopes = [ \"https://www.googleapis.com/auth/compute\" , \"https://www.googleapis.com/auth/devstorage.read_only\" , \"https://www.googleapis.com/auth/logging.write\" , \"https://www.googleapis.com/auth/monitoring\" , \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" , \"https://www.googleapis.com/auth/cloud-platform\" ] } } We should end up with four nodes: master infra node1 node2","title":"Terraform Configuration"},{"location":"openshift/rhos311-gcp-minimal/#create-vms-with-terraform","text":"terraform init terraform validate terraform plan -out plan.out terraform apply \"plan.out\"","title":"Create VMs with Terraform"},{"location":"openshift/rhos311-gcp-minimal/#verify-vms","text":"Before we can install the OpenShift pre-requisites, we verify if the VMs are ready to use. To verify the VMs, we will do the following: confirm we can ssh into each VM confirm we can use sudo in each VM confirm the infra node can call each VM by name ( infra , master , node1 , node2 ) confirm the infra node can ssh into all VMs (including itself!)","title":"Verify VMs"},{"location":"openshift/rhos311-gcp-minimal/#ssh-into-vms","text":"There are several ways to ssh into the VMs. You can do so via ssh installed on your machine, you can do so via the GCP console. I will use another option, using the gcloud CLI, using the ssh key I've configured in Terrafom ( ssh-keys = \"joostvdg:${file(\"~/.ssh/id_rsa.pub\")}\" ). Why am I using this form? Well, it makes it easier to reason about which machine I ssh into, as I can use the VM name . # your google project id PROJECT_ID = # the google zone the vm is in, for example: europe-west-4a VM_ZONE = gcloud beta compute --project $PROJECT_ID ssh --zone $VM_ZONE \"node1\" Confirm you can ssh into each VM by changing the zone/name accordingly.","title":"SSH into VMs"},{"location":"openshift/rhos311-gcp-minimal/#confirm-sudo","text":"Our ssh user isn't root - as it should be - so we need to use sudo for some tasks. Confirm sudo works; sudo cat /etc/locale.conf","title":"Confirm Sudo"},{"location":"openshift/rhos311-gcp-minimal/#confirm-local-dns","text":"The OpenShift installation process and later OpenShift itself, relies on local dns . This means, it assumes if there's a node called master , it can do ssh master and it works. In GCP, DNS works within a Project by default. So assuming all the VMs are within the same project this works out-of-the-box. But, to avoid any surprises later, confirm it. [ joostvdg@master ~ ] $ ping master PING master.c.MY_PROJECT_ID.internal ( 10 .164.0.49 ) 56 ( 84 ) bytes of data. 64 bytes from master.c.MY_PROJECT_ID.internal ( 10 .164.0.49 ) : icmp_seq = 1 ttl = 64 time = 0 .041 ms 64 bytes from master.c.MY_PROJECT_ID.internal ( 10 .164.0.49 ) : icmp_seq = 2 ttl = 64 time = 0 .094 ms [ joostvdg@master ~ ] $ ping node1 PING node1.c.MY_PROJECT_ID.internal ( 10 .164.0.50 ) 56 ( 84 ) bytes of data. 64 bytes from node1.c.MY_PROJECT_ID.internal ( 10 .164.0.50 ) : icmp_seq = 1 ttl = 64 time = 1 .13 ms 64 bytes from node1.c.MY_PROJECT_ID.internal ( 10 .164.0.50 ) : icmp_seq = 2 ttl = 64 time = 0 .343 ms Note As you might expect, MY_PROJECT_ID will be the Google project Id where your VMs are. I've hidden that as a safety precaution, confirm it looks correct!","title":"Confirm Local DNS"},{"location":"openshift/rhos311-gcp-minimal/#infra-node-can-ssh-into-others","text":"For the OpenShift installation, our installation VM has to be able to ssh into every other VM 3 . This doesn't work out of the box. Warning I used my own keys here directly, because this is a temporary project only used by me. If your usecase is different, and you're not sure how to proceed, consult a security professional! We have to create the ssh public key on every node for our ssh user (in my case, joostvdg ) and the private also for our installation host (for example, infra ). This might not be a security best practice, but I did this by copying over my ~/.ssh/id_rsa and ~/.ssh/id_rsa.pub to each node's user home ( /home/joostvdg/.ssh/ ). Important Once you've done this, ssh into the infra node, and confirm it can ssh to every other node. ssh joostvdg@node1 ssh joostvdg@node2 ssh joostvdg@master ssh joostvdg@infra -> YES, you have to ssh into yourself! This is important, because through this step, you can accept the prompt so the installation process can run unattended ! Make sure to set the correct permissions to the id_rsa file via sudo chmod 0400 ~/.ssh/id_rsa !","title":"Infra Node can SSH into others"},{"location":"openshift/rhos311-gcp-minimal/#fix-locale","text":"I kept running into a locale warning, about using one that didn't exist on the VM. If you want to get rid of this, you can change the /etc/locale.conf file. sudo vim /etc/locale.conf Make sure it looks like this. LANG = \"en_US.UTF-8\" LC_CTYPE = \"en_US.UTF-8\" LC_ALL = en_US.UTF-8","title":"Fix Locale"},{"location":"openshift/rhos311-gcp-minimal/#openshift-installation-pre-requisites","text":"Before we can install OpenShift, we have to bring our nodes into a certain state. We will do the following: register our VMs to RedHat register our VMs as part of our OpenShift Enterprise license configure yum for the installation process install and configure docker for the installation process login to the RedHat docker registry","title":"OpenShift Installation Pre-requisites"},{"location":"openshift/rhos311-gcp-minimal/#register-vms","text":"Please note, these steps have to be done on every VM! If you use something like iterm2 , you can save yourself some time by having four parallel sessions for each VM. You do this by creating a split window ( control + command + D ), and once logged in, create a shared cursor via command + shift + i . We start by installing the subscription manager. sudo yum install subscription-manager -y We then register our instance with our RedHat account. sudo subscription-manager register --username = <user_name> --password = <password> sudo subscription-manager refresh Find the OpenShift subscription and you should get a single option. Use the id as the --pool in the next command. sudo subscription-manager list --available --matches '*OpenShift*' sudo subscription-manager attach --pool = ?","title":"Register VMs"},{"location":"openshift/rhos311-gcp-minimal/#configure-yum-repos","text":"There's commands to disable each individual repository, but I found it easier to disable all, and then add those we need after. sudo subscription-manager repos --disable = \"*\" sudo yum repolist sudo yum-config-manager --disable \\*","title":"Configure Yum Repos"},{"location":"openshift/rhos311-gcp-minimal/#install-default-packages","text":"As we've disable all of our yum repositories, we first add the once we need. sudo subscription-manager repos \\ --enable = \"rhel-7-server-rpms\" \\ --enable = \"rhel-7-server-extras-rpms\" \\ --enable = \"rhel-7-server-ose-3.11-rpms\" \\ --enable = \"rhel-7-server-ansible-2.8-rpms\" Once we have a set of usable yum repositories, we can then install all the packages we need. sudo yum install wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct openshift-ansible atomic python-docker-py docker device-mapper-libs device-mapper-event-libs -y Note There have been some bugs in the past, related to docker versions. If, for some reason, you have to downgrade to a known working version of docker, this is a way of doing that. sudo yum downgrade docker-rhel-push-plugin-1.13.1-75.git8633870.el7_5 docker-client-1.13.1-75.git8633870.el7_5 docker-common-1.13.1-75.git8633870.el7_5 docker-1.13.1-75.git8633870.el7_5 Once we have all the packages installed, make sure they're updated and then we reboot our machines. sudo yum update -y sudo reboot","title":"Install Default Packages"},{"location":"openshift/rhos311-gcp-minimal/#install-docker","text":"I sneaked the docker packages into the previous installation command already, so we only have to enable/configure docker at this point. If you want to configure more details, such as where docker stores its volumes/data, please take a look at RedHat's installation guide 4 . sudo systemctl start docker.service sudo systemctl enable docker.service To confirm docker works: sudo systemctl status docker.service Make sure that on each node, your default user can use docker. sudo setfacl --modify user:joostvdg:rw /var/run/docker.sock","title":"Install Docker"},{"location":"openshift/rhos311-gcp-minimal/#setup-registry-authentication","text":"The images for OpenShift come from RedHat's own docker registry. We have to login, before we can use those images 5 . So use your RedHat account credentials. docker login https://registry.redhat.io -u <USER> -p <PASS>","title":"Setup Registry Authentication"},{"location":"openshift/rhos311-gcp-minimal/#create-ansible-inventory-file","text":"OpenShift comes with two ways of installing, via docker or via Ansible. The fun part, the docker container will use Ansible to install anyway. So no matter which way you will install OpenShift, you need to create a InventoryFile. RedHat has a couple of example files 6 , but these aren't complate - you need infra nodes as well!","title":"Create Ansible Inventory File"},{"location":"openshift/rhos311-gcp-minimal/#important-configuration-items","text":"Bellow follow some variables I recommend configuring, for information, consult the RedHat documentation[^7]. OSEv3:children : the types of nodes to be configured OSEv3:vars : variables for the installation process ansible_become : set the True if Ansible can not run as root ansible_ssh_user : if Ansible cannot run as root, as which user should it ssh into the other nodes oreg_url : template for the docker images used by OpenShift, this should be registry.access.redhat.com/openshift3/ose-${component}:${version} , it will be used by components such as ETCD, Kubelet and so on oreg_auth_user : your RedHat account username oreg_auth_password : your RedHat account password openshift_cloudprovider_kind : the kind of cloud provider where RHOS is installed on, in the case of GCP its gce (don't ask me) openshift_gcp_project : is required to allow OpenShift the ability to create local disks in GCP for PersistentVolumes, should be your Google Project ID os_firewall_use_firewalld : use firewalld instead of iptables, seems to work better and is recommended by the RHOS 3.11 install guide (as of 2018+ I believe) Node definitions ( etcd , masters , nodes ): instructs Ansible which machine should be configured and with what Important If you use an external LoadBalancer, also set openshift_master_cluster_public_hostname . This variable overrides the public host name for the cluster, which defaults to the host name of the master. If you use an external load balancer, specify the address of the external load balancer.","title":"Important Configuration Items"},{"location":"openshift/rhos311-gcp-minimal/#example-inventory-file","text":"# Create an OSEv3 group that contains the masters, nodes, and etcd groups [ OSEv3 : children ] masters nodes etcd # Set variables common for all OSEv3 hosts [OSEv3:vars] # SSH user, this user should allow ssh based auth without requiring a password ansible_ssh_user=joostvdg # If ansible_ssh_user is not root, ansible_become must be set to true ansible_become=true openshift_deployment_type=openshift-enterprise # This is supposed to be a template, do not change! oreg_url=registry.access.redhat.com/openshift3/ose-${component}:${version} oreg_auth_user=\"YOUR_RED_HAT_USERNAME\" oreg_auth_password=\"YOUR_RED_HAT_PASSWORD\" openshift_cloudprovider_kind=gce openshift_gcp_project=\"YOUR_GOOGLE_PROJECT_ID\" openshift_gcp_prefix=joostvdgrhos # If deploying single zone cluster set to \"False\" openshift_gcp_multizone=\"False\" openshift_master_api_port=443 openshift_master_console_port=443 os_firewall_use_firewalld=True # Enable if you want to use httpd for managing additional users # openshift_master_identity_providers=[{'name' : 'htpasswd_auth' , 'login' : 'true' , 'challenge' : 'true' , 'kind' : 'HTPasswdPasswordIdentityProvider' }] # openshift_master_htpasswd_users={'administrator': 'password'} # host group for masters [ masters ] master.c.<YOUR_GOOGLE_PROJECT_ID>.internal # host group for etcd [ etcd ] master.c.<YOUR_GOOGLE_PROJECT_ID>.internal # host group for nodes, includes region info [ nodes ] master.c.<YOUR_GOOGLE_PROJECT_ID>.internal openshift_node_group_name='node-config-master' node1.c.<YOUR_GOOGLE_PROJECT_ID>.internal openshift_node_group_name='node-config-compute' node2.c.<YOUR_GOOGLE_PROJECT_ID>.internal openshift_node_group_name='node-config-compute' infra1.c.<YOUR_GOOGLE_PROJECT_ID>.internal openshift_node_group_name='node-config-infra' Important Make sure to replace the YOUR_... placeholder values with your actual values. oreg_auth_user (YOUR_RED_HAT_USERNAME) oreg_auth_password (YOUR_RED_HAT_PASSWORD) YOUR_GOOGLE_PROJECT_ID","title":"Example Inventory File"},{"location":"openshift/rhos311-gcp-minimal/#install-rhos-311-with-ansible","text":"There are two ways to install RHOS 3.11. Via Ansible directly 7 , or via Ansible in a container 8 . As our nodes are configured according to what the Ansible installation requires, there's no need to rely on the container. Additionally, if you want to use the container way, you have to make sure the container can use the same DNS configuration as the nodes can themselves. I've not done this, so this would be on you!","title":"Install RHOS 3.11 with Ansible"},{"location":"openshift/rhos311-gcp-minimal/#final-preparations","text":"Ansible creates a fact file. It does so at a location a non-root user doesn't have access to. So it is best to create this file upfront - on every node - and chown it to the user that will do the ssh/Ansible install. sudo mkdir -p /etc/ansible/facts.d sudo chown -R joostvdg /etc/ansible/facts.d","title":"Final Preparations"},{"location":"openshift/rhos311-gcp-minimal/#install-openshift","text":"We install OpenShift via two scripts, playbooks/prerequisites.yml and playbooks/deploy_cluster.yml . When we install openshift-ansible atomic via yum, we also get the Ansible playbooks for OpenShift. Either go into the directory of those files, or use the entire path; cd /usr/share/ansible/openshift-ansible Execute OpenShift Pre-requisites script: ansible-playbook -i /home/joostvdg/inventoryFile /usr/share/ansible/openshift-ansible/playbooks/prerequisites.yml If all is successful, it will end with all actions in green and finished successfully (or similar) . Once this is the case, execute OpenShift Installation: ansible-playbook -i /home/joostvdg/inventoryFile /usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml Now you should be able to run oc get nodes on the installation node.","title":"Install OpenShift"},{"location":"openshift/rhos311-gcp-minimal/#read-more","text":"https://access.redhat.com/documentation/en-us/reference_architectures/2018/html-single/deploying_and_managing_openshift_3.9_on_google_cloud_platform/index#google_cloud_platform_networking https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html-single/installing_clusters/index#what-s-next https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-install-prerequisites#prereq-network-access http://crunchtools.com/hackers-guide-to-installing-openshift-container-platform-3-11/ https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html-single/installing_clusters/index#install-config-example-inventories https://docs.openshift.com/container-platform/3.11/admin_guide/manage_users.html https://itnext.io/explore-different-methods-to-build-and-push-image-to-private-registry-with-tekton-pipelines-5cad9dec1ddc","title":"Read More"},{"location":"openshift/rhos311-gcp-minimal/#references","text":"https://cloud.google.com/solutions/partners/openshift-on-gcp \u21a9 https://access.redhat.com/documentation/en-us/reference_architectures/2018/html/deploying_and_managing_openshift_3.9_on_google_cloud_platform/ \u21a9 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-install-host-preparation#ensuring-host-access \u21a9 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-install-host-preparation#configuring-docker-storage \u21a9 https://access.redhat.com/RegistryAuthentication \u21a9 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html-single/installing_clusters/index#install-config-example-inventories [ ^7]: https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-configuring-inventory-file#configuring-cluster-variables \u21a9 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-running-installation-playbooks#running-the-advanced-installation-rpm \u21a9 https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-running-installation-playbooks#running-the-advanced-installation-containerized \u21a9","title":"References"},{"location":"other/mkdocs/","text":"MKDocs \u00b6 This website is build using the following: MKDocs a python tool for building static websites from MarkDown files MK Material expansion/theme of MK Docs that makes it a responsive website with Google's Material theme Add information to the docs \u00b6 MKDocs can be a bit daunting to use, especially when extended with MKDocs Material and PyMdown Extensions . There are two parts to the site: 1) the markdown files, they're in docs/ and 2) the site listing (mkdocs.yml) and automation scripts, these can be found in docs-scripts/ . Extends current page \u00b6 To extend a current page, simply write the MarkDown as you're used to. For the specific extensions offered by PyMX and Material, checkout the following pages: MKDocs Material Getting Started Guide MKDocs Extensions PyMdown Extensions Usage Guide Add a new page \u00b6 In the docs-scripts/mkdocs.yml you will find the site structure under the yml item of pages . pages: - Home: index.md - Other Root Page: some-page.md - Root with children: - ChildOne: root2/child1.md - ChildTwo: root2/child2.md Things to know \u00b6 All .md files that are listed in the pages will be translated to an HTML file and dubbed {OriginalFileName}.html Naming a file index.md will allow you to refer to it by path without the file name we can refer to root2 simply by site/root2 and can omit the index. - Root: index.md - Root2: root2/index.html Configuration Of This Website \u00b6 # Theme # Configuration theme : feature : tabs : true name : 'material' language : 'en' logo : icon : 'public' palette : primary : 'orange' accent : 'red' font : text : 'Roboto' code : 'Roboto Mono' plugins : - search - minify : minify_html : true extra : social : - type : 'github' link : 'https://github.com/joostvdg' - type : 'twitter' link : 'https://twitter.com/joost_vdg' - type : 'linkedin' link : 'https://linkedin.com/in/joostvdg' # Extensions markdown_extensions : - admonition - codehilite : linenums : true guess_lang : true - footnotes - meta - toc : permalink : true - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.details - pymdownx.critic - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde Build the site locally \u00b6 As it is a Python tool, you can easily build it with Python (2.7 is recommended). The requirements are captured in a pip install scripts: docs-scripts/install.sh where the dependencies are in Pip's requirements.txt . Once that is done, you can do the following: mkdocs build --clean Which will generate the site into docs-scripts/site where you can simply open the index.html with a browser - it is a static site. For docker, you can use the *.sh scripts, or simply run.sh to kick of the entire build. Dependencies \u00b6 You can use pip to manage the dependencies required for building the site. pip install -r requirements.txt Requirements.txt \u00b6 mkdocs> = 1 .0.4 mkdocs-bootswatch> = 0 .4.0 python-jenkins> = 0 .4.10 mkdocs-material> = 4 .4.0 mkdocs-minify-plugin> = 0 .1.0 pygments> = 2 .4.2 pymdown-extensions> = 6 .0.0 Markdown> = 3 .0.1 Host It With Docker \u00b6 Dockerfile \u00b6 FROM nginx:mainline LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" LABEL version = \"1.0.0\" LABEL description = \"Mr J's knowledge base\" RUN apt-get update && apt-get install --no-install-recommends -y curl = 7 .* && rm -rf /var/lib/apt/lists/* HEALTHCHECK CMD curl --fail http://localhost:80/docs/ || exit 1 COPY site/ /usr/share/nginx/html/docs RUN ls -lath /usr/share/nginx/html/docs Build \u00b6 #!/usr/bin/env bash TAGNAME = \"joostvdg-github-io-image\" echo \"# Building new image with tag: $TAGNAME \" docker build --tag = $TAGNAME . Run \u00b6 #!/usr/bin/env bash IMAGE = \"joostvdg-github-io-image\" NAME = \"joostvdg-github-io-instance\" RUNNING = ` docker ps | grep -c $NAME ` if [ $RUNNING -gt 0 ] then echo \"Stopping $NAME \" docker stop $NAME fi EXISTING = ` docker ps -a | grep -c $NAME ` if [ $EXISTING -gt 0 ] then echo \"Removing $NAME \" docker rm $NAME fi echo \"Create new instance $NAME based on $IMAGE \" docker run --name $NAME -d -p 8088 :80 $IMAGE echo \"Tail the logs of the new instance\" docker logs $NAME # IP=$(docker inspect --format '{{.NetworkSettings.Networks.bridge.IPAddress}}' $NAME) # echo \"IP address of the container: $IP\" echo \"http://127.0.0.1.nip.io:8088/docs/\" Jenkins build \u00b6 Declarative format \u00b6 pipeline { agent none options { timeout(time: 10, unit: 'MINUTES') timestamps() buildDiscarder(logRotator(numToKeepStr: '5')) } stages { stage('Prepare'){ agent { label 'docker' } steps { deleteDir() } } stage('Checkout') { agent { label 'docker' } steps { checkout scm script { env.GIT_COMMIT_HASH = sh returnStdout: true, script: 'git rev-parse --verify HEAD' } } } stage('Build Docs') { agent { docker { image \"caladreas/mkdocs-docker-build-container\" label \"docker\" } } steps { sh 'cd docs-scripts && mkdocs build' } } stage('Prepare Docker Image') { agent { label 'docker' } environment { DOCKER_CRED = credentials('ldap') } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true, script: 'cd docs-scripts && docker run --rm -i lukasmartinelli/hadolint < Dockerfile' if (lintResult.trim() == '') { println 'Lint finished with no errors' } else { println 'Error found in Lint' println \"${lintResult}\" currentBuild.result = 'UNSTABLE' } } }, // end test dockerfile BuildImage: { sh 'chmod +x docs-scripts/build.sh' sh 'cd docs-scripts && ./build.sh' } , login: { sh \"docker login -u ${DOCKER_CRED_USR} -p ${DOCKER_CRED_PSW} registry\" } ) } post { success { sh 'chmod +x push.sh' sh './push.sh' } } } } }","title":"MKDocs (This Static Website)"},{"location":"other/mkdocs/#mkdocs","text":"This website is build using the following: MKDocs a python tool for building static websites from MarkDown files MK Material expansion/theme of MK Docs that makes it a responsive website with Google's Material theme","title":"MKDocs"},{"location":"other/mkdocs/#add-information-to-the-docs","text":"MKDocs can be a bit daunting to use, especially when extended with MKDocs Material and PyMdown Extensions . There are two parts to the site: 1) the markdown files, they're in docs/ and 2) the site listing (mkdocs.yml) and automation scripts, these can be found in docs-scripts/ .","title":"Add information to the docs"},{"location":"other/mkdocs/#extends-current-page","text":"To extend a current page, simply write the MarkDown as you're used to. For the specific extensions offered by PyMX and Material, checkout the following pages: MKDocs Material Getting Started Guide MKDocs Extensions PyMdown Extensions Usage Guide","title":"Extends current page"},{"location":"other/mkdocs/#add-a-new-page","text":"In the docs-scripts/mkdocs.yml you will find the site structure under the yml item of pages . pages: - Home: index.md - Other Root Page: some-page.md - Root with children: - ChildOne: root2/child1.md - ChildTwo: root2/child2.md","title":"Add a new page"},{"location":"other/mkdocs/#things-to-know","text":"All .md files that are listed in the pages will be translated to an HTML file and dubbed {OriginalFileName}.html Naming a file index.md will allow you to refer to it by path without the file name we can refer to root2 simply by site/root2 and can omit the index. - Root: index.md - Root2: root2/index.html","title":"Things to know"},{"location":"other/mkdocs/#configuration-of-this-website","text":"# Theme # Configuration theme : feature : tabs : true name : 'material' language : 'en' logo : icon : 'public' palette : primary : 'orange' accent : 'red' font : text : 'Roboto' code : 'Roboto Mono' plugins : - search - minify : minify_html : true extra : social : - type : 'github' link : 'https://github.com/joostvdg' - type : 'twitter' link : 'https://twitter.com/joost_vdg' - type : 'linkedin' link : 'https://linkedin.com/in/joostvdg' # Extensions markdown_extensions : - admonition - codehilite : linenums : true guess_lang : true - footnotes - meta - toc : permalink : true - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.details - pymdownx.critic - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde","title":"Configuration Of This Website"},{"location":"other/mkdocs/#build-the-site-locally","text":"As it is a Python tool, you can easily build it with Python (2.7 is recommended). The requirements are captured in a pip install scripts: docs-scripts/install.sh where the dependencies are in Pip's requirements.txt . Once that is done, you can do the following: mkdocs build --clean Which will generate the site into docs-scripts/site where you can simply open the index.html with a browser - it is a static site. For docker, you can use the *.sh scripts, or simply run.sh to kick of the entire build.","title":"Build the site locally"},{"location":"other/mkdocs/#dependencies","text":"You can use pip to manage the dependencies required for building the site. pip install -r requirements.txt","title":"Dependencies"},{"location":"other/mkdocs/#requirementstxt","text":"mkdocs> = 1 .0.4 mkdocs-bootswatch> = 0 .4.0 python-jenkins> = 0 .4.10 mkdocs-material> = 4 .4.0 mkdocs-minify-plugin> = 0 .1.0 pygments> = 2 .4.2 pymdown-extensions> = 6 .0.0 Markdown> = 3 .0.1","title":"Requirements.txt"},{"location":"other/mkdocs/#host-it-with-docker","text":"","title":"Host It With Docker"},{"location":"other/mkdocs/#dockerfile","text":"FROM nginx:mainline LABEL authors = \"Joost van der Griendt <joostvdg@gmail.com>\" LABEL version = \"1.0.0\" LABEL description = \"Mr J's knowledge base\" RUN apt-get update && apt-get install --no-install-recommends -y curl = 7 .* && rm -rf /var/lib/apt/lists/* HEALTHCHECK CMD curl --fail http://localhost:80/docs/ || exit 1 COPY site/ /usr/share/nginx/html/docs RUN ls -lath /usr/share/nginx/html/docs","title":"Dockerfile"},{"location":"other/mkdocs/#build","text":"#!/usr/bin/env bash TAGNAME = \"joostvdg-github-io-image\" echo \"# Building new image with tag: $TAGNAME \" docker build --tag = $TAGNAME .","title":"Build"},{"location":"other/mkdocs/#run","text":"#!/usr/bin/env bash IMAGE = \"joostvdg-github-io-image\" NAME = \"joostvdg-github-io-instance\" RUNNING = ` docker ps | grep -c $NAME ` if [ $RUNNING -gt 0 ] then echo \"Stopping $NAME \" docker stop $NAME fi EXISTING = ` docker ps -a | grep -c $NAME ` if [ $EXISTING -gt 0 ] then echo \"Removing $NAME \" docker rm $NAME fi echo \"Create new instance $NAME based on $IMAGE \" docker run --name $NAME -d -p 8088 :80 $IMAGE echo \"Tail the logs of the new instance\" docker logs $NAME # IP=$(docker inspect --format '{{.NetworkSettings.Networks.bridge.IPAddress}}' $NAME) # echo \"IP address of the container: $IP\" echo \"http://127.0.0.1.nip.io:8088/docs/\"","title":"Run"},{"location":"other/mkdocs/#jenkins-build","text":"","title":"Jenkins build"},{"location":"other/mkdocs/#declarative-format","text":"pipeline { agent none options { timeout(time: 10, unit: 'MINUTES') timestamps() buildDiscarder(logRotator(numToKeepStr: '5')) } stages { stage('Prepare'){ agent { label 'docker' } steps { deleteDir() } } stage('Checkout') { agent { label 'docker' } steps { checkout scm script { env.GIT_COMMIT_HASH = sh returnStdout: true, script: 'git rev-parse --verify HEAD' } } } stage('Build Docs') { agent { docker { image \"caladreas/mkdocs-docker-build-container\" label \"docker\" } } steps { sh 'cd docs-scripts && mkdocs build' } } stage('Prepare Docker Image') { agent { label 'docker' } environment { DOCKER_CRED = credentials('ldap') } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true, script: 'cd docs-scripts && docker run --rm -i lukasmartinelli/hadolint < Dockerfile' if (lintResult.trim() == '') { println 'Lint finished with no errors' } else { println 'Error found in Lint' println \"${lintResult}\" currentBuild.result = 'UNSTABLE' } } }, // end test dockerfile BuildImage: { sh 'chmod +x docs-scripts/build.sh' sh 'cd docs-scripts && ./build.sh' } , login: { sh \"docker login -u ${DOCKER_CRED_USR} -p ${DOCKER_CRED_PSW} registry\" } ) } post { success { sh 'chmod +x push.sh' sh './push.sh' } } } } }","title":"Declarative format"},{"location":"other/shell/","text":"Shell \u00b6 End Result \u00b6 OSX \u00b6 Iterm2 + Solarized + OZSH + Font Awesome \u00b6 https://gist.github.com/kevin-smets/8568070 Using Powerline Font with VS Code Terminal \u00b6 https://medium.com/@hippojs.guo/vs-code-fix-fonts-in-terminal-761cc821ef41 My Config - 9K \u00b6 ZSH_THEME = \"powerlevel9k/powerlevel9k\" #ZSH_THEME=\"powerlevel10k/powerlevel10k\" POWERLEVEL9K_MODE = \"awesome-patched\" P9KGT_BACKGROUND = 'dark' P9KGT_COLORS = 'light' POWERLEVEL9K_LEFT_PROMPT_ELEMENTS =( dir vcs ) POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS =( status command_execution_time kubecontext battery ram time ) POWERLEVEL9K_STATUS_ICON_BEFORE_CONTENT = true POWERLEVEL9K_PROMPT_ON_NEWLINE = true POWERLEVEL9K_RPROMPT_ON_NEWLINE = true POWERLEVEL9K_PROMPT_ADD_NEWLINE = true","title":"Shell"},{"location":"other/shell/#shell","text":"","title":"Shell"},{"location":"other/shell/#end-result","text":"","title":"End Result"},{"location":"other/shell/#osx","text":"","title":"OSX"},{"location":"other/shell/#iterm2-solarized-ozsh-font-awesome","text":"https://gist.github.com/kevin-smets/8568070","title":"Iterm2 + Solarized + OZSH + Font Awesome"},{"location":"other/shell/#using-powerline-font-with-vs-code-terminal","text":"https://medium.com/@hippojs.guo/vs-code-fix-fonts-in-terminal-761cc821ef41","title":"Using Powerline Font with VS Code Terminal"},{"location":"other/shell/#my-config-9k","text":"ZSH_THEME = \"powerlevel9k/powerlevel9k\" #ZSH_THEME=\"powerlevel10k/powerlevel10k\" POWERLEVEL9K_MODE = \"awesome-patched\" P9KGT_BACKGROUND = 'dark' P9KGT_COLORS = 'light' POWERLEVEL9K_LEFT_PROMPT_ELEMENTS =( dir vcs ) POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS =( status command_execution_time kubecontext battery ram time ) POWERLEVEL9K_STATUS_ICON_BEFORE_CONTENT = true POWERLEVEL9K_PROMPT_ON_NEWLINE = true POWERLEVEL9K_RPROMPT_ON_NEWLINE = true POWERLEVEL9K_PROMPT_ADD_NEWLINE = true","title":"My Config - 9K"},{"location":"other/vim/","text":"VIM \u00b6 End Result \u00b6 Install Vundle \u00b6 git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim Install plugins \u00b6 vim ~/.vimrc filetype off filetype plugin indent on syntax on set rtp += ~/.vim/bundle/Vundle.vim call vundle#begin () Plugin 'gmarik/Vundle.vim' Plugin 'reedes/vim-thematic' Plugin 'airblade/vim-gitgutter' Plugin 'vim-airline/vim-airline' Plugin 'vim-airline/vim-airline-themes' Plugin 'itchyny/lightline.vim' Plugin 'nathanaelkane/vim-indent-guides' Plugin 'scrooloose/nerdtree' Plugin 'editorconfig/editorconfig-vim' Plugin 'mhinz/vim-signify' call vundle#end () filetype plugin indent on Open VIM, and install the plugins: :installPlugins","title":"VIM"},{"location":"other/vim/#vim","text":"","title":"VIM"},{"location":"other/vim/#end-result","text":"","title":"End Result"},{"location":"other/vim/#install-vundle","text":"git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim","title":"Install Vundle"},{"location":"other/vim/#install-plugins","text":"vim ~/.vimrc filetype off filetype plugin indent on syntax on set rtp += ~/.vim/bundle/Vundle.vim call vundle#begin () Plugin 'gmarik/Vundle.vim' Plugin 'reedes/vim-thematic' Plugin 'airblade/vim-gitgutter' Plugin 'vim-airline/vim-airline' Plugin 'vim-airline/vim-airline-themes' Plugin 'itchyny/lightline.vim' Plugin 'nathanaelkane/vim-indent-guides' Plugin 'scrooloose/nerdtree' Plugin 'editorconfig/editorconfig-vim' Plugin 'mhinz/vim-signify' call vundle#end () filetype plugin indent on Open VIM, and install the plugins: :installPlugins","title":"Install plugins"},{"location":"productivity/","text":"Developer Productivity \u00b6 Commoditization \u00b6 \"The big change has been in the hardware/software cost ratio. The buyer of a $2 million machine in 1960 felt that he could afford $250,000 more ofr a customized payroll program, one that slipped easily and nondisruptively into the computer-hostile social environment. Buyers of %50,000 office machines today cannot conceivably afford customized payroll programs; so they adapt their paryoll procedures to the packages available.\" - 2 F. Brooks - No Silver Bullet Where should productivity be sought \u00b6 If you're looking to increase productivity, it would be best to answer some fundamental questions first. What should we be productive in? What is productivity? How do you measure productivity? The first step is to determine, what you should be productive in. If you're building software for example, it is in finding out what to build. \"The hardest single part of building a software system is deciding precisely what to build.\" - F. Brooks 2 That is actually already one step to far, as you would need a reason to build a software system. So the first step for any individual or organization (start up, or otherwise) is to find out what people want that you can offer. \"The fundamental activity of a startup is to turn ideas into products, measure how customers respond, and then learn whether to pivot or persevere. All successful startup processes should be geared to accelerate that feedback loop.\" - The Lean Startup 3 How do you measure productivity \u00b6 Grow v.s. Build \u00b6 The Balancing act between centralized and decentralized \u00b6 On Multitasking \u00b6 Deep Work Attention Residue > Why is it so hard to do my work Learning from Lean/Toyota \u00b6 Open Space Floor Plans \u00b6 http://rstb.royalsocietypublishing.org/content/373/1753/20170239 Conway's Law \u00b6 \"organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations.\" - M. Conway 1 Undifferentiated Heavy Lifting \u00b6 \"Work that needs to get done, but having it done doesn't bring our customers any direct benefit.\" - Dave Hahn Agile \u00b6 https://www.infoq.com/articles/agile-agile-blah-blah/ https://www.infoq.com/articles/death-agile-beyond Further reading \u00b6 Others \u00b6 https://www.youtube.com/watch?v=UTKIT6STSVM https://en.wikipedia.org/wiki/Complex_adaptive_system https://jobs.netflix.com/culture http://blackswanfarming.com/cost-of-delay/ https://www.rundeck.com/blog/tickets_make_operations_unnecessarily_miserable https://www.digitalocean.com/community/tutorials/what-is-immutable-infrastructure https://hbr.org/2015/12/what-the-research-tells-us-about-team-creativity-and-innovation https://www.thoughtworks.com/insights/blog/continuous-improvement-safe-environment https://qualitysafety.bmj.com/content/13/suppl_2/ii22 https://www.plutora.com/wp-content/uploads/dlm_uploads/2018/03/StateOfDevOpsTools_v14.pdf https://medium.com/@ATavgen/never-fail-twice-608147cb49b https://blogs.dropbox.com/dropbox/2018/07/study-high-performing-teams/?_tk=social&oqa=183tl01liov&linkId=100000003064606 http://psycnet.apa.org/record/1979-28632-001 https://pdfs.semanticscholar.org/a85d/432f44e43d61753bb8a121c246127b562a39.pdf https://medium.com/@dr_eprice/laziness-does-not-exist-3af27e312d01 https://en.wikipedia.org/wiki/Mindset#Fixed_and_growth http://www.reinventingorganizationswiki.com/Teal_Organizations https://www.mckinsey.com/business-functions/organization/our-insights/the-irrational-side-of-change-management https://www.barrypopik.com/index.php/new_york_city/entry/how_do_you_eat_an_elephant https://kadavy.net/blog/posts/mind-management-intro/ https://en.wikipedia.org/wiki/Planning_fallacy https://stories.lemonade.com/lemonade-proves-trust-pays-off-big-time-fdcf587af5a1 https://www.venturi-group.com/developer-to-cto/ https://dzone.com/articles/an-introduction-to-devops-principles https://www.thoughtworks.com/insights/blog/evolving-thoughtworks-internal-it-solve-broader-cross-cutting-problems https://www.thoughtworks.com/insights/blog/platform-tech-strategy-three-layers https://www.thoughtworks.com/insights/blog/why-it-departments-must-reinvent-themselves-part-1 https://en.wikipedia.org/wiki/Peter_principle https://hackernoon.com/why-all-engineers-must-understand-management-the-view-from-both-ladders-cc749ae14905 https://medium.freecodecamp.org/cognitive-bias-and-why-performance-management-is-so-hard-8852a1b874cd https://en.wikipedia.org/wiki/Horn_effect https://en.wikipedia.org/wiki/Halo_effect http://serendipstudio.org/bb/neuro/neuro02/web2/hhochman.html https://betterhumans.coach.me/how-to-be-a-better-manager-by-understanding-the-difference-between-market-norms-and-social-norms-3082d97d440f https://skillsmatter.com/skillscasts/10466-deep-dive-on-kubernetes-networking https://purplegriffon.com/blog/is-itil-agile-enough https://launchdarkly.com/blog/progressive-delivery-a-history-condensed/ http://www.collaborativefund.com/blog/real-world-vs-book-knowledge/ https://blog.codeship.com/using-jx-create-gitops-managed-jenkins-x-installation-cloudbees-codeship-terraform-google-kubernetes-engine/ https://martinfowler.com/articles/serverless.html https://www.quora.com/Some-people-including-the-creator-of-C-claim-that-there-is-a-huge-decline-of-quality-among-software-developers-What-seems-to-be-the-main-cause https://medium.com/netflix-techblog/full-cycle-developers-at-netflix-a08c31f83249 https://queue.acm.org/detail.cfm?id=3182626 http://www.safetydifferently.com/why-do-things-go-right/ https://www.quora.com/What-is-better-to-become-an-specialist-or-a-generalist-in-software-development-Does-the-full-stack-term-still-makes-sense https://cengizhan.com/3-pillars-of-observability-8e6cb5434206 https://www.thoughtworks.com/perspectives/edition1-agile-article https://blog.mexia.com.au/a-pace-layered-integration-architecture Presentations \u00b6 https://speakerdeck.com/tylertreat/the-future-of-ops https://www.slideshare.net/rheinwein/the-container-shame-spiral Articles \u00b6 https://dzone.com/articles/a-praise-for-self-service-in-it-value-streams http://blog.christianposta.com/microservices/application-safety-and-correctness-cannot-be-offloaded-to-istio-or-any-service-mesh/ https://www.gatesnotes.com/Books/Capitalism-Without-Capital?WT.mc_id=08_16_2018_06_CapitalismWithoutCapital_BG-LI_&WT.tsrc=BGLI&linkId=55623312 https://uxdesign.cc/stop-delivering-software-with-agile-it-doesn-t-work-edccea3ab5d3 Concept of Shared Services and beyond Introduction to Observability by Weave Net Article on the state of Systems Languages Article on SILO's Blog on Twitter's Engineering Efficiency Why Companies should have a Heroku platform for their developers Multitasking is bad for your health Microsoft research on Developer's perception of productivity Developer Productivity Struggles You cannot measure productivity The Productivity Paradox There is no Productivity Paradox: it lags behind investments Economist: solving the paradox The Myth Of Developer Productivity Effectiveness vs. Efficiency Lean Manufactoring Theory of Constraints Thoughtworks: demystifying Conway's Law John Allspaw: a mature role for automation Research from DORA Mik Kersten - Cambrian Explosion of DevOps Tools Mik Kersten - End of Manufacturing Line Analogy Mik Kersten - Mining the ground thruth of Enterprise Toolchains Framework for putting Mental Models to practice Books \u00b6 The Goal The Phoenix Project Continuous Delivery The Lean Startup The Lean Enterprise DevOps Handbook Thinking Fast and Slow Sapiens Project to Product Debugging Teams The Trusted Advisor Crossing the Chasm Papers \u00b6 https://www.researchgate.net/publication/200085969_Using_task_context_to_improve_programmer_productivity/link/540dce6b0cf2df04e75676d0/download https://storage.googleapis.com/pub-tools-public-publication-data/pdf/3d102e42ad79a345ebd6464047ac9a6cd10670f4.pdf On Writing \u00b6 https://www.proofreadingservices.com/pages/very References \u00b6 Conway's law in wikipedia \u21a9 No Silver Bullet - F. Brooks \u21a9 \u21a9 The Lean Startup Principles \u21a9","title":"General"},{"location":"productivity/#developer-productivity","text":"","title":"Developer Productivity"},{"location":"productivity/#commoditization","text":"\"The big change has been in the hardware/software cost ratio. The buyer of a $2 million machine in 1960 felt that he could afford $250,000 more ofr a customized payroll program, one that slipped easily and nondisruptively into the computer-hostile social environment. Buyers of %50,000 office machines today cannot conceivably afford customized payroll programs; so they adapt their paryoll procedures to the packages available.\" - 2 F. Brooks - No Silver Bullet","title":"Commoditization"},{"location":"productivity/#where-should-productivity-be-sought","text":"If you're looking to increase productivity, it would be best to answer some fundamental questions first. What should we be productive in? What is productivity? How do you measure productivity? The first step is to determine, what you should be productive in. If you're building software for example, it is in finding out what to build. \"The hardest single part of building a software system is deciding precisely what to build.\" - F. Brooks 2 That is actually already one step to far, as you would need a reason to build a software system. So the first step for any individual or organization (start up, or otherwise) is to find out what people want that you can offer. \"The fundamental activity of a startup is to turn ideas into products, measure how customers respond, and then learn whether to pivot or persevere. All successful startup processes should be geared to accelerate that feedback loop.\" - The Lean Startup 3","title":"Where should productivity be sought"},{"location":"productivity/#how-do-you-measure-productivity","text":"","title":"How do you measure productivity"},{"location":"productivity/#grow-vs-build","text":"","title":"Grow v.s. Build"},{"location":"productivity/#the-balancing-act-between-centralized-and-decentralized","text":"","title":"The Balancing act between centralized and decentralized"},{"location":"productivity/#on-multitasking","text":"Deep Work Attention Residue > Why is it so hard to do my work","title":"On Multitasking"},{"location":"productivity/#learning-from-leantoyota","text":"","title":"Learning from Lean/Toyota"},{"location":"productivity/#open-space-floor-plans","text":"http://rstb.royalsocietypublishing.org/content/373/1753/20170239","title":"Open Space Floor Plans"},{"location":"productivity/#conways-law","text":"\"organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations.\" - M. Conway 1","title":"Conway's Law"},{"location":"productivity/#undifferentiated-heavy-lifting","text":"\"Work that needs to get done, but having it done doesn't bring our customers any direct benefit.\" - Dave Hahn","title":"Undifferentiated Heavy Lifting"},{"location":"productivity/#agile","text":"https://www.infoq.com/articles/agile-agile-blah-blah/ https://www.infoq.com/articles/death-agile-beyond","title":"Agile"},{"location":"productivity/#further-reading","text":"","title":"Further reading"},{"location":"productivity/#others","text":"https://www.youtube.com/watch?v=UTKIT6STSVM https://en.wikipedia.org/wiki/Complex_adaptive_system https://jobs.netflix.com/culture http://blackswanfarming.com/cost-of-delay/ https://www.rundeck.com/blog/tickets_make_operations_unnecessarily_miserable https://www.digitalocean.com/community/tutorials/what-is-immutable-infrastructure https://hbr.org/2015/12/what-the-research-tells-us-about-team-creativity-and-innovation https://www.thoughtworks.com/insights/blog/continuous-improvement-safe-environment https://qualitysafety.bmj.com/content/13/suppl_2/ii22 https://www.plutora.com/wp-content/uploads/dlm_uploads/2018/03/StateOfDevOpsTools_v14.pdf https://medium.com/@ATavgen/never-fail-twice-608147cb49b https://blogs.dropbox.com/dropbox/2018/07/study-high-performing-teams/?_tk=social&oqa=183tl01liov&linkId=100000003064606 http://psycnet.apa.org/record/1979-28632-001 https://pdfs.semanticscholar.org/a85d/432f44e43d61753bb8a121c246127b562a39.pdf https://medium.com/@dr_eprice/laziness-does-not-exist-3af27e312d01 https://en.wikipedia.org/wiki/Mindset#Fixed_and_growth http://www.reinventingorganizationswiki.com/Teal_Organizations https://www.mckinsey.com/business-functions/organization/our-insights/the-irrational-side-of-change-management https://www.barrypopik.com/index.php/new_york_city/entry/how_do_you_eat_an_elephant https://kadavy.net/blog/posts/mind-management-intro/ https://en.wikipedia.org/wiki/Planning_fallacy https://stories.lemonade.com/lemonade-proves-trust-pays-off-big-time-fdcf587af5a1 https://www.venturi-group.com/developer-to-cto/ https://dzone.com/articles/an-introduction-to-devops-principles https://www.thoughtworks.com/insights/blog/evolving-thoughtworks-internal-it-solve-broader-cross-cutting-problems https://www.thoughtworks.com/insights/blog/platform-tech-strategy-three-layers https://www.thoughtworks.com/insights/blog/why-it-departments-must-reinvent-themselves-part-1 https://en.wikipedia.org/wiki/Peter_principle https://hackernoon.com/why-all-engineers-must-understand-management-the-view-from-both-ladders-cc749ae14905 https://medium.freecodecamp.org/cognitive-bias-and-why-performance-management-is-so-hard-8852a1b874cd https://en.wikipedia.org/wiki/Horn_effect https://en.wikipedia.org/wiki/Halo_effect http://serendipstudio.org/bb/neuro/neuro02/web2/hhochman.html https://betterhumans.coach.me/how-to-be-a-better-manager-by-understanding-the-difference-between-market-norms-and-social-norms-3082d97d440f https://skillsmatter.com/skillscasts/10466-deep-dive-on-kubernetes-networking https://purplegriffon.com/blog/is-itil-agile-enough https://launchdarkly.com/blog/progressive-delivery-a-history-condensed/ http://www.collaborativefund.com/blog/real-world-vs-book-knowledge/ https://blog.codeship.com/using-jx-create-gitops-managed-jenkins-x-installation-cloudbees-codeship-terraform-google-kubernetes-engine/ https://martinfowler.com/articles/serverless.html https://www.quora.com/Some-people-including-the-creator-of-C-claim-that-there-is-a-huge-decline-of-quality-among-software-developers-What-seems-to-be-the-main-cause https://medium.com/netflix-techblog/full-cycle-developers-at-netflix-a08c31f83249 https://queue.acm.org/detail.cfm?id=3182626 http://www.safetydifferently.com/why-do-things-go-right/ https://www.quora.com/What-is-better-to-become-an-specialist-or-a-generalist-in-software-development-Does-the-full-stack-term-still-makes-sense https://cengizhan.com/3-pillars-of-observability-8e6cb5434206 https://www.thoughtworks.com/perspectives/edition1-agile-article https://blog.mexia.com.au/a-pace-layered-integration-architecture","title":"Others"},{"location":"productivity/#presentations","text":"https://speakerdeck.com/tylertreat/the-future-of-ops https://www.slideshare.net/rheinwein/the-container-shame-spiral","title":"Presentations"},{"location":"productivity/#articles","text":"https://dzone.com/articles/a-praise-for-self-service-in-it-value-streams http://blog.christianposta.com/microservices/application-safety-and-correctness-cannot-be-offloaded-to-istio-or-any-service-mesh/ https://www.gatesnotes.com/Books/Capitalism-Without-Capital?WT.mc_id=08_16_2018_06_CapitalismWithoutCapital_BG-LI_&WT.tsrc=BGLI&linkId=55623312 https://uxdesign.cc/stop-delivering-software-with-agile-it-doesn-t-work-edccea3ab5d3 Concept of Shared Services and beyond Introduction to Observability by Weave Net Article on the state of Systems Languages Article on SILO's Blog on Twitter's Engineering Efficiency Why Companies should have a Heroku platform for their developers Multitasking is bad for your health Microsoft research on Developer's perception of productivity Developer Productivity Struggles You cannot measure productivity The Productivity Paradox There is no Productivity Paradox: it lags behind investments Economist: solving the paradox The Myth Of Developer Productivity Effectiveness vs. Efficiency Lean Manufactoring Theory of Constraints Thoughtworks: demystifying Conway's Law John Allspaw: a mature role for automation Research from DORA Mik Kersten - Cambrian Explosion of DevOps Tools Mik Kersten - End of Manufacturing Line Analogy Mik Kersten - Mining the ground thruth of Enterprise Toolchains Framework for putting Mental Models to practice","title":"Articles"},{"location":"productivity/#books","text":"The Goal The Phoenix Project Continuous Delivery The Lean Startup The Lean Enterprise DevOps Handbook Thinking Fast and Slow Sapiens Project to Product Debugging Teams The Trusted Advisor Crossing the Chasm","title":"Books"},{"location":"productivity/#papers","text":"https://www.researchgate.net/publication/200085969_Using_task_context_to_improve_programmer_productivity/link/540dce6b0cf2df04e75676d0/download https://storage.googleapis.com/pub-tools-public-publication-data/pdf/3d102e42ad79a345ebd6464047ac9a6cd10670f4.pdf","title":"Papers"},{"location":"productivity/#on-writing","text":"https://www.proofreadingservices.com/pages/very","title":"On Writing"},{"location":"productivity/#references","text":"Conway's law in wikipedia \u21a9 No Silver Bullet - F. Brooks \u21a9 \u21a9 The Lean Startup Principles \u21a9","title":"References"},{"location":"productivity/incidents/","text":"Incidents \u00b6 incidents five why's blameless postmortems identify causes (not culprits) assume good will take your time dangers of automation observability human bias human factors percententize work how much percent of work should be what?m figure out how to track, make it easy/automated identify real vs. desired, figure out how to get (closer) to desired \"Just Culture\" (as in, justice, it is just) bad apple theory = debunked bad apple theory = remove the small percentage of bad apples and the problem goes away Notes \u00b6 most incidents happen near updates/upgrades/deployments make deployments a non-event small increments, high frequency, automated, tested it is not systems, it not humans, but humans within systems human will make mistakes processes are (almost) always part of the problem automation needs to include sanity checks (ranges of sane values) References \u00b6 Below is a significant collection of references to resources that tackle the different parts of incident management. They can explain it better than I ever can, so use them to better your own understanding just as I have. Books \u00b6 Foundations of Safety Science Code Complete Talks \u00b6 Ironies Of Automation Google SRE: Postmortems and Retrospectives John Allspaw: Blameless Post Mortems Papers \u00b6 Patient Safety and the \"Just Culture\" - Marx D How do systems manage their adaptive capacity to successfully handle disruptions - M Branlat & D Woods Ironies Of Automation - Lisanne Bainbridge Managing The Development Of Large Software Systems - Winston Royce Articles \u00b6 Weathering The Unexpected - Kripa Krishnan John Allspaw: a mature role for automation John Allspaw: Resillience Engineering: Part I John Allspaw: getting the messy details is critical John Allspaw: Ask Me Anything John Allspaw: Blameless PostMortems And A Just Culture Etsy's Postmortem Proces Etsy's Winning Secret: Don't Play The Blame Game Blameless Portmortems at Etsy Google SRE - Postmortem Culture: Learning from Failure HoneyComb.io - Incident Review GitHub Outage Incident Analysis Google Postmortem AWS Postmortem (S3 outage) GitHub Page Listing Public Post Mortems Charity Majors: I Test In Prod The Network Is Reliable: an informal survey of real-world communications failures Charity Majors: Shipping Software Should Not Be Scary Circle CI: A brief history of devops part I: waterfall Circle CI: A brief history of devops part II: agile development Circle CI: A brief history of devops part III: automated testing and continuous integration Circle CI: A brief history of devops part IV: continuous delivery and deployment","title":"Incidents"},{"location":"productivity/incidents/#incidents","text":"incidents five why's blameless postmortems identify causes (not culprits) assume good will take your time dangers of automation observability human bias human factors percententize work how much percent of work should be what?m figure out how to track, make it easy/automated identify real vs. desired, figure out how to get (closer) to desired \"Just Culture\" (as in, justice, it is just) bad apple theory = debunked bad apple theory = remove the small percentage of bad apples and the problem goes away","title":"Incidents"},{"location":"productivity/incidents/#notes","text":"most incidents happen near updates/upgrades/deployments make deployments a non-event small increments, high frequency, automated, tested it is not systems, it not humans, but humans within systems human will make mistakes processes are (almost) always part of the problem automation needs to include sanity checks (ranges of sane values)","title":"Notes"},{"location":"productivity/incidents/#references","text":"Below is a significant collection of references to resources that tackle the different parts of incident management. They can explain it better than I ever can, so use them to better your own understanding just as I have.","title":"References"},{"location":"productivity/incidents/#books","text":"Foundations of Safety Science Code Complete","title":"Books"},{"location":"productivity/incidents/#talks","text":"Ironies Of Automation Google SRE: Postmortems and Retrospectives John Allspaw: Blameless Post Mortems","title":"Talks"},{"location":"productivity/incidents/#papers","text":"Patient Safety and the \"Just Culture\" - Marx D How do systems manage their adaptive capacity to successfully handle disruptions - M Branlat & D Woods Ironies Of Automation - Lisanne Bainbridge Managing The Development Of Large Software Systems - Winston Royce","title":"Papers"},{"location":"productivity/incidents/#articles","text":"Weathering The Unexpected - Kripa Krishnan John Allspaw: a mature role for automation John Allspaw: Resillience Engineering: Part I John Allspaw: getting the messy details is critical John Allspaw: Ask Me Anything John Allspaw: Blameless PostMortems And A Just Culture Etsy's Postmortem Proces Etsy's Winning Secret: Don't Play The Blame Game Blameless Portmortems at Etsy Google SRE - Postmortem Culture: Learning from Failure HoneyComb.io - Incident Review GitHub Outage Incident Analysis Google Postmortem AWS Postmortem (S3 outage) GitHub Page Listing Public Post Mortems Charity Majors: I Test In Prod The Network Is Reliable: an informal survey of real-world communications failures Charity Majors: Shipping Software Should Not Be Scary Circle CI: A brief history of devops part I: waterfall Circle CI: A brief history of devops part II: agile development Circle CI: A brief history of devops part III: automated testing and continuous integration Circle CI: A brief history of devops part IV: continuous delivery and deployment","title":"Articles"},{"location":"productivity/intellij/","text":"Intelli J \u00b6 Beneficial OS changes \u00b6 Linux \u00b6 Increase inotify watches \\","title":"Intelli J"},{"location":"productivity/intellij/#intelli-j","text":"","title":"Intelli J"},{"location":"productivity/intellij/#beneficial-os-changes","text":"","title":"Beneficial OS changes"},{"location":"productivity/intellij/#linux","text":"Increase inotify watches \\","title":"Linux"},{"location":"productivity/paradigms/","text":"Paradigms \u00b6 Product centered \u00b6 Resources \u00b6","title":"Paradigms"},{"location":"productivity/paradigms/#paradigms","text":"","title":"Paradigms"},{"location":"productivity/paradigms/#product-centered","text":"","title":"Product centered"},{"location":"productivity/paradigms/#resources","text":"","title":"Resources"},{"location":"productivity/remote/","text":"Working Remote \u00b6 Resources \u00b6 https://open.nytimes.com/how-to-grow-as-an-engineer-working-remotely-3baff8211f3e","title":"Working Remote"},{"location":"productivity/remote/#working-remote","text":"","title":"Working Remote"},{"location":"productivity/remote/#resources","text":"https://open.nytimes.com/how-to-grow-as-an-engineer-working-remotely-3baff8211f3e","title":"Resources"},{"location":"productivity/studies/","text":"Developer Productivity Studies \u00b6","title":"Developer Productivity Studies"},{"location":"productivity/studies/#developer-productivity-studies","text":"","title":"Developer Productivity Studies"},{"location":"productivity/team-development/","text":"Team Development \u00b6 Resources \u00b6 Article about Tuckman's theory of group development Wiki of Tuckman's theory of group development How google thinks about Team Effectiveness Google Re:Work tutorial on Team Effectiveness https://charity.wtf/2018/12/02/software-sprawl-the-golden-path-and-scaling-teams-with-agency/","title":"Team Development"},{"location":"productivity/team-development/#team-development","text":"","title":"Team Development"},{"location":"productivity/team-development/#resources","text":"Article about Tuckman's theory of group development Wiki of Tuckman's theory of group development How google thinks about Team Effectiveness Google Re:Work tutorial on Team Effectiveness https://charity.wtf/2018/12/02/software-sprawl-the-golden-path-and-scaling-teams-with-agency/","title":"Resources"},{"location":"productivity/tools/","text":"Developer Productivity Tools \u00b6","title":"Developer Productivity Tools"},{"location":"productivity/tools/#developer-productivity-tools","text":"","title":"Developer Productivity Tools"},{"location":"swe/API/","text":"API's \u00b6 https://docs.microsoft.com/en-us/azure/architecture/best-practices/api-design","title":"API's"},{"location":"swe/API/#apis","text":"https://docs.microsoft.com/en-us/azure/architecture/best-practices/api-design","title":"API's"},{"location":"swe/algorithms/","text":"","title":"Algorithms"},{"location":"swe/ddd/","text":"Domain Driven Design \u00b6","title":"Domain Driven Design"},{"location":"swe/ddd/#domain-driven-design","text":"","title":"Domain Driven Design"},{"location":"swe/distributed/","text":"Distributed Computing \u00b6 Distributed Computing fundamentals \u00b6 Time and Event ordering \u00b6 See: Lamport timestamp Distributed Applications \u00b6 Topics to take into account \u00b6 logging structured pulled into central log service Java: SLF4J + LogBack? Go: logrus tracing sampling based metrics prometheus including alert definitions network connection stability services discovery loadbalancing circuit brakers backpressure shallow queues connection pools dynamic/randomized backoff procedures network connection performance 3-step handshake binary over http standard protocols thin wrapper for UI: GraphQL thick wrapper for UI: JSON over HTTP (restful) Service to Service: gRPC / twirp Designing Distributed Systems - Brandon Burns \u00b6 Sidecar pattern \u00b6 docker run -d <my-app-image> After you run that image, you will receive the identifier for that specific container. It will look something like: cccf82b85000... If you don\u2019t have it, you can always look it up using the docker ps command, which will show all currently running containers. Assuming you have stashed that value in an environment variable named APP_ID, you can then run the topz container in the same PID namespace using: docker run --pid = container: ${ APP_ID } \\ -p 8080 :8080 brendanburns/topz:db0fa58 /server --address = 0 .0.0.0:8080 Resources \u00b6 Coursera course Article on synchronization in a distributed system http://label-schema.org/ https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236 https://eng.lyft.com/announcing-envoy-c-l7-proxy-and-communication-bus-92520b6c8191 https://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc https://cse.buffalo.edu/~demirbas/publications/cloudConsensus.pdf http://www.read.seas.harvard.edu/~kohler/class/08w-dsi/chandra07paxos.pdf https://medium.com/source-code/understanding-the-memcached-source-code-slab-i-9199de613762","title":"Distributed Computing"},{"location":"swe/distributed/#distributed-computing","text":"","title":"Distributed Computing"},{"location":"swe/distributed/#distributed-computing-fundamentals","text":"","title":"Distributed Computing fundamentals"},{"location":"swe/distributed/#time-and-event-ordering","text":"See: Lamport timestamp","title":"Time and Event ordering"},{"location":"swe/distributed/#distributed-applications","text":"","title":"Distributed Applications"},{"location":"swe/distributed/#topics-to-take-into-account","text":"logging structured pulled into central log service Java: SLF4J + LogBack? Go: logrus tracing sampling based metrics prometheus including alert definitions network connection stability services discovery loadbalancing circuit brakers backpressure shallow queues connection pools dynamic/randomized backoff procedures network connection performance 3-step handshake binary over http standard protocols thin wrapper for UI: GraphQL thick wrapper for UI: JSON over HTTP (restful) Service to Service: gRPC / twirp","title":"Topics to take into account"},{"location":"swe/distributed/#designing-distributed-systems-brandon-burns","text":"","title":"Designing Distributed Systems - Brandon Burns"},{"location":"swe/distributed/#sidecar-pattern","text":"docker run -d <my-app-image> After you run that image, you will receive the identifier for that specific container. It will look something like: cccf82b85000... If you don\u2019t have it, you can always look it up using the docker ps command, which will show all currently running containers. Assuming you have stashed that value in an environment variable named APP_ID, you can then run the topz container in the same PID namespace using: docker run --pid = container: ${ APP_ID } \\ -p 8080 :8080 brendanburns/topz:db0fa58 /server --address = 0 .0.0.0:8080","title":"Sidecar pattern"},{"location":"swe/distributed/#resources","text":"Coursera course Article on synchronization in a distributed system http://label-schema.org/ https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236 https://eng.lyft.com/announcing-envoy-c-l7-proxy-and-communication-bus-92520b6c8191 https://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc https://cse.buffalo.edu/~demirbas/publications/cloudConsensus.pdf http://www.read.seas.harvard.edu/~kohler/class/08w-dsi/chandra07paxos.pdf https://medium.com/source-code/understanding-the-memcached-source-code-slab-i-9199de613762","title":"Resources"},{"location":"swe/http-caching/","text":"HTTP Caching \u00b6 https://medium.freecodecamp.org/http-caching-in-depth-part-1-a853c6af99db","title":"HTTP Caching"},{"location":"swe/http-caching/#http-caching","text":"https://medium.freecodecamp.org/http-caching-in-depth-part-1-a853c6af99db","title":"HTTP Caching"},{"location":"swe/important-concepts/","text":"Software Engineering Concepts \u00b6 Resource management \u00b6 When there are finite resources in your system, manage them explicitly. Be that memory, CPU, amount of connections to a database or incoming http connections. Back Pressure \u00b6 Back pressure When one component is struggling to keep-up, the system as a whole needs to respond in a sensible way. It is unacceptable for the component under stress to fail catastrophically or to drop messages in an uncontrolled fashion. Since it can\u2019t cope and it can\u2019t fail it should communicate the fact that it is under stress to upstream components and so get them to reduce the load. This back-pressure is an important feedback mechanism that allows systems to gracefully respond to load rather than collapse under it. The back-pressure may cascade all the way up to the user, at which point responsiveness may degrade, but this mechanism will ensure that the system is resilient under load, and will provide information that may allow the system itself to apply other resources to help distribute the load, see Elasticity. 1 Further reading: DZone article Spotify Engineering Memoization \u00b6 Memoization In computing, memoization or memoisation is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again. Memoization has also been used in other contexts (and for purposes other than speed gains), such as in simple mutually recursive descent parsing. Important Theories \u00b6 Theory of constraints Law of demeter Conway's law Little's law Commoditization Amdahl's Law Web Technologies \u00b6 HTTP Caching \u00b6 https://medium.freecodecamp.org/http-caching-in-depth-part-1-a853c6af99db Reactive Manifesto \u21a9 Wikipedia article on Memoization \u21a9","title":"Important Concepts"},{"location":"swe/important-concepts/#software-engineering-concepts","text":"","title":"Software Engineering Concepts"},{"location":"swe/important-concepts/#resource-management","text":"When there are finite resources in your system, manage them explicitly. Be that memory, CPU, amount of connections to a database or incoming http connections.","title":"Resource management"},{"location":"swe/important-concepts/#back-pressure","text":"Back pressure When one component is struggling to keep-up, the system as a whole needs to respond in a sensible way. It is unacceptable for the component under stress to fail catastrophically or to drop messages in an uncontrolled fashion. Since it can\u2019t cope and it can\u2019t fail it should communicate the fact that it is under stress to upstream components and so get them to reduce the load. This back-pressure is an important feedback mechanism that allows systems to gracefully respond to load rather than collapse under it. The back-pressure may cascade all the way up to the user, at which point responsiveness may degrade, but this mechanism will ensure that the system is resilient under load, and will provide information that may allow the system itself to apply other resources to help distribute the load, see Elasticity. 1 Further reading: DZone article Spotify Engineering","title":"Back Pressure"},{"location":"swe/important-concepts/#memoization","text":"Memoization In computing, memoization or memoisation is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again. Memoization has also been used in other contexts (and for purposes other than speed gains), such as in simple mutually recursive descent parsing.","title":"Memoization"},{"location":"swe/important-concepts/#important-theories","text":"Theory of constraints Law of demeter Conway's law Little's law Commoditization Amdahl's Law","title":"Important Theories"},{"location":"swe/important-concepts/#web-technologies","text":"","title":"Web Technologies"},{"location":"swe/important-concepts/#http-caching","text":"https://medium.freecodecamp.org/http-caching-in-depth-part-1-a853c6af99db Reactive Manifesto \u21a9 Wikipedia article on Memoization \u21a9","title":"HTTP Caching"},{"location":"swe/microservices/","text":"Microservices \u00b6","title":"Microservices"},{"location":"swe/microservices/#microservices","text":"","title":"Microservices"},{"location":"swe/naming/","text":"On Naming \u00b6 There are only two hard things in Computer Science: cache invalidation and naming things. -- Phil Karlton There are 2 hard problems in computer science: cache invalidation, naming things, and off-by-1 errors. - ? Naming things is hard, but very important. Naming things correctly conveys meaning, helps us understand and reduces cognitive load . Talks \u00b6 Peter Hilton: How To Name Things: The Hardest Problem In Programming Jimmy Bogard: Domain Driven Design: The Good Parts Slides \u00b6 Peter Hilto: Naming Guidelines For Professional Programmers Books \u00b6 Domain Driven Design - Eric Evans Clean Code Collection - Robert C Martin Other \u00b6 Martin Fowler - Two Hard Things Martin Fowler - Ubiquitous Language Industry Specific Dictionaries Clean Coders Peter Hilton - Why Naming Things Is Hard","title":"Naming"},{"location":"swe/naming/#on-naming","text":"There are only two hard things in Computer Science: cache invalidation and naming things. -- Phil Karlton There are 2 hard problems in computer science: cache invalidation, naming things, and off-by-1 errors. - ? Naming things is hard, but very important. Naming things correctly conveys meaning, helps us understand and reduces cognitive load .","title":"On Naming"},{"location":"swe/naming/#talks","text":"Peter Hilton: How To Name Things: The Hardest Problem In Programming Jimmy Bogard: Domain Driven Design: The Good Parts","title":"Talks"},{"location":"swe/naming/#slides","text":"Peter Hilto: Naming Guidelines For Professional Programmers","title":"Slides"},{"location":"swe/naming/#books","text":"Domain Driven Design - Eric Evans Clean Code Collection - Robert C Martin","title":"Books"},{"location":"swe/naming/#other","text":"Martin Fowler - Two Hard Things Martin Fowler - Ubiquitous Language Industry Specific Dictionaries Clean Coders Peter Hilton - Why Naming Things Is Hard","title":"Other"},{"location":"swe/observability/","text":"Observability \u00b6 Resources \u00b6 https://www.vividcortex.com/blog/monitoring-isnt-observability https://medium.com/@copyconstruct/monitoring-and-observability-8417d1952e1c https://codeascraft.com/2011/02/15/measure-anything-measure-everything/","title":"Observability"},{"location":"swe/observability/#observability","text":"","title":"Observability"},{"location":"swe/observability/#resources","text":"https://www.vividcortex.com/blog/monitoring-isnt-observability https://medium.com/@copyconstruct/monitoring-and-observability-8417d1952e1c https://codeascraft.com/2011/02/15/measure-anything-measure-everything/","title":"Resources"},{"location":"swe/others/","text":"Software Engineering Concepts \u00b6 Resource management \u00b6 When there are finite resources in your system, manage them explicitly. Be that memory, CPU, amount of connections to a database or incoming http connections. Back Pressure \u00b6 Back pressure When one component is struggling to keep-up, the system as a whole needs to respond in a sensible way. It is unacceptable for the component under stress to fail catastrophically or to drop messages in an uncontrolled fashion. Since it can\u2019t cope and it can\u2019t fail it should communicate the fact that it is under stress to upstream components and so get them to reduce the load. This back-pressure is an important feedback mechanism that allows systems to gracefully respond to load rather than collapse under it. The back-pressure may cascade all the way up to the user, at which point responsiveness may degrade, but this mechanism will ensure that the system is resilient under load, and will provide information that may allow the system itself to apply other resources to help distribute the load, see Elasticity. 1 Further reading: DZone article Spotify Engineering Memoization \u00b6 Memoization In computing, memoization or memoisation is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again. Memoization has also been used in other contexts (and for purposes other than speed gains), such as in simple mutually recursive descent parsing. Important Theories \u00b6 Theory of constraints Law of demeter Conway's law Little's law Commoditization Amdahl's Law Reactive Manifesto \u21a9 Wikipedia article on Memoization \u21a9","title":"Software Engineering Concepts"},{"location":"swe/others/#software-engineering-concepts","text":"","title":"Software Engineering Concepts"},{"location":"swe/others/#resource-management","text":"When there are finite resources in your system, manage them explicitly. Be that memory, CPU, amount of connections to a database or incoming http connections.","title":"Resource management"},{"location":"swe/others/#back-pressure","text":"Back pressure When one component is struggling to keep-up, the system as a whole needs to respond in a sensible way. It is unacceptable for the component under stress to fail catastrophically or to drop messages in an uncontrolled fashion. Since it can\u2019t cope and it can\u2019t fail it should communicate the fact that it is under stress to upstream components and so get them to reduce the load. This back-pressure is an important feedback mechanism that allows systems to gracefully respond to load rather than collapse under it. The back-pressure may cascade all the way up to the user, at which point responsiveness may degrade, but this mechanism will ensure that the system is resilient under load, and will provide information that may allow the system itself to apply other resources to help distribute the load, see Elasticity. 1 Further reading: DZone article Spotify Engineering","title":"Back Pressure"},{"location":"swe/others/#memoization","text":"Memoization In computing, memoization or memoisation is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again. Memoization has also been used in other contexts (and for purposes other than speed gains), such as in simple mutually recursive descent parsing.","title":"Memoization"},{"location":"swe/others/#important-theories","text":"Theory of constraints Law of demeter Conway's law Little's law Commoditization Amdahl's Law Reactive Manifesto \u21a9 Wikipedia article on Memoization \u21a9","title":"Important Theories"}]}