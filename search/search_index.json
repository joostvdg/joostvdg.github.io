{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Joost van der Griendt's CI/CD Knowledge Docs! CI/CD stands for Continuous Integration Continuous Delivery . This is a collection of knowledge that I keep reusing at different customers. So I figured, lets just host it online for everyone to use. Continuous Integration A good definition can be found here: http://www.martinfowler.com/articles/continuousIntegration.html Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\" Continuous Delivery Continuous Delivery/deployment is the next step in getting yr software changes at the desired server in order to let your clients take a look at it. This article provides a good example of it: http://www.martinfowler.com/articles/continuousIntegration.html To do Continuous Integration you need multiple environments, one to run commit tests, one or more to run secondary tests. Since you are moving executables between these environments multiple times a day, you'll want to do this automatically. So it's important to have scripts that will allow you to deploy the application into any environment easily.","title":"Home"},{"location":"#joost-van-der-griendts-cicd-knowledge-docs","text":"CI/CD stands for Continuous Integration Continuous Delivery . This is a collection of knowledge that I keep reusing at different customers. So I figured, lets just host it online for everyone to use.","title":"Joost van der Griendt's CI/CD Knowledge Docs!"},{"location":"#continuous-integration","text":"A good definition can be found here: http://www.martinfowler.com/articles/continuousIntegration.html Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\"","title":"Continuous Integration"},{"location":"#continuous-delivery","text":"Continuous Delivery/deployment is the next step in getting yr software changes at the desired server in order to let your clients take a look at it. This article provides a good example of it: http://www.martinfowler.com/articles/continuousIntegration.html To do Continuous Integration you need multiple environments, one to run commit tests, one or more to run secondary tests. Since you are moving executables between these environments multiple times a day, you'll want to do this automatically. So it's important to have scripts that will allow you to deploy the application into any environment easily.","title":"Continuous Delivery"},{"location":"blogs/docker-graceful-shutdown/","text":"Gracefully Shutting Down Applications in Docker I'm not sure about you, but I prefer it when my neighbors leave our shared spaces clean and don't take up parking spaces when they don't need them. Imagine you live in an apartment complex with the above-mentioned parking lot. Some tenants go away and never come back. If nothing is done to clean up after them - to reclaim their apartment and parking space - then after some time, more and more apartments are unavailable for no reason, and the parking lot fills with cars which belong to no one. Some tenants did not get a parking lot and are getting frustrated that none are becoming available. When they moved in, they were told that when others leave, they would be next in line. While they're waiting, they have to park outside the complex. Eventually, the entrance gets blocked and no one can enter or leave. The end result is a completely unlivable apartment block with trapped tenants - never to be seen or heard from again. If you agree with me that when a tenant leaves, they should clean the apartment and free the parking spot to make it ready for the next inhabitant; then please read on. We're going to dive into the equivalent of doing this with containers. We will explore running our containers with Docker (run, compose, swarm) and Kubernetes. Even if you use another way to run your containers, this article should provide you with enough insight to get you on your way. The case for graceful shutdown We're in an age where many applications are running in Docker containers across a multitude of clusters. These applications are then confronted with new concerns to tackle such as more moving parts, networking between these parts, remote storage and others. One significant way we defend ourselves against the perils of this distributed nature is to make our applications more robust - able to survive errors. However, even then there is still no guarantee your application is always up and running. So another concern we should tackle is how it responds when it needs to shut down. Where we can differentiate between an unexpected shutdown - we crashed - or an expected shutdown. On top of that, failing instead of trying to recover when something bad happens also adheres to \"fail fast\" - as strongly advocated by Michael Nygard in ReleaseIt. Shutting down can happen for a variety of reasons, in this post we dive into how to deal with an expected shutdown such as it being told to stop by an orchestrator such as Kubernetes. Containers can be purposefully shut down for a variety of reasons, including but not limited too: your application's health check fails your application consumed more resources than allowed the application is scaling down Just as cleaning up when leaving makes you a better tenant, having your application clean up connections, resources Moreover, the more tenants behaving in a good way increases the quality of living for all tenants. In our case, it improves the reliability and consistency of our cluster. Graceful shutdown is not unique to Docker, as it has been part of Linux's best practices for quite some years before Docker's existence. However, applying them to Docker container adds extra dimensions. Start Good So You Can End Well When you sign up for an apartment, you probably have to sign a contract detailing your rights and obligations. The more you state explicitly, the easier it is to deal with bad behaving neighbors. The same is true when running processes; we should make sure that we set the rules, obligations, and expectations from the start. As we say in Dutch: a good beginning is half the work. We will start with how you can run a process in a container with a process that shuts down gracefully. There are many ways to start a process in a container. In this article, we look at processes started by commands defined in a Dockerfile. There are two ways to specify this: CMD : runs a command when the container gets started ENTRYPOINT : provides the location (entrypoint) from where commands get run when the container starts You need at least one ENTRYPOINT or CMD in a Dockerfile for it to be valid. They can be used in collaboration but they can do similar things. For more information on the details of these commands, read Docker's docs on Entrypoint vs. CMD . Docker Shell form example We start with the shell form and see if it can do what we want; begin in such a way, we can stop it nicely. Shell form means we define a shell command without any special format or keywords. Please create Dockerfile with the content that follows. 1 2 FROM ubuntu:18.04 ENTRYPOINT top -b Then build an image and run a container. 1 2 docker image build --tag shell-form . docker run --name shell-form --rm shell-form The above command yields the following output. 1 2 3 4 5 6 7 8 9 top - 16 :34:56 up 1 day, 5 :15, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 541984 free, 302668 used, 1202280 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1579380 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 4624 760 696 S 0 .0 0 .0 0 :00.05 sh 6 root 20 0 36480 2928 2580 R 0 .0 0 .1 0 :00.01 top As you can see, two processes are running, sh and top . Meaning, that killing the process, with ctrl+c for example, terminates the sh process, but not top . This happens because the sh process forked the top process, but the termination will only be send to PID 1 - in this case sh . As sh will not stop the top process for us it will continue running and leave the container alive. To kill this container, open a second terminal and execute the following command. 1 docker rm -f shell-form Shell form doesn't do what we need. Starting a process with shell form will only lead us to the disaster of parking lots filling up unless there's a someone actively cleaning up. Docker exec form example This leads us to the exec form. Hopefully, this gets us somewhere. The exec form is written as an array of parameters: ENTRYPOINT [ top , -b ] To continue in the same line of examples, we will create a Dockerfile, build and run it. 1 2 FROM ubuntu:18.04 ENTRYPOINT [ top , -b ] Then build and run it. 1 2 docker image build --tag exec-form . docker run --name exec-form --rm exec-form This yields the following output. 1 2 3 4 5 6 7 8 top - 18 :12:30 up 1 day, 6 :53, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 535896 free, 307196 used, 1203840 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1574880 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 36480 2940 2584 R 0 .0 0 .1 0 :00.03 top Now we got something we can work with. If something would tell this Container to stop, it will tell our only running process so it is sure to reach the correct one! Gotchas Knowing we can use the exec form for our goal - gracefully shutting down our container - we can move on to the next part of our efforts. For the sake of imparting you with some hard learned lessons, we will explore two gotchas. They're optional, so you can also choose to skip to Make Sure Your Process Listens . Docker exec form with parameters A caveat with the exec form is that it doesn't interpolate parameters. You can try the following: 1 2 3 FROM ubuntu:18.04 ENV PARAM= -b ENTRYPOINT [ top , ${PARAM} ] Then build and run it: 1 2 docker image build --tag exec-param . docker run --name exec-form --rm exec-param This should yield the following: 1 /bin/sh: 1 : [ top: not found This is where Docker created a mix between the two styles. It allows you to create an Entrypoint with a shell command - performing interpolation - but executing it as an exec form. This can be done by prefixing the shell form, with, you guessed it, exec . 1 2 3 FROM ubuntu:18.04 ENV PARAM= -b ENTRYPOINT exec top ${PARAM} Then build and run it: 1 2 docker image build --tag exec-param . docker run --name exec-form --rm exec-param This will return the exact same as if we would've run ENTRYPOINT [ top , -b ] . Now you can also override the param, by using the environment variable flag. 1 2 docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = help exec-param Resulting in top's help string. The special case of Alpine One of the main best practices for Dockerfiles , is to make them as small as possible. The easiest way to do this is to start with a minimal image. This is where Alpine Linux comes in. We will revisit out shell form example, but replace ubuntu with alpine. Create the following Dockerfile. 1 2 FROM alpine:3.8 ENTRYPOINT top -b Then build and run it. 1 2 docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = help exec-param This yields the following output. 1 2 3 4 5 Mem: 1509068K used, 537864K free, 640K shrd, 126756K buff, 1012436K cached CPU: 0 % usr 0 % sys 0 % nic 100 % idle 0 % io 0 % irq 0 % sirq Load average: 0 .00 0 .00 0 .00 2 /404 5 PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND 1 0 root R 1516 0 % 0 0 % top -b Aside from top 's output looking a bit different, there is only one command. Alpine Linux helps us avoid the problem of shell form altogether! Make Sure Your Process Listens It is excellent if your tenants are all signed up, know their rights and obligations. But you can't contact them when something happens, how will they ever know when to act? Translating that into our process. It starts and can be told to shut down, but does it process listen? Can it interpret the message it gets from Docker or Kubernetes? And if it does, can it relay the message correctly to its Child Processes? In order for your process to gracefully shutdown, it should know when to do so. As such, it should listen not only for itself but also on behalf of its children - yours never do anything wrong though! Some processes do, but many aren't designed to listen or tell their Children . They expect someone else to listen for them and tell them and their children - process managers. In order to listen to these signals , we can call in the help of others. We will look at two options. we let Docker manage the process and its children we use a process manager Let Docker manage it for us If you're not using Docker to run or manage your containers, you should skip to Depend on a process manager . Docker has a build in feature, that it uses a lightweight process manager to help you. So if you're running your images with Docker itself, either directly or via Compose or Swarm, you're fine. You can use the init flag in your run command or your compose file. Please, note that the below examples require a certain minimum version of Docker. run - 1.13+ compose (v 2.2) - 1.13.0+ swarm (v 3.7) - 18.06.0+ With Docker Run 1 docker run --rm -ti --init caladreas/dui With Docker Compose 1 2 3 4 5 version : 2.2 services : web : image : caladreas/java-docker-signal-demo:no-tini init : true With Docker Swarm 1 2 3 4 5 version : 3.7 services : web : image : caladreas/java-docker-signal-demo:no-tini init : true Relying on Docker does create a dependency on how your container runs. It only runs correctly in Docker-related technologies (run, compose, swarm) and only if the proper versions are available. Creating either a different experience for users running your application somewhere else or not able to meet the version requirements. So maybe another solution is to bake a process manager into your image and guarantee its behavior. Depend on a process manager One of our goals for Docker images is to keep them small. We should look for a lightweight process manager. It does not have too many a whole machine worth or processes, just one and perhaps some children. Here we would like to introduce you to Tini , a lightweight process manager designed for this purpose . It is a very successful and widely adopted process manager in the Docker world. So successful, that the before mentioned init flags from Docker are implemented by baking Tini into Docker . Debian example For brevity, the build process is excluded, and for image size, we use Debian slim instead of default Debian. 1 2 3 4 5 6 FROM debian:stable-slim ENV TINI_VERSION v0.18.0 ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui , -XX:+UseCGroupMemoryLimitForHeap , -XX:+UnlockExperimentalVMOptions ] COPY --from = build /usr/bin/dui-image/ /usr/bin/dui Alpine example Alpine Linux works wonders for Docker images, so to improve our lives, you can very easily install it if you want. 1 2 3 4 FROM alpine RUN apk add --no-cache tini ENTRYPOINT [ /sbin/tini , -vv , -g , -s , -- ] CMD [ top -b ] How To Be Told What You Want To Hear You've made it this far; your tenets are reachable so you can inform them if they need to act. However, there's another problem lurking around the corner. Do they speak your language? Our process now starts knowing it can be talked to, it has someone who takes care of listening for it and its children. Now we need to make sure it can understand what it hears, it should be able to handle the incoming signals. We have two main ways of doing this. Handle signals as they come : we should make sure our process deal with the signals as they come State the signals we want : we can also tell up front, which signals we want to hear and put the burden of translation on our callers For more details on the subject of Signals and Docker, please read this excellent blog from Grigorii Chudnov . Handle signals as they come Handling process signals depend on your application, programming language or framework. State the signals we want Sometimes your language or framework of choice, doesn't handle signals all that well. It might be very rigid in what it does with specific signals, removing your ability to do the right thing. Of course, not all languages or frameworks are designed with Docker container or Microservices in mind, are yet to catch up to this more dynamic environment. Luckily Docker and Kubernetes allow you to specify what signal too sent to your process. Docker run 1 2 docker run --rm -ti --init --stop-signal = SIGINT \\ caladreas/java-docker-signal-demo Docker compose/swarm Docker's compose file format allows you to specify a stop signal . This is the signal sent when the container is stopped in a normal fashion. Normal in this case, meaning docker stop or when docker itself determines it should stop the container. If you forcefully remove the container, for example with docker rm -f it will directly kill the process, so don't do that. 1 2 3 4 5 6 version : 2.2 services : web : image : caladreas/java-docker-signal-demo stop_signal : SIGINT stop_grace_period : 15s If you run this with docker-compose up and then in a second terminal, stop the container, you will see something like this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 web_1 | HelloWorld! web_1 | Shutdown hook called! web_1 | We re told to stop early... web_1 | java.lang.InterruptedException: sleep interrupted web_1 | at java.base/java.lang.Thread.sleep(Native Method) web_1 | at joostvdg.demo.signal@1.0/com.github.joostvdg.demo.signal.HelloWorld.printHelloWorld(Unknown Source) web_1 | at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) web_1 | at java.base/java.util.concurrent.FutureTask.run(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) web_1 | at java.base/java.lang.Thread.run(Unknown Source) web_1 | [DEBUG tini (1)] Passing signal: Interrupt web_1 | [DEBUG tini (1)] Received SIGCHLD web_1 | [DEBUG tini (1)] Reaped child with pid: 7 web_1 | [INFO tini (1)] Main child exited with signal (with signal Interrupt ) Kubernetes In Kubernetes we can make use of Container Lifecycle Hooks to manage how our container should be stopped. We could, for example, send a SIGINT (interrupt) to tell our application to stop. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : apps/v1 kind : Deployment metadata : name : java-signal-demo namespace : default labels : app : java-signal-demo spec : replicas : 1 template : metadata : labels : app : java-signal-demo spec : containers : - name : main image : caladreas/java-docker-signal-demo lifecycle : preStop : exec : command : [ killall , java , -INT ] terminationGracePeriodSeconds : 60 When you create this as deployment.yml, create and delete it - kubectl apply -f deployment.yml / kubectl delete -f deployment.yml - you will see the same behavior. How To Be Told When You Want To Hear It Our process now will now start knowing it will hear what it wants to hear. But we now have to make sure we hear it when we need to hear it. An intervention is excellent when you can still be saved, but it is a bit useless if you're already dead. Docker You can either configure your health check in your Dockerfile or configure it in your docker-compose.yml for either compose or swarm. Considering only Docker can use the health check in your Dockerfile, it is strongly recommended to have health checks in your application and document how they can be used. Kubernetes In Kubernetes we have the concept of Container Probes . This allows you to configure whether your container is ready (readinessProbe) to be used and if it is still working as expected (livenessProbe).","title":"Docker Graceful Shutdown"},{"location":"blogs/docker-graceful-shutdown/#gracefully-shutting-down-applications-in-docker","text":"I'm not sure about you, but I prefer it when my neighbors leave our shared spaces clean and don't take up parking spaces when they don't need them. Imagine you live in an apartment complex with the above-mentioned parking lot. Some tenants go away and never come back. If nothing is done to clean up after them - to reclaim their apartment and parking space - then after some time, more and more apartments are unavailable for no reason, and the parking lot fills with cars which belong to no one. Some tenants did not get a parking lot and are getting frustrated that none are becoming available. When they moved in, they were told that when others leave, they would be next in line. While they're waiting, they have to park outside the complex. Eventually, the entrance gets blocked and no one can enter or leave. The end result is a completely unlivable apartment block with trapped tenants - never to be seen or heard from again. If you agree with me that when a tenant leaves, they should clean the apartment and free the parking spot to make it ready for the next inhabitant; then please read on. We're going to dive into the equivalent of doing this with containers. We will explore running our containers with Docker (run, compose, swarm) and Kubernetes. Even if you use another way to run your containers, this article should provide you with enough insight to get you on your way.","title":"Gracefully Shutting Down Applications in Docker"},{"location":"blogs/docker-graceful-shutdown/#the-case-for-graceful-shutdown","text":"We're in an age where many applications are running in Docker containers across a multitude of clusters. These applications are then confronted with new concerns to tackle such as more moving parts, networking between these parts, remote storage and others. One significant way we defend ourselves against the perils of this distributed nature is to make our applications more robust - able to survive errors. However, even then there is still no guarantee your application is always up and running. So another concern we should tackle is how it responds when it needs to shut down. Where we can differentiate between an unexpected shutdown - we crashed - or an expected shutdown. On top of that, failing instead of trying to recover when something bad happens also adheres to \"fail fast\" - as strongly advocated by Michael Nygard in ReleaseIt. Shutting down can happen for a variety of reasons, in this post we dive into how to deal with an expected shutdown such as it being told to stop by an orchestrator such as Kubernetes. Containers can be purposefully shut down for a variety of reasons, including but not limited too: your application's health check fails your application consumed more resources than allowed the application is scaling down Just as cleaning up when leaving makes you a better tenant, having your application clean up connections, resources Moreover, the more tenants behaving in a good way increases the quality of living for all tenants. In our case, it improves the reliability and consistency of our cluster. Graceful shutdown is not unique to Docker, as it has been part of Linux's best practices for quite some years before Docker's existence. However, applying them to Docker container adds extra dimensions.","title":"The case for graceful shutdown"},{"location":"blogs/docker-graceful-shutdown/#start-good-so-you-can-end-well","text":"When you sign up for an apartment, you probably have to sign a contract detailing your rights and obligations. The more you state explicitly, the easier it is to deal with bad behaving neighbors. The same is true when running processes; we should make sure that we set the rules, obligations, and expectations from the start. As we say in Dutch: a good beginning is half the work. We will start with how you can run a process in a container with a process that shuts down gracefully. There are many ways to start a process in a container. In this article, we look at processes started by commands defined in a Dockerfile. There are two ways to specify this: CMD : runs a command when the container gets started ENTRYPOINT : provides the location (entrypoint) from where commands get run when the container starts You need at least one ENTRYPOINT or CMD in a Dockerfile for it to be valid. They can be used in collaboration but they can do similar things. For more information on the details of these commands, read Docker's docs on Entrypoint vs. CMD .","title":"Start Good So You Can End Well"},{"location":"blogs/docker-graceful-shutdown/#docker-shell-form-example","text":"We start with the shell form and see if it can do what we want; begin in such a way, we can stop it nicely. Shell form means we define a shell command without any special format or keywords. Please create Dockerfile with the content that follows. 1 2 FROM ubuntu:18.04 ENTRYPOINT top -b Then build an image and run a container. 1 2 docker image build --tag shell-form . docker run --name shell-form --rm shell-form The above command yields the following output. 1 2 3 4 5 6 7 8 9 top - 16 :34:56 up 1 day, 5 :15, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 541984 free, 302668 used, 1202280 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1579380 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 4624 760 696 S 0 .0 0 .0 0 :00.05 sh 6 root 20 0 36480 2928 2580 R 0 .0 0 .1 0 :00.01 top As you can see, two processes are running, sh and top . Meaning, that killing the process, with ctrl+c for example, terminates the sh process, but not top . This happens because the sh process forked the top process, but the termination will only be send to PID 1 - in this case sh . As sh will not stop the top process for us it will continue running and leave the container alive. To kill this container, open a second terminal and execute the following command. 1 docker rm -f shell-form Shell form doesn't do what we need. Starting a process with shell form will only lead us to the disaster of parking lots filling up unless there's a someone actively cleaning up.","title":"Docker Shell form example"},{"location":"blogs/docker-graceful-shutdown/#docker-exec-form-example","text":"This leads us to the exec form. Hopefully, this gets us somewhere. The exec form is written as an array of parameters: ENTRYPOINT [ top , -b ] To continue in the same line of examples, we will create a Dockerfile, build and run it. 1 2 FROM ubuntu:18.04 ENTRYPOINT [ top , -b ] Then build and run it. 1 2 docker image build --tag exec-form . docker run --name exec-form --rm exec-form This yields the following output. 1 2 3 4 5 6 7 8 top - 18 :12:30 up 1 day, 6 :53, 0 users, load average: 0 .00, 0 .00, 0 .00 Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .4 us, 0 .3 sy, 0 .0 ni, 99 .2 id, 0 .1 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046932 total, 535896 free, 307196 used, 1203840 buff/cache KiB Swap: 1048572 total, 1042292 free, 6280 used. 1574880 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 36480 2940 2584 R 0 .0 0 .1 0 :00.03 top Now we got something we can work with. If something would tell this Container to stop, it will tell our only running process so it is sure to reach the correct one!","title":"Docker exec form example"},{"location":"blogs/docker-graceful-shutdown/#gotchas","text":"Knowing we can use the exec form for our goal - gracefully shutting down our container - we can move on to the next part of our efforts. For the sake of imparting you with some hard learned lessons, we will explore two gotchas. They're optional, so you can also choose to skip to Make Sure Your Process Listens .","title":"Gotchas"},{"location":"blogs/docker-graceful-shutdown/#docker-exec-form-with-parameters","text":"A caveat with the exec form is that it doesn't interpolate parameters. You can try the following: 1 2 3 FROM ubuntu:18.04 ENV PARAM= -b ENTRYPOINT [ top , ${PARAM} ] Then build and run it: 1 2 docker image build --tag exec-param . docker run --name exec-form --rm exec-param This should yield the following: 1 /bin/sh: 1 : [ top: not found This is where Docker created a mix between the two styles. It allows you to create an Entrypoint with a shell command - performing interpolation - but executing it as an exec form. This can be done by prefixing the shell form, with, you guessed it, exec . 1 2 3 FROM ubuntu:18.04 ENV PARAM= -b ENTRYPOINT exec top ${PARAM} Then build and run it: 1 2 docker image build --tag exec-param . docker run --name exec-form --rm exec-param This will return the exact same as if we would've run ENTRYPOINT [ top , -b ] . Now you can also override the param, by using the environment variable flag. 1 2 docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = help exec-param Resulting in top's help string.","title":"Docker exec form with parameters"},{"location":"blogs/docker-graceful-shutdown/#the-special-case-of-alpine","text":"One of the main best practices for Dockerfiles , is to make them as small as possible. The easiest way to do this is to start with a minimal image. This is where Alpine Linux comes in. We will revisit out shell form example, but replace ubuntu with alpine. Create the following Dockerfile. 1 2 FROM alpine:3.8 ENTRYPOINT top -b Then build and run it. 1 2 docker image build --tag exec-param . docker run --name exec-form --rm -e PARAM = help exec-param This yields the following output. 1 2 3 4 5 Mem: 1509068K used, 537864K free, 640K shrd, 126756K buff, 1012436K cached CPU: 0 % usr 0 % sys 0 % nic 100 % idle 0 % io 0 % irq 0 % sirq Load average: 0 .00 0 .00 0 .00 2 /404 5 PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND 1 0 root R 1516 0 % 0 0 % top -b Aside from top 's output looking a bit different, there is only one command. Alpine Linux helps us avoid the problem of shell form altogether!","title":"The special case of Alpine"},{"location":"blogs/docker-graceful-shutdown/#make-sure-your-process-listens","text":"It is excellent if your tenants are all signed up, know their rights and obligations. But you can't contact them when something happens, how will they ever know when to act? Translating that into our process. It starts and can be told to shut down, but does it process listen? Can it interpret the message it gets from Docker or Kubernetes? And if it does, can it relay the message correctly to its Child Processes? In order for your process to gracefully shutdown, it should know when to do so. As such, it should listen not only for itself but also on behalf of its children - yours never do anything wrong though! Some processes do, but many aren't designed to listen or tell their Children . They expect someone else to listen for them and tell them and their children - process managers. In order to listen to these signals , we can call in the help of others. We will look at two options. we let Docker manage the process and its children we use a process manager","title":"Make Sure Your Process Listens"},{"location":"blogs/docker-graceful-shutdown/#let-docker-manage-it-for-us","text":"If you're not using Docker to run or manage your containers, you should skip to Depend on a process manager . Docker has a build in feature, that it uses a lightweight process manager to help you. So if you're running your images with Docker itself, either directly or via Compose or Swarm, you're fine. You can use the init flag in your run command or your compose file. Please, note that the below examples require a certain minimum version of Docker. run - 1.13+ compose (v 2.2) - 1.13.0+ swarm (v 3.7) - 18.06.0+","title":"Let Docker manage it for us"},{"location":"blogs/docker-graceful-shutdown/#with-docker-run","text":"1 docker run --rm -ti --init caladreas/dui","title":"With Docker Run"},{"location":"blogs/docker-graceful-shutdown/#with-docker-compose","text":"1 2 3 4 5 version : 2.2 services : web : image : caladreas/java-docker-signal-demo:no-tini init : true","title":"With Docker Compose"},{"location":"blogs/docker-graceful-shutdown/#with-docker-swarm","text":"1 2 3 4 5 version : 3.7 services : web : image : caladreas/java-docker-signal-demo:no-tini init : true Relying on Docker does create a dependency on how your container runs. It only runs correctly in Docker-related technologies (run, compose, swarm) and only if the proper versions are available. Creating either a different experience for users running your application somewhere else or not able to meet the version requirements. So maybe another solution is to bake a process manager into your image and guarantee its behavior.","title":"With Docker Swarm"},{"location":"blogs/docker-graceful-shutdown/#depend-on-a-process-manager","text":"One of our goals for Docker images is to keep them small. We should look for a lightweight process manager. It does not have too many a whole machine worth or processes, just one and perhaps some children. Here we would like to introduce you to Tini , a lightweight process manager designed for this purpose . It is a very successful and widely adopted process manager in the Docker world. So successful, that the before mentioned init flags from Docker are implemented by baking Tini into Docker .","title":"Depend on a process manager"},{"location":"blogs/docker-graceful-shutdown/#debian-example","text":"For brevity, the build process is excluded, and for image size, we use Debian slim instead of default Debian. 1 2 3 4 5 6 FROM debian:stable-slim ENV TINI_VERSION v0.18.0 ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui , -XX:+UseCGroupMemoryLimitForHeap , -XX:+UnlockExperimentalVMOptions ] COPY --from = build /usr/bin/dui-image/ /usr/bin/dui","title":"Debian example"},{"location":"blogs/docker-graceful-shutdown/#alpine-example","text":"Alpine Linux works wonders for Docker images, so to improve our lives, you can very easily install it if you want. 1 2 3 4 FROM alpine RUN apk add --no-cache tini ENTRYPOINT [ /sbin/tini , -vv , -g , -s , -- ] CMD [ top -b ]","title":"Alpine example"},{"location":"blogs/docker-graceful-shutdown/#how-to-be-told-what-you-want-to-hear","text":"You've made it this far; your tenets are reachable so you can inform them if they need to act. However, there's another problem lurking around the corner. Do they speak your language? Our process now starts knowing it can be talked to, it has someone who takes care of listening for it and its children. Now we need to make sure it can understand what it hears, it should be able to handle the incoming signals. We have two main ways of doing this. Handle signals as they come : we should make sure our process deal with the signals as they come State the signals we want : we can also tell up front, which signals we want to hear and put the burden of translation on our callers For more details on the subject of Signals and Docker, please read this excellent blog from Grigorii Chudnov .","title":"How To Be Told What You Want To Hear"},{"location":"blogs/docker-graceful-shutdown/#handle-signals-as-they-come","text":"Handling process signals depend on your application, programming language or framework.","title":"Handle signals as they come"},{"location":"blogs/docker-graceful-shutdown/#state-the-signals-we-want","text":"Sometimes your language or framework of choice, doesn't handle signals all that well. It might be very rigid in what it does with specific signals, removing your ability to do the right thing. Of course, not all languages or frameworks are designed with Docker container or Microservices in mind, are yet to catch up to this more dynamic environment. Luckily Docker and Kubernetes allow you to specify what signal too sent to your process.","title":"State the signals we want"},{"location":"blogs/docker-graceful-shutdown/#docker-run","text":"1 2 docker run --rm -ti --init --stop-signal = SIGINT \\ caladreas/java-docker-signal-demo","title":"Docker run"},{"location":"blogs/docker-graceful-shutdown/#docker-composeswarm","text":"Docker's compose file format allows you to specify a stop signal . This is the signal sent when the container is stopped in a normal fashion. Normal in this case, meaning docker stop or when docker itself determines it should stop the container. If you forcefully remove the container, for example with docker rm -f it will directly kill the process, so don't do that. 1 2 3 4 5 6 version : 2.2 services : web : image : caladreas/java-docker-signal-demo stop_signal : SIGINT stop_grace_period : 15s If you run this with docker-compose up and then in a second terminal, stop the container, you will see something like this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 web_1 | HelloWorld! web_1 | Shutdown hook called! web_1 | We re told to stop early... web_1 | java.lang.InterruptedException: sleep interrupted web_1 | at java.base/java.lang.Thread.sleep(Native Method) web_1 | at joostvdg.demo.signal@1.0/com.github.joostvdg.demo.signal.HelloWorld.printHelloWorld(Unknown Source) web_1 | at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) web_1 | at java.base/java.util.concurrent.FutureTask.run(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) web_1 | at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) web_1 | at java.base/java.lang.Thread.run(Unknown Source) web_1 | [DEBUG tini (1)] Passing signal: Interrupt web_1 | [DEBUG tini (1)] Received SIGCHLD web_1 | [DEBUG tini (1)] Reaped child with pid: 7 web_1 | [INFO tini (1)] Main child exited with signal (with signal Interrupt )","title":"Docker compose/swarm"},{"location":"blogs/docker-graceful-shutdown/#kubernetes","text":"In Kubernetes we can make use of Container Lifecycle Hooks to manage how our container should be stopped. We could, for example, send a SIGINT (interrupt) to tell our application to stop. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : apps/v1 kind : Deployment metadata : name : java-signal-demo namespace : default labels : app : java-signal-demo spec : replicas : 1 template : metadata : labels : app : java-signal-demo spec : containers : - name : main image : caladreas/java-docker-signal-demo lifecycle : preStop : exec : command : [ killall , java , -INT ] terminationGracePeriodSeconds : 60 When you create this as deployment.yml, create and delete it - kubectl apply -f deployment.yml / kubectl delete -f deployment.yml - you will see the same behavior.","title":"Kubernetes"},{"location":"blogs/docker-graceful-shutdown/#how-to-be-told-when-you-want-to-hear-it","text":"Our process now will now start knowing it will hear what it wants to hear. But we now have to make sure we hear it when we need to hear it. An intervention is excellent when you can still be saved, but it is a bit useless if you're already dead.","title":"How To Be Told When You Want To Hear It"},{"location":"blogs/docker-graceful-shutdown/#docker","text":"You can either configure your health check in your Dockerfile or configure it in your docker-compose.yml for either compose or swarm. Considering only Docker can use the health check in your Dockerfile, it is strongly recommended to have health checks in your application and document how they can be used.","title":"Docker"},{"location":"blogs/docker-graceful-shutdown/#kubernetes_1","text":"In Kubernetes we have the concept of Container Probes . This allows you to configure whether your container is ready (readinessProbe) to be used and if it is still working as expected (livenessProbe).","title":"Kubernetes"},{"location":"blogs/dockercon-eu-2018/","text":"DockerCon EU 2018 - Recap Generally outline From my perspective, there were a few red threads throughout the conference. Security begins at the Developer A shift left of security, bringing the responsibility of knowing your dependencies and their quality firmly to the developer. Most of this tooling is still aimed at enterprises though, being part of paid solutions mostly. At least that which was shown at the conference. docker-assemble, that can build in an image from a Maven pom.xml and will include meta-data of all your dependencies (transitive included) JFrog X-Ray Docker EE tooling such as Docker Trusted Registry (DTR) Broader Automation More focus on the entire lifecycle of a system and not just an application. It seems people are starting to understand that doing CI/CD and Infrastructure As Code is not a single event for a single application. There is likely to be a few applications belonging together making a whole system which will land on more than one type of infrastructure and possibly more types of clusters. What we see is tools looking at either a broader scope, a higher level abstraction or more developer focussed (more love for the Dev in DevOps) to allow for easier integration with multiple platforms. For example, Pulumi will enable you to create any type of infrastructure - like Hashicorp's Terraform - but then in programming languages, you're used to (TypeScript, Python, Go). Pulumi Docker App CNAB Build-Kit Containerization Influences Everything Containerization has left deep and easy to spot imprints in our industry from startups building entirely on top of containers to programming languages changing their ways to stay relevant. There are new monitoring kings in the world, DataDog, Sysdig, Honeycomb.io and so on. They live and breathe containers and are not afraid of being thrown around different public clouds, networks and what not. In contrast to traditional monitoring tools, which are often bolting on container support and struggle with the dynamic nature of containerized clusters. Another extraordinary influence is that on the Java language. Declared dead a million times over and deemed obsolete in the container era due to its massive footprint in image size and runtime size. Both are being addressed, and we see a lot of work done on reducing footprint and static linking (JLink, Graal). The most significant influence might be on the software behemoth that has rejuvenated itself. Microsoft has sworn allegiance to open source, Linux and containers. Windows 2019 server can run container workloads natively and work as nodes alongside a Docker EE cluster - which can include Kubernetes workloads. The next step would be support for Kubernetes integration, and as in the case of Java, smaller container footprint. Java Docker Windows Container Windows Server Support Observability tools Kubernetes offerings everywhere... Docker Build with Build-Kit Instead of investing in improving docker image building via the Docker Client, Docker created a new API and client library. This library called BuildKit, is completely independent. With Docker 18.09, it is included in the Docker Client allowing anyone to use it as easily as the traditional docker image build . BuildKit is already used by some other tools, such as Buildah and IMG, and allows you to create custom DSL \"Frontends\". As long as the API of BuikdKit is adhered to, the resulting image will be OCI compliant. So further remarks below and how to use it. BuildKit In-Depth session Supercharged Docker Build with BuildKit Usable from Docker 18.09 HighLights: allows custom DSL for specifying image (BuildKit) to still be used with Docker client/daemon build cache for your own files during build, think Go, Maven, Gradle... much more optimized, builds less, quicker, with more cache in less time support mounts (cache) such as secrets, during build phase 1 2 3 # Set env variable to enable # Or configure docker s json config export DOCKER_BUILDKIT = 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # syntax=docker/dockerfile:experimental ####################################### ## 1. BUILD JAR WITH MAVEN FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src ! RUN --mount = type = cache,target = /root/.m2/ mvn clean package -e ####################################### ## 2. BUILD NATIVE IMAGE WITH GRAAL FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from = BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath ####################################### ## 3. BUILD DOCKER RUNTIME IMAGE FROM alpine:3.8 CMD [ jpc-graal ] COPY --from = NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal ####################################### Secure your Kubernetes https://www.openpolicyagent.org + admission controller Network Policies Service Accounts Broader Cloud Automation Two of the broader cloud automation initiatives that impressed me at DockerCon were Pulumi and CNAB. Where Pulumi is an attempt to provide a more developer-friendly alternative to Terraform, CNAB is an attempt to create an environment agnostic installer specification. Meaning, you could create a CNAB installer which uses Pulumi to install all required infrastructure, applications and other resources. CNAB: cloud native application bundle Bundle.json invocation image (oci) = installer https://cnab.io docker app implements it helm support https://github.com/deislabs Install an implementation There are currently two implementations - that I found. Duffle from DeisLabs - open source from Azure - and Docker App - From Docker Inc.. Duffle create a new repo clone the repo init a duffle bundle copy duffle bundle data to our repo folder 1 2 3 4 git clone git@github.com:demomon/cnab-duffle-demo-1.git duffle create cnab-duffle-demo-2 mv cnab-duffle-demo-2/cnab cnab-duffle-demo-1/ mv cnab-duffle-demo-2/duffle.json cnab-duffle-demo-1/ edit our install file ( cnab/run ) build our duffle bundle ( duffle build . ) We can now inspect our bundle with duffle. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 duffle show cnab-duffle-demo-1:0.1.0 -r -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA256 { name : cnab-duffle-demo-1 , version : 0.1.0 , description : A short description of your bundle , keywords : [ cnab-duffle-demo-1 , cnab , demo , joostvdg ] , maintainers : [ { name : Joost van der Griendt , email : joostvdg@gmail.com , url : https://joostvdg.github.io } ] , invocationImages : [ { imageType : docker , image : deislabs/cnab-duffle-demo-1-cnab:2965aad7406e1b651a98fffe1194fcaaec5e623c } ] , images : null, parameters : null, credentials : null } -----BEGIN PGP SIGNATURE----- wsDcBAEBCAAQBQJcL3L8CRDgq4YEOipZ/QAAjDYMAJI5o66SteUP2o4HsbVk+Viw 3Fd874vSVmpPKcmN3tRCEDWGdMdvqQiirDpa//ghx4y5oFTahK2ihQ35GbJLlq8S v9/CK6CKGJtp5g38s2LZrKIvESzEF2lTXwHB03PG8PJ37iWiYkHkxvMpyzded3Rs 4d+VgUnip0Cre7DemaUaz5+fTQjs88WNTIhqPg47YvgqFXV0s1y7yN3RTLr3ohQ1 9mkw87nWfOD+ULpbCUaq9FhNZ+v4dK5IZcWlkyv+yrtguyBBiA3MC54ueVBAdFCl 2OhxXgZjbBHPfQPV1mPqCQudOsWjK/+gqyNb6KTzKrAnyrumVQli/C/8BVk/SRC/ GS2o4EdTS2lfREc2Gl0/VTmMkqzFZZhWd7pwt/iMjl0bICFehSU0N6OqN1d+v6Sq vWIZ5ppxt1NnCzp05Y+NRfVZOxBc2xjYTquFwIa/+qGPrmXBKamw/irjmCOndvx+ l1tf/g0UVSQI2R2/19svl7dlMkYpDdlth1YGgZi/Hg == = 8Xwi -----END PGP SIGNATURE----- Demo I've created a demo on GitHub: github.com/demomon/cnab-duffle-demo-1 Its goal is to install CloudBees Core and its prerequisites in a (GKE) Kubernetes cluster. It wraps a Go (lang) binary that will execute the commands, for which you can find the source code on GitHub . Components A CNAB bundle has some components by default, for this demo we needed the following: duffle.json : Duffle configuration file Dockerfile : the CNAB installer runtime run (script): the installer script kgi (binary): the binary executable from my k8s-gitops-installer code, that we will leverage for the installation Dockerfile The installer tool ( kgi ) requires Helm and Kubectl , so we a Docker image that has those. As we might end up packaging the entire image as part of the full CNAB package, it should also be based on Alpine (or similar minimal Linux). There seems to be one very well maintained and widely used (according to GitHub and Dockerhub stats): dtzar/helm-kubect . So no need to roll our own. 1 2 3 4 5 6 7 FROM dtzar/helm-kubectl:2.12.1 COPY Dockerfile /cnab/Dockerfile COPY app /cnab/app COPY kgi /usr/bin RUN ls -lath /cnab/app/ RUN kgi --help CMD [ /cnab/app/run ] duffle.json The only thing we need to add beyond the auto-generated file, is the credentials section. 1 2 3 4 5 credentials : { kubeconfig : { path : /cnab/app/kube-config } } kgi I pre-build a binary suitable for Linux that works in Alpine and included it in the CNAB folder. run script First thing we need to make sure, is to configure the Kubeconfig location. 1 export KUBECONFIG = /cnab/app/kube-config This should match what we defined in the duffle.json configuration - as you might expect - to make sure it gets bound in the right location. The kubectl command now knows which file to use. For the rest, we can do what we want, but convention tells us we need at least support status , install and uninstall . I'm lazy and only implemented install at the moment. In the install action, we will use the kgi executable to install CloudBees Core and it's pre-requisites. 1 2 3 4 5 6 7 8 action = $CNAB_ACTION case $action in install ) echo [Install] kgi validate kubectl ;; esac For the rest, I recommend you look at the sources. Run the demo First, we have to build the bundle. 1 duffle build . Once the build succeeded, we can create a credentials configuration. This will be a separate configuration file managed by Duffle. This configuration config must then be used with any installation that requires it - which makes it reusable as well. We have to populate it with the credential. In this case a path to a kube-config file. If you do not have one that you can export - e.g. based on GCloud - you can create a new user/certificate with a script. This is taken from gravitational , which were nice enough to create a script for doing so. You can find the script on GitHub (get-kubeconfig.sh) . Once you have that, store the end result at a decent place and configure it as your credential. 1 duffle creds generate demo1 cnab-duffle-demo-1 With the above command we can create a credential config object based on the build bundle cnab-duffle-demo-1 . The credential object will be demo-1 , which we can now use for installing. 1 duffle install demo1 cnab-duffle-demo-1:1.0.0 -c demo1 Further reading Howto guide on creating a Duffle Bundle Howto on handling credentials Wordpress with Kubernetes and AWS demo by Bitnami Example bundles Pulumi Just pointing to the documentation and GitHub repository is a bit boring and most of all, lazy. So I've gone through the trouble to create a demo project that shows what Pulumi can do. One of the things to keep in mind is that Pulumi keeps its state in a remote location by default - Pulumi's SaaS service. Much like using an S3 bucket to store your Terraform state. The goal of the demo is to show what you can do with Pulumi. My personal opinion is that Pulumi is excellent for managing entire systems as code. From public cloud infrastructure to public open source resources right down to your applications and their configurations. The system that I've written with Pulumi is a CI/CD system running in a GKE cluster. The system contains Jenkins, Artifactory and a custom LDAP server. To make it interesting, we'll use different aspects of Pulumi to create the system. We will create the GKE cluster via Pulumi's wrapper for the gcloud cli - in a similar way as JenkinsX does -, install Jenkins and Artifactory via Helm and install the LDAP server via Pulumi's Kubernetes resource classes. Steps taken For more info Pulumi.io install: brew install pulumi clone demo: git clone https://github.com/demomon/pulumi-demo-1 init stack: pulumi stack init demomon-pulumi-demo-1 connect to GitHub set kubernetes config pulumi config set kubernetes:context gke_ps-dev-201405_europe-west4_joostvdg-reg-dec18-1 pulumi config set isMinikube false install npm resources: npm install (I've used the TypeScript demo's) pulumi config set username administrator pulumi config set password 3OvlgaockdnTsYRU5JAcgM1o --secret pulumi preview incase pulumi loses your stack: pulumi stack select demomon-pulumi-demo-1 pulumi destroy Based on the following demo's: https://pulumi.io/quickstart/kubernetes/tutorial-exposed-deployment.html https://github.com/pulumi/examples/tree/master/kubernetes-ts-jenkins Artifactory via Helm To install the helm charts of Artifactory using Helm, we will first need to add JFrog's repository. helm repo add jfrog https://charts.jfrog.io helm repo update GKE Cluster Below is the code for the cluster. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import * as gcp from @pulumi/gcp ; import * as k8s from @pulumi/kubernetes ; import * as pulumi from @pulumi/pulumi ; import { nodeCount , nodeMachineType , password , username } from ./gke-config ; export const k8sCluster = new gcp . container . Cluster ( gke-cluster , { name : joostvdg-dec-2018-pulumi , initialNodeCount : nodeCount , nodeVersion : latest , minMasterVersion : latest , nodeConfig : { machineType : nodeMachineType , oauthScopes : [ https://www.googleapis.com/auth/compute , https://www.googleapis.com/auth/devstorage.read_only , https://www.googleapis.com/auth/logging.write , https://www.googleapis.com/auth/monitoring ], }, }); GKE Config As you could see, we import variables from a configuration file gke-config . 1 2 3 4 5 6 7 8 import { Config } from @pulumi/pulumi ; const config = new Config (); export const nodeCount = config . getNumber ( nodeCount ) || 3 ; export const nodeMachineType = config . get ( nodeMachineType ) || n1-standard-2 ; // username is the admin username for the cluster. export const username = config . get ( username ) || admin ; // password is the password for the admin user in the cluster. export const password = config . require ( password ); Kubeconfig As you probably will want to install other things (Helm charts, services) inside this cluster, we need to make sure we get the kubeconfig file of our cluster - which is yet to be created. Below is the code - courtesy of Pulumi's GKE Example - that generates and exports the Kubernetes Client Configuration. This client configuration can then be used by Helm charts or other Kubernetes services. Pulumi will then also understand it depends on the cluster and create/update it first before moving on to the others. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // Manufacture a GKE-style Kubeconfig. Note that this is slightly different because of the way GKE requires // gcloud to be in the picture for cluster authentication (rather than using the client cert/key directly). export const k8sConfig = pulumi . all ([ k8sCluster . name , k8sCluster . endpoint , k8sCluster . masterAuth ]). apply (([ name , endpoint , auth ]) = { const context = ` ${ gcp . config . project } _ ${ gcp . config . zone } _ ${ name } ` ; return `apiVersion: v1 clusters: - cluster: certificate-authority-data: ${ auth . clusterCaCertificate } server: https:// ${ endpoint } name: ${ context } contexts: - context: cluster: ${ context } user: ${ context } name: ${ context } current-context: ${ context } kind: Config preferences: {} users: - name: ${ context } user: auth-provider: config: cmd-args: config config-helper --format=json cmd-path: gcloud expiry-key: {.credential.token_expiry} token-key: {.credential.access_token} name: gcp ` ; }); // Export a Kubernetes provider instance that uses our cluster from above. export const k8sProvider = new k8s . Provider ( gkeK8s , { kubeconfig : k8sConfig , }); Pulumi GCP Config https://github.com/pulumi/examples/blob/master/gcp-ts-gke/README.md 1 2 3 4 export GCP_PROJECT = ... export GCP_ZONE = europe-west4-a export CLUSTER_PASSWORD = ... export GCP_SA_NAME = ... Make sure you have a Google SA (Service Account) by that name first, as you can read here . For me it worked best to NOT set any environment variables mentioned. They invairably caused authentication or authorization issues. Just make sure the SA account and it's credential file (see below) are authorized and the gcloud cli works. 1 2 3 4 gcloud iam service-accounts keys create gcp-credentials.json \\ --iam-account ${ GCP_SA_NAME } @ ${ GCP_PROJECT } .iam.gserviceaccount.com gcloud auth activate-service-account --key-file gcp-credentials.json gcloud auth application-default login 1 2 3 pulumi config set gcp:project ${ GCP_PROJECT } pulumi config set gcp:zone ${ GCP_ZONE } pulumi config set password --secret ${ CLUSTER_PASSWORD } Post Cluster Creation 1 2 gcloud container clusters get-credentials joostvdg-dec-2018-pulumi kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $( gcloud config get-value account ) Install failed Failed to install kubernetes:rbac.authorization.k8s.io:Role artifactory-artifactory . Probably due to missing rights, so probably have to execute the admin binding before the helm charts. 1 error: Plan apply failed: roles.rbac.authorization.k8s.io artifactory-artifactory is forbidden: attempt to grant extra privileges: ... Helm Charts Using Pulumi to install a Helm Chart feels a bit like adding layers of wrapping upon wrapping. The power of Pulumi becomes visible when using more than one related service on the same cluster - for example a SDLC Tool Chain. This example application installs two helm charts, Jenkins and Artifactory, on a GKE cluster that is also created and managed by Pulumi. Below is an example of installing a Helm chart of Jenkins, where we provide the Kubernetes config from the GKE cluster as Provider. This way, Pulumi knows it must install the helm chart in that GKE cluster and not in the current Kubeconfig. 1 2 3 4 5 6 7 8 9 10 import { k8sProvider , k8sConfig } from ./gke-cluster ; const jenkins = new k8s . helm . v2 . Chart ( jenkins , { repo : stable , version : 0.25.1 , chart : jenkins , }, { providers : { kubernetes : k8sProvider } } ); Deployment Service First, make sure you have an interface for the configuration arguments. 1 2 3 4 5 export interface LdapArgs { readonly name : string , readonly imageName : string , readonly imageTag : string } Then, create a exportable Pulumi resource class that can be reused. 1 2 3 4 5 6 export class LdapInstallation extends pulumi . ComponentResource { public readonly deployment : k8s.apps.v1.Deployment ; public readonly service : k8s.core.v1.Service ; // constructor } Inside the constructor placehold we will create a constructor method. It will do all the configuration we need to do for this resource, in this case a Kubernetes Service and Deployment. 1 2 3 4 5 constructor ( args : LdapArgs ) { super ( k8stypes:service:LdapInstallation , args . name , {}); const labels = { app : args.name }; const name = args . name } First Kubernetes resource to create is a container specification for the Deployment. 1 2 3 4 5 6 7 8 9 10 11 12 const container : k8stypes.core.v1.Container = { name , image : args.imageName + : + args . imageTag , resources : { requests : { cpu : 100m , memory : 200Mi }, limits : { cpu : 100m , memory : 200Mi }, }, ports : [{ name : ldap , containerPort : 1389 , }, ] }; As the configuration arguments can be any TypeScript type, you can allow people to override entire segments (such as Resources). Which you would do as follows: 1 2 3 4 resources : args.resources || { requests : { cpu : 100m , memory : 200Mi }, limits : { cpu : 100m , memory : 200Mi }, }, The Deployment and Service construction are quite similar. 1 2 3 4 5 6 7 8 9 10 this . deployment = new k8s . apps . v1 . Deployment ( args . name , { spec : { selector : { matchLabels : labels }, replicas : 1 , template : { metadata : { labels : labels }, spec : { containers : [ container ] }, }, }, },{ provider : cluster.k8sProvider }); 1 2 3 4 5 6 7 8 9 10 11 12 13 this . service = new k8s . core . v1 . Service ( args . name , { metadata : { labels : this.deployment.metadata.apply ( meta = meta . labels ), }, spec : { ports : [{ name : ldap , port : 389 , targetPort : ldap , protocol : TCP }, ], selector : this.deployment.spec.apply ( spec = spec . template . metadata . labels ), type : ClusterIP , }, }, { provider : cluster.k8sProvider }); JFrog Jenkins Challenge Visited the stand of JFrog where they had stories about two main products: Artifactory and X-Ray. For both there is a Challenge, an X-Ray Challenge and a Jenkins Artifactory Challenge . Jenkins Challenge The instructions for the Challenge were simply, follow what is stated in their GitHub repository and email a screenshot of the result. The instruction were as follows: Get an Artifactory instance (you can start a free trial on prem or in the cloud) Install Jenkins Install Artifactory Jenkins Plugin Add Artifactory credentials to Jenkins Credentials Create a new pipeline job Use the Artifactory Plugin DSL documentation to complete the following script: With a Scripted Pipeline as starting point: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 node { def rtServer def rtGradle def buildInfo stage ( Preparation ) { git https://github.com/jbaruch/gradle-example.git // create a new Artifactory server using the credentials defined in Jenkins // create a new Gradle build // set the resolver to the Gradle build to resolve from Artifactory // set the deployer to the Gradle build to deploy to Artifactory // declare that your gradle script does not use Artifactory plugin // declare that your gradle script uses Gradle wrapper } stage ( Build ) { //run the artifactoryPublish gradle task and collect the build info } stage ( Publish Build Info ) { //collect the environment variables to build info //publish the build info } } I don't like scripted, so I opted for Declarative with Jenkins in Kubernetes with the Jenkins Kubernetes plugin. Steps I took: get a trial license from the JFrog website install Artifactory and copy in the license when prompted change admin password create local maven repo 'libs-snapshot-local' create remote maven repo 'jcenter' (default remote value is jcenter, so only have to set the name) install Jenkins Artifactory plugin Kubernetes plugin add Artifactory username/password as credential in Jenkins create a gradle application (Spring boot via start.spring.io) which you can find here create a Jenkinsfile Installing Artifactory I installed Artifactory via Helm. JFrog has their own Helm repository - of course, would weird otherwise tbh - and you have to add that first. 1 2 helm repo add jfrog https://charts.jfrog.io helm install --name artifactory stable/artifactory Jenkinsfile This uses the Gradle wrapper - as per instructions in the challenge. So we can use the standard JNLP container, which is default, so agent any will do. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 pipeline { agent any environment { rtServer = rtGradle = buildInfo = artifactoryServerAddress = http://..../artifactory } stages { stage ( Test Container ) { steps { container ( gradle ) { sh which gradle sh uname -a sh gradle -version } } } stage ( Checkout ){ steps { git https://github.com/demomon/gradle-jenkins-challenge.git } } stage ( Preparation ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: artifactoryServerAddress , credentialsId: art-admin // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: jcenter , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: libs-snapshot-local , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( Build ) { steps { script { //run the artifactoryPublish gradle task and collect the build info buildInfo = rtGradle . run buildFile: build.gradle , tasks: clean build artifactoryPublish } } } stage ( Publish Build Info ) { steps { script { //collect the environment variables to build info buildInfo . env . capture = true //publish the build info rtServer . publishBuildInfo buildInfo } } } } } Jenkinsfile without Gradle Wrapper I'd rather not install the Gradle tool if I can just use a pre-build container with it. Unfortunately, to use it correctly with the Artifactory plugin and a Jenkins Kubernetes plugin, we need to do two things. create a Gradle Tool in the Jenkins master because the Artifactory plugin expects a Jenkins Tool object, not a location Manage Jenkins - Global Tool Configuration - Gradle - Add As value supply /usr , the Artifactory build will add /gradle/bin to it automatically set the user of build Pod to id 1000 explicitly else the build will not be allowed to touch files in /home/jenkins/workspace 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 pipeline { agent { kubernetes { label mypod yaml apiVersion: v1 kind: Pod spec: securityContext: runAsUser: 1000 fsGroup: 1000 containers: - name: gradle image: gradle:4.10-jdk-alpine command: [ cat ] tty: true } } environment { rtServer = rtGradle = buildInfo = CONTAINER_GRADLE_TOOL = /usr/bin/gradle } stages { stage ( Test Container ) { steps { container ( gradle ) { sh which gradle sh uname -a sh gradle -version } } } stage ( Checkout ){ steps { // git https://github.com/demomon/gradle-jenkins-challenge.git checkout scm } } stage ( Preparation ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: http://35.204.238.14/artifactory , credentialsId: art-admin // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: jcenter , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: libs-snapshot-local , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( Build ) { //run the artifactoryPublish gradle task and collect the build info steps { script { buildInfo = rtGradle . run buildFile: build.gradle , tasks: clean build artifactoryPublish } } } stage ( Publish Build Info ) { //collect the environment variables to build info //publish the build info steps { script { buildInfo . env . capture = true rtServer . publishBuildInfo buildInfo } } } } } Docker security standards security takes place in every layer/lifecycle phase for scaling, security needs to be part of developer's day-to-day as everything is code, anything part of the sdlc should be secure and auditable use an admission controller network policies automate your security processes expand your security automation by adding learnings Docker Java CICD telepresence Distroless (google mini os) OpenJ9 Portala (for jdk 12) wagoodman/dive use jre for the runtime instead of jdk buildkit can use mounttarget for local caches add labels with Metadata (depency trees) grafeas kritis FindSecBugs org.owasp:dependency-check-maven arminc/clair-scanner jlink = in limbo Docker Windows specific base images for different use cases Docker capabilities heavily depend on Windows Server version Other Docker pipeline Dind + privileged mount socket windows linux Windows build agent provisioning with docker EE Jenkins Docker swarm update_config Idea: build a dynamic ci/cd platform with kubernetes jenkins evergreen + jcasc kubernetes plugin gitops pipeline AKS + virtual kubelet + ACI Jenkins + EC2 Pluging + ECS/Fargate jenkins agent as ecs task (fargate agent) docker on windows, only on ECS Apply Diplomacy to Code Review apply diplomacy to code review always positive remove human resistantance with inclusive language improvement focused persist, kindly Citizens Bank journey started with swarm, grew towards kubernetes (ucp) elk stack, centralised operations cluster Docker EE - Assemble Docker EE now has a binary called docker-assemble . This allows you to build a Docker image directly from something like a pom.xml, much like JIB. Other","title":"DockerCon EU 2018"},{"location":"blogs/dockercon-eu-2018/#dockercon-eu-2018-recap","text":"","title":"DockerCon EU 2018 - Recap"},{"location":"blogs/dockercon-eu-2018/#generally-outline","text":"From my perspective, there were a few red threads throughout the conference.","title":"Generally outline"},{"location":"blogs/dockercon-eu-2018/#security-begins-at-the-developer","text":"A shift left of security, bringing the responsibility of knowing your dependencies and their quality firmly to the developer. Most of this tooling is still aimed at enterprises though, being part of paid solutions mostly. At least that which was shown at the conference. docker-assemble, that can build in an image from a Maven pom.xml and will include meta-data of all your dependencies (transitive included) JFrog X-Ray Docker EE tooling such as Docker Trusted Registry (DTR)","title":"Security begins at the Developer"},{"location":"blogs/dockercon-eu-2018/#broader-automation","text":"More focus on the entire lifecycle of a system and not just an application. It seems people are starting to understand that doing CI/CD and Infrastructure As Code is not a single event for a single application. There is likely to be a few applications belonging together making a whole system which will land on more than one type of infrastructure and possibly more types of clusters. What we see is tools looking at either a broader scope, a higher level abstraction or more developer focussed (more love for the Dev in DevOps) to allow for easier integration with multiple platforms. For example, Pulumi will enable you to create any type of infrastructure - like Hashicorp's Terraform - but then in programming languages, you're used to (TypeScript, Python, Go). Pulumi Docker App CNAB Build-Kit","title":"Broader Automation"},{"location":"blogs/dockercon-eu-2018/#containerization-influences-everything","text":"Containerization has left deep and easy to spot imprints in our industry from startups building entirely on top of containers to programming languages changing their ways to stay relevant. There are new monitoring kings in the world, DataDog, Sysdig, Honeycomb.io and so on. They live and breathe containers and are not afraid of being thrown around different public clouds, networks and what not. In contrast to traditional monitoring tools, which are often bolting on container support and struggle with the dynamic nature of containerized clusters. Another extraordinary influence is that on the Java language. Declared dead a million times over and deemed obsolete in the container era due to its massive footprint in image size and runtime size. Both are being addressed, and we see a lot of work done on reducing footprint and static linking (JLink, Graal). The most significant influence might be on the software behemoth that has rejuvenated itself. Microsoft has sworn allegiance to open source, Linux and containers. Windows 2019 server can run container workloads natively and work as nodes alongside a Docker EE cluster - which can include Kubernetes workloads. The next step would be support for Kubernetes integration, and as in the case of Java, smaller container footprint. Java Docker Windows Container Windows Server Support Observability tools Kubernetes offerings everywhere...","title":"Containerization Influences Everything"},{"location":"blogs/dockercon-eu-2018/#docker-build-with-build-kit","text":"Instead of investing in improving docker image building via the Docker Client, Docker created a new API and client library. This library called BuildKit, is completely independent. With Docker 18.09, it is included in the Docker Client allowing anyone to use it as easily as the traditional docker image build . BuildKit is already used by some other tools, such as Buildah and IMG, and allows you to create custom DSL \"Frontends\". As long as the API of BuikdKit is adhered to, the resulting image will be OCI compliant. So further remarks below and how to use it. BuildKit In-Depth session Supercharged Docker Build with BuildKit Usable from Docker 18.09 HighLights: allows custom DSL for specifying image (BuildKit) to still be used with Docker client/daemon build cache for your own files during build, think Go, Maven, Gradle... much more optimized, builds less, quicker, with more cache in less time support mounts (cache) such as secrets, during build phase 1 2 3 # Set env variable to enable # Or configure docker s json config export DOCKER_BUILDKIT = 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # syntax=docker/dockerfile:experimental ####################################### ## 1. BUILD JAR WITH MAVEN FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src ! RUN --mount = type = cache,target = /root/.m2/ mvn clean package -e ####################################### ## 2. BUILD NATIVE IMAGE WITH GRAAL FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from = BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath ####################################### ## 3. BUILD DOCKER RUNTIME IMAGE FROM alpine:3.8 CMD [ jpc-graal ] COPY --from = NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal #######################################","title":"Docker Build with Build-Kit"},{"location":"blogs/dockercon-eu-2018/#secure-your-kubernetes","text":"https://www.openpolicyagent.org + admission controller Network Policies Service Accounts","title":"Secure your Kubernetes"},{"location":"blogs/dockercon-eu-2018/#broader-cloud-automation","text":"Two of the broader cloud automation initiatives that impressed me at DockerCon were Pulumi and CNAB. Where Pulumi is an attempt to provide a more developer-friendly alternative to Terraform, CNAB is an attempt to create an environment agnostic installer specification. Meaning, you could create a CNAB installer which uses Pulumi to install all required infrastructure, applications and other resources.","title":"Broader Cloud Automation"},{"location":"blogs/dockercon-eu-2018/#cnab-cloud-native-application-bundle","text":"Bundle.json invocation image (oci) = installer https://cnab.io docker app implements it helm support https://github.com/deislabs","title":"CNAB: cloud native application bundle"},{"location":"blogs/dockercon-eu-2018/#install-an-implementation","text":"There are currently two implementations - that I found. Duffle from DeisLabs - open source from Azure - and Docker App - From Docker Inc..","title":"Install an implementation"},{"location":"blogs/dockercon-eu-2018/#duffle","text":"create a new repo clone the repo init a duffle bundle copy duffle bundle data to our repo folder 1 2 3 4 git clone git@github.com:demomon/cnab-duffle-demo-1.git duffle create cnab-duffle-demo-2 mv cnab-duffle-demo-2/cnab cnab-duffle-demo-1/ mv cnab-duffle-demo-2/duffle.json cnab-duffle-demo-1/ edit our install file ( cnab/run ) build our duffle bundle ( duffle build . ) We can now inspect our bundle with duffle. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 duffle show cnab-duffle-demo-1:0.1.0 -r -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA256 { name : cnab-duffle-demo-1 , version : 0.1.0 , description : A short description of your bundle , keywords : [ cnab-duffle-demo-1 , cnab , demo , joostvdg ] , maintainers : [ { name : Joost van der Griendt , email : joostvdg@gmail.com , url : https://joostvdg.github.io } ] , invocationImages : [ { imageType : docker , image : deislabs/cnab-duffle-demo-1-cnab:2965aad7406e1b651a98fffe1194fcaaec5e623c } ] , images : null, parameters : null, credentials : null } -----BEGIN PGP SIGNATURE----- wsDcBAEBCAAQBQJcL3L8CRDgq4YEOipZ/QAAjDYMAJI5o66SteUP2o4HsbVk+Viw 3Fd874vSVmpPKcmN3tRCEDWGdMdvqQiirDpa//ghx4y5oFTahK2ihQ35GbJLlq8S v9/CK6CKGJtp5g38s2LZrKIvESzEF2lTXwHB03PG8PJ37iWiYkHkxvMpyzded3Rs 4d+VgUnip0Cre7DemaUaz5+fTQjs88WNTIhqPg47YvgqFXV0s1y7yN3RTLr3ohQ1 9mkw87nWfOD+ULpbCUaq9FhNZ+v4dK5IZcWlkyv+yrtguyBBiA3MC54ueVBAdFCl 2OhxXgZjbBHPfQPV1mPqCQudOsWjK/+gqyNb6KTzKrAnyrumVQli/C/8BVk/SRC/ GS2o4EdTS2lfREc2Gl0/VTmMkqzFZZhWd7pwt/iMjl0bICFehSU0N6OqN1d+v6Sq vWIZ5ppxt1NnCzp05Y+NRfVZOxBc2xjYTquFwIa/+qGPrmXBKamw/irjmCOndvx+ l1tf/g0UVSQI2R2/19svl7dlMkYpDdlth1YGgZi/Hg == = 8Xwi -----END PGP SIGNATURE-----","title":"Duffle"},{"location":"blogs/dockercon-eu-2018/#demo","text":"I've created a demo on GitHub: github.com/demomon/cnab-duffle-demo-1 Its goal is to install CloudBees Core and its prerequisites in a (GKE) Kubernetes cluster. It wraps a Go (lang) binary that will execute the commands, for which you can find the source code on GitHub .","title":"Demo"},{"location":"blogs/dockercon-eu-2018/#components","text":"A CNAB bundle has some components by default, for this demo we needed the following: duffle.json : Duffle configuration file Dockerfile : the CNAB installer runtime run (script): the installer script kgi (binary): the binary executable from my k8s-gitops-installer code, that we will leverage for the installation","title":"Components"},{"location":"blogs/dockercon-eu-2018/#dockerfile","text":"The installer tool ( kgi ) requires Helm and Kubectl , so we a Docker image that has those. As we might end up packaging the entire image as part of the full CNAB package, it should also be based on Alpine (or similar minimal Linux). There seems to be one very well maintained and widely used (according to GitHub and Dockerhub stats): dtzar/helm-kubect . So no need to roll our own. 1 2 3 4 5 6 7 FROM dtzar/helm-kubectl:2.12.1 COPY Dockerfile /cnab/Dockerfile COPY app /cnab/app COPY kgi /usr/bin RUN ls -lath /cnab/app/ RUN kgi --help CMD [ /cnab/app/run ]","title":"Dockerfile"},{"location":"blogs/dockercon-eu-2018/#dufflejson","text":"The only thing we need to add beyond the auto-generated file, is the credentials section. 1 2 3 4 5 credentials : { kubeconfig : { path : /cnab/app/kube-config } }","title":"duffle.json"},{"location":"blogs/dockercon-eu-2018/#kgi","text":"I pre-build a binary suitable for Linux that works in Alpine and included it in the CNAB folder.","title":"kgi"},{"location":"blogs/dockercon-eu-2018/#run-script","text":"First thing we need to make sure, is to configure the Kubeconfig location. 1 export KUBECONFIG = /cnab/app/kube-config This should match what we defined in the duffle.json configuration - as you might expect - to make sure it gets bound in the right location. The kubectl command now knows which file to use. For the rest, we can do what we want, but convention tells us we need at least support status , install and uninstall . I'm lazy and only implemented install at the moment. In the install action, we will use the kgi executable to install CloudBees Core and it's pre-requisites. 1 2 3 4 5 6 7 8 action = $CNAB_ACTION case $action in install ) echo [Install] kgi validate kubectl ;; esac For the rest, I recommend you look at the sources.","title":"run script"},{"location":"blogs/dockercon-eu-2018/#run-the-demo","text":"First, we have to build the bundle. 1 duffle build . Once the build succeeded, we can create a credentials configuration. This will be a separate configuration file managed by Duffle. This configuration config must then be used with any installation that requires it - which makes it reusable as well. We have to populate it with the credential. In this case a path to a kube-config file. If you do not have one that you can export - e.g. based on GCloud - you can create a new user/certificate with a script. This is taken from gravitational , which were nice enough to create a script for doing so. You can find the script on GitHub (get-kubeconfig.sh) . Once you have that, store the end result at a decent place and configure it as your credential. 1 duffle creds generate demo1 cnab-duffle-demo-1 With the above command we can create a credential config object based on the build bundle cnab-duffle-demo-1 . The credential object will be demo-1 , which we can now use for installing. 1 duffle install demo1 cnab-duffle-demo-1:1.0.0 -c demo1","title":"Run the demo"},{"location":"blogs/dockercon-eu-2018/#further-reading","text":"Howto guide on creating a Duffle Bundle Howto on handling credentials Wordpress with Kubernetes and AWS demo by Bitnami Example bundles","title":"Further reading"},{"location":"blogs/dockercon-eu-2018/#pulumi","text":"Just pointing to the documentation and GitHub repository is a bit boring and most of all, lazy. So I've gone through the trouble to create a demo project that shows what Pulumi can do. One of the things to keep in mind is that Pulumi keeps its state in a remote location by default - Pulumi's SaaS service. Much like using an S3 bucket to store your Terraform state. The goal of the demo is to show what you can do with Pulumi. My personal opinion is that Pulumi is excellent for managing entire systems as code. From public cloud infrastructure to public open source resources right down to your applications and their configurations. The system that I've written with Pulumi is a CI/CD system running in a GKE cluster. The system contains Jenkins, Artifactory and a custom LDAP server. To make it interesting, we'll use different aspects of Pulumi to create the system. We will create the GKE cluster via Pulumi's wrapper for the gcloud cli - in a similar way as JenkinsX does -, install Jenkins and Artifactory via Helm and install the LDAP server via Pulumi's Kubernetes resource classes.","title":"Pulumi"},{"location":"blogs/dockercon-eu-2018/#steps-taken","text":"For more info Pulumi.io install: brew install pulumi clone demo: git clone https://github.com/demomon/pulumi-demo-1 init stack: pulumi stack init demomon-pulumi-demo-1 connect to GitHub set kubernetes config pulumi config set kubernetes:context gke_ps-dev-201405_europe-west4_joostvdg-reg-dec18-1 pulumi config set isMinikube false install npm resources: npm install (I've used the TypeScript demo's) pulumi config set username administrator pulumi config set password 3OvlgaockdnTsYRU5JAcgM1o --secret pulumi preview incase pulumi loses your stack: pulumi stack select demomon-pulumi-demo-1 pulumi destroy Based on the following demo's: https://pulumi.io/quickstart/kubernetes/tutorial-exposed-deployment.html https://github.com/pulumi/examples/tree/master/kubernetes-ts-jenkins","title":"Steps taken"},{"location":"blogs/dockercon-eu-2018/#artifactory-via-helm","text":"To install the helm charts of Artifactory using Helm, we will first need to add JFrog's repository. helm repo add jfrog https://charts.jfrog.io helm repo update","title":"Artifactory via Helm"},{"location":"blogs/dockercon-eu-2018/#gke-cluster","text":"Below is the code for the cluster. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import * as gcp from @pulumi/gcp ; import * as k8s from @pulumi/kubernetes ; import * as pulumi from @pulumi/pulumi ; import { nodeCount , nodeMachineType , password , username } from ./gke-config ; export const k8sCluster = new gcp . container . Cluster ( gke-cluster , { name : joostvdg-dec-2018-pulumi , initialNodeCount : nodeCount , nodeVersion : latest , minMasterVersion : latest , nodeConfig : { machineType : nodeMachineType , oauthScopes : [ https://www.googleapis.com/auth/compute , https://www.googleapis.com/auth/devstorage.read_only , https://www.googleapis.com/auth/logging.write , https://www.googleapis.com/auth/monitoring ], }, });","title":"GKE Cluster"},{"location":"blogs/dockercon-eu-2018/#gke-config","text":"As you could see, we import variables from a configuration file gke-config . 1 2 3 4 5 6 7 8 import { Config } from @pulumi/pulumi ; const config = new Config (); export const nodeCount = config . getNumber ( nodeCount ) || 3 ; export const nodeMachineType = config . get ( nodeMachineType ) || n1-standard-2 ; // username is the admin username for the cluster. export const username = config . get ( username ) || admin ; // password is the password for the admin user in the cluster. export const password = config . require ( password );","title":"GKE Config"},{"location":"blogs/dockercon-eu-2018/#kubeconfig","text":"As you probably will want to install other things (Helm charts, services) inside this cluster, we need to make sure we get the kubeconfig file of our cluster - which is yet to be created. Below is the code - courtesy of Pulumi's GKE Example - that generates and exports the Kubernetes Client Configuration. This client configuration can then be used by Helm charts or other Kubernetes services. Pulumi will then also understand it depends on the cluster and create/update it first before moving on to the others. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // Manufacture a GKE-style Kubeconfig. Note that this is slightly different because of the way GKE requires // gcloud to be in the picture for cluster authentication (rather than using the client cert/key directly). export const k8sConfig = pulumi . all ([ k8sCluster . name , k8sCluster . endpoint , k8sCluster . masterAuth ]). apply (([ name , endpoint , auth ]) = { const context = ` ${ gcp . config . project } _ ${ gcp . config . zone } _ ${ name } ` ; return `apiVersion: v1 clusters: - cluster: certificate-authority-data: ${ auth . clusterCaCertificate } server: https:// ${ endpoint } name: ${ context } contexts: - context: cluster: ${ context } user: ${ context } name: ${ context } current-context: ${ context } kind: Config preferences: {} users: - name: ${ context } user: auth-provider: config: cmd-args: config config-helper --format=json cmd-path: gcloud expiry-key: {.credential.token_expiry} token-key: {.credential.access_token} name: gcp ` ; }); // Export a Kubernetes provider instance that uses our cluster from above. export const k8sProvider = new k8s . Provider ( gkeK8s , { kubeconfig : k8sConfig , });","title":"Kubeconfig"},{"location":"blogs/dockercon-eu-2018/#pulumi-gcp-config","text":"https://github.com/pulumi/examples/blob/master/gcp-ts-gke/README.md 1 2 3 4 export GCP_PROJECT = ... export GCP_ZONE = europe-west4-a export CLUSTER_PASSWORD = ... export GCP_SA_NAME = ... Make sure you have a Google SA (Service Account) by that name first, as you can read here . For me it worked best to NOT set any environment variables mentioned. They invairably caused authentication or authorization issues. Just make sure the SA account and it's credential file (see below) are authorized and the gcloud cli works. 1 2 3 4 gcloud iam service-accounts keys create gcp-credentials.json \\ --iam-account ${ GCP_SA_NAME } @ ${ GCP_PROJECT } .iam.gserviceaccount.com gcloud auth activate-service-account --key-file gcp-credentials.json gcloud auth application-default login 1 2 3 pulumi config set gcp:project ${ GCP_PROJECT } pulumi config set gcp:zone ${ GCP_ZONE } pulumi config set password --secret ${ CLUSTER_PASSWORD }","title":"Pulumi GCP Config"},{"location":"blogs/dockercon-eu-2018/#post-cluster-creation","text":"1 2 gcloud container clusters get-credentials joostvdg-dec-2018-pulumi kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $( gcloud config get-value account )","title":"Post Cluster Creation"},{"location":"blogs/dockercon-eu-2018/#install-failed","text":"Failed to install kubernetes:rbac.authorization.k8s.io:Role artifactory-artifactory . Probably due to missing rights, so probably have to execute the admin binding before the helm charts. 1 error: Plan apply failed: roles.rbac.authorization.k8s.io artifactory-artifactory is forbidden: attempt to grant extra privileges: ...","title":"Install failed"},{"location":"blogs/dockercon-eu-2018/#helm-charts","text":"Using Pulumi to install a Helm Chart feels a bit like adding layers of wrapping upon wrapping. The power of Pulumi becomes visible when using more than one related service on the same cluster - for example a SDLC Tool Chain. This example application installs two helm charts, Jenkins and Artifactory, on a GKE cluster that is also created and managed by Pulumi. Below is an example of installing a Helm chart of Jenkins, where we provide the Kubernetes config from the GKE cluster as Provider. This way, Pulumi knows it must install the helm chart in that GKE cluster and not in the current Kubeconfig. 1 2 3 4 5 6 7 8 9 10 import { k8sProvider , k8sConfig } from ./gke-cluster ; const jenkins = new k8s . helm . v2 . Chart ( jenkins , { repo : stable , version : 0.25.1 , chart : jenkins , }, { providers : { kubernetes : k8sProvider } } );","title":"Helm Charts"},{"location":"blogs/dockercon-eu-2018/#deployment-service","text":"First, make sure you have an interface for the configuration arguments. 1 2 3 4 5 export interface LdapArgs { readonly name : string , readonly imageName : string , readonly imageTag : string } Then, create a exportable Pulumi resource class that can be reused. 1 2 3 4 5 6 export class LdapInstallation extends pulumi . ComponentResource { public readonly deployment : k8s.apps.v1.Deployment ; public readonly service : k8s.core.v1.Service ; // constructor } Inside the constructor placehold we will create a constructor method. It will do all the configuration we need to do for this resource, in this case a Kubernetes Service and Deployment. 1 2 3 4 5 constructor ( args : LdapArgs ) { super ( k8stypes:service:LdapInstallation , args . name , {}); const labels = { app : args.name }; const name = args . name } First Kubernetes resource to create is a container specification for the Deployment. 1 2 3 4 5 6 7 8 9 10 11 12 const container : k8stypes.core.v1.Container = { name , image : args.imageName + : + args . imageTag , resources : { requests : { cpu : 100m , memory : 200Mi }, limits : { cpu : 100m , memory : 200Mi }, }, ports : [{ name : ldap , containerPort : 1389 , }, ] }; As the configuration arguments can be any TypeScript type, you can allow people to override entire segments (such as Resources). Which you would do as follows: 1 2 3 4 resources : args.resources || { requests : { cpu : 100m , memory : 200Mi }, limits : { cpu : 100m , memory : 200Mi }, }, The Deployment and Service construction are quite similar. 1 2 3 4 5 6 7 8 9 10 this . deployment = new k8s . apps . v1 . Deployment ( args . name , { spec : { selector : { matchLabels : labels }, replicas : 1 , template : { metadata : { labels : labels }, spec : { containers : [ container ] }, }, }, },{ provider : cluster.k8sProvider }); 1 2 3 4 5 6 7 8 9 10 11 12 13 this . service = new k8s . core . v1 . Service ( args . name , { metadata : { labels : this.deployment.metadata.apply ( meta = meta . labels ), }, spec : { ports : [{ name : ldap , port : 389 , targetPort : ldap , protocol : TCP }, ], selector : this.deployment.spec.apply ( spec = spec . template . metadata . labels ), type : ClusterIP , }, }, { provider : cluster.k8sProvider });","title":"Deployment &amp; Service"},{"location":"blogs/dockercon-eu-2018/#jfrog-jenkins-challenge","text":"Visited the stand of JFrog where they had stories about two main products: Artifactory and X-Ray. For both there is a Challenge, an X-Ray Challenge and a Jenkins Artifactory Challenge .","title":"JFrog Jenkins Challenge"},{"location":"blogs/dockercon-eu-2018/#jenkins-challenge","text":"The instructions for the Challenge were simply, follow what is stated in their GitHub repository and email a screenshot of the result. The instruction were as follows: Get an Artifactory instance (you can start a free trial on prem or in the cloud) Install Jenkins Install Artifactory Jenkins Plugin Add Artifactory credentials to Jenkins Credentials Create a new pipeline job Use the Artifactory Plugin DSL documentation to complete the following script: With a Scripted Pipeline as starting point: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 node { def rtServer def rtGradle def buildInfo stage ( Preparation ) { git https://github.com/jbaruch/gradle-example.git // create a new Artifactory server using the credentials defined in Jenkins // create a new Gradle build // set the resolver to the Gradle build to resolve from Artifactory // set the deployer to the Gradle build to deploy to Artifactory // declare that your gradle script does not use Artifactory plugin // declare that your gradle script uses Gradle wrapper } stage ( Build ) { //run the artifactoryPublish gradle task and collect the build info } stage ( Publish Build Info ) { //collect the environment variables to build info //publish the build info } } I don't like scripted, so I opted for Declarative with Jenkins in Kubernetes with the Jenkins Kubernetes plugin. Steps I took: get a trial license from the JFrog website install Artifactory and copy in the license when prompted change admin password create local maven repo 'libs-snapshot-local' create remote maven repo 'jcenter' (default remote value is jcenter, so only have to set the name) install Jenkins Artifactory plugin Kubernetes plugin add Artifactory username/password as credential in Jenkins create a gradle application (Spring boot via start.spring.io) which you can find here create a Jenkinsfile","title":"Jenkins Challenge"},{"location":"blogs/dockercon-eu-2018/#installing-artifactory","text":"I installed Artifactory via Helm. JFrog has their own Helm repository - of course, would weird otherwise tbh - and you have to add that first. 1 2 helm repo add jfrog https://charts.jfrog.io helm install --name artifactory stable/artifactory","title":"Installing Artifactory"},{"location":"blogs/dockercon-eu-2018/#jenkinsfile","text":"This uses the Gradle wrapper - as per instructions in the challenge. So we can use the standard JNLP container, which is default, so agent any will do. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 pipeline { agent any environment { rtServer = rtGradle = buildInfo = artifactoryServerAddress = http://..../artifactory } stages { stage ( Test Container ) { steps { container ( gradle ) { sh which gradle sh uname -a sh gradle -version } } } stage ( Checkout ){ steps { git https://github.com/demomon/gradle-jenkins-challenge.git } } stage ( Preparation ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: artifactoryServerAddress , credentialsId: art-admin // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: jcenter , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: libs-snapshot-local , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( Build ) { steps { script { //run the artifactoryPublish gradle task and collect the build info buildInfo = rtGradle . run buildFile: build.gradle , tasks: clean build artifactoryPublish } } } stage ( Publish Build Info ) { steps { script { //collect the environment variables to build info buildInfo . env . capture = true //publish the build info rtServer . publishBuildInfo buildInfo } } } } }","title":"Jenkinsfile"},{"location":"blogs/dockercon-eu-2018/#jenkinsfile-without-gradle-wrapper","text":"I'd rather not install the Gradle tool if I can just use a pre-build container with it. Unfortunately, to use it correctly with the Artifactory plugin and a Jenkins Kubernetes plugin, we need to do two things. create a Gradle Tool in the Jenkins master because the Artifactory plugin expects a Jenkins Tool object, not a location Manage Jenkins - Global Tool Configuration - Gradle - Add As value supply /usr , the Artifactory build will add /gradle/bin to it automatically set the user of build Pod to id 1000 explicitly else the build will not be allowed to touch files in /home/jenkins/workspace 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 pipeline { agent { kubernetes { label mypod yaml apiVersion: v1 kind: Pod spec: securityContext: runAsUser: 1000 fsGroup: 1000 containers: - name: gradle image: gradle:4.10-jdk-alpine command: [ cat ] tty: true } } environment { rtServer = rtGradle = buildInfo = CONTAINER_GRADLE_TOOL = /usr/bin/gradle } stages { stage ( Test Container ) { steps { container ( gradle ) { sh which gradle sh uname -a sh gradle -version } } } stage ( Checkout ){ steps { // git https://github.com/demomon/gradle-jenkins-challenge.git checkout scm } } stage ( Preparation ) { steps { script { // create a new Artifactory server using the credentials defined in Jenkins rtServer = Artifactory . newServer url: http://35.204.238.14/artifactory , credentialsId: art-admin // create a new Gradle build rtGradle = Artifactory . newGradleBuild () // set the resolver to the Gradle build to resolve from Artifactory rtGradle . resolver repo: jcenter , server: rtServer // set the deployer to the Gradle build to deploy to Artifactory rtGradle . deployer repo: libs-snapshot-local , server: rtServer // declare that your gradle script does not use Artifactory plugin rtGradle . usesPlugin = false // declare that your gradle script uses Gradle wrapper rtGradle . useWrapper = true } } } stage ( Build ) { //run the artifactoryPublish gradle task and collect the build info steps { script { buildInfo = rtGradle . run buildFile: build.gradle , tasks: clean build artifactoryPublish } } } stage ( Publish Build Info ) { //collect the environment variables to build info //publish the build info steps { script { buildInfo . env . capture = true rtServer . publishBuildInfo buildInfo } } } } }","title":"Jenkinsfile without Gradle Wrapper"},{"location":"blogs/dockercon-eu-2018/#docker-security-standards","text":"security takes place in every layer/lifecycle phase for scaling, security needs to be part of developer's day-to-day as everything is code, anything part of the sdlc should be secure and auditable use an admission controller network policies automate your security processes expand your security automation by adding learnings","title":"Docker security &amp; standards"},{"location":"blogs/dockercon-eu-2018/#docker-java-cicd","text":"telepresence Distroless (google mini os) OpenJ9 Portala (for jdk 12) wagoodman/dive use jre for the runtime instead of jdk buildkit can use mounttarget for local caches add labels with Metadata (depency trees) grafeas kritis FindSecBugs org.owasp:dependency-check-maven arminc/clair-scanner jlink = in limbo","title":"Docker &amp; Java &amp; CICD"},{"location":"blogs/dockercon-eu-2018/#docker-windows","text":"specific base images for different use cases Docker capabilities heavily depend on Windows Server version","title":"Docker &amp; Windows"},{"location":"blogs/dockercon-eu-2018/#other","text":"","title":"Other"},{"location":"blogs/dockercon-eu-2018/#docker-pipeline","text":"Dind + privileged mount socket windows linux Windows build agent provisioning with docker EE Jenkins Docker swarm update_config","title":"Docker pipeline"},{"location":"blogs/dockercon-eu-2018/#idea-build-a-dynamic-cicd-platform-with-kubernetes","text":"jenkins evergreen + jcasc kubernetes plugin gitops pipeline AKS + virtual kubelet + ACI Jenkins + EC2 Pluging + ECS/Fargate jenkins agent as ecs task (fargate agent) docker on windows, only on ECS","title":"Idea: build a dynamic ci/cd platform with kubernetes"},{"location":"blogs/dockercon-eu-2018/#apply-diplomacy-to-code-review","text":"apply diplomacy to code review always positive remove human resistantance with inclusive language improvement focused persist, kindly","title":"Apply Diplomacy to Code Review"},{"location":"blogs/dockercon-eu-2018/#citizens-bank-journey","text":"started with swarm, grew towards kubernetes (ucp) elk stack, centralised operations cluster","title":"Citizens Bank journey"},{"location":"blogs/dockercon-eu-2018/#docker-ee-assemble","text":"Docker EE now has a binary called docker-assemble . This allows you to build a Docker image directly from something like a pom.xml, much like JIB.","title":"Docker EE - Assemble"},{"location":"blogs/dockercon-eu-2018/#other_1","text":"","title":"Other"},{"location":"blogs/gitops-pipeline/","text":"GitOps Pipeline with Jenkins on Kubernetes Missing pieces secrets via Vault service mesh infrastructure as code (around the clusters) multiple clusters Pulumi Rancher","title":"GitOps Pipeline"},{"location":"blogs/gitops-pipeline/#gitops-pipeline-with-jenkins-on-kubernetes","text":"","title":"GitOps Pipeline with Jenkins on Kubernetes"},{"location":"blogs/gitops-pipeline/#missing-pieces","text":"secrets via Vault service mesh infrastructure as code (around the clusters) multiple clusters Pulumi Rancher","title":"Missing pieces"},{"location":"blogs/jenkins-pipeline-docker-alternatives/","text":"Pipelines With Docker Alternatives Building pipelines with Jenkins on Docker has been common for a while. But accessing the docker socket has always been a bit tricky. The easiest solution is directly mounting the docker socket of the host into your build container. However, this is a big security risk and something that is undesirable at best and, pretty dangerous at worst. When you're using an orchestrator such as Kubernetes - which is where Jenkins is currently best put to work - the problem gets worse. Not only do you open up security holes, you also mess up the schedular's insight into available resources, and mess with it's ability to keep your cluster in a correct state. In some sense, using docker directly in a Kubernetes cluster, is likened to running with scissors . Potential Alternatives So, directly running docker containers via a docker engine inside your cluster is out. Let's look at some alternatives. Kubernetes Pod and External Node : the simple way out, use a cloud (such as EC2 Cloud) to provision a classic VM with docker engine as an agent and build there JIB : tool from Google, only works for Java applications and is supported directly from Gradle and Maven - Link Kaniko : tool from Google, works as 1-on-1 replacement for docker image build (except Windows Containers ) - Link IMG : tool from Jess Frazelle to avoid building docker images with a root user involved Link Kubernetes Pod and External Node One of the most used cloud environments is AWS, so I created this solution with AWS's Amazon EC2 Plugin . Warning Unfortunately, you cannot combine the Kubernetes plugin with external nodes (none pod container nodes) in a Declarative pipeline. So you have to use Scripted . This can be done with various different cloud providers such as Digital Ocean, Google, AWS or Azure. This guide will use AWS, as it has the most mature Jenkins Cloud plugin. Prerequisites AWS Account with rights to create AMI's and run EC2 instances Packer Jenkins with Amazon EC2 Plugin installed Steps create AMI with Packer install and configure Amazon EC2 plugin create a test pipeline Create AMI with Packer Packer needs to be able to access EC2 API's and be able to spin up an EC2 instance and create an AMI out of it. AWS setup for Packer You need to configure two things: account details for Packer to use security group where your EC2 instances will be running with this security group needs to open port 22 both Packer and Jenkins will use this for their connection 1 2 3 export AWS_DEFAULT_REGION = eu-west-1 export AWS_ACCESS_KEY_ID = XXX export AWS_SECRET_ACCESS_KEY = XXX 1 2 3 4 5 6 7 aws ec2 --profile myAwsProfile create-security-group \\ --description For building Docker images \\ --group-name docker { GroupId : sg-08079f78cXXXXXXX } Export the security group ID. 1 2 export SG_ID = sg-08079f78cXXXXXXX echo $SG_ID Enable port 22 1 2 3 4 5 6 7 aws ec2 \\ --profile myAwsProfile \\ authorize-security-group-ingress \\ --group-name docker \\ --protocol tcp \\ --port 22 \\ --cidr 0 .0.0.0/0 Packer AMI definition Here's an example definition for Packer for a Ubuntu 18.04 LTS base image with JDK 8 (required by Jenkins) and Docker. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { builders : [{ type : amazon-ebs , region : eu-west-1 , source_ami_filter : { filters : { virtualization-type : hvm , name : *ubuntu-bionic-18.04-amd64-server-* , root-device-type : ebs }, owners : [ 679593333241 ], most_recent : true }, instance_type : t2.micro , ssh_username : ubuntu , ami_name : docker , force_deregister : true }], provisioners : [{ type : shell , inline : [ sleep 15 , sudo apt-get clean , sudo apt-get update , sudo apt-get install -y apt-transport-https ca-certificates nfs-common , curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - , sudo add-apt-repository \\ deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\ , sudo add-apt-repository -y ppa:openjdk-r/ppa , sudo apt-get update , sudo apt-get install -y docker-ce , sudo usermod -aG docker ubuntu , sudo apt-get install -y openjdk-8-jdk , java -version , docker version ] }] } Build the new AMI with packer. 1 2 packer build docker-ami.json export AMI = ami-0212ab37f84e418f4 EC2 Key Pair Create EC2 key pair, this will be used by Jenkins to connect to the instances via ssh. 1 2 3 4 aws ec2 --profile myAwsProfile create-key-pair \\ --key-name jenkinsec2 \\ | jq -r .KeyMaterial \\ jenkins-ec2-proton.pem EC2 Cloud Configuration In a Jenkins' master main configuration, you add a new cloud . In this case, we will use a ec2-cloud so we can instantiate our EC2 VM's with docker. use EC2 credentials for initial connection use key ( .pem ) for VM connection (jenkins agent) configure the following: AMI: ami-0212ab37f84e418f4 availability zone: eu-west-1a VPC SubnetID: subnet-aa54XXXX Remote user: ubuntu labels: docker ubuntu linux SecurityGroup Name: (the id) sg-08079f78cXXXXXXX public ip = true connect via public ip = true Pipeline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @Library ( jenkins-pipeline-library@master ) _ def scmVars def label = jenkins-slave-${UUID.randomUUID().toString()} podTemplate ( label: label , yaml: apiVersion: v1 kind: Pod spec: containers: - name: kubectl image: vfarcic/kubectl command: [ cat ] tty: true ) { node ( label ) { node ( docker ) { stage ( SCM Prepare ) { scmVars = checkout scm } stage ( Lint ) { dockerfileLint () } stage ( Build Docker ) { sh docker image build -t demo:rc-1 . } stage ( Tag Push Docker ) { IMAGE = ${DOCKER_IMAGE_NAME} TAG = ${DOCKER_IMAGE_TAG} FULL_NAME = ${FULL_IMAGE_NAME} withCredentials ([ usernamePassword ( credentialsId: dockerhub , usernameVariable: USER , passwordVariable: PASS )]) { sh docker login -u $USER -p $PASS } sh docker image tag ${IMAGE}:${TAG} ${FULL_NAME} sh docker image push ${FULL_NAME} } } // end node docker stage ( Prepare Pod ) { // have to checkout on our kubernetes pod aswell checkout scm } stage ( Check version ) { container ( kubectl ) { sh kubectl version } } } // end node random label } // end pod def Maven JIB If you use Java with either Gradle or Maven , you can use JIB to create docker image without requiring a docker client or docker engine. JIB will communicate with DockerHub and Google's container registry for sending image layers back and forth. The last layer will be created by the JIB plugin itself, adding your self-executing Jar to the layer and a Entrypoint with the correct flags. For more information about the runtime options, see either jib-maven-plugin documentation or jib-gradle-plugin . Prerequisites Java project build with Gradle or Maven Java project that can start it self, such as Spring Boot or Thorntail (previously Wildfly Swarm, from the JBoss family) able to build either gradle or maven applications The project used can be found at github.com/demomon/maven-spring-boot-demo . Steps configure the plugin for either Gradle or Maven build using an official docker image via the kubernetes pod template Pipeline Using a bit more elaborate pipeline example here. Using SonarCloud for static code analysis and then JIB to create a docker image. We could then use that image either directly in anything that runs docker or in the same cluster via a Helm Chart. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def scmVars def tag pipeline { options { buildDiscarder logRotator ( artifactDaysToKeepStr: 5 , artifactNumToKeepStr: 5 , daysToKeepStr: 5 , numToKeepStr: 5 ) } libraries { lib ( core@master ) lib ( maven@master ) } agent { kubernetes { label mypod defaultContainer jnlp yaml apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:3-jdk-11-slim command: - cat tty: true } } stages { stage ( Test versions ) { steps { container ( maven ) { sh uname -a sh mvn -version } } } stage ( Checkout ) { steps { script { scmVars = checkout scm } gitRemoteConfigByUrl ( scmVars . GIT_URL , githubtoken ) sh git config --global user.email jenkins@jenkins.io git config --global user.name Jenkins } } stage ( Build ) { steps { container ( maven ) { sh mvn clean verify -B -e } } } stage ( Version Analysis ) { parallel { stage ( Version Bump ) { when { branch master } environment { NEW_VERSION = gitNextSemverTagMaven ( pom.xml ) } steps { script { tag = ${NEW_VERSION} } container ( maven ) { sh mvn versions:set -DnewVersion=${NEW_VERSION} } gitTag ( v${NEW_VERSION} ) } } stage ( Sonar Analysis ) { when { branch master } environment { SONAR_HOST = https://sonarcloud.io KEY = spring-maven-demo ORG = demomon SONAR_TOKEN = credentials ( sonarcloud ) } steps { container ( maven ) { sh mvn sonar:sonar \\ -Dsonar.projectKey=${KEY} \\ -Dsonar.organization=${ORG} \\ -Dsonar.host.url=${SONAR_HOST} \\ -Dsonar.login=${SONAR_TOKEN} } } } } } stage ( Publish Artifact ) { when { branch master } environment { DHUB = credentials ( dockerhub ) } steps { container ( maven ) { // we should never come here if the tests have not run, as we run verify before sh mvn clean compile -B -e jib:build -Djib.to.auth.username=${DHUB_USR} -Djib.to.auth.password=${DHUB_PSW} -DskipTests } } } } post { always { cleanWs () } } } Kaniko Google loves Kubernetes and Google prefers people building docker images in Kubernetes without Docker. As JIB is only available for Java projects, there's needs to be an alternative for any other usecase/programming language. The answer to that is Kaniko , a specialized Docker image to create Docker images. Kaniko isn't the most secure way to create docker images, it barely beats mounting a Docker Socket, or might even be worse if you ask others (such as Jess Frazelle ). That said, it is gaining some traction being used by JenkinsX and having an example in Jenkins' Kubernetes Plugin . Info When building more than one image inside the kaniko container, make sure to use the --cleanup flag. So it cleans its temporary cache data before building the next image, as discussed in this google group . Prerequisites Steps Create docker registry secret Configure pod container template Configure stage Create docker registry secret This is an example for DockerHub inside the build namespace. 1 2 3 4 5 kubectl create secret docker-registry -n build regcred \\ --docker-server = index.docker.io \\ --docker-username = myDockerHubAccount \\ --docker-password = myDockerHubPassword \\ --docker-email = myDockerHub@Email.com Example Ppeline Warning Although multi-stage Dockerfile 's are supported, it did fail in my case. So I created a second Dockerfile which is only for running the application ( Dockerfile.run ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 pipeline { agent { kubernetes { //cloud kubernetes label kaniko yaml kind: Pod metadata: name: kaniko spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root - name: go-build-cache mountPath: /root/.cache/go-build - name: img-build-cache mountPath: /root/.local volumes: - name: go-build-cache emptyDir: {} - name: img-build-cache emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { environment { PATH = /busybox:$PATH } steps { container ( name: kaniko , shell: /busybox/sh ) { sh #!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=index.docker.io/caladreas/cat } } } } } IMG img is the brainchild of Jess Frazelle, a prominent figure in the container space. The goal is to be the safest and best way to build OCI compliant images, as outlined in her blog building container images securely on kubernetes . Not working (for me) yet It does not seem to work for me on AWS's EKS. To many little details with relation to runc, file permissions and other configuration. For those who want to give it a spin, here are some resources to take a look at. https://blog.jessfraz.com/post/building-container-images-securely-on-kubernetes/ https://github.com/genuinetools/img https://github.com/opencontainers/runc https://git.j3ss.co/genuinetools/img/+/d05b3e4e10cd0e3c074ffb03dc22d7bb6cde1e78 Pipeline Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 pipeline { agent { kubernetes { label img yaml kind: Pod metadata: name: img annotations: container.apparmor.security.beta.kubernetes.io/img: unconfined spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: img workingDir: /home/jenkins image: caladreas/img:0.5.1 imagePullPolicy: Always securityContext: rawProc: true privileged: true command: - cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: temp emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { steps { container ( img ) { sh mkdir cache sh img build -s ./cache -f Dockerfile.run -t caladreas/cat . } } } } }","title":"Pipelines with Docker Alternatives"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#pipelines-with-docker-alternatives","text":"Building pipelines with Jenkins on Docker has been common for a while. But accessing the docker socket has always been a bit tricky. The easiest solution is directly mounting the docker socket of the host into your build container. However, this is a big security risk and something that is undesirable at best and, pretty dangerous at worst. When you're using an orchestrator such as Kubernetes - which is where Jenkins is currently best put to work - the problem gets worse. Not only do you open up security holes, you also mess up the schedular's insight into available resources, and mess with it's ability to keep your cluster in a correct state. In some sense, using docker directly in a Kubernetes cluster, is likened to running with scissors .","title":"Pipelines With Docker Alternatives"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#potential-alternatives","text":"So, directly running docker containers via a docker engine inside your cluster is out. Let's look at some alternatives. Kubernetes Pod and External Node : the simple way out, use a cloud (such as EC2 Cloud) to provision a classic VM with docker engine as an agent and build there JIB : tool from Google, only works for Java applications and is supported directly from Gradle and Maven - Link Kaniko : tool from Google, works as 1-on-1 replacement for docker image build (except Windows Containers ) - Link IMG : tool from Jess Frazelle to avoid building docker images with a root user involved Link","title":"Potential Alternatives"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#kubernetes-pod-and-external-node","text":"One of the most used cloud environments is AWS, so I created this solution with AWS's Amazon EC2 Plugin . Warning Unfortunately, you cannot combine the Kubernetes plugin with external nodes (none pod container nodes) in a Declarative pipeline. So you have to use Scripted . This can be done with various different cloud providers such as Digital Ocean, Google, AWS or Azure. This guide will use AWS, as it has the most mature Jenkins Cloud plugin.","title":"Kubernetes Pod and External Node"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#prerequisites","text":"AWS Account with rights to create AMI's and run EC2 instances Packer Jenkins with Amazon EC2 Plugin installed","title":"Prerequisites"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#steps","text":"create AMI with Packer install and configure Amazon EC2 plugin create a test pipeline","title":"Steps"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#create-ami-with-packer","text":"Packer needs to be able to access EC2 API's and be able to spin up an EC2 instance and create an AMI out of it.","title":"Create AMI with Packer"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#aws-setup-for-packer","text":"You need to configure two things: account details for Packer to use security group where your EC2 instances will be running with this security group needs to open port 22 both Packer and Jenkins will use this for their connection 1 2 3 export AWS_DEFAULT_REGION = eu-west-1 export AWS_ACCESS_KEY_ID = XXX export AWS_SECRET_ACCESS_KEY = XXX 1 2 3 4 5 6 7 aws ec2 --profile myAwsProfile create-security-group \\ --description For building Docker images \\ --group-name docker { GroupId : sg-08079f78cXXXXXXX } Export the security group ID. 1 2 export SG_ID = sg-08079f78cXXXXXXX echo $SG_ID","title":"AWS setup for Packer"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#enable-port-22","text":"1 2 3 4 5 6 7 aws ec2 \\ --profile myAwsProfile \\ authorize-security-group-ingress \\ --group-name docker \\ --protocol tcp \\ --port 22 \\ --cidr 0 .0.0.0/0","title":"Enable port 22"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#packer-ami-definition","text":"Here's an example definition for Packer for a Ubuntu 18.04 LTS base image with JDK 8 (required by Jenkins) and Docker. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { builders : [{ type : amazon-ebs , region : eu-west-1 , source_ami_filter : { filters : { virtualization-type : hvm , name : *ubuntu-bionic-18.04-amd64-server-* , root-device-type : ebs }, owners : [ 679593333241 ], most_recent : true }, instance_type : t2.micro , ssh_username : ubuntu , ami_name : docker , force_deregister : true }], provisioners : [{ type : shell , inline : [ sleep 15 , sudo apt-get clean , sudo apt-get update , sudo apt-get install -y apt-transport-https ca-certificates nfs-common , curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - , sudo add-apt-repository \\ deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\ , sudo add-apt-repository -y ppa:openjdk-r/ppa , sudo apt-get update , sudo apt-get install -y docker-ce , sudo usermod -aG docker ubuntu , sudo apt-get install -y openjdk-8-jdk , java -version , docker version ] }] } Build the new AMI with packer. 1 2 packer build docker-ami.json export AMI = ami-0212ab37f84e418f4","title":"Packer AMI definition"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#ec2-key-pair","text":"Create EC2 key pair, this will be used by Jenkins to connect to the instances via ssh. 1 2 3 4 aws ec2 --profile myAwsProfile create-key-pair \\ --key-name jenkinsec2 \\ | jq -r .KeyMaterial \\ jenkins-ec2-proton.pem","title":"EC2 Key Pair"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#ec2-cloud-configuration","text":"In a Jenkins' master main configuration, you add a new cloud . In this case, we will use a ec2-cloud so we can instantiate our EC2 VM's with docker. use EC2 credentials for initial connection use key ( .pem ) for VM connection (jenkins agent) configure the following: AMI: ami-0212ab37f84e418f4 availability zone: eu-west-1a VPC SubnetID: subnet-aa54XXXX Remote user: ubuntu labels: docker ubuntu linux SecurityGroup Name: (the id) sg-08079f78cXXXXXXX public ip = true connect via public ip = true","title":"EC2 Cloud Configuration"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#pipeline","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @Library ( jenkins-pipeline-library@master ) _ def scmVars def label = jenkins-slave-${UUID.randomUUID().toString()} podTemplate ( label: label , yaml: apiVersion: v1 kind: Pod spec: containers: - name: kubectl image: vfarcic/kubectl command: [ cat ] tty: true ) { node ( label ) { node ( docker ) { stage ( SCM Prepare ) { scmVars = checkout scm } stage ( Lint ) { dockerfileLint () } stage ( Build Docker ) { sh docker image build -t demo:rc-1 . } stage ( Tag Push Docker ) { IMAGE = ${DOCKER_IMAGE_NAME} TAG = ${DOCKER_IMAGE_TAG} FULL_NAME = ${FULL_IMAGE_NAME} withCredentials ([ usernamePassword ( credentialsId: dockerhub , usernameVariable: USER , passwordVariable: PASS )]) { sh docker login -u $USER -p $PASS } sh docker image tag ${IMAGE}:${TAG} ${FULL_NAME} sh docker image push ${FULL_NAME} } } // end node docker stage ( Prepare Pod ) { // have to checkout on our kubernetes pod aswell checkout scm } stage ( Check version ) { container ( kubectl ) { sh kubectl version } } } // end node random label } // end pod def","title":"Pipeline"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#maven-jib","text":"If you use Java with either Gradle or Maven , you can use JIB to create docker image without requiring a docker client or docker engine. JIB will communicate with DockerHub and Google's container registry for sending image layers back and forth. The last layer will be created by the JIB plugin itself, adding your self-executing Jar to the layer and a Entrypoint with the correct flags. For more information about the runtime options, see either jib-maven-plugin documentation or jib-gradle-plugin .","title":"Maven JIB"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#prerequisites_1","text":"Java project build with Gradle or Maven Java project that can start it self, such as Spring Boot or Thorntail (previously Wildfly Swarm, from the JBoss family) able to build either gradle or maven applications The project used can be found at github.com/demomon/maven-spring-boot-demo .","title":"Prerequisites"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#steps_1","text":"configure the plugin for either Gradle or Maven build using an official docker image via the kubernetes pod template","title":"Steps"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#pipeline_1","text":"Using a bit more elaborate pipeline example here. Using SonarCloud for static code analysis and then JIB to create a docker image. We could then use that image either directly in anything that runs docker or in the same cluster via a Helm Chart. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def scmVars def tag pipeline { options { buildDiscarder logRotator ( artifactDaysToKeepStr: 5 , artifactNumToKeepStr: 5 , daysToKeepStr: 5 , numToKeepStr: 5 ) } libraries { lib ( core@master ) lib ( maven@master ) } agent { kubernetes { label mypod defaultContainer jnlp yaml apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:3-jdk-11-slim command: - cat tty: true } } stages { stage ( Test versions ) { steps { container ( maven ) { sh uname -a sh mvn -version } } } stage ( Checkout ) { steps { script { scmVars = checkout scm } gitRemoteConfigByUrl ( scmVars . GIT_URL , githubtoken ) sh git config --global user.email jenkins@jenkins.io git config --global user.name Jenkins } } stage ( Build ) { steps { container ( maven ) { sh mvn clean verify -B -e } } } stage ( Version Analysis ) { parallel { stage ( Version Bump ) { when { branch master } environment { NEW_VERSION = gitNextSemverTagMaven ( pom.xml ) } steps { script { tag = ${NEW_VERSION} } container ( maven ) { sh mvn versions:set -DnewVersion=${NEW_VERSION} } gitTag ( v${NEW_VERSION} ) } } stage ( Sonar Analysis ) { when { branch master } environment { SONAR_HOST = https://sonarcloud.io KEY = spring-maven-demo ORG = demomon SONAR_TOKEN = credentials ( sonarcloud ) } steps { container ( maven ) { sh mvn sonar:sonar \\ -Dsonar.projectKey=${KEY} \\ -Dsonar.organization=${ORG} \\ -Dsonar.host.url=${SONAR_HOST} \\ -Dsonar.login=${SONAR_TOKEN} } } } } } stage ( Publish Artifact ) { when { branch master } environment { DHUB = credentials ( dockerhub ) } steps { container ( maven ) { // we should never come here if the tests have not run, as we run verify before sh mvn clean compile -B -e jib:build -Djib.to.auth.username=${DHUB_USR} -Djib.to.auth.password=${DHUB_PSW} -DskipTests } } } } post { always { cleanWs () } } }","title":"Pipeline"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#kaniko","text":"Google loves Kubernetes and Google prefers people building docker images in Kubernetes without Docker. As JIB is only available for Java projects, there's needs to be an alternative for any other usecase/programming language. The answer to that is Kaniko , a specialized Docker image to create Docker images. Kaniko isn't the most secure way to create docker images, it barely beats mounting a Docker Socket, or might even be worse if you ask others (such as Jess Frazelle ). That said, it is gaining some traction being used by JenkinsX and having an example in Jenkins' Kubernetes Plugin . Info When building more than one image inside the kaniko container, make sure to use the --cleanup flag. So it cleans its temporary cache data before building the next image, as discussed in this google group .","title":"Kaniko"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#prerequisites_2","text":"","title":"Prerequisites"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#steps_2","text":"Create docker registry secret Configure pod container template Configure stage","title":"Steps"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#create-docker-registry-secret","text":"This is an example for DockerHub inside the build namespace. 1 2 3 4 5 kubectl create secret docker-registry -n build regcred \\ --docker-server = index.docker.io \\ --docker-username = myDockerHubAccount \\ --docker-password = myDockerHubPassword \\ --docker-email = myDockerHub@Email.com","title":"Create docker registry secret"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#example-ppeline","text":"Warning Although multi-stage Dockerfile 's are supported, it did fail in my case. So I created a second Dockerfile which is only for running the application ( Dockerfile.run ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 pipeline { agent { kubernetes { //cloud kubernetes label kaniko yaml kind: Pod metadata: name: kaniko spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root - name: go-build-cache mountPath: /root/.cache/go-build - name: img-build-cache mountPath: /root/.local volumes: - name: go-build-cache emptyDir: {} - name: img-build-cache emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { environment { PATH = /busybox:$PATH } steps { container ( name: kaniko , shell: /busybox/sh ) { sh #!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=index.docker.io/caladreas/cat } } } } }","title":"Example Ppeline"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#img","text":"img is the brainchild of Jess Frazelle, a prominent figure in the container space. The goal is to be the safest and best way to build OCI compliant images, as outlined in her blog building container images securely on kubernetes .","title":"IMG"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#not-working-for-me-yet","text":"It does not seem to work for me on AWS's EKS. To many little details with relation to runc, file permissions and other configuration. For those who want to give it a spin, here are some resources to take a look at. https://blog.jessfraz.com/post/building-container-images-securely-on-kubernetes/ https://github.com/genuinetools/img https://github.com/opencontainers/runc https://git.j3ss.co/genuinetools/img/+/d05b3e4e10cd0e3c074ffb03dc22d7bb6cde1e78","title":"Not working (for me) yet"},{"location":"blogs/jenkins-pipeline-docker-alternatives/#pipeline-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 pipeline { agent { kubernetes { label img yaml kind: Pod metadata: name: img annotations: container.apparmor.security.beta.kubernetes.io/img: unconfined spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: img workingDir: /home/jenkins image: caladreas/img:0.5.1 imagePullPolicy: Always securityContext: rawProc: true privileged: true command: - cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: temp emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { steps { container ( img ) { sh mkdir cache sh img build -s ./cache -f Dockerfile.run -t caladreas/cat . } } } } }","title":"Pipeline Example"},{"location":"blogs/jenkins-pipeline-support-tool/","text":"Jenkins Pipeline Support Tools With Jenkins now becoming Cloud Native and a first class citizen of Kubernetes, it is time to review how we use build tools. This content assumes you're using Jenkins in a Kubernetes cluster, but most of it should also work in other Docker-based environments. Ideal Pipeline Anyway, one thing we often see people do wrong with Jenkins pipelines is to use the Groovy Scripts as a general-purpose programming language. This creates many problems, bloated complicated pipelines, much more stress on the master instead of on the build agent and generally making things unreliable. A much better way is to use Jenkins pipelines only as orchestration and lean heavily on your build tools - e.g., Maven, Gradle, Yarn, Bazel - and shell scripts. Alas, if you created complicated pipelines in Groovy scripts, it is likely you'll end up the same with Bash scripts. An even better solution would be to create custom CLI applications that take care of large operations and convoluted logic. You can test and reuse them. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 pipeline { agent any stages { stage ( Build ) { steps { sh ./build.sh } } stage ( Test ) { steps { sh ./test.sh } } stage ( Deploy ) { steps { sh ./deploy.sh } } } post { success { sh ./successNotification.sh } failure { sh ./failureNotification.sh } } } Now, this might look a bit like a pipe dream, but it illustrates how you should use Jenkins Pipeline. The groovy script engine allows for a lot of freedom, but only rarely is its use justified. To create robust, modular and generic pipelines, it is recommended to use build tools, shell scripts, Shared Libraries and custom CLI's . It was always a bit difficult to manage generic scripts and tools across instances of Jenkins, pipelines, and teams. But with Pod Templates we have an excellent mechanism for using, versioning and distributing them with ease. Kubernetes Pods When Jenkins runs in Kubernetes, it can leverage it via the Kubernetes Plugin . I realize Jenkins conjures up mixed emotions when it comes to plugins, but this setup might replace most of them. How so? By using a Kubernetes Pod as the agent where instead of putting all your tools into a single VM you can use multiple small scoped containers. You can specify Pod Templates in multiple ways, where my personal favorite is to define it as yaml inside a declarative pipeline - see example below. For each tool you need, you specify the container and its configuration - if required. By default, you will always get a container with a Jenkins JNLP client and the workspace mounted as a volume in the pod. This allows you to create several tiny containers, each containing only the tools you need for a specific job. Now, it could happen you use two or more tools together a lot - let's say npm and maven - so it is ok to sometimes deviate from this to lower the overall memory of the pod. When you need custom logic, you will have to create a script or tool. This is where PodTemplate, Docker images and our desire for small narrow focus tools come together. PodTemplate example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 pipeline { agent { kubernetes { label mypod defaultContainer jnlp yaml apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:alpine command: - cat tty: true - name: busybox image: busybox command: - cat tty: true } } stages { stage ( Run maven ) { steps { container ( maven ) { sh mvn -version } container ( busybox ) { sh /bin/busybox } } } } } Java Example I bet most people do not think about Java when it comes lightweight CLI applications, but I think that is a shame. Java has excellent tooling to help you build well-tested applications which can be understood and maintained by a vast majority of developers. To make the images small, we will use some of the new tools available in Java land. We will first dive into using Java Modularity and JLink to create a compact and strict binary package, and then we move onto Graal for creating a Native image. Custom JDK Image All the source code of this example application is at github.com/joostvdg/jpb . It is a small CLI which does only one thing; it parses a git commit log to see which folders changed. Quite a useful tool for Monorepo's or other repositories containing more than one changeable resource. Such a CLI should have specific characteristics: * testable * small memory footprint * small disk footprint * quick start * easy to setup * easy to maintain These points sound like an excellent use case for Java Modules and JLink. For those who don't know, read up on Java Modules here and read up on JLink here . JLink will create a binary image that we can use with Alpine Linux to form a minimal Java (Docker) Image. Unfortunately, the plugins for Maven (JMod and JLink) seem to have died. The support on Gradle side is not much better. So I created a solution myself with a multi-stage Docker build. Which does detract a bit from the ease of setup. But overall, it hits the other characteristics spot on. Application Model For ease of maintenance and testing, we separate the parts of the CLI into Java Modules, as you can see in the model below. For using JLink, we need to be a module ourselves. So I figured to expand the exercise to use it to not only create boundaries via packages but also with Modules. Build The current LTS version of Java is 11, which means we need at least that if we want to be up-to-date. As we want to run the application in Alpine Linux, we need to build it with Alpine Linux - if you create a custom JDK image its OS specific. To my surprise, the official LTS release is not released for Alpine, so we use OpenJDK 12. Everything is built via a Multi-Stage Docker Build. This Dockerfile can be divided into five segments. creation of the base with a JDK 11+ on Alpine Linux compiling our Java Modules in into Module Jars test our code create our custom JDK image with just our code and whatever we need from the JDK create the runtime Docker image The Dockerfile looks a bit complicated, but we did get a Java runtime that is about 44MB in size and can run as a direct binary with no startup time. The Dockerfile can be much short if we use only a single module, but as our logic grows it is a thoughtful way to separate different concerns. Still, I'm not too happy with this for creating many small CLI's. To much handwork goes into creating the images like this. Relying on unmaintained Maven or Gradle Plugins doesn't seem a better choice. Luckily, there's a new game in town, GraalVM . We'll make an image with Graal next, stay tuned. Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 ############################################################### ############################################################### ##### 1. CREATE ALPINE BASE WITH JDK11+ #### OpenJDK image produces weird results with JLink (400mb + sizes) FROM alpine:3.8 AS build ENV JAVA_HOME=/opt/jdk \\ PATH = ${ PATH } :/opt/jdk/bin \\ LANG = C.UTF-8 RUN set -ex \\ apk add --no-cache bash \\ wget https://download.java.net/java/early_access/alpine/18/binaries/openjdk-12-ea+18_linux-x64-musl_bin.tar.gz -O jdk.tar.gz \\ mkdir -p /opt/jdk \\ tar zxvf jdk.tar.gz -C /opt/jdk --strip-components = 1 \\ rm jdk.tar.gz \\ rm /opt/jdk/lib/src.zip #################################### ## 2.a PREPARE COMPILE PHASE RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src ## 2.b COMPILE ALL JAVA FILES RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name *.java ) ## 2.c CREATE ALL JAVA MODULE JARS RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.jpb.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.core.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.jpb.core . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.cli.jar --module-version 1 .0 -e com.github.joostvdg.jpb.cli.JpbApp \\ -C /usr/src/mods/compiled/joostvdg.jpb.cli . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.core.test.jar --module-version 1 .0 -e com.github.joostvdg.jpb.core.test.ParseChangeListTest \\ -C /usr/src/mods/compiled/joostvdg.jpb.core.test . #################################### ## 3 RUN TESTS RUN rm -rf /usr/bin/jpb-test-image RUN jlink \\ --verbose \\ --compress 2 \\ --no-header-files \\ --no-man-pages \\ --strip-debug \\ --limit-modules java.base \\ --launcher jpb-test = joostvdg.jpb.core.test \\ --module-path /usr/src/mods/jars/: $JAVA_HOME /jmods \\ --add-modules joostvdg.jpb.core.test \\ --add-modules joostvdg.jpb.core \\ --add-modules joostvdg.jpb.api \\ --output /usr/bin/jpb-test-image RUN /usr/bin/jpb-test-image/bin/java --list-modules RUN /usr/bin/jpb-test-image/bin/jpb-test #################################### ## 4 BUILD RUNTIME - CUSTOM JDK IMAGE RUN rm -rf /usr/bin/jpb-image RUN jlink \\ --verbose \\ --compress 2 \\ --no-header-files \\ --no-man-pages \\ --strip-debug \\ --limit-modules java.base \\ --launcher jpb = joostvdg.jpb.cli \\ --module-path /usr/src/mods/jars/: $JAVA_HOME /jmods \\ --add-modules joostvdg.jpb.cli \\ --add-modules joostvdg.jpb.api \\ --add-modules joostvdg.jpb.core \\ --output /usr/bin/jpb-image RUN /usr/bin/jpb-image/bin/java --list-modules #################################### ##### 5. RUNTIME IMAGE - ALPINE FROM panga/alpine:3.8-glibc2.27 LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.1.0 LABEL description = Docker image for running Jenkins Pipeline Binary ENV DATE_CHANGED= 20181014-2035 ENV JAVA_OPTS= -XX:+UseCGroupMemoryLimitForHeap -XX:+UnlockExperimentalVMOptions COPY --from = build /usr/bin/jpb-image/ /usr/bin/jpb ENTRYPOINT [ /usr/bin/jpb/bin/jpb ] Image disk size 1 2 REPOSITORY TAG IMAGE ID CREATED SIZE jpb latest af7dda45732a About a minute ago 43 .8MB Graal GraalVM is a universal virtual machine for running applications written in JavaScript, Python, Ruby, R, JVM-based languages like Java, Scala, Kotlin, Clojure, and LLVM-based languages such as C and C++. - graalvm.org Ok, that doesn't tell you why using GraalVM is excellent for creating small CLI docker images. Maybe this quote helps: Native images compiled with GraalVM ahead-of-time improve the startup time and reduce the memory footprint of JVM-based applications. Where JLink allows you to create a custom JDK image and embed your application as a runtime binary, Graal goes one step further. It replaces the VM altogether and uses Substrate VM to run your binary. It can't do a lot of the fantastic things the JVM can do and isn't suited for long running applications or those with a large memory footprint and so on. Well, our CLI applications are single shot executions with low memory footprint, the perfect fit for Graal/Substrate! All the code from this example can is on GitHub at github.com/demomon/jpc-graal--maven . Application Model While building modular Java applications is excellent, the current tooling support terrible. So this time the application is a single Jar - Graal can create images from classes or jars - where packages do the separation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 docker-graal-build.sh \u251c\u2500\u2500 pom.xml \u2514\u2500\u2500 src \u251c\u2500\u2500 main \u2514\u2500\u2500 java \u2514\u2500\u2500 com \u2514\u2500\u2500 github \u2514\u2500\u2500 joostvdg \u2514\u2500\u2500 demo \u251c\u2500\u2500 App.java \u2514\u2500\u2500 Hello.java Build Graal can build a native image based on a Jar file. This allows us to use any standard Java build tool such as Maven or Gradle to build the jar. The actual Graal build will be done in a Dockerfile. The people over at Oracle have created an official Docker image reducing effort spend on our side. The Dockerfile has three segments: build the jar with Maven build the native image with Graal assembly the runtime Docker image based on Alpine As you can see below, the Graal image is only half the size of the JLink image! Let's see how that stacks up to other languages such as Go and Python. Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ####################################### ## 1. BUILD JAR WITH MAVEN FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src RUN mvn clean package -e ####################################### ## 2. BUILD NATIVE IMAGE WITH GRAAL FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from = BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath ####################################### ## 3. BUILD DOCKER RUNTIME IMAGE FROM alpine:3.8 CMD [ jpc-graal ] COPY --from = NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal ####################################### Image disk size 1 2 REPOSITORY TAG IMAGE ID CREATED SIZE jpc-graal-maven latest dc33ebb10813 About an hour ago 19 .6MB Go Example Application Model Build Dockerfile Image Disk size 1 2 REPOSITORY TAG IMAGE ID CREATED SIZE jpc-go latest bb4a8e546601 6 minutes ago 12 .3MB Python Example Application Model Build Dockerfile Image Disk size Container footprint 1 2 3 4 5 6 kubectl top pods mypod-s4wpb-7dz4q --containers POD NAME CPU ( cores ) MEMORY ( bytes ) mypod-7lxnk-gw1sj jpc-go 0m 0Mi mypod-7lxnk-gw1sj java-jlink 0m 0Mi mypod-7lxnk-gw1sj java-graal 0m 0Mi mypod-7lxnk-gw1sj jnlp 150m 96Mi So, the 0Mi memory seems wrong. So I decided to dive into the Google Cloud Console, to see if there's any information in there. What I found there, is the data you can see below. The memory is indeed 0Mi, as they're using between 329 and 815 Kilobytes and not hitting the MB threshold (and thus get listed as 0Mi). We do see that graal uses more CPU and slightly less memory than the JLink setup. Both are still significantly larger than the Go CLI tool, but as long as the JNLP container takes ~100MB, I don't think we should worry about 400-500KB. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 CPU container/cpu/usage_time:gke_container:REDUCE_SUM ( , ps-dev-201405 ) : 0 .24 java-graal: 5e-4 java-jlink: 3e-3 jnlp: 0 .23 jpc-go: 2e-4 Memory java-graal: 729 ,088.00 java-jlink: 815 ,104.00 jnlp: 101 .507M jpc-go: 327 ,680.00 Disk java-graal: 49 ,152.00 java-jlink: 49 ,152.00 jnlp: 94 ,208.00 jpc-go: 49 ,152.00 Pipeline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 pipeline { agent { kubernetes { label mypod defaultContainer jnlp yaml apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: java-graal image: caladreas/jpc-graal:0.1.0-maven-b1 command: - cat tty: true - name: java-jlink image: caladreas/jpc-jlink:0.1.0-b1 command: - cat tty: true - name: jpc-go image: caladreas/jpc-go:0.1.0-b1 command: - cat tty: true } } stages { stage ( Test Versions ) { steps { container ( java-graal ) { echo java-graal sh /usr/local/bin/jpc-graal sleep 5 } container ( java-jlink ) { echo java-jlink sh /usr/bin/jpb/bin/jpb GitChangeListToFolder abc abc sleep 5 } container ( jpc-go ) { sh jpc-go sayHello -n joost sleep 5 } sleep 60 } } } }","title":"Jenkins Pipeline Support Tools"},{"location":"blogs/jenkins-pipeline-support-tool/#jenkins-pipeline-support-tools","text":"With Jenkins now becoming Cloud Native and a first class citizen of Kubernetes, it is time to review how we use build tools. This content assumes you're using Jenkins in a Kubernetes cluster, but most of it should also work in other Docker-based environments.","title":"Jenkins Pipeline Support Tools"},{"location":"blogs/jenkins-pipeline-support-tool/#ideal-pipeline","text":"Anyway, one thing we often see people do wrong with Jenkins pipelines is to use the Groovy Scripts as a general-purpose programming language. This creates many problems, bloated complicated pipelines, much more stress on the master instead of on the build agent and generally making things unreliable. A much better way is to use Jenkins pipelines only as orchestration and lean heavily on your build tools - e.g., Maven, Gradle, Yarn, Bazel - and shell scripts. Alas, if you created complicated pipelines in Groovy scripts, it is likely you'll end up the same with Bash scripts. An even better solution would be to create custom CLI applications that take care of large operations and convoluted logic. You can test and reuse them. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 pipeline { agent any stages { stage ( Build ) { steps { sh ./build.sh } } stage ( Test ) { steps { sh ./test.sh } } stage ( Deploy ) { steps { sh ./deploy.sh } } } post { success { sh ./successNotification.sh } failure { sh ./failureNotification.sh } } } Now, this might look a bit like a pipe dream, but it illustrates how you should use Jenkins Pipeline. The groovy script engine allows for a lot of freedom, but only rarely is its use justified. To create robust, modular and generic pipelines, it is recommended to use build tools, shell scripts, Shared Libraries and custom CLI's . It was always a bit difficult to manage generic scripts and tools across instances of Jenkins, pipelines, and teams. But with Pod Templates we have an excellent mechanism for using, versioning and distributing them with ease.","title":"Ideal Pipeline"},{"location":"blogs/jenkins-pipeline-support-tool/#kubernetes-pods","text":"When Jenkins runs in Kubernetes, it can leverage it via the Kubernetes Plugin . I realize Jenkins conjures up mixed emotions when it comes to plugins, but this setup might replace most of them. How so? By using a Kubernetes Pod as the agent where instead of putting all your tools into a single VM you can use multiple small scoped containers. You can specify Pod Templates in multiple ways, where my personal favorite is to define it as yaml inside a declarative pipeline - see example below. For each tool you need, you specify the container and its configuration - if required. By default, you will always get a container with a Jenkins JNLP client and the workspace mounted as a volume in the pod. This allows you to create several tiny containers, each containing only the tools you need for a specific job. Now, it could happen you use two or more tools together a lot - let's say npm and maven - so it is ok to sometimes deviate from this to lower the overall memory of the pod. When you need custom logic, you will have to create a script or tool. This is where PodTemplate, Docker images and our desire for small narrow focus tools come together.","title":"Kubernetes Pods"},{"location":"blogs/jenkins-pipeline-support-tool/#podtemplate-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 pipeline { agent { kubernetes { label mypod defaultContainer jnlp yaml apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:alpine command: - cat tty: true - name: busybox image: busybox command: - cat tty: true } } stages { stage ( Run maven ) { steps { container ( maven ) { sh mvn -version } container ( busybox ) { sh /bin/busybox } } } } }","title":"PodTemplate example"},{"location":"blogs/jenkins-pipeline-support-tool/#java-example","text":"I bet most people do not think about Java when it comes lightweight CLI applications, but I think that is a shame. Java has excellent tooling to help you build well-tested applications which can be understood and maintained by a vast majority of developers. To make the images small, we will use some of the new tools available in Java land. We will first dive into using Java Modularity and JLink to create a compact and strict binary package, and then we move onto Graal for creating a Native image.","title":"Java Example"},{"location":"blogs/jenkins-pipeline-support-tool/#custom-jdk-image","text":"All the source code of this example application is at github.com/joostvdg/jpb . It is a small CLI which does only one thing; it parses a git commit log to see which folders changed. Quite a useful tool for Monorepo's or other repositories containing more than one changeable resource. Such a CLI should have specific characteristics: * testable * small memory footprint * small disk footprint * quick start * easy to setup * easy to maintain These points sound like an excellent use case for Java Modules and JLink. For those who don't know, read up on Java Modules here and read up on JLink here . JLink will create a binary image that we can use with Alpine Linux to form a minimal Java (Docker) Image. Unfortunately, the plugins for Maven (JMod and JLink) seem to have died. The support on Gradle side is not much better. So I created a solution myself with a multi-stage Docker build. Which does detract a bit from the ease of setup. But overall, it hits the other characteristics spot on.","title":"Custom JDK Image"},{"location":"blogs/jenkins-pipeline-support-tool/#application-model","text":"For ease of maintenance and testing, we separate the parts of the CLI into Java Modules, as you can see in the model below. For using JLink, we need to be a module ourselves. So I figured to expand the exercise to use it to not only create boundaries via packages but also with Modules.","title":"Application Model"},{"location":"blogs/jenkins-pipeline-support-tool/#build","text":"The current LTS version of Java is 11, which means we need at least that if we want to be up-to-date. As we want to run the application in Alpine Linux, we need to build it with Alpine Linux - if you create a custom JDK image its OS specific. To my surprise, the official LTS release is not released for Alpine, so we use OpenJDK 12. Everything is built via a Multi-Stage Docker Build. This Dockerfile can be divided into five segments. creation of the base with a JDK 11+ on Alpine Linux compiling our Java Modules in into Module Jars test our code create our custom JDK image with just our code and whatever we need from the JDK create the runtime Docker image The Dockerfile looks a bit complicated, but we did get a Java runtime that is about 44MB in size and can run as a direct binary with no startup time. The Dockerfile can be much short if we use only a single module, but as our logic grows it is a thoughtful way to separate different concerns. Still, I'm not too happy with this for creating many small CLI's. To much handwork goes into creating the images like this. Relying on unmaintained Maven or Gradle Plugins doesn't seem a better choice. Luckily, there's a new game in town, GraalVM . We'll make an image with Graal next, stay tuned.","title":"Build"},{"location":"blogs/jenkins-pipeline-support-tool/#dockerfile","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 ############################################################### ############################################################### ##### 1. CREATE ALPINE BASE WITH JDK11+ #### OpenJDK image produces weird results with JLink (400mb + sizes) FROM alpine:3.8 AS build ENV JAVA_HOME=/opt/jdk \\ PATH = ${ PATH } :/opt/jdk/bin \\ LANG = C.UTF-8 RUN set -ex \\ apk add --no-cache bash \\ wget https://download.java.net/java/early_access/alpine/18/binaries/openjdk-12-ea+18_linux-x64-musl_bin.tar.gz -O jdk.tar.gz \\ mkdir -p /opt/jdk \\ tar zxvf jdk.tar.gz -C /opt/jdk --strip-components = 1 \\ rm jdk.tar.gz \\ rm /opt/jdk/lib/src.zip #################################### ## 2.a PREPARE COMPILE PHASE RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src ## 2.b COMPILE ALL JAVA FILES RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name *.java ) ## 2.c CREATE ALL JAVA MODULE JARS RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.jpb.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.core.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.jpb.core . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.cli.jar --module-version 1 .0 -e com.github.joostvdg.jpb.cli.JpbApp \\ -C /usr/src/mods/compiled/joostvdg.jpb.cli . RUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.core.test.jar --module-version 1 .0 -e com.github.joostvdg.jpb.core.test.ParseChangeListTest \\ -C /usr/src/mods/compiled/joostvdg.jpb.core.test . #################################### ## 3 RUN TESTS RUN rm -rf /usr/bin/jpb-test-image RUN jlink \\ --verbose \\ --compress 2 \\ --no-header-files \\ --no-man-pages \\ --strip-debug \\ --limit-modules java.base \\ --launcher jpb-test = joostvdg.jpb.core.test \\ --module-path /usr/src/mods/jars/: $JAVA_HOME /jmods \\ --add-modules joostvdg.jpb.core.test \\ --add-modules joostvdg.jpb.core \\ --add-modules joostvdg.jpb.api \\ --output /usr/bin/jpb-test-image RUN /usr/bin/jpb-test-image/bin/java --list-modules RUN /usr/bin/jpb-test-image/bin/jpb-test #################################### ## 4 BUILD RUNTIME - CUSTOM JDK IMAGE RUN rm -rf /usr/bin/jpb-image RUN jlink \\ --verbose \\ --compress 2 \\ --no-header-files \\ --no-man-pages \\ --strip-debug \\ --limit-modules java.base \\ --launcher jpb = joostvdg.jpb.cli \\ --module-path /usr/src/mods/jars/: $JAVA_HOME /jmods \\ --add-modules joostvdg.jpb.cli \\ --add-modules joostvdg.jpb.api \\ --add-modules joostvdg.jpb.core \\ --output /usr/bin/jpb-image RUN /usr/bin/jpb-image/bin/java --list-modules #################################### ##### 5. RUNTIME IMAGE - ALPINE FROM panga/alpine:3.8-glibc2.27 LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.1.0 LABEL description = Docker image for running Jenkins Pipeline Binary ENV DATE_CHANGED= 20181014-2035 ENV JAVA_OPTS= -XX:+UseCGroupMemoryLimitForHeap -XX:+UnlockExperimentalVMOptions COPY --from = build /usr/bin/jpb-image/ /usr/bin/jpb ENTRYPOINT [ /usr/bin/jpb/bin/jpb ]","title":"Dockerfile"},{"location":"blogs/jenkins-pipeline-support-tool/#image-disk-size","text":"1 2 REPOSITORY TAG IMAGE ID CREATED SIZE jpb latest af7dda45732a About a minute ago 43 .8MB","title":"Image disk size"},{"location":"blogs/jenkins-pipeline-support-tool/#graal","text":"GraalVM is a universal virtual machine for running applications written in JavaScript, Python, Ruby, R, JVM-based languages like Java, Scala, Kotlin, Clojure, and LLVM-based languages such as C and C++. - graalvm.org Ok, that doesn't tell you why using GraalVM is excellent for creating small CLI docker images. Maybe this quote helps: Native images compiled with GraalVM ahead-of-time improve the startup time and reduce the memory footprint of JVM-based applications. Where JLink allows you to create a custom JDK image and embed your application as a runtime binary, Graal goes one step further. It replaces the VM altogether and uses Substrate VM to run your binary. It can't do a lot of the fantastic things the JVM can do and isn't suited for long running applications or those with a large memory footprint and so on. Well, our CLI applications are single shot executions with low memory footprint, the perfect fit for Graal/Substrate! All the code from this example can is on GitHub at github.com/demomon/jpc-graal--maven .","title":"Graal"},{"location":"blogs/jenkins-pipeline-support-tool/#application-model_1","text":"While building modular Java applications is excellent, the current tooling support terrible. So this time the application is a single Jar - Graal can create images from classes or jars - where packages do the separation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 docker-graal-build.sh \u251c\u2500\u2500 pom.xml \u2514\u2500\u2500 src \u251c\u2500\u2500 main \u2514\u2500\u2500 java \u2514\u2500\u2500 com \u2514\u2500\u2500 github \u2514\u2500\u2500 joostvdg \u2514\u2500\u2500 demo \u251c\u2500\u2500 App.java \u2514\u2500\u2500 Hello.java","title":"Application Model"},{"location":"blogs/jenkins-pipeline-support-tool/#build_1","text":"Graal can build a native image based on a Jar file. This allows us to use any standard Java build tool such as Maven or Gradle to build the jar. The actual Graal build will be done in a Dockerfile. The people over at Oracle have created an official Docker image reducing effort spend on our side. The Dockerfile has three segments: build the jar with Maven build the native image with Graal assembly the runtime Docker image based on Alpine As you can see below, the Graal image is only half the size of the JLink image! Let's see how that stacks up to other languages such as Go and Python.","title":"Build"},{"location":"blogs/jenkins-pipeline-support-tool/#dockerfile_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ####################################### ## 1. BUILD JAR WITH MAVEN FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src RUN mvn clean package -e ####################################### ## 2. BUILD NATIVE IMAGE WITH GRAAL FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from = BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath ####################################### ## 3. BUILD DOCKER RUNTIME IMAGE FROM alpine:3.8 CMD [ jpc-graal ] COPY --from = NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal #######################################","title":"Dockerfile"},{"location":"blogs/jenkins-pipeline-support-tool/#image-disk-size_1","text":"1 2 REPOSITORY TAG IMAGE ID CREATED SIZE jpc-graal-maven latest dc33ebb10813 About an hour ago 19 .6MB","title":"Image disk size"},{"location":"blogs/jenkins-pipeline-support-tool/#go-example","text":"","title":"Go Example"},{"location":"blogs/jenkins-pipeline-support-tool/#application-model_2","text":"","title":"Application Model"},{"location":"blogs/jenkins-pipeline-support-tool/#build_2","text":"","title":"Build"},{"location":"blogs/jenkins-pipeline-support-tool/#dockerfile_2","text":"","title":"Dockerfile"},{"location":"blogs/jenkins-pipeline-support-tool/#image-disk-size_2","text":"1 2 REPOSITORY TAG IMAGE ID CREATED SIZE jpc-go latest bb4a8e546601 6 minutes ago 12 .3MB","title":"Image Disk size"},{"location":"blogs/jenkins-pipeline-support-tool/#python-example","text":"","title":"Python Example"},{"location":"blogs/jenkins-pipeline-support-tool/#application-model_3","text":"","title":"Application Model"},{"location":"blogs/jenkins-pipeline-support-tool/#build_3","text":"","title":"Build"},{"location":"blogs/jenkins-pipeline-support-tool/#dockerfile_3","text":"","title":"Dockerfile"},{"location":"blogs/jenkins-pipeline-support-tool/#image-disk-size_3","text":"","title":"Image Disk size"},{"location":"blogs/jenkins-pipeline-support-tool/#container-footprint","text":"1 2 3 4 5 6 kubectl top pods mypod-s4wpb-7dz4q --containers POD NAME CPU ( cores ) MEMORY ( bytes ) mypod-7lxnk-gw1sj jpc-go 0m 0Mi mypod-7lxnk-gw1sj java-jlink 0m 0Mi mypod-7lxnk-gw1sj java-graal 0m 0Mi mypod-7lxnk-gw1sj jnlp 150m 96Mi So, the 0Mi memory seems wrong. So I decided to dive into the Google Cloud Console, to see if there's any information in there. What I found there, is the data you can see below. The memory is indeed 0Mi, as they're using between 329 and 815 Kilobytes and not hitting the MB threshold (and thus get listed as 0Mi). We do see that graal uses more CPU and slightly less memory than the JLink setup. Both are still significantly larger than the Go CLI tool, but as long as the JNLP container takes ~100MB, I don't think we should worry about 400-500KB. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 CPU container/cpu/usage_time:gke_container:REDUCE_SUM ( , ps-dev-201405 ) : 0 .24 java-graal: 5e-4 java-jlink: 3e-3 jnlp: 0 .23 jpc-go: 2e-4 Memory java-graal: 729 ,088.00 java-jlink: 815 ,104.00 jnlp: 101 .507M jpc-go: 327 ,680.00 Disk java-graal: 49 ,152.00 java-jlink: 49 ,152.00 jnlp: 94 ,208.00 jpc-go: 49 ,152.00","title":"Container footprint"},{"location":"blogs/jenkins-pipeline-support-tool/#pipeline","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 pipeline { agent { kubernetes { label mypod defaultContainer jnlp yaml apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: java-graal image: caladreas/jpc-graal:0.1.0-maven-b1 command: - cat tty: true - name: java-jlink image: caladreas/jpc-jlink:0.1.0-b1 command: - cat tty: true - name: jpc-go image: caladreas/jpc-go:0.1.0-b1 command: - cat tty: true } } stages { stage ( Test Versions ) { steps { container ( java-graal ) { echo java-graal sh /usr/local/bin/jpc-graal sleep 5 } container ( java-jlink ) { echo java-jlink sh /usr/bin/jpb/bin/jpb GitChangeListToFolder abc abc sleep 5 } container ( jpc-go ) { sh jpc-go sayHello -n joost sleep 5 } sleep 60 } } } }","title":"Pipeline"},{"location":"blogs/jenkins-x/","text":"Jenkins X Choose your distribution GKE via JX binary 1 2 3 4 5 6 7 export JX_TOOL_PSW = ZfwYM0odeI5W41GGzXgGqFmP export MACHINE_TYPE = n1-standard-2 export GKE_ZONE = europe-west4-a export K8S_VERSION = 1 .11.2-gke.18 export GKE_NAME = joostvdg-jx-nov18-1 export GIT_API_TOKEN = export PROJECT_ID = 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 jx create cluster gke \\ --cluster-name = ${ GKE_NAME } \\ --default-admin-password = ${ JX_TOOL_PSW } \\ --domain = kearos.net \\ --git-api-token = ${ GIT_API_TOKEN } \\ --git-username = joostvdg \\ --no-tiller \\ --project-id = ${ PROJECT_ID } \\ --prow = true \\ --vault = true \\ --zone = ${ GKE_ZONE } \\ --machine-type = ${ MACHINE_TYPE } \\ --labels = owner=jvandergriendt,purpose=practice,team=ps \\ --skip-login = true \\ --max-num-nodes = 3 \\ --min-num-nodes = 2 \\ --kubernetes-version = ${ K8S_VERSION } \\ --batch-mode = true 1 2 --skip-installation = false: Provision cluster only, don t install Jenkins X into it --skip-login = false: Skip Google auth if already logged in via gcloud auth","title":"Jenkins X"},{"location":"blogs/jenkins-x/#jenkins-x","text":"","title":"Jenkins X"},{"location":"blogs/jenkins-x/#choose-your-distribution","text":"","title":"Choose your distribution"},{"location":"blogs/jenkins-x/#gke-via-jx-binary","text":"1 2 3 4 5 6 7 export JX_TOOL_PSW = ZfwYM0odeI5W41GGzXgGqFmP export MACHINE_TYPE = n1-standard-2 export GKE_ZONE = europe-west4-a export K8S_VERSION = 1 .11.2-gke.18 export GKE_NAME = joostvdg-jx-nov18-1 export GIT_API_TOKEN = export PROJECT_ID = 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 jx create cluster gke \\ --cluster-name = ${ GKE_NAME } \\ --default-admin-password = ${ JX_TOOL_PSW } \\ --domain = kearos.net \\ --git-api-token = ${ GIT_API_TOKEN } \\ --git-username = joostvdg \\ --no-tiller \\ --project-id = ${ PROJECT_ID } \\ --prow = true \\ --vault = true \\ --zone = ${ GKE_ZONE } \\ --machine-type = ${ MACHINE_TYPE } \\ --labels = owner=jvandergriendt,purpose=practice,team=ps \\ --skip-login = true \\ --max-num-nodes = 3 \\ --min-num-nodes = 2 \\ --kubernetes-version = ${ K8S_VERSION } \\ --batch-mode = true 1 2 --skip-installation = false: Provision cluster only, don t install Jenkins X into it --skip-login = false: Skip Google auth if already logged in via gcloud auth","title":"GKE via JX binary"},{"location":"blogs/k8s-controller/","text":"Create your own custom Kubernetes controller Before we dive into the why and how of creating a Kubernetes Controller, let's take a brief look at what it does. What is a Controller I will only briefly touch on what a controller is. If you already know what it is you can safely skip this paragraph. In applications of robotics and automation, a control loop is a non-terminating loop that regulates the state of the system. In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the API server and makes changes attempting to move the current state towards the desired state. Examples of controllers that ship with Kubernetes today are the replication controller, endpoints controller, namespace controller, and serviceaccounts controller. So the purpose of controllers is to control - hence the name - the state of the system - our Kubernetes cluster. A controller is generally created to watch a single resource type and make sure that its desired state is met. As there is already very well written material on the details of controllers, I'll leave it at this. For more information on controllers and how they work, I recommend reading bitnami's deepdive and the kubernetes documentation . When to create your controller Great, you're still reading this. So when would you put in the effort to create your controller? I'm pretty sure there will be more cases, but the following two are the main ones. Process events on Core resources, Core being the resources any Kubernetes ships with Process events on Customer resources Examples of customer controllers for the first use case are tools such as Kubediff, which will compare resources in the cluster with their definition in a Git repository. For the second use case - custom controller for custom resource - there are many more examples. As most custom resources will have their controller to act on the events of the resources because existing controllers will not process the custom resource. Additionally, in most cases having resources sitting in a cluster with nothing happening is a bit of a waste. So we write a controller to match the resource. How to create your controller When it comes to making a controller, it will be some Go (lang) code using the Kubernetes client library. This is straightforward if you're creating a controller for the core resources, but quite a few steps if you write a custom controller. Write a core resource controller To ease ourselves into it lets first create a core resource controller. We're aiming for a controller that can read our ConfigMaps resources. To be able to do this, we need the following: Handler : for the events (Created, Deleted, Updated) Controller : retrieves events from an informer, puts work on a queue, and delegates the events to the handler Entrypoint : typically a main.go file, that creates a connection to the Kubernetes API server and ties all of the resources together Dockerfile : to package our binary for running inside the cluster Resource Definition YAML : typical Kubernetes resource definition file, in our case a Deployment, so our controller will run as a pod/container Handler Controller Entrypoint Dockerfile Resource Definition Resources https://medium.com/@trstringer/create-kubernetes-controllers-for-core-and-custom-resources-62fc35ad64a3 https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/ https://coreos.com/blog/introducing-operators.html https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html https://github.com/joostvdg/k8s-core-resource-controller https://github.com/kubernetes/sample-controller/blob/master/controller.go","title":"Create your own k8s controller"},{"location":"blogs/k8s-controller/#create-your-own-custom-kubernetes-controller","text":"Before we dive into the why and how of creating a Kubernetes Controller, let's take a brief look at what it does.","title":"Create your own custom Kubernetes controller"},{"location":"blogs/k8s-controller/#what-is-a-controller","text":"I will only briefly touch on what a controller is. If you already know what it is you can safely skip this paragraph. In applications of robotics and automation, a control loop is a non-terminating loop that regulates the state of the system. In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the API server and makes changes attempting to move the current state towards the desired state. Examples of controllers that ship with Kubernetes today are the replication controller, endpoints controller, namespace controller, and serviceaccounts controller. So the purpose of controllers is to control - hence the name - the state of the system - our Kubernetes cluster. A controller is generally created to watch a single resource type and make sure that its desired state is met. As there is already very well written material on the details of controllers, I'll leave it at this. For more information on controllers and how they work, I recommend reading bitnami's deepdive and the kubernetes documentation .","title":"What is a Controller"},{"location":"blogs/k8s-controller/#when-to-create-your-controller","text":"Great, you're still reading this. So when would you put in the effort to create your controller? I'm pretty sure there will be more cases, but the following two are the main ones. Process events on Core resources, Core being the resources any Kubernetes ships with Process events on Customer resources Examples of customer controllers for the first use case are tools such as Kubediff, which will compare resources in the cluster with their definition in a Git repository. For the second use case - custom controller for custom resource - there are many more examples. As most custom resources will have their controller to act on the events of the resources because existing controllers will not process the custom resource. Additionally, in most cases having resources sitting in a cluster with nothing happening is a bit of a waste. So we write a controller to match the resource.","title":"When to create your controller"},{"location":"blogs/k8s-controller/#how-to-create-your-controller","text":"When it comes to making a controller, it will be some Go (lang) code using the Kubernetes client library. This is straightforward if you're creating a controller for the core resources, but quite a few steps if you write a custom controller.","title":"How to create your controller"},{"location":"blogs/k8s-controller/#write-a-core-resource-controller","text":"To ease ourselves into it lets first create a core resource controller. We're aiming for a controller that can read our ConfigMaps resources. To be able to do this, we need the following: Handler : for the events (Created, Deleted, Updated) Controller : retrieves events from an informer, puts work on a queue, and delegates the events to the handler Entrypoint : typically a main.go file, that creates a connection to the Kubernetes API server and ties all of the resources together Dockerfile : to package our binary for running inside the cluster Resource Definition YAML : typical Kubernetes resource definition file, in our case a Deployment, so our controller will run as a pod/container","title":"Write a core resource controller"},{"location":"blogs/k8s-controller/#handler","text":"","title":"Handler"},{"location":"blogs/k8s-controller/#controller","text":"","title":"Controller"},{"location":"blogs/k8s-controller/#entrypoint","text":"","title":"Entrypoint"},{"location":"blogs/k8s-controller/#dockerfile","text":"","title":"Dockerfile"},{"location":"blogs/k8s-controller/#resource-definition","text":"","title":"Resource Definition"},{"location":"blogs/k8s-controller/#resources","text":"https://medium.com/@trstringer/create-kubernetes-controllers-for-core-and-custom-resources-62fc35ad64a3 https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/ https://coreos.com/blog/introducing-operators.html https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html https://github.com/joostvdg/k8s-core-resource-controller https://github.com/kubernetes/sample-controller/blob/master/controller.go","title":"Resources"},{"location":"blogs/k8s-crd/","text":"Kubernetes is a fantastic platform that allows you to run a lot of different workloads in various ways. It has APIs front and center, allowing you to choose different implementation as they suit you. Sometimes you feel something is missing. There is a concept with your application or something you want from the cluster that isn't (however) available in Kubernetes. It is then that you can look for extending Kubernetes itself. Either its API or by creating a new kind of resource: a Custom Resource Definition or CRD. What you need resource definition : the yaml definition of your custom resource custom controller : a controller to interact with your custom resource Resource Definition As with any Kubernetes resource, you need a yaml file that defines it with the lexicon of Kubernetes. In this case, the Kind is CustomerResourceDefinition . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : apiextensions.k8s.io/v1beta1 kind : CustomResourceDefinition metadata : name : manifests.cat.kearos.net spec : group : cat.kearos.net version : v1 names : kind : ApplicationManifest plural : applicationmanifests singular : applicationmanifest shortNames : - cam scope : Namespaced apiVersion : as the name implies, it's an API extension kind : has to be CustomResourceDefinition else it wouldn't be a CRD name : name must match the spec fields below, and be in the form: . group : API group name so that you can group multiple resources somewhat together names : kind : the resource kind, used for other resource definitions plural is the official name used in the Kubernetes API, also the default for interaction with kubectl singular : alias for the API usage in kubectl and used as the display value shortNames : shortNames allow a shorter string to match your resource on the CLI scope : can either be Namespaced , tied to a specific namespace, or Cluster where it must be cluster-wide unique Install CRD Taking the above example and saving it as application-manifest.yml , we can install the CRD into the cluster as follows. 1 kubectl create -f application-manifest.yml Resource Usage Example 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : cat.kearos.net/v1 kind : ApplicationManifest metadata : name : manifest-cat spec : name : cat description : Central Application Tracker namespace : cat artifactIDs : - github.com/joostvdg/cat sources : - git@github.com:joostvdg/cat.git Looking at this example, you might wonder how this works. There is a specification in there - spec - with all kinds of custom fields. But where do they come from? Nowhere really, so you cannot validate this with the CRD alone. You can put any arbitrary field in there. So what do you do with the CRD then? You can create a custom controller that processes your custom resources. Because creating a custom controller for your custom resources is complicated and takes several steps, we will do this in a separate article.","title":"Create your own k8s resource"},{"location":"blogs/k8s-crd/#what-you-need","text":"resource definition : the yaml definition of your custom resource custom controller : a controller to interact with your custom resource","title":"What you need"},{"location":"blogs/k8s-crd/#resource-definition","text":"As with any Kubernetes resource, you need a yaml file that defines it with the lexicon of Kubernetes. In this case, the Kind is CustomerResourceDefinition . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : apiextensions.k8s.io/v1beta1 kind : CustomResourceDefinition metadata : name : manifests.cat.kearos.net spec : group : cat.kearos.net version : v1 names : kind : ApplicationManifest plural : applicationmanifests singular : applicationmanifest shortNames : - cam scope : Namespaced apiVersion : as the name implies, it's an API extension kind : has to be CustomResourceDefinition else it wouldn't be a CRD name : name must match the spec fields below, and be in the form: . group : API group name so that you can group multiple resources somewhat together names : kind : the resource kind, used for other resource definitions plural is the official name used in the Kubernetes API, also the default for interaction with kubectl singular : alias for the API usage in kubectl and used as the display value shortNames : shortNames allow a shorter string to match your resource on the CLI scope : can either be Namespaced , tied to a specific namespace, or Cluster where it must be cluster-wide unique","title":"Resource Definition"},{"location":"blogs/k8s-crd/#install-crd","text":"Taking the above example and saving it as application-manifest.yml , we can install the CRD into the cluster as follows. 1 kubectl create -f application-manifest.yml","title":"Install CRD"},{"location":"blogs/k8s-crd/#resource-usage-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : cat.kearos.net/v1 kind : ApplicationManifest metadata : name : manifest-cat spec : name : cat description : Central Application Tracker namespace : cat artifactIDs : - github.com/joostvdg/cat sources : - git@github.com:joostvdg/cat.git Looking at this example, you might wonder how this works. There is a specification in there - spec - with all kinds of custom fields. But where do they come from? Nowhere really, so you cannot validate this with the CRD alone. You can put any arbitrary field in there. So what do you do with the CRD then? You can create a custom controller that processes your custom resources. Because creating a custom controller for your custom resources is complicated and takes several steps, we will do this in a separate article.","title":"Resource Usage Example"},{"location":"blogs/k8s-lets-encrypt/","text":"Let's Encrypt for Kubernetes Let's Encrypt is a free, automated and open Certificate Authority. The kind of service you need if you want to have a secure website with https - yes I know that requires more than that - and it's now more straightforward to use than ever. This about using Let's Encrypt for generating a certificate for your service on Kubernetes. There are several ways to do this, with more or less automation, cluster-wide or namespace bound or with a DNS or HTTP validation check. I'll choose the route that was the easiest for me, and then I'll briefly look at the other options. Prerequisites There are some prerequisites required, that are best discussed on their own. So we will continue with the assumption that you have these in order. valid Class A or CNAME domain name kubernetes cluster with ingress controller (such as nginx) with helm and tiller installed in the cluster web application Steps The steps to take to get a web application to get a certificate from Let's Encrypt are the following. install cert-manager from the official helm chart deploy a Issuer resource deploy a certificate resource confirm certificate and secret are created/filled use in web app Install Cert Manager For more details on Cert Manager, I recommend reading their introduction . In essence, it's a tool that helps you initiate a certificate request with a service such as Let's Encrypt. You can install it via Helm, and it's meant to be installed only once per cluster. The once per cluster restriction is due to the usage of Custom Resource Definitions (CRD's) which will block any (re-)installation. To confirm if there are any CRD's from cert-manager, you can issue the following command. 1 kubectl get customresourcedefinitions.apiextensions.k8s.io The CRD's belonging to cert-manager are the following: certificates.certmanager.k8s.io clusterissuers.certmanager.k8s.io issuers.certmanager.k8s.io You will notice later, that we will use these CRD's for getting our certificate, so keep them in mind. To remove them in case of a re-install, you can issue the command below. 1 2 3 4 kubectl delete customresourcedefinitions.apiextensions.k8s.io \\ certificates.certmanager.k8s.io \\ clusterissuers.certmanager.k8s.io \\ issuers.certmanager.k8s.io When you're sure there are no CRD's left, you can install cert-manager via it's helm chart. It has some options in case you need them, you read about them here , but in my case that wasn't needed. 1 helm install --name cert-manager --namespace default stable/cert-manager Deploy Issuer To be able to use a certificate we need to have a Certificate Issuer. If you remember from our cert-manager , there are two CRD's that can take this role: ClusterIssuer : clusterissuers.certmanager.k8s.io Issuer : issuers.certmanager.k8s.io Both issuer type can use two ways of providing the proof of ownership, either by dns-01 or http-01 . We'll be using the http-01 method, for the dns-01 method, refer to the cert-manager documenation . ClusterIssuer As the resource Kind implies, a ClusterIssuer is a cluster-wide resource and not bound to a specific namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : # The ACME server URL server : https://acme-staging.api.letsencrypt.org/directory # Email address used for ACME registration email : user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} Issuer Not everyone wants a cluster-wide resource, and not everyone has the rights to install something elsewhere than their namespace. I prefer having as much as possible tied to a namespace - either a team or an application - I will use this type. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : myapp-letsencrypt-staging namespace : myapp spec : acme : # The ACME server URL server : https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email : myadmin@myapp.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : myapp-letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} There's a few things to note here: server : this refers to the server executing the ACME test, in this case: Let's Encrypt Staging (with the v2 API) email : this will be the account it will use for registering the certificate privateKeySecretRef : this is the Kubernetes secret resource in which the privateKey will be stored, just in case you need or want to remove it Deploy Certificate Resource Next up is our Certificate resource, this is where cert-manager will store our certificate details to be used by our application. In case you forgot, this is one of the three CRD's provided by cert-manager. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : myapp-example-com namespace : myapp spec : secretName : myapp-tls dnsNames : - myapp.example.com acme : config : - http01 : ingressClass : nginx domains : - myapp.example.com issuerRef : name : myapp-letsencrypt-staging kind : Issuer The things to note here: name : so far I've found it a naming convention to write the domain name where - replaces the . 's. secretName : the name of the Kubernetes secret that will house the certificate and certificate key dnsNames : you can specify more than one name, in our case just a single one, should match acme.config.domains acme.config : this defines the configuration for how the ownership proof should be done, this should match the method defined in the Issuer issuerRef : in good Kubernetes fashion, we reference the Issuer that should issue our certificate, the name and kind should match our Issue resource Confirm Resources We have defined our set of resources that should create our valid - though not trusted, as it is staging - certificate. Before we use it, we should confirm our secret and our certificate are both valid. 1 kubectl describe certificate myapp-example-com --namespace myapp The response includes the latest status, which looks like this: 1 2 3 4 5 6 7 8 9 10 Status : Acme : Order : URL : https://acme-staging-v02.api.letsencrypt.org/acme/order/705.../960... Conditions : Last Transition Time : 2018-10-02T21:17:34Z Message : Certificate issued successfully Reason : CertIssued Status : True Type : Ready Next up is our secret, containing the actual certificate and the certificate key. 1 kubectl describe secret myapp-tls --namespace myapp Which results in something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Name: myapp-tls Namespace: myapp Labels: certmanager.k8s.io/certificate-name = myapp-example-com Annotations: certmanager.k8s.io/alt-names: myapp.example.com certmanager.k8s.io/common-name: myapp.example.com certmanager.k8s.io/issuer-kind: Issuer certmanager.k8s.io/issuer-name: myapp-letsencrypt-staging Type: kubernetes.io/tls Data ==== tls.crt: 3797 bytes tls.key: 1679 bytes Use certificate to enable https Assuming the secret and the certificate are correct, we can use them to enable https on our web app. We put the tls certificate on the ingress of the application, in this case, Nginx, which assumes the following about the app: it has a deployment or stateful set it has a service which provides and endpoint to one or more instances it has an nginx ingress which points to the service Deployment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 kind : Deployment apiVersion : apps/v1 metadata : name : myapp namespace : myapp labels : app : myapp spec : replicas : 3 selector : matchLabels : app : myapp template : metadata : labels : app : myapp spec : containers : - name : myapp image : caladreas/catnip-master imagePullPolicy : Always ports : - containerPort : 8087 You should always include livenessprobe, resource limits and so on. But for the sake of brevity, these are omitted! Service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : v1 kind : Service metadata : name : myapp namespace : myapp labels : app : myapp spec : selector : app : myapp ports : - name : http port : 80 targetPort : 8087 protocol : TCP Ingress for Issuer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/ssl-redirect : true certmanager.k8s.io/issuer : myapp-letsencrypt-staging certmanager.k8s.io/acme-challenge-type : http01 spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls Ingress for ClusterIssuer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/ssl-redirect : true certmanager.k8s.io/cluster-issuer : letsencrypt-staging certmanager.k8s.io/acme-challenge-type : http01 spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls Further resources How Does Let's Encrypt Work Tutorial that inspired this page Amazone EKS Ingress Guide Kuberetes EKS Ingress and TLS How To Configure LTS for Nginx Ingress","title":"Let's Encrypt for Kubernetes Apps"},{"location":"blogs/k8s-lets-encrypt/#lets-encrypt-for-kubernetes","text":"Let's Encrypt is a free, automated and open Certificate Authority. The kind of service you need if you want to have a secure website with https - yes I know that requires more than that - and it's now more straightforward to use than ever. This about using Let's Encrypt for generating a certificate for your service on Kubernetes. There are several ways to do this, with more or less automation, cluster-wide or namespace bound or with a DNS or HTTP validation check. I'll choose the route that was the easiest for me, and then I'll briefly look at the other options.","title":"Let's Encrypt for Kubernetes"},{"location":"blogs/k8s-lets-encrypt/#prerequisites","text":"There are some prerequisites required, that are best discussed on their own. So we will continue with the assumption that you have these in order. valid Class A or CNAME domain name kubernetes cluster with ingress controller (such as nginx) with helm and tiller installed in the cluster web application","title":"Prerequisites"},{"location":"blogs/k8s-lets-encrypt/#steps","text":"The steps to take to get a web application to get a certificate from Let's Encrypt are the following. install cert-manager from the official helm chart deploy a Issuer resource deploy a certificate resource confirm certificate and secret are created/filled use in web app","title":"Steps"},{"location":"blogs/k8s-lets-encrypt/#install-cert-manager","text":"For more details on Cert Manager, I recommend reading their introduction . In essence, it's a tool that helps you initiate a certificate request with a service such as Let's Encrypt. You can install it via Helm, and it's meant to be installed only once per cluster. The once per cluster restriction is due to the usage of Custom Resource Definitions (CRD's) which will block any (re-)installation. To confirm if there are any CRD's from cert-manager, you can issue the following command. 1 kubectl get customresourcedefinitions.apiextensions.k8s.io The CRD's belonging to cert-manager are the following: certificates.certmanager.k8s.io clusterissuers.certmanager.k8s.io issuers.certmanager.k8s.io You will notice later, that we will use these CRD's for getting our certificate, so keep them in mind. To remove them in case of a re-install, you can issue the command below. 1 2 3 4 kubectl delete customresourcedefinitions.apiextensions.k8s.io \\ certificates.certmanager.k8s.io \\ clusterissuers.certmanager.k8s.io \\ issuers.certmanager.k8s.io When you're sure there are no CRD's left, you can install cert-manager via it's helm chart. It has some options in case you need them, you read about them here , but in my case that wasn't needed. 1 helm install --name cert-manager --namespace default stable/cert-manager","title":"Install Cert Manager"},{"location":"blogs/k8s-lets-encrypt/#deploy-issuer","text":"To be able to use a certificate we need to have a Certificate Issuer. If you remember from our cert-manager , there are two CRD's that can take this role: ClusterIssuer : clusterissuers.certmanager.k8s.io Issuer : issuers.certmanager.k8s.io Both issuer type can use two ways of providing the proof of ownership, either by dns-01 or http-01 . We'll be using the http-01 method, for the dns-01 method, refer to the cert-manager documenation .","title":"Deploy Issuer"},{"location":"blogs/k8s-lets-encrypt/#clusterissuer","text":"As the resource Kind implies, a ClusterIssuer is a cluster-wide resource and not bound to a specific namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : # The ACME server URL server : https://acme-staging.api.letsencrypt.org/directory # Email address used for ACME registration email : user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {}","title":"ClusterIssuer"},{"location":"blogs/k8s-lets-encrypt/#issuer","text":"Not everyone wants a cluster-wide resource, and not everyone has the rights to install something elsewhere than their namespace. I prefer having as much as possible tied to a namespace - either a team or an application - I will use this type. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : myapp-letsencrypt-staging namespace : myapp spec : acme : # The ACME server URL server : https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email : myadmin@myapp.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : myapp-letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} There's a few things to note here: server : this refers to the server executing the ACME test, in this case: Let's Encrypt Staging (with the v2 API) email : this will be the account it will use for registering the certificate privateKeySecretRef : this is the Kubernetes secret resource in which the privateKey will be stored, just in case you need or want to remove it","title":"Issuer"},{"location":"blogs/k8s-lets-encrypt/#deploy-certificate-resource","text":"Next up is our Certificate resource, this is where cert-manager will store our certificate details to be used by our application. In case you forgot, this is one of the three CRD's provided by cert-manager. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : myapp-example-com namespace : myapp spec : secretName : myapp-tls dnsNames : - myapp.example.com acme : config : - http01 : ingressClass : nginx domains : - myapp.example.com issuerRef : name : myapp-letsencrypt-staging kind : Issuer The things to note here: name : so far I've found it a naming convention to write the domain name where - replaces the . 's. secretName : the name of the Kubernetes secret that will house the certificate and certificate key dnsNames : you can specify more than one name, in our case just a single one, should match acme.config.domains acme.config : this defines the configuration for how the ownership proof should be done, this should match the method defined in the Issuer issuerRef : in good Kubernetes fashion, we reference the Issuer that should issue our certificate, the name and kind should match our Issue resource","title":"Deploy Certificate Resource"},{"location":"blogs/k8s-lets-encrypt/#confirm-resources","text":"We have defined our set of resources that should create our valid - though not trusted, as it is staging - certificate. Before we use it, we should confirm our secret and our certificate are both valid. 1 kubectl describe certificate myapp-example-com --namespace myapp The response includes the latest status, which looks like this: 1 2 3 4 5 6 7 8 9 10 Status : Acme : Order : URL : https://acme-staging-v02.api.letsencrypt.org/acme/order/705.../960... Conditions : Last Transition Time : 2018-10-02T21:17:34Z Message : Certificate issued successfully Reason : CertIssued Status : True Type : Ready Next up is our secret, containing the actual certificate and the certificate key. 1 kubectl describe secret myapp-tls --namespace myapp Which results in something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Name: myapp-tls Namespace: myapp Labels: certmanager.k8s.io/certificate-name = myapp-example-com Annotations: certmanager.k8s.io/alt-names: myapp.example.com certmanager.k8s.io/common-name: myapp.example.com certmanager.k8s.io/issuer-kind: Issuer certmanager.k8s.io/issuer-name: myapp-letsencrypt-staging Type: kubernetes.io/tls Data ==== tls.crt: 3797 bytes tls.key: 1679 bytes","title":"Confirm Resources"},{"location":"blogs/k8s-lets-encrypt/#use-certificate-to-enable-https","text":"Assuming the secret and the certificate are correct, we can use them to enable https on our web app. We put the tls certificate on the ingress of the application, in this case, Nginx, which assumes the following about the app: it has a deployment or stateful set it has a service which provides and endpoint to one or more instances it has an nginx ingress which points to the service","title":"Use certificate to enable https"},{"location":"blogs/k8s-lets-encrypt/#deployment","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 kind : Deployment apiVersion : apps/v1 metadata : name : myapp namespace : myapp labels : app : myapp spec : replicas : 3 selector : matchLabels : app : myapp template : metadata : labels : app : myapp spec : containers : - name : myapp image : caladreas/catnip-master imagePullPolicy : Always ports : - containerPort : 8087 You should always include livenessprobe, resource limits and so on. But for the sake of brevity, these are omitted!","title":"Deployment"},{"location":"blogs/k8s-lets-encrypt/#service","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : v1 kind : Service metadata : name : myapp namespace : myapp labels : app : myapp spec : selector : app : myapp ports : - name : http port : 80 targetPort : 8087 protocol : TCP","title":"Service"},{"location":"blogs/k8s-lets-encrypt/#ingress-for-issuer","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/ssl-redirect : true certmanager.k8s.io/issuer : myapp-letsencrypt-staging certmanager.k8s.io/acme-challenge-type : http01 spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls","title":"Ingress for Issuer"},{"location":"blogs/k8s-lets-encrypt/#ingress-for-clusterissuer","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/ssl-redirect : true certmanager.k8s.io/cluster-issuer : letsencrypt-staging certmanager.k8s.io/acme-challenge-type : http01 spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls","title":"Ingress for ClusterIssuer"},{"location":"blogs/k8s-lets-encrypt/#further-resources","text":"How Does Let's Encrypt Work Tutorial that inspired this page Amazone EKS Ingress Guide Kuberetes EKS Ingress and TLS How To Configure LTS for Nginx Ingress","title":"Further resources"},{"location":"blogs/kubernetes-post-install/","text":"Kubernetes Post Install What to do after you've installed your Kubernetes cluster, whether that was EKS via eksctl or GKE via gcloud. make network more secure with encryption weavenet for example install package manager install helm tiller use nginx for ssl termination together with Let's Encrypt install nginx install cert-manager Helm Tiller Helm is the defacto standard package manager for Kubernetes. Its current iteration is version 2, which has a client component - Helm - and a serverside component, Tiller. There's a problem with that, due this setup with Helm and Tiller, Tiller is aking to a cluster admin. This isn't very secure and there are several ways around that. JenkinsX : its binary ( jx ) can install helm charts without using Tiller. It generates the kubernetes resource files and installs these directly custom RBAC setup : you can also setup RBAC in such a way that every separate namespace gets its own Tiller, limiting the reach of any Tiller Tiller Custom RBAC Example Namespaces 1 2 3 4 5 6 7 8 9 10 11 12 13 14 kind : Namespace apiVersion : v1 metadata : name : sre --- kind : Namespace apiVersion : v1 metadata : name : dev1 --- kind : Namespace apiVersion : v1 metadata : name : dev2 Service Accounts 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kind : ServiceAccount apiVersion : v1 metadata : name : tiller namespace : dev1 --- kind : ServiceAccount apiVersion : v1 metadata : name : tiller namespace : dev2 --- kind : ServiceAccount apiVersion : v1 metadata : name : helm namespace : sre Roles 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-manager namespace : dev1 rules : - apiGroups : [ , batch , extensions , apps ] resources : [ * ] verbs : [ * ] --- kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-manager namespace : dev2 rules : - apiGroups : [ , batch , extensions , apps ] resources : [ * ] verbs : [ * ] --- kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1 metadata : name : helm-clusterrole rules : - apiGroups : [ ] resources : [ pods/portforward ] verbs : [ create ] - apiGroups : [ ] resources : [ pods ] verbs : [ list , get ] RoleBindings 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-binding namespace : dev1 subjects : - kind : ServiceAccount name : tiller namespace : dev1 roleRef : kind : Role name : tiller-manager apiGroup : rbac.authorization.k8s.io --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-binding namespace : dev2 subjects : - kind : ServiceAccount name : tiller namespace : dev2 roleRef : kind : Role name : tiller-manager apiGroup : rbac.authorization.k8s.io --- kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : helm-clusterrolebinding roleRef : kind : ClusterRole apiGroup : rbac.authorization.k8s.io name : helm-clusterrole subjects : - kind : ServiceAccount name : helm namespace : sre Install Tiller 1 2 helm init --service-account tiller --tiller-namespace dev1 helm init --service-account tiller --tiller-namespace dev2 Create KubeConfig for Helm client 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # Find the secret associated with the Service Account SECRET = $( kubectl -n sre get sa helm -o jsonpath = {.secrets[].name} ) # Retrieve the token from the secret and decode it TOKEN = $( kubectl get secrets -n sre $SECRET -o jsonpath = {.data.token} | base64 -D ) # Retrieve the CA from the secret, decode it and write it to disk kubectl get secrets -n sre $SECRET -o jsonpath = {.data.ca\\.crt} | base64 -D ca.crt # Retrieve the current context CONTEXT = $( kubectl config current-context ) # Retrieve the cluster name CLUSTER_NAME = $( kubectl config get-contexts $CONTEXT --no-headers = true | awk {print $3} ) # Retrieve the API endpoint SERVER = $( kubectl config view -o jsonpath = {.clusters[?(@.name == \\ ${ CLUSTER_NAME } \\ )].cluster.server} ) # Set up variables KUBECONFIG_FILE = config USER = helm CA = ca.crt # Set up config kubectl config set-cluster $CLUSTER_NAME \\ --kubeconfig = $KUBECONFIG_FILE \\ --server = $SERVER \\ --certificate-authority = $CA \\ --embed-certs = true # Set token credentials kubectl config set-credentials \\ $USER \\ --kubeconfig = $KUBECONFIG_FILE \\ --token = $TOKEN # Set context entry kubectl config set-context \\ $USER \\ --kubeconfig = $KUBECONFIG_FILE \\ --cluster = $CLUSTER_NAME \\ --user = $USER # Set the current-context kubectl config use-context $USER \\ --kubeconfig = $KUBECONFIG_FILE Helm Install 1 2 3 4 5 6 7 8 9 10 11 12 helm install \\ --name prometheus \\ stable/prometheus \\ --tiller-namespace dev1 \\ --kubeconfig config \\ --namespace dev1 \\ --set rbac.create = false NAME: prometheus LAST DEPLOYED: Sun Oct 28 16 :22:46 2018 NAMESPACE: dev1 STATUS: DEPLOYED 1 2 3 4 5 6 7 8 9 10 11 12 helm install --name grafana \\ stable/grafana \\ --tiller-namespace dev2 \\ --kubeconfig config \\ --namespace dev2 \\ --set rbac.pspEnabled = false \\ --set rbac.create = false NAME: grafana LAST DEPLOYED: Sun Oct 28 16 :25:18 2018 NAMESPACE: dev2 STATUS: DEPLOYED References https://medium.com/@elijudah/configuring-minimal-rbac-permissions-for-helm-and-tiller-e7d792511d10 https://medium.com/virtuslab/think-twice-before-using-helm-25fbb18bc822 https://jenkins-x.io/architecture/helm3/ https://gist.github.com/innovia/fbba8259042f71db98ea8d4ad19bd708#file-kubernetes_add_service_account_kubeconfig-sh","title":"Kubernetes Post Install"},{"location":"blogs/kubernetes-post-install/#kubernetes-post-install","text":"What to do after you've installed your Kubernetes cluster, whether that was EKS via eksctl or GKE via gcloud. make network more secure with encryption weavenet for example install package manager install helm tiller use nginx for ssl termination together with Let's Encrypt install nginx install cert-manager","title":"Kubernetes Post Install"},{"location":"blogs/kubernetes-post-install/#helm-tiller","text":"Helm is the defacto standard package manager for Kubernetes. Its current iteration is version 2, which has a client component - Helm - and a serverside component, Tiller. There's a problem with that, due this setup with Helm and Tiller, Tiller is aking to a cluster admin. This isn't very secure and there are several ways around that. JenkinsX : its binary ( jx ) can install helm charts without using Tiller. It generates the kubernetes resource files and installs these directly custom RBAC setup : you can also setup RBAC in such a way that every separate namespace gets its own Tiller, limiting the reach of any Tiller","title":"Helm &amp; Tiller"},{"location":"blogs/kubernetes-post-install/#tiller-custom-rbac-example","text":"","title":"Tiller Custom RBAC Example"},{"location":"blogs/kubernetes-post-install/#namespaces","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 kind : Namespace apiVersion : v1 metadata : name : sre --- kind : Namespace apiVersion : v1 metadata : name : dev1 --- kind : Namespace apiVersion : v1 metadata : name : dev2","title":"Namespaces"},{"location":"blogs/kubernetes-post-install/#service-accounts","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kind : ServiceAccount apiVersion : v1 metadata : name : tiller namespace : dev1 --- kind : ServiceAccount apiVersion : v1 metadata : name : tiller namespace : dev2 --- kind : ServiceAccount apiVersion : v1 metadata : name : helm namespace : sre","title":"Service Accounts"},{"location":"blogs/kubernetes-post-install/#roles","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-manager namespace : dev1 rules : - apiGroups : [ , batch , extensions , apps ] resources : [ * ] verbs : [ * ] --- kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-manager namespace : dev2 rules : - apiGroups : [ , batch , extensions , apps ] resources : [ * ] verbs : [ * ] --- kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1 metadata : name : helm-clusterrole rules : - apiGroups : [ ] resources : [ pods/portforward ] verbs : [ create ] - apiGroups : [ ] resources : [ pods ] verbs : [ list , get ]","title":"Roles"},{"location":"blogs/kubernetes-post-install/#rolebindings","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-binding namespace : dev1 subjects : - kind : ServiceAccount name : tiller namespace : dev1 roleRef : kind : Role name : tiller-manager apiGroup : rbac.authorization.k8s.io --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : tiller-binding namespace : dev2 subjects : - kind : ServiceAccount name : tiller namespace : dev2 roleRef : kind : Role name : tiller-manager apiGroup : rbac.authorization.k8s.io --- kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : helm-clusterrolebinding roleRef : kind : ClusterRole apiGroup : rbac.authorization.k8s.io name : helm-clusterrole subjects : - kind : ServiceAccount name : helm namespace : sre","title":"RoleBindings"},{"location":"blogs/kubernetes-post-install/#install-tiller","text":"1 2 helm init --service-account tiller --tiller-namespace dev1 helm init --service-account tiller --tiller-namespace dev2","title":"Install Tiller"},{"location":"blogs/kubernetes-post-install/#create-kubeconfig-for-helm-client","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # Find the secret associated with the Service Account SECRET = $( kubectl -n sre get sa helm -o jsonpath = {.secrets[].name} ) # Retrieve the token from the secret and decode it TOKEN = $( kubectl get secrets -n sre $SECRET -o jsonpath = {.data.token} | base64 -D ) # Retrieve the CA from the secret, decode it and write it to disk kubectl get secrets -n sre $SECRET -o jsonpath = {.data.ca\\.crt} | base64 -D ca.crt # Retrieve the current context CONTEXT = $( kubectl config current-context ) # Retrieve the cluster name CLUSTER_NAME = $( kubectl config get-contexts $CONTEXT --no-headers = true | awk {print $3} ) # Retrieve the API endpoint SERVER = $( kubectl config view -o jsonpath = {.clusters[?(@.name == \\ ${ CLUSTER_NAME } \\ )].cluster.server} ) # Set up variables KUBECONFIG_FILE = config USER = helm CA = ca.crt # Set up config kubectl config set-cluster $CLUSTER_NAME \\ --kubeconfig = $KUBECONFIG_FILE \\ --server = $SERVER \\ --certificate-authority = $CA \\ --embed-certs = true # Set token credentials kubectl config set-credentials \\ $USER \\ --kubeconfig = $KUBECONFIG_FILE \\ --token = $TOKEN # Set context entry kubectl config set-context \\ $USER \\ --kubeconfig = $KUBECONFIG_FILE \\ --cluster = $CLUSTER_NAME \\ --user = $USER # Set the current-context kubectl config use-context $USER \\ --kubeconfig = $KUBECONFIG_FILE","title":"Create KubeConfig for Helm client"},{"location":"blogs/kubernetes-post-install/#helm-install","text":"1 2 3 4 5 6 7 8 9 10 11 12 helm install \\ --name prometheus \\ stable/prometheus \\ --tiller-namespace dev1 \\ --kubeconfig config \\ --namespace dev1 \\ --set rbac.create = false NAME: prometheus LAST DEPLOYED: Sun Oct 28 16 :22:46 2018 NAMESPACE: dev1 STATUS: DEPLOYED 1 2 3 4 5 6 7 8 9 10 11 12 helm install --name grafana \\ stable/grafana \\ --tiller-namespace dev2 \\ --kubeconfig config \\ --namespace dev2 \\ --set rbac.pspEnabled = false \\ --set rbac.create = false NAME: grafana LAST DEPLOYED: Sun Oct 28 16 :25:18 2018 NAMESPACE: dev2 STATUS: DEPLOYED","title":"Helm Install"},{"location":"blogs/kubernetes-post-install/#references","text":"https://medium.com/@elijudah/configuring-minimal-rbac-permissions-for-helm-and-tiller-e7d792511d10 https://medium.com/virtuslab/think-twice-before-using-helm-25fbb18bc822 https://jenkins-x.io/architecture/helm3/ https://gist.github.com/innovia/fbba8259042f71db98ea8d4ad19bd708#file-kubernetes_add_service_account_kubeconfig-sh","title":"References"},{"location":"certificates/lets-encrypt-k8s/","text":"Let's Encrypt for Kubernetes Let's Encrypt is a free, automated and open Certificate Authority. The kind of service you need if you want to have a secure website with https - yes I know that requires more than that - and it's now more straightforward to use than ever. This about using Let's Encrypt for generating a certificate for your service on Kubernetes. There are several ways to do this, with more or less automation, cluster-wide or namespace bound or with a DNS or HTTP validation check. I'll choose the route that was the easiest for me, and then I'll briefly look at the other options. Prerequisites There are some prerequisites required, that are best discussed on their own. So we will continue with the assumption that you have these in order. valid Class A or CNAME domain name kubernetes cluster with ingress controller (such as nginx) with helm and tiller installed in the cluster web application Steps The steps to take to get a web application to get a certificate from Let's Encrypt are the following. install cert-manager from the official helm chart deploy a Issuer resource deploy a certificate resource confirm certificate and secret are created/filled use in web app Install Cert Manager For more details on Cert Manager, I recommend reading their introduction . In essence, it's a tool that helps you initiate a certificate request with a service such as Let's Encrypt. You can install it via Helm, and it's meant to be installed only once per cluster. The once per cluster restriction is due to the usage of Custom Resource Definitions (CRD's) which will block any (re-)installation. To confirm if there are any CRD's from cert-manager, you can issue the following command. 1 kubectl get customresourcedefinitions.apiextensions.k8s.io The CRD's belonging to cert-manager are the following: certificates.certmanager.k8s.io clusterissuers.certmanager.k8s.io issuers.certmanager.k8s.io You will notice later, that we will use these CRD's for getting our certificate, so keep them in mind. To remove them in case of a re-install, you can issue the command below. 1 2 3 4 kubectl delete customresourcedefinitions.apiextensions.k8s.io \\ certificates.certmanager.k8s.io \\ clusterissuers.certmanager.k8s.io \\ issuers.certmanager.k8s.io When you're sure there are no CRD's left, you can install cert-manager via it's helm chart. It has some options in case you need them, you read about them here , but in my case that wasn't needed. 1 helm install --name cert-manager --namespace default stable/cert-manager Deploy Issuer To be able to use a certificate we need to have a Certificate Issuer. If you remember from our cert-manager , there are two CRD's that can take this role: ClusterIssuer : clusterissuers.certmanager.k8s.io Issuer : issuers.certmanager.k8s.io Both issuer type can use two ways of providing the proof of ownership, either by dns-01 or http-01 . We'll be using the http-01 method, for the dns-01 method, refer to the cert-manager documenation . ClusterIssuer As the resource Kind implies, a ClusterIssuer is a cluster-wide resource and not bound to a specific namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : # The ACME server URL server : https://acme-staging.api.letsencrypt.org/directory # Email address used for ACME registration email : user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} Issuer Not everyone wants a cluster-wide resource, and not everyone has the rights to install something elsewhere than their namespace. I prefer having as much as possible tied to a namespace - either a team or an application - I will use this type. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : myapp-letsencrypt-staging namespace : myapp spec : acme : # The ACME server URL server : https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email : myadmin@myapp.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : myapp-letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} There's a few things to note here: server : this refers to the server executing the ACME test, in this case: Let's Encrypt Staging (with the v2 API) email : this will be the account it will use for registering the certificate privateKeySecretRef : this is the Kubernetes secret resource in which the privateKey will be stored, just in case you need or want to remove it Deploy Certificate Resource Next up is our Certificate resource, this is where cert-manager will store our certificate details to be used by our application. In case you forgot, this is one of the three CRD's provided by cert-manager. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : myapp-example-com namespace : myapp spec : secretName : myapp-tls dnsNames : - myapp.example.com acme : config : - http01 : ingressClass : nginx domains : - myapp.example.com issuerRef : name : myapp-letsencrypt-staging kind : Issuer The things to note here: name : so far I've found it a naming convention to write the domain name where - replaces the . 's. secretName : the name of the Kubernetes secret that will house the certificate and certificate key dnsNames : you can specify more than one name, in our case just a single one, should match acme.config.domains acme.config : this defines the configuration for how the ownership proof should be done, this should match the method defined in the Issuer issuerRef : in good Kubernetes fashion, we reference the Issuer that should issue our certificate, the name and kind should match our Issue resource Confirm Resources We have defined our set of resources that should create our valid - though not trusted, as it is staging - certificate. Before we use it, we should confirm our secret and our certificate are both valid. 1 kubectl describe certificate myapp-example-com --namespace myapp The response includes the latest status, which looks like this: 1 2 3 4 5 6 7 8 9 10 Status : Acme : Order : URL : https://acme-staging-v02.api.letsencrypt.org/acme/order/705.../960... Conditions : Last Transition Time : 2018-10-02T21:17:34Z Message : Certificate issued successfully Reason : CertIssued Status : True Type : Ready Next up is our secret, containing the actual certificate and the certificate key. 1 kubectl describe secret myapp-tls --namespace myapp Which results in something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Name: myapp-tls Namespace: myapp Labels: certmanager.k8s.io/certificate-name = myapp-example-com Annotations: certmanager.k8s.io/alt-names: myapp.example.com certmanager.k8s.io/common-name: myapp.example.com certmanager.k8s.io/issuer-kind: Issuer certmanager.k8s.io/issuer-name: myapp-letsencrypt-staging Type: kubernetes.io/tls Data ==== tls.crt: 3797 bytes tls.key: 1679 bytes Use certificate to enable https Assuming the secret and the certificate are correct, we can use them to enable https on our web app. We put the tls certificate on the ingress of the application, in this case, Nginx, which assumes the following about the app: it has a deployment or stateful set it has a service which provides and endpoint to one or more instances it has an nginx ingress which points to the service Deployment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 kind : Deployment apiVersion : apps/v1 metadata : name : myapp namespace : myapp labels : app : myapp spec : replicas : 3 selector : matchLabels : app : myapp template : metadata : labels : app : myapp spec : containers : - name : myapp image : caladreas/catnip-master imagePullPolicy : Always ports : - containerPort : 8087 You should always include livenessprobe, resource limits and so on. But for the sake of brevity, these are omitted! Service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : v1 kind : Service metadata : name : myapp namespace : myapp labels : app : myapp spec : selector : app : myapp ports : - name : http port : 80 targetPort : 8087 protocol : TCP Ingress 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/ssl-redirect : true certmanager.k8s.io/issuer-kind : Issuer certmanager.k8s.io/issuer-name : myapp-letsencrypt-staging spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls Further resources How Does Let's Encrypt Work Tutorial that inspired this page Amazone EKS Ingress Guide Kuberetes EKS Ingress and TLS How To Configure LTS for Nginx Ingress","title":"Let's Encrypt K8S"},{"location":"certificates/lets-encrypt-k8s/#lets-encrypt-for-kubernetes","text":"Let's Encrypt is a free, automated and open Certificate Authority. The kind of service you need if you want to have a secure website with https - yes I know that requires more than that - and it's now more straightforward to use than ever. This about using Let's Encrypt for generating a certificate for your service on Kubernetes. There are several ways to do this, with more or less automation, cluster-wide or namespace bound or with a DNS or HTTP validation check. I'll choose the route that was the easiest for me, and then I'll briefly look at the other options.","title":"Let's Encrypt for Kubernetes"},{"location":"certificates/lets-encrypt-k8s/#prerequisites","text":"There are some prerequisites required, that are best discussed on their own. So we will continue with the assumption that you have these in order. valid Class A or CNAME domain name kubernetes cluster with ingress controller (such as nginx) with helm and tiller installed in the cluster web application","title":"Prerequisites"},{"location":"certificates/lets-encrypt-k8s/#steps","text":"The steps to take to get a web application to get a certificate from Let's Encrypt are the following. install cert-manager from the official helm chart deploy a Issuer resource deploy a certificate resource confirm certificate and secret are created/filled use in web app","title":"Steps"},{"location":"certificates/lets-encrypt-k8s/#install-cert-manager","text":"For more details on Cert Manager, I recommend reading their introduction . In essence, it's a tool that helps you initiate a certificate request with a service such as Let's Encrypt. You can install it via Helm, and it's meant to be installed only once per cluster. The once per cluster restriction is due to the usage of Custom Resource Definitions (CRD's) which will block any (re-)installation. To confirm if there are any CRD's from cert-manager, you can issue the following command. 1 kubectl get customresourcedefinitions.apiextensions.k8s.io The CRD's belonging to cert-manager are the following: certificates.certmanager.k8s.io clusterissuers.certmanager.k8s.io issuers.certmanager.k8s.io You will notice later, that we will use these CRD's for getting our certificate, so keep them in mind. To remove them in case of a re-install, you can issue the command below. 1 2 3 4 kubectl delete customresourcedefinitions.apiextensions.k8s.io \\ certificates.certmanager.k8s.io \\ clusterissuers.certmanager.k8s.io \\ issuers.certmanager.k8s.io When you're sure there are no CRD's left, you can install cert-manager via it's helm chart. It has some options in case you need them, you read about them here , but in my case that wasn't needed. 1 helm install --name cert-manager --namespace default stable/cert-manager","title":"Install Cert Manager"},{"location":"certificates/lets-encrypt-k8s/#deploy-issuer","text":"To be able to use a certificate we need to have a Certificate Issuer. If you remember from our cert-manager , there are two CRD's that can take this role: ClusterIssuer : clusterissuers.certmanager.k8s.io Issuer : issuers.certmanager.k8s.io Both issuer type can use two ways of providing the proof of ownership, either by dns-01 or http-01 . We'll be using the http-01 method, for the dns-01 method, refer to the cert-manager documenation .","title":"Deploy Issuer"},{"location":"certificates/lets-encrypt-k8s/#clusterissuer","text":"As the resource Kind implies, a ClusterIssuer is a cluster-wide resource and not bound to a specific namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : certmanager.k8s.io/v1alpha1 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : # The ACME server URL server : https://acme-staging.api.letsencrypt.org/directory # Email address used for ACME registration email : user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {}","title":"ClusterIssuer"},{"location":"certificates/lets-encrypt-k8s/#issuer","text":"Not everyone wants a cluster-wide resource, and not everyone has the rights to install something elsewhere than their namespace. I prefer having as much as possible tied to a namespace - either a team or an application - I will use this type. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : certmanager.k8s.io/v1alpha1 kind : Issuer metadata : name : myapp-letsencrypt-staging namespace : myapp spec : acme : # The ACME server URL server : https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email : myadmin@myapp.com # Name of a secret used to store the ACME account private key privateKeySecretRef : name : myapp-letsencrypt-staging # Enable the HTTP-01 challenge provider http01 : {} There's a few things to note here: server : this refers to the server executing the ACME test, in this case: Let's Encrypt Staging (with the v2 API) email : this will be the account it will use for registering the certificate privateKeySecretRef : this is the Kubernetes secret resource in which the privateKey will be stored, just in case you need or want to remove it","title":"Issuer"},{"location":"certificates/lets-encrypt-k8s/#deploy-certificate-resource","text":"Next up is our Certificate resource, this is where cert-manager will store our certificate details to be used by our application. In case you forgot, this is one of the three CRD's provided by cert-manager. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : certmanager.k8s.io/v1alpha1 kind : Certificate metadata : name : myapp-example-com namespace : myapp spec : secretName : myapp-tls dnsNames : - myapp.example.com acme : config : - http01 : ingressClass : nginx domains : - myapp.example.com issuerRef : name : myapp-letsencrypt-staging kind : Issuer The things to note here: name : so far I've found it a naming convention to write the domain name where - replaces the . 's. secretName : the name of the Kubernetes secret that will house the certificate and certificate key dnsNames : you can specify more than one name, in our case just a single one, should match acme.config.domains acme.config : this defines the configuration for how the ownership proof should be done, this should match the method defined in the Issuer issuerRef : in good Kubernetes fashion, we reference the Issuer that should issue our certificate, the name and kind should match our Issue resource","title":"Deploy Certificate Resource"},{"location":"certificates/lets-encrypt-k8s/#confirm-resources","text":"We have defined our set of resources that should create our valid - though not trusted, as it is staging - certificate. Before we use it, we should confirm our secret and our certificate are both valid. 1 kubectl describe certificate myapp-example-com --namespace myapp The response includes the latest status, which looks like this: 1 2 3 4 5 6 7 8 9 10 Status : Acme : Order : URL : https://acme-staging-v02.api.letsencrypt.org/acme/order/705.../960... Conditions : Last Transition Time : 2018-10-02T21:17:34Z Message : Certificate issued successfully Reason : CertIssued Status : True Type : Ready Next up is our secret, containing the actual certificate and the certificate key. 1 kubectl describe secret myapp-tls --namespace myapp Which results in something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Name: myapp-tls Namespace: myapp Labels: certmanager.k8s.io/certificate-name = myapp-example-com Annotations: certmanager.k8s.io/alt-names: myapp.example.com certmanager.k8s.io/common-name: myapp.example.com certmanager.k8s.io/issuer-kind: Issuer certmanager.k8s.io/issuer-name: myapp-letsencrypt-staging Type: kubernetes.io/tls Data ==== tls.crt: 3797 bytes tls.key: 1679 bytes","title":"Confirm Resources"},{"location":"certificates/lets-encrypt-k8s/#use-certificate-to-enable-https","text":"Assuming the secret and the certificate are correct, we can use them to enable https on our web app. We put the tls certificate on the ingress of the application, in this case, Nginx, which assumes the following about the app: it has a deployment or stateful set it has a service which provides and endpoint to one or more instances it has an nginx ingress which points to the service","title":"Use certificate to enable https"},{"location":"certificates/lets-encrypt-k8s/#deployment","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 kind : Deployment apiVersion : apps/v1 metadata : name : myapp namespace : myapp labels : app : myapp spec : replicas : 3 selector : matchLabels : app : myapp template : metadata : labels : app : myapp spec : containers : - name : myapp image : caladreas/catnip-master imagePullPolicy : Always ports : - containerPort : 8087 You should always include livenessprobe, resource limits and so on. But for the sake of brevity, these are omitted!","title":"Deployment"},{"location":"certificates/lets-encrypt-k8s/#service","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : v1 kind : Service metadata : name : myapp namespace : myapp labels : app : myapp spec : selector : app : myapp ports : - name : http port : 80 targetPort : 8087 protocol : TCP","title":"Service"},{"location":"certificates/lets-encrypt-k8s/#ingress","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : myapp namespace : myapp annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/ssl-redirect : true certmanager.k8s.io/issuer-kind : Issuer certmanager.k8s.io/issuer-name : myapp-letsencrypt-staging spec : rules : - host : myapp.example.com http : paths : - path : / backend : serviceName : myapp servicePort : 80 tls : - hosts : - myapp.example.com secretName : myapp-tls","title":"Ingress"},{"location":"certificates/lets-encrypt-k8s/#further-resources","text":"How Does Let's Encrypt Work Tutorial that inspired this page Amazone EKS Ingress Guide Kuberetes EKS Ingress and TLS How To Configure LTS for Nginx Ingress","title":"Further resources"},{"location":"cloudbees/cbc-eks/","text":"CloudBees Core on AWS EKS https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/eks-install/# https://go.cloudbees.com/docs/cloudbees-core/cloud-reference-architecture/ra-for-eks/#_ingress_tls_termination","title":"CloudBees Core on EKS"},{"location":"cloudbees/cbc-eks/#cloudbees-core-on-aws-eks","text":"https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/eks-install/# https://go.cloudbees.com/docs/cloudbees-core/cloud-reference-architecture/ra-for-eks/#_ingress_tls_termination","title":"CloudBees Core on AWS EKS"},{"location":"cloudbees/cbc-post-install-tips/","text":"CloudBees Core Post Install Tips","title":"CloudBees Core Post Install"},{"location":"cloudbees/cbc-post-install-tips/#cloudbees-core-post-install-tips","text":"","title":"CloudBees Core Post Install Tips"},{"location":"devops/","text":"DevOps Assessment How can I assess an organisation for what to do next. Questions To which extent can your development teams request/create an environment on their own, without going through lengthy approval processes? To which extent can your development teams use pre-configured/ template tool sets (e.g. Jenkins master jobs, master POM etc) which they can extend and/or modify to their needs? To which extent can your developments teams deploy to any environment (including production)? If not, what do they lack: knowledge or passwords to higher environments? Does your system of record provide you tractability from idea to production? How tightly coupled are your key delivery pipeline tools? Is it easy to replace them? Do you have different release management activities based on application blocks? Who is keeping it up-to-date? Maturity Model Resources https://www.devon.nl/continuous-delivery-at-enterprise-level-pitfall-1/ https://www.devon.nl/continuous-delivery-at-enterprise-level-pitfall-2/ https://devops-research.com/ References","title":"DevOps Assessment"},{"location":"devops/#devops-assessment","text":"How can I assess an organisation for what to do next.","title":"DevOps Assessment"},{"location":"devops/#questions","text":"To which extent can your development teams request/create an environment on their own, without going through lengthy approval processes? To which extent can your development teams use pre-configured/ template tool sets (e.g. Jenkins master jobs, master POM etc) which they can extend and/or modify to their needs? To which extent can your developments teams deploy to any environment (including production)? If not, what do they lack: knowledge or passwords to higher environments? Does your system of record provide you tractability from idea to production? How tightly coupled are your key delivery pipeline tools? Is it easy to replace them? Do you have different release management activities based on application blocks? Who is keeping it up-to-date?","title":"Questions"},{"location":"devops/#maturity-model","text":"","title":"Maturity Model"},{"location":"devops/#resources","text":"https://www.devon.nl/continuous-delivery-at-enterprise-level-pitfall-1/ https://www.devon.nl/continuous-delivery-at-enterprise-level-pitfall-2/ https://devops-research.com/","title":"Resources"},{"location":"devops/#references","text":"","title":"References"},{"location":"devops/progressive-delivery/","text":"Progressive Delivery Resources https://redmonk.com/jgovernor/2018/08/06/towards-progressive-delivery/ https://chrisshort.tumblr.com/post/176701070950/recommended-read-towards-progressive-delivery https://dzone.com/articles/gitops-workflows-for-istio-canary-deployments https://launchdarkly.com/blog/progressive-delivery-a-history-condensed/ https://tech.target.com/infrastructure/2018/06/20/enter-unimatrix.html https://dzone.com/articles/deployments-at-scale-using-kubernetes-and-launchda https://blog.csanchez.org/2019/01/22/progressive-delivery-in-kubernetes-blue-green-and-canary-deployments/ https://blog.csanchez.org/2019/01/24/progressive-delivery-with-jenkins-x/ https://blog.csanchez.org/2019/03/05/progressive-delivery-with-jenkins-x-automatic-canary-deployments/ https://devblogs.microsoft.com/devops/configuring-your-release-pipelines-for-safe-deployments/ https://www.linkedin.com/pulse/counting-down-zero-time-takes-launch-app-target-tom-kadlec-1/","title":"Progressive Delivery"},{"location":"devops/progressive-delivery/#progressive-delivery","text":"","title":"Progressive Delivery"},{"location":"devops/progressive-delivery/#resources","text":"https://redmonk.com/jgovernor/2018/08/06/towards-progressive-delivery/ https://chrisshort.tumblr.com/post/176701070950/recommended-read-towards-progressive-delivery https://dzone.com/articles/gitops-workflows-for-istio-canary-deployments https://launchdarkly.com/blog/progressive-delivery-a-history-condensed/ https://tech.target.com/infrastructure/2018/06/20/enter-unimatrix.html https://dzone.com/articles/deployments-at-scale-using-kubernetes-and-launchda https://blog.csanchez.org/2019/01/22/progressive-delivery-in-kubernetes-blue-green-and-canary-deployments/ https://blog.csanchez.org/2019/01/24/progressive-delivery-with-jenkins-x/ https://blog.csanchez.org/2019/03/05/progressive-delivery-with-jenkins-x-automatic-canary-deployments/ https://devblogs.microsoft.com/devops/configuring-your-release-pipelines-for-safe-deployments/ https://www.linkedin.com/pulse/counting-down-zero-time-takes-launch-app-target-tom-kadlec-1/","title":"Resources"},{"location":"docker/graceful-shutdown/","text":"Docker Graceful shutdown The case for graceful shutdown We can speak about the graceful shutdown of our application, when all of the resources it used and all of the traffic and/or data processing what it handled are closed and released properly. It means that no database connection remains open and no ongoing request fails because we stop our application. 1 I thank P\u00e9ter M\u00e1rton for the quote and giving a nice case for the graceful shutdown and docker. Where his blog post goes into how to do this with/for Kubernetes, I will do this for Docker's Swarm (mode) orchestrator. So, the case for graceful shutdown. As the quote shows, what we mean with it, is that an application shuts down gracefully if it cleans up all its mess. All things considered, I think most people would agree that cleaning up your mess - resources, connections or saying goodbye is preferred above just disappearing. Shutting down nicely and leaving nothing behind will reduce the amount of potential (hard to debug) errors. It also allows other applications or services to reliably know when you are there and when you're not there. Not every application will have such dependencies (to it), but in today's cluster environments with many moving parts you'll never know. So I would recommend to always do a graceful shutdown if you're able. In the light of Docker, that might be a bit different than you're use to. Exec (form) vs Shell (form) There are several ways to run a command in a Dockerfile . These are are: RUN : runs a command during the docker build phase CMD : runs a command when the container gets started ENTRYPOINT : provides the location from where commands get run when the container starts Note You need at least one ENTRYPOINT or CMD in a Dockerfile for it to be valid. They can be used in collaboration but they can do similar things. Al these commands can be put in both a shell form and a exec form 2 . For more information on these commands you should check out John Zaccone's blog on Entrypoint vs CMD . In summary, the shell form will run the command as a shell command and spawn a process via /bin/sh -c . Whereas the exec form will execute a child process that is still attached to PID1 4 . This means that if you run the examples below, you will notice that you cannot ctrl+c out of the shell form, but you can out of the exec form. Exec form is the recommended form to use and is a requirement for graceful shutdown. Below you'll find some further reading on the CMD and ENTRYPOINT commands 5 6 . Shell form example 1 2 FROM alpine ENTRYPOINT ping www.google.com # shell format 3 Exec form example 1 2 FROM alpine ENTRYPOINT [ ping , www.google.com ] # exec format 3 PID1 Now you run your commands nicely as PID1 and make sure it is your process that receives the SIGNALS . Then all is good for a while, but at one point you will run into the problems of either having zombie child processes or processes that aren't designed for running as PID1 . This means you will have to do something about this, and luckily there's already some people who have done this for you. There's tini : a tiny initialization system designed for Docker. If used with the exec form it will run as PID1 and will manage your process and its child processes for you. For what it adds exactly and why it was introduced, you can rever to this excellent explanation from its creator . In fact, tini was so successful, that docker included it in docker ! While it works for docker run and docker compose it doesn't yet work for Docker Swarm stacks. Example docker run with init 1 2 3 4 5 6 docker run \\ --rm \\ -ti \\ --init \\ --name dui-test \\ dui Example Dockerfile with tini 1 2 3 4 5 6 FROM debian:stable-slim ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui ] COPY --from = build /usr/bin/dui-image/ /usr/bin/dui Tini will be the entrypoint, executing our own user program (/usr/bin/dui). This can also be achieved as follows: 1 2 ENTRYPOINT [ /tini , -vv , -g , -- ] CMD [ /usr/bin/dui ] -vv : debug log level 2 (-v =1, -vv=2, -vvv=3) -g : kill the entire group of processes when signal is received -- : end of tini and start of your command Signals Now that we can correctly respond to signals we need to take care of which signals to listen to. There are essentially two commands: docker stop and docker kill that can be used to stop it. Behind the scenes, docker stop stops a running container by sending it SIGINT signal, let the main process process it, and after a grace period uses SIGKILL to terminate the application. 7 You can test it with the following image, which uses Java's shutdown hook to shutdown gracefully: this only happens when it is stopped. Run the command below and press ctrl+c , and you will see that the application shuts down gracefully. 1 docker run --rm -ti --name test caladreas/buming Make sure you have two terminal windows open. In terminal one, run this command: 1 2 docker run -d --name test caladreas/buming docker logs -f test In terminal two, run this command: 1 docker rm -f test And now you will not see the graceful shutdown log, as the JVM was killed without being allowed to call shutdown hooks. Docker allows you to specify which signal it should send via --stop-signal in the run command or stop_signal: in a compose file. 1 2 3 4 5 6 7 version : 3.5 services : yi : image : dui build : . stop_signal : SIGINT Examples How to actually listen to the signals and determine which one to use will depend on your programming language. There's three examples I have worked out, one for Go (lang) and two for Java: pojo and Spring Boot. Go Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 # build stage FROM golang:latest AS build-env RUN go get -v github.com/docker/docker/client/... RUN go get -v github.com/docker/docker/api/... ADD src/ $GOPATH /flow-proxy-service-lister WORKDIR $GOPATH /flow-proxy-service-lister RUN go build -o main -tags netgo main.go # final stage FROM alpine ENTRYPOINT [ /app/main ] COPY --from = build-env /go/flow-proxy-service-lister/main /app/ RUN chmod +x /app/main Go code for graceful shutdown The following is a way for Go to shutdown a http server when receiving a termination signal. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func main () { c := make ( chan bool ) // make channel for main -- webserver communication go webserver . Start ( 7777 , webserverData , c ) // ignore the missing data stop := make ( chan os . Signal , 1 ) // make a channel that listens to is signals signal . Notify ( stop , syscall . SIGINT , syscall . SIGTERM ) // we listen to some specific syscall signals for i := 1 ; ; i ++ { // this is still infinite t := time . NewTicker ( time . Second * 30 ) // set a timer for the polling select { case - stop : // this means we got a os signal on our channel break // so we can stop case - t . C : // our timer expired, refresh our data continue // and continue with the loop } break } fmt . Println ( Shutting down webserver ) // if we got here, we have to inform the webserver to close shop c - true // we do this by sending a message on the channel if b := - c ; b { // when we get true back, that means the webserver is doing with a graceful shutdown fmt . Println ( Webserver shut down ) // webserver is done } fmt . Println ( Shut down app ) // we can close shop ourselves now } Java plain (Docker Swarm) This application is a Java 9 modular application, which can be found on github, github.com/joostvdg . Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name *.java ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.1.0 LABEL description = Docker image for playing with java applications in a concurrent, parallel and distributed manor. # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui ] ENV DATE_CHANGED= 20180120-1525 COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules Handling code The code first initializes the server which and when started, creates the Shutdown Hook . Java handles certain signals in specific ways, as can be found in this table for linux. For more information, you can read the docs from Oracle . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 public class DockerApp { public static void main ( String [] args ) { ServiceLoader Logger loggers = ServiceLoader . load ( Logger . class ); Logger logger = loggers . findFirst (). isPresent () ? loggers . findFirst (). get () : null ; if ( logger == null ) { System . err . println ( Did not find any loggers, quiting ); System . exit ( 1 ); } logger . start ( LogLevel . INFO ); int pseudoRandom = new Random (). nextInt ( ProtocolConstants . POTENTIAL_SERVER_NAMES . length - 1 ); String serverName = ProtocolConstants . POTENTIAL_SERVER_NAMES [ pseudoRandom ]; int listenPort = ProtocolConstants . EXTERNAL_COMMUNICATION_PORT_A ; String multicastGroup = ProtocolConstants . MULTICAST_GROUP ; DuiServer distributedServer = DuiServerFactory . newDistributedServer ( listenPort , multicastGroup , serverName , logger ); distributedServer . logMembership (); ExecutorService executorService = Executors . newFixedThreadPool ( 1 ); executorService . submit ( distributedServer :: startServer ); long threadId = Thread . currentThread (). getId (); Runtime . getRuntime (). addShutdownHook ( new Thread (() - { System . out . println ( Shutdown hook called! ); logger . log ( LogLevel . WARN , App , ShotdownHook , threadId , Shutting down at request of Docker ); distributedServer . stopServer (); distributedServer . closeServer (); executorService . shutdown (); try { Thread . sleep ( 100 ); executorService . shutdownNow (); logger . stop (); } catch ( InterruptedException e ) { e . printStackTrace (); } })); } } Java Plain (Kubernetes) So far we've utilized the utilities from Docker itself in conjunction with it's native Docker Swarm orchestrator. Unfortunately, when it comes to popularity Kubernetes beats Swarm hands down . So this isn't complete if it doesn't also do graceful shutdown in Kubernetes. In Dockerfile Our original file had to be changed, as Debian's Slim image doesn't actually contain the kill package. And we need a kill package, as we cannot instruct Kubernetes to issue a specific SIGNAL. Instead, we can issue a PreStop exec command , which we can utilise to execute a killall java -INT . The command will be specified in the Kubernetes deployment definition below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name *.java ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.1.0 LABEL description = Docker image for playing with java applications in a concurrent, parallel and distributed manor. # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui ] ENV DATE_CHANGED= 20180120-1525 RUN apt-get update apt-get install --no-install-recommends -y psmisc = 22 .* rm -rf /var/lib/apt/lists/* COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules Kubernetes Deployment So here we have the image's K8s Deployment descriptor. Including the Pod's lifecycle preStop with a exec style command. You should know by now why we prefer that . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion : extensions/v1beta1 kind : Deployment metadata : name : dui-deployment namespace : default labels : k8s-app : dui spec : replicas : 3 template : metadata : labels : k8s-app : dui spec : containers : - name : master image : caladreas/buming ports : - name : http containerPort : 7777 lifecycle : preStop : exec : command : [ killall , java , -INT ] terminationGracePeriodSeconds : 60 Java Spring Boot (1.x) This example is for Spring Boot 1.x, in time we will have an example for 2.x. This example is for the scenario of a Fat Jar with Tomcat as container 8 . Execute example 1 docker-compose build Execute the following command: 1 docker run --rm -ti --name test spring-boot-graceful Exit the application/container via ctrl+c and you should see the application shutting down gracefully. 1 2 3 2018 -01-30 13 :35:46.327 INFO 7 --- [ Thread-3 ] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@6e5e91e4: startup date [ Tue Jan 30 13 :35:42 GMT 2018 ] ; root of context hierarchy 2018 -01-30 13 :35:46.405 INFO 7 --- [ Thread-3 ] BootGracefulApplication $GracefulShutdown : Tomcat was shutdown gracefully within the allotted time. 2018 -01-30 13 :35:46.408 INFO 7 --- [ Thread-3 ] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans on shutdown Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 FROM maven:3-jdk-8 AS build ENV MAVEN_OPTS=-Dmaven.repo.local=/usr/share/maven/repository ENV WORKDIR=/usr/src/graceful RUN mkdir $WORKDIR WORKDIR $WORKDIR COPY pom.xml $WORKDIR RUN mvn -B -e org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline COPY . $WORKSPACE RUN mvn -B -e clean verify FROM anapsix/alpine-java:8_jdk_unlimited LABEL authors = Joost van der Griendt joostvdg@gmail.com ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- ] ENV DATE_CHANGED= 20180120-1525 COPY --from = build /usr/src/graceful/target/spring-boot-graceful.jar /app.jar CMD [ java , -Xms256M , -Xmx480M , -Djava.security.egd=file:/dev/./urandom , -jar , /app.jar ] Docker compose file 1 2 3 4 5 6 7 version : 3.5 services : web : image : spring-boot-graceful build : . stop_signal : SIGINT Java handling code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 package com.github.joostvdg.demo.springbootgraceful ; import org.springframework.boot.SpringApplication ; import org.springframework.boot.autoconfigure.SpringBootApplication ; import org.apache.catalina.connector.Connector ; import org.apache.tomcat.util.threads.ThreadPoolExecutor ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer ; import org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatConnectorCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory ; import org.springframework.context.ApplicationListener ; import org.springframework.context.annotation.Bean ; import org.springframework.context.event.ContextClosedEvent ; import java.util.concurrent.Executor ; import java.util.concurrent.TimeUnit ; @SpringBootApplication public class SpringBootGracefulApplication { public static void main ( String [] args ) { SpringApplication . run ( SpringBootGracefulApplication . class , args ); } @Bean public GracefulShutdown gracefulShutdown () { return new GracefulShutdown (); } @Bean public EmbeddedServletContainerCustomizer tomcatCustomizer () { return new EmbeddedServletContainerCustomizer () { @Override public void customize ( ConfigurableEmbeddedServletContainer container ) { if ( container instanceof TomcatEmbeddedServletContainerFactory ) { (( TomcatEmbeddedServletContainerFactory ) container ) . addConnectorCustomizers ( gracefulShutdown ()); } } }; } private static class GracefulShutdown implements TomcatConnectorCustomizer , ApplicationListener ContextClosedEvent { private static final Logger log = LoggerFactory . getLogger ( GracefulShutdown . class ); private volatile Connector connector ; @Override public void customize ( Connector connector ) { this . connector = connector ; } @Override public void onApplicationEvent ( ContextClosedEvent event ) { this . connector . pause (); Executor executor = this . connector . getProtocolHandler (). getExecutor (); if ( executor instanceof ThreadPoolExecutor ) { try { ThreadPoolExecutor threadPoolExecutor = ( ThreadPoolExecutor ) executor ; threadPoolExecutor . shutdown (); if (! threadPoolExecutor . awaitTermination ( 30 , TimeUnit . SECONDS )) { log . warn ( Tomcat thread pool did not shut down gracefully within + 30 seconds. Proceeding with forceful shutdown ); } else { log . info ( Tomcat was shutdown gracefully within the allotted time. ); } } catch ( InterruptedException ex ) { Thread . currentThread (). interrupt (); } } } } } Example with Docker Swarm For now there's only an example with docker swarm , in time there will also be a Kubernetes example. Now that you can create Java applications packaged neatly in Docker images that support graceful shutdown, it would be nice to utilize. A good scenario would be a microservices architecture where services can come and go, but are registered in a service registry such as Eureka . Or a membership based protocol where members interact with each other and perhaps shard data. In these cases, of course the interactions are designed to be fault tolerant and discover faulty nodes on their own. But wouldn't it be better that if you knew you're going to quit, you inform the rest? We can reuse the caladreas/buming image and make it a docker swarm stack and run the service on every node. This way, we can easily see members coming and going and reduce the time to detect failure by notifying our peers of our impeding end. Docker swarm cluster Setting up a docker swarm cluster is easy, but has some requirements: virtual box 4.x+ docker-machine 1.12+ docker 17.06+ Warn Make sure this is the first and only virtualbox docker-machine VM being created/running, so that the ip range starts with 192.168.99.100 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 docker-machine create --driver virtualbox dui-1 docker-machine create --driver virtualbox dui-2 docker-machine create --driver virtualbox dui-3 eval $( docker-machine env dui-1 ) IP = 192 .168.99.100 docker swarm init --advertise-addr $IP TOKEN = $( docker swarm join-token -q worker ) eval $( docker-machine env dui-2 ) docker swarm join --token ${ TOKEN } ${ IP } :2377 eval $( docker-machine env dui-3 ) docker swarm join --token ${ TOKEN } ${ IP } :2377 eval $( docker-machine env dui-1 ) docker node ls Docker swarm network and multicast Unfortunately, docker swarm's swarm mode network overlay does not support multicast 9 10 . Why is this a problem? Well, the application I use to test the graceful shutdown requires this, sorry. Luckily there is a very easy solution for this, its by using Weavenet 's docker network plugin. Don't want to know about it or how you install it? Don't worry, just execute the script below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/usr/bin/env bash echo = Prepare dui-2 eval $( docker-machine env dui-2 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo = Prepare dui-3 eval $( docker-machine env dui-3 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo = Prepare dui-1 eval $( docker-machine env dui-1 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 docker network create --driver = weaveworks/net-plugin:2.1.3 --opt works.weave.multicast = true --attachable dui Docker stack Now to create a service that runs on every node it is the easiest to create a docker stack . Compose file (docker-stack.yml) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 version : 3.5 services : dui : image : caladreas/buming build : . stop_signal : SIGINT networks : - dui deploy : mode : global networks : dui : external : true Create stack 1 docker stack deploy --compose-file docker-stack.yml buming Execute example Now that we have a docker swarm cluster and a stack - which has a service running on every node - we can showcase the power of graceful shutdown in a cluster of dependent services. Confirm the service is running correctly on every node, first lets check our nodes. 1 2 eval $( docker-machine env dui-1 ) docker node ls Which should look like this: 1 2 3 4 ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS f21ilm4thxegn5xbentmss5ur * dui-1 Ready Active Leader y7475bo5uplt2b58d050b4wfd dui-2 Ready Active 6ssxola6y1i6h9p8256pi7bfv dui-3 Ready Active Then check the service. 1 docker service ps buming_dui Which should look like this. 1 2 3 4 ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 3mrpr0jg31x1 buming_dui.6ssxola6y1i6h9p8256pi7bfv dui:latest dui-3 Running Running 17 seconds ago pfubtiy4j7vo buming_dui.f21ilm4thxegn5xbentmss5ur dui:latest dui-1 Running Running 17 seconds ago f4gjnmhoe3y4 buming_dui.y7475bo5uplt2b58d050b4wfd dui:latest dui-2 Running Running 17 seconds ago Now open a second terminal window. In window one, follow the service logs: 1 2 eval $( docker-machine env dui-1 ) docker service logs -f buming_dui In window two, go to a different node and stop the container. 1 2 3 eval $( docker-machine env dui-2 ) docker ps docker stop buming_dui.y7475bo5uplt2b58d050b4wfd.pnoui2x6elrz0tvkjz51njz94 In this case, you will see the other nodes receiving a leave notice and then the node stopping. 1 2 3 4 5 6 buming_dui.0.ryd8szexxku3@dui-3 | [ Server-John D. Carmack ] [ WARN ] [ 14 :19:02.604011 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = 83918f6ad817 , ip = 10.0.0.7 , name = Ken Thompson } buming_dui.0.so5m14sz8ksh@dui-1 | [ Server-Alan Kay ] [ WARN ] [ 14 :19:02.602082 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = 83918f6ad817 , ip = 10.0.0.7 , name = Ken Thompson } buming_dui.0.pnoui2x6elrz@dui-2 | Shutdown hook called! buming_dui.0.pnoui2x6elrz@dui-2 | [ App ] [ WARN ] [ 14 :19:02.598759 ] [ 1 ] [ ShotdownHook ] Shutting down at request of Docker buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.598858 ] [ 12 ] [ Main ] Stopping buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.601008 ] [ 12 ] [ Main ] Closing Further reading Wikipedia page on reboots Microsoft about graceful shutdown Gracefully stopping docker containers What to know about Java and shutdown hooks https://www.weave.works/blog/docker-container-networking-multicast-fast/ https://www.weave.works/docs/net/latest/install/plugin/plugin-how-it-works/ https://www.weave.works/docs/net/latest/install/plugin/plugin-v2/ https://www.auzias.net/en/docker-network-multihost/ https://forums.docker.com/t/cannot-get-zookeeper-to-work-running-in-docker-using-swarm-mode/27109 https://github.com/docker/libnetwork/issues/740 References P\u00e9ter M\u00e1rton(@slashdotpeter) Gracefule Shutdown NodeJS Kubernetes Docker docs on building John Zaccone on Entrypoint vs CMD Linux Exec command Stackoverflow thread on CMD vs Entrypoint Codeship blog on CMD and Entrypoint details Grigorii Chudnov blog on Trapping Docker Signals Andy Wilkinson (from pivotal) explaining Spring Boot shutdown hook for Tomcat Docker Swarm issue with multicast Docker network library issue with multicast Excellent article on JVM details inside Containers","title":"Graceful shutdown"},{"location":"docker/graceful-shutdown/#docker-graceful-shutdown","text":"","title":"Docker &amp; Graceful shutdown"},{"location":"docker/graceful-shutdown/#the-case-for-graceful-shutdown","text":"We can speak about the graceful shutdown of our application, when all of the resources it used and all of the traffic and/or data processing what it handled are closed and released properly. It means that no database connection remains open and no ongoing request fails because we stop our application. 1 I thank P\u00e9ter M\u00e1rton for the quote and giving a nice case for the graceful shutdown and docker. Where his blog post goes into how to do this with/for Kubernetes, I will do this for Docker's Swarm (mode) orchestrator. So, the case for graceful shutdown. As the quote shows, what we mean with it, is that an application shuts down gracefully if it cleans up all its mess. All things considered, I think most people would agree that cleaning up your mess - resources, connections or saying goodbye is preferred above just disappearing. Shutting down nicely and leaving nothing behind will reduce the amount of potential (hard to debug) errors. It also allows other applications or services to reliably know when you are there and when you're not there. Not every application will have such dependencies (to it), but in today's cluster environments with many moving parts you'll never know. So I would recommend to always do a graceful shutdown if you're able. In the light of Docker, that might be a bit different than you're use to.","title":"The case for graceful shutdown"},{"location":"docker/graceful-shutdown/#exec-form-vs-shell-form","text":"There are several ways to run a command in a Dockerfile . These are are: RUN : runs a command during the docker build phase CMD : runs a command when the container gets started ENTRYPOINT : provides the location from where commands get run when the container starts Note You need at least one ENTRYPOINT or CMD in a Dockerfile for it to be valid. They can be used in collaboration but they can do similar things. Al these commands can be put in both a shell form and a exec form 2 . For more information on these commands you should check out John Zaccone's blog on Entrypoint vs CMD . In summary, the shell form will run the command as a shell command and spawn a process via /bin/sh -c . Whereas the exec form will execute a child process that is still attached to PID1 4 . This means that if you run the examples below, you will notice that you cannot ctrl+c out of the shell form, but you can out of the exec form. Exec form is the recommended form to use and is a requirement for graceful shutdown. Below you'll find some further reading on the CMD and ENTRYPOINT commands 5 6 .","title":"Exec (form) vs Shell (form)"},{"location":"docker/graceful-shutdown/#shell-form-example","text":"1 2 FROM alpine ENTRYPOINT ping www.google.com # shell format 3","title":"Shell form example"},{"location":"docker/graceful-shutdown/#exec-form-example","text":"1 2 FROM alpine ENTRYPOINT [ ping , www.google.com ] # exec format 3","title":"Exec form example"},{"location":"docker/graceful-shutdown/#pid1","text":"Now you run your commands nicely as PID1 and make sure it is your process that receives the SIGNALS . Then all is good for a while, but at one point you will run into the problems of either having zombie child processes or processes that aren't designed for running as PID1 . This means you will have to do something about this, and luckily there's already some people who have done this for you. There's tini : a tiny initialization system designed for Docker. If used with the exec form it will run as PID1 and will manage your process and its child processes for you. For what it adds exactly and why it was introduced, you can rever to this excellent explanation from its creator . In fact, tini was so successful, that docker included it in docker ! While it works for docker run and docker compose it doesn't yet work for Docker Swarm stacks.","title":"PID1"},{"location":"docker/graceful-shutdown/#example-docker-run-with-init","text":"1 2 3 4 5 6 docker run \\ --rm \\ -ti \\ --init \\ --name dui-test \\ dui","title":"Example docker run with init"},{"location":"docker/graceful-shutdown/#example-dockerfile-with-tini","text":"1 2 3 4 5 6 FROM debian:stable-slim ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui ] COPY --from = build /usr/bin/dui-image/ /usr/bin/dui Tini will be the entrypoint, executing our own user program (/usr/bin/dui). This can also be achieved as follows: 1 2 ENTRYPOINT [ /tini , -vv , -g , -- ] CMD [ /usr/bin/dui ] -vv : debug log level 2 (-v =1, -vv=2, -vvv=3) -g : kill the entire group of processes when signal is received -- : end of tini and start of your command","title":"Example Dockerfile with tini"},{"location":"docker/graceful-shutdown/#signals","text":"Now that we can correctly respond to signals we need to take care of which signals to listen to. There are essentially two commands: docker stop and docker kill that can be used to stop it. Behind the scenes, docker stop stops a running container by sending it SIGINT signal, let the main process process it, and after a grace period uses SIGKILL to terminate the application. 7 You can test it with the following image, which uses Java's shutdown hook to shutdown gracefully: this only happens when it is stopped. Run the command below and press ctrl+c , and you will see that the application shuts down gracefully. 1 docker run --rm -ti --name test caladreas/buming Make sure you have two terminal windows open. In terminal one, run this command: 1 2 docker run -d --name test caladreas/buming docker logs -f test In terminal two, run this command: 1 docker rm -f test And now you will not see the graceful shutdown log, as the JVM was killed without being allowed to call shutdown hooks. Docker allows you to specify which signal it should send via --stop-signal in the run command or stop_signal: in a compose file. 1 2 3 4 5 6 7 version : 3.5 services : yi : image : dui build : . stop_signal : SIGINT","title":"Signals"},{"location":"docker/graceful-shutdown/#examples","text":"How to actually listen to the signals and determine which one to use will depend on your programming language. There's three examples I have worked out, one for Go (lang) and two for Java: pojo and Spring Boot.","title":"Examples"},{"location":"docker/graceful-shutdown/#go","text":"","title":"Go"},{"location":"docker/graceful-shutdown/#dockerfile","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 # build stage FROM golang:latest AS build-env RUN go get -v github.com/docker/docker/client/... RUN go get -v github.com/docker/docker/api/... ADD src/ $GOPATH /flow-proxy-service-lister WORKDIR $GOPATH /flow-proxy-service-lister RUN go build -o main -tags netgo main.go # final stage FROM alpine ENTRYPOINT [ /app/main ] COPY --from = build-env /go/flow-proxy-service-lister/main /app/ RUN chmod +x /app/main","title":"Dockerfile"},{"location":"docker/graceful-shutdown/#go-code-for-graceful-shutdown","text":"The following is a way for Go to shutdown a http server when receiving a termination signal. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func main () { c := make ( chan bool ) // make channel for main -- webserver communication go webserver . Start ( 7777 , webserverData , c ) // ignore the missing data stop := make ( chan os . Signal , 1 ) // make a channel that listens to is signals signal . Notify ( stop , syscall . SIGINT , syscall . SIGTERM ) // we listen to some specific syscall signals for i := 1 ; ; i ++ { // this is still infinite t := time . NewTicker ( time . Second * 30 ) // set a timer for the polling select { case - stop : // this means we got a os signal on our channel break // so we can stop case - t . C : // our timer expired, refresh our data continue // and continue with the loop } break } fmt . Println ( Shutting down webserver ) // if we got here, we have to inform the webserver to close shop c - true // we do this by sending a message on the channel if b := - c ; b { // when we get true back, that means the webserver is doing with a graceful shutdown fmt . Println ( Webserver shut down ) // webserver is done } fmt . Println ( Shut down app ) // we can close shop ourselves now }","title":"Go code for graceful shutdown"},{"location":"docker/graceful-shutdown/#java-plain-docker-swarm","text":"This application is a Java 9 modular application, which can be found on github, github.com/joostvdg .","title":"Java plain (Docker Swarm)"},{"location":"docker/graceful-shutdown/#dockerfile_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name *.java ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.1.0 LABEL description = Docker image for playing with java applications in a concurrent, parallel and distributed manor. # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui ] ENV DATE_CHANGED= 20180120-1525 COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules","title":"Dockerfile"},{"location":"docker/graceful-shutdown/#handling-code","text":"The code first initializes the server which and when started, creates the Shutdown Hook . Java handles certain signals in specific ways, as can be found in this table for linux. For more information, you can read the docs from Oracle . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 public class DockerApp { public static void main ( String [] args ) { ServiceLoader Logger loggers = ServiceLoader . load ( Logger . class ); Logger logger = loggers . findFirst (). isPresent () ? loggers . findFirst (). get () : null ; if ( logger == null ) { System . err . println ( Did not find any loggers, quiting ); System . exit ( 1 ); } logger . start ( LogLevel . INFO ); int pseudoRandom = new Random (). nextInt ( ProtocolConstants . POTENTIAL_SERVER_NAMES . length - 1 ); String serverName = ProtocolConstants . POTENTIAL_SERVER_NAMES [ pseudoRandom ]; int listenPort = ProtocolConstants . EXTERNAL_COMMUNICATION_PORT_A ; String multicastGroup = ProtocolConstants . MULTICAST_GROUP ; DuiServer distributedServer = DuiServerFactory . newDistributedServer ( listenPort , multicastGroup , serverName , logger ); distributedServer . logMembership (); ExecutorService executorService = Executors . newFixedThreadPool ( 1 ); executorService . submit ( distributedServer :: startServer ); long threadId = Thread . currentThread (). getId (); Runtime . getRuntime (). addShutdownHook ( new Thread (() - { System . out . println ( Shutdown hook called! ); logger . log ( LogLevel . WARN , App , ShotdownHook , threadId , Shutting down at request of Docker ); distributedServer . stopServer (); distributedServer . closeServer (); executorService . shutdown (); try { Thread . sleep ( 100 ); executorService . shutdownNow (); logger . stop (); } catch ( InterruptedException e ) { e . printStackTrace (); } })); } }","title":"Handling code"},{"location":"docker/graceful-shutdown/#java-plain-kubernetes","text":"So far we've utilized the utilities from Docker itself in conjunction with it's native Docker Swarm orchestrator. Unfortunately, when it comes to popularity Kubernetes beats Swarm hands down . So this isn't complete if it doesn't also do graceful shutdown in Kubernetes.","title":"Java Plain (Kubernetes)"},{"location":"docker/graceful-shutdown/#in-dockerfile","text":"Our original file had to be changed, as Debian's Slim image doesn't actually contain the kill package. And we need a kill package, as we cannot instruct Kubernetes to issue a specific SIGNAL. Instead, we can issue a PreStop exec command , which we can utilise to execute a killall java -INT . The command will be specified in the Kubernetes deployment definition below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 FROM openjdk:9-jdk AS build RUN mkdir -p /usr/src/mods/jars RUN mkdir -p /usr/src/mods/compiled COPY . /usr/src WORKDIR /usr/src RUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $( find src -name *.java ) RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.logging . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.api . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1 .0 -C /usr/src/mods/compiled/joostvdg.dui.client . RUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1 .0 -e com.github.joostvdg.dui.server.cli.DockerApp \\ -C /usr/src/mods/compiled/joostvdg.dui.server . RUN rm -rf /usr/bin/dui-image RUN jlink --module-path /usr/src/mods/jars/:/ ${ JAVA_HOME } /jmods \\ --add-modules joostvdg.dui.api \\ --add-modules joostvdg.dui.logging \\ --add-modules joostvdg.dui.server \\ --add-modules joostvdg.dui.client \\ --launcher dui = joostvdg.dui.server \\ --output /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN ls -lath /usr/bin/dui-image RUN /usr/bin/dui-image/bin/java --list-modules FROM debian:stable-slim LABEL authors = Joost van der Griendt joostvdg@gmail.com LABEL version = 0.1.0 LABEL description = Docker image for playing with java applications in a concurrent, parallel and distributed manor. # Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/ ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- , /usr/bin/dui/bin/dui ] ENV DATE_CHANGED= 20180120-1525 RUN apt-get update apt-get install --no-install-recommends -y psmisc = 22 .* rm -rf /var/lib/apt/lists/* COPY --from = build /usr/bin/dui-image/ /usr/bin/dui RUN /usr/bin/dui/bin/java --list-modules","title":"In Dockerfile"},{"location":"docker/graceful-shutdown/#kubernetes-deployment","text":"So here we have the image's K8s Deployment descriptor. Including the Pod's lifecycle preStop with a exec style command. You should know by now why we prefer that . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion : extensions/v1beta1 kind : Deployment metadata : name : dui-deployment namespace : default labels : k8s-app : dui spec : replicas : 3 template : metadata : labels : k8s-app : dui spec : containers : - name : master image : caladreas/buming ports : - name : http containerPort : 7777 lifecycle : preStop : exec : command : [ killall , java , -INT ] terminationGracePeriodSeconds : 60","title":"Kubernetes Deployment"},{"location":"docker/graceful-shutdown/#java-spring-boot-1x","text":"This example is for Spring Boot 1.x, in time we will have an example for 2.x. This example is for the scenario of a Fat Jar with Tomcat as container 8 .","title":"Java Spring Boot (1.x)"},{"location":"docker/graceful-shutdown/#execute-example","text":"1 docker-compose build Execute the following command: 1 docker run --rm -ti --name test spring-boot-graceful Exit the application/container via ctrl+c and you should see the application shutting down gracefully. 1 2 3 2018 -01-30 13 :35:46.327 INFO 7 --- [ Thread-3 ] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@6e5e91e4: startup date [ Tue Jan 30 13 :35:42 GMT 2018 ] ; root of context hierarchy 2018 -01-30 13 :35:46.405 INFO 7 --- [ Thread-3 ] BootGracefulApplication $GracefulShutdown : Tomcat was shutdown gracefully within the allotted time. 2018 -01-30 13 :35:46.408 INFO 7 --- [ Thread-3 ] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans on shutdown","title":"Execute example"},{"location":"docker/graceful-shutdown/#dockerfile_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 FROM maven:3-jdk-8 AS build ENV MAVEN_OPTS=-Dmaven.repo.local=/usr/share/maven/repository ENV WORKDIR=/usr/src/graceful RUN mkdir $WORKDIR WORKDIR $WORKDIR COPY pom.xml $WORKDIR RUN mvn -B -e org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline COPY . $WORKSPACE RUN mvn -B -e clean verify FROM anapsix/alpine-java:8_jdk_unlimited LABEL authors = Joost van der Griendt joostvdg@gmail.com ENV TINI_VERSION v0.16.1 ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini RUN chmod +x /tini ENTRYPOINT [ /tini , -vv , -g , -- ] ENV DATE_CHANGED= 20180120-1525 COPY --from = build /usr/src/graceful/target/spring-boot-graceful.jar /app.jar CMD [ java , -Xms256M , -Xmx480M , -Djava.security.egd=file:/dev/./urandom , -jar , /app.jar ]","title":"Dockerfile"},{"location":"docker/graceful-shutdown/#docker-compose-file","text":"1 2 3 4 5 6 7 version : 3.5 services : web : image : spring-boot-graceful build : . stop_signal : SIGINT","title":"Docker compose file"},{"location":"docker/graceful-shutdown/#java-handling-code","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 package com.github.joostvdg.demo.springbootgraceful ; import org.springframework.boot.SpringApplication ; import org.springframework.boot.autoconfigure.SpringBootApplication ; import org.apache.catalina.connector.Connector ; import org.apache.tomcat.util.threads.ThreadPoolExecutor ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer ; import org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatConnectorCustomizer ; import org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory ; import org.springframework.context.ApplicationListener ; import org.springframework.context.annotation.Bean ; import org.springframework.context.event.ContextClosedEvent ; import java.util.concurrent.Executor ; import java.util.concurrent.TimeUnit ; @SpringBootApplication public class SpringBootGracefulApplication { public static void main ( String [] args ) { SpringApplication . run ( SpringBootGracefulApplication . class , args ); } @Bean public GracefulShutdown gracefulShutdown () { return new GracefulShutdown (); } @Bean public EmbeddedServletContainerCustomizer tomcatCustomizer () { return new EmbeddedServletContainerCustomizer () { @Override public void customize ( ConfigurableEmbeddedServletContainer container ) { if ( container instanceof TomcatEmbeddedServletContainerFactory ) { (( TomcatEmbeddedServletContainerFactory ) container ) . addConnectorCustomizers ( gracefulShutdown ()); } } }; } private static class GracefulShutdown implements TomcatConnectorCustomizer , ApplicationListener ContextClosedEvent { private static final Logger log = LoggerFactory . getLogger ( GracefulShutdown . class ); private volatile Connector connector ; @Override public void customize ( Connector connector ) { this . connector = connector ; } @Override public void onApplicationEvent ( ContextClosedEvent event ) { this . connector . pause (); Executor executor = this . connector . getProtocolHandler (). getExecutor (); if ( executor instanceof ThreadPoolExecutor ) { try { ThreadPoolExecutor threadPoolExecutor = ( ThreadPoolExecutor ) executor ; threadPoolExecutor . shutdown (); if (! threadPoolExecutor . awaitTermination ( 30 , TimeUnit . SECONDS )) { log . warn ( Tomcat thread pool did not shut down gracefully within + 30 seconds. Proceeding with forceful shutdown ); } else { log . info ( Tomcat was shutdown gracefully within the allotted time. ); } } catch ( InterruptedException ex ) { Thread . currentThread (). interrupt (); } } } } }","title":"Java handling code"},{"location":"docker/graceful-shutdown/#example-with-docker-swarm","text":"For now there's only an example with docker swarm , in time there will also be a Kubernetes example. Now that you can create Java applications packaged neatly in Docker images that support graceful shutdown, it would be nice to utilize. A good scenario would be a microservices architecture where services can come and go, but are registered in a service registry such as Eureka . Or a membership based protocol where members interact with each other and perhaps shard data. In these cases, of course the interactions are designed to be fault tolerant and discover faulty nodes on their own. But wouldn't it be better that if you knew you're going to quit, you inform the rest? We can reuse the caladreas/buming image and make it a docker swarm stack and run the service on every node. This way, we can easily see members coming and going and reduce the time to detect failure by notifying our peers of our impeding end.","title":"Example with Docker Swarm"},{"location":"docker/graceful-shutdown/#docker-swarm-cluster","text":"Setting up a docker swarm cluster is easy, but has some requirements: virtual box 4.x+ docker-machine 1.12+ docker 17.06+ Warn Make sure this is the first and only virtualbox docker-machine VM being created/running, so that the ip range starts with 192.168.99.100 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 docker-machine create --driver virtualbox dui-1 docker-machine create --driver virtualbox dui-2 docker-machine create --driver virtualbox dui-3 eval $( docker-machine env dui-1 ) IP = 192 .168.99.100 docker swarm init --advertise-addr $IP TOKEN = $( docker swarm join-token -q worker ) eval $( docker-machine env dui-2 ) docker swarm join --token ${ TOKEN } ${ IP } :2377 eval $( docker-machine env dui-3 ) docker swarm join --token ${ TOKEN } ${ IP } :2377 eval $( docker-machine env dui-1 ) docker node ls","title":"Docker swarm cluster"},{"location":"docker/graceful-shutdown/#docker-swarm-network-and-multicast","text":"Unfortunately, docker swarm's swarm mode network overlay does not support multicast 9 10 . Why is this a problem? Well, the application I use to test the graceful shutdown requires this, sorry. Luckily there is a very easy solution for this, its by using Weavenet 's docker network plugin. Don't want to know about it or how you install it? Don't worry, just execute the script below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/usr/bin/env bash echo = Prepare dui-2 eval $( docker-machine env dui-2 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo = Prepare dui-3 eval $( docker-machine env dui-3 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 echo = Prepare dui-1 eval $( docker-machine env dui-1 ) docker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions docker plugin disable weaveworks/net-plugin:2.1.3 docker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST = 1 docker plugin enable weaveworks/net-plugin:2.1.3 docker network create --driver = weaveworks/net-plugin:2.1.3 --opt works.weave.multicast = true --attachable dui","title":"Docker swarm network and multicast"},{"location":"docker/graceful-shutdown/#docker-stack","text":"Now to create a service that runs on every node it is the easiest to create a docker stack .","title":"Docker stack"},{"location":"docker/graceful-shutdown/#compose-file-docker-stackyml","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 version : 3.5 services : dui : image : caladreas/buming build : . stop_signal : SIGINT networks : - dui deploy : mode : global networks : dui : external : true","title":"Compose file (docker-stack.yml)"},{"location":"docker/graceful-shutdown/#create-stack","text":"1 docker stack deploy --compose-file docker-stack.yml buming","title":"Create stack"},{"location":"docker/graceful-shutdown/#execute-example_1","text":"Now that we have a docker swarm cluster and a stack - which has a service running on every node - we can showcase the power of graceful shutdown in a cluster of dependent services. Confirm the service is running correctly on every node, first lets check our nodes. 1 2 eval $( docker-machine env dui-1 ) docker node ls Which should look like this: 1 2 3 4 ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS f21ilm4thxegn5xbentmss5ur * dui-1 Ready Active Leader y7475bo5uplt2b58d050b4wfd dui-2 Ready Active 6ssxola6y1i6h9p8256pi7bfv dui-3 Ready Active Then check the service. 1 docker service ps buming_dui Which should look like this. 1 2 3 4 ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 3mrpr0jg31x1 buming_dui.6ssxola6y1i6h9p8256pi7bfv dui:latest dui-3 Running Running 17 seconds ago pfubtiy4j7vo buming_dui.f21ilm4thxegn5xbentmss5ur dui:latest dui-1 Running Running 17 seconds ago f4gjnmhoe3y4 buming_dui.y7475bo5uplt2b58d050b4wfd dui:latest dui-2 Running Running 17 seconds ago Now open a second terminal window. In window one, follow the service logs: 1 2 eval $( docker-machine env dui-1 ) docker service logs -f buming_dui In window two, go to a different node and stop the container. 1 2 3 eval $( docker-machine env dui-2 ) docker ps docker stop buming_dui.y7475bo5uplt2b58d050b4wfd.pnoui2x6elrz0tvkjz51njz94 In this case, you will see the other nodes receiving a leave notice and then the node stopping. 1 2 3 4 5 6 buming_dui.0.ryd8szexxku3@dui-3 | [ Server-John D. Carmack ] [ WARN ] [ 14 :19:02.604011 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = 83918f6ad817 , ip = 10.0.0.7 , name = Ken Thompson } buming_dui.0.so5m14sz8ksh@dui-1 | [ Server-Alan Kay ] [ WARN ] [ 14 :19:02.602082 ] [ 16 ] [ Main ] Received membership leave notice from MessageOrigin { host = 83918f6ad817 , ip = 10.0.0.7 , name = Ken Thompson } buming_dui.0.pnoui2x6elrz@dui-2 | Shutdown hook called! buming_dui.0.pnoui2x6elrz@dui-2 | [ App ] [ WARN ] [ 14 :19:02.598759 ] [ 1 ] [ ShotdownHook ] Shutting down at request of Docker buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.598858 ] [ 12 ] [ Main ] Stopping buming_dui.0.pnoui2x6elrz@dui-2 | [ Server-Ken Thompson ] [ INFO ] [ 14 :19:02.601008 ] [ 12 ] [ Main ] Closing","title":"Execute example"},{"location":"docker/graceful-shutdown/#further-reading","text":"Wikipedia page on reboots Microsoft about graceful shutdown Gracefully stopping docker containers What to know about Java and shutdown hooks https://www.weave.works/blog/docker-container-networking-multicast-fast/ https://www.weave.works/docs/net/latest/install/plugin/plugin-how-it-works/ https://www.weave.works/docs/net/latest/install/plugin/plugin-v2/ https://www.auzias.net/en/docker-network-multihost/ https://forums.docker.com/t/cannot-get-zookeeper-to-work-running-in-docker-using-swarm-mode/27109 https://github.com/docker/libnetwork/issues/740","title":"Further reading"},{"location":"docker/graceful-shutdown/#references","text":"P\u00e9ter M\u00e1rton(@slashdotpeter) Gracefule Shutdown NodeJS Kubernetes Docker docs on building John Zaccone on Entrypoint vs CMD Linux Exec command Stackoverflow thread on CMD vs Entrypoint Codeship blog on CMD and Entrypoint details Grigorii Chudnov blog on Trapping Docker Signals Andy Wilkinson (from pivotal) explaining Spring Boot shutdown hook for Tomcat Docker Swarm issue with multicast Docker network library issue with multicast Excellent article on JVM details inside Containers","title":"References"},{"location":"docker/kubernetes/","text":"Kubernetes Kubernetes terminology Kubernetes model Resources https://github.com/weaveworks/scope https://github.com/hjacobs/kube-ops-view https://coreos.com/tectonic/docs/latest/tutorials/sandbox/install.html https://github.com/kubernetes/dashboard https://blog.alexellis.io/you-need-to-know-kubernetes-and-swarm/ https://kubernetes.io/docs/reference/kubectl/cheatsheet/ https://blog.heptio.com/core-kubernetes-jazz-improv-over-orchestration-a7903ea92ca","title":"Kubernetes"},{"location":"docker/kubernetes/#kubernetes","text":"","title":"Kubernetes"},{"location":"docker/kubernetes/#kubernetes-terminology","text":"","title":"Kubernetes terminology"},{"location":"docker/kubernetes/#kubernetes-model","text":"","title":"Kubernetes model"},{"location":"docker/kubernetes/#resources","text":"https://github.com/weaveworks/scope https://github.com/hjacobs/kube-ops-view https://coreos.com/tectonic/docs/latest/tutorials/sandbox/install.html https://github.com/kubernetes/dashboard https://blog.alexellis.io/you-need-to-know-kubernetes-and-swarm/ https://kubernetes.io/docs/reference/kubectl/cheatsheet/ https://blog.heptio.com/core-kubernetes-jazz-improv-over-orchestration-a7903ea92ca","title":"Resources"},{"location":"docker/multi-stage-builds/","text":"Docker Multi-Stage Builds","title":"Multi-Stage Builds"},{"location":"docker/multi-stage-builds/#docker-multi-stage-builds","text":"","title":"Docker Multi-Stage Builds"},{"location":"docker/swarm/","text":"Docker Swarm (mode)","title":"Swarm (mode)"},{"location":"docker/swarm/#docker-swarm-mode","text":"","title":"Docker Swarm (mode)"},{"location":"java/","text":"Java Patterns/Anti-patterns Constants Use a class that cannot be instantiated for the use of constants. Using an interface is an anti-pattern because of what an interface implies. 1 2 3 4 5 6 7 8 9 /** * It should also be final, else we can extend this and create a constructor allowing us to instantiate it anyway. */ public final class Constants { private Constants () {} // we should not instantiate this class public static final String HELLO = WORLD ; public static final int AMOUNT_OF_CONSTANTS = 2 ; } edge Other usefull things Random integer","title":"General"},{"location":"java/#java","text":"","title":"Java"},{"location":"java/#patternsanti-patterns","text":"","title":"Patterns/Anti-patterns"},{"location":"java/#constants","text":"Use a class that cannot be instantiated for the use of constants. Using an interface is an anti-pattern because of what an interface implies. 1 2 3 4 5 6 7 8 9 /** * It should also be final, else we can extend this and create a constructor allowing us to instantiate it anyway. */ public final class Constants { private Constants () {} // we should not instantiate this class public static final String HELLO = WORLD ; public static final int AMOUNT_OF_CONSTANTS = 2 ; } edge","title":"Constants"},{"location":"java/#other-usefull-things","text":"Random integer","title":"Other usefull things"},{"location":"java/concurrency/","text":"Java Concurrency Terminology Correctness Correctness means that a class conforms to its specification . A good specification defines invariants constraining an object\u2019s state and postconditions describing the effects of its operations. 6 Thread Safe Class a class is thread-safe when it continues to behave correctly when accessed from multiple threads No set of operations performed sequentially or concurrently on instances of a thread-safe class can cause an instance to be in an invalid state. 6 Mutex Every Java object can implicitly act as a lock for purposes of synchronization; these built-in locks are called intrinsic locks or monitor locks . The lock is auto-matically acquired by the executing thread before entering a synchronized block and automatically released when control exits the synchronized block, whether by the normal control path or by throwing an exception out of the block. Intrinsic locks in Java act as mutexes (or mutual exclusion locks ), which means that at most one thread may own the lock. When thread A attempts to acquire a lock held by thread B, A must wait, or block, until B releases it. If B never releases the lock, A waits forever. 6 Reentrant locks When a thread requests a lock that is already held by another thread, the requesting thread blocks. But because intrinsic locks are reentrant , if a thread tries to acquire a lock that it already holds, the request succeeds. Reentrancy means that locks are acquired on a per-thread rather than per-invocation basis. Reentrancy is implemented by associating with each lock an acquisition count and an owning thread . When the count is zero, the lock is considered unheld. When a thread acquires a previously unheld lock, the JVM records the owner and sets the acquisition count to one. If that same thread acquires the lock again, the count is incremented, and when the owning thread exits the synchronized block , the count is decremented. When the count reaches zero, the lock is released. 6 Liveness In concurrent computing, liveness refers to a set of properties of concurrent systems, that require a system to make progress despite the fact that its concurrently executing components (\"processes\") may have to \"take turns\" in critical sections, parts of the program that cannot be simultaneously run by multiple processes. 1 Liveness guarantees are important properties in operating systems and distributed systems. 2 A liveness property cannot be violated in a finite execution of a distributed system because the \"good\" event might only theoretically occur at some time after execution ends. Eventual consistency is an example of a liveness property. 3 All properties can be expressed as the intersection of safety and liveness properties. 4 Volatile fields When a field is declared volatile , the compiler and runtime are put on notice that this variable is shared and that operations on it should not be reordered with other memory operations. Volatile variables are not cached in registers or in caches where they are hidden from other processors, so a read of a volatile variable always returns the most recent write by any thread. 6 You can use volatile variables only when all the following criteria are met: Writes to the variable do not depend on its current value, or you can ensure that only a single thread ever updates the value; The variable does not participate in invariants with other state variables; Locking is not required for any other reason while the variable is being accessed Confinement Confined objects must not escape their intended scope. An object may be confined to a class instance (such as a private class member), a lexical scope (such as a local variable), or a thread (such as an object that is passed from method to method within a thread, but not supposed to be shared across threads). Objects don\u2019t escape on their own, of course\u2014they need help from the developer, who assists by publishing the object beyond its intended scope. 6 Latch Simply put, a CountDownLatch has a counter field, which you can decrement as we require. We can then use it to block a calling thread until it\u2019s been counted down to zero. If we were doing some parallel processing, we could instantiate the CountDownLatch with the same value for the counter as a number of threads we want to work across. Then, we could just call countdown() after each thread finishes, guaranteeing that a dependent thread calling await() will block until the worker threads are finished. 7 Semaphore In computer science, a semaphore is a variable or abstract data type used to control access to a common resource by multiple processes in a concurrent system such as a multiprogramming operating system. A trivial semaphore is a plain variable that is changed (for example, incremented or decremented, or toggled) depending on programmer-defined conditions. The variable is then used as a condition to control access to some system resource. A useful way to think of a semaphore as used in the real-world systems is as a record of how many units of a particular resource are available, coupled with operations to adjust that record safely (i.e. to avoid race conditions) as units are required or become free, and, if necessary, wait until a unit of the resource becomes available. 7 Java Thread pools There are several different types of Thread pools available. FixedThreadPool : A fixed-size thread pool creates threads as tasks are submitted, up to the maximum pool size, and then attempts to keep the pool size constant (adding new threads if a thread dies due to an unexpected Exception ). CachedThreadPool : A cached thread pool has more flexibility to reap idle threads when the current size of the pool exceeds the demand for processing, and to add new threads when demand increases, but places no bounds on the size of the pool. SingleThreadExecutor : A single-threaded executor creates a single worker thread to process tasks, replacing it if it dies unexpectedly. Tasks are guaranteed to be processed sequentially according to the order imposed by the task queue (FIFO, LIFO, priority order). 4 ScheduledThreadPool : A fixed-size thread pool that supports delayed and periodic task execution, similar to Timer. 6 Interrupt Thread provides the interrupt method for interrupting a thread and for querying whether a thread has been interrupted. Each thread has a boolean property that represents its interrupted status; interrupting a thread sets this status. Interruption is a cooperative mechanism. One thread cannot force another to stop what it is doing and do something else; when thread A interrupts thread B, A is merely requesting that B stop what it is doing when it gets to a convenient stopping point\u2014if it feels like it. When your code calls a method that throws InterruptedException , then your method is a blocking method too, and must have a plan for responding to inter- ruption. For library code, there are basically two choices: Propagate the InterruptedException : This is often the most sensible policy if you can get away with it: just propagate the InterruptedException to your caller. This could involve not catching InterruptedException , or catching it and throwing it again after performing some brief activity-specific cleanup. Restore the interrupt : Sometimes you cannot throw InterruptedException , for instance when your code is part of a Runnable . In these situations, you must catch InterruptedException and restore the interrupted status by calling interrupt on the current thread, so that code higher up the call stack can see that an interrupt was issued. 6 Patterns Queue Deque Queue Deque A Deque is a double-ended queue that allows efficient insertion and removal from both the head and the tail. Implementations include ArrayDeque and LinkedBlockingDeque . Just as blocking queues lend themselves to the producer-consumer pattern, deques lend themselves to a related pattern called work stealing. A producer-consumer design has one shared work queue for all consumers; in a work stealing design, every consumer has its own deque. If a consumer exhausts the work in its own deque, it can steal work from the tail of someone else\u2019s deque. Work stealing can be more scalable than a traditional producer-consumer design because workers don\u2019t contend for a shared work queue; most of the time they access only their own deque, reducing contention. When a worker has to access another\u2019s queue, it does so from the tail rather than the head, further reducing contention. 6 Monitor pattern Resources concurrency-patterns-monitor-object Wikipedia article on monitor pattern e-zest blog on monitor pattern java Examples Confinement PersonSet (below) illustrates how confinement and locking can work together to make a class thread-safe even when its component state variables are not. The state of PersonSet is managed by a HashSet , which is not thread-safe. But because mySet is private and not allowed to escape, the HashSet is confined to the PersonSet. The only code paths that can access mySet are addPerson and containsPerson , and each of these acquires the lock on the PersonSet. All its state is guarded by its intrinsic lock, making PersonSet thread-safe. 6 1 2 3 4 5 6 7 8 9 10 11 12 public class PersonSet { @GuardedBy ( this ) private final Set Person mySet = new HashSet Person (); public synchronized void addPerson ( Person p ) { mySet . add ( p ); } public synchronized boolean containsPerson ( Person p ) { return mySet . contains ( p ); } } HTTP Call Counter Unsafe Counter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class UnsafeCounter { private long count = 0 ; public long getCount () { return count ; } public void service () { // do some work try { int pseudoRandom = new Random (). nextInt ( 20 ); Thread . sleep ( pseudoRandom * 100 ); ++ count ; } catch ( InterruptedException e ) { e . printStackTrace (); } } } Safe Counter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class SafeCounter { private final AtomicLong count = new AtomicLong ( 0 ); public long getCount () { return count . get (); } public void service () { try { int pseudoRandom = new Random (). nextInt ( 20 ); Thread . sleep ( pseudoRandom * 100 ); count . incrementAndGet (); } catch ( InterruptedException e ) { e . printStackTrace (); } } } Caller 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 public class Server { public void start ( int port ) throws Exception { HttpServer server = HttpServer . create ( new InetSocketAddress ( port ), 0 ); UnsafeCounter unsafeCounter = new UnsafeCounter (); SafeCounter safeCounter = new SafeCounter (); server . createContext ( /test , new MyTestHandler ( unsafeCounter , safeCounter )); server . createContext ( / , new MyHandler ( unsafeCounter , safeCounter )); Executor executor = Executors . newFixedThreadPool ( 5 ); server . setExecutor ( executor ); // creates a default executor server . start (); } static class MyTestHandler implements HttpHandler { private UnsafeCounter unsafeCounter ; private SafeCounter safeCounter ; public MyTestHandler ( UnsafeCounter unsafeCounter , SafeCounter safeCounter ) { this . unsafeCounter = unsafeCounter ; this . safeCounter = safeCounter ; } @Override public void handle ( HttpExchange t ) throws IOException { safeCounter . service (); unsafeCounter . service (); System . out . println ( Got a request on /test, counts so far: + unsafeCounter . getCount () + :: + safeCounter . getCount ()); String response = This is the response ; t . sendResponseHeaders ( 200 , response . length ()); try ( OutputStream os = t . getResponseBody ()) { os . write ( response . getBytes ()); } } } } Outcome 1 2 3 4 5 6 7 8 Starting server on port 8080 Server started Got a request on /, counts so far:2::1 Got a request on /, counts so far:6::2 Got a request on /, counts so far:6::3 Got a request on /, counts so far:6::4 Got a request on /, counts so far:6::5 Got a request on /, counts so far:6::6 Lamport, L. (1977). \"Proving the Correctness of Multiprocess Programs\". IEEE Transactions on Software Engineering (2): 125\u2013143. doi: 10.1109/TSE.1977.229904 . Lu\u00eds Rodrigues, Christian Cachin; Rachid Guerraoui (2010). Introduction to reliable and secure distributed programming (2. ed.). Berlin: Springer Berlin. pp. 22\u201324. ISBN 978-3-642-15259-7 . Bailis, P.; Ghodsi, A. (2013). \"Eventual Consistency Today: Limitations, Extensions, and Beyond\". Queue. 11 (3): 20. doi: 10.1145/2460276.2462076 . Alpern, B.; Schneider, F. B. (1987). \"Recognizing safety and liveness\". Distributed Computing. 2 (3): 117. doi: 10.1007/BF01782772 . Liveness article Wikipedia Java Concurrency in Practice / Brian Goetz, with Tim Peierls. . . [et al.] Concurrency in Practice Baeldung tutorial on CountDownLatch Wikipedia article on Semaphore","title":"Java Concurrency"},{"location":"java/concurrency/#java-concurrency","text":"","title":"Java Concurrency"},{"location":"java/concurrency/#terminology","text":"Correctness Correctness means that a class conforms to its specification . A good specification defines invariants constraining an object\u2019s state and postconditions describing the effects of its operations. 6 Thread Safe Class a class is thread-safe when it continues to behave correctly when accessed from multiple threads No set of operations performed sequentially or concurrently on instances of a thread-safe class can cause an instance to be in an invalid state. 6 Mutex Every Java object can implicitly act as a lock for purposes of synchronization; these built-in locks are called intrinsic locks or monitor locks . The lock is auto-matically acquired by the executing thread before entering a synchronized block and automatically released when control exits the synchronized block, whether by the normal control path or by throwing an exception out of the block. Intrinsic locks in Java act as mutexes (or mutual exclusion locks ), which means that at most one thread may own the lock. When thread A attempts to acquire a lock held by thread B, A must wait, or block, until B releases it. If B never releases the lock, A waits forever. 6 Reentrant locks When a thread requests a lock that is already held by another thread, the requesting thread blocks. But because intrinsic locks are reentrant , if a thread tries to acquire a lock that it already holds, the request succeeds. Reentrancy means that locks are acquired on a per-thread rather than per-invocation basis. Reentrancy is implemented by associating with each lock an acquisition count and an owning thread . When the count is zero, the lock is considered unheld. When a thread acquires a previously unheld lock, the JVM records the owner and sets the acquisition count to one. If that same thread acquires the lock again, the count is incremented, and when the owning thread exits the synchronized block , the count is decremented. When the count reaches zero, the lock is released. 6 Liveness In concurrent computing, liveness refers to a set of properties of concurrent systems, that require a system to make progress despite the fact that its concurrently executing components (\"processes\") may have to \"take turns\" in critical sections, parts of the program that cannot be simultaneously run by multiple processes. 1 Liveness guarantees are important properties in operating systems and distributed systems. 2 A liveness property cannot be violated in a finite execution of a distributed system because the \"good\" event might only theoretically occur at some time after execution ends. Eventual consistency is an example of a liveness property. 3 All properties can be expressed as the intersection of safety and liveness properties. 4 Volatile fields When a field is declared volatile , the compiler and runtime are put on notice that this variable is shared and that operations on it should not be reordered with other memory operations. Volatile variables are not cached in registers or in caches where they are hidden from other processors, so a read of a volatile variable always returns the most recent write by any thread. 6 You can use volatile variables only when all the following criteria are met: Writes to the variable do not depend on its current value, or you can ensure that only a single thread ever updates the value; The variable does not participate in invariants with other state variables; Locking is not required for any other reason while the variable is being accessed Confinement Confined objects must not escape their intended scope. An object may be confined to a class instance (such as a private class member), a lexical scope (such as a local variable), or a thread (such as an object that is passed from method to method within a thread, but not supposed to be shared across threads). Objects don\u2019t escape on their own, of course\u2014they need help from the developer, who assists by publishing the object beyond its intended scope. 6 Latch Simply put, a CountDownLatch has a counter field, which you can decrement as we require. We can then use it to block a calling thread until it\u2019s been counted down to zero. If we were doing some parallel processing, we could instantiate the CountDownLatch with the same value for the counter as a number of threads we want to work across. Then, we could just call countdown() after each thread finishes, guaranteeing that a dependent thread calling await() will block until the worker threads are finished. 7 Semaphore In computer science, a semaphore is a variable or abstract data type used to control access to a common resource by multiple processes in a concurrent system such as a multiprogramming operating system. A trivial semaphore is a plain variable that is changed (for example, incremented or decremented, or toggled) depending on programmer-defined conditions. The variable is then used as a condition to control access to some system resource. A useful way to think of a semaphore as used in the real-world systems is as a record of how many units of a particular resource are available, coupled with operations to adjust that record safely (i.e. to avoid race conditions) as units are required or become free, and, if necessary, wait until a unit of the resource becomes available. 7 Java Thread pools There are several different types of Thread pools available. FixedThreadPool : A fixed-size thread pool creates threads as tasks are submitted, up to the maximum pool size, and then attempts to keep the pool size constant (adding new threads if a thread dies due to an unexpected Exception ). CachedThreadPool : A cached thread pool has more flexibility to reap idle threads when the current size of the pool exceeds the demand for processing, and to add new threads when demand increases, but places no bounds on the size of the pool. SingleThreadExecutor : A single-threaded executor creates a single worker thread to process tasks, replacing it if it dies unexpectedly. Tasks are guaranteed to be processed sequentially according to the order imposed by the task queue (FIFO, LIFO, priority order). 4 ScheduledThreadPool : A fixed-size thread pool that supports delayed and periodic task execution, similar to Timer. 6 Interrupt Thread provides the interrupt method for interrupting a thread and for querying whether a thread has been interrupted. Each thread has a boolean property that represents its interrupted status; interrupting a thread sets this status. Interruption is a cooperative mechanism. One thread cannot force another to stop what it is doing and do something else; when thread A interrupts thread B, A is merely requesting that B stop what it is doing when it gets to a convenient stopping point\u2014if it feels like it. When your code calls a method that throws InterruptedException , then your method is a blocking method too, and must have a plan for responding to inter- ruption. For library code, there are basically two choices: Propagate the InterruptedException : This is often the most sensible policy if you can get away with it: just propagate the InterruptedException to your caller. This could involve not catching InterruptedException , or catching it and throwing it again after performing some brief activity-specific cleanup. Restore the interrupt : Sometimes you cannot throw InterruptedException , for instance when your code is part of a Runnable . In these situations, you must catch InterruptedException and restore the interrupted status by calling interrupt on the current thread, so that code higher up the call stack can see that an interrupt was issued. 6","title":"Terminology"},{"location":"java/concurrency/#patterns","text":"","title":"Patterns"},{"location":"java/concurrency/#queue-deque","text":"Queue Deque A Deque is a double-ended queue that allows efficient insertion and removal from both the head and the tail. Implementations include ArrayDeque and LinkedBlockingDeque . Just as blocking queues lend themselves to the producer-consumer pattern, deques lend themselves to a related pattern called work stealing. A producer-consumer design has one shared work queue for all consumers; in a work stealing design, every consumer has its own deque. If a consumer exhausts the work in its own deque, it can steal work from the tail of someone else\u2019s deque. Work stealing can be more scalable than a traditional producer-consumer design because workers don\u2019t contend for a shared work queue; most of the time they access only their own deque, reducing contention. When a worker has to access another\u2019s queue, it does so from the tail rather than the head, further reducing contention. 6","title":"Queue &amp; Deque"},{"location":"java/concurrency/#monitor-pattern","text":"","title":"Monitor pattern"},{"location":"java/concurrency/#resources","text":"concurrency-patterns-monitor-object Wikipedia article on monitor pattern e-zest blog on monitor pattern java","title":"Resources"},{"location":"java/concurrency/#examples","text":"","title":"Examples"},{"location":"java/concurrency/#confinement","text":"PersonSet (below) illustrates how confinement and locking can work together to make a class thread-safe even when its component state variables are not. The state of PersonSet is managed by a HashSet , which is not thread-safe. But because mySet is private and not allowed to escape, the HashSet is confined to the PersonSet. The only code paths that can access mySet are addPerson and containsPerson , and each of these acquires the lock on the PersonSet. All its state is guarded by its intrinsic lock, making PersonSet thread-safe. 6 1 2 3 4 5 6 7 8 9 10 11 12 public class PersonSet { @GuardedBy ( this ) private final Set Person mySet = new HashSet Person (); public synchronized void addPerson ( Person p ) { mySet . add ( p ); } public synchronized boolean containsPerson ( Person p ) { return mySet . contains ( p ); } }","title":"Confinement"},{"location":"java/concurrency/#http-call-counter","text":"","title":"HTTP Call Counter"},{"location":"java/concurrency/#unsafe-counter","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class UnsafeCounter { private long count = 0 ; public long getCount () { return count ; } public void service () { // do some work try { int pseudoRandom = new Random (). nextInt ( 20 ); Thread . sleep ( pseudoRandom * 100 ); ++ count ; } catch ( InterruptedException e ) { e . printStackTrace (); } } }","title":"Unsafe Counter"},{"location":"java/concurrency/#safe-counter","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class SafeCounter { private final AtomicLong count = new AtomicLong ( 0 ); public long getCount () { return count . get (); } public void service () { try { int pseudoRandom = new Random (). nextInt ( 20 ); Thread . sleep ( pseudoRandom * 100 ); count . incrementAndGet (); } catch ( InterruptedException e ) { e . printStackTrace (); } } }","title":"Safe Counter"},{"location":"java/concurrency/#caller","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 public class Server { public void start ( int port ) throws Exception { HttpServer server = HttpServer . create ( new InetSocketAddress ( port ), 0 ); UnsafeCounter unsafeCounter = new UnsafeCounter (); SafeCounter safeCounter = new SafeCounter (); server . createContext ( /test , new MyTestHandler ( unsafeCounter , safeCounter )); server . createContext ( / , new MyHandler ( unsafeCounter , safeCounter )); Executor executor = Executors . newFixedThreadPool ( 5 ); server . setExecutor ( executor ); // creates a default executor server . start (); } static class MyTestHandler implements HttpHandler { private UnsafeCounter unsafeCounter ; private SafeCounter safeCounter ; public MyTestHandler ( UnsafeCounter unsafeCounter , SafeCounter safeCounter ) { this . unsafeCounter = unsafeCounter ; this . safeCounter = safeCounter ; } @Override public void handle ( HttpExchange t ) throws IOException { safeCounter . service (); unsafeCounter . service (); System . out . println ( Got a request on /test, counts so far: + unsafeCounter . getCount () + :: + safeCounter . getCount ()); String response = This is the response ; t . sendResponseHeaders ( 200 , response . length ()); try ( OutputStream os = t . getResponseBody ()) { os . write ( response . getBytes ()); } } } }","title":"Caller"},{"location":"java/concurrency/#outcome","text":"1 2 3 4 5 6 7 8 Starting server on port 8080 Server started Got a request on /, counts so far:2::1 Got a request on /, counts so far:6::2 Got a request on /, counts so far:6::3 Got a request on /, counts so far:6::4 Got a request on /, counts so far:6::5 Got a request on /, counts so far:6::6 Lamport, L. (1977). \"Proving the Correctness of Multiprocess Programs\". IEEE Transactions on Software Engineering (2): 125\u2013143. doi: 10.1109/TSE.1977.229904 . Lu\u00eds Rodrigues, Christian Cachin; Rachid Guerraoui (2010). Introduction to reliable and secure distributed programming (2. ed.). Berlin: Springer Berlin. pp. 22\u201324. ISBN 978-3-642-15259-7 . Bailis, P.; Ghodsi, A. (2013). \"Eventual Consistency Today: Limitations, Extensions, and Beyond\". Queue. 11 (3): 20. doi: 10.1145/2460276.2462076 . Alpern, B.; Schneider, F. B. (1987). \"Recognizing safety and liveness\". Distributed Computing. 2 (3): 117. doi: 10.1007/BF01782772 . Liveness article Wikipedia Java Concurrency in Practice / Brian Goetz, with Tim Peierls. . . [et al.] Concurrency in Practice Baeldung tutorial on CountDownLatch Wikipedia article on Semaphore","title":"Outcome"},{"location":"java/ecosystem/","text":"Java Ecosysten","title":"Java Ecosystem"},{"location":"java/ecosystem/#java-ecosysten","text":"","title":"Java Ecosysten"},{"location":"java/java9/","text":"Java 9 https://jaxenter.com/maven-on-java-9-things-you-need-to-know-140985.html https://blog.codefx.org/java/five-command-line-options-to-hack-the-java-9-module-system/","title":"Java 9"},{"location":"java/java9/#java-9","text":"https://jaxenter.com/maven-on-java-9-things-you-need-to-know-140985.html https://blog.codefx.org/java/five-command-line-options-to-hack-the-java-9-module-system/","title":"Java 9"},{"location":"java/networking/","text":"Java Networking General Remarks Network API works for IPv4 (32-bit adrressing) and IPv6 (128-bit addressing) Java only supports TCP/IP and UDP/IP Java proxy system params socksProxyHost socksProxyPort http.proxySet http.proxyHost http.proxyPort https.proxySet https.proxyHost https.proxyPort ftpProxySet ftpProxyHost ftpProxyPort gopherProxySet gopherProxyHost gopherProxyPort Special IPv4 segments Internal 10. . .* 172.17. . - 172.31. . 192.168. . Local 127. . .* Broadcast 255.255.255.255 Packets sent to this address are received by all nodes on the local network, though they are not routed beyond the local network Special IPv6 segments Local 0:0:0:0:0:0:0:1 (or ::::::1 or ::1)","title":"Java Networking"},{"location":"java/networking/#java-networking","text":"","title":"Java Networking"},{"location":"java/networking/#general-remarks","text":"Network API works for IPv4 (32-bit adrressing) and IPv6 (128-bit addressing) Java only supports TCP/IP and UDP/IP","title":"General Remarks"},{"location":"java/networking/#java-proxy-system-params","text":"socksProxyHost socksProxyPort http.proxySet http.proxyHost http.proxyPort https.proxySet https.proxyHost https.proxyPort ftpProxySet ftpProxyHost ftpProxyPort gopherProxySet gopherProxyHost gopherProxyPort","title":"Java proxy system params"},{"location":"java/networking/#special-ipv4-segments","text":"","title":"Special IPv4 segments"},{"location":"java/networking/#internal","text":"10. . .* 172.17. . - 172.31. . 192.168. .","title":"Internal"},{"location":"java/networking/#local","text":"127. . .*","title":"Local"},{"location":"java/networking/#broadcast","text":"255.255.255.255 Packets sent to this address are received by all nodes on the local network, though they are not routed beyond the local network","title":"Broadcast"},{"location":"java/networking/#special-ipv6-segments","text":"","title":"Special IPv6 segments"},{"location":"java/networking/#local_1","text":"0:0:0:0:0:0:0:1 (or ::::::1 or ::1)","title":"Local"},{"location":"java/streams/","text":"Java Streams Try-with-Resources try with resources can be used with any object that implements the Closeable interface, which includes almost every object you need to dispose. So far, JavaMail Transport objects are the only exceptions I\u2019ve encountered. Those still need to be disposed of explicitly. 1 2 3 4 5 6 7 8 9 public class Main { public static void main ( String [] args ) { try ( OutputStream out = new FileOutputStream ( /tmp/data.txt )) { // work with the output stream... } catch ( IOException ex ) { System . err . println ( ex . getMessage ()); } } }","title":"Java Streams"},{"location":"java/streams/#java-streams","text":"","title":"Java Streams"},{"location":"java/streams/#try-with-resources","text":"try with resources can be used with any object that implements the Closeable interface, which includes almost every object you need to dispose. So far, JavaMail Transport objects are the only exceptions I\u2019ve encountered. Those still need to be disposed of explicitly. 1 2 3 4 5 6 7 8 9 public class Main { public static void main ( String [] args ) { try ( OutputStream out = new FileOutputStream ( /tmp/data.txt )) { // work with the output stream... } catch ( IOException ex ) { System . err . println ( ex . getMessage ()); } } }","title":"Try-with-Resources"},{"location":"java/spring/boot/","text":"","title":"Spring Boot"},{"location":"jenkins/","text":"Jenkins Cloudbees Study Guide Base configuration abc Tuning Please read the following articles from Cloudbees: Prepare-Jenkins-for-support tuning-jenkins-gc-responsiveness-and-stability After-moving-a-job-symlinks-for-folders-became-actual-folders How-to-disable-the-weather-column-to-resolve-instance-slowness Accessing-graphs-on-a-Build-History-page-can-cause-Jenkins-to-become-unresponsive AutoBrowser-Feature-Can-Cause-Performance-Issues Disk-Space-Issue-after-upgrading-Branch-API-plugin JVM-Memory-settings-best-practice Pipeline as code The default interaction model with Jenkins, historically, has been very web UI driven, requiring users to manually create jobs, then manually fill in the details through a web browser. This requires additional effort to create and manage jobs to test and build multiple projects, it also keeps the configuration of a job to build/test/deploy separate from the actual code being built/tested/deployed. This prevents users from applying their existing CI/CD best practices to the job configurations themselves. With the introduction of the Pipeline plugin, users now can implement a project\u2019s entire build/test/deploy pipeline in a Jenkinsfile and store that alongside their code, treating their pipeline as another piece of code checked into source control. We will dive into several things that come into play when writing Jenkins pipelines. Kind of Pipeline jobs Info about Pipeline DSL (a groovy DSL) Reuse pipeline DSL scripts Things to keep in mind Do's and Don't Resources Pipeline Steps Pipeline Solution Pipeline as Code Dzone RefCard Type of pipeline jobs Pipeline (inline) Pipeline (from SCM) Multi-Branch Pipeline GitHub Organization BitBucket Team/Project Danger When using the stash function keep in mind that the copying goes from where you are now to the master. When you unstash, it will copy the files from the master to where you are building. When your pipeline runs on a node and you stash and then unstash, it will copy the files from the node to the master and then back to the node. This can have a severe penalty on the performance of your pipeline when you are copying over a network. API Jenkins has an extensive API allowing you to retrieve a lot of information from the server. Plugin For this way you of course have to know how to write a plugin. There are some usefull resources to get started: * https://github.com/joostvdg/hello-world-jenkins-pipeline-plugin * https://wiki.jenkins-ci.org/display/JENKINS/Plugin+tutorial * https://jenkins.io/blog/2016/05/25/update-plugin-for-pipeline/ Do's and Don't Aside from the Do's and Don'ts from Cloudbees, there are some we want to share. This changes the requirement for the component identifier property, as a job may only match a single group and a job listing in a group can only match a single. Thus the easiest way to make sure everything will stay unique (template names probably don\u2019t), is to make the component identifier property unique per file - let it use the name of the project.","title":"Jenkins"},{"location":"jenkins/#jenkins","text":"Cloudbees Study Guide","title":"Jenkins"},{"location":"jenkins/#base-configuration","text":"abc","title":"Base configuration"},{"location":"jenkins/#tuning","text":"Please read the following articles from Cloudbees: Prepare-Jenkins-for-support tuning-jenkins-gc-responsiveness-and-stability After-moving-a-job-symlinks-for-folders-became-actual-folders How-to-disable-the-weather-column-to-resolve-instance-slowness Accessing-graphs-on-a-Build-History-page-can-cause-Jenkins-to-become-unresponsive AutoBrowser-Feature-Can-Cause-Performance-Issues Disk-Space-Issue-after-upgrading-Branch-API-plugin JVM-Memory-settings-best-practice","title":"Tuning"},{"location":"jenkins/#pipeline-as-code","text":"The default interaction model with Jenkins, historically, has been very web UI driven, requiring users to manually create jobs, then manually fill in the details through a web browser. This requires additional effort to create and manage jobs to test and build multiple projects, it also keeps the configuration of a job to build/test/deploy separate from the actual code being built/tested/deployed. This prevents users from applying their existing CI/CD best practices to the job configurations themselves. With the introduction of the Pipeline plugin, users now can implement a project\u2019s entire build/test/deploy pipeline in a Jenkinsfile and store that alongside their code, treating their pipeline as another piece of code checked into source control. We will dive into several things that come into play when writing Jenkins pipelines. Kind of Pipeline jobs Info about Pipeline DSL (a groovy DSL) Reuse pipeline DSL scripts Things to keep in mind Do's and Don't","title":"Pipeline as code"},{"location":"jenkins/#resources","text":"Pipeline Steps Pipeline Solution Pipeline as Code Dzone RefCard","title":"Resources"},{"location":"jenkins/#type-of-pipeline-jobs","text":"Pipeline (inline) Pipeline (from SCM) Multi-Branch Pipeline GitHub Organization BitBucket Team/Project Danger When using the stash function keep in mind that the copying goes from where you are now to the master. When you unstash, it will copy the files from the master to where you are building. When your pipeline runs on a node and you stash and then unstash, it will copy the files from the node to the master and then back to the node. This can have a severe penalty on the performance of your pipeline when you are copying over a network.","title":"Type of pipeline jobs"},{"location":"jenkins/#api","text":"Jenkins has an extensive API allowing you to retrieve a lot of information from the server.","title":"API"},{"location":"jenkins/#plugin","text":"For this way you of course have to know how to write a plugin. There are some usefull resources to get started: * https://github.com/joostvdg/hello-world-jenkins-pipeline-plugin * https://wiki.jenkins-ci.org/display/JENKINS/Plugin+tutorial * https://jenkins.io/blog/2016/05/25/update-plugin-for-pipeline/","title":"Plugin"},{"location":"jenkins/#dos-and-dont","text":"Aside from the Do's and Don'ts from Cloudbees, there are some we want to share. This changes the requirement for the component identifier property, as a job may only match a single group and a job listing in a group can only match a single. Thus the easiest way to make sure everything will stay unique (template names probably don\u2019t), is to make the component identifier property unique per file - let it use the name of the project.","title":"Do's and Don't"},{"location":"jenkins/java-gradle/","text":"","title":"Java gradle"},{"location":"jenkins/plugins/","text":"","title":"Plugins"},{"location":"jenkins-jobs/jenkins-jobs-builder/","text":"Jenkins Job Builder The configuration setup of Jenkins Job Builder is composed of two main categories. Basic configuration and job configuration. Job configuration can be further split into several sub categories. Basic Configuration In the basic configuration you will have to specify how the Jenkins Job Builder CLI can connect to the Jenkins instance you want to configure and how it should act. To use such a configuration file, you add --conf to the CLI command. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 localhost.ini [job_builder] ignore_cache=True keep_descriptions=False include_path=.:scripts:~/git/ recursive=False exclude=.*:manual:./development allow_duplicates=False [jenkins] #user=jenkins #password= url=http://localhost:8080/ For more information see http://docs.openstack.org/infra/jenkins-job-builder/installation.html . Job Configuration The configuration for configuring the jobs consists of several distinct parts which can all be in the same file or can be distributed in their own respected files. These different parts can also be split into two different categories, those that are strictly linked within the configuration - via template matching - and those that are separate. Separate: * Macro\u2019s * Global defaults * Job configuration defaults * External configuration files Linked: * Templates * Groups * Projects * Job definitions Here\u2019s a schematic representation on how they are linked. Exampe in YAML config: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - job-template : name : {name}-{configComponentId}-ci description : CI Job of {configComponentId} : *config_job_defaults builders : - shell : jenkins-jobs test -r global/:definitions/ -o compiled/ - job-template : name : {name}-{configComponentId}-execute description : Executor Job of {configComponentId} : *config_job_defaults builders : - shell : jenkins-jobs --conf configuration/localhost.ini update definitions/ - job-group : name : {name}-config gitlab-user : jvandergriendt jobs : - {name}-{configComponentId}-ci : - {name}-{configComponentId}-execute : - project : name : RnD-Config jobs : - {name}-config : configComponentId : JenkinsJobDefinitions The above will result in the following jobs: RnD-Config-JenkinsJobDefinitions-ci RnD-Config-JenkinsJobDefinitions-execute Macro\u2019s Macro\u2019s are what the name implies, a group of related commands which can be invoked by the group. In Jenkins Job Builder this means you can define specific configurations for a component type (e.g. builders, paramters, publishes etc). A component has a name and a macro name. In general the component name is plural and the macro name is singular. As can be seen in the examples below. Here\u2019s an example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # The add macro takes a number parameter and will creates a # job which prints Adding followed by the number parameter: - builder : name : add builders : - shell : echo Adding {number} # A specialized macro addtwo reusing the add macro but with # a number parameter hardcoded to two : - builder : name : addtwo builders : - add : number : two # Glue to have Jenkins Job Builder to expand this YAML example: - job : name : testingjob builders : # The specialized macro: - addtwo # Generic macro call with a parameter - add : number : ZERO # Generic macro called without a parameter. Never do this! # See below for the resulting wrong output :( - add To expand the schematic representation, you will get the following. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 - builder : name : test builders : - shell : jenkins-jobs test -r global/:definitions/ -o compiled/ - builder : name : update builders : - shell : jenkins-jobs --conf config.ini update -r global/:definitions/ - job-template : name : {name}-{configComponentId}-ci : *config_job_defaults builders : - test - job-template : name : {name}-{configComponentId}-update : *config_job_defaults builders : - update Global defaults Global defaults are defaults that should be global for the jobs you configure for a certain environment. It is the job counterpart of the basic configuration, usually containing variables for the specific environment. For example, url\u2019s, credential id\u2019s, JDK\u2019s etc. Example: 1 2 3 4 5 6 7 8 global-defaults-localhost.yaml - defaults : name : global flusso-gitlab-url : https://gitlab.flusso.nl nexus-npm-url : http://localhost:8081/nexus/content/repositories/npm-internal default-jdk : JDK 1.8 jenkinsJobsDefinitionJobName : RnD-Config-JenkinsJobDefinitions-ci credentialsId : 4f0dfb96-a7b1-421c-a4ea-b6a154f91b08 Job configuration defaults Job configuration defaults are nothing specific on their own. It refers to using a build in structure from YAML to create basic building blocks to be used by other configuration parts, usually the Templates. Example (definition): 1 2 3 4 5 6 7 8 9 10 - config_job_defaults : config_job_defaults name : config_job_defaults project-type : freestyle disabled : false logrotate : daysToKeep : 7 numToKeep : 5 artifactDaysToKeep : -1 artifactNumToKeep : -1 jdk : {default-jdk} Example (usage): 1 2 3 - job-template : name : {name}-{configComponentId}-ci : *config_job_defaults Templates Templates are used to define job templates. You define the entirety of the job using global defaults, configuration defaults and where useful refer to placeholders to be filled in by the other downstream configuration items. You can configure almost every plugin that is available for Jenkins, these are divided in subdivisions which reflect the Jenkins\u2019 job definition sections. For these subdivision and the available plugins see: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#modules For those plugins that are not supported, you can include the raw XML generated by the plugin. For how to do this, see: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#raw-config Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - job-template : name : {name}-{configComponentId}-ci display-name : {name}-{configComponentId}-ci description : CI Job of {configComponentId} : *config_job_defaults builders : - shell : jenkins-jobs test -r global/:definitions/ -o compiled/ publishers : - archive : artifacts : {filesToArchive_1} fingerprint : true - archive : artifacts : {filesToArchive_2} fingerprint : true - email : notify-every-unstable-build : true send-to-individuals : true Groups Groups are used to group together related components that require the same set of jobs. Where you can also specify a similar set of properties, for example, a different JDK to be used. The name property is mandatory and will be used to match Job definitions. The jobs property is also mandatory and will be used to match Templates for which a Job will be generated per matching Job definition. Example 1 2 3 4 5 6 7 8 9 10 11 12 - job-group : name : {name}-gulp gitlab-user : jvandergriendt artifactId : {gulpComponentId} jobs : - {name}-{gulpComponentId}-ci : - {name}-{gulpComponentId}-version : - {name}-{gulpComponentId}-sonar : - {name}-{gulpComponentId}-publish : - {name}-{gulpComponentId}-deploy-prep : - {name}-{gulpComponentId}-deploy : - {name}-{gulpComponentId}-acceptance : Projects Projects are used to list the actual Job definitions, which via grouping and Templates get generated, and can obviously be used to define jobs for a specific project. The name property is mandatory and will be passed along with a Job definition and is generally used to tie job definitions to Groups. 1 2 3 4 5 6 - project : name : RnD-Maven jobs : - {name}-keep : gulpComponentId : keep-backend displayName : Keep-Backend Job definitions Job definitions are what is all about. Although they are part of the Project configuration item I treat them separately. You list the jobs under a Project and start with the name of the Group it belongs to. After that, you should define at least a name component to be able to differentiate the different jobs you want. As can be seen in the above examples with the gulpComponentId. External configuration files Sometimes you run into the situation you want to use a multi-line configuration for a plugin, or a set of commands. Or, used at in different configurations or templates. Then you run into the situation that it is very difficult to manage in them neatly inside YAML configuration files. For this situation you are able to simply include a text file, via a native YAML construct. See: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#module-jenkins_jobs.local_yaml For example 1 2 3 4 5 6 7 - job : name : test-job-include-raw-1 builders : - shell : !include-raw include-raw001-hello-world.sh - shell : !include-raw include-raw001-vars.sh Usage The information to how you use the tool is very well explained in the documentation. See http://docs.openstack.org/infra/jenkins-job-builder/installation.html#running Automated maintenance If all the jobs you can administer are done via Jenkins Job Builder, you can start to automate the maintenance of these jobs. Simply make jobs that poll/push on the code base where you have your Jenkins Job Builder configuration files. Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 - config_job_defaults : config_job_defaults name : config_job_defaults project-type : freestyle disabled : false logrotate : daysToKeep : 7 numToKeep : 5 artifactDaysToKeep : -1 artifactNumToKeep : -1 jdk : {default-jdk} triggers : - pollscm : H/15 * * * * scm : - git : url : {flusso-gitlab-url}/{gitlab-user}/{componentGitName}.git credentials-id : {credentialsId} publishers : - email : notify-every-unstable-build : true send-to-individuals : true - job-template : name : {name}-{configComponentId}-ci display-name : {name}-{configComponentId}-ci description : CI Job of {configComponentId} : *config_job_defaults builders : - shell : jenkins-jobs test -r global/:definitions/ -o compiled/ publishers : - archive : artifacts : {filesToArchive_1} fingerprint : true - archive : artifacts : {filesToArchive_2} fingerprint : true - email : notify-every-unstable-build : true send-to-individuals : true - job-template : name : {name}-{configComponentId}-x display-name : {name}-{configComponentId}-execute description : Executor Job of {configComponentId}, it will execute the update and delete old command : *config_job_defaults builders : - shell : jenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/ - job-group : name : {name}-config gitlab-user : jvandergriendt jobs : - {name}-{configComponentId}-ci : - {name}-{configComponentId}-x : - project : name : RnD-Config jobs : - {name}-config : configComponentId : JenkinsJobDefinitions componentGitName : jenkins-job-definitions filesToArchive_1 : scripts/*.sh filesToArchive_2 : maven/settings.xml Tips Trick As the documentation is so extensive, it can sometimes be difficult to figure out what would be a good way to deal with some constructs. Component identifier property One important thing to keep in mind is that in order to create a whole set of jobs via the groups and templates it imperative to have a component* identifier property. This way you can define hundreds of jobs in a project, dozens of groups and dozens of templates and generate thousands of unique individual jobs. Scale does not actually matter in this case, if you have more than one job in a project you will need this property. If the jobs that will be generated will not differ the execution will fail. Bulk you can combine multiple files or even entire folder structures together in a single call. For example, if you manage all the jobs of a company or a department and configure them in separate files. For example 1 jenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/","title":"JenkinsJobsBuilder"},{"location":"jenkins-jobs/jenkins-jobs-builder/#jenkins-job-builder","text":"The configuration setup of Jenkins Job Builder is composed of two main categories. Basic configuration and job configuration. Job configuration can be further split into several sub categories.","title":"Jenkins Job Builder"},{"location":"jenkins-jobs/jenkins-jobs-builder/#basic-configuration","text":"In the basic configuration you will have to specify how the Jenkins Job Builder CLI can connect to the Jenkins instance you want to configure and how it should act. To use such a configuration file, you add --conf to the CLI command. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 localhost.ini [job_builder] ignore_cache=True keep_descriptions=False include_path=.:scripts:~/git/ recursive=False exclude=.*:manual:./development allow_duplicates=False [jenkins] #user=jenkins #password= url=http://localhost:8080/ For more information see http://docs.openstack.org/infra/jenkins-job-builder/installation.html .","title":"Basic Configuration"},{"location":"jenkins-jobs/jenkins-jobs-builder/#job-configuration","text":"The configuration for configuring the jobs consists of several distinct parts which can all be in the same file or can be distributed in their own respected files. These different parts can also be split into two different categories, those that are strictly linked within the configuration - via template matching - and those that are separate. Separate: * Macro\u2019s * Global defaults * Job configuration defaults * External configuration files Linked: * Templates * Groups * Projects * Job definitions Here\u2019s a schematic representation on how they are linked. Exampe in YAML config: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - job-template : name : {name}-{configComponentId}-ci description : CI Job of {configComponentId} : *config_job_defaults builders : - shell : jenkins-jobs test -r global/:definitions/ -o compiled/ - job-template : name : {name}-{configComponentId}-execute description : Executor Job of {configComponentId} : *config_job_defaults builders : - shell : jenkins-jobs --conf configuration/localhost.ini update definitions/ - job-group : name : {name}-config gitlab-user : jvandergriendt jobs : - {name}-{configComponentId}-ci : - {name}-{configComponentId}-execute : - project : name : RnD-Config jobs : - {name}-config : configComponentId : JenkinsJobDefinitions The above will result in the following jobs: RnD-Config-JenkinsJobDefinitions-ci RnD-Config-JenkinsJobDefinitions-execute","title":"Job Configuration"},{"location":"jenkins-jobs/jenkins-jobs-builder/#macros","text":"Macro\u2019s are what the name implies, a group of related commands which can be invoked by the group. In Jenkins Job Builder this means you can define specific configurations for a component type (e.g. builders, paramters, publishes etc). A component has a name and a macro name. In general the component name is plural and the macro name is singular. As can be seen in the examples below. Here\u2019s an example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # The add macro takes a number parameter and will creates a # job which prints Adding followed by the number parameter: - builder : name : add builders : - shell : echo Adding {number} # A specialized macro addtwo reusing the add macro but with # a number parameter hardcoded to two : - builder : name : addtwo builders : - add : number : two # Glue to have Jenkins Job Builder to expand this YAML example: - job : name : testingjob builders : # The specialized macro: - addtwo # Generic macro call with a parameter - add : number : ZERO # Generic macro called without a parameter. Never do this! # See below for the resulting wrong output :( - add To expand the schematic representation, you will get the following. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 - builder : name : test builders : - shell : jenkins-jobs test -r global/:definitions/ -o compiled/ - builder : name : update builders : - shell : jenkins-jobs --conf config.ini update -r global/:definitions/ - job-template : name : {name}-{configComponentId}-ci : *config_job_defaults builders : - test - job-template : name : {name}-{configComponentId}-update : *config_job_defaults builders : - update","title":"Macro\u2019s"},{"location":"jenkins-jobs/jenkins-jobs-builder/#global-defaults","text":"Global defaults are defaults that should be global for the jobs you configure for a certain environment. It is the job counterpart of the basic configuration, usually containing variables for the specific environment. For example, url\u2019s, credential id\u2019s, JDK\u2019s etc. Example: 1 2 3 4 5 6 7 8 global-defaults-localhost.yaml - defaults : name : global flusso-gitlab-url : https://gitlab.flusso.nl nexus-npm-url : http://localhost:8081/nexus/content/repositories/npm-internal default-jdk : JDK 1.8 jenkinsJobsDefinitionJobName : RnD-Config-JenkinsJobDefinitions-ci credentialsId : 4f0dfb96-a7b1-421c-a4ea-b6a154f91b08","title":"Global defaults"},{"location":"jenkins-jobs/jenkins-jobs-builder/#job-configuration-defaults","text":"Job configuration defaults are nothing specific on their own. It refers to using a build in structure from YAML to create basic building blocks to be used by other configuration parts, usually the Templates. Example (definition): 1 2 3 4 5 6 7 8 9 10 - config_job_defaults : config_job_defaults name : config_job_defaults project-type : freestyle disabled : false logrotate : daysToKeep : 7 numToKeep : 5 artifactDaysToKeep : -1 artifactNumToKeep : -1 jdk : {default-jdk} Example (usage): 1 2 3 - job-template : name : {name}-{configComponentId}-ci : *config_job_defaults","title":"Job configuration defaults"},{"location":"jenkins-jobs/jenkins-jobs-builder/#templates","text":"Templates are used to define job templates. You define the entirety of the job using global defaults, configuration defaults and where useful refer to placeholders to be filled in by the other downstream configuration items. You can configure almost every plugin that is available for Jenkins, these are divided in subdivisions which reflect the Jenkins\u2019 job definition sections. For these subdivision and the available plugins see: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#modules For those plugins that are not supported, you can include the raw XML generated by the plugin. For how to do this, see: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#raw-config Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - job-template : name : {name}-{configComponentId}-ci display-name : {name}-{configComponentId}-ci description : CI Job of {configComponentId} : *config_job_defaults builders : - shell : jenkins-jobs test -r global/:definitions/ -o compiled/ publishers : - archive : artifacts : {filesToArchive_1} fingerprint : true - archive : artifacts : {filesToArchive_2} fingerprint : true - email : notify-every-unstable-build : true send-to-individuals : true","title":"Templates"},{"location":"jenkins-jobs/jenkins-jobs-builder/#groups","text":"Groups are used to group together related components that require the same set of jobs. Where you can also specify a similar set of properties, for example, a different JDK to be used. The name property is mandatory and will be used to match Job definitions. The jobs property is also mandatory and will be used to match Templates for which a Job will be generated per matching Job definition. Example 1 2 3 4 5 6 7 8 9 10 11 12 - job-group : name : {name}-gulp gitlab-user : jvandergriendt artifactId : {gulpComponentId} jobs : - {name}-{gulpComponentId}-ci : - {name}-{gulpComponentId}-version : - {name}-{gulpComponentId}-sonar : - {name}-{gulpComponentId}-publish : - {name}-{gulpComponentId}-deploy-prep : - {name}-{gulpComponentId}-deploy : - {name}-{gulpComponentId}-acceptance :","title":"Groups"},{"location":"jenkins-jobs/jenkins-jobs-builder/#projects","text":"Projects are used to list the actual Job definitions, which via grouping and Templates get generated, and can obviously be used to define jobs for a specific project. The name property is mandatory and will be passed along with a Job definition and is generally used to tie job definitions to Groups. 1 2 3 4 5 6 - project : name : RnD-Maven jobs : - {name}-keep : gulpComponentId : keep-backend displayName : Keep-Backend","title":"Projects"},{"location":"jenkins-jobs/jenkins-jobs-builder/#job-definitions","text":"Job definitions are what is all about. Although they are part of the Project configuration item I treat them separately. You list the jobs under a Project and start with the name of the Group it belongs to. After that, you should define at least a name component to be able to differentiate the different jobs you want. As can be seen in the above examples with the gulpComponentId. External configuration files Sometimes you run into the situation you want to use a multi-line configuration for a plugin, or a set of commands. Or, used at in different configurations or templates. Then you run into the situation that it is very difficult to manage in them neatly inside YAML configuration files. For this situation you are able to simply include a text file, via a native YAML construct. See: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#module-jenkins_jobs.local_yaml For example 1 2 3 4 5 6 7 - job : name : test-job-include-raw-1 builders : - shell : !include-raw include-raw001-hello-world.sh - shell : !include-raw include-raw001-vars.sh","title":"Job definitions"},{"location":"jenkins-jobs/jenkins-jobs-builder/#usage","text":"The information to how you use the tool is very well explained in the documentation. See http://docs.openstack.org/infra/jenkins-job-builder/installation.html#running Automated maintenance If all the jobs you can administer are done via Jenkins Job Builder, you can start to automate the maintenance of these jobs. Simply make jobs that poll/push on the code base where you have your Jenkins Job Builder configuration files. Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 - config_job_defaults : config_job_defaults name : config_job_defaults project-type : freestyle disabled : false logrotate : daysToKeep : 7 numToKeep : 5 artifactDaysToKeep : -1 artifactNumToKeep : -1 jdk : {default-jdk} triggers : - pollscm : H/15 * * * * scm : - git : url : {flusso-gitlab-url}/{gitlab-user}/{componentGitName}.git credentials-id : {credentialsId} publishers : - email : notify-every-unstable-build : true send-to-individuals : true - job-template : name : {name}-{configComponentId}-ci display-name : {name}-{configComponentId}-ci description : CI Job of {configComponentId} : *config_job_defaults builders : - shell : jenkins-jobs test -r global/:definitions/ -o compiled/ publishers : - archive : artifacts : {filesToArchive_1} fingerprint : true - archive : artifacts : {filesToArchive_2} fingerprint : true - email : notify-every-unstable-build : true send-to-individuals : true - job-template : name : {name}-{configComponentId}-x display-name : {name}-{configComponentId}-execute description : Executor Job of {configComponentId}, it will execute the update and delete old command : *config_job_defaults builders : - shell : jenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/ - job-group : name : {name}-config gitlab-user : jvandergriendt jobs : - {name}-{configComponentId}-ci : - {name}-{configComponentId}-x : - project : name : RnD-Config jobs : - {name}-config : configComponentId : JenkinsJobDefinitions componentGitName : jenkins-job-definitions filesToArchive_1 : scripts/*.sh filesToArchive_2 : maven/settings.xml","title":"Usage"},{"location":"jenkins-jobs/jenkins-jobs-builder/#tips-trick","text":"As the documentation is so extensive, it can sometimes be difficult to figure out what would be a good way to deal with some constructs. Component identifier property One important thing to keep in mind is that in order to create a whole set of jobs via the groups and templates it imperative to have a component* identifier property. This way you can define hundreds of jobs in a project, dozens of groups and dozens of templates and generate thousands of unique individual jobs. Scale does not actually matter in this case, if you have more than one job in a project you will need this property. If the jobs that will be generated will not differ the execution will fail. Bulk you can combine multiple files or even entire folder structures together in a single call. For example, if you manage all the jobs of a company or a department and configure them in separate files. For example 1 jenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/","title":"Tips &amp; Trick"},{"location":"jenkins-jobs/jobdsl/","text":"Jenkins Job DSL Jenkins is a wonderful system for managing builds, and people love using its UI to configure jobs. Unfortunately, as the number of jobs grows, maintaining them becomes tedious, and the paradigm of using a UI falls apart. Additionally, the common pattern in this situation is to copy jobs to create new ones, these \"children\" have a habit of diverging from their original \"template\" and consequently it becomes difficult to maintain consistency between these jobs. The Jenkins job-dsl-plugin attempts to solve this problem by allowing jobs to be defined with the absolute minimum necessary in a programmatic form, with the help of templates that are synced with the generated jobs. The goal is for your project to be able to define all the jobs they want to be related to their project, declaring their intent for the jobs, leaving the common stuff up to a template that were defined earlier or hidden behind the DSL. Pipeline with folder example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import hudson.model.* import jenkins.model.* def dslExamplesFolder = DSL-Examples def gitLabCredentialsId = joost-flusso-gitlab-ssh def gitLabUrl = git@gitlab.flusso.nl def gitLabNamespace = keep def gitLabProject = keep-api if (! jenkins . model . Jenkins . instance . getItem ( dslExamplesFolder )) { //folder doesn t exist because item doesn t exist in runtime //Therefore, create the folder. folder ( dslExamplesFolder ) { displayName ( DSL Examples ) description ( Folder for job dsl examples ) } } createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , keep , keep-api ) createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , keep , keep-backend-spring ) createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , keep , keep-frontend ) def createMultibranchPipelineJob ( def gitLabCredentialsId , def gitLabUrl , def folder , def gitNamespace , def project ) { multibranchPipelineJob ( ${folder}/${project}-mb ) { branchSources { git { remote ( ${gitLabUrl}:${gitNamespace}/${project}.git ) credentialsId ( gitLabCredentialsId ) } } orphanedItemStrategy { discardOldItems { numToKeep ( 20 ) } } } } Freestyle maven job 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def project = quidryan/aws-sdk-test def branchApi = new URL ( https://api.github.com/repos/${project}/branches ) def branches = new groovy . json . JsonSlurper (). parse ( branchApi . newReader ()) branches . each { def branchName = it . name def jobName = ${project}-${branchName} . replaceAll ( / , - ) job ( jobName ) { scm { git ( git://github.com/${project}.git , branchName ) } steps { maven ( test -Dproject.name=${project}/${branchName} ) } } } Resources Tutorial Live Playground Main DSL Commands API Viewer Other References Talks and Blogs User Power Movies DZone article Testing DSL Scripts","title":"JobDSL"},{"location":"jenkins-jobs/jobdsl/#jenkins-job-dsl","text":"Jenkins is a wonderful system for managing builds, and people love using its UI to configure jobs. Unfortunately, as the number of jobs grows, maintaining them becomes tedious, and the paradigm of using a UI falls apart. Additionally, the common pattern in this situation is to copy jobs to create new ones, these \"children\" have a habit of diverging from their original \"template\" and consequently it becomes difficult to maintain consistency between these jobs. The Jenkins job-dsl-plugin attempts to solve this problem by allowing jobs to be defined with the absolute minimum necessary in a programmatic form, with the help of templates that are synced with the generated jobs. The goal is for your project to be able to define all the jobs they want to be related to their project, declaring their intent for the jobs, leaving the common stuff up to a template that were defined earlier or hidden behind the DSL.","title":"Jenkins Job DSL"},{"location":"jenkins-jobs/jobdsl/#pipeline-with-folder-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import hudson.model.* import jenkins.model.* def dslExamplesFolder = DSL-Examples def gitLabCredentialsId = joost-flusso-gitlab-ssh def gitLabUrl = git@gitlab.flusso.nl def gitLabNamespace = keep def gitLabProject = keep-api if (! jenkins . model . Jenkins . instance . getItem ( dslExamplesFolder )) { //folder doesn t exist because item doesn t exist in runtime //Therefore, create the folder. folder ( dslExamplesFolder ) { displayName ( DSL Examples ) description ( Folder for job dsl examples ) } } createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , keep , keep-api ) createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , keep , keep-backend-spring ) createMultibranchPipelineJob ( gitLabCredentialsId , gitLabUrl , dslExamplesFolder , keep , keep-frontend ) def createMultibranchPipelineJob ( def gitLabCredentialsId , def gitLabUrl , def folder , def gitNamespace , def project ) { multibranchPipelineJob ( ${folder}/${project}-mb ) { branchSources { git { remote ( ${gitLabUrl}:${gitNamespace}/${project}.git ) credentialsId ( gitLabCredentialsId ) } } orphanedItemStrategy { discardOldItems { numToKeep ( 20 ) } } } }","title":"Pipeline with folder example"},{"location":"jenkins-jobs/jobdsl/#freestyle-maven-job","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def project = quidryan/aws-sdk-test def branchApi = new URL ( https://api.github.com/repos/${project}/branches ) def branches = new groovy . json . JsonSlurper (). parse ( branchApi . newReader ()) branches . each { def branchName = it . name def jobName = ${project}-${branchName} . replaceAll ( / , - ) job ( jobName ) { scm { git ( git://github.com/${project}.git , branchName ) } steps { maven ( test -Dproject.name=${project}/${branchName} ) } } }","title":"Freestyle maven job"},{"location":"jenkins-jobs/jobdsl/#resources","text":"Tutorial Live Playground Main DSL Commands API Viewer","title":"Resources"},{"location":"jenkins-jobs/jobdsl/#other-references","text":"Talks and Blogs User Power Movies DZone article Testing DSL Scripts","title":"Other References"},{"location":"jenkins-pipeline/core-concepts/","text":"Core Concepts Below are some core concepts to understand before building pipelines in Jenkins. Pipeline as Code Step Master vs Nodes Checkout Workspace Stage Sandbox and Script Security Java vs. Groovy Env (object) Stash archive Credentials Tools Build Environment Pipeline Syntax Page Terminology The terminology used in this page is based upon the terms used by Cloudbees as related to Jenkins. If in doubt, please consult the Jenkins Glossary . Pipeline as Code Step A single task; fundamentally steps tell Jenkins what to do inside of a Pipeline or Project. Consider the following piece of pipeline code: 1 2 3 4 5 6 7 8 9 10 11 node { timestamps { stage ( My FIrst Stage ) { if ( isUnix ()) { sh echo this is Unix! } else { bat echo this is windows } } } } The only execution that happens (almost) exclusively on the node (or build slave) are the isUnix() , sh and bat shell commands. Those specific tasks are the steps in pipeline code. Master vs Nodes There are many things to keep in mind about Pipelines in Jenkins. By far the most important are those related to the distinction between Masters and Nodes. Aside from the points below, the key thing to keep in mind: Nodes (build slaves) are designed to executes task, Masters are not. Except for the steps themselves, all of the Pipeline logic, the Groovy conditionals, loops, etc execute on the master . Whether simple or complex! Even inside a node block ! Steps may use executors to do work where appropriate, but each step has a small on-master overhead too. Pipeline code is written as Groovy but the execution model is radically transformed at compile-time to Continuation Passing Style (CPS). This transformation provides valuable safety and durability guarantees for Pipelines, but it comes with trade-offs: Steps can invoke Java and execute fast and efficiently, but Groovy is much slower to run than normal. Groovy logic requires far more memory, because an object-based syntax/block tree is kept in memory. Pipelines persist the program and its state frequently to be able to survive failure of the master. Source: Sam van Oort , Cloudbees Engineer Node A machine which is part of the Jenkins environment and capable of executing Pipelines or Projects. Both the Master and Agents are considered to be Nodes. Master The central, coordinating process which stores configuration, loads plugins, and renders the various user interfaces for Jenkins. What to do? So, if Pipeline code can cause big loads on Master, what should we do than? Try to limit the use of logic in your groovy code Avoid blocking or I/O calls unless explicitly done on a slave via a Step If you need heavy processing, and there isn't a Step, create either a plugin Shared Library Or use a CLI tool via a platform independent language, such as Java or Go Tip If need to do any I/O, use a plugin or anything related to a workspace, you need a node. If you only need to interact with variables, for example for an input form, do this outside of a node block. See Pipeline Input for how that works. Workspace A disposable directory on the file system of a Node where work can be done by a Pipeline or Project. Workspaces are typically left in place after a Build or Pipeline run completes unless specific Workspace cleanup policies have been put in place on the Jenkins Master. The key part of the glossary entry there is disposable directory . There are absolutely no guarantees about Workspaces in pipeline jobs. That said, what you should take care of: always clean your workspace before you start, you don't know the state of the folder you get always clean your workspace after you finish, this way you're less likely to run into problems in subsequent builds a workspace is a temporary folder on a single node's filesystem: so every time you use node{} you have a new workspace after your build is finish or leaving the node otherwise, your workspace should be considered gone: need something from? stash or archive it! Checkout There are several ways to do a checkout in the Jenkins pipeline code. In the groovy DSL you can use the Checkout dsl command, svn shorthand or the git shorthand. 1 2 3 4 5 node { stage ( scm ) { git https://github.com/joostvdg/jishi } } Danger If you use a pipeline from SCM, multi-branch pipeline or a derived job type, beware! Only the Jenkinsfile gets checked out. You still need to checkout the rest of your files yourself! Tip However, when using pipeline from SCM, multi-branch pipeline or a derived job type. You can use a shorthand: checkout scm . This checks out the scm defined in your job (where the Jenkinsfile came from). 1 2 3 4 5 node { stage ( scm ) { checkout scm } } Stage Stage is a step for defining a conceptually distinct subset of the entire Pipeline, for example: \"Build\", \"Test\", and \"Deploy\", which is used by many plugins to visualize or present Jenkins Pipeline status/progress. The stage \"step\" has a primary function and a secondary function. Its primary function is to define the visual boundaries between logically separable parts of the pipeline. For example, you can define SCM, Build, QA, Deploy as stages to tell you where the build currently is or where it failed. The secondary function is to provided a scope for variables. Just like most programming languages, code blocks are a more than just syntactic sugar, they also limit the scope of variables. 1 2 3 4 5 6 7 8 9 10 node { stage ( SCM ) { def myVar = abc checkout scm } stage ( Build ) { sh mvn clean install echo myVar # will fail because the variable doesn t exist here } } Stages in classic view Stages in Blue Ocean view Sandbox and Script Security In Jenkins some plugins - such as the pipeline plugin - allow you to write groovy code that gets executed on the master. This means you could run code on the master that accesses the host machine with the same rights as Jenkins. As is unsafe, Jenkins has some guards against this in the form the sandbox mode and the script security . When you create a pipeline job, you get a inline code editor by default. If you're an administrator you get the option to turn the \"sandbox\" mode of. If you use a pipeline from SCM or any of the higher abstraction pipeline job types (Multibranch Pipeline, BitBucket Team) you are always running in sandbox mode. When you're in sandbox mode, your script will run past the script security. This uses a whitelisting technique to block dangerous or undesired methods, but is does so in a very restrictive manner. It could be you're doing something that is safe but still gets blocked. An administrator can then go to the script approval page (under Jenkins Administration) and approve your script. For more details, please consult Script Security plugin page. Example error 1 2 3 4 5 6 7 8 org . jenkinsci . plugins . scriptsecurity . sandbox . RejectedAccessException : unclassified staticMethod org . tmatesoft . svn . core . internal . io . dav . DAVRepositoryFactory create org . tmatesoft . svn . core . SVNURL at org . jenkinsci . plugins . scriptsecurity . sandbox . groovy . SandboxInterceptor . onStaticCall ( SandboxInterceptor . java : 138 ) at org . kohsuke . groovy . sandbox . impl . Checker$2 . call ( Checker . java : 180 ) at org . kohsuke . groovy . sandbox . impl . Checker . checkedStaticCall ( Checker . java : 177 ) at org . kohsuke . groovy . sandbox . impl . Checker . checkedCall ( Checker . java : 91 ) at com . cloudbees . groovy . cps . sandbox . SandboxInvoker . methodCall ( SandboxInvoker . java : 16 ) at WorkflowScript . run ( WorkflowScript : 12 ) at ___cps . transform___ ( Native Method ) Tip There are three ways to deal with these errors. go to manage jenkins script approval and approve the script use a Shared Library use a CLI tool/script via a shell command to do what you need to do Java vs. Groovy The pipeline code has to be written in groovy and therefor can also use java code. Two big difference to note: the usage of double quoted string (gstring, interpreted) and single quoted strings (literal) def abc = 'xyz' # is a literal echo '$abc' # prints $abc echo $abc # prints xyz no use of ; Unfortunately, due to the way the Pipeline code is processed, many of the groovy features don't work or don't work as expected. Things like the lambda's and for-each loops don't work well and are best avoided. In these situations, it is best to keep to the standard syntax of Java. For more information on how the groovy is being processed, it is best to read the technical-design . Env (object) The env object is an object that is available to use in any pipeline script. The env object allows you to store objects and variables to be used anywhere during the script. So things can be shared between nodes, the master and nodes and code blocks. Why would you want to use it? As in general, global variables are a bad practice. But if you need to have variables to be available through the execution on different machines (master, nodes) it is good to use this. Also the env object contains context variables, such as BRANCH_NAME, JOB_NAME and so one. For a complete overview, view the pipeline syntax page. Don't use the env object in functions, always feed them the parameters directly. Only use it in the \"pipeline flow\" and use it for the parameters of the methods. 1 2 3 4 5 6 7 8 node { stage ( SCM ) { checkout scm } stage ( Echo ){ echo Branch=$env.BRANCH_NAME // will print Branch=master } } Stash archive If you need to store files for keeping for later, there are two options available stash and archive . Both should be avoided as they cause heavy I/O traffic, usually between the Node and Master. For more specific information, please consult the Pipeline Syntax Page. Stash Stash allows you to copy files from the current workspace to a temp folder in the workspace in the master. If you're currently on a different machine it will copy them one by one over the network, keep this in mind. The files can only be retrieved during the pipeline execution and you can do so via the unstash command. 1 2 3 4 5 6 7 8 9 10 11 node ( Machine1 ) { stage ( A ) { // generate some files stash excludes: secret.txt , includes: *.txt , name: abc } } node ( Machine2 ) { stage ( B ) { unstash abc } } Saves a set of files for use later in the same build, generally on another node/workspace. Stashed files are not otherwise available and are generally discarded at the end of the build. Note that the stash and unstash steps are designed for use with small files. For large data transfers, use the External Workspace Manager plugin, or use an external repository manager such as Nexus or Artifactory. Archive archiveArtifacts Archives build output artifacts for later use. As of Jenkins 2.x, you may use the more configurable archiveArtifacts. With archive you can store a file semi-permanently in your job. Semi as the files will be overridden by the latest build. The files you archive will be stored in the Job folder on the master. One usecase is to save a log file from a build tool. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 node { stage ( A ) { try { // do some build } finally { // This step should not normally be used in your script. Consult the inline help for details. archive excludes: useless.log , includes: *.log // Use this instead, but only for permanent files, or external logfiles archiveArtifacts allowEmptyArchive: true , artifacts: *.log , excludes: useless.log , fingerprint: true , onlyIfSuccessful: true } } } Credentials In many pipelines you will have to deal with external systems, requiring credentials. Jenkins has the Credentials API which you can also utilize in the pipeline. You can use do this via the Credentials and Credentials Binding plugins, the first is the core plugin the second provides the integration for the pipeline. The best way to generate the required code snippet, is to go to the pipeline syntax page, select withCredentials and configure what you need. 1 2 3 4 5 6 7 node { stage ( someRemoteCall ) { withCredentials ([ usernameColonPassword ( credentialsId: someCredentialsId , variable: USRPASS )]) { sh curl -u $env.USRPASS $URL } } } For more examples, please consult Cloudbees' Injecting-Secrets-into-Jenkins-Build-Jobs blog post. Tools Build Environment Jenkins would not be Jenkins without the direct support for the build tools, such as JDK's, SDK's, Maven, Ant what have you not. So, how do you use them in the pipeline? Unfortunately, this is a bit more cumbersome than it is in a freestyle (or legacy ) job. You have to do two things: retrieve the tool's location via the tool DSL method set the environment variables to suit the tool 1 2 3 4 5 6 7 8 9 10 11 12 13 14 node { stage ( Maven ) { String jdk = tool name: jdk_8 , type: jdk String maven = tool name: maven_3.5.0 , type: maven withEnv ([ JAVA_HOME=$jdk , PATH+MAVEN=${jdk}/bin:${maven}/bin ]) { sh mvn clean install } // or in one go withEnv ([ JAVA_HOME=${ tool jdk_8 } , PATH+MAVEN=${tool maven_3.5.0 }/bin:${env.JAVA_HOME}/bin ]) { sh mvn clean install } } } Pipeline Syntax Page Soooo, do I always have to figure out how to write these code snippets? No, don't worry. You don't have to. At every pipeline job type there is a link called \"Pipeline Syntax\". This gives you a page with a drop down menu, from where you can select all the available steps. Once you select a step, you can use the UI to setup the step and then use the generate button to give you the correct syntax.","title":"Core Concepts"},{"location":"jenkins-pipeline/core-concepts/#core-concepts","text":"Below are some core concepts to understand before building pipelines in Jenkins. Pipeline as Code Step Master vs Nodes Checkout Workspace Stage Sandbox and Script Security Java vs. Groovy Env (object) Stash archive Credentials Tools Build Environment Pipeline Syntax Page","title":"Core Concepts"},{"location":"jenkins-pipeline/core-concepts/#terminology","text":"The terminology used in this page is based upon the terms used by Cloudbees as related to Jenkins. If in doubt, please consult the Jenkins Glossary .","title":"Terminology"},{"location":"jenkins-pipeline/core-concepts/#pipeline-as-code","text":"","title":"Pipeline as Code"},{"location":"jenkins-pipeline/core-concepts/#step","text":"A single task; fundamentally steps tell Jenkins what to do inside of a Pipeline or Project. Consider the following piece of pipeline code: 1 2 3 4 5 6 7 8 9 10 11 node { timestamps { stage ( My FIrst Stage ) { if ( isUnix ()) { sh echo this is Unix! } else { bat echo this is windows } } } } The only execution that happens (almost) exclusively on the node (or build slave) are the isUnix() , sh and bat shell commands. Those specific tasks are the steps in pipeline code.","title":"Step"},{"location":"jenkins-pipeline/core-concepts/#master-vs-nodes","text":"There are many things to keep in mind about Pipelines in Jenkins. By far the most important are those related to the distinction between Masters and Nodes. Aside from the points below, the key thing to keep in mind: Nodes (build slaves) are designed to executes task, Masters are not. Except for the steps themselves, all of the Pipeline logic, the Groovy conditionals, loops, etc execute on the master . Whether simple or complex! Even inside a node block ! Steps may use executors to do work where appropriate, but each step has a small on-master overhead too. Pipeline code is written as Groovy but the execution model is radically transformed at compile-time to Continuation Passing Style (CPS). This transformation provides valuable safety and durability guarantees for Pipelines, but it comes with trade-offs: Steps can invoke Java and execute fast and efficiently, but Groovy is much slower to run than normal. Groovy logic requires far more memory, because an object-based syntax/block tree is kept in memory. Pipelines persist the program and its state frequently to be able to survive failure of the master. Source: Sam van Oort , Cloudbees Engineer","title":"Master vs Nodes"},{"location":"jenkins-pipeline/core-concepts/#node","text":"A machine which is part of the Jenkins environment and capable of executing Pipelines or Projects. Both the Master and Agents are considered to be Nodes.","title":"Node"},{"location":"jenkins-pipeline/core-concepts/#master","text":"The central, coordinating process which stores configuration, loads plugins, and renders the various user interfaces for Jenkins.","title":"Master"},{"location":"jenkins-pipeline/core-concepts/#what-to-do","text":"So, if Pipeline code can cause big loads on Master, what should we do than? Try to limit the use of logic in your groovy code Avoid blocking or I/O calls unless explicitly done on a slave via a Step If you need heavy processing, and there isn't a Step, create either a plugin Shared Library Or use a CLI tool via a platform independent language, such as Java or Go Tip If need to do any I/O, use a plugin or anything related to a workspace, you need a node. If you only need to interact with variables, for example for an input form, do this outside of a node block. See Pipeline Input for how that works.","title":"What to do?"},{"location":"jenkins-pipeline/core-concepts/#workspace","text":"A disposable directory on the file system of a Node where work can be done by a Pipeline or Project. Workspaces are typically left in place after a Build or Pipeline run completes unless specific Workspace cleanup policies have been put in place on the Jenkins Master. The key part of the glossary entry there is disposable directory . There are absolutely no guarantees about Workspaces in pipeline jobs. That said, what you should take care of: always clean your workspace before you start, you don't know the state of the folder you get always clean your workspace after you finish, this way you're less likely to run into problems in subsequent builds a workspace is a temporary folder on a single node's filesystem: so every time you use node{} you have a new workspace after your build is finish or leaving the node otherwise, your workspace should be considered gone: need something from? stash or archive it!","title":"Workspace"},{"location":"jenkins-pipeline/core-concepts/#checkout","text":"There are several ways to do a checkout in the Jenkins pipeline code. In the groovy DSL you can use the Checkout dsl command, svn shorthand or the git shorthand. 1 2 3 4 5 node { stage ( scm ) { git https://github.com/joostvdg/jishi } } Danger If you use a pipeline from SCM, multi-branch pipeline or a derived job type, beware! Only the Jenkinsfile gets checked out. You still need to checkout the rest of your files yourself! Tip However, when using pipeline from SCM, multi-branch pipeline or a derived job type. You can use a shorthand: checkout scm . This checks out the scm defined in your job (where the Jenkinsfile came from). 1 2 3 4 5 node { stage ( scm ) { checkout scm } }","title":"Checkout"},{"location":"jenkins-pipeline/core-concepts/#stage","text":"Stage is a step for defining a conceptually distinct subset of the entire Pipeline, for example: \"Build\", \"Test\", and \"Deploy\", which is used by many plugins to visualize or present Jenkins Pipeline status/progress. The stage \"step\" has a primary function and a secondary function. Its primary function is to define the visual boundaries between logically separable parts of the pipeline. For example, you can define SCM, Build, QA, Deploy as stages to tell you where the build currently is or where it failed. The secondary function is to provided a scope for variables. Just like most programming languages, code blocks are a more than just syntactic sugar, they also limit the scope of variables. 1 2 3 4 5 6 7 8 9 10 node { stage ( SCM ) { def myVar = abc checkout scm } stage ( Build ) { sh mvn clean install echo myVar # will fail because the variable doesn t exist here } }","title":"Stage"},{"location":"jenkins-pipeline/core-concepts/#stages-in-classic-view","text":"","title":"Stages in classic view"},{"location":"jenkins-pipeline/core-concepts/#stages-in-blue-ocean-view","text":"","title":"Stages in Blue Ocean view"},{"location":"jenkins-pipeline/core-concepts/#sandbox-and-script-security","text":"In Jenkins some plugins - such as the pipeline plugin - allow you to write groovy code that gets executed on the master. This means you could run code on the master that accesses the host machine with the same rights as Jenkins. As is unsafe, Jenkins has some guards against this in the form the sandbox mode and the script security . When you create a pipeline job, you get a inline code editor by default. If you're an administrator you get the option to turn the \"sandbox\" mode of. If you use a pipeline from SCM or any of the higher abstraction pipeline job types (Multibranch Pipeline, BitBucket Team) you are always running in sandbox mode. When you're in sandbox mode, your script will run past the script security. This uses a whitelisting technique to block dangerous or undesired methods, but is does so in a very restrictive manner. It could be you're doing something that is safe but still gets blocked. An administrator can then go to the script approval page (under Jenkins Administration) and approve your script. For more details, please consult Script Security plugin page.","title":"Sandbox and Script Security"},{"location":"jenkins-pipeline/core-concepts/#example-error","text":"1 2 3 4 5 6 7 8 org . jenkinsci . plugins . scriptsecurity . sandbox . RejectedAccessException : unclassified staticMethod org . tmatesoft . svn . core . internal . io . dav . DAVRepositoryFactory create org . tmatesoft . svn . core . SVNURL at org . jenkinsci . plugins . scriptsecurity . sandbox . groovy . SandboxInterceptor . onStaticCall ( SandboxInterceptor . java : 138 ) at org . kohsuke . groovy . sandbox . impl . Checker$2 . call ( Checker . java : 180 ) at org . kohsuke . groovy . sandbox . impl . Checker . checkedStaticCall ( Checker . java : 177 ) at org . kohsuke . groovy . sandbox . impl . Checker . checkedCall ( Checker . java : 91 ) at com . cloudbees . groovy . cps . sandbox . SandboxInvoker . methodCall ( SandboxInvoker . java : 16 ) at WorkflowScript . run ( WorkflowScript : 12 ) at ___cps . transform___ ( Native Method ) Tip There are three ways to deal with these errors. go to manage jenkins script approval and approve the script use a Shared Library use a CLI tool/script via a shell command to do what you need to do","title":"Example error"},{"location":"jenkins-pipeline/core-concepts/#java-vs-groovy","text":"The pipeline code has to be written in groovy and therefor can also use java code. Two big difference to note: the usage of double quoted string (gstring, interpreted) and single quoted strings (literal) def abc = 'xyz' # is a literal echo '$abc' # prints $abc echo $abc # prints xyz no use of ; Unfortunately, due to the way the Pipeline code is processed, many of the groovy features don't work or don't work as expected. Things like the lambda's and for-each loops don't work well and are best avoided. In these situations, it is best to keep to the standard syntax of Java. For more information on how the groovy is being processed, it is best to read the technical-design .","title":"Java vs. Groovy"},{"location":"jenkins-pipeline/core-concepts/#env-object","text":"The env object is an object that is available to use in any pipeline script. The env object allows you to store objects and variables to be used anywhere during the script. So things can be shared between nodes, the master and nodes and code blocks. Why would you want to use it? As in general, global variables are a bad practice. But if you need to have variables to be available through the execution on different machines (master, nodes) it is good to use this. Also the env object contains context variables, such as BRANCH_NAME, JOB_NAME and so one. For a complete overview, view the pipeline syntax page. Don't use the env object in functions, always feed them the parameters directly. Only use it in the \"pipeline flow\" and use it for the parameters of the methods. 1 2 3 4 5 6 7 8 node { stage ( SCM ) { checkout scm } stage ( Echo ){ echo Branch=$env.BRANCH_NAME // will print Branch=master } }","title":"Env (object)"},{"location":"jenkins-pipeline/core-concepts/#stash-archive","text":"If you need to store files for keeping for later, there are two options available stash and archive . Both should be avoided as they cause heavy I/O traffic, usually between the Node and Master. For more specific information, please consult the Pipeline Syntax Page.","title":"Stash &amp; archive"},{"location":"jenkins-pipeline/core-concepts/#stash","text":"Stash allows you to copy files from the current workspace to a temp folder in the workspace in the master. If you're currently on a different machine it will copy them one by one over the network, keep this in mind. The files can only be retrieved during the pipeline execution and you can do so via the unstash command. 1 2 3 4 5 6 7 8 9 10 11 node ( Machine1 ) { stage ( A ) { // generate some files stash excludes: secret.txt , includes: *.txt , name: abc } } node ( Machine2 ) { stage ( B ) { unstash abc } } Saves a set of files for use later in the same build, generally on another node/workspace. Stashed files are not otherwise available and are generally discarded at the end of the build. Note that the stash and unstash steps are designed for use with small files. For large data transfers, use the External Workspace Manager plugin, or use an external repository manager such as Nexus or Artifactory.","title":"Stash"},{"location":"jenkins-pipeline/core-concepts/#archive-archiveartifacts","text":"Archives build output artifacts for later use. As of Jenkins 2.x, you may use the more configurable archiveArtifacts. With archive you can store a file semi-permanently in your job. Semi as the files will be overridden by the latest build. The files you archive will be stored in the Job folder on the master. One usecase is to save a log file from a build tool. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 node { stage ( A ) { try { // do some build } finally { // This step should not normally be used in your script. Consult the inline help for details. archive excludes: useless.log , includes: *.log // Use this instead, but only for permanent files, or external logfiles archiveArtifacts allowEmptyArchive: true , artifacts: *.log , excludes: useless.log , fingerprint: true , onlyIfSuccessful: true } } }","title":"Archive &amp; archiveArtifacts"},{"location":"jenkins-pipeline/core-concepts/#credentials","text":"In many pipelines you will have to deal with external systems, requiring credentials. Jenkins has the Credentials API which you can also utilize in the pipeline. You can use do this via the Credentials and Credentials Binding plugins, the first is the core plugin the second provides the integration for the pipeline. The best way to generate the required code snippet, is to go to the pipeline syntax page, select withCredentials and configure what you need. 1 2 3 4 5 6 7 node { stage ( someRemoteCall ) { withCredentials ([ usernameColonPassword ( credentialsId: someCredentialsId , variable: USRPASS )]) { sh curl -u $env.USRPASS $URL } } } For more examples, please consult Cloudbees' Injecting-Secrets-into-Jenkins-Build-Jobs blog post.","title":"Credentials"},{"location":"jenkins-pipeline/core-concepts/#tools-build-environment","text":"Jenkins would not be Jenkins without the direct support for the build tools, such as JDK's, SDK's, Maven, Ant what have you not. So, how do you use them in the pipeline? Unfortunately, this is a bit more cumbersome than it is in a freestyle (or legacy ) job. You have to do two things: retrieve the tool's location via the tool DSL method set the environment variables to suit the tool 1 2 3 4 5 6 7 8 9 10 11 12 13 14 node { stage ( Maven ) { String jdk = tool name: jdk_8 , type: jdk String maven = tool name: maven_3.5.0 , type: maven withEnv ([ JAVA_HOME=$jdk , PATH+MAVEN=${jdk}/bin:${maven}/bin ]) { sh mvn clean install } // or in one go withEnv ([ JAVA_HOME=${ tool jdk_8 } , PATH+MAVEN=${tool maven_3.5.0 }/bin:${env.JAVA_HOME}/bin ]) { sh mvn clean install } } }","title":"Tools &amp; Build Environment"},{"location":"jenkins-pipeline/core-concepts/#pipeline-syntax-page","text":"Soooo, do I always have to figure out how to write these code snippets? No, don't worry. You don't have to. At every pipeline job type there is a link called \"Pipeline Syntax\". This gives you a page with a drop down menu, from where you can select all the available steps. Once you select a step, you can use the UI to setup the step and then use the generate button to give you the correct syntax.","title":"Pipeline Syntax Page"},{"location":"jenkins-pipeline/declarative-pipeline/","text":"Declarative Pipeline Declarative Pipeline is a relatively recent addition to Jenkins Pipeline [1] which presents a more simplified and opinionated syntax on top of the Pipeline sub-systems. All valid Declarative Pipelines must be enclosed within a pipeline block, for example: 1 2 3 pipeline { /* insert Declarative Pipeline here */ } Hello World Example 1 2 3 4 5 6 7 8 9 10 pipeline { agent { docker python:3.5.1 } stages { stage ( build ) { steps { sh python --version } } } } MKDocs Build Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 pipeline { agent none options { timeout ( time: 10 , unit: MINUTES ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: 5 )) } stages { stage ( Prepare ){ agent { label docker } steps { parallel ( Clean: { deleteDir () }, NotifySlack: { slackSend channel: cicd , color: #FFFF00 , message: STARTED: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } ) } } stage ( Checkout ){ agent { label docker } steps { git credentialsId: 355df378-e726-4abd-90fa-e723c5c21ad5 , url: git@gitlab.flusso.nl:CICD/ci-cd-docs.git script { env . GIT_COMMIT_HASH = sh returnStdout: true , script: git rev-parse --verify HEAD } } } stage ( Build Docs ) { agent { docker { image caladreas/mkdocs-docker-build-container label docker } } steps { sh mkdocs build } } stage ( Prepare Docker Image ){ agent { label docker } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true , script: docker run --rm -i lukasmartinelli/hadolint Dockerfile if ( lintResult . trim () == ) { println Lint finished with no errors } else { println Error found in Lint println ${lintResult} currentBuild . result = UNSTABLE } } }, // end test dockerfile BuildImage: { sh chmod +x build.sh sh ./build.sh } ) } post { success { sh chmod +x push.sh sh ./push.sh } } } stage ( Update Docker Container ) { agent { label docker } steps { sh chmod +x container-update.sh sh ./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH} } } } post { success { slackSend channel: cicd , color: #00FF00 , message: SUCCESSFUL: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } failure { slackSend channel: cicd , color: #FF0000 , message: FAILED: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } } } Resources Syntax Reference Getting started Notifications","title":"Pipeline (declarative)"},{"location":"jenkins-pipeline/declarative-pipeline/#declarative-pipeline","text":"Declarative Pipeline is a relatively recent addition to Jenkins Pipeline [1] which presents a more simplified and opinionated syntax on top of the Pipeline sub-systems. All valid Declarative Pipelines must be enclosed within a pipeline block, for example: 1 2 3 pipeline { /* insert Declarative Pipeline here */ }","title":"Declarative Pipeline"},{"location":"jenkins-pipeline/declarative-pipeline/#hello-world-example","text":"1 2 3 4 5 6 7 8 9 10 pipeline { agent { docker python:3.5.1 } stages { stage ( build ) { steps { sh python --version } } } }","title":"Hello World Example"},{"location":"jenkins-pipeline/declarative-pipeline/#mkdocs-build-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 pipeline { agent none options { timeout ( time: 10 , unit: MINUTES ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: 5 )) } stages { stage ( Prepare ){ agent { label docker } steps { parallel ( Clean: { deleteDir () }, NotifySlack: { slackSend channel: cicd , color: #FFFF00 , message: STARTED: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } ) } } stage ( Checkout ){ agent { label docker } steps { git credentialsId: 355df378-e726-4abd-90fa-e723c5c21ad5 , url: git@gitlab.flusso.nl:CICD/ci-cd-docs.git script { env . GIT_COMMIT_HASH = sh returnStdout: true , script: git rev-parse --verify HEAD } } } stage ( Build Docs ) { agent { docker { image caladreas/mkdocs-docker-build-container label docker } } steps { sh mkdocs build } } stage ( Prepare Docker Image ){ agent { label docker } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true , script: docker run --rm -i lukasmartinelli/hadolint Dockerfile if ( lintResult . trim () == ) { println Lint finished with no errors } else { println Error found in Lint println ${lintResult} currentBuild . result = UNSTABLE } } }, // end test dockerfile BuildImage: { sh chmod +x build.sh sh ./build.sh } ) } post { success { sh chmod +x push.sh sh ./push.sh } } } stage ( Update Docker Container ) { agent { label docker } steps { sh chmod +x container-update.sh sh ./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH} } } } post { success { slackSend channel: cicd , color: #00FF00 , message: SUCCESSFUL: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } failure { slackSend channel: cicd , color: #FF0000 , message: FAILED: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } } }","title":"MKDocs Build Example"},{"location":"jenkins-pipeline/declarative-pipeline/#resources","text":"Syntax Reference Getting started Notifications","title":"Resources"},{"location":"jenkins-pipeline/global-shared-library/","text":"Global Shared Library https://jenkins.io/doc/book/pipeline/shared-libraries/ When you're making pipelines on Jenkins you will run into the situation that you will want to stay DRY . To share pipeline code there are several ways. SCM: Have a pipeline dsl script in a SCM and load it from there Plugin: A Jenkins plugin that you can call via the pipeline dsl Global Workflow Library: There is a global library for pipeline dsl scripts in the Jekins master Preferred solution Please read the documentation to get a basic idea. Danger When using a Global Library you will always have to import something from this library. This doesn't make sense when you online use functions (via the vars folder). In this case, you have to import nothing, which you do via: \"_\" 1 2 @Library ( FlussoGlobal ) import nl.flusso.Utilities 1 @Library ( FlussoGlobal ) _ Library Directory structure The directory structure of a shared library repository is as follows: 1 2 3 4 5 6 7 8 9 10 11 12 (root) +- src # Groovy source files | +- org | +- foo | +- Bar.groovy # for org.foo.Bar class +- vars | +- foo.groovy # for global foo variable/function | +- foo.txt # help for foo variable/function +- resources # resource files (external libraries only) | +- org | +- foo | +- bar.json # static helper data for org.foo.Bar The src directory should look like standard Java source directory structure. This directory is added to the classpath when executing Pipelines. The vars directory hosts scripts that define global variables accessible from Pipeline scripts. The basename of each *.groovy file should be a Groovy (~ Java) identifier, conventionally camelCased . The matching *.txt , if present, can contain documentation, processed through the system\u2019s configured markup formatter (so may really be HTML, Markdown, etc., though the txt extension is required). The Groovy source files in these directories get the same \u201cCPS transformation\u201d as your Pipeline scripts. A resources directory allows the libraryResource step to be used from an external library to load associated non-Groovy files. Currently this feature is not supported for internal libraries. Other directories under the root are reserved for future enhancements. Configure libraries in Jenkins The a Jenkins Master you can configure the Global Pipeline Libraries. You can find this in: Manage Jenkins - Configure System - Global Pipeline Libraries You can configure multiple libraries, where the there is a preference for Git repositories. You can select a default version (for example: the master branch), and either allow or disallow overrides to this. To be able to use a different version, you would use the @ in case of Git. 1 @Library ( FlussoGlobal @my - feature - branch ) HelloWorld Example Create Git repository (see below for structure) Configure this Git repository as an \"Global Pipeline Libraries\" entry Name: FlussoGlobal Default Version: master Modern SCM: git Project repository: :CICD/jenkins-pipeline-library.git Create the resources you want in the git repository Use the library in a pipeline Util Class (class) Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/usr/bin/groovy # /src/ nl /flusso/ Utilities . groovy package nl . flusso import java.io.Serializable class Utilities implements Serializable { def steps Utilities ( steps ) { this . steps = steps } def sayHello ( String name ) { steps . sh echo $name } } 1 2 3 4 5 6 7 8 9 @Library ( FlussoGlobal ) import nl.flusso.Utilities def utils = new Utilities ( steps ) node { String name = Joost utils . sayHello ( name ) } Util method (var) Example 1 2 3 4 5 6 #!/usr/bin/groovy # /vars/ sayHello . groovy def call ( name ) { // you can call any valid step functions from your code, just like you can from Pipeline scripts echo Hello world, ${name} } 1 2 3 4 5 @Library ( FlussoGlobal ) _ node { String name = Joost sayHello name } Combining libraries Lets say you want to want to have a core library and multiple specific libraries that utilize these. There are several to do this, we will show two. Import both One way is to explicitly import both libraries in the Jenkinsfile. 1 @Library ([ github.com/joostvdg/jenkins-pipeline-lib , github.com/joostvdg/jenkins-pipeline-go ]) _ Con: you have to import all the required libraries yourself Pro: you can specify the versions of each Implicit Import + Explicit Import You can also configure the core (in this case jenkins-pipeline-lib) as \"loaded implicitly\". This will make anything from this library available by default. Be careful with the naming of the vars though! The resulting Jenkinsfile would then be. 1 @Library ( github.com/joostvdg/jenkins-pipeline-go ) _ Resources implement-reusable-function-call","title":"Pipeline Libraries"},{"location":"jenkins-pipeline/global-shared-library/#global-shared-library","text":"https://jenkins.io/doc/book/pipeline/shared-libraries/ When you're making pipelines on Jenkins you will run into the situation that you will want to stay DRY . To share pipeline code there are several ways. SCM: Have a pipeline dsl script in a SCM and load it from there Plugin: A Jenkins plugin that you can call via the pipeline dsl Global Workflow Library: There is a global library for pipeline dsl scripts in the Jekins master Preferred solution Please read the documentation to get a basic idea. Danger When using a Global Library you will always have to import something from this library. This doesn't make sense when you online use functions (via the vars folder). In this case, you have to import nothing, which you do via: \"_\" 1 2 @Library ( FlussoGlobal ) import nl.flusso.Utilities 1 @Library ( FlussoGlobal ) _","title":"Global Shared Library"},{"location":"jenkins-pipeline/global-shared-library/#library-directory-structure","text":"The directory structure of a shared library repository is as follows: 1 2 3 4 5 6 7 8 9 10 11 12 (root) +- src # Groovy source files | +- org | +- foo | +- Bar.groovy # for org.foo.Bar class +- vars | +- foo.groovy # for global foo variable/function | +- foo.txt # help for foo variable/function +- resources # resource files (external libraries only) | +- org | +- foo | +- bar.json # static helper data for org.foo.Bar The src directory should look like standard Java source directory structure. This directory is added to the classpath when executing Pipelines. The vars directory hosts scripts that define global variables accessible from Pipeline scripts. The basename of each *.groovy file should be a Groovy (~ Java) identifier, conventionally camelCased . The matching *.txt , if present, can contain documentation, processed through the system\u2019s configured markup formatter (so may really be HTML, Markdown, etc., though the txt extension is required). The Groovy source files in these directories get the same \u201cCPS transformation\u201d as your Pipeline scripts. A resources directory allows the libraryResource step to be used from an external library to load associated non-Groovy files. Currently this feature is not supported for internal libraries. Other directories under the root are reserved for future enhancements.","title":"Library Directory structure"},{"location":"jenkins-pipeline/global-shared-library/#configure-libraries-in-jenkins","text":"The a Jenkins Master you can configure the Global Pipeline Libraries. You can find this in: Manage Jenkins - Configure System - Global Pipeline Libraries You can configure multiple libraries, where the there is a preference for Git repositories. You can select a default version (for example: the master branch), and either allow or disallow overrides to this. To be able to use a different version, you would use the @ in case of Git. 1 @Library ( FlussoGlobal @my - feature - branch )","title":"Configure libraries in Jenkins"},{"location":"jenkins-pipeline/global-shared-library/#helloworld-example","text":"Create Git repository (see below for structure) Configure this Git repository as an \"Global Pipeline Libraries\" entry Name: FlussoGlobal Default Version: master Modern SCM: git Project repository: :CICD/jenkins-pipeline-library.git Create the resources you want in the git repository Use the library in a pipeline","title":"HelloWorld Example"},{"location":"jenkins-pipeline/global-shared-library/#util-class-class-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/usr/bin/groovy # /src/ nl /flusso/ Utilities . groovy package nl . flusso import java.io.Serializable class Utilities implements Serializable { def steps Utilities ( steps ) { this . steps = steps } def sayHello ( String name ) { steps . sh echo $name } } 1 2 3 4 5 6 7 8 9 @Library ( FlussoGlobal ) import nl.flusso.Utilities def utils = new Utilities ( steps ) node { String name = Joost utils . sayHello ( name ) }","title":"Util Class (class) Example"},{"location":"jenkins-pipeline/global-shared-library/#util-method-var-example","text":"1 2 3 4 5 6 #!/usr/bin/groovy # /vars/ sayHello . groovy def call ( name ) { // you can call any valid step functions from your code, just like you can from Pipeline scripts echo Hello world, ${name} } 1 2 3 4 5 @Library ( FlussoGlobal ) _ node { String name = Joost sayHello name }","title":"Util method (var) Example"},{"location":"jenkins-pipeline/global-shared-library/#combining-libraries","text":"Lets say you want to want to have a core library and multiple specific libraries that utilize these. There are several to do this, we will show two.","title":"Combining libraries"},{"location":"jenkins-pipeline/global-shared-library/#import-both","text":"One way is to explicitly import both libraries in the Jenkinsfile. 1 @Library ([ github.com/joostvdg/jenkins-pipeline-lib , github.com/joostvdg/jenkins-pipeline-go ]) _ Con: you have to import all the required libraries yourself Pro: you can specify the versions of each","title":"Import both"},{"location":"jenkins-pipeline/global-shared-library/#implicit-import-explicit-import","text":"You can also configure the core (in this case jenkins-pipeline-lib) as \"loaded implicitly\". This will make anything from this library available by default. Be careful with the naming of the vars though! The resulting Jenkinsfile would then be. 1 @Library ( github.com/joostvdg/jenkins-pipeline-go ) _","title":"Implicit Import + Explicit Import"},{"location":"jenkins-pipeline/global-shared-library/#resources","text":"implement-reusable-function-call","title":"Resources"},{"location":"jenkins-pipeline/groovy-pipeline/","text":"Jenkins Pipelines Warning This style of pipeline definition is deprecated. When possible, please use the declarative version. Jenkins Pipeline is a suite of plugins which supports implementing and integrating continuous delivery pipelines into Jenkins. Pipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines \"as code\" via the Pipeline DSL. There are two ways to create pipelines in Jenkins. Either via the Groovy DSL or via the Declarative pipeline . For more information about the declarative pipeline, read the next page . Hello World Example 1 2 3 4 5 6 7 8 9 10 11 node { timestamps { stage ( My FIrst Stage ) { if ( isUnix ()) { sh echo this is Unix! } else { bat echo this is windows } } } } Resources Getting started Best practices Best practices for scaling Possible Steps","title":"Groovy DSL Pipeline"},{"location":"jenkins-pipeline/groovy-pipeline/#jenkins-pipelines","text":"Warning This style of pipeline definition is deprecated. When possible, please use the declarative version. Jenkins Pipeline is a suite of plugins which supports implementing and integrating continuous delivery pipelines into Jenkins. Pipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines \"as code\" via the Pipeline DSL. There are two ways to create pipelines in Jenkins. Either via the Groovy DSL or via the Declarative pipeline . For more information about the declarative pipeline, read the next page .","title":"Jenkins Pipelines"},{"location":"jenkins-pipeline/groovy-pipeline/#hello-world-example","text":"1 2 3 4 5 6 7 8 9 10 11 node { timestamps { stage ( My FIrst Stage ) { if ( isUnix ()) { sh echo this is Unix! } else { bat echo this is windows } } } }","title":"Hello World Example"},{"location":"jenkins-pipeline/groovy-pipeline/#resources","text":"Getting started Best practices Best practices for scaling Possible Steps","title":"Resources"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/","text":"IDE Integration for Jenkins Pipeline DSL Supported IDE's Currently only Jetbrain's Intelli J's IDEA is supported . This via a Groovy DSL file (.gdsl). Configure Intelli J IDEA Go to a Jenkins Pipeline job and open the Pipeline Syntax page. On the page in the left hand menu, you will see a link to download a Jenkins Master specific Groovy DSL file. Download this and save it into your project's workspace. It will have to be part of your classpath, the easiest way to do this is to add the file as pipeline.gdsl in a/the src folder. For more information, you can read Steffen Gerbert 's blog. Remarks from Kohsuke Kawaguchi More effort in this space will be taken by Cloudbees. But the priority is low compared to other initiatives. Integration of Pipeline Library If you're using the Global Shared Libraries for sharing generic pipeline building blocks, it would be nice to have this awareness in your editor as well. One of the ways to do this, is to checkout the source code of this library and make sure it is compiled. In your editor (assuming Intelli J IDEA) you can then add the compiled classes as dependency (type: classes). This way, at least every class defined in your library is usable as a normal dependency would be. Final configuration Intelli J IDEA","title":"DSL IDE Integration"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#ide-integration-for-jenkins-pipeline-dsl","text":"","title":"IDE Integration for Jenkins Pipeline DSL"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#supported-ides","text":"Currently only Jetbrain's Intelli J's IDEA is supported . This via a Groovy DSL file (.gdsl).","title":"Supported IDE's"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#configure-intelli-j-idea","text":"Go to a Jenkins Pipeline job and open the Pipeline Syntax page. On the page in the left hand menu, you will see a link to download a Jenkins Master specific Groovy DSL file. Download this and save it into your project's workspace. It will have to be part of your classpath, the easiest way to do this is to add the file as pipeline.gdsl in a/the src folder. For more information, you can read Steffen Gerbert 's blog.","title":"Configure Intelli J IDEA"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#remarks-from-kohsuke-kawaguchi","text":"More effort in this space will be taken by Cloudbees. But the priority is low compared to other initiatives.","title":"Remarks from Kohsuke Kawaguchi"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#integration-of-pipeline-library","text":"If you're using the Global Shared Libraries for sharing generic pipeline building blocks, it would be nice to have this awareness in your editor as well. One of the ways to do this, is to checkout the source code of this library and make sure it is compiled. In your editor (assuming Intelli J IDEA) you can then add the compiled classes as dependency (type: classes). This way, at least every class defined in your library is usable as a normal dependency would be.","title":"Integration of Pipeline Library"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#final-configuration-intelli-j-idea","text":"","title":"Final configuration Intelli J IDEA"},{"location":"jenkins-pipeline/input/","text":"Jenkins Pipeline - Input The Jenkins Pipeline has a plugin for dealing with external input. Generally it is used to gather user input (values or approval), but it also has a REST API for this. General Info The Pipeline Input Step allows you to The plugin allows you to capture input in a variety of ways, but there are some gotcha's. If you have a single parameter, it will be returned as a single value If you have multiple parameters, it will be returned as a map The choices for the Choice parameter should be a single line, where values are separated with /n Don't use input within a node {}, as this will block an executor slot .. Examples Single Parameter 1 2 3 4 5 def hello = input id: CustomId , message: Want to continue? , ok: Yes , parameters: [ string ( defaultValue: world , description: , name: hello )] node { println echo $hello } Multiple Parameters 1 2 3 4 5 6 7 def userInput = input id: CustomId , message: Want to continue? , ok: Yes , parameters: [ string ( defaultValue: world , description: , name: hello ), string ( defaultValue: , description: , name: token )] node { def hello = userInput [ hello ] def token = userInput [ token ] println hello=$hello, token=$token } Timeout on Input 1 2 3 4 5 6 def userInput timeout ( time: 10 , unit: SECONDS ) { println Waiting for input userInput = input id: CustomId , message: Want to continue? , ok: Yes , parameters: [ string ( defaultValue: world , description: , name: hello ), string ( defaultValue: , description: , name: token )] } REST API There's a rest API for sending the input to a waiting input step. The format of the url: {JenkinsURL}/ {JenkinsURL}/ {JobURL}/ {Build#}/input/ {Build#}/input/ {InputID}/submit. There are some things to keep in mind: If Jenkins has CSRF protection enabled, you need a Crumb (see below) for the requests Requests are send via POST For supplying values you need to have a JSON with the parameters with as json param You need to supply the proceed value: the value of the ok button, as proceed param You will have to fill in the input id , so it is best to configure a unique input id for the input steps you want to connect to from outside Examples 1 2 3 4 5 6 { parameter : [ { name : hello , value : joost }, { name : token , value : not a token } ] } 1 2 # single parameter curl --user $USER : $PASS -X POST -H Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33 -d json = { parameter : { name : hello , value : joost }} -d proceed = Yes https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit 1 2 # Multiple Parameters curl --user $USER : $PASS -X POST -H Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33 -d json = { parameter : [{ name : hello , value : joost },{ name : token , value : not a token }]} -d proceed = Yes https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit Crumb (secured Jenkins) If Jenkins is secured against CSRF (via Global Security: Prevent Cross Site Request Forgery exploits), any API call requires a Crumb. You can read more about it here . To get a valid crumb you have to send a crumb request as authenticated user. JSON: https://ci.flusso.nl/jenkins/crumbIssuer/api/json XML (parsed): https://ci.flusso.nl/jenkins/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,%22:%22,//crumb )","title":"Input"},{"location":"jenkins-pipeline/input/#jenkins-pipeline-input","text":"The Jenkins Pipeline has a plugin for dealing with external input. Generally it is used to gather user input (values or approval), but it also has a REST API for this.","title":"Jenkins Pipeline - Input"},{"location":"jenkins-pipeline/input/#general-info","text":"The Pipeline Input Step allows you to The plugin allows you to capture input in a variety of ways, but there are some gotcha's. If you have a single parameter, it will be returned as a single value If you have multiple parameters, it will be returned as a map The choices for the Choice parameter should be a single line, where values are separated with /n Don't use input within a node {}, as this will block an executor slot ..","title":"General Info"},{"location":"jenkins-pipeline/input/#examples","text":"","title":"Examples"},{"location":"jenkins-pipeline/input/#single-parameter","text":"1 2 3 4 5 def hello = input id: CustomId , message: Want to continue? , ok: Yes , parameters: [ string ( defaultValue: world , description: , name: hello )] node { println echo $hello }","title":"Single Parameter"},{"location":"jenkins-pipeline/input/#multiple-parameters","text":"1 2 3 4 5 6 7 def userInput = input id: CustomId , message: Want to continue? , ok: Yes , parameters: [ string ( defaultValue: world , description: , name: hello ), string ( defaultValue: , description: , name: token )] node { def hello = userInput [ hello ] def token = userInput [ token ] println hello=$hello, token=$token }","title":"Multiple Parameters"},{"location":"jenkins-pipeline/input/#timeout-on-input","text":"1 2 3 4 5 6 def userInput timeout ( time: 10 , unit: SECONDS ) { println Waiting for input userInput = input id: CustomId , message: Want to continue? , ok: Yes , parameters: [ string ( defaultValue: world , description: , name: hello ), string ( defaultValue: , description: , name: token )] }","title":"Timeout on Input"},{"location":"jenkins-pipeline/input/#rest-api","text":"There's a rest API for sending the input to a waiting input step. The format of the url: {JenkinsURL}/ {JenkinsURL}/ {JobURL}/ {Build#}/input/ {Build#}/input/ {InputID}/submit. There are some things to keep in mind: If Jenkins has CSRF protection enabled, you need a Crumb (see below) for the requests Requests are send via POST For supplying values you need to have a JSON with the parameters with as json param You need to supply the proceed value: the value of the ok button, as proceed param You will have to fill in the input id , so it is best to configure a unique input id for the input steps you want to connect to from outside","title":"REST API"},{"location":"jenkins-pipeline/input/#examples_1","text":"1 2 3 4 5 6 { parameter : [ { name : hello , value : joost }, { name : token , value : not a token } ] } 1 2 # single parameter curl --user $USER : $PASS -X POST -H Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33 -d json = { parameter : { name : hello , value : joost }} -d proceed = Yes https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit 1 2 # Multiple Parameters curl --user $USER : $PASS -X POST -H Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33 -d json = { parameter : [{ name : hello , value : joost },{ name : token , value : not a token }]} -d proceed = Yes https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit","title":"Examples"},{"location":"jenkins-pipeline/input/#crumb-secured-jenkins","text":"If Jenkins is secured against CSRF (via Global Security: Prevent Cross Site Request Forgery exploits), any API call requires a Crumb. You can read more about it here . To get a valid crumb you have to send a crumb request as authenticated user. JSON: https://ci.flusso.nl/jenkins/crumbIssuer/api/json XML (parsed): https://ci.flusso.nl/jenkins/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,%22:%22,//crumb )","title":"Crumb (secured Jenkins)"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/","text":"","title":"Parallel"},{"location":"jenkins-pipeline/shared-library/","text":"","title":"Shared library"},{"location":"jenkins-pipeline-examples/","text":"Jenkins Pipeline Examples Please mind that these examples all assume the following: you have Jenkins 2.32+ you have a recent set of pipeline plugins Jenkins Pipeline Model Jenkins Blue Ocean Jenkins Pipeline Maven Build timeout plugin Credentials Binding Credentials Pipeline Multi-Branch SonarQube Timestamper Pipeline Supporting APIs Pipeline Shared Groovy Libraries","title":"Index"},{"location":"jenkins-pipeline-examples/#jenkins-pipeline-examples","text":"Please mind that these examples all assume the following: you have Jenkins 2.32+ you have a recent set of pipeline plugins Jenkins Pipeline Model Jenkins Blue Ocean Jenkins Pipeline Maven Build timeout plugin Credentials Binding Credentials Pipeline Multi-Branch SonarQube Timestamper Pipeline Supporting APIs Pipeline Shared Groovy Libraries","title":"Jenkins Pipeline Examples"},{"location":"jenkins-pipeline-examples/docker-alternatives/","text":"Pipelines With Docker Alternatives Building pipelines with Jenkins on Docker has been common for a while. But accessing the docker socket has always been a bit tricky. The easiest solution is directly mounting the docker socket of the host into your build container. However, this is a big security risk and something that is undesirable at best and, pretty dangerous at worst. When you're using an orchestrator such as Kubernetes - which is where Jenkins is currently best put to work - the problem gets worse. Not only do you open up security holes, you also mess up the schedular's insight into available resources, and mess with it's ability to keep your cluster in a correct state. In some sense, using docker directly in a Kubernetes cluster, is likened to running with scissors . Potential Alternatives So, directly running docker containers via a docker engine inside your cluster is out. Let's look at some alternatives. Kubernetes Pod and External Node : the simple way out, use a cloud (such as EC2 Cloud) to provision a classic VM with docker engine as an agent and build there JIB : tool from Google, only works for Java applications and is supported directly from Gradle and Maven - Link Kaniko : tool from Google, works as 1-on-1 replacement for docker image build (except Windows Containers ) - Link IMG : tool from Jess Frazelle to avoid building docker images with a root user involved Link Kubernetes Pod and External Node One of the most used cloud environments is AWS, so I created this solution with AWS's Amazon EC2 Plugin . Warning Unfortunately, you cannot combine the Kubernetes plugin with external nodes (none pod container nodes) in a Declarative pipeline. So you have to use Scripted . This can be done with various different cloud providers such as Digital Ocean, Google, AWS or Azure. This guide will use AWS, as it has the most mature Jenkins Cloud plugin. Prerequisites AWS Account with rights to create AMI's and run EC2 instances Packer Jenkins with Amazon EC2 Plugin installed Steps create AMI with Packer install and configure Amazon EC2 plugin create a test pipeline Create AMI with Packer Packer needs to be able to access EC2 API's and be able to spin up an EC2 instance and create an AMI out of it. AWS setup for Packer You need to configure two things: account details for Packer to use security group where your EC2 instances will be running with this security group needs to open port 22 both Packer and Jenkins will use this for their connection 1 2 3 export AWS_DEFAULT_REGION = eu-west-1 export AWS_ACCESS_KEY_ID = XXX export AWS_SECRET_ACCESS_KEY = XXX 1 2 3 4 5 6 7 aws ec2 --profile myAwsProfile create-security-group \\ --description For building Docker images \\ --group-name docker { GroupId : sg-08079f78cXXXXXXX } Export the security group ID. 1 2 export SG_ID = sg-08079f78cXXXXXXX echo $SG_ID Enable port 22 1 2 3 4 5 6 7 aws ec2 \\ --profile myAwsProfile \\ authorize-security-group-ingress \\ --group-name docker \\ --protocol tcp \\ --port 22 \\ --cidr 0 .0.0.0/0 Packer AMI definition Here's an example definition for Packer for a Ubuntu 18.04 LTS base image with JDK 8 (required by Jenkins) and Docker. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { builders : [{ type : amazon-ebs , region : eu-west-1 , source_ami_filter : { filters : { virtualization-type : hvm , name : *ubuntu-bionic-18.04-amd64-server-* , root-device-type : ebs }, owners : [ 679593333241 ], most_recent : true }, instance_type : t2.micro , ssh_username : ubuntu , ami_name : docker , force_deregister : true }], provisioners : [{ type : shell , inline : [ sleep 15 , sudo apt-get clean , sudo apt-get update , sudo apt-get install -y apt-transport-https ca-certificates nfs-common , curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - , sudo add-apt-repository \\ deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\ , sudo add-apt-repository -y ppa:openjdk-r/ppa , sudo apt-get update , sudo apt-get install -y docker-ce , sudo usermod -aG docker ubuntu , sudo apt-get install -y openjdk-8-jdk , java -version , docker version ] }] } Build the new AMI with packer. 1 2 packer build docker-ami.json export AMI = ami-0212ab37f84e418f4 EC2 Key Pair Create EC2 key pair, this will be used by Jenkins to connect to the instances via ssh. 1 2 3 4 aws ec2 --profile myAwsProfile create-key-pair \\ --key-name jenkinsec2 \\ | jq -r .KeyMaterial \\ jenkins-ec2-proton.pem EC2 Cloud Configuration In a Jenkins' master main configuration, you add a new cloud . In this case, we will use a ec2-cloud so we can instantiate our EC2 VM's with docker. use EC2 credentials for initial connection use key ( .pem ) for VM connection (jenkins agent) configure the following: AMI: ami-0212ab37f84e418f4 availability zone: eu-west-1a VPC SubnetID: subnet-aa54XXXX Remote user: ubuntu labels: docker ubuntu linux SecurityGroup Name: (the id) sg-08079f78cXXXXXXX public ip = true connect via public ip = true Pipeline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @Library ( jenkins-pipeline-library@master ) _ def scmVars def label = jenkins-slave-${UUID.randomUUID().toString()} podTemplate ( label: label , yaml: apiVersion: v1 kind: Pod spec: containers: - name: kubectl image: vfarcic/kubectl command: [ cat ] tty: true ) { node ( label ) { node ( docker ) { stage ( SCM Prepare ) { scmVars = checkout scm } stage ( Lint ) { dockerfileLint () } stage ( Build Docker ) { sh docker image build -t demo:rc-1 . } stage ( Tag Push Docker ) { IMAGE = ${DOCKER_IMAGE_NAME} TAG = ${DOCKER_IMAGE_TAG} FULL_NAME = ${FULL_IMAGE_NAME} withCredentials ([ usernamePassword ( credentialsId: dockerhub , usernameVariable: USER , passwordVariable: PASS )]) { sh docker login -u $USER -p $PASS } sh docker image tag ${IMAGE}:${TAG} ${FULL_NAME} sh docker image push ${FULL_NAME} } } // end node docker stage ( Prepare Pod ) { // have to checkout on our kubernetes pod aswell checkout scm } stage ( Check version ) { container ( kubectl ) { sh kubectl version } } } // end node random label } // end pod def Maven JIB If you use Java with either Gradle or Maven , you can use JIB to create docker image without requiring a docker client or docker engine. JIB will communicate with DockerHub and Google's container registry for sending image layers back and forth. The last layer will be created by the JIB plugin itself, adding your self-executing Jar to the layer and a Entrypoint with the correct flags. For more information about the runtime options, see either jib-maven-plugin documentation or jib-gradle-plugin . Prerequisites Java project build with Gradle or Maven Java project that can start it self, such as Spring Boot or Thorntail (previously Wildfly Swarm, from the JBoss family) able to build either gradle or maven applications Steps configure the plugin for either Gradle or Maven build using an official docker image via the kubernetes pod template Pipeline Using a bit more elaborate pipeline example here. Using SonarCloud for static code analysis and then JIB to create a docker image. We could then use that image either directly in anything that runs docker or in the same cluster via a Helm Chart. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def scmVars def tag pipeline { options { buildDiscarder logRotator ( artifactDaysToKeepStr: 5 , artifactNumToKeepStr: 5 , daysToKeepStr: 5 , numToKeepStr: 5 ) } libraries { lib ( jenkins-pipeline-library@master ) } agent { kubernetes { label mypod defaultContainer jnlp yaml apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:3-jdk-11-slim command: - cat tty: true } } stages { stage ( Test versions ) { steps { container ( maven ) { sh uname -a sh mvn -version } } } stage ( Checkout ) { steps { script { scmVars = checkout scm } gitRemoteConfigByUrl ( scmVars . GIT_URL , githubtoken ) sh git config --global user.email jenkins@jenkins.io git config --global user.name Jenkins } } stage ( Build ) { steps { container ( maven ) { sh mvn clean verify -B -e } } } stage ( Version Analysis ) { parallel { stage ( Version Bump ) { when { branch master } environment { NEW_VERSION = gitNextSemverTagMaven ( pom.xml ) } steps { script { tag = ${NEW_VERSION} } container ( maven ) { sh mvn versions:set -DnewVersion=${NEW_VERSION} } gitTag ( v${NEW_VERSION} ) } } stage ( Sonar Analysis ) { when { branch master } environment { SONAR_HOST = https://sonarcloud.io KEY = spring-maven-demo ORG = demomon SONAR_TOKEN = credentials ( sonarcloud ) } steps { container ( maven ) { sh mvn sonar:sonar \\ -Dsonar.projectKey=${KEY} \\ -Dsonar.organization=${ORG} \\ -Dsonar.host.url=${SONAR_HOST} \\ -Dsonar.login=${SONAR_TOKEN} } } } } } stage ( Publish Artifact ) { when { branch master } environment { DHUB = credentials ( dockerhub ) } steps { container ( maven ) { // we should never come here if the tests have not run, as we run verify before sh mvn clean compile -B -e jib:build -Djib.to.auth.username=${DHUB_USR} -Djib.to.auth.password=${DHUB_PSW} -DskipTests } } } } post { always { cleanWs () } } } Kaniko Google loves Kubernetes and Google prefers people building docker images in Kubernetes without Docker. As JIB is only available for Java projects, there's needs to be an alternative for any other usecase/programming language. The answer to that is Kaniko , a specialized Docker image to create Docker images. Kaniko isn't the most secure way to create docker images, it barely beats mounting a Docker Socket, or might even be worse if you ask others (such as Jess Frazelle ). That said, it is gaining some traction being used by JenkinsX and having an example in Jenkins' Kubernetes Plugin . Info When building more than one image inside the kaniko container, make sure to use the --cleanup flag. So it cleans its temporary cache data before building the next image, as discussed in this google group . Prerequisites Steps Create docker registry secret Configure pod container template Configure stage Create docker registry secret This is an example for DockerHub inside the build namespace. 1 2 3 4 5 kubectl create secret docker-registry -n build regcred \\ --docker-server = index.docker.io \\ --docker-username = myDockerHubAccount \\ --docker-password = myDockerHubPassword \\ --docker-email = myDockerHub@Email.com Example Ppeline Warning Although multi-stage Dockerfile 's are supported, it did fail in my case. So I created a second Dockerfile which is only for running the application ( Dockerfile.run ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 pipeline { agent { kubernetes { //cloud kubernetes label kaniko yaml kind: Pod metadata: name: kaniko spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root - name: go-build-cache mountPath: /root/.cache/go-build - name: img-build-cache mountPath: /root/.local volumes: - name: go-build-cache emptyDir: {} - name: img-build-cache emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { environment { PATH = /busybox:$PATH } steps { container ( name: kaniko , shell: /busybox/sh ) { sh #!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=index.docker.io/caladreas/cat } } } } } IMG img is the brainchild of Jess Frazelle, a prominent figure in the container space. The goal is to be the safest and best way to build OCI compliant images, as outlined in her blog building container images securely on kubernetes . Not working (for me) yet It does not seem to work for me on AWS's EKS. To many little details with relation to runc, file permissions and other configuration. For those who want to give it a spin, here are some resources to take a look at. https://blog.jessfraz.com/post/building-container-images-securely-on-kubernetes/ https://github.com/genuinetools/img https://github.com/opencontainers/runc https://git.j3ss.co/genuinetools/img/+/d05b3e4e10cd0e3c074ffb03dc22d7bb6cde1e78 Pipeline Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 pipeline { agent { kubernetes { label img yaml kind: Pod metadata: name: img annotations: container.apparmor.security.beta.kubernetes.io/img: unconfined spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: img workingDir: /home/jenkins image: caladreas/img:0.5.1 imagePullPolicy: Always securityContext: rawProc: true privileged: true command: - cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: temp emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { steps { container ( img ) { sh mkdir cache sh img build -s ./cache -f Dockerfile.run -t caladreas/cat . } } } } }","title":"Docker Alternatives"},{"location":"jenkins-pipeline-examples/docker-alternatives/#pipelines-with-docker-alternatives","text":"Building pipelines with Jenkins on Docker has been common for a while. But accessing the docker socket has always been a bit tricky. The easiest solution is directly mounting the docker socket of the host into your build container. However, this is a big security risk and something that is undesirable at best and, pretty dangerous at worst. When you're using an orchestrator such as Kubernetes - which is where Jenkins is currently best put to work - the problem gets worse. Not only do you open up security holes, you also mess up the schedular's insight into available resources, and mess with it's ability to keep your cluster in a correct state. In some sense, using docker directly in a Kubernetes cluster, is likened to running with scissors .","title":"Pipelines With Docker Alternatives"},{"location":"jenkins-pipeline-examples/docker-alternatives/#potential-alternatives","text":"So, directly running docker containers via a docker engine inside your cluster is out. Let's look at some alternatives. Kubernetes Pod and External Node : the simple way out, use a cloud (such as EC2 Cloud) to provision a classic VM with docker engine as an agent and build there JIB : tool from Google, only works for Java applications and is supported directly from Gradle and Maven - Link Kaniko : tool from Google, works as 1-on-1 replacement for docker image build (except Windows Containers ) - Link IMG : tool from Jess Frazelle to avoid building docker images with a root user involved Link","title":"Potential Alternatives"},{"location":"jenkins-pipeline-examples/docker-alternatives/#kubernetes-pod-and-external-node","text":"One of the most used cloud environments is AWS, so I created this solution with AWS's Amazon EC2 Plugin . Warning Unfortunately, you cannot combine the Kubernetes plugin with external nodes (none pod container nodes) in a Declarative pipeline. So you have to use Scripted . This can be done with various different cloud providers such as Digital Ocean, Google, AWS or Azure. This guide will use AWS, as it has the most mature Jenkins Cloud plugin.","title":"Kubernetes Pod and External Node"},{"location":"jenkins-pipeline-examples/docker-alternatives/#prerequisites","text":"AWS Account with rights to create AMI's and run EC2 instances Packer Jenkins with Amazon EC2 Plugin installed","title":"Prerequisites"},{"location":"jenkins-pipeline-examples/docker-alternatives/#steps","text":"create AMI with Packer install and configure Amazon EC2 plugin create a test pipeline","title":"Steps"},{"location":"jenkins-pipeline-examples/docker-alternatives/#create-ami-with-packer","text":"Packer needs to be able to access EC2 API's and be able to spin up an EC2 instance and create an AMI out of it.","title":"Create AMI with Packer"},{"location":"jenkins-pipeline-examples/docker-alternatives/#aws-setup-for-packer","text":"You need to configure two things: account details for Packer to use security group where your EC2 instances will be running with this security group needs to open port 22 both Packer and Jenkins will use this for their connection 1 2 3 export AWS_DEFAULT_REGION = eu-west-1 export AWS_ACCESS_KEY_ID = XXX export AWS_SECRET_ACCESS_KEY = XXX 1 2 3 4 5 6 7 aws ec2 --profile myAwsProfile create-security-group \\ --description For building Docker images \\ --group-name docker { GroupId : sg-08079f78cXXXXXXX } Export the security group ID. 1 2 export SG_ID = sg-08079f78cXXXXXXX echo $SG_ID","title":"AWS setup for Packer"},{"location":"jenkins-pipeline-examples/docker-alternatives/#enable-port-22","text":"1 2 3 4 5 6 7 aws ec2 \\ --profile myAwsProfile \\ authorize-security-group-ingress \\ --group-name docker \\ --protocol tcp \\ --port 22 \\ --cidr 0 .0.0.0/0","title":"Enable port 22"},{"location":"jenkins-pipeline-examples/docker-alternatives/#packer-ami-definition","text":"Here's an example definition for Packer for a Ubuntu 18.04 LTS base image with JDK 8 (required by Jenkins) and Docker. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { builders : [{ type : amazon-ebs , region : eu-west-1 , source_ami_filter : { filters : { virtualization-type : hvm , name : *ubuntu-bionic-18.04-amd64-server-* , root-device-type : ebs }, owners : [ 679593333241 ], most_recent : true }, instance_type : t2.micro , ssh_username : ubuntu , ami_name : docker , force_deregister : true }], provisioners : [{ type : shell , inline : [ sleep 15 , sudo apt-get clean , sudo apt-get update , sudo apt-get install -y apt-transport-https ca-certificates nfs-common , curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - , sudo add-apt-repository \\ deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\ , sudo add-apt-repository -y ppa:openjdk-r/ppa , sudo apt-get update , sudo apt-get install -y docker-ce , sudo usermod -aG docker ubuntu , sudo apt-get install -y openjdk-8-jdk , java -version , docker version ] }] } Build the new AMI with packer. 1 2 packer build docker-ami.json export AMI = ami-0212ab37f84e418f4","title":"Packer AMI definition"},{"location":"jenkins-pipeline-examples/docker-alternatives/#ec2-key-pair","text":"Create EC2 key pair, this will be used by Jenkins to connect to the instances via ssh. 1 2 3 4 aws ec2 --profile myAwsProfile create-key-pair \\ --key-name jenkinsec2 \\ | jq -r .KeyMaterial \\ jenkins-ec2-proton.pem","title":"EC2 Key Pair"},{"location":"jenkins-pipeline-examples/docker-alternatives/#ec2-cloud-configuration","text":"In a Jenkins' master main configuration, you add a new cloud . In this case, we will use a ec2-cloud so we can instantiate our EC2 VM's with docker. use EC2 credentials for initial connection use key ( .pem ) for VM connection (jenkins agent) configure the following: AMI: ami-0212ab37f84e418f4 availability zone: eu-west-1a VPC SubnetID: subnet-aa54XXXX Remote user: ubuntu labels: docker ubuntu linux SecurityGroup Name: (the id) sg-08079f78cXXXXXXX public ip = true connect via public ip = true","title":"EC2 Cloud Configuration"},{"location":"jenkins-pipeline-examples/docker-alternatives/#pipeline","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @Library ( jenkins-pipeline-library@master ) _ def scmVars def label = jenkins-slave-${UUID.randomUUID().toString()} podTemplate ( label: label , yaml: apiVersion: v1 kind: Pod spec: containers: - name: kubectl image: vfarcic/kubectl command: [ cat ] tty: true ) { node ( label ) { node ( docker ) { stage ( SCM Prepare ) { scmVars = checkout scm } stage ( Lint ) { dockerfileLint () } stage ( Build Docker ) { sh docker image build -t demo:rc-1 . } stage ( Tag Push Docker ) { IMAGE = ${DOCKER_IMAGE_NAME} TAG = ${DOCKER_IMAGE_TAG} FULL_NAME = ${FULL_IMAGE_NAME} withCredentials ([ usernamePassword ( credentialsId: dockerhub , usernameVariable: USER , passwordVariable: PASS )]) { sh docker login -u $USER -p $PASS } sh docker image tag ${IMAGE}:${TAG} ${FULL_NAME} sh docker image push ${FULL_NAME} } } // end node docker stage ( Prepare Pod ) { // have to checkout on our kubernetes pod aswell checkout scm } stage ( Check version ) { container ( kubectl ) { sh kubectl version } } } // end node random label } // end pod def","title":"Pipeline"},{"location":"jenkins-pipeline-examples/docker-alternatives/#maven-jib","text":"If you use Java with either Gradle or Maven , you can use JIB to create docker image without requiring a docker client or docker engine. JIB will communicate with DockerHub and Google's container registry for sending image layers back and forth. The last layer will be created by the JIB plugin itself, adding your self-executing Jar to the layer and a Entrypoint with the correct flags. For more information about the runtime options, see either jib-maven-plugin documentation or jib-gradle-plugin .","title":"Maven JIB"},{"location":"jenkins-pipeline-examples/docker-alternatives/#prerequisites_1","text":"Java project build with Gradle or Maven Java project that can start it self, such as Spring Boot or Thorntail (previously Wildfly Swarm, from the JBoss family) able to build either gradle or maven applications","title":"Prerequisites"},{"location":"jenkins-pipeline-examples/docker-alternatives/#steps_1","text":"configure the plugin for either Gradle or Maven build using an official docker image via the kubernetes pod template","title":"Steps"},{"location":"jenkins-pipeline-examples/docker-alternatives/#pipeline_1","text":"Using a bit more elaborate pipeline example here. Using SonarCloud for static code analysis and then JIB to create a docker image. We could then use that image either directly in anything that runs docker or in the same cluster via a Helm Chart. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def scmVars def tag pipeline { options { buildDiscarder logRotator ( artifactDaysToKeepStr: 5 , artifactNumToKeepStr: 5 , daysToKeepStr: 5 , numToKeepStr: 5 ) } libraries { lib ( jenkins-pipeline-library@master ) } agent { kubernetes { label mypod defaultContainer jnlp yaml apiVersion: v1 kind: Pod metadata: labels: some-label: some-label-value spec: containers: - name: maven image: maven:3-jdk-11-slim command: - cat tty: true } } stages { stage ( Test versions ) { steps { container ( maven ) { sh uname -a sh mvn -version } } } stage ( Checkout ) { steps { script { scmVars = checkout scm } gitRemoteConfigByUrl ( scmVars . GIT_URL , githubtoken ) sh git config --global user.email jenkins@jenkins.io git config --global user.name Jenkins } } stage ( Build ) { steps { container ( maven ) { sh mvn clean verify -B -e } } } stage ( Version Analysis ) { parallel { stage ( Version Bump ) { when { branch master } environment { NEW_VERSION = gitNextSemverTagMaven ( pom.xml ) } steps { script { tag = ${NEW_VERSION} } container ( maven ) { sh mvn versions:set -DnewVersion=${NEW_VERSION} } gitTag ( v${NEW_VERSION} ) } } stage ( Sonar Analysis ) { when { branch master } environment { SONAR_HOST = https://sonarcloud.io KEY = spring-maven-demo ORG = demomon SONAR_TOKEN = credentials ( sonarcloud ) } steps { container ( maven ) { sh mvn sonar:sonar \\ -Dsonar.projectKey=${KEY} \\ -Dsonar.organization=${ORG} \\ -Dsonar.host.url=${SONAR_HOST} \\ -Dsonar.login=${SONAR_TOKEN} } } } } } stage ( Publish Artifact ) { when { branch master } environment { DHUB = credentials ( dockerhub ) } steps { container ( maven ) { // we should never come here if the tests have not run, as we run verify before sh mvn clean compile -B -e jib:build -Djib.to.auth.username=${DHUB_USR} -Djib.to.auth.password=${DHUB_PSW} -DskipTests } } } } post { always { cleanWs () } } }","title":"Pipeline"},{"location":"jenkins-pipeline-examples/docker-alternatives/#kaniko","text":"Google loves Kubernetes and Google prefers people building docker images in Kubernetes without Docker. As JIB is only available for Java projects, there's needs to be an alternative for any other usecase/programming language. The answer to that is Kaniko , a specialized Docker image to create Docker images. Kaniko isn't the most secure way to create docker images, it barely beats mounting a Docker Socket, or might even be worse if you ask others (such as Jess Frazelle ). That said, it is gaining some traction being used by JenkinsX and having an example in Jenkins' Kubernetes Plugin . Info When building more than one image inside the kaniko container, make sure to use the --cleanup flag. So it cleans its temporary cache data before building the next image, as discussed in this google group .","title":"Kaniko"},{"location":"jenkins-pipeline-examples/docker-alternatives/#prerequisites_2","text":"","title":"Prerequisites"},{"location":"jenkins-pipeline-examples/docker-alternatives/#steps_2","text":"Create docker registry secret Configure pod container template Configure stage","title":"Steps"},{"location":"jenkins-pipeline-examples/docker-alternatives/#create-docker-registry-secret","text":"This is an example for DockerHub inside the build namespace. 1 2 3 4 5 kubectl create secret docker-registry -n build regcred \\ --docker-server = index.docker.io \\ --docker-username = myDockerHubAccount \\ --docker-password = myDockerHubPassword \\ --docker-email = myDockerHub@Email.com","title":"Create docker registry secret"},{"location":"jenkins-pipeline-examples/docker-alternatives/#example-ppeline","text":"Warning Although multi-stage Dockerfile 's are supported, it did fail in my case. So I created a second Dockerfile which is only for running the application ( Dockerfile.run ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 pipeline { agent { kubernetes { //cloud kubernetes label kaniko yaml kind: Pod metadata: name: kaniko spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: kaniko image: gcr.io/kaniko-project/executor:debug imagePullPolicy: Always command: - /busybox/cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root - name: go-build-cache mountPath: /root/.cache/go-build - name: img-build-cache mountPath: /root/.local volumes: - name: go-build-cache emptyDir: {} - name: img-build-cache emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { environment { PATH = /busybox:$PATH } steps { container ( name: kaniko , shell: /busybox/sh ) { sh #!/busybox/sh /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=index.docker.io/caladreas/cat } } } } }","title":"Example Ppeline"},{"location":"jenkins-pipeline-examples/docker-alternatives/#img","text":"img is the brainchild of Jess Frazelle, a prominent figure in the container space. The goal is to be the safest and best way to build OCI compliant images, as outlined in her blog building container images securely on kubernetes .","title":"IMG"},{"location":"jenkins-pipeline-examples/docker-alternatives/#not-working-for-me-yet","text":"It does not seem to work for me on AWS's EKS. To many little details with relation to runc, file permissions and other configuration. For those who want to give it a spin, here are some resources to take a look at. https://blog.jessfraz.com/post/building-container-images-securely-on-kubernetes/ https://github.com/genuinetools/img https://github.com/opencontainers/runc https://git.j3ss.co/genuinetools/img/+/d05b3e4e10cd0e3c074ffb03dc22d7bb6cde1e78","title":"Not working (for me) yet"},{"location":"jenkins-pipeline-examples/docker-alternatives/#pipeline-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 pipeline { agent { kubernetes { label img yaml kind: Pod metadata: name: img annotations: container.apparmor.security.beta.kubernetes.io/img: unconfined spec: containers: - name: golang image: golang:1.11 command: - cat tty: true - name: img workingDir: /home/jenkins image: caladreas/img:0.5.1 imagePullPolicy: Always securityContext: rawProc: true privileged: true command: - cat tty: true volumeMounts: - name: jenkins-docker-cfg mountPath: /root volumes: - name: temp emptyDir: {} - name: jenkins-docker-cfg projected: sources: - secret: name: regcred items: - key: .dockerconfigjson path: .docker/config.json } } stages { stage ( Checkout ) { steps { git https://github.com/joostvdg/cat.git } } stage ( Build ) { steps { container ( golang ) { sh ./build-go-bin.sh } } } stage ( Make Image ) { steps { container ( img ) { sh mkdir cache sh img build -s ./cache -f Dockerfile.run -t caladreas/cat . } } } } }","title":"Pipeline Example"},{"location":"jenkins-pipeline-examples/docker-declarative/","text":"Docker Declarative Examples 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 pipeline { agent none options { timeout ( time: 10 , unit: MINUTES ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: 5 )) } stages { stage ( Prepare ){ agent { label docker } steps { parallel ( Clean: { deleteDir () }, NotifySlack: { slackSend channel: cicd , color: #FFFF00 , message: STARTED: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } ) } } stage ( Checkout ){ agent { label docker } steps { git credentialsId: 355df378-e726-4abd-90fa-e723c5c21ad5 , url: git@gitlab.flusso.nl:CICD/ci-cd-docs.git script { env . GIT_COMMIT_HASH = sh returnStdout: true , script: git rev-parse --verify HEAD } } } stage ( Build Docs ) { agent { docker { image caladreas/mkdocs-docker-build-container label docker } } steps { sh mkdocs build } } stage ( Prepare Docker Image ){ agent { label docker } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true , script: docker run --rm -i lukasmartinelli/hadolint Dockerfile if ( lintResult . trim () == ) { println Lint finished with no errors } else { println Error found in Lint println ${lintResult} currentBuild . result = UNSTABLE } } }, // end test dockerfile BuildImage: { sh chmod +x build.sh sh ./build.sh } ) } post { success { sh chmod +x push.sh sh ./push.sh } } } stage ( Update Docker Container ) { agent { label docker } steps { sh chmod +x container-update.sh sh ./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH} } } } post { success { slackSend channel: cicd , color: #00FF00 , message: SUCCESSFUL: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } failure { slackSend channel: cicd , color: #FF0000 , message: FAILED: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } } }","title":"Docker Declarative"},{"location":"jenkins-pipeline-examples/docker-declarative/#docker-declarative-examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 pipeline { agent none options { timeout ( time: 10 , unit: MINUTES ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: 5 )) } stages { stage ( Prepare ){ agent { label docker } steps { parallel ( Clean: { deleteDir () }, NotifySlack: { slackSend channel: cicd , color: #FFFF00 , message: STARTED: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } ) } } stage ( Checkout ){ agent { label docker } steps { git credentialsId: 355df378-e726-4abd-90fa-e723c5c21ad5 , url: git@gitlab.flusso.nl:CICD/ci-cd-docs.git script { env . GIT_COMMIT_HASH = sh returnStdout: true , script: git rev-parse --verify HEAD } } } stage ( Build Docs ) { agent { docker { image caladreas/mkdocs-docker-build-container label docker } } steps { sh mkdocs build } } stage ( Prepare Docker Image ){ agent { label docker } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true , script: docker run --rm -i lukasmartinelli/hadolint Dockerfile if ( lintResult . trim () == ) { println Lint finished with no errors } else { println Error found in Lint println ${lintResult} currentBuild . result = UNSTABLE } } }, // end test dockerfile BuildImage: { sh chmod +x build.sh sh ./build.sh } ) } post { success { sh chmod +x push.sh sh ./push.sh } } } stage ( Update Docker Container ) { agent { label docker } steps { sh chmod +x container-update.sh sh ./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH} } } } post { success { slackSend channel: cicd , color: #00FF00 , message: SUCCESSFUL: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } failure { slackSend channel: cicd , color: #FF0000 , message: FAILED: Job ${env.JOB_NAME} [${env.BUILD_NUMBER}] (${env.BUILD_URL}) } } }","title":"Docker Declarative Examples"},{"location":"jenkins-pipeline-examples/maven-declarative/","text":"Maven Declarative Examples Basics We have to wrap the entire script in pipeline { } , for it to be marked a declarative script. As we will be using different agents for different stages, we select none as the default. For house keeping, we add the options{} block, where we configure the following: timeout: make sure this jobs succeeds in 10 minutes, else just cancel it timestamps(): to make sure we have timestamps in our logs buildDiscarder(): this will make sure we will only keep the latest 5 builds 1 2 3 4 5 6 7 8 pipeline { agent none options { timeout ( time: 10 , unit: MINUTES ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: 5 )) } } Checkout There are several ways to checkout the code. Let's assume our code is somewhere in a git repository. Full Checkout command The main command for checking out is the Checkout command. It will look like this. 1 2 3 4 5 6 7 8 9 10 11 12 stage ( SCM ) { checkout ([ $class : GitSCM , branches: [[ name: */master ]], doGenerateSubmoduleConfigurations: false , extensions: [], submoduleCfg: [], userRemoteConfigs: [[ credentialsId: MyCredentialsId , url: https://github.com/joostvdg/keep-watching ]] ]) } Git shorthand Thats a lot of configuration for a simple checkout. So what if I'm just using the master branch of a publicly accessible repository (as is the case with GitHub)? 1 2 3 stage ( SCM ) { git https://github.com/joostvdg/keep-watching } Or with a different branch and credentials: 1 2 3 stage ( SCM ) { git credentialsId: MyCredentialsId , url: https://github.com/joostvdg/keep-watching } That's much better, but we can do even better. SCM shorthand If you're starting this pipeline job via a SCM, you've already configured the SCM. So assuming you've configured a pipeline job with 'Jenkinsfile from SCM' or an abstraction job - such as Multibranch-Pipeline, GitHub Organization or BitBucket Team/Project - you can do this. 1 2 3 stage ( SCM ) { checkout scm } The checkout scm line will use the checkout command we've used in the first example together with the object scm . This scm object, will contain the SCM configuration of the Job and will be reused for checking out. Warning A pipeline job from SCM or abstraction, will only checkout your Jenkinsfile. You will always need to checkout the rest of your code if you want to build it. For that, just use checkout scm Different Agent per Stage As you could see on the top, we've set agent to none. So for every stage we now need to tell it which agent to use - without it, the stage will fail. Agent any If you don't care what node it comes on, you specify any. 1 2 3 4 5 6 stage ( Checkout ) { agent any steps { git https://github.com/joostvdg/keep-watching } } Agent via Label If you want build on a node with a specific label - here docker - you do so with agent { label ' LABEL ' } . 1 2 3 4 5 6 stage ( Checkout ) { agent { label docker } steps { git https://github.com/joostvdg/keep-watching } } Docker Container as Agent Many developers are using docker for their CI/CD builds. So being able to use docker containers as build agents is a requirement these days. 1 2 3 4 5 6 7 8 9 10 11 12 stage ( Maven Build ) { agent { docker { image maven:3-alpine label docker args -v /home/joost/.m2:/root/.m2 } } steps { sh mvn -B clean package } } Cache Maven repo When you're using a docker build container, it will be clean every time. So if you want to avoid downloading the maven dependencies every build, you have to cache them. One way to do this, is to map a volume into the container so the container will use that folder instead. 1 2 3 4 5 6 7 8 9 10 11 12 stage ( Maven Build ) { agent { docker { image maven:3-alpine label docker args -v /home/joost/.m2:/root/.m2 } } steps { sh mvn -B clean package } } Post stage/build The declarative pipeline allows for Post actions, on both stage and complete build level. For both types there are different post hooks you can use, such as success, failure. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 stage ( Maven Build ) { agent { docker { image maven:3-alpine label docker args -v /home/joost/.m2:/root/.m2 } } steps { sh mvn -B clean package } post { success { junit target/surefire-reports/**/*.xml } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 stages { stage ( Example ) { steps { echo Hello World } } } post { always { echo This will always run } success { echo SUCCESS! } failure { echo We Failed } unstable { echo We re unstable } changed { echo Status Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result] } } Entire example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 pipeline { agent none options { timeout ( time: 10 , unit: MINUTES ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: 5 )) } stages { stage ( Example ) { steps { echo Hello World } } stage ( Checkout ) { agent { label docker } steps { git https://github.com/joostvdg/keep-watching } } stage ( Maven Build ) { agent { docker { image maven:3-alpine label docker args -v /home/joost/.m2:/root/.m2 } } steps { sh mvn -B clean package } post { success { junit target/surefire-reports/**/*.xml } } } stage ( Docker Build ) { agent { label docker } steps { sh docker build --tag=keep-watching-be . } } } post { always { echo This will always run } success { echo SUCCESS! } failure { echo We Failed } unstable { echo We re unstable } changed { echo Status Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result] } } }","title":"Maven Declarative"},{"location":"jenkins-pipeline-examples/maven-declarative/#maven-declarative-examples","text":"","title":"Maven Declarative Examples"},{"location":"jenkins-pipeline-examples/maven-declarative/#basics","text":"We have to wrap the entire script in pipeline { } , for it to be marked a declarative script. As we will be using different agents for different stages, we select none as the default. For house keeping, we add the options{} block, where we configure the following: timeout: make sure this jobs succeeds in 10 minutes, else just cancel it timestamps(): to make sure we have timestamps in our logs buildDiscarder(): this will make sure we will only keep the latest 5 builds 1 2 3 4 5 6 7 8 pipeline { agent none options { timeout ( time: 10 , unit: MINUTES ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: 5 )) } }","title":"Basics"},{"location":"jenkins-pipeline-examples/maven-declarative/#checkout","text":"There are several ways to checkout the code. Let's assume our code is somewhere in a git repository.","title":"Checkout"},{"location":"jenkins-pipeline-examples/maven-declarative/#full-checkout-command","text":"The main command for checking out is the Checkout command. It will look like this. 1 2 3 4 5 6 7 8 9 10 11 12 stage ( SCM ) { checkout ([ $class : GitSCM , branches: [[ name: */master ]], doGenerateSubmoduleConfigurations: false , extensions: [], submoduleCfg: [], userRemoteConfigs: [[ credentialsId: MyCredentialsId , url: https://github.com/joostvdg/keep-watching ]] ]) }","title":"Full Checkout command"},{"location":"jenkins-pipeline-examples/maven-declarative/#git-shorthand","text":"Thats a lot of configuration for a simple checkout. So what if I'm just using the master branch of a publicly accessible repository (as is the case with GitHub)? 1 2 3 stage ( SCM ) { git https://github.com/joostvdg/keep-watching } Or with a different branch and credentials: 1 2 3 stage ( SCM ) { git credentialsId: MyCredentialsId , url: https://github.com/joostvdg/keep-watching } That's much better, but we can do even better.","title":"Git shorthand"},{"location":"jenkins-pipeline-examples/maven-declarative/#scm-shorthand","text":"If you're starting this pipeline job via a SCM, you've already configured the SCM. So assuming you've configured a pipeline job with 'Jenkinsfile from SCM' or an abstraction job - such as Multibranch-Pipeline, GitHub Organization or BitBucket Team/Project - you can do this. 1 2 3 stage ( SCM ) { checkout scm } The checkout scm line will use the checkout command we've used in the first example together with the object scm . This scm object, will contain the SCM configuration of the Job and will be reused for checking out. Warning A pipeline job from SCM or abstraction, will only checkout your Jenkinsfile. You will always need to checkout the rest of your code if you want to build it. For that, just use checkout scm","title":"SCM shorthand"},{"location":"jenkins-pipeline-examples/maven-declarative/#different-agent-per-stage","text":"As you could see on the top, we've set agent to none. So for every stage we now need to tell it which agent to use - without it, the stage will fail.","title":"Different Agent per Stage"},{"location":"jenkins-pipeline-examples/maven-declarative/#agent-any","text":"If you don't care what node it comes on, you specify any. 1 2 3 4 5 6 stage ( Checkout ) { agent any steps { git https://github.com/joostvdg/keep-watching } }","title":"Agent any"},{"location":"jenkins-pipeline-examples/maven-declarative/#agent-via-label","text":"If you want build on a node with a specific label - here docker - you do so with agent { label ' LABEL ' } . 1 2 3 4 5 6 stage ( Checkout ) { agent { label docker } steps { git https://github.com/joostvdg/keep-watching } }","title":"Agent via Label"},{"location":"jenkins-pipeline-examples/maven-declarative/#docker-container-as-agent","text":"Many developers are using docker for their CI/CD builds. So being able to use docker containers as build agents is a requirement these days. 1 2 3 4 5 6 7 8 9 10 11 12 stage ( Maven Build ) { agent { docker { image maven:3-alpine label docker args -v /home/joost/.m2:/root/.m2 } } steps { sh mvn -B clean package } }","title":"Docker Container as Agent"},{"location":"jenkins-pipeline-examples/maven-declarative/#cache-maven-repo","text":"When you're using a docker build container, it will be clean every time. So if you want to avoid downloading the maven dependencies every build, you have to cache them. One way to do this, is to map a volume into the container so the container will use that folder instead. 1 2 3 4 5 6 7 8 9 10 11 12 stage ( Maven Build ) { agent { docker { image maven:3-alpine label docker args -v /home/joost/.m2:/root/.m2 } } steps { sh mvn -B clean package } }","title":"Cache Maven repo"},{"location":"jenkins-pipeline-examples/maven-declarative/#post-stagebuild","text":"The declarative pipeline allows for Post actions, on both stage and complete build level. For both types there are different post hooks you can use, such as success, failure. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 stage ( Maven Build ) { agent { docker { image maven:3-alpine label docker args -v /home/joost/.m2:/root/.m2 } } steps { sh mvn -B clean package } post { success { junit target/surefire-reports/**/*.xml } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 stages { stage ( Example ) { steps { echo Hello World } } } post { always { echo This will always run } success { echo SUCCESS! } failure { echo We Failed } unstable { echo We re unstable } changed { echo Status Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result] } }","title":"Post stage/build"},{"location":"jenkins-pipeline-examples/maven-declarative/#entire-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 pipeline { agent none options { timeout ( time: 10 , unit: MINUTES ) timestamps () buildDiscarder ( logRotator ( numToKeepStr: 5 )) } stages { stage ( Example ) { steps { echo Hello World } } stage ( Checkout ) { agent { label docker } steps { git https://github.com/joostvdg/keep-watching } } stage ( Maven Build ) { agent { docker { image maven:3-alpine label docker args -v /home/joost/.m2:/root/.m2 } } steps { sh mvn -B clean package } post { success { junit target/surefire-reports/**/*.xml } } } stage ( Docker Build ) { agent { label docker } steps { sh docker build --tag=keep-watching-be . } } } post { always { echo This will always run } success { echo SUCCESS! } failure { echo We Failed } unstable { echo We re unstable } changed { echo Status Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result] } } }","title":"Entire example"},{"location":"jenkins-pipeline-examples/maven-groovy-dsl/","text":"Maven Groovy DSL Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 node { timestamps { timeout ( time: 15 , unit: MINUTES ) { deleteDir () stage SCM //git branch: master , credentialsId: flusso-gitlab , url: https://gitlab.flusso.nl/keep/keep-backend-spring.git checkout scm env . JAVA_HOME = ${tool JDK 8 Latest } env . PATH = ${env.JAVA_HOME}/bin:${env.PATH} sh java -version try { def gradleHome = tool name: Gradle Latest , type: hudson.plugins.gradle.GradleInstallation stage Build sh ${gradleHome}/bin/gradle clean build javadoc step ([ $class : CheckStylePublisher , canComputeNew: false , defaultEncoding: , healthy: , pattern: build/reports/checkstyle/main.xml , unHealthy: ]) step ([ $class : JUnitResultArchiver , testResults: build/test-results/*.xml ]) step ([ $class : JavadocArchiver , javadocDir: build/docs/javadoc ]) stage SonarQube sh ${gradleHome}/bin/gradle sonarqube -Dsonar.host.url=http://sonarqube5-instance:9000 stash workspace } catch ( err ) { archive build/**/*.html echo Caught: ${err} currentBuild . result = FAILURE } } } } node ( docker ) { timestamps { timeout ( time: 15 , unit: MINUTES ) { deleteDir () unstash workspace stage Build Docker image sh ./build.sh def image = docker . image ( keep-backend-spring-img ) stage Push Docker image try { sh docker tag keep-backend-spring-img nexus.docker:18443/flusso/keep-backend-spring-img:latest sh docker push nexus.docker:18443/flusso/keep-backend-spring-img:latest } catch ( err ) { archive build/**/*.html echo Caught: ${err} currentBuild . result = FAILURE } } } }","title":"Maven Groovy DSL"},{"location":"jenkins-pipeline-examples/maven-groovy-dsl/#maven-groovy-dsl-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 node { timestamps { timeout ( time: 15 , unit: MINUTES ) { deleteDir () stage SCM //git branch: master , credentialsId: flusso-gitlab , url: https://gitlab.flusso.nl/keep/keep-backend-spring.git checkout scm env . JAVA_HOME = ${tool JDK 8 Latest } env . PATH = ${env.JAVA_HOME}/bin:${env.PATH} sh java -version try { def gradleHome = tool name: Gradle Latest , type: hudson.plugins.gradle.GradleInstallation stage Build sh ${gradleHome}/bin/gradle clean build javadoc step ([ $class : CheckStylePublisher , canComputeNew: false , defaultEncoding: , healthy: , pattern: build/reports/checkstyle/main.xml , unHealthy: ]) step ([ $class : JUnitResultArchiver , testResults: build/test-results/*.xml ]) step ([ $class : JavadocArchiver , javadocDir: build/docs/javadoc ]) stage SonarQube sh ${gradleHome}/bin/gradle sonarqube -Dsonar.host.url=http://sonarqube5-instance:9000 stash workspace } catch ( err ) { archive build/**/*.html echo Caught: ${err} currentBuild . result = FAILURE } } } } node ( docker ) { timestamps { timeout ( time: 15 , unit: MINUTES ) { deleteDir () unstash workspace stage Build Docker image sh ./build.sh def image = docker . image ( keep-backend-spring-img ) stage Push Docker image try { sh docker tag keep-backend-spring-img nexus.docker:18443/flusso/keep-backend-spring-img:latest sh docker push nexus.docker:18443/flusso/keep-backend-spring-img:latest } catch ( err ) { archive build/**/*.html echo Caught: ${err} currentBuild . result = FAILURE } } } }","title":"Maven Groovy DSL Example"},{"location":"jenkins-pipeline-examples/maven-shared-library/","text":"Maven Shared Library Example","title":"Maven Shared Library"},{"location":"jenkins-pipeline-examples/maven-shared-library/#maven-shared-library-example","text":"","title":"Maven Shared Library Example"},{"location":"jenkinsx/buildpack/","text":"Build Packs There are multiple ways to create your own buildpack for Jenkins X. Start from a working example : either create a quickstart project or import your existing application. Make the build and promotions work and then create a new buildpack by making the same changes (parameterized where applicable) to a copy of the buildpack you started from. Start from a working example We're going to build a buildpack for the following application: Micronaut framework build with Gradle with a Redis datastore with a TLS certificate for the ingress (https) Create Micronaut application create application via Micronaut CLI add a controller enable default healthendpoint import application with Jenkins X update helm chart: change healtcheck endpoint update helm chart: add dependency on Redis update values: set redis to not use a password 1 mn create-app example.micronaut.complete --features = kotlin,spek,tracing-jaeger,redis-lettuce 1 jx import Secrets 1 2 helm repo add soluto https://charts.soluto.io helm repo update 1 helm upgrade --install kamus soluto/kamus","title":"BuildPack"},{"location":"jenkinsx/buildpack/#build-packs","text":"There are multiple ways to create your own buildpack for Jenkins X. Start from a working example : either create a quickstart project or import your existing application. Make the build and promotions work and then create a new buildpack by making the same changes (parameterized where applicable) to a copy of the buildpack you started from.","title":"Build Packs"},{"location":"jenkinsx/buildpack/#start-from-a-working-example","text":"We're going to build a buildpack for the following application: Micronaut framework build with Gradle with a Redis datastore with a TLS certificate for the ingress (https)","title":"Start from a working example"},{"location":"jenkinsx/buildpack/#create-micronaut-application","text":"create application via Micronaut CLI add a controller enable default healthendpoint import application with Jenkins X update helm chart: change healtcheck endpoint update helm chart: add dependency on Redis update values: set redis to not use a password 1 mn create-app example.micronaut.complete --features = kotlin,spek,tracing-jaeger,redis-lettuce 1 jx import","title":"Create Micronaut application"},{"location":"jenkinsx/buildpack/#secrets","text":"1 2 helm repo add soluto https://charts.soluto.io helm repo update 1 helm upgrade --install kamus soluto/kamus","title":"Secrets"},{"location":"jenkinsx/custom-domain/","text":"Custom Domain At Creation Time After Creation Changing Domain","title":"Custom Domain"},{"location":"jenkinsx/custom-domain/#custom-domain","text":"","title":"Custom Domain"},{"location":"jenkinsx/custom-domain/#at-creation-time","text":"","title":"At Creation Time"},{"location":"jenkinsx/custom-domain/#after-creation","text":"","title":"After Creation"},{"location":"jenkinsx/custom-domain/#changing-domain","text":"","title":"Changing Domain"},{"location":"jenkinsx/hello-world/","text":"Hello World Demo Create GKE + Jenkins X cluster 1 2 3 4 5 6 7 8 9 10 11 12 export JX_CLUSTER_NAME = joostvdg export JX_ENV_PREFIX = joostvdg export JX_ADMIN_PSS = vXDzpiaVAthneXJR355J7PBT export JX_DOMAIN = jx.kearos.net export JX_GIT_USER = joostvdg export JX_API_TOKEN = 61edcbf6507d31b3f2fe811baa82aa6de33db001 export JX_ORG = demomon export JX_GCE_PROJECT = ps-dev-201405 export JX_K8S_REGION = europe-west4 export JX_K8S_ZONE = europe-west4-a export GKE_NODE_LOCATIONS = ${ REGION } -a, ${ REGION } -b export JX_K8S_VERSION = Get supported K8S versions Zonal 1 gcloud container get-server-config --zone ${ JX_K8S_ZONE } Regional 1 gcloud container get-server-config --region ${ JX_K8S_REGION } 1 export JX_K8S_VERSION = 1 .11.7-gke.4 Create regional cluster w/ Domain Currently only possible if you create a regional cluster first and then install jx. 1 2 3 4 5 6 7 8 9 10 gcloud container clusters create ${ JX_CLUSTER_NAME } \\ --region ${ JX_K8S_REGION } --node-locations ${ GKE_NODE_LOCATIONS } \\ --cluster-version ${ JX_K8S_VERSION } \\ --enable-pod-security-policy \\ --enable-network-policy \\ --num-nodes 2 --machine-type n1-standard-2 \\ --min-nodes 2 --max-nodes 3 \\ --enable-autoupgrade \\ --enable-autoscaling \\ --labels = owner = jvandergriendt,purpose = practice 1 jx install Create zonal cluster w/ Domain 1 2 3 4 5 6 7 8 9 10 11 12 jx create cluster gke \\ --cluster-name = ${ JX_CLUSTER_NAME } \\ --default-admin-password = ${ JX_ADMIN_PSS } \\ --domain = ${ JX_DOMAIN } \\ --kubernetes-version = ${ JX_K8S_VERSION } \\ --machine-type = n1-standard-2 \\ --max-num-nodes = 3 \\ --min-num-nodes = 2 \\ --project-id = ${ JX_GCE_PROJECT } \\ --zone = ${ JX_K8S_ZONE } \\ --kaniko = true \\ --skip-login Reinstall 1 2 3 4 5 6 7 jx install \\ --default-environment-prefix = $JX_ENV_PREFIX \\ --git-api-token = $JX_API_TOKEN \\ --git-username = $JX_GIT_USER \\ --environment-git-owner = $JX_GIT_USER \\ --default-admin-password = ${ JX_ADMIN_PSS } \\ --domain = ${ JX_DOMAIN } Configure Domain Once the cluster is up and the Jenkins X basics are installed, jx will prompt us about missing an ingress controller. 1 ? No existing ingress controller found in the kube-system namespace, shall we install one? Yes Reply yes, and in a little while, you will see the following message: 1 2 You can now configure your wildcard DNS jx.kearos.net to point to 35 .204.0.182 nginx ingress controller installed and configured We can now go to our Domain configuration and set *.jx.${DomainName} to the ip listed. If you're using Google Domains by any chance, you create an A class record for *.jx with ip 35.204.0.182 . Unfortunately, that's not enough, as the ingress resources created by jx after will have a different IP address. So we have to add a second IP address to your Class A record. Still assuming GKE, you can retrieve the second IP address as follows: 1 2 INGRESS_IP = $( kubectl get ing chartmuseum -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) echo $INGRESS_IP To test the domain, you can do the following: 1 CM_URL = $( kubectl get ing chartmuseum -o jsonpath = {.spec.rules[0].host} ) Curl 1 curl $CM_URL Httpie 1 http $CM_URL Configure TLS If we have a proper domain configured and working, we can also enable TLS. As we've not done so at the start, we will have to update the Ingress configuration. For all the options for updating the Ingress configuration, use the command below. 1 jx upgrade ingress --help To configure TLS for our ingresses, we need TLS certificates. Jenkins X does this via Let's Encrypt, which in Kubernetes is easily done via Certmanager . The command we will issue will ask us if we want to install CertManager , and then delete all existing ingress resources and recreate them with the certificate. Unfortunately, when in Batch mode ( -b ) it does not install CertManager nor is there an option to force it in this case. When asked if we want to delete and recreate the existing ingress rules, say yes ( y ). Select the expose type , which should be Ingress (route is for OpenShift). Confirm your domain - do not change it, as this upgrade does NOT change your domain configuration everywhere and you will end up with a broken system Say yes to cluster wide TLS If you're certain your Domain works, select the production LetEncrypt configuration, else choose staging for tests Confirm your email address and the summary Agree with installing CertManager Do Not agree with updating the webhooks (see below) 1 jx upgrade ingress --cluster --verbose Warning There's currently a bug with changing the webhooks via this command; see issue #3115 It somehow can only select a different GitHub user than the current one, which makes no sense for an UPDATE. So we must update the webhooks manually ourselves! Manually update webhooks Due to issue #3115 we need to manually update our webhooks for the environment repositories. If you're not sure where your environment repositories are, you can retrieve them with the command below: 1 js get env Open each environment repository, go to the settings tabs (top right), open the webhooks menu (on the left), and edit the webhook. Simply change the http:// to https:// and save. Warning If you've selected the staging configuration for Let's Encrypt, you have set the SSL configuration to Disable (not recommended) . Create options --buildpack='': The name of the build pack to use for the Team --vault --helm3=false: Use helm3 to install Jenkins X which does not use Tiller --kaniko=false --urltemplate='': For ingress; exposers can set the urltemplate to expose Addons 1 2 3 4 5 6 7 8 9 10 11 12 13 14 create addon ambassador Create an ambassador addon create addon anchore Create the Anchore addon for verifying container images create addon cloudbees Create the CloudBees app for Kubernetes ( a web console for working with CI/CD, Environments and GitOps ) create addon flagger Create the Flagger addon for Canary deployments create addon gitea Create a Gitea addon for hosting Git repositories create addon istio Create the Istio addon for service mesh create addon knative-build Create the knative build addon create addon kubeless Create a kubeless addon for hosting Git repositories create addon owasp-zap Create the OWASP Zed Attack Proxy addon for dynamic security checks against running apps create addon pipeline-events Create the pipeline events addon create addon prometheus Creates a prometheus addon create addon prow Create a Prow addon create addon sso Create a SSO addon for Single Sign-On create addon vault-operator Create an vault-operator addon for Hashicorp Vault Upgrade 1 jx upgrade --help 1 2 3 4 5 6 7 8 upgrade addons Upgrades any Addons added to Jenkins X if there are any new releases available upgrade apps Upgrades any Apps to the latest release upgrade binaries Upgrades the command line binaries ( like helm or eksctl ) - if there are new versions available upgrade cli Upgrades the command line applications - if there are new versions available upgrade cluster Upgrades the Kubernetes master to the specified version upgrade extensions Upgrades the Jenkins X extensions available to this Jenkins X install if there are new versions available upgrade ingress Upgrades Ingress rules upgrade platform Upgrades the Jenkins X platform if there is a new release available Go lang example 1 jx create quickstartjc select golang-http Promote 1 2 APP = jx-go-demo-5 VERSION = 0 .0.2 1 jx promote ${ APP } --version $VERSION --env production -b 1 jx get apps Spring Boot Example 1 jx create spring -d web -d actuator Serverless 1 2 3 4 5 6 jx create terraform gke \\ --vault = true \\ --cluster = ${ JX_CLUSTER_NAME } = gke \\ --gke-project-id = ${ JX_GCE_PROJECT } \\ --prow \\ --skip-login Demo - Show JX Stuff GitOps 1 2 3 4 5 6 Get environments: jx get environments Watch pipeline activity via: jx get activity -f golang-http -w Browse the pipeline log via: jx get build logs demomon/golang-http/master Open the Jenkins console via jx console You can list the pipelines via: jx get pipelines When the pipeline is complete: jx get applications Build It explain build packs jx create quickstart show Jenkinsfile open application jx get applications create new branch git checkout -b wip change main.go commit git -a -m better message push git push remote wip open PR page explain tide add cat picture: /meow test it add comment /test this open logs jx logs -k open PR environment approve the change /approve or add approved label open logs for next step promote: jx promote myapp --version 1.2.3 --env production promote: jx promote ${APP} --version ${VERSION} --env production","title":"HelloWorld"},{"location":"jenkinsx/hello-world/#hello-world-demo","text":"","title":"Hello World Demo"},{"location":"jenkinsx/hello-world/#create-gke-jenkins-x-cluster","text":"1 2 3 4 5 6 7 8 9 10 11 12 export JX_CLUSTER_NAME = joostvdg export JX_ENV_PREFIX = joostvdg export JX_ADMIN_PSS = vXDzpiaVAthneXJR355J7PBT export JX_DOMAIN = jx.kearos.net export JX_GIT_USER = joostvdg export JX_API_TOKEN = 61edcbf6507d31b3f2fe811baa82aa6de33db001 export JX_ORG = demomon export JX_GCE_PROJECT = ps-dev-201405 export JX_K8S_REGION = europe-west4 export JX_K8S_ZONE = europe-west4-a export GKE_NODE_LOCATIONS = ${ REGION } -a, ${ REGION } -b export JX_K8S_VERSION =","title":"Create GKE + Jenkins X cluster"},{"location":"jenkinsx/hello-world/#get-supported-k8s-versions","text":"Zonal 1 gcloud container get-server-config --zone ${ JX_K8S_ZONE } Regional 1 gcloud container get-server-config --region ${ JX_K8S_REGION } 1 export JX_K8S_VERSION = 1 .11.7-gke.4","title":"Get supported K8S versions"},{"location":"jenkinsx/hello-world/#create-regional-cluster-w-domain","text":"Currently only possible if you create a regional cluster first and then install jx. 1 2 3 4 5 6 7 8 9 10 gcloud container clusters create ${ JX_CLUSTER_NAME } \\ --region ${ JX_K8S_REGION } --node-locations ${ GKE_NODE_LOCATIONS } \\ --cluster-version ${ JX_K8S_VERSION } \\ --enable-pod-security-policy \\ --enable-network-policy \\ --num-nodes 2 --machine-type n1-standard-2 \\ --min-nodes 2 --max-nodes 3 \\ --enable-autoupgrade \\ --enable-autoscaling \\ --labels = owner = jvandergriendt,purpose = practice 1 jx install","title":"Create regional cluster w/ Domain"},{"location":"jenkinsx/hello-world/#create-zonal-cluster-w-domain","text":"1 2 3 4 5 6 7 8 9 10 11 12 jx create cluster gke \\ --cluster-name = ${ JX_CLUSTER_NAME } \\ --default-admin-password = ${ JX_ADMIN_PSS } \\ --domain = ${ JX_DOMAIN } \\ --kubernetes-version = ${ JX_K8S_VERSION } \\ --machine-type = n1-standard-2 \\ --max-num-nodes = 3 \\ --min-num-nodes = 2 \\ --project-id = ${ JX_GCE_PROJECT } \\ --zone = ${ JX_K8S_ZONE } \\ --kaniko = true \\ --skip-login","title":"Create zonal cluster w/ Domain"},{"location":"jenkinsx/hello-world/#reinstall","text":"1 2 3 4 5 6 7 jx install \\ --default-environment-prefix = $JX_ENV_PREFIX \\ --git-api-token = $JX_API_TOKEN \\ --git-username = $JX_GIT_USER \\ --environment-git-owner = $JX_GIT_USER \\ --default-admin-password = ${ JX_ADMIN_PSS } \\ --domain = ${ JX_DOMAIN }","title":"Reinstall"},{"location":"jenkinsx/hello-world/#configure-domain","text":"Once the cluster is up and the Jenkins X basics are installed, jx will prompt us about missing an ingress controller. 1 ? No existing ingress controller found in the kube-system namespace, shall we install one? Yes Reply yes, and in a little while, you will see the following message: 1 2 You can now configure your wildcard DNS jx.kearos.net to point to 35 .204.0.182 nginx ingress controller installed and configured We can now go to our Domain configuration and set *.jx.${DomainName} to the ip listed. If you're using Google Domains by any chance, you create an A class record for *.jx with ip 35.204.0.182 . Unfortunately, that's not enough, as the ingress resources created by jx after will have a different IP address. So we have to add a second IP address to your Class A record. Still assuming GKE, you can retrieve the second IP address as follows: 1 2 INGRESS_IP = $( kubectl get ing chartmuseum -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) echo $INGRESS_IP To test the domain, you can do the following: 1 CM_URL = $( kubectl get ing chartmuseum -o jsonpath = {.spec.rules[0].host} ) Curl 1 curl $CM_URL Httpie 1 http $CM_URL","title":"Configure Domain"},{"location":"jenkinsx/hello-world/#configure-tls","text":"If we have a proper domain configured and working, we can also enable TLS. As we've not done so at the start, we will have to update the Ingress configuration. For all the options for updating the Ingress configuration, use the command below. 1 jx upgrade ingress --help To configure TLS for our ingresses, we need TLS certificates. Jenkins X does this via Let's Encrypt, which in Kubernetes is easily done via Certmanager . The command we will issue will ask us if we want to install CertManager , and then delete all existing ingress resources and recreate them with the certificate. Unfortunately, when in Batch mode ( -b ) it does not install CertManager nor is there an option to force it in this case. When asked if we want to delete and recreate the existing ingress rules, say yes ( y ). Select the expose type , which should be Ingress (route is for OpenShift). Confirm your domain - do not change it, as this upgrade does NOT change your domain configuration everywhere and you will end up with a broken system Say yes to cluster wide TLS If you're certain your Domain works, select the production LetEncrypt configuration, else choose staging for tests Confirm your email address and the summary Agree with installing CertManager Do Not agree with updating the webhooks (see below) 1 jx upgrade ingress --cluster --verbose Warning There's currently a bug with changing the webhooks via this command; see issue #3115 It somehow can only select a different GitHub user than the current one, which makes no sense for an UPDATE. So we must update the webhooks manually ourselves!","title":"Configure TLS"},{"location":"jenkinsx/hello-world/#manually-update-webhooks","text":"Due to issue #3115 we need to manually update our webhooks for the environment repositories. If you're not sure where your environment repositories are, you can retrieve them with the command below: 1 js get env Open each environment repository, go to the settings tabs (top right), open the webhooks menu (on the left), and edit the webhook. Simply change the http:// to https:// and save. Warning If you've selected the staging configuration for Let's Encrypt, you have set the SSL configuration to Disable (not recommended) .","title":"Manually update webhooks"},{"location":"jenkinsx/hello-world/#create-options","text":"--buildpack='': The name of the build pack to use for the Team --vault --helm3=false: Use helm3 to install Jenkins X which does not use Tiller --kaniko=false --urltemplate='': For ingress; exposers can set the urltemplate to expose","title":"Create options"},{"location":"jenkinsx/hello-world/#addons","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 create addon ambassador Create an ambassador addon create addon anchore Create the Anchore addon for verifying container images create addon cloudbees Create the CloudBees app for Kubernetes ( a web console for working with CI/CD, Environments and GitOps ) create addon flagger Create the Flagger addon for Canary deployments create addon gitea Create a Gitea addon for hosting Git repositories create addon istio Create the Istio addon for service mesh create addon knative-build Create the knative build addon create addon kubeless Create a kubeless addon for hosting Git repositories create addon owasp-zap Create the OWASP Zed Attack Proxy addon for dynamic security checks against running apps create addon pipeline-events Create the pipeline events addon create addon prometheus Creates a prometheus addon create addon prow Create a Prow addon create addon sso Create a SSO addon for Single Sign-On create addon vault-operator Create an vault-operator addon for Hashicorp Vault","title":"Addons"},{"location":"jenkinsx/hello-world/#upgrade","text":"1 jx upgrade --help 1 2 3 4 5 6 7 8 upgrade addons Upgrades any Addons added to Jenkins X if there are any new releases available upgrade apps Upgrades any Apps to the latest release upgrade binaries Upgrades the command line binaries ( like helm or eksctl ) - if there are new versions available upgrade cli Upgrades the command line applications - if there are new versions available upgrade cluster Upgrades the Kubernetes master to the specified version upgrade extensions Upgrades the Jenkins X extensions available to this Jenkins X install if there are new versions available upgrade ingress Upgrades Ingress rules upgrade platform Upgrades the Jenkins X platform if there is a new release available","title":"Upgrade"},{"location":"jenkinsx/hello-world/#go-lang-example","text":"1 jx create quickstartjc select golang-http","title":"Go lang example"},{"location":"jenkinsx/hello-world/#promote","text":"1 2 APP = jx-go-demo-5 VERSION = 0 .0.2 1 jx promote ${ APP } --version $VERSION --env production -b 1 jx get apps","title":"Promote"},{"location":"jenkinsx/hello-world/#spring-boot-example","text":"1 jx create spring -d web -d actuator","title":"Spring Boot Example"},{"location":"jenkinsx/hello-world/#serverless","text":"1 2 3 4 5 6 jx create terraform gke \\ --vault = true \\ --cluster = ${ JX_CLUSTER_NAME } = gke \\ --gke-project-id = ${ JX_GCE_PROJECT } \\ --prow \\ --skip-login","title":"Serverless"},{"location":"jenkinsx/hello-world/#demo-show-jx-stuff","text":"","title":"Demo - Show JX Stuff"},{"location":"jenkinsx/hello-world/#gitops","text":"1 2 3 4 5 6 Get environments: jx get environments Watch pipeline activity via: jx get activity -f golang-http -w Browse the pipeline log via: jx get build logs demomon/golang-http/master Open the Jenkins console via jx console You can list the pipelines via: jx get pipelines When the pipeline is complete: jx get applications","title":"GitOps"},{"location":"jenkinsx/hello-world/#build-it","text":"explain build packs jx create quickstart show Jenkinsfile open application jx get applications create new branch git checkout -b wip change main.go commit git -a -m better message push git push remote wip open PR page explain tide add cat picture: /meow test it add comment /test this open logs jx logs -k open PR environment approve the change /approve or add approved label open logs for next step promote: jx promote myapp --version 1.2.3 --env production promote: jx promote ${APP} --version ${VERSION} --env production","title":"Build It"},{"location":"jenkinsx/hybrid/","text":"Jenkins X Hybrid TLS Jenkins X Hybrid TLS is a configuration of Jenkins X using both Static Jenkins and Jenkins X Serverless with Tekton within the same cluster. As the TLS suffix hints at, it also uses TLS for both installations to make sure all the services and your applications are accessible via https with a valid certificate. Pre-requisites GCP account with active subscription with an active project with which you are authenticated gcloud CLI Jenkins X CLI jx httpie or curl Steps create JX cluster in GKE with static Jenkins without Nexus create Go (lang) quickstart configure TLS install Serverless Jenkins X in the same cluster create Spring Boot Quickstart configure TLS for Serverless namespaces only re-install Jenkins X with Nexus Static Prepare Variables 1 2 3 4 5 CLUSTER_NAME = #name of your cluster PROJECT = #name of your GCP project REGION = #GCP region to install cluster in GITHUB_USER = #your GitHub Username GITHUB_TOKEN = #GitHub apitoken myvalues.yaml We're going to use a demo application based on Go, so we don't need Nexus. To configure Jenkins X to skip Nexus' installation, create the file myvalues.yaml with the following contents: 1 2 3 4 nexus : enabled : false docker-registry : enabled : true Install JX Make sure you execute this command where you have the myvalues.yaml file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 jx create cluster gke \\ --cluster-name ${ CLUSTER_NAME } \\ --project-id ${ PROJECT } \\ --region ${ REGION } \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --git-username ${ GITHUB_USER } \\ --git-provider-kind github \\ --git-api-token ${ GITHUB_TOKEN } \\ --batch-mode Go Quickstart 1 2 3 4 5 6 jx create quickstart \\ -l go --org ${ GITHUB_USER } \\ --project-name jx-static-go \\ --import-mode = Jenkinsfile \\ --deploy-kind default \\ -b Watch activity You can either go to Jenkins and watch the job there: jx console or watch in your console via jx get activity . 1 jx get activity -f jx-static-go -w Once the build completes, you should see something like the line below, you can test the application. 1 Promoted 28m5s 1m41s Succeeded Application is at: http://jx-static-go.jx-staging.34.90.105.15.nip.io Test application To confirm the application is running in the staging environment: 1 jx get applications Which should show something like this: 1 2 APPLICATION STAGING PODS URL jx-static-go 0 .0.1 1 /1 http://jx-static-go.jx-staging. ${ LIB_IP } .nip.io 1 LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) 1 http jx-static-go.jx-staging. ${ LB_IP } .nip.io Which should show the following: 1 2 3 4 5 6 7 8 HTTP/1.1 200 OK Connection: keep-alive Content-Length: 43 Content-Type: text/plain ; charset = utf-8 Date: Thu, 13 Jun 2019 12 :17:39 GMT Server: nginx/1.15.8 Hello from: Jenkins X golang http example Configure TLS Make sure you have two things: the address of your LoadBalancer (see below how to retrieve this) a Domain name with a quick and easy DNS configuration (incl. wildcard support) Retrieve LoadBalancer address 1 LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) Configure DNS Go to your Domain provider of choice, if you don't have one, consider Google Domains for 12 Euro per year. They might no be the cheapest, but the service is great and works quick - changes like we're about to do, take a few minutes to be effectuated. Configure the following wildcards to direct to your LoadBalancer's IP address: *.jx - type A *.jx-staging - type A *.jx-production - type A *.serverless - type A (for the serverless section) Upgrade Ingress To configure TLS inside Jenkins X, we make use of Let's Encrypt and cert-manager . To get Jenkins X to configure TLS, we use the jx upgrade ingress command. 1 DOMAIN = #your domain name 1 2 3 jx upgrade ingress \\ --cluster true \\ --domain $DOMAIN Info To be sure, the Domain name above should the base hostname only. Any resource within your JX installation will automatically get the following domain name: {name}.{namespace}.{DOMAIN} . For example, if your domain is example.com Jenkins will become jenkins.jx.example.com . Test applications Confirm your application now has a https protocol. 1 jx get applications 1 http https://jx-static-go.jx-staging. ${ DOMAIN } Serverless Prepare The values for INGRESS_NS and INGRESS_DEP are the default based on the static install created above. If your ingress controller namespace and/or deployment have different names, replace the values. For the LB_IP , we're also assuming default names and namespaces. 1 2 3 4 5 6 7 8 PROVIDER = gke LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) DOMAIN_SUFFIX = #your domain name DOMAIN = serverless. ${ DOMAIN_SUFFIX } INGRESS_NS = kube-system INGRESS_DEP = jxing-nginx-ingress-controller INSTALL_NS = cdx PROJECT = #your GCP project Info We're going to use the cdx namespace, this will create namespaces such as cdx and cdx-staging . In order to avoid having to register every environment in at our DNS provider, we will use an additional domain prefix serverless . Making the domain serverless.{DOMAIN} and the JX components {name}.cdx.serverless.{DOMAIN} . Install Serverless JX 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b Spring Boot Quickstart Create quickstart 1 2 3 4 jx create spring -d web -d actuator \\ --group com.example \\ --artifact jx-spring-boot-demo \\ -b 1 cd jx-spring-boot-demo Add controller Assuming you kept the group the same, you should find a folder src/main/java/com/example/jxspringbootdemo containing a file, DemoApplication.java . We're going to have to add two files to the same folder: Greeting.java GreetingController.java Greeting 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package com.example.jxspringbootdemo ; public class Greeting { private final long id ; private final String content ; public Greeting ( long id , String content ) { this . id = id ; this . content = content ; } public long getId () { return id ; } public String getContent () { return content ; } } GreetingController 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package com.example.jxspringbootdemo ; import java.util.concurrent.atomic.AtomicLong ; import org.springframework.web.bind.annotation.RequestMapping ; import org.springframework.web.bind.annotation.RequestParam ; import org.springframework.web.bind.annotation.RestController ; @RestController public class GreetingController { private static final String template = Hello, %s! ; private final AtomicLong counter = new AtomicLong (); @RequestMapping ( /greeting ) public Greeting greeting ( @RequestParam ( value = name , defaultValue = World ) String name ) { return new Greeting ( counter . incrementAndGet (), String . format ( template , name )); } } Test application 1 jx get activity -f jx-cdx-spring-boot-demo-1 -w Re-Install with Nexus myvalues.yaml Our application didn't work because now we have an application that depends on a Maven repository. We have to \"re-install\" Jenkins X, to have it install Nexus for us in the cdx namespace. 1 2 3 4 nexus : enabled : true docker-registry : enabled : true Install Make sure you execute this command where you have the myvalues.yaml file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain serverless. $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b Test Application To trigger a new build, make a change - for example to the README.md and push it. 1 jx get activity -f jx-cdx-spring-boot-demo-1 -w 1 http jx-cdx-spring-boot-demo-1.cdx-staging.serverless. ${ DOMAIN } /greeting Configure TLS 1 jx upgrade ingress --domain $DOMAIN --namespaces cdx,cdx-staging Re-test application 1 ORG = #the GitHub user or organisation your application is in 1 2 3 4 jx update webhooks --repo = jx-cdx-spring-boot-demo-1 --org = ${ ORG } jx get applications jx get activity -f jx-cdx-spring-boot-demo-1 -w http https://jx-cdx-spring-boot-demo-1.cdx-staging.serverless. ${ DOMAIN } /greeting","title":"Hybrid"},{"location":"jenkinsx/hybrid/#jenkins-x-hybrid-tls","text":"Jenkins X Hybrid TLS is a configuration of Jenkins X using both Static Jenkins and Jenkins X Serverless with Tekton within the same cluster. As the TLS suffix hints at, it also uses TLS for both installations to make sure all the services and your applications are accessible via https with a valid certificate.","title":"Jenkins X Hybrid TLS"},{"location":"jenkinsx/hybrid/#pre-requisites","text":"GCP account with active subscription with an active project with which you are authenticated gcloud CLI Jenkins X CLI jx httpie or curl","title":"Pre-requisites"},{"location":"jenkinsx/hybrid/#steps","text":"create JX cluster in GKE with static Jenkins without Nexus create Go (lang) quickstart configure TLS install Serverless Jenkins X in the same cluster create Spring Boot Quickstart configure TLS for Serverless namespaces only re-install Jenkins X with Nexus","title":"Steps"},{"location":"jenkinsx/hybrid/#static","text":"","title":"Static"},{"location":"jenkinsx/hybrid/#prepare","text":"","title":"Prepare"},{"location":"jenkinsx/hybrid/#variables","text":"1 2 3 4 5 CLUSTER_NAME = #name of your cluster PROJECT = #name of your GCP project REGION = #GCP region to install cluster in GITHUB_USER = #your GitHub Username GITHUB_TOKEN = #GitHub apitoken","title":"Variables"},{"location":"jenkinsx/hybrid/#myvaluesyaml","text":"We're going to use a demo application based on Go, so we don't need Nexus. To configure Jenkins X to skip Nexus' installation, create the file myvalues.yaml with the following contents: 1 2 3 4 nexus : enabled : false docker-registry : enabled : true","title":"myvalues.yaml"},{"location":"jenkinsx/hybrid/#install-jx","text":"Make sure you execute this command where you have the myvalues.yaml file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 jx create cluster gke \\ --cluster-name ${ CLUSTER_NAME } \\ --project-id ${ PROJECT } \\ --region ${ REGION } \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --git-username ${ GITHUB_USER } \\ --git-provider-kind github \\ --git-api-token ${ GITHUB_TOKEN } \\ --batch-mode","title":"Install JX"},{"location":"jenkinsx/hybrid/#go-quickstart","text":"1 2 3 4 5 6 jx create quickstart \\ -l go --org ${ GITHUB_USER } \\ --project-name jx-static-go \\ --import-mode = Jenkinsfile \\ --deploy-kind default \\ -b","title":"Go Quickstart"},{"location":"jenkinsx/hybrid/#watch-activity","text":"You can either go to Jenkins and watch the job there: jx console or watch in your console via jx get activity . 1 jx get activity -f jx-static-go -w Once the build completes, you should see something like the line below, you can test the application. 1 Promoted 28m5s 1m41s Succeeded Application is at: http://jx-static-go.jx-staging.34.90.105.15.nip.io","title":"Watch activity"},{"location":"jenkinsx/hybrid/#test-application","text":"To confirm the application is running in the staging environment: 1 jx get applications Which should show something like this: 1 2 APPLICATION STAGING PODS URL jx-static-go 0 .0.1 1 /1 http://jx-static-go.jx-staging. ${ LIB_IP } .nip.io 1 LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) 1 http jx-static-go.jx-staging. ${ LB_IP } .nip.io Which should show the following: 1 2 3 4 5 6 7 8 HTTP/1.1 200 OK Connection: keep-alive Content-Length: 43 Content-Type: text/plain ; charset = utf-8 Date: Thu, 13 Jun 2019 12 :17:39 GMT Server: nginx/1.15.8 Hello from: Jenkins X golang http example","title":"Test application"},{"location":"jenkinsx/hybrid/#configure-tls","text":"Make sure you have two things: the address of your LoadBalancer (see below how to retrieve this) a Domain name with a quick and easy DNS configuration (incl. wildcard support)","title":"Configure TLS"},{"location":"jenkinsx/hybrid/#retrieve-loadbalancer-address","text":"1 LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = {.status.loadBalancer.ingress[0].ip} )","title":"Retrieve LoadBalancer address"},{"location":"jenkinsx/hybrid/#configure-dns","text":"Go to your Domain provider of choice, if you don't have one, consider Google Domains for 12 Euro per year. They might no be the cheapest, but the service is great and works quick - changes like we're about to do, take a few minutes to be effectuated. Configure the following wildcards to direct to your LoadBalancer's IP address: *.jx - type A *.jx-staging - type A *.jx-production - type A *.serverless - type A (for the serverless section)","title":"Configure DNS"},{"location":"jenkinsx/hybrid/#upgrade-ingress","text":"To configure TLS inside Jenkins X, we make use of Let's Encrypt and cert-manager . To get Jenkins X to configure TLS, we use the jx upgrade ingress command. 1 DOMAIN = #your domain name 1 2 3 jx upgrade ingress \\ --cluster true \\ --domain $DOMAIN Info To be sure, the Domain name above should the base hostname only. Any resource within your JX installation will automatically get the following domain name: {name}.{namespace}.{DOMAIN} . For example, if your domain is example.com Jenkins will become jenkins.jx.example.com .","title":"Upgrade Ingress"},{"location":"jenkinsx/hybrid/#test-applications","text":"Confirm your application now has a https protocol. 1 jx get applications 1 http https://jx-static-go.jx-staging. ${ DOMAIN }","title":"Test applications"},{"location":"jenkinsx/hybrid/#serverless","text":"","title":"Serverless"},{"location":"jenkinsx/hybrid/#prepare_1","text":"The values for INGRESS_NS and INGRESS_DEP are the default based on the static install created above. If your ingress controller namespace and/or deployment have different names, replace the values. For the LB_IP , we're also assuming default names and namespaces. 1 2 3 4 5 6 7 8 PROVIDER = gke LB_IP = $( kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) DOMAIN_SUFFIX = #your domain name DOMAIN = serverless. ${ DOMAIN_SUFFIX } INGRESS_NS = kube-system INGRESS_DEP = jxing-nginx-ingress-controller INSTALL_NS = cdx PROJECT = #your GCP project Info We're going to use the cdx namespace, this will create namespaces such as cdx and cdx-staging . In order to avoid having to register every environment in at our DNS provider, we will use an additional domain prefix serverless . Making the domain serverless.{DOMAIN} and the JX components {name}.cdx.serverless.{DOMAIN} .","title":"Prepare"},{"location":"jenkinsx/hybrid/#install-serverless-jx","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b","title":"Install Serverless JX"},{"location":"jenkinsx/hybrid/#spring-boot-quickstart","text":"","title":"Spring Boot Quickstart"},{"location":"jenkinsx/hybrid/#create-quickstart","text":"1 2 3 4 jx create spring -d web -d actuator \\ --group com.example \\ --artifact jx-spring-boot-demo \\ -b 1 cd jx-spring-boot-demo","title":"Create quickstart"},{"location":"jenkinsx/hybrid/#add-controller","text":"Assuming you kept the group the same, you should find a folder src/main/java/com/example/jxspringbootdemo containing a file, DemoApplication.java . We're going to have to add two files to the same folder: Greeting.java GreetingController.java","title":"Add controller"},{"location":"jenkinsx/hybrid/#greeting","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package com.example.jxspringbootdemo ; public class Greeting { private final long id ; private final String content ; public Greeting ( long id , String content ) { this . id = id ; this . content = content ; } public long getId () { return id ; } public String getContent () { return content ; } }","title":"Greeting"},{"location":"jenkinsx/hybrid/#greetingcontroller","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package com.example.jxspringbootdemo ; import java.util.concurrent.atomic.AtomicLong ; import org.springframework.web.bind.annotation.RequestMapping ; import org.springframework.web.bind.annotation.RequestParam ; import org.springframework.web.bind.annotation.RestController ; @RestController public class GreetingController { private static final String template = Hello, %s! ; private final AtomicLong counter = new AtomicLong (); @RequestMapping ( /greeting ) public Greeting greeting ( @RequestParam ( value = name , defaultValue = World ) String name ) { return new Greeting ( counter . incrementAndGet (), String . format ( template , name )); } }","title":"GreetingController"},{"location":"jenkinsx/hybrid/#test-application_1","text":"1 jx get activity -f jx-cdx-spring-boot-demo-1 -w","title":"Test application"},{"location":"jenkinsx/hybrid/#re-install-with-nexus","text":"","title":"Re-Install with Nexus"},{"location":"jenkinsx/hybrid/#myvaluesyaml_1","text":"Our application didn't work because now we have an application that depends on a Maven repository. We have to \"re-install\" Jenkins X, to have it install Nexus for us in the cdx namespace. 1 2 3 4 nexus : enabled : true docker-registry : enabled : true","title":"myvalues.yaml"},{"location":"jenkinsx/hybrid/#install","text":"Make sure you execute this command where you have the myvalues.yaml file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain serverless. $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b","title":"Install"},{"location":"jenkinsx/hybrid/#test-application_2","text":"To trigger a new build, make a change - for example to the README.md and push it. 1 jx get activity -f jx-cdx-spring-boot-demo-1 -w 1 http jx-cdx-spring-boot-demo-1.cdx-staging.serverless. ${ DOMAIN } /greeting","title":"Test Application"},{"location":"jenkinsx/hybrid/#configure-tls_1","text":"1 jx upgrade ingress --domain $DOMAIN --namespaces cdx,cdx-staging","title":"Configure TLS"},{"location":"jenkinsx/hybrid/#re-test-application","text":"1 ORG = #the GitHub user or organisation your application is in 1 2 3 4 jx update webhooks --repo = jx-cdx-spring-boot-demo-1 --org = ${ ORG } jx get applications jx get activity -f jx-cdx-spring-boot-demo-1 -w http https://jx-cdx-spring-boot-demo-1.cdx-staging.serverless. ${ DOMAIN } /greeting","title":"Re-test application"},{"location":"jenkinsx/intro/","text":"Introduction to Jenkins X","title":"Intro"},{"location":"jenkinsx/intro/#introduction-to-jenkins-x","text":"","title":"Introduction to Jenkins X"},{"location":"jenkinsx/maven/","text":"Jenkins X + Maven + Nexus The goal of this article it to demonstrate how Jenkins X works with Maven and Sonatype Nexus . Unless you configure otherwise, Jenkins X comes with a Nexus instance pre-configure out-of-the-box. Create Jenkins X Cluster Static See: Example Here's an example for creating a standard Jenkins X installation in Google Cloud with GKE. This example uses Google Cloud and it's CLI, gcloud . Where: CLUSTER_NAME : the name of your GKE cluster PROJECT : the project ID of your Google Project/account ( gcloud config list ) REGION : the region in Google Cloud where you want to run this cluster, if you don't know, use us-east1 1 2 3 4 5 6 7 8 9 10 11 12 jx create cluster gke \\ --cluster-name ${ CLUSTER_NAME } \\ --project-id ${ PROJECT } \\ --region ${ REGION } \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --skip-login \\ --batch-mode Serverless See: Nexus Open To open Nexus' Web UI, you can use jx open to see it's URL. By default, the URL will be nexus.jx. domain , for example http://nexus.jx.${LB_IP}.nip.io . I recommend using a proper Domain name and use TLS via Let's Encrypt. Jenkins X has built in support for this, via the jx upgrade ingress command. This is food for another article though. Credentials The username will be admin , the password depends on you. If you specified the --default-admin-password , it will be that. If you didn't specify the password, you can find it in a Kubernetes secret. 1 kubectl get secret nexus -o yaml Which should look like this: 1 2 3 4 apiVersion : v1 data : password : YWRtaW4= kind : Secret To retrieve the password, we have to decode the value of password with Base64. On Mac or Linux, this should be as easy as the command below. 1 echo YWRtaW4= | base64 -D Use Log in as Administrator and you get two views. Either browse, which allows you to discover and inspect packages. Or, Administrate (the Gear Icon) which allows you to manage the repositories. For more information, read the Nexus 3 documentation . Use Nexus with Maven in Jenkins X Maven Library Steps create new Jenkins X buildpack create new maven application import application into Jenkins X (with the Build Pack) double check job in Jenkins double check webhook in GitHub build the application in Jenkins verify package in Nexus Maven Application Steps create new maven application add repository for local dev add dependency on library build locally import application into Jenkins X build application in Jenkins How the magic works build image let's dig to see whats in it kubernetes secret with settings.xml maven repo maven distribution management maven mirror How would you do this yourself Options adjust Jenkins X's solution bridge Jenkins X's solution to your existing repo's create something yourself Adjust Jenkins X solution ? Bridge to existing Only to external Library buildpack https://github.com/jenkins-x-buildpacks/jenkins-x-classic/blob/master/packs/maven/pipeline.yaml Create new application 1 mvn archetype:generate -DarchetypeGroupId = org.apache.maven.archetypes -DarchetypeArtifactId = maven-archetype-quickstart -DarchetypeVersion = 1 .4 Import JX 1 jx import --pack maven-lib -b Do we need it? Seems to work without it as well. Perhaps its inside the build image? Edit secret secret: jenkins-maven-settings add labels: jenkins.io/credentials-type: secretFile https://jenkinsci.github.io/kubernetes-credentials-provider-plugin/examples/ https://jenkinsci.github.io/kubernetes-credentials-provider-plugin/ Pom xml config In order to publish stuff, we need to make sure we have our distribution config setup. 1 2 3 4 5 6 7 8 9 10 11 12 profiles profile id jx-nexus /id distributionManagement repository id nexus /id name nexus /name url ${altReleaseDeploymentRepository} /url /repository /distributionManagement /profile /profiles Pipeline example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 pipeline { agent { label jenkins-maven-java11 } stages { stage ( Test ) { environment { SETTINGS = credentials ( another-test-file2 ) } steps { sh echo ${SETTINGS} sh cat ${SETTINGS} container ( maven ) { sh mvn clean javadoc:aggregate verify -C -e sh mvn deploy --show-version --errors --activate-profiles jx-nexus --strict-checksums --settings ${SETTINGS} } } } } } Config example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : Secret metadata : # this is the jenkins id. name : another-test-file2 labels : # so we know what type it is. jenkins.io/credentials-type : secretFile annotations : # description - can not be a label as spaces are not allowed jenkins.io/credentials-description : secret file credential from Kubernetes type : Opaque stringData : filename : mySecret.txt data : # base64 encoded bytes data : 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 Create App create new java application with maven or gradle add dependency add repo: https://nexus.jx.kearos.net/repository/maven-public/ Know Issues Jenkins X doesn't have a kubernetes buildpack for Maven libraries, so I'm not sure how to import that directly which is why, for now, we create a new build pack first Jenkins X cannot import more than one application into static Jenkins within the same folder requires GitHub issue + PR","title":"Maven"},{"location":"jenkinsx/maven/#jenkins-x-maven-nexus","text":"The goal of this article it to demonstrate how Jenkins X works with Maven and Sonatype Nexus . Unless you configure otherwise, Jenkins X comes with a Nexus instance pre-configure out-of-the-box.","title":"Jenkins X + Maven + Nexus"},{"location":"jenkinsx/maven/#create-jenkins-x-cluster","text":"","title":"Create Jenkins X Cluster"},{"location":"jenkinsx/maven/#static","text":"See:","title":"Static"},{"location":"jenkinsx/maven/#example","text":"Here's an example for creating a standard Jenkins X installation in Google Cloud with GKE. This example uses Google Cloud and it's CLI, gcloud . Where: CLUSTER_NAME : the name of your GKE cluster PROJECT : the project ID of your Google Project/account ( gcloud config list ) REGION : the region in Google Cloud where you want to run this cluster, if you don't know, use us-east1 1 2 3 4 5 6 7 8 9 10 11 12 jx create cluster gke \\ --cluster-name ${ CLUSTER_NAME } \\ --project-id ${ PROJECT } \\ --region ${ REGION } \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --skip-login \\ --batch-mode","title":"Example"},{"location":"jenkinsx/maven/#serverless","text":"See:","title":"Serverless"},{"location":"jenkinsx/maven/#nexus","text":"","title":"Nexus"},{"location":"jenkinsx/maven/#open","text":"To open Nexus' Web UI, you can use jx open to see it's URL. By default, the URL will be nexus.jx. domain , for example http://nexus.jx.${LB_IP}.nip.io . I recommend using a proper Domain name and use TLS via Let's Encrypt. Jenkins X has built in support for this, via the jx upgrade ingress command. This is food for another article though.","title":"Open"},{"location":"jenkinsx/maven/#credentials","text":"The username will be admin , the password depends on you. If you specified the --default-admin-password , it will be that. If you didn't specify the password, you can find it in a Kubernetes secret. 1 kubectl get secret nexus -o yaml Which should look like this: 1 2 3 4 apiVersion : v1 data : password : YWRtaW4= kind : Secret To retrieve the password, we have to decode the value of password with Base64. On Mac or Linux, this should be as easy as the command below. 1 echo YWRtaW4= | base64 -D","title":"Credentials"},{"location":"jenkinsx/maven/#use","text":"Log in as Administrator and you get two views. Either browse, which allows you to discover and inspect packages. Or, Administrate (the Gear Icon) which allows you to manage the repositories. For more information, read the Nexus 3 documentation .","title":"Use"},{"location":"jenkinsx/maven/#use-nexus-with-maven-in-jenkins-x","text":"","title":"Use Nexus with Maven in Jenkins X"},{"location":"jenkinsx/maven/#maven-library","text":"","title":"Maven Library"},{"location":"jenkinsx/maven/#steps","text":"create new Jenkins X buildpack create new maven application import application into Jenkins X (with the Build Pack) double check job in Jenkins double check webhook in GitHub build the application in Jenkins verify package in Nexus","title":"Steps"},{"location":"jenkinsx/maven/#maven-application","text":"","title":"Maven Application"},{"location":"jenkinsx/maven/#steps_1","text":"create new maven application add repository for local dev add dependency on library build locally import application into Jenkins X build application in Jenkins","title":"Steps"},{"location":"jenkinsx/maven/#how-the-magic-works","text":"build image let's dig to see whats in it kubernetes secret with settings.xml maven repo maven distribution management maven mirror","title":"How the magic works"},{"location":"jenkinsx/maven/#how-would-you-do-this-yourself","text":"","title":"How would you do this yourself"},{"location":"jenkinsx/maven/#options","text":"adjust Jenkins X's solution bridge Jenkins X's solution to your existing repo's create something yourself","title":"Options"},{"location":"jenkinsx/maven/#adjust-jenkins-x-solution","text":"?","title":"Adjust Jenkins X solution"},{"location":"jenkinsx/maven/#bridge-to-existing","text":"","title":"Bridge to existing"},{"location":"jenkinsx/maven/#only-to-external","text":"","title":"Only to external"},{"location":"jenkinsx/maven/#library","text":"buildpack https://github.com/jenkins-x-buildpacks/jenkins-x-classic/blob/master/packs/maven/pipeline.yaml","title":"Library"},{"location":"jenkinsx/maven/#create-new-application","text":"1 mvn archetype:generate -DarchetypeGroupId = org.apache.maven.archetypes -DarchetypeArtifactId = maven-archetype-quickstart -DarchetypeVersion = 1 .4","title":"Create new application"},{"location":"jenkinsx/maven/#import-jx","text":"1 jx import --pack maven-lib -b","title":"Import JX"},{"location":"jenkinsx/maven/#do-we-need-it","text":"Seems to work without it as well. Perhaps its inside the build image?","title":"Do we need it?"},{"location":"jenkinsx/maven/#edit-secret","text":"secret: jenkins-maven-settings add labels: jenkins.io/credentials-type: secretFile https://jenkinsci.github.io/kubernetes-credentials-provider-plugin/examples/ https://jenkinsci.github.io/kubernetes-credentials-provider-plugin/","title":"Edit secret"},{"location":"jenkinsx/maven/#pom-xml-config","text":"In order to publish stuff, we need to make sure we have our distribution config setup. 1 2 3 4 5 6 7 8 9 10 11 12 profiles profile id jx-nexus /id distributionManagement repository id nexus /id name nexus /name url ${altReleaseDeploymentRepository} /url /repository /distributionManagement /profile /profiles","title":"Pom xml config"},{"location":"jenkinsx/maven/#pipeline-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 pipeline { agent { label jenkins-maven-java11 } stages { stage ( Test ) { environment { SETTINGS = credentials ( another-test-file2 ) } steps { sh echo ${SETTINGS} sh cat ${SETTINGS} container ( maven ) { sh mvn clean javadoc:aggregate verify -C -e sh mvn deploy --show-version --errors --activate-profiles jx-nexus --strict-checksums --settings ${SETTINGS} } } } } }","title":"Pipeline example"},{"location":"jenkinsx/maven/#config-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : Secret metadata : # this is the jenkins id. name : another-test-file2 labels : # so we know what type it is. jenkins.io/credentials-type : secretFile annotations : # description - can not be a label as spaces are not allowed jenkins.io/credentials-description : secret file credential from Kubernetes type : Opaque stringData : filename : mySecret.txt data : # base64 encoded bytes data : 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","title":"Config example"},{"location":"jenkinsx/maven/#create-app","text":"create new java application with maven or gradle add dependency add repo: https://nexus.jx.kearos.net/repository/maven-public/","title":"Create App"},{"location":"jenkinsx/maven/#know-issues","text":"Jenkins X doesn't have a kubernetes buildpack for Maven libraries, so I'm not sure how to import that directly which is why, for now, we create a new build pack first Jenkins X cannot import more than one application into static Jenkins within the same folder requires GitHub issue + PR","title":"Know Issues"},{"location":"jenkinsx/serverless/","text":"Jenkins X Serverless What Tekton Jenkins X Serverless Jenkins X Pipelines Commands Create Cluster 1 2 3 4 5 6 7 8 9 10 11 jx create cluster gke \\ --cluster-name jx-rocks \\ --project-id $PROJECT \\ --region us-east1 \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --batch-mode Install JX Where Project, is Gcloud Project ID. Requires docker-registry gcr.io , else it doesn't work. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b Notes jx create cluster gke cannot use kubernetes server version flag it somehow sets an empty --machineType flag instead jx install --tekton --prow requires --dockerRegistry to be set jx install --tekton --prow can be install multiple times in the same cluster to differentiate, set different namespace (which becomes part of the domain) might need to update webhooks incase env's already existed two jx serverless installs, and now jx get build logs doesn't work error: no Tekton pipelines have been triggered which match the current filter","title":"Jenkins X Serverless"},{"location":"jenkinsx/serverless/#jenkins-x-serverless","text":"","title":"Jenkins X Serverless"},{"location":"jenkinsx/serverless/#what","text":"Tekton Jenkins X Serverless Jenkins X Pipelines","title":"What"},{"location":"jenkinsx/serverless/#commands","text":"","title":"Commands"},{"location":"jenkinsx/serverless/#create-cluster","text":"1 2 3 4 5 6 7 8 9 10 11 jx create cluster gke \\ --cluster-name jx-rocks \\ --project-id $PROJECT \\ --region us-east1 \\ --machine-type n1-standard-2 \\ --min-num-nodes 1 \\ --max-num-nodes 2 \\ --default-admin-password = admin \\ --default-environment-prefix jx-rocks \\ --git-provider-kind github \\ --batch-mode","title":"Create Cluster"},{"location":"jenkinsx/serverless/#install-jx","text":"Where Project, is Gcloud Project ID. Requires docker-registry gcr.io , else it doesn't work. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jx install \\ --provider $PROVIDER \\ --external-ip $LB_IP \\ --domain $DOMAIN \\ --default-admin-password = admin \\ --ingress-namespace $INGRESS_NS \\ --ingress-deployment $INGRESS_DEP \\ --default-environment-prefix tekton \\ --git-provider-kind github \\ --namespace ${ INSTALL_NS } \\ --prow \\ --docker-registry gcr.io \\ --docker-registry-org $PROJECT \\ --tekton \\ --kaniko \\ -b","title":"Install JX"},{"location":"jenkinsx/serverless/#notes","text":"jx create cluster gke cannot use kubernetes server version flag it somehow sets an empty --machineType flag instead jx install --tekton --prow requires --dockerRegistry to be set jx install --tekton --prow can be install multiple times in the same cluster to differentiate, set different namespace (which becomes part of the domain) might need to update webhooks incase env's already existed two jx serverless installs, and now jx get build logs doesn't work error: no Tekton pipelines have been triggered which match the current filter","title":"Notes"},{"location":"jenkinsx/workshop/","text":"Jenkins X Workshop Create Cluster 1 2 3 4 5 6 7 8 PROJECT = NAME = ws-feb ZONE = europe-west4 MACHINE = n1-standard-2 MIN_NODES = 3 MAX_NODES = 5 PASS = admin PREFIX = ws Warning You might want to do this: (not sure why) 1 2 3 echo nexus: enabled: false | tee myvalues.yaml 1 2 3 4 jx create cluster gke -n $NAME -p $PROJECT -z $ZONE -m $MACHINE \\ --min-num-nodes $MIN_NODES --max-num-nodes $MAX_NODES \\ --default-admin-password = $PASS \\ --default-environment-prefix $NAME Alternatively Info Domain will get .jx as a prefix anyway. 1 2 3 4 5 6 JX_CLUSTER_NAME = joostvdg JX_DOMAIN = kearos.net JX_GIT_USER = joostvdg JX_ORG = joostvdg JX_K8S_REGION = europe-west4 JX_NAME = jx-joostvdg 1 2 3 JX_API_TOKEN = JX_ADMIN_PSS = JX_GCE_PROJECT = 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jx create cluster gke \\ -n ${ JX_NAME } \\ --exposer = Ingress \\ --preemptible = false \\ --cluster-name = ${ JX_CLUSTER_NAME } \\ --default-admin-password = ${ JX_ADMIN_PSS } \\ --domain = ${ JX_DOMAIN } \\ --machine-type = n1-standard-2 \\ --max-num-nodes = 3 \\ --min-num-nodes = 2 \\ --project-id = ${ JX_GCE_PROJECT } \\ --default-environment-prefix ${ JX_NAME } \\ --zone = ${ JX_ZONE } \\ --http = false \\ --tls-acme = true \\ --skip-login 1 2 3 --prow \\ --no-tiller = true \\ --vault = true \\ Issues -b doesn't work with --vault as the config is empty --vault='true' doesn't work with https (cert-manager) because sync.go:64] Not syncing ingress jx/cm-acme-http-solver-stx47 as it does not contain necessary annotations ** also, it seems its TLS config isn't correct for some reason does TLS with cert-manager actually work? now it doesn't actually install cert-manager ? Whats up with that. Install Certmanager Via JX Updates the entire ingress configuration, installs cert-mananger, certificates, replaces ingress definitions, updates webhooks, and allows you to set a different domain name. 1 jx upgrade ingress --cluster Manually This does not create certificates nor does it update the ingress defintions. 1 2 3 kubectl create namespace cert-manager kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.6/deploy/manifests/00-crds.yaml helm install --name cert-manager --namespace cert-manager stable/cert-manager Demo App Post creation 1 2 3 4 5 6 7 8 9 Creating GitHub webhook for joostvdg/cmg for url https://hook.jx.jx.kearos.net/hook Watch pipeline activity via: jx get activity -f cmg -w Browse the pipeline log via: jx get build logs joostvdg/cmg/master Open the Jenkins console via jx console You can list the pipelines via: jx get pipelines When the pipeline is complete: jx get applications For more help on available commands see: https://jenkins-x.io/developing/browsing/ Promote 1 jx promote ${ APP } --version $VERSION --env production -b Compliance 1 jx compliance run 1 jx compliance status 1 jx compliance logs -f 1 jx compliance delete Chartmuseum auth faillure uses kubernetes secret relies on kubernetes-secret (Jenkins) plugin can have trouble with special charactes to fix, update the kubernetes secret (used by chartmuseum and the pipeline) Workshop responses send to Alyssa Juni address for sending Responses Extra requirements billing needs to be enabled, else you cannot create a cluster of that size we need to test more with windows ** without admin ** different python versions","title":"Workshop"},{"location":"jenkinsx/workshop/#jenkins-x-workshop","text":"","title":"Jenkins X Workshop"},{"location":"jenkinsx/workshop/#create-cluster","text":"1 2 3 4 5 6 7 8 PROJECT = NAME = ws-feb ZONE = europe-west4 MACHINE = n1-standard-2 MIN_NODES = 3 MAX_NODES = 5 PASS = admin PREFIX = ws Warning You might want to do this: (not sure why) 1 2 3 echo nexus: enabled: false | tee myvalues.yaml 1 2 3 4 jx create cluster gke -n $NAME -p $PROJECT -z $ZONE -m $MACHINE \\ --min-num-nodes $MIN_NODES --max-num-nodes $MAX_NODES \\ --default-admin-password = $PASS \\ --default-environment-prefix $NAME","title":"Create Cluster"},{"location":"jenkinsx/workshop/#alternatively","text":"Info Domain will get .jx as a prefix anyway. 1 2 3 4 5 6 JX_CLUSTER_NAME = joostvdg JX_DOMAIN = kearos.net JX_GIT_USER = joostvdg JX_ORG = joostvdg JX_K8S_REGION = europe-west4 JX_NAME = jx-joostvdg 1 2 3 JX_API_TOKEN = JX_ADMIN_PSS = JX_GCE_PROJECT = 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jx create cluster gke \\ -n ${ JX_NAME } \\ --exposer = Ingress \\ --preemptible = false \\ --cluster-name = ${ JX_CLUSTER_NAME } \\ --default-admin-password = ${ JX_ADMIN_PSS } \\ --domain = ${ JX_DOMAIN } \\ --machine-type = n1-standard-2 \\ --max-num-nodes = 3 \\ --min-num-nodes = 2 \\ --project-id = ${ JX_GCE_PROJECT } \\ --default-environment-prefix ${ JX_NAME } \\ --zone = ${ JX_ZONE } \\ --http = false \\ --tls-acme = true \\ --skip-login 1 2 3 --prow \\ --no-tiller = true \\ --vault = true \\","title":"Alternatively"},{"location":"jenkinsx/workshop/#issues","text":"-b doesn't work with --vault as the config is empty --vault='true' doesn't work with https (cert-manager) because sync.go:64] Not syncing ingress jx/cm-acme-http-solver-stx47 as it does not contain necessary annotations ** also, it seems its TLS config isn't correct for some reason does TLS with cert-manager actually work? now it doesn't actually install cert-manager ? Whats up with that.","title":"Issues"},{"location":"jenkinsx/workshop/#install-certmanager","text":"","title":"Install Certmanager"},{"location":"jenkinsx/workshop/#via-jx","text":"Updates the entire ingress configuration, installs cert-mananger, certificates, replaces ingress definitions, updates webhooks, and allows you to set a different domain name. 1 jx upgrade ingress --cluster","title":"Via JX"},{"location":"jenkinsx/workshop/#manually","text":"This does not create certificates nor does it update the ingress defintions. 1 2 3 kubectl create namespace cert-manager kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.6/deploy/manifests/00-crds.yaml helm install --name cert-manager --namespace cert-manager stable/cert-manager","title":"Manually"},{"location":"jenkinsx/workshop/#demo-app","text":"","title":"Demo App"},{"location":"jenkinsx/workshop/#post-creation","text":"1 2 3 4 5 6 7 8 9 Creating GitHub webhook for joostvdg/cmg for url https://hook.jx.jx.kearos.net/hook Watch pipeline activity via: jx get activity -f cmg -w Browse the pipeline log via: jx get build logs joostvdg/cmg/master Open the Jenkins console via jx console You can list the pipelines via: jx get pipelines When the pipeline is complete: jx get applications For more help on available commands see: https://jenkins-x.io/developing/browsing/","title":"Post creation"},{"location":"jenkinsx/workshop/#promote","text":"1 jx promote ${ APP } --version $VERSION --env production -b","title":"Promote"},{"location":"jenkinsx/workshop/#compliance","text":"1 jx compliance run 1 jx compliance status 1 jx compliance logs -f 1 jx compliance delete","title":"Compliance"},{"location":"jenkinsx/workshop/#chartmuseum-auth-faillure","text":"uses kubernetes secret relies on kubernetes-secret (Jenkins) plugin can have trouble with special charactes to fix, update the kubernetes secret (used by chartmuseum and the pipeline)","title":"Chartmuseum auth faillure"},{"location":"jenkinsx/workshop/#workshop-responses","text":"send to Alyssa Juni address for sending","title":"Workshop responses"},{"location":"jenkinsx/workshop/#responses","text":"","title":"Responses"},{"location":"jenkinsx/workshop/#extra-requirements","text":"billing needs to be enabled, else you cannot create a cluster of that size we need to test more with windows ** without admin ** different python versions","title":"Extra requirements"},{"location":"kubernetes/","text":"Kubernetes Pods https://slides.com/dariotranchitella/containerday-2018#/ Networking https://medium.com/@vikram.fugro/container-networking-interface-aka-cni-bdfe23f865cf https://itnext.io/benchmark-results-of-kubernetes-network-plugins-cni-over-10gbit-s-network-36475925a560?_hsenc=p2ANqtz-950CZwR5-IRC28TSUVlXJzpygqd0KwJrt-dWKfirea5oYdsPArrudyx2SAVCiVvz7W_n0izo_IkpD-fk5BTbnB-a38lg _hsmi=68015399","title":"Introduction"},{"location":"kubernetes/#kubernetes","text":"","title":"Kubernetes"},{"location":"kubernetes/#pods","text":"https://slides.com/dariotranchitella/containerday-2018#/","title":"Pods"},{"location":"kubernetes/#networking","text":"https://medium.com/@vikram.fugro/container-networking-interface-aka-cni-bdfe23f865cf https://itnext.io/benchmark-results-of-kubernetes-network-plugins-cni-over-10gbit-s-network-36475925a560?_hsenc=p2ANqtz-950CZwR5-IRC28TSUVlXJzpygqd0KwJrt-dWKfirea5oYdsPArrudyx2SAVCiVvz7W_n0izo_IkpD-fk5BTbnB-a38lg _hsmi=68015399","title":"Networking"},{"location":"kubernetes/cka-exam-prep/","text":"CKA Exam Prep https://github.com/kelseyhightower/kubernetes-the-hard-way https://github.com/walidshaari/Kubernetes-Certified-Administrator https://github.com/kubernetes/community/blob/master/contributors/devel/e2e-tests.md https://www.cncf.io/certification/cka/ https://oscon2018.container.training https://github.com/ahmetb/kubernetes-network-policy-recipes https://github.com/ramitsurana/awesome-kubernetes https://sysdig.com/blog/kubernetes-security-guide/ https://severalnines.com/blog/installing-kubernetes-cluster-minions-centos7-manage-pods-services https://docs.google.com/presentation/d/1Gp-2blk5WExI_QR59EUZdwfO2BWLJqa626mK2ej-huo/edit#slide=id.g27a78b354c_0_0 https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html Some basic commands 1 kubectl -n kube-public get secrets Test network policy For some common recipes, look at Ahmet's recipe repository . Warning Make sure you have CNI enabled and you have a network plugin that enforces the policies. Note You can check current existing policies like this: kubectl get netpol --all-namespaces Example Ingress Policy 1 2 3 4 5 6 7 8 9 10 11 kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : name : dui-network-policy namespace : dui spec : podSelector : matchLabels : app : dui distribution : server ingress : [] Run test pod Apply above network policy, and then test in the same dui namespace, and in the default namespace. Note Use alpine:3.6 because telnet was dropped starting 3.7. 1 2 3 kubectl -n dui get pods -l app = dui -o wide kubectl run --rm -i -t --image = alpine:3.6 -n dui test -- sh telnet 10 .32.0.7 8888 This should now fail - timeout - due the packages being dropped. Egress 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : dui-network-policy-egress namespace : dui spec : podSelector : matchLabels : app : dui policyTypes : - Egress egress : - ports : - port : 7777 protocol : TCP - to : - podSelector : matchLabels : app : dui Warning This should in theory, block our test pod from reading this. As it doesn't have the label app=dui . But it seems it is working just fine. Allow DNS If it should also be able to do DNS calls, we have to enable port 53. 1 2 3 4 5 6 7 8 9 - ports : - port : 53 protocol : UDP - port : 53 protocol : TCP - port : 7777 protocol : TCP - to : - namespaceSelector : {} Create a test pod with curl 1 2 3 kubectl run --rm -i -t --image = alpine:3.6 -n dui test -- sh apk --no-cache add curl curl 10 .32.0.11:7777/servers Run minikube cluster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 ###################### # Create The Cluster # ###################### # Make sure that your minikube version is v0.25 or higher # WARNING!!! # Some users experienced problems starting the cluster with minikuber v0.26 and v0.27. # A few of the reported issues are https://github.com/kubernetes/minikube/issues/2707 and https://github.com/kubernetes/minikube/issues/2703 # If you are experiencing problems creating a cluster, please consider downgrading to minikube v0.25. minikube start \\ --vm-driver virtualbox \\ --cpus 4 \\ --memory 12228 \\ --network-plugin = cni \\ --extra-config = kubelet.network-plugin = cni ############################### # Install Ingress and Storage # ############################### minikube addons enable ingress minikube addons enable storage-provisioner minikube addons enable default-storageclass ################## # Install Tiller # ################## kubectl create \\ -f https://raw.githubusercontent.com/vfarcic/k8s-specs/master/helm/tiller-rbac.yml \\ --record --save-config helm init --service-account tiller kubectl -n kube-system \\ rollout status deploy tiller-deploy ################## # Get Cluster IP # ################## export LB_IP = $( minikube ip ) ####################### # Install ChartMuseum # ####################### CM_ADDR = cm. $LB_IP .nip.io echo $CM_ADDR CM_ADDR_ESC = $( echo $CM_ADDR \\ | sed -e s@\\.@\\\\\\.@g ) echo $CM_ADDR_ESC helm install stable/chartmuseum \\ --namespace charts \\ --name cm \\ --values helm/chartmuseum-values.yml \\ --set ingress.hosts. $CM_ADDR_ESC ={ / } \\ --set env.secret.BASIC_AUTH_USER = admin \\ --set env.secret.BASIC_AUTH_PASS = admin kubectl -n charts \\ rollout status deploy \\ cm-chartmuseum # http http://$CM_ADDR/health # It should return `{ healthy :true} ###################### # Install Weave Net ## ###################### kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n ) kubectl -n kube-system rollout status daemonset weave-net Weave Net On minikube To run Weave Net on minikube, after upgrading minikube, you need to overwrite the default CNI config shipped with minikube: mkdir -p ~/.minikube/files/etc/cni/net.d/ touch ~/.minikube/files/etc/cni.net.d/k8s.conf and then to start minikube with CNI enabled: minikube start --network-plugin=cni --extra-config=kubelet.network-plugin=cni. Afterwards, you can install Weave Net. 1 kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n ) Install stern Stern - aggregate log rendering tool via brew 1 brew install stern Binary release 1 2 3 sudo curl -L -o /usr/local/bin/stern \\ https://github.com/wercker/stern/releases/download/1.6.0/stern_linux_amd64 sudo chmod +x /usr/local/bin/stern Sysdig Install Sysdig Run Sysdig for Kubernetes collect API server address collect client cert + key https://www.digitalocean.com/community/tutorials/how-to-monitor-your-ubuntu-16-04-system-with-sysdig 1 2 3 4 certificate-authority: /home/joostvdg/.minikube/ca.crt server: https://192.168.99.100:8443 client-certificate: /home/joostvdg/.minikube/client.crt client-key: /home/joostvdg/.minikube/client.key 1 2 3 sysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key sysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key syslog.severity.str = info CSysdig 1 sudo csysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key From Udemy Course","title":"CKA Exam prep"},{"location":"kubernetes/cka-exam-prep/#cka-exam-prep","text":"https://github.com/kelseyhightower/kubernetes-the-hard-way https://github.com/walidshaari/Kubernetes-Certified-Administrator https://github.com/kubernetes/community/blob/master/contributors/devel/e2e-tests.md https://www.cncf.io/certification/cka/ https://oscon2018.container.training https://github.com/ahmetb/kubernetes-network-policy-recipes https://github.com/ramitsurana/awesome-kubernetes https://sysdig.com/blog/kubernetes-security-guide/ https://severalnines.com/blog/installing-kubernetes-cluster-minions-centos7-manage-pods-services https://docs.google.com/presentation/d/1Gp-2blk5WExI_QR59EUZdwfO2BWLJqa626mK2ej-huo/edit#slide=id.g27a78b354c_0_0 https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html","title":"CKA Exam Prep"},{"location":"kubernetes/cka-exam-prep/#some-basic-commands","text":"1 kubectl -n kube-public get secrets","title":"Some basic commands"},{"location":"kubernetes/cka-exam-prep/#test-network-policy","text":"For some common recipes, look at Ahmet's recipe repository . Warning Make sure you have CNI enabled and you have a network plugin that enforces the policies. Note You can check current existing policies like this: kubectl get netpol --all-namespaces","title":"Test network policy"},{"location":"kubernetes/cka-exam-prep/#example-ingress-policy","text":"1 2 3 4 5 6 7 8 9 10 11 kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : name : dui-network-policy namespace : dui spec : podSelector : matchLabels : app : dui distribution : server ingress : []","title":"Example Ingress Policy"},{"location":"kubernetes/cka-exam-prep/#run-test-pod","text":"Apply above network policy, and then test in the same dui namespace, and in the default namespace. Note Use alpine:3.6 because telnet was dropped starting 3.7. 1 2 3 kubectl -n dui get pods -l app = dui -o wide kubectl run --rm -i -t --image = alpine:3.6 -n dui test -- sh telnet 10 .32.0.7 8888 This should now fail - timeout - due the packages being dropped.","title":"Run test pod"},{"location":"kubernetes/cka-exam-prep/#egress","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : dui-network-policy-egress namespace : dui spec : podSelector : matchLabels : app : dui policyTypes : - Egress egress : - ports : - port : 7777 protocol : TCP - to : - podSelector : matchLabels : app : dui Warning This should in theory, block our test pod from reading this. As it doesn't have the label app=dui . But it seems it is working just fine.","title":"Egress"},{"location":"kubernetes/cka-exam-prep/#allow-dns","text":"If it should also be able to do DNS calls, we have to enable port 53. 1 2 3 4 5 6 7 8 9 - ports : - port : 53 protocol : UDP - port : 53 protocol : TCP - port : 7777 protocol : TCP - to : - namespaceSelector : {}","title":"Allow DNS"},{"location":"kubernetes/cka-exam-prep/#create-a-test-pod-with-curl","text":"1 2 3 kubectl run --rm -i -t --image = alpine:3.6 -n dui test -- sh apk --no-cache add curl curl 10 .32.0.11:7777/servers","title":"Create a test pod with curl"},{"location":"kubernetes/cka-exam-prep/#run-minikube-cluster","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 ###################### # Create The Cluster # ###################### # Make sure that your minikube version is v0.25 or higher # WARNING!!! # Some users experienced problems starting the cluster with minikuber v0.26 and v0.27. # A few of the reported issues are https://github.com/kubernetes/minikube/issues/2707 and https://github.com/kubernetes/minikube/issues/2703 # If you are experiencing problems creating a cluster, please consider downgrading to minikube v0.25. minikube start \\ --vm-driver virtualbox \\ --cpus 4 \\ --memory 12228 \\ --network-plugin = cni \\ --extra-config = kubelet.network-plugin = cni ############################### # Install Ingress and Storage # ############################### minikube addons enable ingress minikube addons enable storage-provisioner minikube addons enable default-storageclass ################## # Install Tiller # ################## kubectl create \\ -f https://raw.githubusercontent.com/vfarcic/k8s-specs/master/helm/tiller-rbac.yml \\ --record --save-config helm init --service-account tiller kubectl -n kube-system \\ rollout status deploy tiller-deploy ################## # Get Cluster IP # ################## export LB_IP = $( minikube ip ) ####################### # Install ChartMuseum # ####################### CM_ADDR = cm. $LB_IP .nip.io echo $CM_ADDR CM_ADDR_ESC = $( echo $CM_ADDR \\ | sed -e s@\\.@\\\\\\.@g ) echo $CM_ADDR_ESC helm install stable/chartmuseum \\ --namespace charts \\ --name cm \\ --values helm/chartmuseum-values.yml \\ --set ingress.hosts. $CM_ADDR_ESC ={ / } \\ --set env.secret.BASIC_AUTH_USER = admin \\ --set env.secret.BASIC_AUTH_PASS = admin kubectl -n charts \\ rollout status deploy \\ cm-chartmuseum # http http://$CM_ADDR/health # It should return `{ healthy :true} ###################### # Install Weave Net ## ###################### kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n ) kubectl -n kube-system rollout status daemonset weave-net","title":"Run minikube cluster"},{"location":"kubernetes/cka-exam-prep/#weave-net","text":"","title":"Weave Net"},{"location":"kubernetes/cka-exam-prep/#on-minikube","text":"To run Weave Net on minikube, after upgrading minikube, you need to overwrite the default CNI config shipped with minikube: mkdir -p ~/.minikube/files/etc/cni/net.d/ touch ~/.minikube/files/etc/cni.net.d/k8s.conf and then to start minikube with CNI enabled: minikube start --network-plugin=cni --extra-config=kubelet.network-plugin=cni. Afterwards, you can install Weave Net. 1 kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n )","title":"On minikube"},{"location":"kubernetes/cka-exam-prep/#install-stern","text":"Stern - aggregate log rendering tool","title":"Install stern"},{"location":"kubernetes/cka-exam-prep/#via-brew","text":"1 brew install stern","title":"via brew"},{"location":"kubernetes/cka-exam-prep/#binary-release","text":"1 2 3 sudo curl -L -o /usr/local/bin/stern \\ https://github.com/wercker/stern/releases/download/1.6.0/stern_linux_amd64 sudo chmod +x /usr/local/bin/stern","title":"Binary release"},{"location":"kubernetes/cka-exam-prep/#sysdig","text":"","title":"Sysdig"},{"location":"kubernetes/cka-exam-prep/#install-sysdig","text":"","title":"Install Sysdig"},{"location":"kubernetes/cka-exam-prep/#run-sysdig-for-kubernetes","text":"collect API server address collect client cert + key https://www.digitalocean.com/community/tutorials/how-to-monitor-your-ubuntu-16-04-system-with-sysdig 1 2 3 4 certificate-authority: /home/joostvdg/.minikube/ca.crt server: https://192.168.99.100:8443 client-certificate: /home/joostvdg/.minikube/client.crt client-key: /home/joostvdg/.minikube/client.key 1 2 3 sysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key sysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key syslog.severity.str = info","title":"Run Sysdig for Kubernetes"},{"location":"kubernetes/cka-exam-prep/#csysdig","text":"1 sudo csysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key","title":"CSysdig"},{"location":"kubernetes/cka-exam-prep/#from-udemy-course","text":"","title":"From Udemy Course"},{"location":"kubernetes/cka-exam/","text":"Certified Kubernetes Administrator Exam","title":"CKA Exam details"},{"location":"kubernetes/cka-exam/#certified-kubernetes-administrator-exam","text":"","title":"Certified Kubernetes Administrator Exam"},{"location":"kubernetes/dev-platform/","text":"Kubernetes As Developer Platform Resources https://medium.com/@jpcontad/a-year-of-running-kubernetes-as-a-product-7eed1204eecd","title":"Kubernetes As Developer Platform"},{"location":"kubernetes/dev-platform/#kubernetes-as-developer-platform","text":"","title":"Kubernetes As Developer Platform"},{"location":"kubernetes/dev-platform/#resources","text":"https://medium.com/@jpcontad/a-year-of-running-kubernetes-as-a-product-7eed1204eecd","title":"Resources"},{"location":"kubernetes/observability/","text":"Kubernetes Observability Monitoring Metrics Server Helm chart: https://github.com/helm/charts/tree/master/stable/metrics-server Home: https://github.com/kubernetes-incubator/metrics-server 1 2 3 4 5 6 7 8 helm install stable/metrics-server \\ --name metrics-server \\ --version 2 .0.3 \\ --namespace metrics kubectl -n metrics \\ rollout status \\ deployment metrics-server Prometheus Alert Manager Prometheus Helm Values 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 server: ingress: enabled: true annotations: ingress.kubernetes.io/ssl-redirect: false nginx.ingress.kubernetes.io/ssl-redirect: false resources: limits: cpu: 100m memory: 1000Mi requests: cpu: 10m memory: 500Mi alertmanager: ingress: enabled: true annotations: ingress.kubernetes.io/ssl-redirect: false nginx.ingress.kubernetes.io/ssl-redirect: false resources: limits: cpu: 10m memory: 20Mi requests: cpu: 5m memory: 10Mi kubeStateMetrics: resources: limits: cpu: 10m memory: 50Mi requests: cpu: 5m memory: 25Mi nodeExporter: resources: limits: cpu: 10m memory: 20Mi requests: cpu: 5m memory: 10Mi pushgateway: resources: limits: cpu: 10m memory: 20Mi requests: cpu: 5m memory: 10Mi Grafana Application Metrics Resources https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-b190cc97f0f6 https://brancz.com/2018/01/05/prometheus-vs-heapster-vs-kubernetes-metrics-apis/ https://rancher.com/blog/2018/2018-06-26-measuring-metrics-that-matter-in-kubernetes-clusters/","title":"Kubernetes Observability"},{"location":"kubernetes/observability/#kubernetes-observability","text":"","title":"Kubernetes Observability"},{"location":"kubernetes/observability/#monitoring","text":"","title":"Monitoring"},{"location":"kubernetes/observability/#metrics-server","text":"Helm chart: https://github.com/helm/charts/tree/master/stable/metrics-server Home: https://github.com/kubernetes-incubator/metrics-server 1 2 3 4 5 6 7 8 helm install stable/metrics-server \\ --name metrics-server \\ --version 2 .0.3 \\ --namespace metrics kubectl -n metrics \\ rollout status \\ deployment metrics-server","title":"Metrics Server"},{"location":"kubernetes/observability/#prometheus-alert-manager","text":"","title":"Prometheus &amp; Alert Manager"},{"location":"kubernetes/observability/#prometheus-helm-values","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 server: ingress: enabled: true annotations: ingress.kubernetes.io/ssl-redirect: false nginx.ingress.kubernetes.io/ssl-redirect: false resources: limits: cpu: 100m memory: 1000Mi requests: cpu: 10m memory: 500Mi alertmanager: ingress: enabled: true annotations: ingress.kubernetes.io/ssl-redirect: false nginx.ingress.kubernetes.io/ssl-redirect: false resources: limits: cpu: 10m memory: 20Mi requests: cpu: 5m memory: 10Mi kubeStateMetrics: resources: limits: cpu: 10m memory: 50Mi requests: cpu: 5m memory: 25Mi nodeExporter: resources: limits: cpu: 10m memory: 20Mi requests: cpu: 5m memory: 10Mi pushgateway: resources: limits: cpu: 10m memory: 20Mi requests: cpu: 5m memory: 10Mi","title":"Prometheus Helm Values"},{"location":"kubernetes/observability/#grafana","text":"","title":"Grafana"},{"location":"kubernetes/observability/#application-metrics","text":"","title":"Application Metrics"},{"location":"kubernetes/observability/#resources","text":"https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-b190cc97f0f6 https://brancz.com/2018/01/05/prometheus-vs-heapster-vs-kubernetes-metrics-apis/ https://rancher.com/blog/2018/2018-06-26-measuring-metrics-that-matter-in-kubernetes-clusters/","title":"Resources"},{"location":"kubernetes/tools/","text":"Kubernetes Tools Kuard Kuard is a small demo application to show your cluster works. Also exposes some info you might want to see. 1 2 kubectl run --restart = Never --image = gcr.io/kuar-demo/kuard-amd64:blue kuard kubectl port-forward kuard 8080 :8080 Open your browser to http://localhost:8080 . Stern Stern allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging. 1 brew install stern Usage Imagine a build in Jenkins using more than one container in the Pod. You want to tail the logs of all containers... you can with stern. 1 stern maven- Kube Capacity Kube Capacity is a simple CLI that provides an overview of the resource requests, limits, and utilization in a Kubernetes cluster. 1 2 brew tap robscott/tap brew install robscott/tap/kube-capacity 1 kube-capacity 1 2 3 4 NODE CPU REQUESTS CPU LIMITS MEMORY REQUESTS MEMORY LIMITS * 560m ( 28 % ) 130m ( 7 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) example-node-1 220m ( 22 % ) 10m ( 1 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) example-node-2 340m ( 34 % ) 120m ( 12 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) 1 kube-capacity --pods 1 2 3 4 5 6 7 8 9 10 NODE NAMESPACE POD CPU REQUESTS CPU LIMITS MEMORY REQUESTS MEMORY LIMITS * * * 560m ( 28 % ) 780m ( 38 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) example-node-1 * * 220m ( 22 % ) 320m ( 32 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) example-node-1 kube-system metrics-server-lwc6z 100m ( 10 % ) 200m ( 20 % ) 100Mi ( 3 % ) 200Mi ( 7 % ) example-node-1 kube-system coredns-7b5bcb98f8 120m ( 12 % ) 120m ( 12 % ) 92Mi ( 3 % ) 160Mi ( 5 % ) example-node-2 * * 340m ( 34 % ) 460m ( 46 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) example-node-2 kube-system kube-proxy-3ki7 200m ( 20 % ) 280m ( 28 % ) 210Mi ( 7 % ) 210Mi ( 7 % ) example-node-2 tiller tiller-deploy 140m ( 14 % ) 180m ( 18 % ) 170Mi ( 5 % ) 200Mi ( 7 % ) 1 kube-capacity --util 1 2 3 4 NODE CPU REQUESTS CPU LIMITS CPU UTIL MEMORY REQUESTS MEMORY LIMITS MEMORY UTIL * 560m ( 28 % ) 130m ( 7 % ) 40m ( 2 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) 470Mi ( 8 % ) example-node-1 220m ( 22 % ) 10m ( 1 % ) 10m ( 1 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) 210Mi ( 7 % ) example-node-2 340m ( 34 % ) 120m ( 12 % ) 30m ( 3 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) 260Mi ( 9 % ) 1 kube-capacity --pods --util Velero Velero RBAC Lookup RBAC Lookup Install bash 1 brew install reactiveops/tap/rbac-lookup Krew 1 kubectl krew install rbac-lookup Lookup user 1 rbac-lookup jvandergriendt -owide Lookup GKE user 1 rbac-lookup jvandergriendt --gke K9S K9S is a tool that gives you a console UI on your kubernetes cluster/namespace. Install 1 brew tap derailed/k9s brew install k9s Use By default is looks at a single namespace, and allows you to view elements of the pods running. 1 k9s -n cje","title":"Tools"},{"location":"kubernetes/tools/#kubernetes-tools","text":"","title":"Kubernetes Tools"},{"location":"kubernetes/tools/#kuard","text":"Kuard is a small demo application to show your cluster works. Also exposes some info you might want to see. 1 2 kubectl run --restart = Never --image = gcr.io/kuar-demo/kuard-amd64:blue kuard kubectl port-forward kuard 8080 :8080 Open your browser to http://localhost:8080 .","title":"Kuard"},{"location":"kubernetes/tools/#stern","text":"Stern allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging. 1 brew install stern","title":"Stern"},{"location":"kubernetes/tools/#usage","text":"Imagine a build in Jenkins using more than one container in the Pod. You want to tail the logs of all containers... you can with stern. 1 stern maven-","title":"Usage"},{"location":"kubernetes/tools/#kube-capacity","text":"Kube Capacity is a simple CLI that provides an overview of the resource requests, limits, and utilization in a Kubernetes cluster. 1 2 brew tap robscott/tap brew install robscott/tap/kube-capacity 1 kube-capacity 1 2 3 4 NODE CPU REQUESTS CPU LIMITS MEMORY REQUESTS MEMORY LIMITS * 560m ( 28 % ) 130m ( 7 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) example-node-1 220m ( 22 % ) 10m ( 1 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) example-node-2 340m ( 34 % ) 120m ( 12 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) 1 kube-capacity --pods 1 2 3 4 5 6 7 8 9 10 NODE NAMESPACE POD CPU REQUESTS CPU LIMITS MEMORY REQUESTS MEMORY LIMITS * * * 560m ( 28 % ) 780m ( 38 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) example-node-1 * * 220m ( 22 % ) 320m ( 32 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) example-node-1 kube-system metrics-server-lwc6z 100m ( 10 % ) 200m ( 20 % ) 100Mi ( 3 % ) 200Mi ( 7 % ) example-node-1 kube-system coredns-7b5bcb98f8 120m ( 12 % ) 120m ( 12 % ) 92Mi ( 3 % ) 160Mi ( 5 % ) example-node-2 * * 340m ( 34 % ) 460m ( 46 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) example-node-2 kube-system kube-proxy-3ki7 200m ( 20 % ) 280m ( 28 % ) 210Mi ( 7 % ) 210Mi ( 7 % ) example-node-2 tiller tiller-deploy 140m ( 14 % ) 180m ( 18 % ) 170Mi ( 5 % ) 200Mi ( 7 % ) 1 kube-capacity --util 1 2 3 4 NODE CPU REQUESTS CPU LIMITS CPU UTIL MEMORY REQUESTS MEMORY LIMITS MEMORY UTIL * 560m ( 28 % ) 130m ( 7 % ) 40m ( 2 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) 470Mi ( 8 % ) example-node-1 220m ( 22 % ) 10m ( 1 % ) 10m ( 1 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) 210Mi ( 7 % ) example-node-2 340m ( 34 % ) 120m ( 12 % ) 30m ( 3 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) 260Mi ( 9 % ) 1 kube-capacity --pods --util","title":"Kube Capacity"},{"location":"kubernetes/tools/#velero","text":"Velero","title":"Velero"},{"location":"kubernetes/tools/#rbac-lookup","text":"RBAC Lookup","title":"RBAC Lookup"},{"location":"kubernetes/tools/#install","text":"bash 1 brew install reactiveops/tap/rbac-lookup Krew 1 kubectl krew install rbac-lookup","title":"Install"},{"location":"kubernetes/tools/#lookup-user","text":"1 rbac-lookup jvandergriendt -owide","title":"Lookup user"},{"location":"kubernetes/tools/#lookup-gke-user","text":"1 rbac-lookup jvandergriendt --gke","title":"Lookup GKE user"},{"location":"kubernetes/tools/#k9s","text":"K9S is a tool that gives you a console UI on your kubernetes cluster/namespace.","title":"K9S"},{"location":"kubernetes/tools/#install_1","text":"1 brew tap derailed/k9s brew install k9s","title":"Install"},{"location":"kubernetes/tools/#use","text":"By default is looks at a single namespace, and allows you to view elements of the pods running. 1 k9s -n cje","title":"Use"},{"location":"kubernetes/cicd/cdp-jenkins-helm/","text":"CD Pipeline with Jenkins Helm Prerequisites Kubernetes 1.9.x+ cluster Valid domain names Jenkins 2.x+ with pipeline plugins below Helm/Tiller Tools Jenkins 2.x Install Helm For more information, checkout the github page . Helm's current version (as of October 2018) - version 2 - consists of two parts. One is a local client - Helm - which you should install on your own machine, see here for how. The other is a server component part - Tiller - that should be installed in your Kubernetes cluster. Install Tiller 1 kubectl create serviceaccount tiller --namespace kube-system create rbac config: rbac-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : v1 kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : tiller-role-binding roleRef : kind : ClusterRole name : cluster-admin apiGroup : rbac.authorization.k8s.io subjects : - kind : ServiceAccount name : tiller namespace : kube-system 1 2 kubectl apply -f rbac-config.yaml helm init --service-account tiller install nging helm chart 1 helm install stable/nginx-ingress Jenkins Plugins Warnings Plugin: https://github.com/jenkinsci/warnings-plugin/blob/master/doc/Documentation.md Anchore: https://jenkins.io/blog/2018/06/20/anchore-image-scanning/ Anchore https://github.com/anchore/anchore-engine https://github.com/helm/charts/tree/master/stable/anchore-engine https://wiki.jenkins.io/display/JENKINS/Anchore+Container+Image+Scanner+Plugin","title":"CD Pipeline Jenkins & Helm"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#cd-pipeline-with-jenkins-helm","text":"","title":"CD Pipeline with Jenkins &amp; Helm"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#prerequisites","text":"Kubernetes 1.9.x+ cluster Valid domain names Jenkins 2.x+ with pipeline plugins below Helm/Tiller","title":"Prerequisites"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#tools","text":"Jenkins 2.x","title":"Tools"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#install-helm","text":"For more information, checkout the github page . Helm's current version (as of October 2018) - version 2 - consists of two parts. One is a local client - Helm - which you should install on your own machine, see here for how. The other is a server component part - Tiller - that should be installed in your Kubernetes cluster.","title":"Install Helm"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#install-tiller","text":"1 kubectl create serviceaccount tiller --namespace kube-system create rbac config: rbac-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : v1 kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : tiller-role-binding roleRef : kind : ClusterRole name : cluster-admin apiGroup : rbac.authorization.k8s.io subjects : - kind : ServiceAccount name : tiller namespace : kube-system 1 2 kubectl apply -f rbac-config.yaml helm init --service-account tiller","title":"Install Tiller"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#install-nging-helm-chart","text":"1 helm install stable/nginx-ingress","title":"install nging helm chart"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#jenkins-plugins","text":"Warnings Plugin: https://github.com/jenkinsci/warnings-plugin/blob/master/doc/Documentation.md Anchore: https://jenkins.io/blog/2018/06/20/anchore-image-scanning/","title":"Jenkins Plugins"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#anchore","text":"https://github.com/anchore/anchore-engine https://github.com/helm/charts/tree/master/stable/anchore-engine https://wiki.jenkins.io/display/JENKINS/Anchore+Container+Image+Scanner+Plugin","title":"Anchore"},{"location":"kubernetes/distributions/","text":"Kubernetes What is kubernetes Kubernetes Objects Kubernetes tutorials Kubernetes Guides Linux basics Namespaces CGroups https://jvns.ca/blog/2016/10/10/what-even-is-a-container/ https://www.youtube.com/watch?v=sK5i-N34im8 https://www.ianlewis.org/en/what-are-kubernetes-pods-anyway Networking https://itnext.io/kubernetes-networking-behind-the-scenes-39a1ab1792bb https://github.com/nleiva/kubernetes-networking-links IP Tables CIDR Explanation video Packets Frames introduction Ingress Traefik on AWS Metrics https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-66936addedae Secrets Hashicorp Vault - Kelsey Hightower https://medium.com/qubit-engineering/kubernetes-up-integrated-secrets-configuration-5a15b9f5a6c6 Security RBAC 11 ways not to get hacked on Kubernetes https://www.youtube.com/channel/UCiqnRXPAAk6iv2m47odUFzw Tools to use Microscanner Dex - OpenID Connect solution Sonubuoy Helm Learn LUA book , lua's required for Helm 3.0 ChartMuseum Skaffold KSynch Traefik Istio Falco Prometheus Grafana Jenkins Kaniko Prow Tarmak Kube-Lego Knative Rook Stern - aggregate log rendering tool Linkerd 2","title":"Kubernetes"},{"location":"kubernetes/distributions/#kubernetes","text":"","title":"Kubernetes"},{"location":"kubernetes/distributions/#what-is-kubernetes","text":"","title":"What is kubernetes"},{"location":"kubernetes/distributions/#kubernetes-objects","text":"","title":"Kubernetes Objects"},{"location":"kubernetes/distributions/#kubernetes-tutorials","text":"","title":"Kubernetes tutorials"},{"location":"kubernetes/distributions/#kubernetes-guides","text":"","title":"Kubernetes Guides"},{"location":"kubernetes/distributions/#linux-basics","text":"","title":"Linux basics"},{"location":"kubernetes/distributions/#namespaces-cgroups","text":"https://jvns.ca/blog/2016/10/10/what-even-is-a-container/ https://www.youtube.com/watch?v=sK5i-N34im8 https://www.ianlewis.org/en/what-are-kubernetes-pods-anyway","title":"Namespaces &amp; CGroups"},{"location":"kubernetes/distributions/#networking","text":"https://itnext.io/kubernetes-networking-behind-the-scenes-39a1ab1792bb https://github.com/nleiva/kubernetes-networking-links IP Tables CIDR Explanation video Packets Frames introduction","title":"Networking"},{"location":"kubernetes/distributions/#ingress","text":"Traefik on AWS","title":"Ingress"},{"location":"kubernetes/distributions/#metrics","text":"https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-66936addedae","title":"Metrics"},{"location":"kubernetes/distributions/#secrets","text":"Hashicorp Vault - Kelsey Hightower https://medium.com/qubit-engineering/kubernetes-up-integrated-secrets-configuration-5a15b9f5a6c6","title":"Secrets"},{"location":"kubernetes/distributions/#security","text":"RBAC 11 ways not to get hacked on Kubernetes https://www.youtube.com/channel/UCiqnRXPAAk6iv2m47odUFzw","title":"Security"},{"location":"kubernetes/distributions/#tools-to-use","text":"Microscanner Dex - OpenID Connect solution Sonubuoy Helm Learn LUA book , lua's required for Helm 3.0 ChartMuseum Skaffold KSynch Traefik Istio Falco Prometheus Grafana Jenkins Kaniko Prow Tarmak Kube-Lego Knative Rook Stern - aggregate log rendering tool Linkerd 2","title":"Tools to use"},{"location":"kubernetes/distributions/aks/","text":"Azure Kubernetes Service Resources https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/ https://www.cloudbees.com/blog/securing-jenkins-role-based-access-control-and-azure-active-directory https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/aks-install/# https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/kubernetes-helm-install/#_additional_information_creating_a_tls_certificate Configure AZ CLI 1 az login 1 az account show --query {subscriptionId:id, tenantId:tenantId} 1 export SUBSCRIPTION_ID = 1 2 SUBSCRIPTION_ID = ... az account set --subscription = ${ SUBSCRIPTION_ID } 1 az ad sp create-for-rbac --role = Owner --scopes = /subscriptions/ ${ SUBSCRIPTION_ID } Should be owner Should be owner, else it cannot create a LoadBalancer via the nginx-ingress . See: https://weidongzhou.wordpress.com/2018/06/27/could-not-get-external-ip-for-load-balancer-on-azure-aks/ And: https://github.com/Azure/AKS/issues/427 Terraform Config 1 2 3 4 5 ARM_SUBSCRIPTION_ID ARM_CLIENT_ID ARM_CLIENT_SECRET ARM_TENANT_ID ARM_ENVIRONMENT Create storage account for TF State 1 2 3 4 LOCATION = westeurope RESOURCE_GROUP_NAME = joostvdg-cb-ext-storage STORAGE_ACCOUNT_NAME = joostvdgcbtfstate CONTAINER_NAME = tfstate List locations 1 2 3 az account list-locations \\ --query [].{Region:name} \\ --out table Create resource group 1 2 3 az group create \\ --name ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION } Create storage account 1 2 3 4 5 6 az storage account create \\ --name ${ STORAGE_ACCOUNT_NAME } \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION } \\ --sku Standard_ZRS \\ --kind StorageV2 Retrieve storage account login Apparently, no CLI commands available? Use the Azure Blog on AKS via Terraform for how via the UI. 1 STORAGE_ACCOUNT_KEY = Create TF Storage 1 az storage container create -n ${ CONTAINER_NAME } --account-name ${ STORAGE_ACCOUNT_NAME } --account-key ${ STORAGE_ACCOUNT_KEY } Init Terraform backend 1 2 3 4 terraform init -backend-config = storage_account_name= ${ STORAGE_ACCOUNT_NAME } \\ -backend-config = container_name= ${ CONTAINER_NAME } \\ -backend-config = access_key= ${ STORAGE_ACCOUNT_KEY } \\ -backend-config = key=codelab.microsoft.tfstate Expose temp variables These are from your Service Principle we created earlier. Where client_id = appId, and client_secret the password. 1 2 export TF_VAR_client_id = your-client-id export TF_VAR_client_secret = your-client-secret Rollout Set variables 1 source ../export-variables.sh Validate 1 terraform validate Plan 1 terraform plan -out out.plan Apply the plan 1 terraform apply out.plan Configure Kubecontext 1 az aks get-credentials --resource-group cbcore --name cbcore Configure Cluster Autoscaler 1 az extension add --name aks-preview 1 2 3 4 az feature register --name VMSSPreview --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService/VMSSPreview )].{Name:name,State:properties.state} az provider register --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService )].{Name:name,State:properties.state} Configure multi-node pool 1 2 az feature register --name MultiAgentpoolPreview --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService/MultiAgentpoolPreview )].{Name:name,State:properties.state} 1 az provider register --namespace Microsoft.ContainerService Create AKS cluster via CLI Resources https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/ Get available versions 1 az aks get-versions --location westeurope Create initial cluster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Create a resource group in East US az group create --name myResourceGroup --location eastus # Create a basic single-node AKS cluster az aks create \\ --resource-group myResourceGroup \\ --name myAKSCluster \\ --enable-vmss \\ --node-count 1 \\ --nodepool-name masters \\ --node-vm-size Standard_DS2_v2 \\ --enable-cluster-autoscaler \\ --enable-vmss \\ --generate-ssh-keys PodSecurityPolicy 1 2 --enable-cluster-autoscaler : Enable cluster autoscaler, default value is false. If specified, please make sure the kubernetes version is larger than 1 .10.6. Networking 1 2 3 4 5 6 7 --network-plugin : The Kubernetes network plugin to use. Specify azure for advanced networking configurations. Defaults to kubenet . --network-policy : ( PREVIEW ) The Kubernetes network policy to use. Using together with azure network plugin. Specify azure for Azure network policy manager and calico for calico network policy controller. Defaults to ( network policy disabled ) . Retrieve credentials 1 az aks get-credentials --resource-group myResourceGroup --name myAKSCluster Add second node pool 1 2 3 4 5 az aks nodepool add \\ --resource-group myResourceGroup \\ --cluster-name myAKSCluster \\ --name mynodepool \\ --node-count 3","title":"Azure Kubernetes Service"},{"location":"kubernetes/distributions/aks/#azure-kubernetes-service","text":"","title":"Azure Kubernetes Service"},{"location":"kubernetes/distributions/aks/#resources","text":"https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/ https://www.cloudbees.com/blog/securing-jenkins-role-based-access-control-and-azure-active-directory https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/aks-install/# https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/kubernetes-helm-install/#_additional_information_creating_a_tls_certificate","title":"Resources"},{"location":"kubernetes/distributions/aks/#configure-az-cli","text":"1 az login 1 az account show --query {subscriptionId:id, tenantId:tenantId} 1 export SUBSCRIPTION_ID = 1 2 SUBSCRIPTION_ID = ... az account set --subscription = ${ SUBSCRIPTION_ID } 1 az ad sp create-for-rbac --role = Owner --scopes = /subscriptions/ ${ SUBSCRIPTION_ID }","title":"Configure AZ CLI"},{"location":"kubernetes/distributions/aks/#should-be-owner","text":"Should be owner, else it cannot create a LoadBalancer via the nginx-ingress . See: https://weidongzhou.wordpress.com/2018/06/27/could-not-get-external-ip-for-load-balancer-on-azure-aks/ And: https://github.com/Azure/AKS/issues/427","title":"Should be owner"},{"location":"kubernetes/distributions/aks/#terraform-config","text":"1 2 3 4 5 ARM_SUBSCRIPTION_ID ARM_CLIENT_ID ARM_CLIENT_SECRET ARM_TENANT_ID ARM_ENVIRONMENT","title":"Terraform Config"},{"location":"kubernetes/distributions/aks/#create-storage-account-for-tf-state","text":"1 2 3 4 LOCATION = westeurope RESOURCE_GROUP_NAME = joostvdg-cb-ext-storage STORAGE_ACCOUNT_NAME = joostvdgcbtfstate CONTAINER_NAME = tfstate","title":"Create storage account for TF State"},{"location":"kubernetes/distributions/aks/#list-locations","text":"1 2 3 az account list-locations \\ --query [].{Region:name} \\ --out table","title":"List locations"},{"location":"kubernetes/distributions/aks/#create-resource-group","text":"1 2 3 az group create \\ --name ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION }","title":"Create resource group"},{"location":"kubernetes/distributions/aks/#create-storage-account","text":"1 2 3 4 5 6 az storage account create \\ --name ${ STORAGE_ACCOUNT_NAME } \\ --resource-group ${ RESOURCE_GROUP_NAME } \\ --location ${ LOCATION } \\ --sku Standard_ZRS \\ --kind StorageV2","title":"Create storage account"},{"location":"kubernetes/distributions/aks/#retrieve-storage-account-login","text":"Apparently, no CLI commands available? Use the Azure Blog on AKS via Terraform for how via the UI. 1 STORAGE_ACCOUNT_KEY =","title":"Retrieve storage account login"},{"location":"kubernetes/distributions/aks/#create-tf-storage","text":"1 az storage container create -n ${ CONTAINER_NAME } --account-name ${ STORAGE_ACCOUNT_NAME } --account-key ${ STORAGE_ACCOUNT_KEY }","title":"Create TF Storage"},{"location":"kubernetes/distributions/aks/#init-terraform-backend","text":"1 2 3 4 terraform init -backend-config = storage_account_name= ${ STORAGE_ACCOUNT_NAME } \\ -backend-config = container_name= ${ CONTAINER_NAME } \\ -backend-config = access_key= ${ STORAGE_ACCOUNT_KEY } \\ -backend-config = key=codelab.microsoft.tfstate","title":"Init Terraform backend"},{"location":"kubernetes/distributions/aks/#expose-temp-variables","text":"These are from your Service Principle we created earlier. Where client_id = appId, and client_secret the password. 1 2 export TF_VAR_client_id = your-client-id export TF_VAR_client_secret = your-client-secret","title":"Expose temp variables"},{"location":"kubernetes/distributions/aks/#rollout","text":"","title":"Rollout"},{"location":"kubernetes/distributions/aks/#set-variables","text":"1 source ../export-variables.sh","title":"Set variables"},{"location":"kubernetes/distributions/aks/#validate","text":"1 terraform validate","title":"Validate"},{"location":"kubernetes/distributions/aks/#plan","text":"1 terraform plan -out out.plan","title":"Plan"},{"location":"kubernetes/distributions/aks/#apply-the-plan","text":"1 terraform apply out.plan","title":"Apply the plan"},{"location":"kubernetes/distributions/aks/#configure-kubecontext","text":"1 az aks get-credentials --resource-group cbcore --name cbcore","title":"Configure Kubecontext"},{"location":"kubernetes/distributions/aks/#configure-cluster-autoscaler","text":"1 az extension add --name aks-preview 1 2 3 4 az feature register --name VMSSPreview --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService/VMSSPreview )].{Name:name,State:properties.state} az provider register --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService )].{Name:name,State:properties.state}","title":"Configure Cluster Autoscaler"},{"location":"kubernetes/distributions/aks/#configure-multi-node-pool","text":"1 2 az feature register --name MultiAgentpoolPreview --namespace Microsoft.ContainerService az feature list -o table --query [?contains(name, Microsoft.ContainerService/MultiAgentpoolPreview )].{Name:name,State:properties.state} 1 az provider register --namespace Microsoft.ContainerService","title":"Configure multi-node pool"},{"location":"kubernetes/distributions/aks/#create-aks-cluster-via-cli","text":"","title":"Create AKS cluster via CLI"},{"location":"kubernetes/distributions/aks/#resources_1","text":"https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/","title":"Resources"},{"location":"kubernetes/distributions/aks/#get-available-versions","text":"1 az aks get-versions --location westeurope","title":"Get available versions"},{"location":"kubernetes/distributions/aks/#create-initial-cluster","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Create a resource group in East US az group create --name myResourceGroup --location eastus # Create a basic single-node AKS cluster az aks create \\ --resource-group myResourceGroup \\ --name myAKSCluster \\ --enable-vmss \\ --node-count 1 \\ --nodepool-name masters \\ --node-vm-size Standard_DS2_v2 \\ --enable-cluster-autoscaler \\ --enable-vmss \\ --generate-ssh-keys","title":"Create initial cluster"},{"location":"kubernetes/distributions/aks/#podsecuritypolicy","text":"1 2 --enable-cluster-autoscaler : Enable cluster autoscaler, default value is false. If specified, please make sure the kubernetes version is larger than 1 .10.6.","title":"PodSecurityPolicy"},{"location":"kubernetes/distributions/aks/#networking","text":"1 2 3 4 5 6 7 --network-plugin : The Kubernetes network plugin to use. Specify azure for advanced networking configurations. Defaults to kubenet . --network-policy : ( PREVIEW ) The Kubernetes network policy to use. Using together with azure network plugin. Specify azure for Azure network policy manager and calico for calico network policy controller. Defaults to ( network policy disabled ) .","title":"Networking"},{"location":"kubernetes/distributions/aks/#retrieve-credentials","text":"1 az aks get-credentials --resource-group myResourceGroup --name myAKSCluster","title":"Retrieve credentials"},{"location":"kubernetes/distributions/aks/#add-second-node-pool","text":"1 2 3 4 5 az aks nodepool add \\ --resource-group myResourceGroup \\ --cluster-name myAKSCluster \\ --name mynodepool \\ --node-count 3","title":"Add second node pool"},{"location":"kubernetes/distributions/aws-eks/","text":"AWS EKS Resources https://kubedex.com/90-days-of-aws-eks-in-production/","title":"AWS EKS"},{"location":"kubernetes/distributions/aws-eks/#aws-eks","text":"","title":"AWS EKS"},{"location":"kubernetes/distributions/aws-eks/#resources","text":"https://kubedex.com/90-days-of-aws-eks-in-production/","title":"Resources"},{"location":"kubernetes/distributions/install-gke/","text":"GCE GKE Install Set env 1 2 3 4 5 6 7 8 9 ZONE = $( gcloud compute zones list --filter region:(europe-west4) | awk {print $1} | tail -n 1 ) ZONES = $( gcloud compute zones list --filter region:(europe-west4) | tail -n +2 | awk {print $1} | tr \\n , ) MACHINE_TYPE = n1-highcpu-2 MACHINE_TYPE = n1-standard-2 echo ZONE = $ZONE echo ZONES = $ZONES echo MACHINE_TYPE = $MACHINE_TYPE Get supported K8s versions 1 gcloud container get-server-config --zone = $ZONE --format = json 1 MASTER_VERSION = 1.10.5-gke.0 Create cluster 1 2 3 4 5 6 7 8 9 10 gcloud container clusters \\ create devops24 \\ --zone $ZONE \\ --node-locations $ZONES \\ --machine-type $MACHINE_TYPE \\ --enable-autoscaling \\ --num-nodes 1 \\ --max-nodes 1 \\ --min-nodes 1 \\ --cluster-version $MASTER_VERSION Kubernetes post install create cluster role binding install nginx as ingress controller install tiller configure helm 1 2 3 4 5 6 7 kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $( gcloud config get-value account ) kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/cloud-generic.yaml kubectl create -f https://raw.githubusercontent.com/vfarcic/k8s-specs/master/helm/tiller-rbac.yml --record --save-config kubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller helm init --service-account tiller Install Weave net 1 kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n ) Encryption You can run Weave Net with encryption on. This requires a Kubernetes secret containing the encryption password. 1 2 3 4 5 6 7 8 cat weave-secret EOF MSjNDSC6Rw7F3P3j8klHZq1v EOF kubectl create secret -n kube-system generic weave-secret --from-file = ./weave-secret kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n ) password-secret=weave-secret kubectl get pods -n kube-system -l name = weave-net -o wide kubectl exec -n kube-system weave-net- -c weave -- /home/weave/weave --local status Installation error Note, that installing Weave Net on GKE requires the cluster-admin role to be bound to yourself. Else you will not have enough rights. If you see: 1 2 rror from server ( Forbidden ) : clusterroles.rbac.authorization.k8s.io weave-net is forbidden: attempt to grant extra privileges: [ PolicyRule { APIGroups: [ ] , Resources: [ pods ] , Verbs: [ get ]} PolicyRule { APIGroups: [ ] , Resources: [ pods ] , Verbs: [ list ]} PolicyRule { APIGroups: [ ] , Resources: [ pods ] , Verbs: [ watch ]} PolicyRule { APIGroups: [ ] , Resources: [ namespaces ] , Verbs: [ get ]} PolicyRule { APIGroups: [ ] , Resources: [ namespaces ] , Verbs: [ list ]} PolicyRule { APIGroups: [ ] , Resources: [ namespaces ] , Verbs: [ watch ]} PolicyRule { APIGroups: [ ] , Resources: [ nodes ] , Verbs: [ get ]} PolicyRule { APIGroups: [ ] , Resources: [ nodes ] , Verbs: [ list ]} PolicyRule { APIGroups: [ ] , Resources: [ nodes ] , Verbs: [ watch ]} PolicyRule { APIGroups: [ networking.k8s.io ] , Resources: [ networkpolicies ] , Verbs: [ get ]} PolicyRule { APIGroups: [ networking.k8s.io ] , Resources: [ networkpolicies ] , Verbs: [ list ]} PolicyRule { APIGroups: [ networking.k8s.io ] , Resources: [ networkpolicies ] , Verbs: [ watch ]} PolicyRule { APIGroups: [ ] , Resources: [ nodes/status ] , Verbs: [ patch ]} PolicyRule { APIGroups: [ ] , Resources: [ nodes/status ] , Verbs: [ update ]}] user = { joostvdg@gmail.com [ system:authenticated ] map []} ownerrules =[ PolicyRule { APIGroups: [ authorization.k8s.io ] , Resources: [ selfsubjectaccessreviews selfsubjectrulesreviews ] , Verbs: [ create ]} PolicyRule { NonResourceURLs: [ /api /api/* /apis /apis/* /healthz /openapi /openapi/* /swagger-2.0.0.pb-v1 /swagger.json /swaggerapi /swaggerapi/* /version /version/ ] , Verbs: [ get ]}] ruleResolutionErrors =[] Error from server ( Forbidden ) : roles.rbac.authorization.k8s.io weave-net is forbidden: attempt to grant extra privileges: [ PolicyRule { APIGroups: [ ] , Resources: [ configmaps ] , ResourceNames: [ weave-net ] , Verbs: [ get ]} PolicyRule { APIGroups: [ ] , Resources: [ configmaps ] , ResourceNames: [ weave-net ] , Verbs: [ update ]} PolicyRule { APIGroups: [ ] , Resources: [ configmaps ] , Verbs: [ create ]}] user = { joostvdg@gmail.com [ system:authenticated ] map []} ownerrules =[ PolicyRule { APIGroups: [ authorization.k8s.io ] , Resources: [ selfsubjectaccessreviews selfsubjectrulesreviews ] , Verbs: [ create ]} PolicyRule { NonResourceURLs: [ /api /api/* /apis /apis/* /healthz /openapi /openapi/* /swagger-2.0.0.pb-v1 /swagger.json /swaggerapi /swaggerapi/* /version /version/ ] , Verbs: [ get ]}] ruleResolutionErrors =[] Execute the following before attempting to install Weave Net again. 1 kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $( gcloud config get-value account ) Prometheus Grafana https://rohanc.me/monitoring-kubernetes-prometheus-grafana/ 1 helm install stable/prometheus --name my-prometheus Grafana config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 persistence : enabled : true accessModes : - ReadWriteOnce size : 5Gi datasources : datasources.yaml : apiVersion : 1 datasources : - name : Prometheus type : prometheus url : http://my-prometheus-server access : proxy isDefault : true dashboards : default : kube-dash : gnetId : 6663 revision : 1 datasource : Prometheus kube-official-dash : gnetId : 2 revision : 1 datasource : Prometheus dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : default orgId : 1 folder : type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards Other dashboards to import: * 3131 * 5309 * 5312 * 315 Get Cluster IP 1 2 3 4 5 6 7 8 9 10 11 export LB_IP = $( kubectl -n ingress-nginx \\ get svc ingress-nginx \\ -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) echo $LB_IP export DNS = ${ LB_IP } .nip.io echo $DNS export JENKINS_DNS = jenkins. ${ DNS } echo $JENKINS_DNS Install CJE Create SSD SC 1 2 3 4 5 6 7 8 9 echo apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ssd provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd ssd-storage.yaml kubectl create -f ssd-storage.yaml Setup CJE Namespace 1 2 3 kubectl create namespace cje kubectl label namespace cje name = cje kubectl config set-context $( kubectl config current-context ) --namespace = cje Adjust Domain name 1 export PREV_DOMAIN_NAME = 1 sed -e s, $PREV_DOMAIN_NAME , $JENKINS_DNS ,g cje.yml tmp mv tmp cje.yml Install Jenkins 1 2 3 4 kubectl apply -f cje.yml kubectl rollout status sts cjoc sleep 180 kubectl exec cjoc-0 -- cat /var/jenkins_home/secrets/initialAdminPassword Install Jenkins - k8s-specs 1 2 3 kubectl apply -f joost/jenkins.yml sleep 180 kubectl exec -it --namespace jenkins jenkins-0 cat /var/jenkins_home/secrets/initialAdminPassword Install Keycloak - k8s-specs 1 2 3 4 5 6 kubectl apply -f joost/keycloak.yml sleep 120 kubectl -n jenkins exec -it keycloak-0 -- /bin/bash keycloak/bin/add-user-keycloak.sh -u somekindofuser -p X5qpLMnWKUx7 ps -ef | grep java kill -9 PID Follow log 1 k -n jenkins logs -f keycloak Jenkins Keycloak config 1 2 3 4 5 6 7 { realm : master , auth-server-url : http://35.204.112.229/auth , ssl-required : external , resource : jenkins , public-client : true } Destroy cluster 1 2 3 4 gcloud container clusters \\ delete devops24 \\ --zone $ZONE \\ --quiet","title":"GKE"},{"location":"kubernetes/distributions/install-gke/#gce","text":"","title":"GCE"},{"location":"kubernetes/distributions/install-gke/#gke-install","text":"","title":"GKE Install"},{"location":"kubernetes/distributions/install-gke/#set-env","text":"1 2 3 4 5 6 7 8 9 ZONE = $( gcloud compute zones list --filter region:(europe-west4) | awk {print $1} | tail -n 1 ) ZONES = $( gcloud compute zones list --filter region:(europe-west4) | tail -n +2 | awk {print $1} | tr \\n , ) MACHINE_TYPE = n1-highcpu-2 MACHINE_TYPE = n1-standard-2 echo ZONE = $ZONE echo ZONES = $ZONES echo MACHINE_TYPE = $MACHINE_TYPE","title":"Set env"},{"location":"kubernetes/distributions/install-gke/#get-supported-k8s-versions","text":"1 gcloud container get-server-config --zone = $ZONE --format = json 1 MASTER_VERSION = 1.10.5-gke.0","title":"Get supported K8s versions"},{"location":"kubernetes/distributions/install-gke/#create-cluster","text":"1 2 3 4 5 6 7 8 9 10 gcloud container clusters \\ create devops24 \\ --zone $ZONE \\ --node-locations $ZONES \\ --machine-type $MACHINE_TYPE \\ --enable-autoscaling \\ --num-nodes 1 \\ --max-nodes 1 \\ --min-nodes 1 \\ --cluster-version $MASTER_VERSION","title":"Create cluster"},{"location":"kubernetes/distributions/install-gke/#kubernetes-post-install","text":"create cluster role binding install nginx as ingress controller install tiller configure helm 1 2 3 4 5 6 7 kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $( gcloud config get-value account ) kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/cloud-generic.yaml kubectl create -f https://raw.githubusercontent.com/vfarcic/k8s-specs/master/helm/tiller-rbac.yml --record --save-config kubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller helm init --service-account tiller","title":"Kubernetes post install"},{"location":"kubernetes/distributions/install-gke/#install-weave-net","text":"1 kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n )","title":"Install Weave net"},{"location":"kubernetes/distributions/install-gke/#encryption","text":"You can run Weave Net with encryption on. This requires a Kubernetes secret containing the encryption password. 1 2 3 4 5 6 7 8 cat weave-secret EOF MSjNDSC6Rw7F3P3j8klHZq1v EOF kubectl create secret -n kube-system generic weave-secret --from-file = ./weave-secret kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n ) password-secret=weave-secret kubectl get pods -n kube-system -l name = weave-net -o wide kubectl exec -n kube-system weave-net- -c weave -- /home/weave/weave --local status","title":"Encryption"},{"location":"kubernetes/distributions/install-gke/#installation-error","text":"Note, that installing Weave Net on GKE requires the cluster-admin role to be bound to yourself. Else you will not have enough rights. If you see: 1 2 rror from server ( Forbidden ) : clusterroles.rbac.authorization.k8s.io weave-net is forbidden: attempt to grant extra privileges: [ PolicyRule { APIGroups: [ ] , Resources: [ pods ] , Verbs: [ get ]} PolicyRule { APIGroups: [ ] , Resources: [ pods ] , Verbs: [ list ]} PolicyRule { APIGroups: [ ] , Resources: [ pods ] , Verbs: [ watch ]} PolicyRule { APIGroups: [ ] , Resources: [ namespaces ] , Verbs: [ get ]} PolicyRule { APIGroups: [ ] , Resources: [ namespaces ] , Verbs: [ list ]} PolicyRule { APIGroups: [ ] , Resources: [ namespaces ] , Verbs: [ watch ]} PolicyRule { APIGroups: [ ] , Resources: [ nodes ] , Verbs: [ get ]} PolicyRule { APIGroups: [ ] , Resources: [ nodes ] , Verbs: [ list ]} PolicyRule { APIGroups: [ ] , Resources: [ nodes ] , Verbs: [ watch ]} PolicyRule { APIGroups: [ networking.k8s.io ] , Resources: [ networkpolicies ] , Verbs: [ get ]} PolicyRule { APIGroups: [ networking.k8s.io ] , Resources: [ networkpolicies ] , Verbs: [ list ]} PolicyRule { APIGroups: [ networking.k8s.io ] , Resources: [ networkpolicies ] , Verbs: [ watch ]} PolicyRule { APIGroups: [ ] , Resources: [ nodes/status ] , Verbs: [ patch ]} PolicyRule { APIGroups: [ ] , Resources: [ nodes/status ] , Verbs: [ update ]}] user = { joostvdg@gmail.com [ system:authenticated ] map []} ownerrules =[ PolicyRule { APIGroups: [ authorization.k8s.io ] , Resources: [ selfsubjectaccessreviews selfsubjectrulesreviews ] , Verbs: [ create ]} PolicyRule { NonResourceURLs: [ /api /api/* /apis /apis/* /healthz /openapi /openapi/* /swagger-2.0.0.pb-v1 /swagger.json /swaggerapi /swaggerapi/* /version /version/ ] , Verbs: [ get ]}] ruleResolutionErrors =[] Error from server ( Forbidden ) : roles.rbac.authorization.k8s.io weave-net is forbidden: attempt to grant extra privileges: [ PolicyRule { APIGroups: [ ] , Resources: [ configmaps ] , ResourceNames: [ weave-net ] , Verbs: [ get ]} PolicyRule { APIGroups: [ ] , Resources: [ configmaps ] , ResourceNames: [ weave-net ] , Verbs: [ update ]} PolicyRule { APIGroups: [ ] , Resources: [ configmaps ] , Verbs: [ create ]}] user = { joostvdg@gmail.com [ system:authenticated ] map []} ownerrules =[ PolicyRule { APIGroups: [ authorization.k8s.io ] , Resources: [ selfsubjectaccessreviews selfsubjectrulesreviews ] , Verbs: [ create ]} PolicyRule { NonResourceURLs: [ /api /api/* /apis /apis/* /healthz /openapi /openapi/* /swagger-2.0.0.pb-v1 /swagger.json /swaggerapi /swaggerapi/* /version /version/ ] , Verbs: [ get ]}] ruleResolutionErrors =[] Execute the following before attempting to install Weave Net again. 1 kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $( gcloud config get-value account )","title":"Installation error"},{"location":"kubernetes/distributions/install-gke/#prometheus-grafana","text":"https://rohanc.me/monitoring-kubernetes-prometheus-grafana/ 1 helm install stable/prometheus --name my-prometheus","title":"Prometheus &amp; Grafana"},{"location":"kubernetes/distributions/install-gke/#grafana-config","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 persistence : enabled : true accessModes : - ReadWriteOnce size : 5Gi datasources : datasources.yaml : apiVersion : 1 datasources : - name : Prometheus type : prometheus url : http://my-prometheus-server access : proxy isDefault : true dashboards : default : kube-dash : gnetId : 6663 revision : 1 datasource : Prometheus kube-official-dash : gnetId : 2 revision : 1 datasource : Prometheus dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : default orgId : 1 folder : type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards Other dashboards to import: * 3131 * 5309 * 5312 * 315","title":"Grafana config"},{"location":"kubernetes/distributions/install-gke/#get-cluster-ip","text":"1 2 3 4 5 6 7 8 9 10 11 export LB_IP = $( kubectl -n ingress-nginx \\ get svc ingress-nginx \\ -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) echo $LB_IP export DNS = ${ LB_IP } .nip.io echo $DNS export JENKINS_DNS = jenkins. ${ DNS } echo $JENKINS_DNS","title":"Get Cluster IP"},{"location":"kubernetes/distributions/install-gke/#install-cje","text":"","title":"Install CJE"},{"location":"kubernetes/distributions/install-gke/#create-ssd-sc","text":"1 2 3 4 5 6 7 8 9 echo apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ssd provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd ssd-storage.yaml kubectl create -f ssd-storage.yaml","title":"Create SSD SC"},{"location":"kubernetes/distributions/install-gke/#setup-cje-namespace","text":"1 2 3 kubectl create namespace cje kubectl label namespace cje name = cje kubectl config set-context $( kubectl config current-context ) --namespace = cje","title":"Setup CJE Namespace"},{"location":"kubernetes/distributions/install-gke/#adjust-domain-name","text":"1 export PREV_DOMAIN_NAME = 1 sed -e s, $PREV_DOMAIN_NAME , $JENKINS_DNS ,g cje.yml tmp mv tmp cje.yml","title":"Adjust Domain name"},{"location":"kubernetes/distributions/install-gke/#install-jenkins","text":"1 2 3 4 kubectl apply -f cje.yml kubectl rollout status sts cjoc sleep 180 kubectl exec cjoc-0 -- cat /var/jenkins_home/secrets/initialAdminPassword","title":"Install Jenkins"},{"location":"kubernetes/distributions/install-gke/#install-jenkins-k8s-specs","text":"1 2 3 kubectl apply -f joost/jenkins.yml sleep 180 kubectl exec -it --namespace jenkins jenkins-0 cat /var/jenkins_home/secrets/initialAdminPassword","title":"Install Jenkins - k8s-specs"},{"location":"kubernetes/distributions/install-gke/#install-keycloak-k8s-specs","text":"1 2 3 4 5 6 kubectl apply -f joost/keycloak.yml sleep 120 kubectl -n jenkins exec -it keycloak-0 -- /bin/bash keycloak/bin/add-user-keycloak.sh -u somekindofuser -p X5qpLMnWKUx7 ps -ef | grep java kill -9 PID","title":"Install Keycloak - k8s-specs"},{"location":"kubernetes/distributions/install-gke/#follow-log","text":"1 k -n jenkins logs -f keycloak","title":"Follow log"},{"location":"kubernetes/distributions/install-gke/#jenkins-keycloak-config","text":"1 2 3 4 5 6 7 { realm : master , auth-server-url : http://35.204.112.229/auth , ssl-required : external , resource : jenkins , public-client : true }","title":"Jenkins Keycloak config"},{"location":"kubernetes/distributions/install-gke/#destroy-cluster","text":"1 2 3 4 gcloud container clusters \\ delete devops24 \\ --zone $ZONE \\ --quiet","title":"Destroy cluster"},{"location":"kubernetes/khw-gce/","text":"Kubernetes the Hard Way - GCE This assumes OSX and GCE. Goal The goal is to setup up HA Kubernetes cluster on GCE from it's most basic parts. That means we will install and configure the basic components ourselves, such as the API server and Kubelets. Setup As to limit the scope to doing the setup of the Kubernetes cluster ourselves, we will make it static. That means we will create and configure the network and compute resources to be fit for 3 Control Plane VM's and 3 worker VM's. We will not be able to recover a failing node or accomidate additional resources. Resources in GCE Public IP address, as front-end for the three API servers 3 VM's for the Control Plance 3 VM's as workers VPC Network Routes: from POD CIDR blocks to the host VM (for workers) Firewall configuration: allow health checks, dns, internal communication and connection to API server Kubernetes Resources Control Plane etcd : stores cluster state kube-api server : entry point for interacting with the cluster by exposing the api kube-scheduler : makes sure pods get scheduled kube-controller-manager : aggregate of required controllers Node Controller : Responsible for noticing and responding when nodes go down. Replication Controller : Responsible for maintaining the correct number of pods for every replication controller object in the system. Endpoints Controller : Populates the Endpoints object (that is, joins Services Pods). Service Account Token Controller : Create default accounts and API access tokens for new namespaces. Worker nodes kubelet : An agent that runs on each node in the cluster. It makes sure that containers are running in a pod. kube-proxy : kube-proxy enables the Kubernetes service abstraction by maintaining network rules on the host and performing connection forwarding A container runtime: this can be Docker , rkt or as in our case containerd Network https://blog.csnet.me/k8s-thw/part1/ https://github.com/kelseyhightower/kubernetes-the-hard-way We will be using the network components - with Weave-Net and CoreDNS - as described in the csnet blog. But we will use the CIDR blocks as stated in the Kelsey Hightower's Kubernetes the Hard Way ( KHW ). Kelsey's KHW Range Use 10.240.0.10/24 LAN (GCE VMS) 10.200.0.0/16 k8s Pod network 10.32.0.0/24 k8s Service network 10.32.0.1 k8s API server 10.32.0.10 k8s dns API Server: https://127.0.0.1:6443 service-cluster-ip-range=10.32.0.0/24 cluster-cidr=10.200.0.0/1 CSNETs Range Use 10.32.2.0/24 LAN (csnet.me) 10.16.0.0/16 k8s Pod network 10.10.0.0/22 k8s Service network 10.10.0.1 k8s API server 10.10.0.10 k8s dns API Server: https://10.32.2.97:6443 service-cluster-ip-range=10.10.0.0/22 cluster-cidr=10.16.0.0/16 Install tools On the machine doing the installation, we will need some tools installed. We will use the following tools: kubectl : for communicating with the API server cfssl : for creating the certificates and sign them helm : for installing additional tools later stern : for viewing logs of multiple pods at once (for example, all kube-dns pods) terraform : for managing our resources in GCE 1 2 3 4 5 brew install kubernetes-cli brew install cfssl brew install kubernetes-helm brew install stern brew install terraform Check versions 1 2 3 4 5 kubectl version -c -o yaml cfssl version helm version -c --short stern --version terraform version Terraform remote storage The help with problems of local storage and potential loss of data when local OS problems occur, we will use an S3 bucket as Terraform state storage. create s3 bucket configure Terraform to use this as remote state storage see how to this here read more about this, in Terraform's docs 1 2 3 export AWS_ACCESS_KEY_ID = anaccesskey export AWS_SECRET_ACCESS_KEY = asecretkey export AWS_DEFAULT_REGION = eu-central-1 1 2 3 4 5 6 7 8 terraform { backend s3 { bucket = euros-terraform-state key = terraform.tfstate region = eu-central-1 encrypt = true } } GKE Service Account Create a new GKE service account, and export it's json credentials file for use with Terraform. See GKE Tutorial page for how you can do this.","title":"Installer Preparation"},{"location":"kubernetes/khw-gce/#kubernetes-the-hard-way-gce","text":"This assumes OSX and GCE.","title":"Kubernetes the Hard Way - GCE"},{"location":"kubernetes/khw-gce/#goal","text":"The goal is to setup up HA Kubernetes cluster on GCE from it's most basic parts. That means we will install and configure the basic components ourselves, such as the API server and Kubelets.","title":"Goal"},{"location":"kubernetes/khw-gce/#setup","text":"As to limit the scope to doing the setup of the Kubernetes cluster ourselves, we will make it static. That means we will create and configure the network and compute resources to be fit for 3 Control Plane VM's and 3 worker VM's. We will not be able to recover a failing node or accomidate additional resources.","title":"Setup"},{"location":"kubernetes/khw-gce/#resources-in-gce","text":"Public IP address, as front-end for the three API servers 3 VM's for the Control Plance 3 VM's as workers VPC Network Routes: from POD CIDR blocks to the host VM (for workers) Firewall configuration: allow health checks, dns, internal communication and connection to API server","title":"Resources in GCE"},{"location":"kubernetes/khw-gce/#kubernetes-resources","text":"","title":"Kubernetes Resources"},{"location":"kubernetes/khw-gce/#control-plane","text":"etcd : stores cluster state kube-api server : entry point for interacting with the cluster by exposing the api kube-scheduler : makes sure pods get scheduled kube-controller-manager : aggregate of required controllers Node Controller : Responsible for noticing and responding when nodes go down. Replication Controller : Responsible for maintaining the correct number of pods for every replication controller object in the system. Endpoints Controller : Populates the Endpoints object (that is, joins Services Pods). Service Account Token Controller : Create default accounts and API access tokens for new namespaces.","title":"Control Plane"},{"location":"kubernetes/khw-gce/#worker-nodes","text":"kubelet : An agent that runs on each node in the cluster. It makes sure that containers are running in a pod. kube-proxy : kube-proxy enables the Kubernetes service abstraction by maintaining network rules on the host and performing connection forwarding A container runtime: this can be Docker , rkt or as in our case containerd","title":"Worker nodes"},{"location":"kubernetes/khw-gce/#network","text":"https://blog.csnet.me/k8s-thw/part1/ https://github.com/kelseyhightower/kubernetes-the-hard-way We will be using the network components - with Weave-Net and CoreDNS - as described in the csnet blog. But we will use the CIDR blocks as stated in the Kelsey Hightower's Kubernetes the Hard Way ( KHW ).","title":"Network"},{"location":"kubernetes/khw-gce/#kelseys-khw","text":"Range Use 10.240.0.10/24 LAN (GCE VMS) 10.200.0.0/16 k8s Pod network 10.32.0.0/24 k8s Service network 10.32.0.1 k8s API server 10.32.0.10 k8s dns API Server: https://127.0.0.1:6443 service-cluster-ip-range=10.32.0.0/24 cluster-cidr=10.200.0.0/1","title":"Kelsey's KHW"},{"location":"kubernetes/khw-gce/#csnets","text":"Range Use 10.32.2.0/24 LAN (csnet.me) 10.16.0.0/16 k8s Pod network 10.10.0.0/22 k8s Service network 10.10.0.1 k8s API server 10.10.0.10 k8s dns API Server: https://10.32.2.97:6443 service-cluster-ip-range=10.10.0.0/22 cluster-cidr=10.16.0.0/16","title":"CSNETs"},{"location":"kubernetes/khw-gce/#install-tools","text":"On the machine doing the installation, we will need some tools installed. We will use the following tools: kubectl : for communicating with the API server cfssl : for creating the certificates and sign them helm : for installing additional tools later stern : for viewing logs of multiple pods at once (for example, all kube-dns pods) terraform : for managing our resources in GCE 1 2 3 4 5 brew install kubernetes-cli brew install cfssl brew install kubernetes-helm brew install stern brew install terraform","title":"Install tools"},{"location":"kubernetes/khw-gce/#check-versions","text":"1 2 3 4 5 kubectl version -c -o yaml cfssl version helm version -c --short stern --version terraform version","title":"Check versions"},{"location":"kubernetes/khw-gce/#terraform-remote-storage","text":"The help with problems of local storage and potential loss of data when local OS problems occur, we will use an S3 bucket as Terraform state storage. create s3 bucket configure Terraform to use this as remote state storage see how to this here read more about this, in Terraform's docs 1 2 3 export AWS_ACCESS_KEY_ID = anaccesskey export AWS_SECRET_ACCESS_KEY = asecretkey export AWS_DEFAULT_REGION = eu-central-1 1 2 3 4 5 6 7 8 terraform { backend s3 { bucket = euros-terraform-state key = terraform.tfstate region = eu-central-1 encrypt = true } }","title":"Terraform remote storage"},{"location":"kubernetes/khw-gce/#gke-service-account","text":"Create a new GKE service account, and export it's json credentials file for use with Terraform. See GKE Tutorial page for how you can do this.","title":"GKE Service Account"},{"location":"kubernetes/khw-gce/certificates/","text":"Certificates Note Before we can continue here, we need to have our nodes up and running with their external ip addresses and our fixed public ip address. This is because some certificates require these external ip addresses! 1 2 gcloud compute instances list gcloud compute addresses list --filter = name=( kubernetes-the-hard-way ) We need to create a whole lot of certificates, listed below, with the help of cfssl . A tool from CDN provider CloudFlare. Required certificates CA (or Certificate Authority): will be the root certificate of our trust chain result: ca.pem ca-key.pem Admin : the admin of our cluster (you!) result: admin-key.pem admin.pem Kubelet : the certificates of the kubelet processes on the worker nodes result: 1 2 3 4 5 worker-0.pem worker-1-key.pem worker-1.pem worker-2-key.pem worker-2.pem Controller Manager result: kube-controller-manager-key.pem kube-controller-manager.pem Scheduler result: kube-scheduler-key.pem kube-scheduler.pem API Server result kubernetes-key.pem kubernetes.pem Service Account : ??? result: service-account-key.pem service-account.pem Certificate example Because we will use the cfssl tool from CloudFlare, we will define our certificate signing request (CSR's) in json. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { CN : service-accounts , key : { algo : rsa , size : 2048 }, names : [ { C : NL , L : Utrecht , O : Kubernetes , OU : Kubernetes The Hard Way , ST : Utrecht } ] } Install scripts Make sure you're in k8s-the-hard-way/scripts 1 ./certs.sh","title":"Prepare Certificates"},{"location":"kubernetes/khw-gce/certificates/#certificates","text":"Note Before we can continue here, we need to have our nodes up and running with their external ip addresses and our fixed public ip address. This is because some certificates require these external ip addresses! 1 2 gcloud compute instances list gcloud compute addresses list --filter = name=( kubernetes-the-hard-way ) We need to create a whole lot of certificates, listed below, with the help of cfssl . A tool from CDN provider CloudFlare.","title":"Certificates"},{"location":"kubernetes/khw-gce/certificates/#required-certificates","text":"CA (or Certificate Authority): will be the root certificate of our trust chain result: ca.pem ca-key.pem Admin : the admin of our cluster (you!) result: admin-key.pem admin.pem Kubelet : the certificates of the kubelet processes on the worker nodes result: 1 2 3 4 5 worker-0.pem worker-1-key.pem worker-1.pem worker-2-key.pem worker-2.pem Controller Manager result: kube-controller-manager-key.pem kube-controller-manager.pem Scheduler result: kube-scheduler-key.pem kube-scheduler.pem API Server result kubernetes-key.pem kubernetes.pem Service Account : ??? result: service-account-key.pem service-account.pem","title":"Required certificates"},{"location":"kubernetes/khw-gce/certificates/#certificate-example","text":"Because we will use the cfssl tool from CloudFlare, we will define our certificate signing request (CSR's) in json. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { CN : service-accounts , key : { algo : rsa , size : 2048 }, names : [ { C : NL , L : Utrecht , O : Kubernetes , OU : Kubernetes The Hard Way , ST : Utrecht } ] }","title":"Certificate example"},{"location":"kubernetes/khw-gce/certificates/#install-scripts","text":"Make sure you're in k8s-the-hard-way/scripts 1 ./certs.sh","title":"Install scripts"},{"location":"kubernetes/khw-gce/controller/","text":"Controller Config We have to configure the following: move certificates to the correct location move encryption configuration to /var/lib/kubernetes download and install binaries kubectl kube-apiserver kube-scheduler kube-controller-manager configure API server systemd service configure Controller Manager systemd service configure Scheduler systemd service kubernetes configuration yaml kind: KubeSchedulerConfiguration create nginx reverse proxy to enable GCE's health checks to reach each API Server instance configure RBAC configuration in the API server via ClusterRole and ClusterRoleBinding Install We have an installer script, controller-local.sh , which should be executed on each controller VM. To do so, use the controller.sh script to upload this file to the VM's. 1 ./controller.sh","title":"Controller Config"},{"location":"kubernetes/khw-gce/controller/#controller-config","text":"We have to configure the following: move certificates to the correct location move encryption configuration to /var/lib/kubernetes download and install binaries kubectl kube-apiserver kube-scheduler kube-controller-manager configure API server systemd service configure Controller Manager systemd service configure Scheduler systemd service kubernetes configuration yaml kind: KubeSchedulerConfiguration create nginx reverse proxy to enable GCE's health checks to reach each API Server instance configure RBAC configuration in the API server via ClusterRole and ClusterRoleBinding","title":"Controller Config"},{"location":"kubernetes/khw-gce/controller/#install","text":"We have an installer script, controller-local.sh , which should be executed on each controller VM. To do so, use the controller.sh script to upload this file to the VM's. 1 ./controller.sh","title":"Install"},{"location":"kubernetes/khw-gce/debug/","text":"Debug Kubernetes components not healthy Check for healthy status On a control plane node, check etcd . 1 2 3 4 5 sudo ETCDCTL_API = 3 etcdctl member list \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem 1 2 3 3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379 f98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379 ffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379 On a control plan node, check control plane components. 1 kubectl get componentstatuses --kubeconfig admin.kubeconfig Should look like this: 1 2 3 4 5 6 NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy { health : true } etcd-0 Healthy { health : true } etcd-1 Healthy { health : true } On a control plane node, check API server status (via nginx reverse proxy). 1 curl -H Host: kubernetes.default.svc.cluster.local -i http://127.0.0.1/healthz 1 2 3 4 5 6 7 8 HTTP/1.1 200 OK Server: nginx/1.14.0 ( Ubuntu ) Date: Mon, 14 May 2018 13 :45:39 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 2 Connection: keep-alive ok On an external system, you can check if the API server is working and reachable via routing. 1 curl --cacert ca.pem https:// ${ KUBERNETES_PUBLIC_ADDRESS } :6443/version Assuming that GCE is used. 1 2 3 KUBERNETES_PUBLIC_ADDRESS = $( gcloud compute addresses describe kubernetes-the-hard-way \\ --region $( gcloud config get-value compute/region ) \\ --format value(address) ) Check for errors 1 journalctl Or for specific components. 1 journalctl -u kube-scheduler Weave-Net pods Blocked Sometimes when installing weave-net as the CNI plugin, the pods are blocked. 1 2 3 4 NAME READY STATUS RESTARTS AGE weave-net-fwvsr 0 /2 Blocked 0 3m weave-net-v9z9n 0 /2 Blocked 0 3m weave-net-zfghq 0 /2 Blocked 0 3m Usually this means something went wrong with the CNI configuration. Ideally, Weave-Net will generate this when installed, but sometimes this doesn't happen. This is easily found when checking the journalctl on the worker nodes ( journalctl -u kubelet ). There are three things to be done before installing weave-net again. Ensure ip4 forwarding is enabled 1 2 sysctl net.ipv4.ip_forward = 1 sysctl -p /etc/sysctl.conf See Kubernetes Docs for GCE routing or Michael Champagne 's blog on KHW. Ensure all weave-net resources are gone I've noticed that when this problem occurs, deleting the weave-net resources with kubectl delete -f weaveNet resource leaves the pods. The pods are terminated (they never started) but are not removed. To remove them, use the line below, as explained on stackoverflow . 1 kubectl delete pod NAME --grace-period = 0 --force Restart Kubelet I'm not sure if this is 100% required, but I've had better luck with restarting the kubelet before reinstalling weave-net. So, login to each worker node, gcloud compute ssh worker-? and issue the following commands. 1 2 sudo systemctl daemon-reload sudo systemctl restart kubelet DNS on GCE not working It seemed something has changed in GCE after Kelsey Hightower's Kubernetes The Hardway was written/updated. This means that if you follow through the documentation, you will run into this: 1 2 3 4 kubectl exec -ti $POD_NAME -- nslookup kubernetes ;; connection timed out ; no servers could be reached command terminated with exit code 1 The cure seems to be to add additional resolve.conf file configuration to the kubelet's systemd service definition. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cat EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description = Kubernetes Kubelet Documentation = https://github.com/kubernetes/kubernetes After = containerd.service Requires = containerd.service [Service] ExecStart = /usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --container-runtime=remote \\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\ --resolv-conf=/run/systemd/resolve/resolv.conf \\ --image-pull-progress-deadline=2m \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --network-plugin=cni \\ --register-node=true \\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF In addition, one should also use at least busybox 1.28 to do the dns check. For more information, read this issue .","title":"Debug"},{"location":"kubernetes/khw-gce/debug/#debug","text":"","title":"Debug"},{"location":"kubernetes/khw-gce/debug/#kubernetes-components-not-healthy","text":"","title":"Kubernetes components not healthy"},{"location":"kubernetes/khw-gce/debug/#check-for-healthy-status","text":"On a control plane node, check etcd . 1 2 3 4 5 sudo ETCDCTL_API = 3 etcdctl member list \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem 1 2 3 3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379 f98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379 ffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379 On a control plan node, check control plane components. 1 kubectl get componentstatuses --kubeconfig admin.kubeconfig Should look like this: 1 2 3 4 5 6 NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy { health : true } etcd-0 Healthy { health : true } etcd-1 Healthy { health : true } On a control plane node, check API server status (via nginx reverse proxy). 1 curl -H Host: kubernetes.default.svc.cluster.local -i http://127.0.0.1/healthz 1 2 3 4 5 6 7 8 HTTP/1.1 200 OK Server: nginx/1.14.0 ( Ubuntu ) Date: Mon, 14 May 2018 13 :45:39 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 2 Connection: keep-alive ok On an external system, you can check if the API server is working and reachable via routing. 1 curl --cacert ca.pem https:// ${ KUBERNETES_PUBLIC_ADDRESS } :6443/version Assuming that GCE is used. 1 2 3 KUBERNETES_PUBLIC_ADDRESS = $( gcloud compute addresses describe kubernetes-the-hard-way \\ --region $( gcloud config get-value compute/region ) \\ --format value(address) )","title":"Check for healthy status"},{"location":"kubernetes/khw-gce/debug/#check-for-errors","text":"1 journalctl Or for specific components. 1 journalctl -u kube-scheduler","title":"Check for errors"},{"location":"kubernetes/khw-gce/debug/#weave-net-pods-blocked","text":"Sometimes when installing weave-net as the CNI plugin, the pods are blocked. 1 2 3 4 NAME READY STATUS RESTARTS AGE weave-net-fwvsr 0 /2 Blocked 0 3m weave-net-v9z9n 0 /2 Blocked 0 3m weave-net-zfghq 0 /2 Blocked 0 3m Usually this means something went wrong with the CNI configuration. Ideally, Weave-Net will generate this when installed, but sometimes this doesn't happen. This is easily found when checking the journalctl on the worker nodes ( journalctl -u kubelet ). There are three things to be done before installing weave-net again.","title":"Weave-Net pods Blocked"},{"location":"kubernetes/khw-gce/debug/#ensure-ip4-forwarding-is-enabled","text":"1 2 sysctl net.ipv4.ip_forward = 1 sysctl -p /etc/sysctl.conf See Kubernetes Docs for GCE routing or Michael Champagne 's blog on KHW.","title":"Ensure ip4 forwarding is enabled"},{"location":"kubernetes/khw-gce/debug/#ensure-all-weave-net-resources-are-gone","text":"I've noticed that when this problem occurs, deleting the weave-net resources with kubectl delete -f weaveNet resource leaves the pods. The pods are terminated (they never started) but are not removed. To remove them, use the line below, as explained on stackoverflow . 1 kubectl delete pod NAME --grace-period = 0 --force","title":"Ensure all weave-net resources are gone"},{"location":"kubernetes/khw-gce/debug/#restart-kubelet","text":"I'm not sure if this is 100% required, but I've had better luck with restarting the kubelet before reinstalling weave-net. So, login to each worker node, gcloud compute ssh worker-? and issue the following commands. 1 2 sudo systemctl daemon-reload sudo systemctl restart kubelet","title":"Restart Kubelet"},{"location":"kubernetes/khw-gce/debug/#dns-on-gce-not-working","text":"It seemed something has changed in GCE after Kelsey Hightower's Kubernetes The Hardway was written/updated. This means that if you follow through the documentation, you will run into this: 1 2 3 4 kubectl exec -ti $POD_NAME -- nslookup kubernetes ;; connection timed out ; no servers could be reached command terminated with exit code 1 The cure seems to be to add additional resolve.conf file configuration to the kubelet's systemd service definition. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cat EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description = Kubernetes Kubelet Documentation = https://github.com/kubernetes/kubernetes After = containerd.service Requires = containerd.service [Service] ExecStart = /usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --container-runtime=remote \\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\ --resolv-conf=/run/systemd/resolve/resolv.conf \\ --image-pull-progress-deadline=2m \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --network-plugin=cni \\ --register-node=true \\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF In addition, one should also use at least busybox 1.28 to do the dns check. For more information, read this issue .","title":"DNS on GCE not working"},{"location":"kubernetes/khw-gce/encryption/","text":"Encryption Kubernetes stores a variety of data including cluster state, application configurations, and secrets. Kubernetes supports the ability to encrypt cluster data at rest. In order to use this ability to encrypt data at rest, each member of the control plane has to know the encryption key. So we will have to create one. Encryption configuration We have to create a encryption key first. For the sake of embedding it into a yaml file, we will have to encode it to base64 . 1 ENCRYPTION_KEY = $( head -c 32 /dev/urandom | base64 ) 1 Install scripts Make sure you're in k8s-the-hard-way/scripts 1 ./encryption.sh","title":"Encryption configuration"},{"location":"kubernetes/khw-gce/encryption/#encryption","text":"Kubernetes stores a variety of data including cluster state, application configurations, and secrets. Kubernetes supports the ability to encrypt cluster data at rest. In order to use this ability to encrypt data at rest, each member of the control plane has to know the encryption key. So we will have to create one.","title":"Encryption"},{"location":"kubernetes/khw-gce/encryption/#encryption-configuration","text":"We have to create a encryption key first. For the sake of embedding it into a yaml file, we will have to encode it to base64 . 1 ENCRYPTION_KEY = $( head -c 32 /dev/urandom | base64 ) 1","title":"Encryption configuration"},{"location":"kubernetes/khw-gce/encryption/#install-scripts","text":"Make sure you're in k8s-the-hard-way/scripts 1 ./encryption.sh","title":"Install scripts"},{"location":"kubernetes/khw-gce/etcd/","text":"ETCD Kubernetes components are stateless and store cluster state in etcd. In this lab you will bootstrap a three node etcd cluster and configure it for high availability and secure remote access. The bare minimum is to have a single etcd instance running. But for production purposes it is best to run etcd in HA mode. This means we need to have three instances running that know eachother. Again, this is not a production ready setup, as the static nature prevents automatic recovery if a node fails. Steps to take download install etcd binary prepare required certificates create systemd service definition reload systemd configuration, enable start the service Install script Make sure that the local install script is on every server, you can use the etcd.sh script for this. Then, make sure you're connect to all three controller VM's at the same time, for example via tmux or iterm. For iterm: use ctrl + shift + d to open three horizontal windows use ctrl + shift + i to write output to all three windows at once login to each controller gcloud compute ssh controller-? ./etcd-local.sh Verification 1 2 3 4 5 sudo ETCDCTL_API = 3 etcdctl member list \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem Expected Output 1 2 3 3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379 f98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379 ffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379","title":"ETCD configuration"},{"location":"kubernetes/khw-gce/etcd/#etcd","text":"Kubernetes components are stateless and store cluster state in etcd. In this lab you will bootstrap a three node etcd cluster and configure it for high availability and secure remote access. The bare minimum is to have a single etcd instance running. But for production purposes it is best to run etcd in HA mode. This means we need to have three instances running that know eachother. Again, this is not a production ready setup, as the static nature prevents automatic recovery if a node fails.","title":"ETCD"},{"location":"kubernetes/khw-gce/etcd/#steps-to-take","text":"download install etcd binary prepare required certificates create systemd service definition reload systemd configuration, enable start the service","title":"Steps to take"},{"location":"kubernetes/khw-gce/etcd/#install-script","text":"Make sure that the local install script is on every server, you can use the etcd.sh script for this. Then, make sure you're connect to all three controller VM's at the same time, for example via tmux or iterm. For iterm: use ctrl + shift + d to open three horizontal windows use ctrl + shift + i to write output to all three windows at once login to each controller gcloud compute ssh controller-? ./etcd-local.sh","title":"Install script"},{"location":"kubernetes/khw-gce/etcd/#verification","text":"1 2 3 4 5 sudo ETCDCTL_API = 3 etcdctl member list \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem","title":"Verification"},{"location":"kubernetes/khw-gce/etcd/#expected-output","text":"1 2 3 3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379 f98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379 ffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379","title":"Expected Output"},{"location":"kubernetes/khw-gce/kubeconfigs/","text":"Kubeconfigs Now that we have certificates we have to make sure we have configurations that the Kubernetes parts can actually use - certificates themselves are not enough. This is where we will use kubernetes configuration files, or kubeconfigs . We will have to create the following kubeconfigs : controller manager kubelet kube-proxy kube-scheduler admin user Create Test kubeconfig file Here's an example script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate = kube-controller-manager.pem \\ --client-key = kube-controller-manager-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-controller-manager \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig = kube-controller-manager.kubeconfig The steps we execute in order are the following: create a kubeconfig entry for our kubernetes-the-hard-way cluster and export this into a .kubeconfig file add credentials to this config file, in the form of our kubernetes component's certificate set the default config of this config file to namespace default and user to the component we're configuring test the configuration file by using it Install scripts Make sure you're in k8s-the-hard-way/scripts 1 ./kube-configs.sh","title":"Prepare Kubeconfigs"},{"location":"kubernetes/khw-gce/kubeconfigs/#kubeconfigs","text":"Now that we have certificates we have to make sure we have configurations that the Kubernetes parts can actually use - certificates themselves are not enough. This is where we will use kubernetes configuration files, or kubeconfigs . We will have to create the following kubeconfigs : controller manager kubelet kube-proxy kube-scheduler admin user","title":"Kubeconfigs"},{"location":"kubernetes/khw-gce/kubeconfigs/#create-test-kubeconfig-file","text":"Here's an example script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate = kube-controller-manager.pem \\ --client-key = kube-controller-manager-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-controller-manager \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig = kube-controller-manager.kubeconfig The steps we execute in order are the following: create a kubeconfig entry for our kubernetes-the-hard-way cluster and export this into a .kubeconfig file add credentials to this config file, in the form of our kubernetes component's certificate set the default config of this config file to namespace default and user to the component we're configuring test the configuration file by using it","title":"Create &amp; Test kubeconfig file"},{"location":"kubernetes/khw-gce/kubeconfigs/#install-scripts","text":"Make sure you're in k8s-the-hard-way/scripts 1 ./kube-configs.sh","title":"Install scripts"},{"location":"kubernetes/khw-gce/network/","text":"Networking First, configure external access so we can run kubectl commands from our own machine. Confirm the you can now call the following: 1 kubectl get nodes -o wide Configure WeaveNet 1 kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n ) env.IPALLOC_RANGE=10.200.0.0/16 Confirm WeaveNet works 1 kubectl get pod --namespace = kube-system -l name = weave-net It should look like this: 1 2 3 4 NAME READY STATUS RESTARTS AGE weave-net-fwvsr 2 /2 Running 1 4h weave-net-v9z9n 2 /2 Running 1 4h weave-net-zfghq 2 /2 Running 1 4h Configure CoreDNS Before installing CoreDNS , please confirm networking is in order. 1 kubectl get nodes -o wide Warning If nodes are not Ready , something is wrong and needs to be fixed before you continue. 1 kubectl apply -f ../configs/core-dns-config.yaml Confirm CoreDNS pods 1 kubectl get pod --all-namespaces -l k8s-app = coredns -o wide Confirm DNS works 1 kubectl run busybox --image = busybox:1.28 --command -- sleep 3600 1 POD_NAME = $( kubectl get pods -l run = busybox -o jsonpath = {.items[0].metadata.name} ) 1 kubectl exec -ti $POD_NAME -- nslookup kubernetes Note It should look like this: 1 2 3 4 5 Server: 10 .10.0.10 Address 1 : 10 .10.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1 : 10 .10.0.1 kubernetes.default.svc.cluster.local","title":"Network config"},{"location":"kubernetes/khw-gce/network/#networking","text":"First, configure external access so we can run kubectl commands from our own machine. Confirm the you can now call the following: 1 kubectl get nodes -o wide","title":"Networking"},{"location":"kubernetes/khw-gce/network/#configure-weavenet","text":"1 kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d \\n ) env.IPALLOC_RANGE=10.200.0.0/16","title":"Configure WeaveNet"},{"location":"kubernetes/khw-gce/network/#confirm-weavenet-works","text":"1 kubectl get pod --namespace = kube-system -l name = weave-net It should look like this: 1 2 3 4 NAME READY STATUS RESTARTS AGE weave-net-fwvsr 2 /2 Running 1 4h weave-net-v9z9n 2 /2 Running 1 4h weave-net-zfghq 2 /2 Running 1 4h","title":"Confirm WeaveNet works"},{"location":"kubernetes/khw-gce/network/#configure-coredns","text":"Before installing CoreDNS , please confirm networking is in order. 1 kubectl get nodes -o wide Warning If nodes are not Ready , something is wrong and needs to be fixed before you continue. 1 kubectl apply -f ../configs/core-dns-config.yaml","title":"Configure CoreDNS"},{"location":"kubernetes/khw-gce/network/#confirm-coredns-pods","text":"1 kubectl get pod --all-namespaces -l k8s-app = coredns -o wide","title":"Confirm CoreDNS pods"},{"location":"kubernetes/khw-gce/network/#confirm-dns-works","text":"1 kubectl run busybox --image = busybox:1.28 --command -- sleep 3600 1 POD_NAME = $( kubectl get pods -l run = busybox -o jsonpath = {.items[0].metadata.name} ) 1 kubectl exec -ti $POD_NAME -- nslookup kubernetes Note It should look like this: 1 2 3 4 5 Server: 10 .10.0.10 Address 1 : 10 .10.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1 : 10 .10.0.1 kubernetes.default.svc.cluster.local","title":"Confirm DNS works"},{"location":"kubernetes/khw-gce/remote-access/","text":"Remote Access 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 KUBERNETES_PUBLIC_ADDRESS = $( gcloud compute addresses describe kubernetes-the-hard-way \\ --region $( gcloud config get-value compute/region ) \\ --format value(address) ) echo KUBERNETES_PUBLIC_ADDRESS= ${ KUBERNETES_PUBLIC_ADDRESS } kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https:// ${ KUBERNETES_PUBLIC_ADDRESS } :6443 kubectl config set-credentials admin \\ --client-certificate = admin.pem \\ --client-key = admin-key.pem kubectl config set-context kubernetes-the-hard-way \\ --cluster = kubernetes-the-hard-way \\ --user = admin kubectl config use-context kubernetes-the-hard-way Confirm 1 kubectl get nodes -o wide","title":"Remote access"},{"location":"kubernetes/khw-gce/remote-access/#remote-access","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 KUBERNETES_PUBLIC_ADDRESS = $( gcloud compute addresses describe kubernetes-the-hard-way \\ --region $( gcloud config get-value compute/region ) \\ --format value(address) ) echo KUBERNETES_PUBLIC_ADDRESS= ${ KUBERNETES_PUBLIC_ADDRESS } kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https:// ${ KUBERNETES_PUBLIC_ADDRESS } :6443 kubectl config set-credentials admin \\ --client-certificate = admin.pem \\ --client-key = admin-key.pem kubectl config set-context kubernetes-the-hard-way \\ --cluster = kubernetes-the-hard-way \\ --user = admin kubectl config use-context kubernetes-the-hard-way","title":"Remote Access"},{"location":"kubernetes/khw-gce/remote-access/#confirm","text":"1 kubectl get nodes -o wide","title":"Confirm"},{"location":"kubernetes/khw-gce/terraform-compute/","text":"Compute resources Create network VPC with Firewall rules 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 provider google { credentials = ${file( ${var.credentials_file_path} )} project = ${var.project_name} region = ${var.region} } resource google_compute_network khw { name = kubernetes-the-hard-way auto_create_subnetworks = false } resource google_compute_subnetwork khw-kubernetes { name = kubernetes ip_cidr_range = 10.240.0.0/24 region = ${var.region} network = ${google_compute_network.khw.self_link} } resource google_compute_firewall khw-allow-internal { name = kubernetes-the-hard-way-allow-internal network = ${google_compute_network.khw.name} source_ranges = [ 10.240.0.0/24 , 10.200.0.0/16 ] allow { protocol = tcp } allow { protocol = udp } allow { protocol = icmp } } resource google_compute_firewall khw-allow-external { name = kubernetes-the-hard-way-allow-external network = ${google_compute_network.khw.name} allow { protocol = icmp } allow { protocol = tcp ports = [ 22 , 6443 ] } source_ranges = [ 0.0.0.0/0 ] } resource google_compute_firewall khw-allow-dns { name = kubernetes-the-hard-way-allow-dns network = ${google_compute_network.khw.name} source_ranges = [ 0.0.0.0 ] allow { protocol = tcp ports = [ 53 , 443 ] } allow { protocol = udp ports = [ 53 ] } } resource google_compute_firewall khw-allow-health-check { name = kubernetes-the-hard-way-allow-health-check network = ${google_compute_network.khw.name} allow { protocol = tcp } source_ranges = [ 209.85.152.0/22 , 209.85.204.0/22 , 35.191.0.0/16 ] } Confirm network 1 gcloud compute firewall-rules list --filter = network:kubernetes-the-hard-way Should look like: 1 2 3 NAME NETWORK DIRECTION PRIORITY ALLOW DENY kubernetes-the-hard-way-allow-external kubernetes-the-hard-way INGRESS 1000 icmp,tcp:22,tcp:6443 kubernetes-the-hard-way-allow-internal kubernetes-the-hard-way INGRESS 1000 icmp,udp,tcp Public IP 1 2 3 resource google_compute_address khw-lb-public-ip { name = kubernetes-the-hard-way } Confirm: 1 gcloud compute addresses list --filter = name=( kubernetes-the-hard-way ) Output: 1 2 NAME REGION ADDRESS STATUS kubernetes-the-hard-way europe-west4 35 .204.134.219 RESERVED VM Definitions with Terraform modules We're going to need to create 6 VM's. 3 Controller nodes and 3 worker nodes. Within each of the two categories, all the three VM's will be the same. So it would be a waste to define them more than once. This can be achieved via Terraform's Module system (read more here . Define a module For the sake of naming convention, we'll put all of our modules in a modules subfolder. We'll start with the controller module, but you can do the same for the worker. 1 mkdir -p modules/controller 1 2 3 4 ls -lath drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 . drwxr-xr-x 20 joostvdg staff 640B Aug 22 14 :47 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 modules 1 2 3 4 ls -lath modules drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 . drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :03 controller Inside modules/controller we create two files, main.tf and variables.tf . We have to create an additional variables file, as the module cannot use the main folder's variables. Then, in our main folder we'll create a tf file for using these modules, called nodes.tf . As stated above, we pass along any variable from our main variables.tf to the module. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 module controller { source = modules/controller machine_type = ${var.machine_type_controllers} num = ${var.num_controllers} zone = ${var.region_default_zone} subnet = ${var.subnet_name} } module worker { source = modules/worker machine_type = ${var.machine_type_workers} num = ${var.num_workers} zone = ${var.region_default_zone} network = ${google_compute_network.khw.name} subnet = ${var.subnet_name} } Controller config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 data google_compute_image khw-ubuntu { family = ubuntu-1804-lts project = ubuntu-os-cloud } resource google_compute_instance khw-controller { count = ${var.num} name = controller-${count.index} machine_type = ${var.machine_type} zone = ${var.zone} can_ip_forward = true tags = [ kubernetes-the-hard-way , controller ] boot_disk { initialize_params { image = ${data.google_compute_image.khw-ubuntu.self_link} size = 200 // in GB } } network_interface { subnetwork = ${var.subnet} address = 10.240.0.1${count.index} access_config { // Ephemeral External IP } } # compute-rw,storage-ro,service-management,service-control,logging-write,monitoring service_account { scopes = [ compute-rw , storage-ro , service-management , service-control , logging-write , monitoring , ] } } Variables 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 variable num { description = The number of controller VMs } variable machine_type { description = The type of VM for controllers } variable zone { description = The zone to create the controllers in } variable subnet { description = The subnet to create the nic in } Worker config Extra config for the worker are the routes, to aid the pods going out of the node. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 data google_compute_image khw-ubuntu { family = ubuntu-1804-lts project = ubuntu-os-cloud } resource google_compute_instance khw-worker { count = ${var.num} name = worker-${count.index} machine_type = ${var.machine_type} zone = ${var.zone} can_ip_forward = true tags = [ kubernetes-the-hard-way , worker ] metadata { pod-cidr = 10.200.${count.index}.0/24 } boot_disk { initialize_params { image = ${data.google_compute_image.khw-ubuntu.self_link} size = 200 // in GB } } network_interface { subnetwork = ${var.subnet} address = 10.240.0.2${count.index} access_config { // Ephemeral External IP } } service_account { scopes = [ compute-rw , storage-ro , service-management , service-control , logging-write , monitoring , ] } } resource google_compute_route khw-worker-route { count = ${var.num} name = kubernetes-route-10-200-${count.index}-0-24 network = ${var.network} next_hop_ip = 10.240.0.2${count.index} dest_range = 10.200.${count.index}.0/24 } Variables 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 variable num { description = The number of controller VMs } variable machine_type { description = The type of VM for controllers } variable zone { description = The zone to create the controllers in } variable network { description = The network to use for routes } variable subnet { description = The subnet to create the nic in } Health check Because we will have three controllers, we have to make sure that GKE forwards Kubernetes API requests to each of them via our public IP address. We do this via a http health check, wich involves a forwarding rule and a target pool. Target pool being the group of controller VM's for which the forwarding rule is active. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 resource google_compute_target_pool khw-hc-target-pool { name = instance-pool # TODO: fixed set for now, maybe we can make this dynamic some day instances = [ ${var.region_default_zone}/controller-0 , ${var.region_default_zone}/controller-1 , ${var.region_default_zone}/controller-2 , ] health_checks = [ ${google_compute_http_health_check.khw-health-check.name} , ] } resource google_compute_http_health_check khw-health-check { name = kubernetes request_path = /healthz description = The health check for Kubernetes API server host = ${var.kubernetes-cluster-dns} } resource google_compute_forwarding_rule khw-hc-forward { name = kubernetes-forwarding-rule target = ${google_compute_target_pool.khw-hc-target-pool.self_link} region = ${var.region} port_range = 6443 ip_address = ${google_compute_address.khw-lb-public-ip.self_link} } Apply Terraform state In the end, our configuration should consist out of several .tf files and look something like this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ls -lath drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 . drwxr-xr-x 20 joostvdg staff 640B Aug 22 14 :47 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 modules -rw-r--r-- 1 joostvdg staff 1 .5K Aug 26 12 :50 variables.tf -rw-r--r-- 1 joostvdg staff 1 .3K Aug 17 16 :03 firewall.tf -rw-r--r-- 1 joostvdg staff 4 .4K Aug 17 12 :06 worker-config.md -rw-r--r-- 1 joostvdg staff 1 .6K Aug 17 09 :35 healthcheck.tf -rw-r--r-- 1 joostvdg staff 517B Aug 16 17 :09 nodes.tf -rw-r--r-- 1 joostvdg staff 92B Aug 16 13 :52 publicip.tf -rw-r--r-- 1 joostvdg staff 365B Aug 7 22 :07 vpc.tf -rw-r--r-- 1 joostvdg staff 189B Aug 7 16 :51 base.tf drwxr-xr-x 5 joostvdg staff 160B Aug 7 21 :52 .terraform -rw-r--r-- 1 joostvdg staff 0B Aug 7 18 :28 terraform.tfstate We're now going to plan and then apply our Terraform configuration to create the resources in GCE. 1 terraform plan 1 terraform apply","title":"Create GCE resources"},{"location":"kubernetes/khw-gce/terraform-compute/#compute-resources","text":"","title":"Compute resources"},{"location":"kubernetes/khw-gce/terraform-compute/#create-network","text":"","title":"Create network"},{"location":"kubernetes/khw-gce/terraform-compute/#vpc-with-firewall-rules","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 provider google { credentials = ${file( ${var.credentials_file_path} )} project = ${var.project_name} region = ${var.region} } resource google_compute_network khw { name = kubernetes-the-hard-way auto_create_subnetworks = false } resource google_compute_subnetwork khw-kubernetes { name = kubernetes ip_cidr_range = 10.240.0.0/24 region = ${var.region} network = ${google_compute_network.khw.self_link} } resource google_compute_firewall khw-allow-internal { name = kubernetes-the-hard-way-allow-internal network = ${google_compute_network.khw.name} source_ranges = [ 10.240.0.0/24 , 10.200.0.0/16 ] allow { protocol = tcp } allow { protocol = udp } allow { protocol = icmp } } resource google_compute_firewall khw-allow-external { name = kubernetes-the-hard-way-allow-external network = ${google_compute_network.khw.name} allow { protocol = icmp } allow { protocol = tcp ports = [ 22 , 6443 ] } source_ranges = [ 0.0.0.0/0 ] } resource google_compute_firewall khw-allow-dns { name = kubernetes-the-hard-way-allow-dns network = ${google_compute_network.khw.name} source_ranges = [ 0.0.0.0 ] allow { protocol = tcp ports = [ 53 , 443 ] } allow { protocol = udp ports = [ 53 ] } } resource google_compute_firewall khw-allow-health-check { name = kubernetes-the-hard-way-allow-health-check network = ${google_compute_network.khw.name} allow { protocol = tcp } source_ranges = [ 209.85.152.0/22 , 209.85.204.0/22 , 35.191.0.0/16 ] }","title":"VPC with Firewall rules"},{"location":"kubernetes/khw-gce/terraform-compute/#confirm-network","text":"1 gcloud compute firewall-rules list --filter = network:kubernetes-the-hard-way Should look like: 1 2 3 NAME NETWORK DIRECTION PRIORITY ALLOW DENY kubernetes-the-hard-way-allow-external kubernetes-the-hard-way INGRESS 1000 icmp,tcp:22,tcp:6443 kubernetes-the-hard-way-allow-internal kubernetes-the-hard-way INGRESS 1000 icmp,udp,tcp","title":"Confirm network"},{"location":"kubernetes/khw-gce/terraform-compute/#public-ip","text":"1 2 3 resource google_compute_address khw-lb-public-ip { name = kubernetes-the-hard-way } Confirm: 1 gcloud compute addresses list --filter = name=( kubernetes-the-hard-way ) Output: 1 2 NAME REGION ADDRESS STATUS kubernetes-the-hard-way europe-west4 35 .204.134.219 RESERVED","title":"Public IP"},{"location":"kubernetes/khw-gce/terraform-compute/#vm-definitions-with-terraform-modules","text":"We're going to need to create 6 VM's. 3 Controller nodes and 3 worker nodes. Within each of the two categories, all the three VM's will be the same. So it would be a waste to define them more than once. This can be achieved via Terraform's Module system (read more here .","title":"VM Definitions with Terraform modules"},{"location":"kubernetes/khw-gce/terraform-compute/#define-a-module","text":"For the sake of naming convention, we'll put all of our modules in a modules subfolder. We'll start with the controller module, but you can do the same for the worker. 1 mkdir -p modules/controller 1 2 3 4 ls -lath drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 . drwxr-xr-x 20 joostvdg staff 640B Aug 22 14 :47 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 modules 1 2 3 4 ls -lath modules drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 . drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :03 controller Inside modules/controller we create two files, main.tf and variables.tf . We have to create an additional variables file, as the module cannot use the main folder's variables. Then, in our main folder we'll create a tf file for using these modules, called nodes.tf . As stated above, we pass along any variable from our main variables.tf to the module. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 module controller { source = modules/controller machine_type = ${var.machine_type_controllers} num = ${var.num_controllers} zone = ${var.region_default_zone} subnet = ${var.subnet_name} } module worker { source = modules/worker machine_type = ${var.machine_type_workers} num = ${var.num_workers} zone = ${var.region_default_zone} network = ${google_compute_network.khw.name} subnet = ${var.subnet_name} }","title":"Define a module"},{"location":"kubernetes/khw-gce/terraform-compute/#controller-config","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 data google_compute_image khw-ubuntu { family = ubuntu-1804-lts project = ubuntu-os-cloud } resource google_compute_instance khw-controller { count = ${var.num} name = controller-${count.index} machine_type = ${var.machine_type} zone = ${var.zone} can_ip_forward = true tags = [ kubernetes-the-hard-way , controller ] boot_disk { initialize_params { image = ${data.google_compute_image.khw-ubuntu.self_link} size = 200 // in GB } } network_interface { subnetwork = ${var.subnet} address = 10.240.0.1${count.index} access_config { // Ephemeral External IP } } # compute-rw,storage-ro,service-management,service-control,logging-write,monitoring service_account { scopes = [ compute-rw , storage-ro , service-management , service-control , logging-write , monitoring , ] } }","title":"Controller config"},{"location":"kubernetes/khw-gce/terraform-compute/#variables","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 variable num { description = The number of controller VMs } variable machine_type { description = The type of VM for controllers } variable zone { description = The zone to create the controllers in } variable subnet { description = The subnet to create the nic in }","title":"Variables"},{"location":"kubernetes/khw-gce/terraform-compute/#worker-config","text":"Extra config for the worker are the routes, to aid the pods going out of the node. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 data google_compute_image khw-ubuntu { family = ubuntu-1804-lts project = ubuntu-os-cloud } resource google_compute_instance khw-worker { count = ${var.num} name = worker-${count.index} machine_type = ${var.machine_type} zone = ${var.zone} can_ip_forward = true tags = [ kubernetes-the-hard-way , worker ] metadata { pod-cidr = 10.200.${count.index}.0/24 } boot_disk { initialize_params { image = ${data.google_compute_image.khw-ubuntu.self_link} size = 200 // in GB } } network_interface { subnetwork = ${var.subnet} address = 10.240.0.2${count.index} access_config { // Ephemeral External IP } } service_account { scopes = [ compute-rw , storage-ro , service-management , service-control , logging-write , monitoring , ] } } resource google_compute_route khw-worker-route { count = ${var.num} name = kubernetes-route-10-200-${count.index}-0-24 network = ${var.network} next_hop_ip = 10.240.0.2${count.index} dest_range = 10.200.${count.index}.0/24 }","title":"Worker config"},{"location":"kubernetes/khw-gce/terraform-compute/#variables_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 variable num { description = The number of controller VMs } variable machine_type { description = The type of VM for controllers } variable zone { description = The zone to create the controllers in } variable network { description = The network to use for routes } variable subnet { description = The subnet to create the nic in }","title":"Variables"},{"location":"kubernetes/khw-gce/terraform-compute/#health-check","text":"Because we will have three controllers, we have to make sure that GKE forwards Kubernetes API requests to each of them via our public IP address. We do this via a http health check, wich involves a forwarding rule and a target pool. Target pool being the group of controller VM's for which the forwarding rule is active. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 resource google_compute_target_pool khw-hc-target-pool { name = instance-pool # TODO: fixed set for now, maybe we can make this dynamic some day instances = [ ${var.region_default_zone}/controller-0 , ${var.region_default_zone}/controller-1 , ${var.region_default_zone}/controller-2 , ] health_checks = [ ${google_compute_http_health_check.khw-health-check.name} , ] } resource google_compute_http_health_check khw-health-check { name = kubernetes request_path = /healthz description = The health check for Kubernetes API server host = ${var.kubernetes-cluster-dns} } resource google_compute_forwarding_rule khw-hc-forward { name = kubernetes-forwarding-rule target = ${google_compute_target_pool.khw-hc-target-pool.self_link} region = ${var.region} port_range = 6443 ip_address = ${google_compute_address.khw-lb-public-ip.self_link} }","title":"Health check"},{"location":"kubernetes/khw-gce/terraform-compute/#apply-terraform-state","text":"In the end, our configuration should consist out of several .tf files and look something like this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ls -lath drwxr-xr-x 27 joostvdg staff 864B Aug 26 12 :50 . drwxr-xr-x 20 joostvdg staff 640B Aug 22 14 :47 .. drwxr-xr-x 4 joostvdg staff 128B Aug 7 22 :43 modules -rw-r--r-- 1 joostvdg staff 1 .5K Aug 26 12 :50 variables.tf -rw-r--r-- 1 joostvdg staff 1 .3K Aug 17 16 :03 firewall.tf -rw-r--r-- 1 joostvdg staff 4 .4K Aug 17 12 :06 worker-config.md -rw-r--r-- 1 joostvdg staff 1 .6K Aug 17 09 :35 healthcheck.tf -rw-r--r-- 1 joostvdg staff 517B Aug 16 17 :09 nodes.tf -rw-r--r-- 1 joostvdg staff 92B Aug 16 13 :52 publicip.tf -rw-r--r-- 1 joostvdg staff 365B Aug 7 22 :07 vpc.tf -rw-r--r-- 1 joostvdg staff 189B Aug 7 16 :51 base.tf drwxr-xr-x 5 joostvdg staff 160B Aug 7 21 :52 .terraform -rw-r--r-- 1 joostvdg staff 0B Aug 7 18 :28 terraform.tfstate We're now going to plan and then apply our Terraform configuration to create the resources in GCE. 1 terraform plan 1 terraform apply","title":"Apply Terraform state"},{"location":"kubernetes/khw-gce/worker/","text":"Worker Installation Install base components Download 1 2 3 4 5 6 7 8 9 wget -q --show-progress --https-only --timestamping \\ https://github.com/kubernetes-incubator/cri-tools/releases/download/v1.0.0-beta.0/crictl-v1.0.0-beta.0-linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-the-hard-way/runsc \\ https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \\ https://github.com/containernetworking/plugins/releases/download/v0.7.1/cni-plugins-amd64-v0.7.1.tgz \\ https://github.com/containerd/containerd/releases/download/v1.1.2/containerd-1.1.2.linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-proxy \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubelet Prepare landing folders 1 2 3 4 5 6 7 8 sudo mkdir -p \\ /etc/cni/net.d \\ /opt/cni/bin \\ /var/lib/kubelet \\ /var/lib/kube-proxy \\ /var/lib/kubernetes \\ /var/run/kubernetes \\ /etc/containerd/ Unpack to folders 1 2 3 4 5 6 chmod +x kubectl kube-proxy kubelet runc.amd64 runsc sudo mv runc.amd64 runc sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/ sudo tar -xvf crictl-v1.0.0-beta.0-linux-amd64.tar.gz -C /usr/local/bin/ sudo tar -xvf cni-plugins-amd64-v0.7.1.tgz -C /opt/cni/bin/ sudo tar -xvf containerd-1.1.2.linux-amd64.tar.gz -C / List variables 1 2 3 4 5 POD_CIDR = $( curl -s -H Metadata-Flavor: Google \\ http://metadata.google.internal/computeMetadata/v1/instance/attributes/pod-cidr ) echo HOSTNAME = $HOSTNAME echo POD_CIDR = $POD_CIDR Configure ContainerD Runtime configuration file 1 2 3 4 5 6 7 8 9 10 11 12 13 cat EOF | sudo tee /etc/containerd/config.toml [plugins] [plugins.cri.containerd] snapshotter = overlayfs [plugins.cri.containerd.default_runtime] runtime_type = io.containerd.runtime.v1.linux runtime_engine = /usr/local/bin/runc runtime_root = [plugins.cri.containerd.untrusted_workload_runtime] runtime_type = io.containerd.runtime.v1.linux runtime_engine = /usr/local/bin/runsc runtime_root = /run/containerd/runsc EOF SystemD service configuration file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 cat EOF | sudo tee /etc/systemd/system/containerd.service [Unit] Description = containerd container runtime Documentation = https://containerd.io After = network.target [Service] ExecStartPre = /sbin/modprobe overlay ExecStart = /bin/containerd Restart = always RestartSec = 5 Delegate = yes KillMode = process OOMScoreAdjust = -999 LimitNOFILE = 1048576 LimitNPROC = infinity LimitCORE = infinity [Install] WantedBy = multi-user.target EOF Configure CNI Warning We do not need to configure cni as we will setup Weave and it will do the necessary setup automagically. Configure Kubelet Move certificates to correct places 1 2 3 sudo mv ${ HOSTNAME } -key.pem ${ HOSTNAME } .pem /var/lib/kubelet/ sudo mv ${ HOSTNAME } .kubeconfig /var/lib/kubelet/kubeconfig sudo mv ca.pem /var/lib/kubernetes/ Create k8s yaml configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cat EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml kind : KubeletConfiguration apiVersion : kubelet.config.k8s.io/v1beta1 authentication : anonymous : enabled : false webhook : enabled : true x509 : clientCAFile : /var/lib/kubernetes/ca.pem authorization : mode : Webhook clusterDomain : cluster.local clusterDNS : - 10.32.0.10 podCIDR : ${POD_CIDR} runtimeRequestTimeout : 15m tlsCertFile : /var/lib/kubelet/${HOSTNAME}.pem tlsPrivateKeyFile : /var/lib/kubelet/${HOSTNAME}-key.pem EOF SystemD service configuration file Warning One thing I see missing from your kubelet configuration is --non-masquerade-cidr flag. Kubelet needs to be run with this option for traffic to outside clusterIP range. Refer here - kubenet 1 Kubelet should also be run with the `--non-masquerade-cidr= clusterCidr ` argument to ensure traffic to IPs outside this range will use IP masquerade. Not sure, if this is the cause, but looks like this is a requirement and is missing from the Kubelet config. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cat EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description = Kubernetes Kubelet Documentation = https://github.com/kubernetes/kubernetes After = containerd.service Requires = containerd.service [Service] ExecStart = /usr/local/bin/kubelet \\\\ --allow-privileged=true \\\\ --config=/var/lib/kubelet/kubelet-config.yaml \\\\ --container-runtime=remote \\\\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\ --image-pull-progress-deadline=2m \\\\ --kubeconfig=/var/lib/kubelet/kubeconfig \\\\ --network-plugin=cni \\\\ --register-node=true \\\\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF Kube-Proxy Move kubeconfig 1 sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig Create k8s yaml config 1 2 3 4 5 6 7 8 cat EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml kind : KubeProxyConfiguration apiVersion : kubeproxy.config.k8s.io/v1alpha1 clientConnection : kubeconfig : /var/lib/kube-proxy/kubeconfig mode : iptables clusterCIDR : 10.200.0.0/16 EOF Create SystemD service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat EOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description = Kubernetes Kube Proxy Documentation = https://github.com/kubernetes/kubernetes [Service] ExecStart = /usr/local/bin/kube-proxy \\\\ --config=/var/lib/kube-proxy/kube-proxy-config.yaml \\\\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF Configure and start the SystemD services 1 2 3 sudo systemctl daemon-reload sudo systemctl enable containerd kubelet kube-proxy sudo systemctl start containerd kubelet kube-proxy Validate Note Run this from a machine outside the cluster, with access to the admin kubeconfig. 1 gcloud compute ssh controller-0 --command kubectl get nodes --kubeconfig admin.kubeconfig Note As we didn't configure networking yet, the nodes should be shown as NotReady status.","title":"Worker Config"},{"location":"kubernetes/khw-gce/worker/#worker-installation","text":"","title":"Worker Installation"},{"location":"kubernetes/khw-gce/worker/#install-base-components","text":"","title":"Install base components"},{"location":"kubernetes/khw-gce/worker/#download","text":"1 2 3 4 5 6 7 8 9 wget -q --show-progress --https-only --timestamping \\ https://github.com/kubernetes-incubator/cri-tools/releases/download/v1.0.0-beta.0/crictl-v1.0.0-beta.0-linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-the-hard-way/runsc \\ https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \\ https://github.com/containernetworking/plugins/releases/download/v0.7.1/cni-plugins-amd64-v0.7.1.tgz \\ https://github.com/containerd/containerd/releases/download/v1.1.2/containerd-1.1.2.linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-proxy \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubelet","title":"Download"},{"location":"kubernetes/khw-gce/worker/#prepare-landing-folders","text":"1 2 3 4 5 6 7 8 sudo mkdir -p \\ /etc/cni/net.d \\ /opt/cni/bin \\ /var/lib/kubelet \\ /var/lib/kube-proxy \\ /var/lib/kubernetes \\ /var/run/kubernetes \\ /etc/containerd/","title":"Prepare landing folders"},{"location":"kubernetes/khw-gce/worker/#unpack-to-folders","text":"1 2 3 4 5 6 chmod +x kubectl kube-proxy kubelet runc.amd64 runsc sudo mv runc.amd64 runc sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/ sudo tar -xvf crictl-v1.0.0-beta.0-linux-amd64.tar.gz -C /usr/local/bin/ sudo tar -xvf cni-plugins-amd64-v0.7.1.tgz -C /opt/cni/bin/ sudo tar -xvf containerd-1.1.2.linux-amd64.tar.gz -C /","title":"Unpack to folders"},{"location":"kubernetes/khw-gce/worker/#list-variables","text":"1 2 3 4 5 POD_CIDR = $( curl -s -H Metadata-Flavor: Google \\ http://metadata.google.internal/computeMetadata/v1/instance/attributes/pod-cidr ) echo HOSTNAME = $HOSTNAME echo POD_CIDR = $POD_CIDR","title":"List variables"},{"location":"kubernetes/khw-gce/worker/#configure-containerd","text":"","title":"Configure ContainerD"},{"location":"kubernetes/khw-gce/worker/#runtime-configuration-file","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 cat EOF | sudo tee /etc/containerd/config.toml [plugins] [plugins.cri.containerd] snapshotter = overlayfs [plugins.cri.containerd.default_runtime] runtime_type = io.containerd.runtime.v1.linux runtime_engine = /usr/local/bin/runc runtime_root = [plugins.cri.containerd.untrusted_workload_runtime] runtime_type = io.containerd.runtime.v1.linux runtime_engine = /usr/local/bin/runsc runtime_root = /run/containerd/runsc EOF","title":"Runtime configuration file"},{"location":"kubernetes/khw-gce/worker/#systemd-service-configuration-file","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 cat EOF | sudo tee /etc/systemd/system/containerd.service [Unit] Description = containerd container runtime Documentation = https://containerd.io After = network.target [Service] ExecStartPre = /sbin/modprobe overlay ExecStart = /bin/containerd Restart = always RestartSec = 5 Delegate = yes KillMode = process OOMScoreAdjust = -999 LimitNOFILE = 1048576 LimitNPROC = infinity LimitCORE = infinity [Install] WantedBy = multi-user.target EOF","title":"SystemD service configuration file"},{"location":"kubernetes/khw-gce/worker/#configure-cni","text":"Warning We do not need to configure cni as we will setup Weave and it will do the necessary setup automagically.","title":"Configure CNI"},{"location":"kubernetes/khw-gce/worker/#configure-kubelet","text":"","title":"Configure Kubelet"},{"location":"kubernetes/khw-gce/worker/#move-certificates-to-correct-places","text":"1 2 3 sudo mv ${ HOSTNAME } -key.pem ${ HOSTNAME } .pem /var/lib/kubelet/ sudo mv ${ HOSTNAME } .kubeconfig /var/lib/kubelet/kubeconfig sudo mv ca.pem /var/lib/kubernetes/","title":"Move certificates to correct places"},{"location":"kubernetes/khw-gce/worker/#create-k8s-yaml-configuration","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cat EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml kind : KubeletConfiguration apiVersion : kubelet.config.k8s.io/v1beta1 authentication : anonymous : enabled : false webhook : enabled : true x509 : clientCAFile : /var/lib/kubernetes/ca.pem authorization : mode : Webhook clusterDomain : cluster.local clusterDNS : - 10.32.0.10 podCIDR : ${POD_CIDR} runtimeRequestTimeout : 15m tlsCertFile : /var/lib/kubelet/${HOSTNAME}.pem tlsPrivateKeyFile : /var/lib/kubelet/${HOSTNAME}-key.pem EOF","title":"Create k8s yaml configuration"},{"location":"kubernetes/khw-gce/worker/#systemd-service-configuration-file_1","text":"Warning One thing I see missing from your kubelet configuration is --non-masquerade-cidr flag. Kubelet needs to be run with this option for traffic to outside clusterIP range. Refer here - kubenet 1 Kubelet should also be run with the `--non-masquerade-cidr= clusterCidr ` argument to ensure traffic to IPs outside this range will use IP masquerade. Not sure, if this is the cause, but looks like this is a requirement and is missing from the Kubelet config. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cat EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description = Kubernetes Kubelet Documentation = https://github.com/kubernetes/kubernetes After = containerd.service Requires = containerd.service [Service] ExecStart = /usr/local/bin/kubelet \\\\ --allow-privileged=true \\\\ --config=/var/lib/kubelet/kubelet-config.yaml \\\\ --container-runtime=remote \\\\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\ --image-pull-progress-deadline=2m \\\\ --kubeconfig=/var/lib/kubelet/kubeconfig \\\\ --network-plugin=cni \\\\ --register-node=true \\\\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF","title":"SystemD service configuration file"},{"location":"kubernetes/khw-gce/worker/#kube-proxy","text":"","title":"Kube-Proxy"},{"location":"kubernetes/khw-gce/worker/#move-kubeconfig","text":"1 sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig","title":"Move kubeconfig"},{"location":"kubernetes/khw-gce/worker/#create-k8s-yaml-config","text":"1 2 3 4 5 6 7 8 cat EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml kind : KubeProxyConfiguration apiVersion : kubeproxy.config.k8s.io/v1alpha1 clientConnection : kubeconfig : /var/lib/kube-proxy/kubeconfig mode : iptables clusterCIDR : 10.200.0.0/16 EOF","title":"Create k8s yaml config"},{"location":"kubernetes/khw-gce/worker/#create-systemd-service","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat EOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description = Kubernetes Kube Proxy Documentation = https://github.com/kubernetes/kubernetes [Service] ExecStart = /usr/local/bin/kube-proxy \\\\ --config=/var/lib/kube-proxy/kube-proxy-config.yaml \\\\ --v=2 Restart = on-failure RestartSec = 5 [Install] WantedBy = multi-user.target EOF","title":"Create SystemD service"},{"location":"kubernetes/khw-gce/worker/#configure-and-start-the-systemd-services","text":"1 2 3 sudo systemctl daemon-reload sudo systemctl enable containerd kubelet kube-proxy sudo systemctl start containerd kubelet kube-proxy","title":"Configure and start the SystemD services"},{"location":"kubernetes/khw-gce/worker/#validate","text":"Note Run this from a machine outside the cluster, with access to the admin kubeconfig. 1 gcloud compute ssh controller-0 --command kubectl get nodes --kubeconfig admin.kubeconfig Note As we didn't configure networking yet, the nodes should be shown as NotReady status.","title":"Validate"},{"location":"linux/","text":"Linux","title":"Introduction"},{"location":"linux/#linux","text":"","title":"Linux"},{"location":"linux/iptables/","text":"iptables","title":"iptables"},{"location":"linux/iptables/#iptables","text":"","title":"iptables"},{"location":"linux/networking/","text":"Networking","title":"Networking"},{"location":"linux/networking/#networking","text":"","title":"Networking"},{"location":"linux/systemd/","text":"SystemD Increasingly, Linux distributions are adopting or planning to adopt the systemd init system. This powerful suite of software can manage many aspects of your server, from services to mounted devices and system states. 1 Concepts Unit In systemd , a unit refers to any resource that the system knows how to operate on and manage. This is the primary object that the systemd tools know how to deal with. These resources are defined using configuration files called unit files . 1 Path A path unit defines a filesystem path that systmed can monitor for changes. Another unit must exist that will be be activated when certain activity is detected at the path location. Path activity is determined through inotify events . My idea, you can use this for those services that should trigger on file uploads or backup dumps. Although I wonder if the Unit's main service knows which path was triggered? If it does, than it's easy, else you still need a \"file walker\". Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 [Unit] Description = Timezone Helper Service After = network.target StartLimitIntervalSec = 0 [Service] Type = simple Restart = always RestartSec = 3 User = joostvdg ExecStart = /usr/bin/timezone_helper_service [Install] WantedBy = multi-user.target Resources https://www.linuxjournal.com/content/linux-filesystem-events-inotify References Introduction to systemd from Digital Ocean","title":"SystemD"},{"location":"linux/systemd/#systemd","text":"Increasingly, Linux distributions are adopting or planning to adopt the systemd init system. This powerful suite of software can manage many aspects of your server, from services to mounted devices and system states. 1","title":"SystemD"},{"location":"linux/systemd/#concepts","text":"","title":"Concepts"},{"location":"linux/systemd/#unit","text":"In systemd , a unit refers to any resource that the system knows how to operate on and manage. This is the primary object that the systemd tools know how to deal with. These resources are defined using configuration files called unit files . 1","title":"Unit"},{"location":"linux/systemd/#path","text":"A path unit defines a filesystem path that systmed can monitor for changes. Another unit must exist that will be be activated when certain activity is detected at the path location. Path activity is determined through inotify events . My idea, you can use this for those services that should trigger on file uploads or backup dumps. Although I wonder if the Unit's main service knows which path was triggered? If it does, than it's easy, else you still need a \"file walker\".","title":"Path"},{"location":"linux/systemd/#example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 [Unit] Description = Timezone Helper Service After = network.target StartLimitIntervalSec = 0 [Service] Type = simple Restart = always RestartSec = 3 User = joostvdg ExecStart = /usr/bin/timezone_helper_service [Install] WantedBy = multi-user.target","title":"Example"},{"location":"linux/systemd/#resources","text":"https://www.linuxjournal.com/content/linux-filesystem-events-inotify","title":"Resources"},{"location":"linux/systemd/#references","text":"Introduction to systemd from Digital Ocean","title":"References"},{"location":"other/mkdocs/","text":"MKDocs This website is build using the following: MKDocs a python tool for building static websites from MarkDown files MK Material expansion/theme of MK Docs that makes it a responsive website with Google's Material theme Add information to the docs MKDocs can be a bit daunting to use, especially when extended with MKDocs Material and PyMdown Extensions . There are two parts to the site: 1) the markdown files, they're in docs/ and 2) the site listing (mkdocs.yml) and automation scripts, these can be found in docs-scripts/ . Extends current page To extend a current page, simply write the MarkDown as you're used to. For the specific extensions offered by PyMX and Material, checkout the following pages: MKDocs Material Getting Started Guide MKDocs Extensions PyMdown Extensions Usage Guide Add a new page In the docs-scripts/mkdocs.yml you will find the site structure under the yml item of pages . 1 2 3 4 5 6 pages: - Home: index.md - Other Root Page: some-page.md - Root with children: - ChildOne: root2/child1.md - ChildTwo: root2/child2.md Things to know All .md files that are listed in the pages will be translated to an HTML file and dubbed {OriginalFileName}.html Naming a file index.md will allow you to refer to it by path without the file name we can refer to root2 simply by site/root2 and can omit the index. 1 2 - Root: index.md - Root2: root2/index.html Build the site locally As it is a Python tool, you can easily build it with Python (2.7 is recommended). The requirements are captured in a pip install scripts: docs-scripts/install.sh where the dependencies are in Pip's requirements.txt . Once that is done, you can do the following: 1 mkdocs build --clean Which will generate the site into docs-scripts/site where you can simply open the index.html with a browser - it is a static site. For docker, you can use the *.sh scripts, or simply run.sh to kick of the entire build. Jenkins build Declarative format 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 pipeline { agent none options { timeout(time: 10, unit: MINUTES ) timestamps() buildDiscarder(logRotator(numToKeepStr: 5 )) } stages { stage( Prepare ){ agent { label docker } steps { deleteDir() } } stage( Checkout ) { agent { label docker } steps { checkout scm script { env.GIT_COMMIT_HASH = sh returnStdout: true, script: git rev-parse --verify HEAD } } } stage( Build Docs ) { agent { docker { image caladreas/mkdocs-docker-build-container label docker } } steps { sh cd docs-scripts mkdocs build } } stage( Prepare Docker Image ) { agent { label docker } environment { DOCKER_CRED = credentials( ldap ) } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true, script: cd docs-scripts docker run --rm -i lukasmartinelli/hadolint Dockerfile if (lintResult.trim() == ) { println Lint finished with no errors } else { println Error found in Lint println ${lintResult} currentBuild.result = UNSTABLE } } }, // end test dockerfile BuildImage: { sh chmod +x docs-scripts/build.sh sh cd docs-scripts ./build.sh } , login: { sh docker login -u ${DOCKER_CRED_USR} -p ${DOCKER_CRED_PSW} registry } ) } post { success { sh chmod +x push.sh sh ./push.sh } } } } }","title":"MKDocs (Static Website Generator)"},{"location":"other/mkdocs/#mkdocs","text":"This website is build using the following: MKDocs a python tool for building static websites from MarkDown files MK Material expansion/theme of MK Docs that makes it a responsive website with Google's Material theme","title":"MKDocs"},{"location":"other/mkdocs/#add-information-to-the-docs","text":"MKDocs can be a bit daunting to use, especially when extended with MKDocs Material and PyMdown Extensions . There are two parts to the site: 1) the markdown files, they're in docs/ and 2) the site listing (mkdocs.yml) and automation scripts, these can be found in docs-scripts/ .","title":"Add information to the docs"},{"location":"other/mkdocs/#extends-current-page","text":"To extend a current page, simply write the MarkDown as you're used to. For the specific extensions offered by PyMX and Material, checkout the following pages: MKDocs Material Getting Started Guide MKDocs Extensions PyMdown Extensions Usage Guide","title":"Extends current page"},{"location":"other/mkdocs/#add-a-new-page","text":"In the docs-scripts/mkdocs.yml you will find the site structure under the yml item of pages . 1 2 3 4 5 6 pages: - Home: index.md - Other Root Page: some-page.md - Root with children: - ChildOne: root2/child1.md - ChildTwo: root2/child2.md","title":"Add a new page"},{"location":"other/mkdocs/#things-to-know","text":"All .md files that are listed in the pages will be translated to an HTML file and dubbed {OriginalFileName}.html Naming a file index.md will allow you to refer to it by path without the file name we can refer to root2 simply by site/root2 and can omit the index. 1 2 - Root: index.md - Root2: root2/index.html","title":"Things to know"},{"location":"other/mkdocs/#build-the-site-locally","text":"As it is a Python tool, you can easily build it with Python (2.7 is recommended). The requirements are captured in a pip install scripts: docs-scripts/install.sh where the dependencies are in Pip's requirements.txt . Once that is done, you can do the following: 1 mkdocs build --clean Which will generate the site into docs-scripts/site where you can simply open the index.html with a browser - it is a static site. For docker, you can use the *.sh scripts, or simply run.sh to kick of the entire build.","title":"Build the site locally"},{"location":"other/mkdocs/#jenkins-build","text":"","title":"Jenkins build"},{"location":"other/mkdocs/#declarative-format","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 pipeline { agent none options { timeout(time: 10, unit: MINUTES ) timestamps() buildDiscarder(logRotator(numToKeepStr: 5 )) } stages { stage( Prepare ){ agent { label docker } steps { deleteDir() } } stage( Checkout ) { agent { label docker } steps { checkout scm script { env.GIT_COMMIT_HASH = sh returnStdout: true, script: git rev-parse --verify HEAD } } } stage( Build Docs ) { agent { docker { image caladreas/mkdocs-docker-build-container label docker } } steps { sh cd docs-scripts mkdocs build } } stage( Prepare Docker Image ) { agent { label docker } environment { DOCKER_CRED = credentials( ldap ) } steps { parallel ( TestDockerfile: { script { def lintResult = sh returnStdout: true, script: cd docs-scripts docker run --rm -i lukasmartinelli/hadolint Dockerfile if (lintResult.trim() == ) { println Lint finished with no errors } else { println Error found in Lint println ${lintResult} currentBuild.result = UNSTABLE } } }, // end test dockerfile BuildImage: { sh chmod +x docs-scripts/build.sh sh cd docs-scripts ./build.sh } , login: { sh docker login -u ${DOCKER_CRED_USR} -p ${DOCKER_CRED_PSW} registry } ) } post { success { sh chmod +x push.sh sh ./push.sh } } } } }","title":"Declarative format"},{"location":"other/vim/","text":"VIM Install Vundle 1 git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim Install plugins 1 vim ~/.vimrc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 filetype off filetype plugin indent on syntax on set rtp += ~/.vim/bundle/Vundle.vim call vundle#begin () Plugin gmarik/Vundle.vim Plugin reedes/vim-thematic Plugin airblade/vim-gitgutter Plugin vim-airline/vim-airline Plugin vim-airline/vim-airline-themes Plugin itchyny/lightline.vim Plugin nathanaelkane/vim-indent-guides Plugin scrooloose/nerdtree Plugin editorconfig/editorconfig-vim Plugin mhinz/vim-signify call vundle#end () filetype plugin indent on Open VIM, and install the plugins: 1 :installPlugins","title":"VIM"},{"location":"other/vim/#vim","text":"","title":"VIM"},{"location":"other/vim/#install-vundle","text":"1 git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim","title":"Install Vundle"},{"location":"other/vim/#install-plugins","text":"1 vim ~/.vimrc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 filetype off filetype plugin indent on syntax on set rtp += ~/.vim/bundle/Vundle.vim call vundle#begin () Plugin gmarik/Vundle.vim Plugin reedes/vim-thematic Plugin airblade/vim-gitgutter Plugin vim-airline/vim-airline Plugin vim-airline/vim-airline-themes Plugin itchyny/lightline.vim Plugin nathanaelkane/vim-indent-guides Plugin scrooloose/nerdtree Plugin editorconfig/editorconfig-vim Plugin mhinz/vim-signify call vundle#end () filetype plugin indent on Open VIM, and install the plugins: 1 :installPlugins","title":"Install plugins"},{"location":"productivity/","text":"Developer Productivity Commoditization \"The big change has been in the hardware/software cost ratio. The buyer of a $2 million machine in 1960 felt that he could afford $250,000 more ofr a customized payroll program, one that slipped easily and nondisruptively into the computer-hostile social environment. Buyers of %50,000 office machines today cannot conceivably afford customized payroll programs; so they adapt their paryoll procedures to the packages available.\" - 2 F. Brooks - No Silver Bullet Where should productivity be sought If you're looking to increase productivity, it would be best to answer some fundamental questions first. What should we be productive in? What is productivity? How do you measure productivity? The first step is to determine, what you should be productive in. If you're building software for example, it is in finding out what to build. \"The hardest single part of building a software system is deciding precisely what to build.\" - F. Brooks 2 That is actually already one step to far, as you would need a reason to build a software system. So the first step for any individual or organization (start up, or otherwise) is to find out what people want that you can offer. \"The fundamental activity of a startup is to turn ideas into products, measure how customers respond, and then learn whether to pivot or persevere. All successful startup processes should be geared to accelerate that feedback loop.\" - The Lean Startup 3 How do you measure productivity Grow v.s. Build The Balancing act between centralized and decentralized On Multitasking Deep Work Attention Residue Why is it so hard to do my work Learning from Lean/Toyota Open Space Floor Plans http://rstb.royalsocietypublishing.org/content/373/1753/20170239 Conway's Law \"organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations.\" - M. Conway 1 Undifferentiated Heavy Lifting \"Work that needs to get done, but having it done doesn't bring our customers any direct benefit.\" - Dave Hahn Further reading Others https://www.youtube.com/watch?v=UTKIT6STSVM https://en.wikipedia.org/wiki/Complex_adaptive_system https://jobs.netflix.com/culture http://blackswanfarming.com/cost-of-delay/ https://www.rundeck.com/blog/tickets_make_operations_unnecessarily_miserable https://www.digitalocean.com/community/tutorials/what-is-immutable-infrastructure https://hbr.org/2015/12/what-the-research-tells-us-about-team-creativity-and-innovation https://www.thoughtworks.com/insights/blog/continuous-improvement-safe-environment https://qualitysafety.bmj.com/content/13/suppl_2/ii22 https://www.plutora.com/wp-content/uploads/dlm_uploads/2018/03/StateOfDevOpsTools_v14.pdf https://medium.com/@ATavgen/never-fail-twice-608147cb49b https://blogs.dropbox.com/dropbox/2018/07/study-high-performing-teams/?_tk=social oqa=183tl01liov linkId=100000003064606 http://psycnet.apa.org/record/1979-28632-001 https://pdfs.semanticscholar.org/a85d/432f44e43d61753bb8a121c246127b562a39.pdf https://medium.com/@dr_eprice/laziness-does-not-exist-3af27e312d01 https://en.wikipedia.org/wiki/Mindset#Fixed_and_growth http://www.reinventingorganizationswiki.com/Teal_Organizations https://www.mckinsey.com/business-functions/organization/our-insights/the-irrational-side-of-change-management https://www.barrypopik.com/index.php/new_york_city/entry/how_do_you_eat_an_elephant https://kadavy.net/blog/posts/mind-management-intro/ https://en.wikipedia.org/wiki/Planning_fallacy https://stories.lemonade.com/lemonade-proves-trust-pays-off-big-time-fdcf587af5a1 https://www.venturi-group.com/developer-to-cto/ https://dzone.com/articles/an-introduction-to-devops-principles https://www.thoughtworks.com/insights/blog/evolving-thoughtworks-internal-it-solve-broader-cross-cutting-problems https://www.thoughtworks.com/insights/blog/platform-tech-strategy-three-layers https://www.thoughtworks.com/insights/blog/why-it-departments-must-reinvent-themselves-part-1 https://en.wikipedia.org/wiki/Peter_principle https://hackernoon.com/why-all-engineers-must-understand-management-the-view-from-both-ladders-cc749ae14905 https://medium.freecodecamp.org/cognitive-bias-and-why-performance-management-is-so-hard-8852a1b874cd https://en.wikipedia.org/wiki/Horn_effect https://en.wikipedia.org/wiki/Halo_effect http://serendipstudio.org/bb/neuro/neuro02/web2/hhochman.html https://betterhumans.coach.me/how-to-be-a-better-manager-by-understanding-the-difference-between-market-norms-and-social-norms-3082d97d440f https://skillsmatter.com/skillscasts/10466-deep-dive-on-kubernetes-networking https://purplegriffon.com/blog/is-itil-agile-enough https://launchdarkly.com/blog/progressive-delivery-a-history-condensed/ http://www.collaborativefund.com/blog/real-world-vs-book-knowledge/ https://blog.codeship.com/using-jx-create-gitops-managed-jenkins-x-installation-cloudbees-codeship-terraform-google-kubernetes-engine/ https://martinfowler.com/articles/serverless.html https://www.quora.com/Some-people-including-the-creator-of-C-claim-that-there-is-a-huge-decline-of-quality-among-software-developers-What-seems-to-be-the-main-cause https://medium.com/netflix-techblog/full-cycle-developers-at-netflix-a08c31f83249 https://queue.acm.org/detail.cfm?id=3182626 http://www.safetydifferently.com/why-do-things-go-right/ https://www.quora.com/What-is-better-to-become-an-specialist-or-a-generalist-in-software-development-Does-the-full-stack-term-still-makes-sense https://cengizhan.com/3-pillars-of-observability-8e6cb5434206 https://www.thoughtworks.com/perspectives/edition1-agile-article https://blog.mexia.com.au/a-pace-layered-integration-architecture Presentations https://speakerdeck.com/tylertreat/the-future-of-ops https://www.slideshare.net/rheinwein/the-container-shame-spiral Articles https://dzone.com/articles/a-praise-for-self-service-in-it-value-streams http://blog.christianposta.com/microservices/application-safety-and-correctness-cannot-be-offloaded-to-istio-or-any-service-mesh/ https://www.gatesnotes.com/Books/Capitalism-Without-Capital?WT.mc_id=08_16_2018_06_CapitalismWithoutCapital_BG-LI_ WT.tsrc=BGLI linkId=55623312 https://uxdesign.cc/stop-delivering-software-with-agile-it-doesn-t-work-edccea3ab5d3 Concept of Shared Services and beyond Introduction to Observability by Weave Net Article on the state of Systems Languages Article on SILO's Blog on Twitter's Engineering Efficiency Why Companies should have a Heroku platform for their developers Multitasking is bad for your health Microsoft research on Developer's perception of productivity Developer Productivity Struggles You cannot measure productivity The Productivity Paradox There is no Productivity Paradox: it lags behind investments Economist: solving the paradox The Myth Of Developer Productivity Effectiveness vs. Efficiency Lean Manufactoring Theory of Constraints Thoughtworks: demystifying Conway's Law John Allspaw: a mature role for automation Research from DORA Books The Goal The Phoenix Project Continuous Delivery The Lean Startup The Lean Enterprise DevOps Handbook Thinking Fast and Slow Sapiens On Writing https://www.proofreadingservices.com/pages/very References Conway's law in wikipedia No Silver Bullet - F. Brooks The Lean Startup Principles","title":"General"},{"location":"productivity/#developer-productivity","text":"","title":"Developer Productivity"},{"location":"productivity/#commoditization","text":"\"The big change has been in the hardware/software cost ratio. The buyer of a $2 million machine in 1960 felt that he could afford $250,000 more ofr a customized payroll program, one that slipped easily and nondisruptively into the computer-hostile social environment. Buyers of %50,000 office machines today cannot conceivably afford customized payroll programs; so they adapt their paryoll procedures to the packages available.\" - 2 F. Brooks - No Silver Bullet","title":"Commoditization"},{"location":"productivity/#where-should-productivity-be-sought","text":"If you're looking to increase productivity, it would be best to answer some fundamental questions first. What should we be productive in? What is productivity? How do you measure productivity? The first step is to determine, what you should be productive in. If you're building software for example, it is in finding out what to build. \"The hardest single part of building a software system is deciding precisely what to build.\" - F. Brooks 2 That is actually already one step to far, as you would need a reason to build a software system. So the first step for any individual or organization (start up, or otherwise) is to find out what people want that you can offer. \"The fundamental activity of a startup is to turn ideas into products, measure how customers respond, and then learn whether to pivot or persevere. All successful startup processes should be geared to accelerate that feedback loop.\" - The Lean Startup 3","title":"Where should productivity be sought"},{"location":"productivity/#how-do-you-measure-productivity","text":"","title":"How do you measure productivity"},{"location":"productivity/#grow-vs-build","text":"","title":"Grow v.s. Build"},{"location":"productivity/#the-balancing-act-between-centralized-and-decentralized","text":"","title":"The Balancing act between centralized and decentralized"},{"location":"productivity/#on-multitasking","text":"Deep Work Attention Residue Why is it so hard to do my work","title":"On Multitasking"},{"location":"productivity/#learning-from-leantoyota","text":"","title":"Learning from Lean/Toyota"},{"location":"productivity/#open-space-floor-plans","text":"http://rstb.royalsocietypublishing.org/content/373/1753/20170239","title":"Open Space Floor Plans"},{"location":"productivity/#conways-law","text":"\"organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations.\" - M. Conway 1","title":"Conway's Law"},{"location":"productivity/#undifferentiated-heavy-lifting","text":"\"Work that needs to get done, but having it done doesn't bring our customers any direct benefit.\" - Dave Hahn","title":"Undifferentiated Heavy Lifting"},{"location":"productivity/#further-reading","text":"","title":"Further reading"},{"location":"productivity/#others","text":"https://www.youtube.com/watch?v=UTKIT6STSVM https://en.wikipedia.org/wiki/Complex_adaptive_system https://jobs.netflix.com/culture http://blackswanfarming.com/cost-of-delay/ https://www.rundeck.com/blog/tickets_make_operations_unnecessarily_miserable https://www.digitalocean.com/community/tutorials/what-is-immutable-infrastructure https://hbr.org/2015/12/what-the-research-tells-us-about-team-creativity-and-innovation https://www.thoughtworks.com/insights/blog/continuous-improvement-safe-environment https://qualitysafety.bmj.com/content/13/suppl_2/ii22 https://www.plutora.com/wp-content/uploads/dlm_uploads/2018/03/StateOfDevOpsTools_v14.pdf https://medium.com/@ATavgen/never-fail-twice-608147cb49b https://blogs.dropbox.com/dropbox/2018/07/study-high-performing-teams/?_tk=social oqa=183tl01liov linkId=100000003064606 http://psycnet.apa.org/record/1979-28632-001 https://pdfs.semanticscholar.org/a85d/432f44e43d61753bb8a121c246127b562a39.pdf https://medium.com/@dr_eprice/laziness-does-not-exist-3af27e312d01 https://en.wikipedia.org/wiki/Mindset#Fixed_and_growth http://www.reinventingorganizationswiki.com/Teal_Organizations https://www.mckinsey.com/business-functions/organization/our-insights/the-irrational-side-of-change-management https://www.barrypopik.com/index.php/new_york_city/entry/how_do_you_eat_an_elephant https://kadavy.net/blog/posts/mind-management-intro/ https://en.wikipedia.org/wiki/Planning_fallacy https://stories.lemonade.com/lemonade-proves-trust-pays-off-big-time-fdcf587af5a1 https://www.venturi-group.com/developer-to-cto/ https://dzone.com/articles/an-introduction-to-devops-principles https://www.thoughtworks.com/insights/blog/evolving-thoughtworks-internal-it-solve-broader-cross-cutting-problems https://www.thoughtworks.com/insights/blog/platform-tech-strategy-three-layers https://www.thoughtworks.com/insights/blog/why-it-departments-must-reinvent-themselves-part-1 https://en.wikipedia.org/wiki/Peter_principle https://hackernoon.com/why-all-engineers-must-understand-management-the-view-from-both-ladders-cc749ae14905 https://medium.freecodecamp.org/cognitive-bias-and-why-performance-management-is-so-hard-8852a1b874cd https://en.wikipedia.org/wiki/Horn_effect https://en.wikipedia.org/wiki/Halo_effect http://serendipstudio.org/bb/neuro/neuro02/web2/hhochman.html https://betterhumans.coach.me/how-to-be-a-better-manager-by-understanding-the-difference-between-market-norms-and-social-norms-3082d97d440f https://skillsmatter.com/skillscasts/10466-deep-dive-on-kubernetes-networking https://purplegriffon.com/blog/is-itil-agile-enough https://launchdarkly.com/blog/progressive-delivery-a-history-condensed/ http://www.collaborativefund.com/blog/real-world-vs-book-knowledge/ https://blog.codeship.com/using-jx-create-gitops-managed-jenkins-x-installation-cloudbees-codeship-terraform-google-kubernetes-engine/ https://martinfowler.com/articles/serverless.html https://www.quora.com/Some-people-including-the-creator-of-C-claim-that-there-is-a-huge-decline-of-quality-among-software-developers-What-seems-to-be-the-main-cause https://medium.com/netflix-techblog/full-cycle-developers-at-netflix-a08c31f83249 https://queue.acm.org/detail.cfm?id=3182626 http://www.safetydifferently.com/why-do-things-go-right/ https://www.quora.com/What-is-better-to-become-an-specialist-or-a-generalist-in-software-development-Does-the-full-stack-term-still-makes-sense https://cengizhan.com/3-pillars-of-observability-8e6cb5434206 https://www.thoughtworks.com/perspectives/edition1-agile-article https://blog.mexia.com.au/a-pace-layered-integration-architecture","title":"Others"},{"location":"productivity/#presentations","text":"https://speakerdeck.com/tylertreat/the-future-of-ops https://www.slideshare.net/rheinwein/the-container-shame-spiral","title":"Presentations"},{"location":"productivity/#articles","text":"https://dzone.com/articles/a-praise-for-self-service-in-it-value-streams http://blog.christianposta.com/microservices/application-safety-and-correctness-cannot-be-offloaded-to-istio-or-any-service-mesh/ https://www.gatesnotes.com/Books/Capitalism-Without-Capital?WT.mc_id=08_16_2018_06_CapitalismWithoutCapital_BG-LI_ WT.tsrc=BGLI linkId=55623312 https://uxdesign.cc/stop-delivering-software-with-agile-it-doesn-t-work-edccea3ab5d3 Concept of Shared Services and beyond Introduction to Observability by Weave Net Article on the state of Systems Languages Article on SILO's Blog on Twitter's Engineering Efficiency Why Companies should have a Heroku platform for their developers Multitasking is bad for your health Microsoft research on Developer's perception of productivity Developer Productivity Struggles You cannot measure productivity The Productivity Paradox There is no Productivity Paradox: it lags behind investments Economist: solving the paradox The Myth Of Developer Productivity Effectiveness vs. Efficiency Lean Manufactoring Theory of Constraints Thoughtworks: demystifying Conway's Law John Allspaw: a mature role for automation Research from DORA","title":"Articles"},{"location":"productivity/#books","text":"The Goal The Phoenix Project Continuous Delivery The Lean Startup The Lean Enterprise DevOps Handbook Thinking Fast and Slow Sapiens","title":"Books"},{"location":"productivity/#on-writing","text":"https://www.proofreadingservices.com/pages/very","title":"On Writing"},{"location":"productivity/#references","text":"Conway's law in wikipedia No Silver Bullet - F. Brooks The Lean Startup Principles","title":"References"},{"location":"productivity/intellij/","text":"Intelli J Beneficial OS changes Linux Increase inotify watches \\","title":"Intelli J"},{"location":"productivity/intellij/#intelli-j","text":"","title":"Intelli J"},{"location":"productivity/intellij/#beneficial-os-changes","text":"","title":"Beneficial OS changes"},{"location":"productivity/intellij/#linux","text":"Increase inotify watches \\","title":"Linux"},{"location":"productivity/paradigms/","text":"Paradigms Product centered Resources","title":"Paradigms"},{"location":"productivity/paradigms/#paradigms","text":"","title":"Paradigms"},{"location":"productivity/paradigms/#product-centered","text":"","title":"Product centered"},{"location":"productivity/paradigms/#resources","text":"","title":"Resources"},{"location":"productivity/remote/","text":"Working Remote Resources https://open.nytimes.com/how-to-grow-as-an-engineer-working-remotely-3baff8211f3e","title":"Working Remote"},{"location":"productivity/remote/#working-remote","text":"","title":"Working Remote"},{"location":"productivity/remote/#resources","text":"https://open.nytimes.com/how-to-grow-as-an-engineer-working-remotely-3baff8211f3e","title":"Resources"},{"location":"productivity/studies/","text":"Developer Productivity Studies","title":"Studies"},{"location":"productivity/studies/#developer-productivity-studies","text":"","title":"Developer Productivity Studies"},{"location":"productivity/team-development/","text":"Team Development Resources Article about Tuckman's theory of group development Wiki of Tuckman's theory of group development How google thinks about Team Effectiveness Google Re:Work tutorial on Team Effectiveness","title":"Team Development"},{"location":"productivity/team-development/#team-development","text":"","title":"Team Development"},{"location":"productivity/team-development/#resources","text":"Article about Tuckman's theory of group development Wiki of Tuckman's theory of group development How google thinks about Team Effectiveness Google Re:Work tutorial on Team Effectiveness","title":"Resources"},{"location":"productivity/tools/","text":"Developer Productivity Tools","title":"Tools"},{"location":"productivity/tools/#developer-productivity-tools","text":"","title":"Developer Productivity Tools"},{"location":"swe/API/","text":"API's https://docs.microsoft.com/en-us/azure/architecture/best-practices/api-design","title":"API's"},{"location":"swe/API/#apis","text":"https://docs.microsoft.com/en-us/azure/architecture/best-practices/api-design","title":"API's"},{"location":"swe/algorithms/","text":"","title":"Algorithms"},{"location":"swe/ddd/","text":"Domain Driven Design","title":"Domain Driven Design"},{"location":"swe/ddd/#domain-driven-design","text":"","title":"Domain Driven Design"},{"location":"swe/distributed/","text":"Distributed Computing Distributed Computing fundamentals Time and Event ordering See: Lamport timestamp Distributed Applications Topics to take into account logging structured pulled into central log service Java: SLF4J + LogBack? Go: logrus tracing sampling based metrics prometheus including alert definitions network connection stability services discovery loadbalancing circuit brakers backpressure shallow queues connection pools dynamic/randomized backoff procedures network connection performance 3-step handshake binary over http standard protocols thin wrapper for UI: GraphQL thick wrapper for UI: JSON over HTTP (restful) Service to Service: gRPC / twirp Designing Distributed Systems - Brandon Burns Sidecar pattern 1 docker run -d my-app-image After you run that image, you will receive the identifier for that specific container. It will look something like: cccf82b85000... If you don\u2019t have it, you can always look it up using the docker ps command, which will show all currently running containers. Assuming you have stashed that value in an environment variable named APP_ID, you can then run the topz container in the same PID namespace using: 1 docker run --pid = container: ${ APP_ID } \\ -p 8080 :8080 brendanburns/topz:db0fa58 /server --address = 0 .0.0.0:8080 Resources Coursera course Article on synchronization in a distributed system http://label-schema.org/ https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236 https://eng.lyft.com/announcing-envoy-c-l7-proxy-and-communication-bus-92520b6c8191 https://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc https://cse.buffalo.edu/~demirbas/publications/cloudConsensus.pdf http://www.read.seas.harvard.edu/~kohler/class/08w-dsi/chandra07paxos.pdf https://medium.com/source-code/understanding-the-memcached-source-code-slab-i-9199de613762","title":"Distributed Computing"},{"location":"swe/distributed/#distributed-computing","text":"","title":"Distributed Computing"},{"location":"swe/distributed/#distributed-computing-fundamentals","text":"","title":"Distributed Computing fundamentals"},{"location":"swe/distributed/#time-and-event-ordering","text":"See: Lamport timestamp","title":"Time and Event ordering"},{"location":"swe/distributed/#distributed-applications","text":"","title":"Distributed Applications"},{"location":"swe/distributed/#topics-to-take-into-account","text":"logging structured pulled into central log service Java: SLF4J + LogBack? Go: logrus tracing sampling based metrics prometheus including alert definitions network connection stability services discovery loadbalancing circuit brakers backpressure shallow queues connection pools dynamic/randomized backoff procedures network connection performance 3-step handshake binary over http standard protocols thin wrapper for UI: GraphQL thick wrapper for UI: JSON over HTTP (restful) Service to Service: gRPC / twirp","title":"Topics to take into account"},{"location":"swe/distributed/#designing-distributed-systems-brandon-burns","text":"","title":"Designing Distributed Systems - Brandon Burns"},{"location":"swe/distributed/#sidecar-pattern","text":"1 docker run -d my-app-image After you run that image, you will receive the identifier for that specific container. It will look something like: cccf82b85000... If you don\u2019t have it, you can always look it up using the docker ps command, which will show all currently running containers. Assuming you have stashed that value in an environment variable named APP_ID, you can then run the topz container in the same PID namespace using: 1 docker run --pid = container: ${ APP_ID } \\ -p 8080 :8080 brendanburns/topz:db0fa58 /server --address = 0 .0.0.0:8080","title":"Sidecar pattern"},{"location":"swe/distributed/#resources","text":"Coursera course Article on synchronization in a distributed system http://label-schema.org/ https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236 https://eng.lyft.com/announcing-envoy-c-l7-proxy-and-communication-bus-92520b6c8191 https://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc https://cse.buffalo.edu/~demirbas/publications/cloudConsensus.pdf http://www.read.seas.harvard.edu/~kohler/class/08w-dsi/chandra07paxos.pdf https://medium.com/source-code/understanding-the-memcached-source-code-slab-i-9199de613762","title":"Resources"},{"location":"swe/http-caching/","text":"HTTP Caching https://medium.freecodecamp.org/http-caching-in-depth-part-1-a853c6af99db","title":"HTTP Caching"},{"location":"swe/http-caching/#http-caching","text":"https://medium.freecodecamp.org/http-caching-in-depth-part-1-a853c6af99db","title":"HTTP Caching"},{"location":"swe/microservices/","text":"Microservices","title":"Microservices"},{"location":"swe/microservices/#microservices","text":"","title":"Microservices"},{"location":"swe/naming/","text":"On Naming Resources https://www.slideshare.net/pirhilton/naming-guidelines-for-professional-programmers http://www.yourdictionary.com/diction4.html","title":"Naming"},{"location":"swe/naming/#on-naming","text":"","title":"On Naming"},{"location":"swe/naming/#resources","text":"https://www.slideshare.net/pirhilton/naming-guidelines-for-professional-programmers http://www.yourdictionary.com/diction4.html","title":"Resources"},{"location":"swe/observability/","text":"Observability Resources https://www.vividcortex.com/blog/monitoring-isnt-observability https://medium.com/@copyconstruct/monitoring-and-observability-8417d1952e1c https://codeascraft.com/2011/02/15/measure-anything-measure-everything/","title":"Observability"},{"location":"swe/observability/#observability","text":"","title":"Observability"},{"location":"swe/observability/#resources","text":"https://www.vividcortex.com/blog/monitoring-isnt-observability https://medium.com/@copyconstruct/monitoring-and-observability-8417d1952e1c https://codeascraft.com/2011/02/15/measure-anything-measure-everything/","title":"Resources"},{"location":"swe/others/","text":"Other Software Engineering Concepts Resource management When there are finite resources in your system, manage them explicitly. Be that memory, CPU, amount of connections to a database or incoming http connections. Back Pressure Back pressure When one component is struggling to keep-up, the system as a whole needs to respond in a sensible way. It is unacceptable for the component under stress to fail catastrophically or to drop messages in an uncontrolled fashion. Since it can\u2019t cope and it can\u2019t fail it should communicate the fact that it is under stress to upstream components and so get them to reduce the load. This back-pressure is an important feedback mechanism that allows systems to gracefully respond to load rather than collapse under it. The back-pressure may cascade all the way up to the user, at which point responsiveness may degrade, but this mechanism will ensure that the system is resilient under load, and will provide information that may allow the system itself to apply other resources to help distribute the load, see Elasticity. 1 Further reading: DZone article Spotify Engineering Memoization Memoization In computing, memoization or memoisation is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again. Memoization has also been used in other contexts (and for purposes other than speed gains), such as in simple mutually recursive descent parsing. Important Theories Theory of constraints Law of demeter Conway's law Little's law Commoditization Amdahl's Law Reactive Manifesto Wikipedia article on Memoization","title":"Others"},{"location":"swe/others/#other-software-engineering-concepts","text":"","title":"Other Software Engineering Concepts"},{"location":"swe/others/#resource-management","text":"When there are finite resources in your system, manage them explicitly. Be that memory, CPU, amount of connections to a database or incoming http connections.","title":"Resource management"},{"location":"swe/others/#back-pressure","text":"Back pressure When one component is struggling to keep-up, the system as a whole needs to respond in a sensible way. It is unacceptable for the component under stress to fail catastrophically or to drop messages in an uncontrolled fashion. Since it can\u2019t cope and it can\u2019t fail it should communicate the fact that it is under stress to upstream components and so get them to reduce the load. This back-pressure is an important feedback mechanism that allows systems to gracefully respond to load rather than collapse under it. The back-pressure may cascade all the way up to the user, at which point responsiveness may degrade, but this mechanism will ensure that the system is resilient under load, and will provide information that may allow the system itself to apply other resources to help distribute the load, see Elasticity. 1 Further reading: DZone article Spotify Engineering","title":"Back Pressure"},{"location":"swe/others/#memoization","text":"Memoization In computing, memoization or memoisation is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again. Memoization has also been used in other contexts (and for purposes other than speed gains), such as in simple mutually recursive descent parsing.","title":"Memoization"},{"location":"swe/others/#important-theories","text":"Theory of constraints Law of demeter Conway's law Little's law Commoditization Amdahl's Law Reactive Manifesto Wikipedia article on Memoization","title":"Important Theories"}]}