{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Joost van der Griendt's Software Development Docs","text":"<p>This is a collection of knowledge that I have gathered over the years. I find it helps me learn better if I write it down, and often use the docs at different customers as reference.</p>"},{"location":"#call-me-j","title":"Call me J","text":"<p>My full name is Joost van der Griendt, which unfortunately cannot be pronounced well in English.</p> <p>In order to facilitate non-Dutch speakers, you can refer to me as J (Jay).</p> <p>I've worked as a Java developer in the past, but currently I'm employed as a Solutions Architect for Platform Services at (VMware) Tanzu Labs, previously I worked as a Consultant at CloudBees. My day-to-day work involves helping clients with CI/CD, Kubernetes and Software Development Management. Or, in simple words, how to make it easy and less painful to get software to customers/clients that they want to pay for at scale.</p> <p>In my spare time I keep my development skills active by developing in Go and Java mostly. But I'm also a big fan of automating the creation and management of CI/CD (self-service) platforms.</p> <p>I'm a big fan of Open Source Software and when it makes sense, Free &amp; Free software. Which is also why this site is completely open, and open source as well.</p> <p>Info</p> <p>Curious how this site is build? Read my explanation here</p>"},{"location":"#tracker","title":"Tracker","text":"<p>Your browser will tell you there's a tracker. I'm curious to understand if people are reading my docs and if so, which pages.</p> <p>Feel free to block the tracker (Google Analytics), most browsers are able to do so.</p>"},{"location":"#main-topics","title":"Main Topics","text":"<ul> <li><code>CI/CD</code> (Continuous Integration / Continous Delivery)<ul> <li>Jenkins</li> <li>Jenkins X</li> <li>CloudBees Products (my current employer as of 2018)</li> </ul> </li> <li><code>Containers</code> (Docker, Kubernetes, ...)</li> <li><code>SWE</code>: Software Engineering in all its facets (building, maintaining, social aspects, psychology, etc.)</li> </ul>"},{"location":"#other-docs","title":"Other Docs","text":"<ul> <li>Breakdown of a Spring Boot + ReactJS Application</li> </ul>"},{"location":"#continuous-integration","title":"Continuous Integration","text":"<p>A good definition can be found here: http://www.martinfowler.com/articles/continuousIntegration.html</p> <p>Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\"</p>"},{"location":"#continuous-delivery","title":"Continuous Delivery","text":"<p>Continuous Delivery/deployment is the next step in getting yr software changes at the desired server in order to let your clients take a look at it. This article provides a good example of it: http://www.martinfowler.com/articles/continuousIntegration.html</p> <p>To do Continuous Integration you need multiple environments, one to run commit tests, one or more to run secondary tests. Since you are moving executables between these environments multiple times a day, you'll want to do this automatically. So it's important to have scripts that will allow you to deploy the application into any environment easily.</p>"},{"location":"blogs/docker-alternatives/","title":"Pipelines With Docker Alternatives","text":"<p>Building pipelines with Jenkins on Docker has been common for a while.</p> <p>But accessing the docker socket has always been a bit tricky. The easiest solution is directly mounting the docker socket of the host into your build container.</p> <p>However, this is a big security risk and something that is undesirable at best and, pretty dangerous at worst.</p> <p>When you're using an orchestrator such as Kubernetes - which is where Jenkins is currently best put to work - the problem gets worse. Not only do you open up security holes, you also mess up the schedular's insight into available resources, and mess with it's ability to keep your cluster in a correct state.</p> <p>In some sense, using docker directly in a Kubernetes cluster, is likened to running with scissors.</p>"},{"location":"blogs/docker-alternatives/#potential-alternatives","title":"Potential Alternatives","text":"<p>So, directly running docker containers via a docker engine inside your cluster is out. Let's look at some alternatives.</p> <ul> <li>Kubernetes Pod and External Node: the simple way out, use a cloud (such as EC2 Cloud) to provision a classic VM with docker engine as an agent and build there</li> <li>JIB: tool from Google, only works for Java applications and is supported directly from Gradle and Maven - Link</li> <li>Kaniko: tool from Google, works as 1-on-1 replacement for docker image build (except Windows Containers) - Link</li> <li>IMG: tool from Jess Frazelle to avoid building docker images with a root user involved Link</li> </ul>"},{"location":"blogs/docker-alternatives/#kubernetes-pod-and-external-node","title":"Kubernetes Pod and External Node","text":"<p>One of the most used cloud environments is AWS, so I created this solution with AWS's Amazon EC2 Plugin.</p> <p>Warning</p> <p>Unfortunately, you cannot combine the Kubernetes plugin with external nodes (none pod container nodes) in a <code>Declarative</code> pipeline. So you have to use <code>Scripted</code>.</p> <p>This can be done with various different cloud providers such as Digital Ocean, Google, AWS or Azure. This guide will use AWS, as it has the most mature <code>Jenkins Cloud</code> plugin.</p>"},{"location":"blogs/docker-alternatives/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS Account with rights to create AMI's and run EC2 instances</li> <li>Packer</li> <li>Jenkins with Amazon EC2 Plugin installed</li> </ul>"},{"location":"blogs/docker-alternatives/#steps","title":"Steps","text":"<ul> <li>create AMI with Packer</li> <li>install and configure Amazon EC2 plugin</li> <li>create a test pipeline</li> </ul>"},{"location":"blogs/docker-alternatives/#create-ami-with-packer","title":"Create AMI with Packer","text":"<p>Packer needs to be able to access EC2 API's and be able to spin up an EC2 instance and create an AMI out of it.</p>"},{"location":"blogs/docker-alternatives/#aws-setup-for-packer","title":"AWS setup for Packer","text":"<p>You need to configure two things:</p> <ul> <li>account details for Packer to use</li> <li>security group where your EC2 instances will be running with<ul> <li>this security group needs to open port <code>22</code></li> <li>both Packer and Jenkins will use this for their connection</li> </ul> </li> </ul> <pre><code>export AWS_DEFAULT_REGION=eu-west-1\nexport AWS_ACCESS_KEY_ID=XXX\nexport AWS_SECRET_ACCESS_KEY=XXX\n</code></pre> <pre><code>aws ec2 --profile myAwsProfile create-security-group \\\n    --description \"For building Docker images\" \\\n    --group-name docker\n\n{\n    \"GroupId\": \"sg-08079f78cXXXXXXX\"\n}\n</code></pre> <p>Export the security group ID.</p> <pre><code>export SG_ID=sg-08079f78cXXXXXXX\necho $SG_ID\n</code></pre>"},{"location":"blogs/docker-alternatives/#enable-port-22","title":"Enable port 22","text":"<pre><code>aws ec2 \\\n    --profile myAwsProfile \\\n    authorize-security-group-ingress \\\n    --group-name docker \\\n    --protocol tcp \\\n    --port 22 \\\n    --cidr 0.0.0.0/0\n</code></pre>"},{"location":"blogs/docker-alternatives/#packer-ami-definition","title":"Packer AMI definition","text":"<p>Here's an example definition for Packer for a Ubuntu 18.04 LTS base image with JDK 8 (required by Jenkins) and Docker.</p> <pre><code>{\n    \"builders\": [{\n        \"type\": \"amazon-ebs\",\n        \"region\": \"eu-west-1\",\n        \"source_ami_filter\": {\n            \"filters\": {\n                \"virtualization-type\": \"hvm\",\n                \"name\": \"*ubuntu-bionic-18.04-amd64-server-*\",\n                \"root-device-type\": \"ebs\"\n            },\n            \"owners\": [\"679593333241\"],\n            \"most_recent\": true\n        },\n        \"instance_type\": \"t2.micro\",\n        \"ssh_username\": \"ubuntu\",\n        \"ami_name\": \"docker\",\n        \"force_deregister\": true\n    }],\n    \"provisioners\": [{\n        \"type\": \"shell\",\n        \"inline\": [\n            \"sleep 15\",\n            \"sudo apt-get clean\",\n            \"sudo apt-get update\",\n            \"sudo apt-get install -y apt-transport-https ca-certificates nfs-common\",\n            \"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\",\n            \"sudo add-apt-repository \\\"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\\"\",\n            \"sudo add-apt-repository -y ppa:openjdk-r/ppa\",\n            \"sudo apt-get update\",\n            \"sudo apt-get install -y docker-ce\",\n            \"sudo usermod -aG docker ubuntu\",\n            \"sudo apt-get install -y openjdk-8-jdk\",\n            \"java -version\",\n            \"docker version\"\n        ]\n    }]\n}\n</code></pre> <p>Build the new AMI with packer.</p> <pre><code>packer build docker-ami.json\nexport AMI=ami-0212ab37f84e418f4\n</code></pre>"},{"location":"blogs/docker-alternatives/#ec2-key-pair","title":"EC2 Key Pair","text":"<p>Create EC2 key pair, this will be used by Jenkins to connect to the instances via ssh.</p> <pre><code>aws ec2 --profile myAwsProfile create-key-pair \\\n    --key-name jenkinsec2 \\\n    | jq -r '.KeyMaterial' \\\n    &gt;jenkins-ec2-proton.pem\n</code></pre>"},{"location":"blogs/docker-alternatives/#ec2-cloud-configuration","title":"EC2 Cloud Configuration","text":"<p>In a Jenkins' master main configuration, you add a new <code>cloud</code>. In this case, we will use a <code>ec2-cloud</code> so we can instantiate our EC2 VM's with docker.</p> <ul> <li>use EC2 credentials for initial connection</li> <li>use key (<code>.pem</code>) for VM connection (jenkins &lt;&gt; agent)</li> <li>configure the following:<ul> <li>AMI: ami-0212ab37f84e418f4</li> <li>availability zone: eu-west-1a</li> <li>VPC SubnetID: subnet-aa54XXXX</li> <li>Remote user: ubuntu</li> <li>labels: docker ubuntu linux</li> <li>SecurityGroup Name: (the id) sg-08079f78cXXXXXXX</li> <li>public ip = true</li> <li>connect via public ip = true</li> </ul> </li> </ul>"},{"location":"blogs/docker-alternatives/#pipeline","title":"Pipeline","text":"<pre><code>@Library('jenkins-pipeline-library@master') _\n\ndef scmVars\ndef label = \"jenkins-slave-${UUID.randomUUID().toString()}\"\n\npodTemplate(\n        label: label,\n        yaml: \"\"\"\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: kubectl\n    image: vfarcic/kubectl\n    command: [\"cat\"]\n    tty: true\n\"\"\"\n) {\n    node(label) {\n        node(\"docker\") {\n            stage('SCM &amp; Prepare') {\n                scmVars = checkout scm\n            }\n            stage('Lint') {\n                dockerfileLint()\n            }\n            stage('Build Docker') {\n                sh \"docker image build -t demo:rc-1 .\"\n            }\n            stage('Tag &amp; Push Docker') {\n                IMAGE = \"${DOCKER_IMAGE_NAME}\"\n                TAG = \"${DOCKER_IMAGE_TAG}\"\n                FULL_NAME = \"${FULL_IMAGE_NAME}\"\n\n                withCredentials([usernamePassword(credentialsId: \"dockerhub\", usernameVariable: \"USER\", passwordVariable: \"PASS\")]) {\n                    sh \"docker login -u $USER -p $PASS\"\n                }\n                sh \"docker image tag ${IMAGE}:${TAG} ${FULL_NAME}\"\n                sh \"docker image push ${FULL_NAME}\"\n            }\n        } // end node docker\n        stage('Prepare Pod') {\n            // have to checkout on our kubernetes pod aswell\n            checkout scm\n        }\n        stage('Check version') {\n            container('kubectl') {\n                sh 'kubectl version'\n            }\n        }\n    } // end node random label\n} // end pod def\n</code></pre>"},{"location":"blogs/docker-alternatives/#maven-jib","title":"Maven JIB","text":"<p>If you use Java with either Gradle or Maven, you can use JIB to create docker image without requiring a docker client or docker engine.</p> <p>JIB will communicate with DockerHub and Google's container registry for sending image layers back and forth. The last layer will be created by the JIB plugin itself, adding your self-executing Jar to the layer and a <code>Entrypoint</code> with the correct flags.</p> <p>For more information about the runtime options, see either jib-maven-plugin documentation or jib-gradle-plugin.</p>"},{"location":"blogs/docker-alternatives/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Java project build with Gradle or Maven</li> <li>Java project that can start it self, such as Spring Boot or Thorntail (previously Wildfly Swarm, from the JBoss family)</li> <li>able to build either gradle or maven applications</li> </ul> <p>The project used can be found at github.com/demomon/maven-spring-boot-demo.</p>"},{"location":"blogs/docker-alternatives/#steps_1","title":"Steps","text":"<ul> <li>configure the plugin for either Gradle or Maven</li> <li>build using an official docker image via the kubernetes pod template</li> </ul>"},{"location":"blogs/docker-alternatives/#pipeline_1","title":"Pipeline","text":"<p>Using a bit more elaborate pipeline example here.</p> <p>Using SonarCloud for static code analysis and then JIB to create a docker image. We could then use that image either directly in anything that runs docker or in the same cluster via a Helm Chart.</p> <pre><code>def scmVars\ndef tag\n\npipeline {\n    options {\n        buildDiscarder logRotator(artifactDaysToKeepStr: '5', artifactNumToKeepStr: '5', daysToKeepStr: '5', numToKeepStr: '5')\n    }\n    libraries {\n        lib('core@master')\n        lib('maven@master')\n    }\n    agent {\n        kubernetes {\n            label 'mypod'\n            defaultContainer 'jnlp'\n            yaml \"\"\"\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    some-label: some-label-value\nspec:\n  containers:\n  - name: maven\n    image: maven:3-jdk-11-slim\n    command:\n    - cat\n    tty: true\n\"\"\"\n        }\n    }\n    stages {\n        stage('Test versions') {\n            steps {\n                container('maven') {\n                    sh 'uname -a'\n                    sh 'mvn -version'\n                }\n            }\n        }\n        stage('Checkout') {\n            steps {\n                script {\n                    scmVars = checkout scm\n                }\n                gitRemoteConfigByUrl(scmVars.GIT_URL, 'githubtoken')\n                sh '''\n                git config --global user.email \"jenkins@jenkins.io\"\n                git config --global user.name \"Jenkins\"\n                '''\n            }\n        }\n        stage('Build') {\n            steps {\n                container('maven') {\n                    sh 'mvn clean verify -B -e'\n                }\n            }\n        }\n        stage('Version &amp; Analysis') {\n            parallel {\n                stage('Version Bump') {\n                    when { branch 'master' }\n                    environment {\n                        NEW_VERSION = gitNextSemverTagMaven('pom.xml')\n                    }\n                    steps {\n                        script {\n                            tag = \"${NEW_VERSION}\"\n                        }\n                        container('maven') {\n                            sh 'mvn versions:set -DnewVersion=${NEW_VERSION}'\n                        }\n                        gitTag(\"v${NEW_VERSION}\")\n                    }\n                }\n                stage('Sonar Analysis') {\n                    when {branch 'master'}\n                    environment {\n                        SONAR_HOST='https://sonarcloud.io'\n                        KEY='spring-maven-demo'\n                        ORG='demomon'\n                        SONAR_TOKEN=credentials('sonarcloud')\n                    }\n                    steps {\n                        container('maven') {\n                            sh '''mvn sonar:sonar \\\n                                -Dsonar.projectKey=${KEY} \\\n                                -Dsonar.organization=${ORG} \\\n                                -Dsonar.host.url=${SONAR_HOST} \\\n                                -Dsonar.login=${SONAR_TOKEN}\n                            '''\n                        }\n                    }\n                }\n            }\n        }\n        stage('Publish Artifact') {\n            when { branch 'master' }\n            environment {\n                DHUB=credentials('dockerhub')\n            }\n            steps {\n                container('maven') {\n                    // we should never come here if the tests have not run, as we run verify before\n                    sh 'mvn clean compile -B -e jib:build -Djib.to.auth.username=${DHUB_USR} -Djib.to.auth.password=${DHUB_PSW} -DskipTests'\n                }\n            }\n        }\n    }\n    post {\n        always {\n            cleanWs()\n        }\n    }\n}\n</code></pre>"},{"location":"blogs/docker-alternatives/#kaniko","title":"Kaniko","text":"<p>Google loves Kubernetes and Google prefers people building docker images in Kubernetes without Docker.</p> <p>As JIB is only available for Java projects, there's needs to be an alternative for any other usecase/programming language.</p> <p>The answer to that is Kaniko, a specialized Docker image to create Docker images.</p> <p>Kaniko isn't the most secure way to create docker images, it barely beats mounting a Docker Socket, or might even be worse if you ask others (such as Jess Frazelle). </p> <p>That said, it is gaining some traction being used by JenkinsX and having an example in Jenkins' Kubernetes Plugin.</p> <p>Info</p> <p>When building more than one image inside the kaniko container, make sure to use the <code>--cleanup</code> flag. So it cleans its temporary cache data before building the next image, as discussed in this google group.</p>"},{"location":"blogs/docker-alternatives/#prerequisites_2","title":"Prerequisites","text":""},{"location":"blogs/docker-alternatives/#steps_2","title":"Steps","text":"<ul> <li>Create docker registry secret</li> <li>Configure pod container template</li> <li>Configure stage</li> </ul>"},{"location":"blogs/docker-alternatives/#create-docker-registry-secret","title":"Create docker registry secret","text":"<p>This is an example for DockerHub inside the <code>build</code> namespace.</p> <pre><code>kubectl create secret docker-registry -n build regcred \\\n    --docker-server=index.docker.io \\\n    --docker-username=myDockerHubAccount \\\n    --docker-password=myDockerHubPassword \\\n    --docker-email=myDockerHub@Email.com\n</code></pre>"},{"location":"blogs/docker-alternatives/#example-ppeline","title":"Example Ppeline","text":"<p>Warning</p> <p>Although multi-stage <code>Dockerfile</code>'s are supported, it did fail in my case. So I created a second Dockerfile which is only for running the application (<code>Dockerfile.run</code>).</p> <pre><code>pipeline {\n    agent {\n        kubernetes {\n            //cloud 'kubernetes'\n            label 'kaniko'\n            yaml \"\"\"\nkind: Pod\nmetadata:\n  name: kaniko\nspec:\n  containers:\n  - name: golang\n    image: golang:1.11\n    command:\n    - cat\n    tty: true\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor:debug\n    imagePullPolicy: Always\n    command:\n    - /busybox/cat\n    tty: true\n    volumeMounts:\n      - name: jenkins-docker-cfg\n        mountPath: /root\n      - name: go-build-cache\n        mountPath: /root/.cache/go-build\n      - name: img-build-cache\n        mountPath: /root/.local\n  volumes:\n  - name: go-build-cache\n    emptyDir: {}\n  - name: img-build-cache\n    emptyDir: {}\n  - name: jenkins-docker-cfg\n    projected:\n      sources:\n      - secret:\n          name: regcred\n          items:\n            - key: .dockerconfigjson\n              path: .docker/config.json\n\"\"\"\n        }\n    }\n    stages {\n        stage('Checkout') {\n            steps {\n                git 'https://github.com/joostvdg/cat.git'\n            }\n        }\n        stage('Build') {\n            steps {\n                container('golang') {\n                    sh './build-go-bin.sh'\n                }\n            }\n        }\n        stage('Make Image') {\n            environment {\n                PATH = \"/busybox:$PATH\"\n            }\n            steps {\n                container(name: 'kaniko', shell: '/busybox/sh') {\n                    sh '''#!/busybox/sh\n                    /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=index.docker.io/caladreas/cat\n                    '''\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"blogs/docker-alternatives/#img","title":"IMG","text":"<p><code>img</code> is the brainchild of Jess Frazelle, a prominent figure in the container space.</p> <p>The goal is to be the safest and best way to build OCI compliant images, as outlined in her blog building container images securely on kubernetes.</p>"},{"location":"blogs/docker-alternatives/#not-working-for-me-yet","title":"Not working (for me) yet","text":"<p>It does not seem to work for me on AWS's EKS. To many little details with relation to runc, file permissions and other configuration.</p> <p>For those who want to give it a spin, here are some resources to take a look at.</p> <ul> <li>https://blog.jessfraz.com/post/building-container-images-securely-on-kubernetes/</li> <li>https://github.com/genuinetools/img</li> <li>https://github.com/opencontainers/runc</li> <li>https://git.j3ss.co/genuinetools/img/+/d05b3e4e10cd0e3c074ffb03dc22d7bb6cde1e78</li> </ul>"},{"location":"blogs/docker-alternatives/#pipeline-example","title":"Pipeline Example","text":"<pre><code>pipeline {\n    agent {\n        kubernetes {\n            label 'img'\n            yaml \"\"\"\nkind: Pod\nmetadata:\n  name: img\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/img: unconfined  \nspec:\n  containers:\n  - name: golang\n    image: golang:1.11\n    command:\n    - cat\n    tty: true\n  - name: img\n    workingDir: /home/jenkins\n    image: caladreas/img:0.5.1\n    imagePullPolicy: Always\n    securityContext:\n        rawProc: true\n        privileged: true\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n      - name: jenkins-docker-cfg\n        mountPath: /root\n  volumes:\n  - name: temp\n    emptyDir: {}\n  - name: jenkins-docker-cfg\n    projected:\n      sources:\n      - secret:\n          name: regcred\n          items:\n            - key: .dockerconfigjson\n              path: .docker/config.json\n\"\"\"\n        }\n    }\n    stages {\n        stage('Checkout') {\n            steps {\n                git 'https://github.com/joostvdg/cat.git'\n            }\n        }\n        stage('Build') {\n            steps {\n                container('golang') {\n                    sh './build-go-bin.sh'\n                }\n            }\n        }\n        stage('Make Image') {\n            steps {\n                container('img') {\n                    sh 'mkdir cache'\n                    sh 'img build -s ./cache -f Dockerfile.run -t caladreas/cat .'\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"blogs/dockercon-eu-2018/","title":"DockerCon EU 2018 - Recap","text":""},{"location":"blogs/dockercon-eu-2018/#generally-outline","title":"Generally outline","text":"<p>From my perspective, there were a few red threads throughout the conference.</p>"},{"location":"blogs/dockercon-eu-2018/#security-begins-at-the-developer","title":"Security begins at the Developer","text":"<p>A shift left of security, bringing the responsibility of knowing your dependencies and their quality firmly to the developer. Most of this tooling is still aimed at enterprises though, being part of paid solutions mostly. At least that which was shown at the conference.</p> <ul> <li>docker-assemble, that can build in an image from a Maven pom.xml and will include meta-data of all your dependencies (transitive included)</li> <li>JFrog X-Ray</li> <li>Docker EE tooling such as Docker Trusted Registry (DTR)</li> </ul>"},{"location":"blogs/dockercon-eu-2018/#broader-automation","title":"Broader Automation","text":"<p>More focus on the entire lifecycle of a system and not just an application. It seems people are starting to understand that doing CI/CD and Infrastructure As Code is not a single event for a single application. There is likely to be a few applications belonging together making a whole system which will land on more than one type of infrastructure and possibly more types of clusters.</p> <p>What we see is tools looking at either a broader scope, a higher level abstraction or more developer focussed (more love for the Dev in DevOps) to allow for easier integration with multiple platforms. For example, Pulumi will enable you to create any type of infrastructure - like Hashicorp's Terraform - but then in programming languages, you're used to (TypeScript, Python, Go).</p> <ul> <li>Pulumi</li> <li>Docker App</li> <li>CNAB</li> <li>Build-Kit</li> </ul>"},{"location":"blogs/dockercon-eu-2018/#containerization-influences-everything","title":"Containerization Influences Everything","text":"<p>Containerization has left deep and easy to spot imprints in our industry from startups building entirely on top of containers to programming languages changing their ways to stay relevant.</p> <p>There are new monitoring kings in the world, DataDog, Sysdig, Honeycomb.io and so on. They live and breathe containers and are not afraid of being thrown around different public clouds, networks and what not. In contrast to traditional monitoring tools, which are often bolting on container support and struggle with the dynamic nature of containerized clusters.</p> <p>Another extraordinary influence is that on the Java language. Declared dead a million times over and deemed obsolete in the container era due to its massive footprint in image size and runtime size. Both are being addressed, and we see a lot of work done on reducing footprint and static linking (JLink, Graal).</p> <p>The most significant influence might be on the software behemoth that has rejuvenated itself. Microsoft has sworn allegiance to open source, Linux and containers. Windows 2019 server can run container workloads natively and work as nodes alongside a Docker EE cluster - which can include Kubernetes workloads. The next step would be support for Kubernetes integration, and as in the case of Java, smaller container footprint.</p> <ul> <li>Java &amp; Docker</li> <li>Windows Container &amp; Windows Server Support</li> <li>Observability tools</li> <li>Kubernetes offerings everywhere...</li> </ul>"},{"location":"blogs/dockercon-eu-2018/#docker-build-with-build-kit","title":"Docker Build with Build-Kit","text":"<p>Instead of investing in improving docker image building via the Docker Client, Docker created a new API and client library.</p> <p>This library called BuildKit, is completely independent. With Docker 18.09, it is included in the Docker Client allowing anyone to use it as easily as the traditional <code>docker image build</code>.</p> <p>BuildKit is already used by some other tools, such as Buildah and IMG, and allows you to create custom DSL \"Frontends\". As long as the API of BuikdKit is adhered to, the resulting image will be OCI compliant.</p> <p>So further remarks below and how to use it.</p> <ul> <li>BuildKit</li> <li>In-Depth session Supercharged Docker Build with BuildKit</li> <li>Usable from Docker <code>18.09</code></li> <li>HighLights:<ul> <li>allows custom DSL for specifying image (BuildKit) to still be used with Docker client/daemon</li> <li>build cache for your own files during build, think Go, Maven, Gradle...</li> <li>much more optimized, builds less, quicker, with more cache in less time</li> <li>support mounts (cache) such as secrets, during build phase</li> </ul> </li> </ul> <pre><code># Set env variable to enable\n# Or configure docker's json config\nexport DOCKER_BUILDKIT=1\n</code></pre> <pre><code># syntax=docker/dockerfile:experimental\n#######################################\n## 1. BUILD JAR WITH MAVEN\nFROM maven:3.6-jdk-8 as BUILD\nWORKDIR /usr/src\nCOPY . /usr/src\n! RUN --mount=type=cache,target=/root/.m2/  mvn clean package -e\n#######################################\n## 2. BUILD NATIVE IMAGE WITH GRAAL\nFROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD\nWORKDIR /usr/src\nCOPY --from=BUILD /usr/src/ /usr/src\nRUN ls -lath /usr/src/target/\nCOPY /docker-graal-build.sh /usr/src\nRUN ./docker-graal-build.sh\nRUN ls -lath\n#######################################\n## 3. BUILD DOCKER RUNTIME IMAGE\nFROM alpine:3.8\nCMD [\"jpc-graal\"]\nCOPY --from=NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/\nRUN chmod +x /usr/local/bin/jpc-graal\n#######################################\n</code></pre> <p></p>"},{"location":"blogs/dockercon-eu-2018/#secure-your-kubernetes","title":"Secure your Kubernetes","text":"<ul> <li>https://www.openpolicyagent.org + admission controller</li> <li>Network Policies</li> <li>Service Accounts</li> </ul>"},{"location":"blogs/dockercon-eu-2018/#broader-cloud-automation","title":"Broader Cloud Automation","text":"<p>Two of the <code>broader cloud automation</code> initiatives that impressed me at DockerCon were Pulumi and CNAB.</p> <p>Where Pulumi is an attempt to provide a more developer-friendly alternative to Terraform, CNAB is an attempt to create an environment agnostic installer specification.</p> <p>Meaning, you could create a CNAB installer which uses Pulumi to install all required infrastructure, applications and other resources.</p>"},{"location":"blogs/dockercon-eu-2018/#cnab-cloud-native-application-bundle","title":"CNAB: cloud native application bundle","text":"<ul> <li>Bundle.json</li> <li>invocation image (oci) = installer</li> <li>https://cnab.io</li> <li>docker app implements it</li> <li>helm support</li> <li>https://github.com/deislabs</li> </ul>"},{"location":"blogs/dockercon-eu-2018/#install-an-implementation","title":"Install an implementation","text":"<p>There are currently two implementations - that I found. Duffle from DeisLabs - open source from Azure - and Docker App - From Docker Inc..</p>"},{"location":"blogs/dockercon-eu-2018/#duffle","title":"Duffle","text":"<ul> <li>create a new repo</li> <li>clone the repo</li> <li>init a duffle bundle</li> <li>copy duffle bundle data to our repo folder</li> </ul> <pre><code>git clone git@github.com:demomon/cnab-duffle-demo-1.git\nduffle create cnab-duffle-demo-2\nmv cnab-duffle-demo-2/cnab cnab-duffle-demo-1/\nmv cnab-duffle-demo-2/duffle.json cnab-duffle-demo-1/\n</code></pre> <ul> <li>edit our install file (<code>cnab/run</code>)</li> <li>build our duffle bundle (<code>duffle build .</code>)</li> </ul> <p>We can now inspect our bundle with duffle.</p> <pre><code>duffle show cnab-duffle-demo-1:0.1.0 -r\n\n-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\n{\n  \"name\": \"cnab-duffle-demo-1\",\n  \"version\": \"0.1.0\",\n  \"description\": \"A short description of your bundle\",\n  \"keywords\": [\n    \"cnab-duffle-demo-1\",\n    \"cnab\",\n    \"demo\",\n    \"joostvdg\"\n  ],\n  \"maintainers\": [\n    {\n      \"name\": \"Joost van der Griendt\",\n      \"email\": \"joostvdg@gmail.com\",\n      \"url\": \"https://joostvdg.github.io\"\n    }\n  ],\n  \"invocationImages\": [\n    {\n      \"imageType\": \"docker\",\n      \"image\": \"deislabs/cnab-duffle-demo-1-cnab:2965aad7406e1b651a98fffe1194fcaaec5e623c\"\n    }\n  ],\n  \"images\": null,\n  \"parameters\": null,\n  \"credentials\": null\n}\n-----BEGIN PGP SIGNATURE-----\n\nwsDcBAEBCAAQBQJcL3L8CRDgq4YEOipZ/QAAjDYMAJI5o66SteUP2o4HsbVk+Viw\n3Fd874vSVmpPKcmN3tRCEDWGdMdvqQiirDpa//ghx4y5oFTahK2ihQ35GbJLlq8S\nv9/CK6CKGJtp5g38s2LZrKIvESzEF2lTXwHB03PG8PJ37iWiYkHkxvMpyzded3Rs\n4d+VgUnip0Cre7DemaUaz5+fTQjs88WNTIhqPg47YvgqFXV0s1y7yN3RTLr3ohQ1\n9mkw87nWfOD+ULpbCUaq9FhNZ+v4dK5IZcWlkyv+yrtguyBBiA3MC54ueVBAdFCl\n2OhxXgZjbBHPfQPV1mPqCQudOsWjK/+gqyNb6KTzKrAnyrumVQli/C/8BVk/SRC/\nGS2o4EdTS2lfREc2Gl0/VTmMkqzFZZhWd7pwt/iMjl0bICFehSU0N6OqN1d+v6Sq\nvWIZ5ppxt1NnCzp05Y+NRfVZOxBc2xjYTquFwIa/+qGPrmXBKamw/irjmCOndvx+\nl1tf/g0UVSQI2R2/19svl7dlMkYpDdlth1YGgZi/Hg==\n=8Xwi\n-----END PGP SIGNATURE-----\n</code></pre>"},{"location":"blogs/dockercon-eu-2018/#demo","title":"Demo","text":"<p>I've created a demo on GitHub: github.com/demomon/cnab-duffle-demo-1</p> <p>Its goal is to install CloudBees Core and its prerequisites in a (GKE) Kubernetes cluster.</p> <p>It wraps a Go (lang) binary that will execute the commands, for which you can find the source code on GitHub.</p>"},{"location":"blogs/dockercon-eu-2018/#components","title":"Components","text":"<p>A CNAB bundle has some components by default, for this demo we needed the following:</p> <ul> <li>duffle.json: Duffle configuration file</li> <li>Dockerfile: the CNAB installer runtime</li> <li>run(script): the installer script</li> <li>kgi(binary): the binary executable from my k8s-gitops-installer code, that we will leverage for the installation</li> </ul>"},{"location":"blogs/dockercon-eu-2018/#dockerfile","title":"Dockerfile","text":"<p>The installer tool (<code>kgi</code>) requires <code>Helm</code> and <code>Kubectl</code>, so we a Docker image that has those. As we might end up packaging the entire image as part of the full CNAB package, it should also be based on Alpine (or similar minimal Linux).</p> <p>There seems to be one very well maintained and widely used (according to GitHub and Dockerhub stats): <code>dtzar/helm-kubect</code>. So no need to roll our own.</p> <pre><code>FROM dtzar/helm-kubectl:2.12.1\nCOPY Dockerfile /cnab/Dockerfile\nCOPY app /cnab/app\nCOPY kgi /usr/bin\nRUN ls -lath /cnab/app/\nRUN kgi --help\nCMD [\"/cnab/app/run\"]\n</code></pre>"},{"location":"blogs/dockercon-eu-2018/#dufflejson","title":"duffle.json","text":"<p>The only thing we need to add beyond the auto-generated file, is the credentials section.</p> <pre><code>\"credentials\": {\n    \"kubeconfig\": {\n        \"path\": \"/cnab/app/kube-config\"\n    }\n}\n</code></pre>"},{"location":"blogs/dockercon-eu-2018/#kgi","title":"kgi","text":"<p>I pre-build a binary suitable for Linux that works in Alpine and included it in the CNAB folder.</p>"},{"location":"blogs/dockercon-eu-2018/#run-script","title":"run script","text":"<p>First thing we need to make sure, is to configure the Kubeconfig location.</p> <pre><code>export KUBECONFIG=\"/cnab/app/kube-config\"\n</code></pre> <p>This should match what we defined in the <code>duffle.json</code> configuration - as you might expect - to make sure it gets bound in the right location. The <code>kubectl</code> command now knows which file to use.</p> <p>For the rest, we can do what we want, but convention tells us we need at least support <code>status</code>, <code>install</code> and <code>uninstall</code>. I'm lazy and only implemented <code>install</code> at the moment.</p> <p>In the install action, we will use the <code>kgi</code> executable to install CloudBees Core and it's pre-requisites.</p> <p><pre><code>action=$CNAB_ACTION\n\ncase $action in\n    install)\n        echo \"[Install]\"\n        kgi validate kubectl\n    ;;\nesac\n</code></pre> For the rest, I recommend you look at the sources.</p>"},{"location":"blogs/dockercon-eu-2018/#run-the-demo","title":"Run the demo","text":"<p>First, we have to build the bundle.</p> <pre><code>duffle build .\n</code></pre> <p>Once the build succeeded, we can create a credentials configuration. This will be a separate configuration file managed by Duffle. This configuration config must then be used with any installation that requires it - which makes it reusable as well.</p> <p>We have to populate it with the credential. In this case a path to a kube-config file. If you do not have one that you can export - e.g. based on GCloud - you can create a new user/certificate with a script.</p> <p>This is taken from gravitational, which were nice enough to create a script for doing so.</p> <p>You can find the script on GitHub (get-kubeconfig.sh).</p> <p>Once you have that, store the end result at a decent place and configure it as your credential.</p> <pre><code>duffle creds generate demo1 cnab-duffle-demo-1\n</code></pre> <p>With the above command we can create a credential config object based on the build bundle <code>cnab-duffle-demo-1</code>. The credential object will be <code>demo-1</code>, which we can now use for installing.</p> <pre><code>duffle install demo1 cnab-duffle-demo-1:1.0.0 -c demo1\n</code></pre>"},{"location":"blogs/dockercon-eu-2018/#further-reading","title":"Further reading","text":"<ul> <li>Howto guide on creating a Duffle Bundle</li> <li>Howto on handling credentials</li> <li>Wordpress with Kubernetes and AWS demo by Bitnami</li> <li>Example bundles</li> </ul>"},{"location":"blogs/dockercon-eu-2018/#pulumi","title":"Pulumi","text":"<p>Just pointing to the documentation and GitHub repository is a bit boring and most of all, lazy. So I've gone through the trouble to create a demo project that shows what Pulumi can do.</p> <p>One of the things to keep in mind is that Pulumi keeps its state in a remote location by default - Pulumi's SaaS service. Much like using an S3 bucket to store your Terraform state.</p> <p>The goal of the demo is to show what you can do with Pulumi. My personal opinion is that Pulumi is excellent for managing entire systems as code. From public cloud infrastructure to public open source resources right down to your applications and their configurations.</p> <p>The system that I've written with Pulumi is a CI/CD system running in a GKE cluster. The system contains Jenkins, Artifactory and a custom LDAP server.</p> <p>To make it interesting, we'll use different aspects of Pulumi to create the system. We will create the GKE cluster via Pulumi's wrapper for the <code>gcloud</code> cli - in a similar way as JenkinsX does -, install Jenkins and Artifactory via Helm and install the LDAP server via Pulumi's Kubernetes resource classes.</p>"},{"location":"blogs/dockercon-eu-2018/#steps-taken","title":"Steps taken","text":"<ul> <li>For more info Pulumi.io</li> <li>install: <code>brew install pulumi</code></li> <li>clone demo: <code>git clone https://github.com/demomon/pulumi-demo-1</code></li> <li>init stack: <code>pulumi stack init demomon-pulumi-demo-1</code><ul> <li>connect to GitHub</li> </ul> </li> <li>set kubernetes config <code>pulumi config set kubernetes:context gke_ps-dev-201405_europe-west4_joostvdg-reg-dec18-1</code></li> <li><code>pulumi config set isMinikube false</code></li> <li>install npm resources: <code>npm install</code> (I've used the TypeScript demo's)</li> <li><code>pulumi config set username administrator</code></li> <li><code>pulumi config set password 3OvlgaockdnTsYRU5JAcgM1o --secret</code></li> <li><code>pulumi preview</code></li> <li>incase pulumi loses your stack: <code>pulumi stack select demomon-pulumi-demo-1</code></li> <li><code>pulumi destroy</code></li> </ul> <p>Based on the following demo's:</p> <ul> <li>https://pulumi.io/quickstart/kubernetes/tutorial-exposed-deployment.html</li> <li>https://github.com/pulumi/examples/tree/master/kubernetes-ts-jenkins</li> </ul>"},{"location":"blogs/dockercon-eu-2018/#artifactory-via-helm","title":"Artifactory via Helm","text":"<p>To install the helm charts of Artifactory using Helm, we will first need to add JFrog's repository.</p> <ul> <li><code>helm repo add jfrog https://charts.jfrog.io</code></li> <li><code>helm repo update</code></li> </ul>"},{"location":"blogs/dockercon-eu-2018/#gke-cluster","title":"GKE Cluster","text":"<p>Below is the code for the cluster.</p> <pre><code>import * as gcp from \"@pulumi/gcp\";\nimport * as k8s from \"@pulumi/kubernetes\";\nimport * as pulumi from \"@pulumi/pulumi\";\nimport { nodeCount, nodeMachineType, password, username } from \"./gke-config\";\n\nexport const k8sCluster = new gcp.container.Cluster(\"gke-cluster\", {\n    name: \"joostvdg-dec-2018-pulumi\",\n    initialNodeCount: nodeCount,\n    nodeVersion: \"latest\",\n    minMasterVersion: \"latest\",\n    nodeConfig: {\n        machineType: nodeMachineType,\n        oauthScopes: [\n            \"https://www.googleapis.com/auth/compute\",\n            \"https://www.googleapis.com/auth/devstorage.read_only\",\n            \"https://www.googleapis.com/auth/logging.write\",\n            \"https://www.googleapis.com/auth/monitoring\"\n        ],\n    },\n});\n</code></pre>"},{"location":"blogs/dockercon-eu-2018/#gke-config","title":"GKE Config","text":"<p>As you could see, we import variables from a configuration file <code>gke-config</code>.</p> <pre><code>import { Config } from \"@pulumi/pulumi\";\nconst config = new Config();\nexport const nodeCount = config.getNumber(\"nodeCount\") || 3;\nexport const nodeMachineType = config.get(\"nodeMachineType\") || \"n1-standard-2\";\n// username is the admin username for the cluster.\nexport const username = config.get(\"username\") || \"admin\";\n// password is the password for the admin user in the cluster.\nexport const password = config.require(\"password\");\n</code></pre>"},{"location":"blogs/dockercon-eu-2018/#kubeconfig","title":"Kubeconfig","text":"<p>As you probably will want to install other things (Helm charts, services) inside this cluster, we need to make sure we get the <code>kubeconfig</code> file of our cluster - which is yet to be created. Below is the code - courtesy of Pulumi's GKE Example - that generates and exports the Kubernetes Client Configuration.</p> <p>This client configuration can then be used by Helm charts or other Kubernetes services. Pulumi will then also understand it depends on the cluster and create/update it first before moving on to the others.</p> <pre><code>// Manufacture a GKE-style Kubeconfig. Note that this is slightly \"different\" because of the way GKE requires\n// gcloud to be in the picture for cluster authentication (rather than using the client cert/key directly).\nexport const k8sConfig = pulumi.\n    all([ k8sCluster.name, k8sCluster.endpoint, k8sCluster.masterAuth ]).\n    apply(([ name, endpoint, auth ]) =&gt; {\n        const context = `${gcp.config.project}_${gcp.config.zone}_${name}`;\n        return `apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: ${auth.clusterCaCertificate}\n    server: https://${endpoint}\n  name: ${context}\ncontexts:\n- context:\n    cluster: ${context}\n    user: ${context}\n  name: ${context}\ncurrent-context: ${context}\nkind: Config\npreferences: {}\nusers:\n- name: ${context}\n  user:\n    auth-provider:\n      config:\n        cmd-args: config config-helper --format=json\n        cmd-path: gcloud\n        expiry-key: '{.credential.token_expiry}'\n        token-key: '{.credential.access_token}'\n      name: gcp\n`;\n    });\n\n// Export a Kubernetes provider instance that uses our cluster from above.\nexport const k8sProvider = new k8s.Provider(\"gkeK8s\", {\n    kubeconfig: k8sConfig,\n});\n</code></pre>"},{"location":"blogs/dockercon-eu-2018/#pulumi-gcp-config","title":"Pulumi GCP Config","text":"<ul> <li>https://github.com/pulumi/examples/blob/master/gcp-ts-gke/README.md</li> </ul> <pre><code>export GCP_PROJECT=...\nexport GCP_ZONE=europe-west4-a\nexport CLUSTER_PASSWORD=...\nexport GCP_SA_NAME=...\n</code></pre> <p>Make sure you have a Google SA (Service Account) by that name first, as you can read here. For me it worked best to NOT set any environment variables mentioned. They invairably caused authentication or authorization issues. Just make sure the SA account and it's credential file (see below) are authorized and the <code>gcloud</code> cli works.</p> <pre><code>gcloud iam service-accounts keys create gcp-credentials.json \\\n    --iam-account ${GCP_SA_NAME}@${GCP_PROJECT}.iam.gserviceaccount.com\ngcloud auth activate-service-account --key-file gcp-credentials.json\ngcloud auth application-default login\n</code></pre> <pre><code>pulumi config set gcp:project ${GCP_PROJECT}\npulumi config set gcp:zone ${GCP_ZONE}\npulumi config set password --secret ${CLUSTER_PASSWORD}\n</code></pre>"},{"location":"blogs/dockercon-eu-2018/#post-cluster-creation","title":"Post Cluster Creation","text":"<pre><code>gcloud container clusters get-credentials joostvdg-dec-2018-pulumi\nkubectl create clusterrolebinding cluster-admin-binding  --clusterrole cluster-admin  --user $(gcloud config get-value account)\n</code></pre>"},{"location":"blogs/dockercon-eu-2018/#install-failed","title":"Install failed","text":"<p>Failed to install <code>kubernetes:rbac.authorization.k8s.io:Role         artifactory-artifactory</code>.</p> <p>Probably due to missing rights, so probably have to execute the admin binding before the helm charts.</p> <pre><code>error: Plan apply failed: roles.rbac.authorization.k8s.io \"artifactory-artifactory\" is forbidden: attempt to grant extra privileges: ...\n</code></pre>"},{"location":"blogs/dockercon-eu-2018/#helm-charts","title":"Helm Charts","text":"<p>Using Pulumi to install a Helm Chart feels a bit like adding layers of wrapping upon wrapping. The power of Pulumi becomes visible when using more than one related service on the same cluster - for example a SDLC Tool Chain.</p> <p>This example application installs two helm charts, Jenkins and Artifactory, on a GKE cluster that is also created and managed by Pulumi.</p> <p>Below is an example of installing a Helm chart of Jenkins, where we provide the Kubernetes config from the GKE cluster as Provider. This way, Pulumi knows it must install the helm chart in that GKE cluster and not in the current Kubeconfig.</p> <pre><code>import { k8sProvider, k8sConfig } from \"./gke-cluster\";\n\nconst jenkins = new k8s.helm.v2.Chart(\"jenkins\", {\n    repo: \"stable\",\n    version: \"0.25.1\",\n    chart: \"jenkins\",\n    }, { \n        providers: { kubernetes: k8sProvider }\n    }\n);\n</code></pre>"},{"location":"blogs/dockercon-eu-2018/#deployment-service","title":"Deployment &amp; Service","text":"<p>First, make sure you have an interface for the configuration arguments.</p> <pre><code>export interface LdapArgs {\n    readonly name: string,\n    readonly imageName: string,\n    readonly imageTag: string\n}\n</code></pre> <p>Then, create a exportable Pulumi resource class that can be reused.</p> <pre><code>export class LdapInstallation extends pulumi.ComponentResource {\n    public readonly deployment: k8s.apps.v1.Deployment;\n    public readonly service: k8s.core.v1.Service;\n\n    // constructor\n}\n</code></pre> <p>Inside the constructor placehold we will create a constructor method. It will do all the configuration we need to do for this resource, in this case a Kubernetes Service and Deployment.</p> <pre><code>constructor(args: LdapArgs) {\n    super(\"k8stypes:service:LdapInstallation\", args.name, {});\n    const labels = { app: args.name };\n    const name = args.name\n}\n</code></pre> <p>First Kubernetes resource to create is a container specification for the Deployment.</p> <pre><code>const container: k8stypes.core.v1.Container = {\n    name,\n    image: args.imageName + \":\" + args.imageTag,\n    resources: {\n        requests: { cpu: \"100m\", memory: \"200Mi\" },\n        limits: { cpu: \"100m\", memory: \"200Mi\" },\n    },\n    ports: [{\n            name: \"ldap\",containerPort: 1389,\n        },\n    ]\n};\n</code></pre> <p>As the configuration arguments can be any TypeScript type, you can allow people to override entire segments (such as Resources). Which you would do as follows:</p> <pre><code>    resources: args.resources || {\n        requests: { cpu: \"100m\", memory: \"200Mi\" },\n        limits: { cpu: \"100m\", memory: \"200Mi\" },\n    },\n</code></pre> <p>The Deployment and Service construction are quite similar.</p> <pre><code>this.deployment = new k8s.apps.v1.Deployment(args.name, {\n    spec: {\n        selector: { matchLabels: labels },\n        replicas: 1,\n        template: {\n            metadata: { labels: labels },\n            spec: { containers: [ container ] },\n        },\n    },\n},{ provider: cluster.k8sProvider });\n</code></pre> <pre><code>this.service = new k8s.core.v1.Service(args.name, {\n    metadata: {\n        labels: this.deployment.metadata.apply(meta =&gt; meta.labels),\n    },\n    spec: {\n        ports: [{\n                name: \"ldap\", port: 389, targetPort: \"ldap\" , protocol: \"TCP\"\n            },\n        ],\n        selector: this.deployment.spec.apply(spec =&gt; spec.template.metadata.labels),\n        type: \"ClusterIP\",\n    },\n}, { provider: cluster.k8sProvider });\n</code></pre>"},{"location":"blogs/dockercon-eu-2018/#jfrog-jenkins-challenge","title":"JFrog Jenkins Challenge","text":"<p>Visited the stand of JFrog where they had stories about two main products: Artifactory and X-Ray.</p> <p>For both there is a Challenge, an X-Ray Challenge and a Jenkins &amp; Artifactory Challenge.</p>"},{"location":"blogs/dockercon-eu-2018/#jenkins-challenge","title":"Jenkins Challenge","text":"<p>The instructions for the Challenge were simply, follow what is stated in their GitHub repository and email a screenshot of the result.</p> <p>The instruction were as follows:</p> <ol> <li>Get an Artifactory instance (you can start a free trial on prem or in the cloud)</li> <li>Install Jenkins</li> <li>Install Artifactory Jenkins Plugin</li> <li>Add Artifactory credentials to Jenkins Credentials</li> <li>Create a new pipeline job</li> <li>Use the Artifactory Plugin DSL documentation to complete the following script:</li> </ol> <p>With a Scripted Pipeline as starting point:</p> <pre><code>node {\n    def rtServer\n    def rtGradle\n    def buildInfo\n    stage('Preparation') {\n        git 'https://github.com/jbaruch/gradle-example.git'\n        // create a new Artifactory server using the credentials defined in Jenkins \n        // create a new Gradle build\n        // set the resolver to the Gradle build to resolve from Artifactory\n        // set the deployer to the Gradle build to deploy to Artifactory\n        // declare that your gradle script does not use Artifactory plugin\n        // declare that your gradle script uses Gradle wrapper\n    }\n    stage('Build') {\n        //run the artifactoryPublish gradle task and collect the build info\n    }\n    stage('Publish Build Info') {\n        //collect the environment variables to build info\n        //publish the build info\n    }\n}\n</code></pre> <p>I don't like scripted, so I opted for Declarative with Jenkins in Kubernetes with the Jenkins Kubernetes plugin.</p> <p>Steps I took:</p> <ul> <li>get a trial license from the JFrog website</li> <li>install Artifactory<ul> <li>and copy in the license when prompted</li> <li>change admin password</li> <li>create local maven repo 'libs-snapshot-local'</li> <li>create remote maven repo 'jcenter' (default remote value is jcenter, so only have to set the name)</li> </ul> </li> <li>install Jenkins<ul> <li>Artifactory plugin</li> <li>Kubernetes plugin</li> </ul> </li> <li>add Artifactory username/password as credential in Jenkins</li> <li>create a gradle application (Spring boot via start.spring.io) which you can find here</li> <li>create a Jenkinsfile</li> </ul>"},{"location":"blogs/dockercon-eu-2018/#installing-artifactory","title":"Installing Artifactory","text":"<p>I installed Artifactory via Helm. JFrog has their own Helm repository - of course, would weird otherwise tbh - and you have to add that first.</p> <pre><code>helm repo add jfrog https://charts.jfrog.io\nhelm install --name artifactory stable/artifactory\n</code></pre>"},{"location":"blogs/dockercon-eu-2018/#jenkinsfile","title":"Jenkinsfile","text":"<p>This uses the Gradle wrapper - as per instructions in the challenge.</p> <p>So we can use the standard JNLP container, which is default, so <code>agent any</code> will do.</p> <pre><code>pipeline {\n    agent any\n    environment {\n        rtServer  = ''\n        rtGradle  = ''\n        buildInfo = ''\n        artifactoryServerAddress = 'http://..../artifactory'\n    }\n    stages {\n        stage('Test Container') {\n            steps {\n                container('gradle') {\n                    sh 'which gradle'\n                    sh 'uname -a'\n                    sh 'gradle -version'\n                }\n            }\n        }\n        stage('Checkout'){\n            steps {\n                git 'https://github.com/demomon/gradle-jenkins-challenge.git'\n            }\n        }\n        stage('Preparation') {\n            steps {\n                script{\n                    // create a new Artifactory server using the credentials defined in Jenkins \n                    rtServer = Artifactory.newServer url: artifactoryServerAddress, credentialsId: 'art-admin'\n\n                    // create a new Gradle build\n                    rtGradle = Artifactory.newGradleBuild()\n\n                    // set the resolver to the Gradle build to resolve from Artifactory\n                    rtGradle.resolver repo:'jcenter', server: rtServer\n\n                    // set the deployer to the Gradle build to deploy to Artifactory\n                    rtGradle.deployer repo:'libs-snapshot-local',  server: rtServer\n\n                    // declare that your gradle script does not use Artifactory plugin\n                    rtGradle.usesPlugin = false\n\n                    // declare that your gradle script uses Gradle wrapper\n                    rtGradle.useWrapper = true\n                }\n            }\n        }\n        stage('Build') {\n            steps {\n                script {\n                    //run the artifactoryPublish gradle task and collect the build info\n                    buildInfo = rtGradle.run buildFile: 'build.gradle', tasks: 'clean build artifactoryPublish'\n                }\n            }\n        }\n        stage('Publish Build Info') {\n            steps {\n                script {\n                    //collect the environment variables to build info\n                    buildInfo.env.capture = true\n                    //publish the build info\n                    rtServer.publishBuildInfo buildInfo\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"blogs/dockercon-eu-2018/#jenkinsfile-without-gradle-wrapper","title":"Jenkinsfile without Gradle Wrapper","text":"<p>I'd rather not install the Gradle tool if I can just use a pre-build container with it.</p> <p>Unfortunately, to use it correctly with the Artifactory plugin and a Jenkins Kubernetes plugin, we need to do two things.</p> <ol> <li>create a <code>Gradle</code> Tool in the Jenkins master<ul> <li>because the Artifactory plugin expects a <code>Jenkins Tool</code> object, not a location</li> <li>Manage Jenkins -&gt; Global Tool Configuration -&gt; Gradle -&gt; Add</li> <li>As value supply <code>/usr</code>, the Artifactory build will add <code>/gradle/bin</code> to it automatically</li> </ul> </li> <li>set the user of build Pod to id <code>1000</code> explicitly<ul> <li>else the build will not be allowed to touch files in <code>/home/jenkins/workspace</code></li> </ul> </li> </ol> <pre><code>pipeline {\n    agent {\n        kubernetes {\n        label 'mypod'\n        yaml \"\"\"apiVersion: v1\nkind: Pod\nspec:\n  securityContext:\n    runAsUser: 1000\n    fsGroup: 1000\n  containers:\n  - name: gradle\n    image: gradle:4.10-jdk-alpine\n    command: ['cat']\n    tty: true\n\"\"\"\n        }\n    }\n    environment {\n        rtServer  = ''\n        rtGradle  = ''\n        buildInfo = ''\n        CONTAINER_GRADLE_TOOL = '/usr/bin/gradle'\n    }\n    stages {\n        stage('Test Container') {\n            steps {\n                container('gradle') {\n                    sh 'which gradle'\n                    sh 'uname -a'\n                    sh 'gradle -version'\n                }\n            }\n        }\n        stage('Checkout'){\n            steps {\n                // git 'https://github.com/demomon/gradle-jenkins-challenge.git'\n        checkout scm\n            }\n        }\n        stage('Preparation') {\n            steps {\n                script{\n                    // create a new Artifactory server using the credentials defined in Jenkins \n                    rtServer = Artifactory.newServer url: 'http://35.204.238.14/artifactory', credentialsId: 'art-admin'\n\n                    // create a new Gradle build\n                    rtGradle = Artifactory.newGradleBuild()\n\n                    // set the resolver to the Gradle build to resolve from Artifactory\n                    rtGradle.resolver repo:'jcenter', server: rtServer\n\n                    // set the deployer to the Gradle build to deploy to Artifactory\n                    rtGradle.deployer repo:'libs-snapshot-local',  server: rtServer\n\n                    // declare that your gradle script does not use Artifactory plugin\n                    rtGradle.usesPlugin = false\n\n                    // declare that your gradle script uses Gradle wrapper\n                    rtGradle.useWrapper = true\n                }\n            }\n        }\n        stage('Build') {\n            //run the artifactoryPublish gradle task and collect the build info\n            steps {\n                script {\n                    buildInfo = rtGradle.run buildFile: 'build.gradle', tasks: 'clean build artifactoryPublish'\n                }\n            }\n        }\n        stage('Publish Build Info') {\n            //collect the environment variables to build info\n            //publish the build info\n            steps {\n                script {\n                    buildInfo.env.capture = true\n                    rtServer.publishBuildInfo buildInfo\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"blogs/dockercon-eu-2018/#docker-security-standards","title":"Docker security &amp; standards","text":"<ul> <li>security takes place in every layer/lifecycle phase</li> <li>for scaling, security needs to be part of developer's day-to-day</li> <li>as everything is code, anything part of the sdlc should be secure and auditable</li> <li>use an admission controller</li> <li>network policies</li> <li>automate your security processes</li> <li>expand your security automation by adding learnings</li> </ul>"},{"location":"blogs/dockercon-eu-2018/#docker-java-cicd","title":"Docker &amp; Java &amp; CICD","text":"<ul> <li>telepresence</li> <li>Distroless (google mini os)</li> <li>OpenJ9</li> <li>Portala (for jdk 12)</li> <li>wagoodman/dive</li> <li>use jre for the runtime instead of jdk</li> <li>buildkit can use mounttarget for local caches</li> <li>add labels with Metadata (depency trees)</li> <li>grafeas &amp; kritis</li> <li>FindSecBugs</li> <li>org.owasp:dependency-check-maven</li> <li>arminc/clair-scanner</li> <li>jlink = in limbo</li> </ul>"},{"location":"blogs/dockercon-eu-2018/#docker-windows","title":"Docker &amp; Windows","text":"<ul> <li>specific base images for different use cases</li> <li>Docker capabilities heavily depend on Windows Server version</li> </ul>"},{"location":"blogs/dockercon-eu-2018/#other","title":"Other","text":""},{"location":"blogs/dockercon-eu-2018/#docker-pipeline","title":"Docker pipeline","text":"<ul> <li>Dind + privileged</li> <li>mount socket</li> <li>windows &amp; linux</li> <li>Windows build agent provisioning with docker EE &amp; Jenkins</li> <li>Docker swarm update_config</li> </ul>"},{"location":"blogs/dockercon-eu-2018/#idea-build-a-dynamic-cicd-platform-with-kubernetes","title":"Idea: build a dynamic ci/cd platform with kubernetes","text":"<ul> <li>jenkins evergreen + jcasc</li> <li>kubernetes plugin</li> <li>gitops pipeline</li> <li>AKS + virtual kubelet + ACI</li> <li>Jenkins + EC2 Pluging + ECS/Fargate</li> <li>jenkins agent as ecs task (fargate agent)</li> <li>docker on windows, only on ECS</li> </ul>"},{"location":"blogs/dockercon-eu-2018/#apply-diplomacy-to-code-review","title":"Apply Diplomacy to Code Review","text":"<ul> <li>apply diplomacy to code review</li> <li>always positive</li> <li>remove human resistantance with inclusive language</li> <li>improvement focused</li> <li>persist, kindly</li> </ul>"},{"location":"blogs/dockercon-eu-2018/#citizens-bank-journey","title":"Citizens Bank journey","text":"<ul> <li>started with swarm, grew towards kubernetes (ucp)</li> <li>elk stack, centralised operations cluster</li> </ul>"},{"location":"blogs/dockercon-eu-2018/#docker-ee-assemble","title":"Docker EE - Assemble","text":"<p>Docker EE now has a binary called <code>docker-assemble</code>. This allows you to build a Docker image directly from something like a pom.xml, much like JIB.</p> <p></p>"},{"location":"blogs/dockercon-eu-2018/#other_1","title":"Other","text":""},{"location":"blogs/gitops-pipeline/","title":"GitOps Pipeline with Jenkins on Kubernetes","text":""},{"location":"blogs/gitops-pipeline/#missing-pieces","title":"Missing pieces","text":"<ul> <li>secrets via Vault</li> <li>service mesh</li> <li>infrastructure as code (around the clusters)</li> <li>multiple clusters<ul> <li>Pulumi</li> <li>Rancher</li> </ul> </li> </ul>"},{"location":"blogs/graceful-shutdown/","title":"Gracefully Shutting Down Applications in Docker","text":"<p>I'm not sure about you, but I prefer it when my neighbors leave our shared spaces clean and don't take up parking spaces when they don't need them.</p> <p>Imagine you live in an apartment complex with the above-mentioned parking lot. Some tenants go away and never come back. If nothing is done to clean up after them - to reclaim their apartment and parking space - then after some time, more and more apartments are unavailable for no reason, and the parking lot fills with cars which belong to no one.</p> <p>Some tenants did not get a parking lot and are getting frustrated that none are becoming available. When they moved in, they were told that when others leave, they would be next in line. While they're waiting, they have to park outside the complex. Eventually, the entrance gets blocked and no one can enter or leave. The end result is a completely unlivable apartment block with trapped tenants - never to be seen or heard from again.</p> <p>If you agree with me that when a tenant leaves, they should clean the apartment and free the parking spot to make it ready for the next inhabitant; then please read on. We're going to dive into the equivalent of doing this with containers.</p> <p>We will explore running our containers with Docker (run, compose, swarm) and Kubernetes. Even if you use another way to run your containers, this article should provide you with enough insight to get you on your way.</p>"},{"location":"blogs/graceful-shutdown/#the-case-for-graceful-shutdown","title":"The case for graceful shutdown","text":"<p>We're in an age where many applications are running in Docker containers across a multitude of clusters. These applications are then confronted with new concerns to tackle such as more moving parts, networking between these parts, remote storage and others. One significant way we defend ourselves against the perils of this distributed nature is to make our applications more robust - able to survive errors.</p> <p>However, even then there is still no guarantee your application is always up and running. So another concern we should tackle is how it responds when it needs to shut down. Where we can differentiate between an unexpected shutdown - we crashed - or an expected shutdown. On top of that, failing instead of trying to recover when something bad happens also adheres to \"fail fast\" - as strongly advocated by Michael Nygard in ReleaseIt.</p> <p>Shutting down can happen for a variety of reasons, in this post we dive into how to deal with an expected shutdown such as it being told to stop by an orchestrator such as Kubernetes.</p> <p>Containers can be purposefully shut down for a variety of reasons, including but not limited too:</p> <ul> <li>your application's health check fails</li> <li>your application consumed more resources than allowed</li> <li>the application is scaling down</li> </ul> <p>Just as cleaning up when leaving makes you a better tenant, having your application clean up connections, resources Moreover, the more tenants behaving in a good way increases the quality of living for all tenants. In our case, it improves the reliability and consistency of our cluster.</p> <p>Graceful shutdown is not unique to Docker, as it has been part of Linux's best practices for quite some years before Docker's existence. However, applying them to Docker container adds extra dimensions.</p>"},{"location":"blogs/graceful-shutdown/#start-good-so-you-can-end-well","title":"Start Good So You Can End Well","text":"<p>When you sign up for an apartment, you probably have to sign a contract detailing your rights and obligations. The more you state explicitly, the easier it is to deal with bad behaving neighbors. The same is true when running processes; we should make sure that we set the rules, obligations, and expectations from the start.</p> <p>As we say in Dutch: a good beginning is half the work. We will start with how you can run a process in a container with a process that shuts down gracefully.</p> <p>There are many ways to start a process in a container. In this article, we look at processes started by commands defined in a Dockerfile. There are two ways to specify this:</p> <ul> <li>CMD: runs a command when the container gets started</li> <li>ENTRYPOINT: provides the location (entrypoint) from where commands get run when the container starts</li> </ul> <p>You need at least one ENTRYPOINT or CMD in a Dockerfile for it to be valid. They can be used in collaboration but they can do similar things.</p> <p>For more information on the details of these commands, read\u00a0Docker's docs on Entrypoint vs. CMD.</p>"},{"location":"blogs/graceful-shutdown/#docker-shell-form-example","title":"Docker Shell form example","text":"<p>We start with the shell form and see if it can do what we want; begin in such a way, we can stop it nicely. Shell form means we define a shell command without any special format or keywords.</p> <p>Please create Dockerfile with the content that follows.</p> <pre><code>FROM ubuntu:18.04\nENTRYPOINT top -b\n</code></pre> <p>Then build an image and run a container.</p> <pre><code>docker image build --tag shell-form .\ndocker run --name shell-form --rm shell-form\n</code></pre> <p>The above command yields the following output.</p> <pre><code>top - 16:34:56 up 1 day,  5:15,  0 users,  load average: 0.00, 0.00, 0.00\nTasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0.4 us,  0.3 sy,  0.0 ni, 99.2 id,  0.1 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem :  2046932 total,   541984 free,   302668 used,  1202280 buff/cache\nKiB Swap:  1048572 total,  1042292 free,     6280 used.  1579380 avail Mem\n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n    1 root      20   0    4624    760    696 S   0.0  0.0   0:00.05 sh\n    6 root      20   0   36480   2928   2580 R   0.0  0.1   0:00.01 top\n</code></pre> <p>As you can see, two processes are running, sh and top. Meaning, that killing the process, with ctrl+c for example, terminates the sh process, but not\u00a0top. This happens because the sh process forked the top process, but the termination will only be send to PID 1 - in this case sh. As sh will not stop the top process for us it will continue running and leave the container alive.</p> <p>To kill this container, open a second terminal and execute the following command.</p> <pre><code>docker rm -f shell-form\n</code></pre> <p>Shell form doesn't do what we need. Starting a process with shell form will only lead us to the disaster of parking lots filling up unless there's a someone actively cleaning up.</p>"},{"location":"blogs/graceful-shutdown/#docker-exec-form-example","title":"Docker exec form example","text":"<p>This leads us to the exec form. Hopefully, this gets us somewhere.</p> <p>The exec form is written as an array of parameters:\u00a0<code>ENTRYPOINT [\"top\", \"-b\"]</code></p> <p>To continue in the same line of examples, we will create a Dockerfile, build and run it.</p> <pre><code>FROM ubuntu:18.04\nENTRYPOINT [\"top\", \"-b\"]\n</code></pre> <p>Then build and run it.</p> <pre><code>docker image build --tag exec-form .\ndocker run --name exec-form --rm exec-form\n</code></pre> <p>This yields the following output.</p> <pre><code>top - 18:12:30 up 1 day,  6:53,  0 users,  load average: 0.00, 0.00, 0.00\nTasks:   1 total,   1 running,   0 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0.4 us,  0.3 sy,  0.0 ni, 99.2 id,  0.1 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem :  2046932 total,   535896 free,   307196 used,  1203840 buff/cache\nKiB Swap:  1048572 total,  1042292 free,     6280 used.  1574880 avail Mem\n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n    1 root      20   0   36480   2940   2584 R   0.0  0.1   0:00.03 top\n</code></pre> <p>Now we got something we can work with. If something would tell this Container to stop, it will tell our only running process so it is sure to reach the correct one!</p>"},{"location":"blogs/graceful-shutdown/#gotchas","title":"Gotchas","text":"<p>Knowing we can use the exec form for our goal - gracefully shutting down our container - we can move on to the next part of our efforts. For the sake of imparting you with some hard learned lessons, we will explore two gotchas. They're optional, so you can also choose to skip to Make Sure Your Process Listens.</p>"},{"location":"blogs/graceful-shutdown/#docker-exec-form-with-parameters","title":"Docker exec form with parameters","text":"<p>A caveat with the exec form is that it doesn't interpolate parameters.</p> <p>You can try the following:</p> <pre><code>FROM ubuntu:18.04\nENV PARAM=\"-b\"\nENTRYPOINT [\"top\", \"${PARAM}\"]\n</code></pre> <p>Then build and run it:</p> <pre><code>docker image build --tag exec-param .\ndocker run --name exec-form --rm exec-param\n</code></pre> <p>This should yield the following:</p> <pre><code>/bin/sh: 1: [top: not found\n</code></pre> <p>This is where Docker created a mix between the two styles.  It allows you to create an Entrypoint with a shell command - performing interpolation - but executing it as an exec form.  This can be done by prefixing the shell form, with, you guessed it, exec.</p> <pre><code>FROM ubuntu:18.04\nENV PARAM=\"-b\"\nENTRYPOINT exec \"top\" \"${PARAM}\"\n</code></pre> <p>Then build and run it:</p> <pre><code>docker image build --tag exec-param .\ndocker run --name exec-form --rm exec-param\n</code></pre> <p>This will return the exact same as if we would've run <code>ENTRYPOINT [\"top\", \"-b\"]</code>.</p> <p>Now you can also override the param, by using the environment variable flag.</p> <pre><code>docker image build --tag exec-param .\ndocker run --name exec-form --rm -e PARAM=\"help\" exec-param\n</code></pre> <p>Resulting in top's help string.</p>"},{"location":"blogs/graceful-shutdown/#the-special-case-of-alpine","title":"The special case of Alpine","text":"<p>One of the main best practices for Dockerfiles, is to make them as small as possible.  The easiest way to do this is to start with a minimal image.  This is where Alpine Linux comes in. We will revisit out shell form example, but replace ubuntu with alpine.</p> <p>Create the following Dockerfile.</p> <pre><code>FROM alpine:3.8\nENTRYPOINT top -b\n</code></pre> <p>Then build and run it.</p> <pre><code>docker image build --tag exec-param .\ndocker run --name exec-form --rm -e PARAM=\"help\" exec-param\n</code></pre> <p>This yields the following output.</p> <pre><code>Mem: 1509068K used, 537864K free, 640K shrd, 126756K buff, 1012436K cached\nCPU:   0% usr   0% sys   0% nic 100% idle   0% io   0% irq   0% sirq\nLoad average: 0.00 0.00 0.00 2/404 5\n  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND\n    1     0 root     R     1516   0%   0   0% top -b\n</code></pre> <p>Aside from top's output looking a bit different, there is only one command.</p> <p>Alpine Linux helps us avoid the problem of shell form altogether!</p>"},{"location":"blogs/graceful-shutdown/#make-sure-your-process-listens","title":"Make Sure Your Process Listens","text":"<p>It is excellent if your tenants are all signed up, know their rights and obligations. But you can't contact them when something happens, how will they ever know when to act?</p> <p>Translating that into our process. It starts and can be told to shut down, but does it process listen? Can it interpret the message it gets from Docker or Kubernetes? And if it does, can it relay the message correctly to its Child Processes? In order for your process to gracefully shutdown, it should know when to do so. As such, it should listen not only for itself but also on behalf of its children - yours never do anything wrong though!</p> <p>Some processes do, but many aren't designed to listen or tell their Children. They expect someone else to listen for them and tell them and their children - process managers.</p> <p>In order to listen to these signals, we can call in the help of others. We will look at two options.</p> <ul> <li>we let Docker manage the process and its children</li> <li>we use a process manager</li> </ul>"},{"location":"blogs/graceful-shutdown/#let-docker-manage-it-for-us","title":"Let Docker manage it for us","text":"<p>If you're not using Docker to run or manage your containers, you should skip to Depend on a process manager.</p> <p>Docker has a build in feature, that it uses a lightweight process manager to help you.</p> <p>So if you're running your images with Docker itself, either directly or via Compose or Swarm, you're fine. You can use the init flag in your run command or your compose file.</p> <p>Please, note that the below examples require a certain minimum version of Docker.</p> <ul> <li>run - 1.13+</li> <li>compose (v 2.2) - 1.13.0+</li> <li>swarm (v 3.7) - 18.06.0+</li> </ul>"},{"location":"blogs/graceful-shutdown/#with-docker-run","title":"With Docker Run","text":"<pre><code>docker run --rm -ti --init caladreas/dui\n</code></pre>"},{"location":"blogs/graceful-shutdown/#with-docker-compose","title":"With Docker Compose","text":"<pre><code>version: '2.2'\nservices:\n    web:\n        image: caladreas/java-docker-signal-demo:no-tini\n        init: true\n</code></pre>"},{"location":"blogs/graceful-shutdown/#with-docker-swarm","title":"With Docker Swarm","text":"<pre><code>version: '3.7'\nservices:\n    web:\n        image: caladreas/java-docker-signal-demo:no-tini\n        init: true\n</code></pre> <p>Relying on Docker does create a dependency on how your container runs. It only runs correctly in Docker-related technologies (run, compose, swarm) and only if the proper versions are available.</p> <p>Creating either a different experience for users running your application somewhere else or not able to meet the version requirements. So maybe another solution is to bake a process manager into your image and guarantee its behavior.</p>"},{"location":"blogs/graceful-shutdown/#depend-on-a-process-manager","title":"Depend on a process manager","text":"<p>One of our goals for Docker images is to keep them small. We should look for a lightweight process manager. It does not have too many a whole machine worth or processes, just one and perhaps some children.</p> <p>Here we would like to introduce you to Tini, a lightweight process manager designed for this purpose.  It is a very successful and widely adopted process manager in the Docker world. So successful, that the before mentioned init flags from Docker are implemented by baking Tini into Docker.</p>"},{"location":"blogs/graceful-shutdown/#debian-example","title":"Debian example","text":"<p>For brevity, the build process is excluded, and for image size, we use Debian slim instead of default Debian.</p> <pre><code>FROM debian:stable-slim\nENV TINI_VERSION v0.18.0\nADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini\nRUN chmod +x /tini\nENTRYPOINT [\"/tini\", \"-vv\",\"-g\", \"--\", \"/usr/bin/dui/bin/dui\",\"-XX:+UseCGroupMemoryLimitForHeap\", \"-XX:+UnlockExperimentalVMOptions\"]\nCOPY --from=build /usr/bin/dui-image/ /usr/bin/dui\n</code></pre>"},{"location":"blogs/graceful-shutdown/#alpine-example","title":"Alpine example","text":"<p>Alpine Linux works wonders for Docker images, so to improve our lives, you can very easily install it if you want.</p> <pre><code>FROM alpine\nRUN apk add --no-cache tini\nENTRYPOINT [\"/sbin/tini\", \"-vv\",\"-g\",\"-s\", \"--\"]\nCMD [\"top -b\"]\n</code></pre>"},{"location":"blogs/graceful-shutdown/#how-to-be-told-what-you-want-to-hear","title":"How To Be Told What You Want To Hear","text":"<p>You've made it this far; your tenets are reachable so you can inform them if they need to act. However, there's another problem lurking around the corner. Do they speak your language?</p> <p>Our process now starts knowing it can be talked to, it has someone who takes care of listening for it and its children. Now we need to make sure it can understand what it hears, it should be able to handle the incoming signals. We have two main ways of doing this.</p> <ul> <li>Handle signals as they come: we should make sure our process deal with the signals as they come</li> <li>State the signals we want: we can also tell up front, which signals we want to hear and put the burden of translation on our callers</li> </ul> <p>For more details on the subject of Signals and Docker, please read this excellent blog from Grigorii Chudnov.</p>"},{"location":"blogs/graceful-shutdown/#handle-signals-as-they-come","title":"Handle signals as they come","text":"<p>Handling process signals depend on your application, programming language or framework.</p>"},{"location":"blogs/graceful-shutdown/#state-the-signals-we-want","title":"State the signals we want","text":"<p>Sometimes your language or framework of choice, doesn't handle signals all that well. It might be very rigid in what it does with specific signals, removing your ability to do the right thing. Of course, not all languages or frameworks are designed with Docker container or Microservices in mind, are yet to catch up to this more dynamic environment.</p> <p>Luckily Docker and Kubernetes allow you to specify what signal too sent to your process.</p>"},{"location":"blogs/graceful-shutdown/#docker-run","title":"Docker run","text":"<pre><code>docker run --rm -ti --init --stop-signal=SIGINT \\\n   caladreas/java-docker-signal-demo\n</code></pre>"},{"location":"blogs/graceful-shutdown/#docker-composeswarm","title":"Docker compose/swarm","text":"<p>Docker's compose file format allows you to specify a stop signal.  This is the signal sent when the container is stopped in a normal fashion. Normal in this case, meaning <code>docker stop</code> or when docker itself determines it should stop the container.</p> <p>If you forcefully remove the container, for example with <code>docker rm -f</code> \u00a0it will directly kill the process, so don't do that.</p> <pre><code>version: '2.2'\nservices:\n    web:\n        image: caladreas/java-docker-signal-demo\n        stop_signal: SIGINT\n        stop_grace_period: 15s\n</code></pre> <p>If you run this with <code>docker-compose up</code> and then in a second terminal, stop the container, you will see something like this.</p> <pre><code>web_1  | HelloWorld!\nweb_1  | Shutdown hook called!\nweb_1  | We're told to stop early...\nweb_1  | java.lang.InterruptedException: sleep interrupted\nweb_1  |    at java.base/java.lang.Thread.sleep(Native Method)\nweb_1  |    at joostvdg.demo.signal@1.0/com.github.joostvdg.demo.signal.HelloWorld.printHelloWorld(Unknown Source)\nweb_1  |    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\nweb_1  |    at java.base/java.util.concurrent.FutureTask.run(Unknown Source)\nweb_1  |    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\nweb_1  |    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\nweb_1  |    at java.base/java.lang.Thread.run(Unknown Source)\nweb_1  | [DEBUG tini (1)] Passing signal: 'Interrupt'\nweb_1  | [DEBUG tini (1)] Received SIGCHLD\nweb_1  | [DEBUG tini (1)] Reaped child with pid: '7'\nweb_1  | [INFO  tini (1)] Main child exited with signal (with signal 'Interrupt')\n</code></pre>"},{"location":"blogs/graceful-shutdown/#kubernetes","title":"Kubernetes","text":"<p>In Kubernetes we can make use of Container Lifecycle Hooks to manage how our container should be stopped.  We could, for example, send a SIGINT (interrupt) to tell our application to stop.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n    name: java-signal-demo\n    namespace: default\n    labels:\n        app: java-signal-demo\nspec:\n    replicas: 1\n    template:\n        metadata:\n            labels:\n                app: java-signal-demo\n        spec:\n            containers:\n            - name: main\n              image: caladreas/java-docker-signal-demo\n              lifecycle:\n                  preStop:\n                      exec:\n                          command: [\"killall\", \"java\" , \"-INT\"]\n            terminationGracePeriodSeconds: 60\n</code></pre> <p>When you create this as deployment.yml, create and delete it -\u00a0<code>kubectl apply -f deployment.yml</code>\u00a0/ <code>kubectl delete -f deployment.yml</code> - you will see the same behavior.</p>"},{"location":"blogs/graceful-shutdown/#how-to-be-told-when-you-want-to-hear-it","title":"How To Be Told When You Want To Hear It","text":"<p>Our process now will now start knowing it will hear what it wants to hear. But we now have to make sure we hear it when we need to hear it. An intervention is excellent when you can still be saved, but it is a bit useless if you're already dead.</p>"},{"location":"blogs/graceful-shutdown/#docker","title":"Docker","text":"<p>You can either configure your health check in your Dockerfile or  configure it in your docker-compose.yml for either compose or swarm.</p> <p>Considering only Docker can use the health check in your Dockerfile,   it is strongly recommended to have health checks in your application and document how they can be used.</p>"},{"location":"blogs/graceful-shutdown/#kubernetes_1","title":"Kubernetes","text":"<p>In Kubernetes we have the concept of Container Probes. This allows you to configure whether your container is ready (readinessProbe) to be used and if it is still working as expected (livenessProbe).</p>"},{"location":"blogs/graceful-shutdown/#examples","title":"Examples","text":"<p>How to actually listen to the signals and determine which one to use will depend on your programming language.</p> <p>There's three examples I have worked out, one for Go (lang) and two for Java: pojo and Spring Boot.</p>"},{"location":"blogs/graceful-shutdown/#go","title":"Go","text":""},{"location":"blogs/graceful-shutdown/#dockerfile","title":"Dockerfile","text":"<pre><code># build stage\nFROM golang:latest AS build-env\nRUN go get -v github.com/docker/docker/client/...\nRUN go get -v github.com/docker/docker/api/...\nADD src/ $GOPATH/flow-proxy-service-lister\nWORKDIR $GOPATH/flow-proxy-service-lister\nRUN go build -o main -tags netgo main.go\n\n# final stage\nFROM alpine\nENTRYPOINT [\"/app/main\"]\nCOPY --from=build-env /go/flow-proxy-service-lister/main /app/\nRUN chmod +x /app/main\n</code></pre>"},{"location":"blogs/graceful-shutdown/#go-code-for-graceful-shutdown","title":"Go code for graceful shutdown","text":"<p>The following is a way for Go to shutdown a http server when receiving a termination signal.</p> <pre><code>func main() {\n    c := make(chan bool) // make channel for main &lt;--&gt; webserver communication\n    go webserver.Start(\"7777\", webserverData, c) // ignore the missing data\n\n    stop := make(chan os.Signal, 1) // make a channel that listens to is signals\n    signal.Notify(stop, syscall.SIGINT, syscall.SIGTERM) // we listen to some specific syscall signals\n\n    for i := 1; ; i++ { // this is still infinite\n        t := time.NewTicker(time.Second * 30) // set a timer for the polling\n        select {\n        case &lt;-stop: // this means we got a os signal on our channel\n            break // so we can stop\n        case &lt;-t.C:\n            // our timer expired, refresh our data\n            continue // and continue with the loop\n        }\n        break\n    }\n    fmt.Println(\"Shutting down webserver\") // if we got here, we have to inform the webserver to close shop\n    c &lt;- true // we do this by sending a message on the channel\n    if b := &lt;-c; b { // when we get true back, that means the webserver is doing with a graceful shutdown\n        fmt.Println(\"Webserver shut down\") // webserver is done\n    }\n    fmt.Println(\"Shut down app\") // we can close shop ourselves now\n}\n</code></pre>"},{"location":"blogs/graceful-shutdown/#java-plain-docker-swarm","title":"Java plain (Docker Swarm)","text":"<p>This application is a Java 9 modular application, which can be found on github, github.com/joostvdg.</p>"},{"location":"blogs/graceful-shutdown/#dockerfile_1","title":"Dockerfile","text":"<pre><code>FROM openjdk:9-jdk AS build\n\nRUN mkdir -p /usr/src/mods/jars\nRUN mkdir -p /usr/src/mods/compiled\n\nCOPY . /usr/src\nWORKDIR /usr/src\n\nRUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $(find src -name \"*.java\")\nRUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1.0 -C /usr/src/mods/compiled/joostvdg.dui.logging .\nRUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1.0 -C /usr/src/mods/compiled/joostvdg.dui.api .\nRUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1.0 -C /usr/src/mods/compiled/joostvdg.dui.client .\nRUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1.0  -e com.github.joostvdg.dui.server.cli.DockerApp\\\n    -C /usr/src/mods/compiled/joostvdg.dui.server .\n\nRUN rm -rf /usr/bin/dui-image\nRUN jlink --module-path /usr/src/mods/jars/:/${JAVA_HOME}/jmods \\\n    --add-modules joostvdg.dui.api \\\n    --add-modules joostvdg.dui.logging \\\n    --add-modules joostvdg.dui.server \\\n    --add-modules joostvdg.dui.client \\\n    --launcher dui=joostvdg.dui.server \\\n    --output /usr/bin/dui-image\n\nRUN ls -lath /usr/bin/dui-image\nRUN ls -lath /usr/bin/dui-image\nRUN /usr/bin/dui-image/bin/java --list-modules\n\nFROM debian:stable-slim\nLABEL authors=\"Joost van der Griendt &lt;joostvdg@gmail.com&gt;\"\nLABEL version=\"0.1.0\"\nLABEL description=\"Docker image for playing with java applications in a concurrent, parallel and distributed manor.\"\n# Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/\nENV TINI_VERSION v0.16.1\nADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini\nRUN chmod +x /tini\nENTRYPOINT [\"/tini\", \"-vv\",\"-g\", \"--\", \"/usr/bin/dui/bin/dui\"]\nENV DATE_CHANGED=\"20180120-1525\"\nCOPY --from=build /usr/bin/dui-image/ /usr/bin/dui\nRUN /usr/bin/dui/bin/java --list-modules\n</code></pre>"},{"location":"blogs/graceful-shutdown/#handling-code","title":"Handling code","text":"<p>The code first initializes the server which and when started, creates the Shutdown Hook.</p> <p>Java handles certain signals in specific ways, as can be found in this table for linux. For more information, you can read the docs from Oracle. </p> <pre><code>public class DockerApp {\n    public static void main(String[] args) {\n        ServiceLoader&lt;Logger&gt; loggers = ServiceLoader.load(Logger.class);\n                Logger logger = loggers.findFirst().isPresent() ? loggers.findFirst().get() : null;\n                if (logger == null) {\n                    System.err.println(\"Did not find any loggers, quiting\");\n                    System.exit(1);\n                }\n                logger.start(LogLevel.INFO);\n\n                int pseudoRandom = new Random().nextInt(ProtocolConstants.POTENTIAL_SERVER_NAMES.length -1);\n                String serverName = ProtocolConstants.POTENTIAL_SERVER_NAMES[pseudoRandom];\n                int listenPort = ProtocolConstants.EXTERNAL_COMMUNICATION_PORT_A;\n                String multicastGroup = ProtocolConstants.MULTICAST_GROUP;\n\n                DuiServer distributedServer = DuiServerFactory.newDistributedServer(listenPort,multicastGroup , serverName, logger);\n\n                distributedServer.logMembership();\n\n                ExecutorService executorService = Executors.newFixedThreadPool(1);\n                executorService.submit(distributedServer::startServer);\n\n                long threadId = Thread.currentThread().getId();\n\n                Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; {\n                    System.out.println(\"Shutdown hook called!\");\n                    logger.log(LogLevel.WARN, \"App\", \"ShotdownHook\", threadId, \"Shutting down at request of Docker\");\n                    distributedServer.stopServer();\n                    distributedServer.closeServer();\n                    executorService.shutdown();\n                    try {\n                        Thread.sleep(100);\n                        executorService.shutdownNow();\n                        logger.stop();\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }));        \n    }\n}\n</code></pre>"},{"location":"blogs/graceful-shutdown/#java-plain-kubernetes","title":"Java Plain (Kubernetes)","text":"<p>So far we've utilized the utilities from Docker itself in conjunction with it's native Docker Swarm orchestrator.</p> <p>Unfortunately, when it comes to popularity Kubernetes beats Swarm hands down.</p> <p>So this isn't complete if it doesn't also do graceful shutdown in Kubernetes. </p>"},{"location":"blogs/graceful-shutdown/#in-dockerfile","title":"In Dockerfile","text":"<p>Our original file had to be changed, as Debian's Slim image doesn't actually contain the kill package. And we need a kill package, as we cannot instruct Kubernetes to issue a specific SIGNAL. Instead, we can issue a PreStop exec command, which we can utilise to execute a killall java -INT.</p> <p>The command will be specified in the Kubernetes deployment definition below.</p> <pre><code>FROM openjdk:9-jdk AS build\n\nRUN mkdir -p /usr/src/mods/jars\nRUN mkdir -p /usr/src/mods/compiled\n\nCOPY . /usr/src\nWORKDIR /usr/src\n\nRUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $(find src -name \"*.java\")\nRUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1.0 -C /usr/src/mods/compiled/joostvdg.dui.logging .\nRUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1.0 -C /usr/src/mods/compiled/joostvdg.dui.api .\nRUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1.0 -C /usr/src/mods/compiled/joostvdg.dui.client .\nRUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1.0  -e com.github.joostvdg.dui.server.cli.DockerApp\\\n    -C /usr/src/mods/compiled/joostvdg.dui.server .\n\nRUN rm -rf /usr/bin/dui-image\nRUN jlink --module-path /usr/src/mods/jars/:/${JAVA_HOME}/jmods \\\n    --add-modules joostvdg.dui.api \\\n    --add-modules joostvdg.dui.logging \\\n    --add-modules joostvdg.dui.server \\\n    --add-modules joostvdg.dui.client \\\n    --launcher dui=joostvdg.dui.server \\\n    --output /usr/bin/dui-image\n\nRUN ls -lath /usr/bin/dui-image\nRUN ls -lath /usr/bin/dui-image\nRUN /usr/bin/dui-image/bin/java --list-modules\n\nFROM debian:stable-slim\nLABEL authors=\"Joost van der Griendt &lt;joostvdg@gmail.com&gt;\"\nLABEL version=\"0.1.0\"\nLABEL description=\"Docker image for playing with java applications in a concurrent, parallel and distributed manor.\"\n# Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/\nENV TINI_VERSION v0.16.1\nADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini\nRUN chmod +x /tini\nENTRYPOINT [\"/tini\", \"-vv\",\"-g\", \"--\", \"/usr/bin/dui/bin/dui\"]\nENV DATE_CHANGED=\"20180120-1525\"\nRUN apt-get update &amp;&amp; apt-get install --no-install-recommends -y psmisc=22.* &amp;&amp; rm -rf /var/lib/apt/lists/*\nCOPY --from=build /usr/bin/dui-image/ /usr/bin/dui\nRUN /usr/bin/dui/bin/java --list-modules\n</code></pre>"},{"location":"blogs/graceful-shutdown/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>So here we have the image's K8s Deployment descriptor.</p> <p>Including the Pod's lifecycle <code>preStop</code> with a exec style command. You should know by now why we prefer that.</p> <pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: dui-deployment\n  namespace: default\n  labels:\n    k8s-app: dui\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        k8s-app: dui\n    spec:\n      containers:\n        - name: master\n          image: caladreas/buming\n          ports:\n            - name: http\n              containerPort: 7777\n          lifecycle:\n            preStop:\n              exec:\n                command: [\"killall\", \"java\" , \"-INT\"]\n      terminationGracePeriodSeconds: 60\n</code></pre>"},{"location":"blogs/graceful-shutdown/#java-spring-boot-1x","title":"Java Spring Boot (1.x)","text":"<p>This example is for Spring Boot 1.x, in time we will have an example for 2.x.</p> <p>This example is for the scenario of a Fat Jar with Tomcat as container [^8].</p>"},{"location":"blogs/graceful-shutdown/#execute-example","title":"Execute example","text":"<pre><code>docker-compose build\n</code></pre> <p>Execute the following command:</p> <pre><code>docker run --rm -ti --name test spring-boot-graceful\n</code></pre> <p>Exit the application/container via <code>ctrl+c</code> and you should see the application shutting down gracefully.</p> <pre><code>2018-01-30 13:35:46.327  INFO 7 --- [       Thread-3] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@6e5e91e4: startup date [Tue Jan 30 13:35:42 GMT 2018]; root of context hierarchy\n2018-01-30 13:35:46.405  INFO 7 --- [       Thread-3] BootGracefulApplication$GracefulShutdown : Tomcat was shutdown gracefully within the allotted time.\n2018-01-30 13:35:46.408  INFO 7 --- [       Thread-3] o.s.j.e.a.AnnotationMBeanExporter        : Unregistering JMX-exposed beans on shutdown\n</code></pre>"},{"location":"blogs/graceful-shutdown/#dockerfile_2","title":"Dockerfile","text":"<pre><code>FROM maven:3-jdk-8 AS build\nENV MAVEN_OPTS=-Dmaven.repo.local=/usr/share/maven/repository\nENV WORKDIR=/usr/src/graceful\nRUN mkdir $WORKDIR\nWORKDIR $WORKDIR\nCOPY pom.xml $WORKDIR\nRUN mvn -B -e org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline\nCOPY . $WORKSPACE\nRUN mvn -B -e clean verify\n\nFROM anapsix/alpine-java:8_jdk_unlimited\nLABEL authors=\"Joost van der Griendt &lt;joostvdg@gmail.com&gt;\"\nENV TINI_VERSION v0.16.1\nADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini\nRUN chmod +x /tini\nENTRYPOINT [\"/tini\", \"-vv\",\"-g\", \"--\"]\nENV DATE_CHANGED=\"20180120-1525\"\nCOPY --from=build /usr/src/graceful/target/spring-boot-graceful.jar /app.jar\nCMD [\"java\", \"-Xms256M\",\"-Xmx480M\", \"-Djava.security.egd=file:/dev/./urandom\", \"-jar\", \"/app.jar\"]\n</code></pre>"},{"location":"blogs/graceful-shutdown/#docker-compose-file","title":"Docker compose file","text":"<pre><code>version: \"3.5\"\n\nservices:\n  web:\n    image: spring-boot-graceful\n    build: .\n    stop_signal: SIGINT\n</code></pre>"},{"location":"blogs/graceful-shutdown/#java-handling-code","title":"Java handling code","text":"<pre><code>package com.github.joostvdg.demo.springbootgraceful;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\nimport org.apache.catalina.connector.Connector;\nimport org.apache.tomcat.util.threads.ThreadPoolExecutor;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer;\nimport org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer;\nimport org.springframework.boot.context.embedded.tomcat.TomcatConnectorCustomizer;\nimport org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory;\nimport org.springframework.context.ApplicationListener;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.event.ContextClosedEvent;\n\nimport java.util.concurrent.Executor;\nimport java.util.concurrent.TimeUnit;\n\n@SpringBootApplication\npublic class SpringBootGracefulApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(SpringBootGracefulApplication.class, args);\n    }\n\n    @Bean\n    public GracefulShutdown gracefulShutdown() {\n        return new GracefulShutdown();\n    }\n\n    @Bean\n    public EmbeddedServletContainerCustomizer tomcatCustomizer() {\n        return new EmbeddedServletContainerCustomizer() {\n\n            @Override\n            public void customize(ConfigurableEmbeddedServletContainer container) {\n                if (container instanceof TomcatEmbeddedServletContainerFactory) {\n                    ((TomcatEmbeddedServletContainerFactory) container)\n                            .addConnectorCustomizers(gracefulShutdown());\n                }\n\n            }\n        };\n    }\n\n    private static class GracefulShutdown implements TomcatConnectorCustomizer,\n            ApplicationListener&lt;ContextClosedEvent&gt; {\n\n        private static final Logger log = LoggerFactory.getLogger(GracefulShutdown.class);\n\n        private volatile Connector connector;\n\n        @Override\n        public void customize(Connector connector) {\n            this.connector = connector;\n        }\n\n        @Override\n        public void onApplicationEvent(ContextClosedEvent event) {\n            this.connector.pause();\n            Executor executor = this.connector.getProtocolHandler().getExecutor();\n            if (executor instanceof ThreadPoolExecutor) {\n                try {\n                    ThreadPoolExecutor threadPoolExecutor = (ThreadPoolExecutor) executor;\n                    threadPoolExecutor.shutdown();\n                    if (!threadPoolExecutor.awaitTermination(30, TimeUnit.SECONDS)) {\n                        log.warn(\"Tomcat thread pool did not shut down gracefully within \"\n                                + \"30 seconds. Proceeding with forceful shutdown\");\n                    } else {\n                        log.info(\"Tomcat was shutdown gracefully within the allotted time.\");\n                    }\n                }\n                catch (InterruptedException ex) {\n                    Thread.currentThread().interrupt();\n                }\n            }\n        }\n\n    }\n}\n</code></pre>"},{"location":"blogs/graceful-shutdown/#example-with-docker-swarm","title":"Example with Docker Swarm","text":"<p>For now there's only an example with docker swarm, in time there will also be a Kubernetes example.</p> <p>Now that you can create Java applications packaged neatly in Docker images that support graceful shutdown, it would be nice to utilize.</p> <p>A good scenario would be a microservices architecture where services can come and go, but are registered in a service registry such as Eureka.</p> <p>Or a membership based protocol where members interact with each other and perhaps shard data.</p> <p>In these cases, of course the interactions are designed to be fault tolerant and discover faulty nodes on their own. But wouldn't it be better that if you knew you're going to quit, you inform the rest?</p> <p>We can reuse the <code>caladreas/buming</code> image and make it a docker swarm stack and run the service on every node. This way, we can easily see members coming and going and reduce the time to detect failure by notifying our peers of our impeding end.  </p>"},{"location":"blogs/graceful-shutdown/#docker-swarm-cluster","title":"Docker swarm cluster","text":"<p>Setting up a docker swarm cluster is easy, but has some requirements:</p> <ul> <li>virtual box 4.x+</li> <li>docker-machine 1.12+</li> <li>docker 17.06+</li> </ul> <p>Warn</p> <p>Make sure this is the first and only virtualbox docker-machine VM being created/running, so that the ip range starts with 192.168.99.100</p> <pre><code>docker-machine create --driver virtualbox dui-1\ndocker-machine create --driver virtualbox dui-2\ndocker-machine create --driver virtualbox dui-3\n\neval \"$(docker-machine env dui-1)\"\nIP=192.168.99.100\ndocker swarm init --advertise-addr $IP\nTOKEN=$(docker swarm join-token -q worker)\n\neval \"$(docker-machine env dui-2)\"\ndocker swarm join --token ${TOKEN} ${IP}:2377\n\neval \"$(docker-machine env dui-3)\"\ndocker swarm join --token ${TOKEN} ${IP}:2377\n\neval \"$(docker-machine env dui-1)\"\ndocker node ls\n</code></pre>"},{"location":"blogs/graceful-shutdown/#docker-swarm-network-and-multicast","title":"Docker swarm network and multicast","text":"<p>Unfortunately, docker swarm's swarm mode network overlay does not support multicast [<sup>9][</sup>10].</p> <p>Why is this a problem? Well, the application I use to test the graceful shutdown requires this, sorry.</p> <p>Luckily there is a very easy solution for this, its by using Weavenet's docker network plugin.</p> <p>Don't want to know about it or how you install it? Don't worry, just execute the script below.</p> <pre><code>#!/usr/bin/env bash\necho \"=&gt; Prepare dui-2\"\neval \"$(docker-machine env dui-2)\"\ndocker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions\ndocker plugin disable weaveworks/net-plugin:2.1.3\ndocker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST=1\ndocker plugin enable weaveworks/net-plugin:2.1.3\n\necho \"=&gt; Prepare dui-3\"\neval \"$(docker-machine env dui-3)\"\ndocker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions\ndocker plugin disable weaveworks/net-plugin:2.1.3\ndocker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST=1\ndocker plugin enable weaveworks/net-plugin:2.1.3\n\necho \"=&gt; Prepare dui-1\"\neval \"$(docker-machine env dui-1)\"\ndocker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions\ndocker plugin disable weaveworks/net-plugin:2.1.3\ndocker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST=1\ndocker plugin enable weaveworks/net-plugin:2.1.3\ndocker network create --driver=weaveworks/net-plugin:2.1.3 --opt works.weave.multicast=true --attachable dui\n</code></pre>"},{"location":"blogs/graceful-shutdown/#docker-stack","title":"Docker stack","text":"<p>Now to create a service that runs on every node it is the easiest to create a docker stack.</p>"},{"location":"blogs/graceful-shutdown/#compose-file-docker-stackyml","title":"Compose file (docker-stack.yml)","text":"<pre><code>version: \"3.5\"\n\nservices:\n  dui:\n    image: caladreas/buming\n    build: .\n    stop_signal: SIGINT\n    networks:\n      - dui\n    deploy:\n      mode: global\nnetworks:\n  dui:\n    external: true\n</code></pre>"},{"location":"blogs/graceful-shutdown/#create-stack","title":"Create stack","text":"<pre><code>docker stack deploy --compose-file docker-stack.yml buming\n</code></pre>"},{"location":"blogs/graceful-shutdown/#execute-example_1","title":"Execute example","text":"<p>Now that we have a docker swarm cluster and a stack - which has a service running on every node - we can showcase the power of graceful shutdown in a cluster of dependent services.</p> <p>Confirm the service is running correctly on every node, first lets check our nodes.</p> <p><pre><code>eval \"$(docker-machine env dui-1)\"\ndocker node ls\n</code></pre> Which should look like this:</p> <pre><code>ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS\nf21ilm4thxegn5xbentmss5ur *   dui-1               Ready               Active              Leader\ny7475bo5uplt2b58d050b4wfd     dui-2               Ready               Active              \n6ssxola6y1i6h9p8256pi7bfv     dui-3               Ready               Active                            \n</code></pre> <p>Then check the service.</p> <pre><code>docker service ps buming_dui\n</code></pre> <p>Which should look like this.</p> <pre><code>ID                  NAME                                   IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\n3mrpr0jg31x1        buming_dui.6ssxola6y1i6h9p8256pi7bfv   dui:latest          dui-3               Running             Running 17 seconds ago                       \npfubtiy4j7vo        buming_dui.f21ilm4thxegn5xbentmss5ur   dui:latest          dui-1               Running             Running 17 seconds ago                       \nf4gjnmhoe3y4        buming_dui.y7475bo5uplt2b58d050b4wfd   dui:latest          dui-2               Running             Running 17 seconds ago                       \n</code></pre> <p>Now open a second terminal window. In window one, follow the service logs:</p> <pre><code>eval \"$(docker-machine env dui-1)\"\ndocker service logs -f buming_dui\n</code></pre> <p>In window two, go to a different node and stop the container.</p> <pre><code>eval \"$(docker-machine env dui-2)\"\ndocker ps\ndocker stop buming_dui.y7475bo5uplt2b58d050b4wfd.pnoui2x6elrz0tvkjz51njz94\n</code></pre> <p>In this case, you will see the other nodes receiving a leave notice and then the node stopping.</p> <pre><code>buming_dui.0.ryd8szexxku3@dui-3    | [Server-John D. Carmack]           [WARN]  [14:19:02.604011]   [16]    [Main]              Received membership leave notice from MessageOrigin{host='83918f6ad817', ip='10.0.0.7', name='Ken Thompson'}\nbuming_dui.0.so5m14sz8ksh@dui-1    | [Server-Alan Kay]                  [WARN]  [14:19:02.602082]   [16]    [Main]              Received membership leave notice from MessageOrigin{host='83918f6ad817', ip='10.0.0.7', name='Ken Thompson'}\nbuming_dui.0.pnoui2x6elrz@dui-2    | Shutdown hook called!\nbuming_dui.0.pnoui2x6elrz@dui-2    | [App]                              [WARN]  [14:19:02.598759]   [1] [ShotdownHook]      Shutting down at request of Docker\nbuming_dui.0.pnoui2x6elrz@dui-2    | [Server-Ken Thompson]              [INFO]  [14:19:02.598858]   [12]    [Main]               Stopping\nbuming_dui.0.pnoui2x6elrz@dui-2    | [Server-Ken Thompson]              [INFO]  [14:19:02.601008]   [12]    [Main]               Closing\n</code></pre>"},{"location":"blogs/graceful-shutdown/#further-reading","title":"Further reading","text":"<ul> <li>Wikipedia page on reboots</li> <li>Microsoft about graceful shutdown</li> <li>Gracefully stopping docker containers</li> <li>What to know about Java and shutdown hooks</li> <li>https://www.weave.works/blog/docker-container-networking-multicast-fast/</li> <li>https://www.weave.works/docs/net/latest/install/plugin/plugin-how-it-works/</li> <li>https://www.weave.works/docs/net/latest/install/plugin/plugin-v2/</li> <li>https://www.auzias.net/en/docker-network-multihost/</li> <li>https://forums.docker.com/t/cannot-get-zookeeper-to-work-running-in-docker-using-swarm-mode/27109</li> <li>https://github.com/docker/libnetwork/issues/740</li> </ul>"},{"location":"blogs/jenkins-pipeline-support-tool/","title":"Jenkins Pipeline Support Tools","text":"<p>With Jenkins now becoming Cloud Native and a first class citizen of Kubernetes, it is time to review how we use build tools.</p> <p>This content assumes you're using Jenkins in a Kubernetes cluster, but most of it should also work in other Docker-based environments.</p>"},{"location":"blogs/jenkins-pipeline-support-tool/#ideal-pipeline","title":"Ideal Pipeline","text":"<p>Anyway, one thing we often see people do wrong with Jenkins pipelines is to use the Groovy Scripts as a general-purpose programming language. This creates many problems, bloated &amp; complicated pipelines, much more stress on the master instead of on the build agent and generally making things unreliable.</p> <p>A much better way is to use Jenkins pipelines only as orchestration and lean heavily on your build tools - e.g., Maven, Gradle, Yarn, Bazel - and shell scripts. Alas, if you created complicated pipelines in Groovy scripts, it is likely you'll end up the same with Bash scripts. An even better solution would be to create custom CLI applications that take care of large operations and convoluted logic. You can test and reuse them.</p> <pre><code>pipeline {\n    agent any\n    stages {\n        stage('Build') {\n            steps {\n                sh './build.sh'\n            }\n        }\n        stage('Test') {\n            steps {\n                sh './test.sh'\n            }\n        }\n        stage('Deploy') {\n            steps {\n                sh './deploy.sh'\n            }\n        }\n    }\n    post  {\n        success {\n            sh './successNotification.sh'\n        }\n        failure {\n          sh './failureNotification.sh'\n        }\n    }\n}\n</code></pre> <p>Now, this might look a bit like a pipe dream, but it illustrates how you should use Jenkins Pipeline. The groovy script engine allows for a lot of freedom, but only rarely is its use justified. To create robust, modular and generic pipelines, it is recommended to use build tools, shell scripts, Shared Libraries and custom CLI's.</p> <p>It was always a bit difficult to manage generic scripts and tools across instances of Jenkins, pipelines, and teams. But with  Pod Templates we have an excellent mechanism for using, versioning and distributing them with ease.</p>"},{"location":"blogs/jenkins-pipeline-support-tool/#kubernetes-pods","title":"Kubernetes Pods","text":"<p>When Jenkins runs in Kubernetes, it can leverage it via the Kubernetes Plugin. I realize Jenkins conjures up mixed emotions when it comes to plugins, but this setup might replace most of them.</p> <p>How so? By using a Kubernetes Pod as the agent where instead of putting all your tools into a single VM you can use multiple small scoped containers.</p> <p>You can specify Pod Templates in multiple ways, where my personal favorite is to define it as yaml inside a declarative pipeline - see example below. For each tool you need, you specify the container and its configuration - if required.  By default, you will always get a container with a Jenkins JNLP client and the workspace mounted as a volume in the pod.</p> <p>This allows you to create several tiny containers, each containing only the tools you need for a specific job. Now, it could happen you use two or more tools together a lot - let's say npm and maven - so it is ok to sometimes deviate from this to lower the overall memory of the pod.</p> <p>When you need custom logic, you will have to create a script or tool. This is where PodTemplate, Docker images and our desire for small narrow focus tools come together.</p>"},{"location":"blogs/jenkins-pipeline-support-tool/#podtemplate-example","title":"PodTemplate example","text":"<pre><code>pipeline {\n    agent {\n        kubernetes {\n        label 'mypod'\n        defaultContainer 'jnlp'\n        yaml \"\"\"\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    some-label: some-label-value\nspec:\n  containers:\n  - name: maven\n    image: maven:alpine\n    command:\n    - cat\n    tty: true\n  - name: busybox\n    image: busybox\n    command:\n    - cat\n    tty: true\n        \"\"\"\n        }\n    }\n    stages {\n        stage('Run maven') {\n            steps {\n                container('maven') {\n                    sh 'mvn -version'\n                }\n                container('busybox') {\n                    sh '/bin/busybox'\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"blogs/jenkins-pipeline-support-tool/#java-example","title":"Java Example","text":"<p>I bet most people do not think about Java when it comes lightweight CLI applications, but I think that is a shame. Java has excellent tooling to help you build well-tested applications which can be understood and maintained by a vast majority of developers.</p> <p>To make the images small, we will use some of the new tools available in Java land. We will first dive into using Java Modularity and JLink to create a compact and strict binary package, and then we move onto Graal for creating a Native image.</p>"},{"location":"blogs/jenkins-pipeline-support-tool/#custom-jdk-image","title":"Custom JDK Image","text":"<p>All the source code of this example application is at github.com/joostvdg/jpb.</p> <p>It is a small CLI which does only one thing; it parses a git commit log to see which folders changed. Quite a useful tool for Monorepo's or other repositories containing more than one changeable resource.</p> <p>Such a CLI should have specific characteristics: * testable * small memory footprint * small disk footprint * quick start * easy to setup * easy to maintain</p> <p>These points sound like an excellent use case for Java Modules and JLink.  For those who don't know, read up on Java Modules here and read up on JLink here. JLink will create a binary image that we can use with Alpine Linux to form a minimal Java (Docker) Image.</p> <p>Unfortunately, the plugins for Maven (JMod and JLink) seem to have died. The support on Gradle side is not much better.</p> <p>So I created a solution myself with a multi-stage Docker build. Which does detract a bit from the ease of setup. But overall, it hits the other characteristics spot on. </p>"},{"location":"blogs/jenkins-pipeline-support-tool/#application-model","title":"Application Model","text":"<p>For ease of maintenance and testing, we separate the parts of the CLI into Java Modules, as you can see in the model below. For using JLink, we need to be a module ourselves. So I figured to expand the exercise to use it to not only create boundaries via packages but also with Modules.</p> <p></p>"},{"location":"blogs/jenkins-pipeline-support-tool/#build","title":"Build","text":"<p>The current LTS version of Java is 11, which means we need at least that if we want to be up-to-date. As we want to run the application in Alpine Linux, we need to build it with Alpine Linux - if you create a custom JDK image its OS specific. To my surprise, the official LTS release is not released for Alpine, so we use OpenJDK 12.</p> <p>Everything is built via a Multi-Stage Docker Build. This Dockerfile can be divided into five segments.</p> <ol> <li>creation of the base with a JDK 11+ on Alpine Linux</li> <li>compiling our Java Modules in into Module Jars</li> <li>test our code</li> <li>create our custom JDK image with just our code and whatever we need from the JDK</li> <li>create the runtime Docker image</li> </ol> <p>The Dockerfile looks a bit complicated, but we did get a Java runtime that is about 44MB in size and can run as a direct binary with no startup time. The Dockerfile can be much short if we use only a single module, but as our logic grows it is a thoughtful way to separate different concerns.</p> <p>Still, I'm not too happy with this for creating many small CLI's. To much handwork goes into creating the images like this. Relying on unmaintained Maven or Gradle Plugins doesn't seem a better choice.</p> <p>Luckily, there's a new game in town, GraalVM. We'll make an image with Graal next, stay tuned.</p>"},{"location":"blogs/jenkins-pipeline-support-tool/#dockerfile","title":"Dockerfile","text":"<pre><code>###############################################################\n###############################################################\n##### 1. CREATE ALPINE BASE WITH JDK11+\n#### OpenJDK image produces weird results with JLink (400mb + sizes)\nFROM alpine:3.8 AS build\nENV JAVA_HOME=/opt/jdk \\\n    PATH=${PATH}:/opt/jdk/bin \\\n    LANG=C.UTF-8\n\nRUN set -ex &amp;&amp; \\\n    apk add --no-cache bash &amp;&amp; \\\n    wget https://download.java.net/java/early_access/alpine/18/binaries/openjdk-12-ea+18_linux-x64-musl_bin.tar.gz -O jdk.tar.gz &amp;&amp; \\\n    mkdir -p /opt/jdk &amp;&amp; \\\n    tar zxvf jdk.tar.gz -C /opt/jdk --strip-components=1 &amp;&amp; \\\n    rm jdk.tar.gz &amp;&amp; \\\n    rm /opt/jdk/lib/src.zip\n####################################\n## 2.a PREPARE COMPILE PHASE\nRUN mkdir -p /usr/src/mods/jars\nRUN mkdir -p /usr/src/mods/compiled\nCOPY . /usr/src\nWORKDIR /usr/src\n\n## 2.b COMPILE ALL JAVA FILES\nRUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $(find src -name \"*.java\")\n## 2.c CREATE ALL JAVA MODULE JARS\nRUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.api.jar --module-version 1.0 -C /usr/src/mods/compiled/joostvdg.jpb.api .\nRUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.core.jar --module-version 1.0 -C /usr/src/mods/compiled/joostvdg.jpb.core .\nRUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.cli.jar --module-version 1.0  -e com.github.joostvdg.jpb.cli.JpbApp\\\n    -C /usr/src/mods/compiled/joostvdg.jpb.cli .\nRUN jar --create --file /usr/src/mods/jars/joostvdg.jpb.core.test.jar --module-version 1.0  -e com.github.joostvdg.jpb.core.test.ParseChangeListTest \\\n    -C /usr/src/mods/compiled/joostvdg.jpb.core.test .\n####################################\n## 3 RUN TESTS\nRUN rm -rf /usr/bin/jpb-test-image\nRUN jlink \\\n    --verbose \\\n    --compress 2 \\\n    --no-header-files \\\n    --no-man-pages \\\n    --strip-debug \\\n    --limit-modules java.base \\\n    --launcher jpb-test=joostvdg.jpb.core.test \\\n    --module-path /usr/src/mods/jars/:$JAVA_HOME/jmods \\\n    --add-modules joostvdg.jpb.core.test \\\n    --add-modules joostvdg.jpb.core \\\n    --add-modules joostvdg.jpb.api \\\n     --output /usr/bin/jpb-test-image\nRUN /usr/bin/jpb-test-image/bin/java --list-modules\nRUN /usr/bin/jpb-test-image/bin/jpb-test\n####################################\n## 4 BUILD RUNTIME - CUSTOM JDK IMAGE\nRUN rm -rf /usr/bin/jpb-image\nRUN jlink \\\n    --verbose \\\n    --compress 2 \\\n    --no-header-files \\\n    --no-man-pages \\\n    --strip-debug \\\n    --limit-modules java.base \\\n    --launcher jpb=joostvdg.jpb.cli \\\n    --module-path /usr/src/mods/jars/:$JAVA_HOME/jmods \\\n    --add-modules joostvdg.jpb.cli \\\n    --add-modules joostvdg.jpb.api \\\n    --add-modules joostvdg.jpb.core \\\n     --output /usr/bin/jpb-image\nRUN /usr/bin/jpb-image/bin/java --list-modules\n####################################\n##### 5. RUNTIME IMAGE - ALPINE\nFROM panga/alpine:3.8-glibc2.27\nLABEL authors=\"Joost van der Griendt &lt;joostvdg@gmail.com&gt;\"\nLABEL version=\"0.1.0\"\nLABEL description=\"Docker image for running Jenkins Pipeline Binary\"\nENV DATE_CHANGED=\"20181014-2035\"\nENV JAVA_OPTS=\"-XX:+UseCGroupMemoryLimitForHeap -XX:+UnlockExperimentalVMOptions\"\nCOPY --from=build /usr/bin/jpb-image/ /usr/bin/jpb\nENTRYPOINT [\"/usr/bin/jpb/bin/jpb\"]\n</code></pre>"},{"location":"blogs/jenkins-pipeline-support-tool/#image-disk-size","title":"Image disk size","text":"<pre><code>REPOSITORY                                   TAG                 IMAGE ID            CREATED              SIZE\njpb                                          latest              af7dda45732a        About a minute ago   43.8MB\n</code></pre>"},{"location":"blogs/jenkins-pipeline-support-tool/#graal","title":"Graal","text":"<p>GraalVM is a universal virtual machine for running applications written in JavaScript, Python, Ruby, R, JVM-based languages like Java, Scala, Kotlin, Clojure, and LLVM-based languages such as C and C++.   - graalvm.org</p> <p>Ok, that doesn't tell you why using GraalVM is excellent for creating small CLI docker images. Maybe this quote helps:</p> <p>Native images compiled with GraalVM ahead-of-time improve the startup time and reduce the memory footprint of JVM-based applications. </p> <p>Where JLink allows you to create a custom JDK image and embed your application as a runtime binary, Graal goes one step further. It replaces the VM altogether and uses Substrate VM to run your binary. It can't do a lot of the fantastic things the JVM can do and isn't suited for long running applications or those with a large memory footprint and so on. Well, our CLI applications are single shot executions with low memory footprint, the perfect fit for Graal/Substrate!</p> <p>All the code from this example can is on GitHub at github.com/demomon/jpc-graal--maven.</p>"},{"location":"blogs/jenkins-pipeline-support-tool/#application-model_1","title":"Application Model","text":"<p>While building modular Java applications is excellent, the current tooling support terrible. So this time the application is a single Jar - Graal can create images from classes or jars - where packages do the separation.</p> <pre><code>.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 docker-graal-build.sh\n\u251c\u2500\u2500 pom.xml\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 main\n    \u00a0\u00a0 \u2514\u2500\u2500 java\n    \u00a0\u00a0     \u2514\u2500\u2500 com\n    \u00a0\u00a0         \u2514\u2500\u2500 github\n    \u00a0\u00a0             \u2514\u2500\u2500 joostvdg\n    \u00a0\u00a0                 \u2514\u2500\u2500 demo\n    \u00a0\u00a0                     \u251c\u2500\u2500 App.java\n    \u00a0\u00a0                     \u2514\u2500\u2500 Hello.java\n</code></pre>"},{"location":"blogs/jenkins-pipeline-support-tool/#build_1","title":"Build","text":"<p>Graal can build a native image based on a Jar file. This allows us to use any standard Java build tool such as Maven or Gradle to build the jar. The actual Graal build will be done in a Dockerfile.</p> <p>The people over at Oracle have created an official Docker image reducing effort spend on our side.</p> <p>The Dockerfile has three segments:</p> <ul> <li>build the jar with Maven</li> <li>build the native image with Graal</li> <li>assembly the runtime Docker image based on Alpine</li> </ul> <p>As you can see below, the Graal image is only half the size of the JLink image! Let's see how that stacks up to other languages such as Go and Python.</p>"},{"location":"blogs/jenkins-pipeline-support-tool/#dockerfile_1","title":"Dockerfile","text":"<pre><code>#######################################\n## 1. BUILD JAR WITH MAVEN\nFROM maven:3.6-jdk-8 as BUILD\nWORKDIR /usr/src\nCOPY . /usr/src\nRUN mvn clean package -e\n#######################################\n## 2. BUILD NATIVE IMAGE WITH GRAAL\nFROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD\nWORKDIR /usr/src\nCOPY --from=BUILD /usr/src/ /usr/src\nRUN ls -lath /usr/src/target/\nCOPY /docker-graal-build.sh /usr/src\nRUN ./docker-graal-build.sh\nRUN ls -lath\n#######################################\n## 3. BUILD DOCKER RUNTIME IMAGE\nFROM alpine:3.8\nCMD [\"jpc-graal\"]\nCOPY --from=NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/\nRUN chmod +x /usr/local/bin/jpc-graal\n#######################################\n</code></pre>"},{"location":"blogs/jenkins-pipeline-support-tool/#image-disk-size_1","title":"Image disk size","text":"<pre><code>REPOSITORY                                   TAG                 IMAGE ID            CREATED             SIZE\njpc-graal-maven                              latest              dc33ebb10813        About an hour ago   19.6MB\n</code></pre>"},{"location":"blogs/jenkins-pipeline-support-tool/#go-example","title":"Go Example","text":""},{"location":"blogs/jenkins-pipeline-support-tool/#application-model_2","title":"Application Model","text":""},{"location":"blogs/jenkins-pipeline-support-tool/#build_2","title":"Build","text":""},{"location":"blogs/jenkins-pipeline-support-tool/#dockerfile_2","title":"Dockerfile","text":""},{"location":"blogs/jenkins-pipeline-support-tool/#image-disk-size_2","title":"Image Disk size","text":"<pre><code>REPOSITORY                                   TAG                 IMAGE ID            CREATED             SIZE\njpc-go                                       latest              bb4a8e546601        6 minutes ago       12.3MB\n</code></pre>"},{"location":"blogs/jenkins-pipeline-support-tool/#python-example","title":"Python Example","text":""},{"location":"blogs/jenkins-pipeline-support-tool/#application-model_3","title":"Application Model","text":""},{"location":"blogs/jenkins-pipeline-support-tool/#build_3","title":"Build","text":""},{"location":"blogs/jenkins-pipeline-support-tool/#dockerfile_3","title":"Dockerfile","text":""},{"location":"blogs/jenkins-pipeline-support-tool/#image-disk-size_3","title":"Image Disk size","text":""},{"location":"blogs/jenkins-pipeline-support-tool/#container-footprint","title":"Container footprint","text":"<pre><code>kubectl top pods mypod-s4wpb-7dz4q --containers\nPOD                 NAME         CPU(cores)   MEMORY(bytes)\nmypod-7lxnk-gw1sj   jpc-go       0m           0Mi\nmypod-7lxnk-gw1sj   java-jlink   0m           0Mi\nmypod-7lxnk-gw1sj   java-graal   0m           0Mi\nmypod-7lxnk-gw1sj   jnlp         150m         96Mi\n</code></pre> <p>So, the 0Mi memory seems wrong. So I decided to dive into the Google Cloud Console, to see if there's any information in there. What I found there, is the data you can see below. The memory is indeed 0Mi, as they're using between 329 and 815 Kilobytes and not hitting the MB threshold (and thus get listed as 0Mi).</p> <p>We do see that graal uses more CPU and slightly less memory than the JLink setup. Both are still significantly larger than the Go CLI tool, but as long as the JNLP container takes ~100MB, I don't think we should worry about 400-500KB.</p> <pre><code>CPU \ncontainer/cpu/usage_time:gke_container:REDUCE_SUM(, ps-dev-201405): 0.24\n java-graal: 5e-4\n java-jlink: 3e-3\n jnlp: 0.23\n jpc-go: 2e-4\nMemory \n java-graal: 729,088.00\n java-jlink: 815,104.00\n jnlp: 101.507M\n jpc-go: 327,680.00\nDisk \n java-graal: 49,152.00\n java-jlink: 49,152.00\n jnlp: 94,208.00\n jpc-go: 49,152.00\n</code></pre>"},{"location":"blogs/jenkins-pipeline-support-tool/#pipeline","title":"Pipeline","text":"<pre><code>pipeline {\n    agent {\n        kubernetes {\n        label 'mypod'\n        defaultContainer 'jnlp'\n        yaml \"\"\"\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    some-label: some-label-value\nspec:\n  containers:\n  - name: java-graal\n    image: caladreas/jpc-graal:0.1.0-maven-b1\n    command:\n    - cat\n    tty: true\n  - name: java-jlink\n    image: caladreas/jpc-jlink:0.1.0-b1\n    command:\n    - cat\n    tty: true\n  - name: jpc-go\n    image: caladreas/jpc-go:0.1.0-b1\n    command:\n    - cat\n    tty: true\n        \"\"\"\n        }\n    }\n    stages {\n        stage('Test Versions') {\n            steps {\n                container('java-graal') {\n                    echo \"java-graal\"\n                    sh '/usr/local/bin/jpc-graal'\n                    sleep 5\n                }\n\n                container('java-jlink') {\n                    echo \"java-jlink\"\n                    sh '/usr/bin/jpb/bin/jpb GitChangeListToFolder abc abc'\n                    sleep 5\n                }\n\n                container('jpc-go') {\n                    sh 'jpc-go sayHello -n joost'\n                    sleep 5\n                }\n                sleep 60\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"blogs/jenkins-x/","title":"Jenkins X","text":""},{"location":"blogs/jenkins-x/#choose-your-distribution","title":"Choose your distribution","text":""},{"location":"blogs/jenkins-x/#gke-via-jx-binary","title":"GKE via JX binary","text":"<pre><code>export JX_TOOL_PSW=ZfwYM0odeI5W41GGzXgGqFmP\nexport MACHINE_TYPE=n1-standard-2\nexport GKE_ZONE=europe-west4-a\nexport K8S_VERSION=1.11.2-gke.18\nexport GKE_NAME=joostvdg-jx-nov18-1\nexport GIT_API_TOKEN=\nexport PROJECT_ID=\n</code></pre> <pre><code>jx create cluster gke \\\n    --cluster-name=${GKE_NAME} \\\n    --default-admin-password=${JX_TOOL_PSW} \\\n    --domain='kearos.net' \\\n    --git-api-token=${GIT_API_TOKEN} \\\n    --git-username='joostvdg' \\\n    --no-tiller \\\n    --project-id=${PROJECT_ID} \\\n    --prow=true  \\\n    --vault=true \\\n    --zone=${GKE_ZONE} \\\n    --machine-type=${MACHINE_TYPE} \\\n    --labels='owner=jvandergriendt,purpose=practice,team=ps' \\\n    --skip-login=true \\\n    --max-num-nodes='3' \\\n    --min-num-nodes='2' \\\n    --kubernetes-version=${K8S_VERSION} \\\n    --batch-mode=true\n</code></pre> <pre><code>--skip-installation=false: Provision cluster only, don't install Jenkins X into it\n--skip-login=false: Skip Google auth if already logged in via gcloud auth\n</code></pre>"},{"location":"blogs/k8s-controller/","title":"Create your own custom Kubernetes controller","text":"<p>Before we dive into the why and how of creating a Kubernetes Controller, let's take a brief look at what it does.</p>"},{"location":"blogs/k8s-controller/#what-is-a-controller","title":"What is a Controller","text":"<p>I will only briefly touch on what a controller is. If you already know what it is you can safely skip this paragraph. </p> <p>In applications of robotics and automation, a control loop is a non-terminating loop that regulates the state of the system. In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the API server and makes changes attempting to move the current state towards the desired state. Examples of controllers that ship with Kubernetes today are the replication controller, endpoints controller, namespace controller, and serviceaccounts controller.</p> <p>So the purpose of controllers is to control - hence the name - the state of the system - our Kubernetes cluster. A controller is generally created to watch a single resource type and make sure that its desired state is met.</p> <p>As there is already very well written material on the details of controllers, I'll leave it at this. For more information on controllers and how they work, I recommend reading bitnami's deepdive and the kubernetes documentation.</p>"},{"location":"blogs/k8s-controller/#when-to-create-your-controller","title":"When to create your controller","text":"<p>Great, you're still reading this. So when would you put in the effort to create your controller?</p> <p>I'm pretty sure there will be more cases, but the following two are the main ones.</p> <ul> <li>Process events on Core resources, Core being the resources any Kubernetes ships with</li> <li>Process events on Customer resources</li> </ul> <p>Examples of customer controllers for the first use case are tools such as Kubediff, which will compare resources in the cluster with their definition in a Git repository.</p> <p>For the second use case - custom controller for custom resource - there are many more examples. As most custom resources will have their controller to act on the events of the resources because existing controllers will not process the custom resource. Additionally, in most cases having resources sitting in a cluster with nothing happening is a bit of a waste. So we write a controller to match the resource.</p>"},{"location":"blogs/k8s-controller/#how-to-create-your-controller","title":"How to create your controller","text":"<p>When it comes to making a controller, it will be some Go (lang) code using the Kubernetes client library. This is straightforward if you're creating a controller for the core resources, but quite a few steps if you write a custom controller.</p>"},{"location":"blogs/k8s-controller/#write-a-core-resource-controller","title":"Write a core resource controller","text":"<p>To ease ourselves into it lets first create a core resource controller.</p> <p>We're aiming for a controller that can read our ConfigMaps resources. To be able to do this, we need the following:</p> <ul> <li>Handler: for the events (Created, Deleted, Updated)</li> <li>Controller: retrieves events from an informer, puts work on a queue, and delegates the events to the handler</li> <li>Entrypoint: typically a main.go file, that creates a connection to the Kubernetes API server and ties all of the resources together</li> <li>Dockerfile: to package our binary for running inside the cluster</li> <li>Resource Definition YAML: typical Kubernetes resource definition file, in our case a Deployment, so our controller will run as a pod/container</li> </ul>"},{"location":"blogs/k8s-controller/#handler","title":"Handler","text":""},{"location":"blogs/k8s-controller/#controller","title":"Controller","text":""},{"location":"blogs/k8s-controller/#entrypoint","title":"Entrypoint","text":""},{"location":"blogs/k8s-controller/#dockerfile","title":"Dockerfile","text":""},{"location":"blogs/k8s-controller/#resource-definition","title":"Resource Definition","text":""},{"location":"blogs/k8s-controller/#resources","title":"Resources","text":"<ul> <li>https://medium.com/@trstringer/create-kubernetes-controllers-for-core-and-custom-resources-62fc35ad64a3</li> <li>https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/</li> <li>https://coreos.com/blog/introducing-operators.html</li> <li>https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html</li> <li>https://github.com/joostvdg/k8s-core-resource-controller</li> <li>https://github.com/kubernetes/sample-controller/blob/master/controller.go</li> </ul>"},{"location":"blogs/k8s-crd/","title":"Kubernetes CRD","text":"<p>Kubernetes is a fantastic platform that allows you to run a lot of different workloads in various ways. It has APIs front and center, allowing you to choose different implementation as they suit you.</p> <p>Sometimes you feel something is missing. There is a concept with your application or something you want from the cluster that isn't (however) available in Kubernetes.</p> <p>It is then that you can look for extending Kubernetes itself. Either its API or by creating a new kind of resource: a Custom Resource Definition or CRD.</p>"},{"location":"blogs/k8s-crd/#what-you-need","title":"What you need","text":"<ul> <li>resource definition: the yaml  definition of your custom resource</li> <li>custom controller: a controller to interact with your custom resource</li> </ul>"},{"location":"blogs/k8s-crd/#resource-definition","title":"Resource Definition","text":"<p>As with any Kubernetes resource, you need a yaml file that defines it with the lexicon of Kubernetes. In this case, the Kind is <code>CustomerResourceDefinition</code>.</p> <pre><code>apiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: manifests.cat.kearos.net\nspec:\n  group: cat.kearos.net\n  version: v1\n  names:\n    kind: ApplicationManifest\n    plural: applicationmanifests\n    singular: applicationmanifest\n    shortNames:\n      - cam\n  scope: Namespaced\n</code></pre> <ul> <li>apiVersion: as the name implies, it's an API extension</li> <li>kind:  has to be <code>CustomResourceDefinition</code> else it wouldn't be a CRD</li> <li>name:  name must match the spec fields below, and be in the form: . <li>group:  API group name so that you can group multiple resources somewhat together</li> <li>names: <ul> <li>kind: the resource kind, used for other resource definitions</li> <li>plural is the official name used in the Kubernetes API, also the default for interaction with <code>kubectl</code></li> <li>singular:  alias for the API usage in kubectl and used as the display value</li> <li>shortNames: shortNames allow a shorter string to match your resource on the CLI</li> </ul> </li> <li>scope:  can either be <code>Namespaced</code>, tied to a specific namespace, or <code>Cluster</code> where it must be cluster-wide unique</li>"},{"location":"blogs/k8s-crd/#install-crd","title":"Install CRD","text":"<p>Taking the above example and saving it as <code>application-manifest.yml</code>, we can install the CRD into the cluster as follows.</p> <pre><code>kubectl create -f application-manifest.yml\n</code></pre>"},{"location":"blogs/k8s-crd/#resource-usage-example","title":"Resource Usage Example","text":"<pre><code>apiVersion: cat.kearos.net/v1\nkind: ApplicationManifest\nmetadata:\n    name: manifest-cat\nspec:\n    name: cat\n    description: Central Application Tracker\n    namespace: cat\n    artifactIDs:\n        - github.com/joostvdg/cat\n    sources:\n        - git@github.com:joostvdg/cat.git\n</code></pre> <p>Looking at this example, you might wonder how this works. There is a specification in there - <code>spec</code> - with all kinds of custom fields. But where do they come from?</p> <p>Nowhere really, so you cannot validate this with the CRD alone. You can put any arbitrary field in there. </p> <p>So what do you do with the CRD then? You can create a custom controller that processes your custom resources. Because creating a custom controller for your custom resources is complicated and takes several steps, we will do this in a separate article.</p>"},{"location":"blogs/k8s-lets-encrypt/","title":"Let's Encrypt for Kubernetes","text":"<p>Let's Encrypt is a free, automated and open Certificate Authority. The kind of service you need if you want to have a secure website with https - yes I know that requires more than that - and it's now more straightforward to use than ever.</p> <p>This about using Let's Encrypt for generating a certificate for your service on Kubernetes. There are several ways to do this, with more or less automation, cluster-wide or namespace bound or with a DNS or HTTP validation check.</p> <p>I'll choose the route that was the easiest for me, and then I'll briefly look at the other options.</p>"},{"location":"blogs/k8s-lets-encrypt/#prerequisites","title":"Prerequisites","text":"<p>There are some prerequisites required, that are best discussed on their own. So we will continue with the assumption that you have these in order.</p> <ul> <li>valid Class A or CNAME domain name</li> <li>kubernetes cluster<ul> <li>with ingress controller (such as nginx)</li> <li>with helm and tiller installed in the cluster</li> </ul> </li> <li>web application</li> </ul>"},{"location":"blogs/k8s-lets-encrypt/#steps","title":"Steps","text":"<p>The steps to take to get a web application to get a certificate from Let's Encrypt are the following.</p> <ul> <li>install cert-manager from the official helm chart</li> <li>deploy a <code>Issuer</code> resource</li> <li>deploy a certificate resource</li> <li>confirm certificate and secret are created/filled</li> <li>use in web app</li> </ul>"},{"location":"blogs/k8s-lets-encrypt/#install-cert-manager","title":"Install Cert Manager","text":"<p>For more details on Cert Manager, I recommend reading their introduction.</p> <p>In essence, it's a tool that helps you initiate a certificate request with a service such as Let's Encrypt.</p> <p>You can install it via Helm, and it's meant to be installed only once per cluster.  The once per cluster restriction is due to the usage of Custom Resource Definitions (CRD's) which will block any (re-)installation.</p> <p>To confirm if there are any CRD's from cert-manager, you can issue the following command.</p> <pre><code>kubectl get customresourcedefinitions.apiextensions.k8s.io\n</code></pre> <p>The CRD's belonging to cert-manager are the following:</p> <ul> <li>certificates.certmanager.k8s.io</li> <li>clusterissuers.certmanager.k8s.io</li> <li>issuers.certmanager.k8s.io</li> </ul> <p>You will notice later, that we will use these CRD's for getting our certificate, so keep them in mind. To remove them in case of a re-install, you can issue the command below.</p> <pre><code>kubectl delete customresourcedefinitions.apiextensions.k8s.io \\\n    certificates.certmanager.k8s.io \\\n    clusterissuers.certmanager.k8s.io \\\n    issuers.certmanager.k8s.io\n</code></pre> <p>When you're sure there are no CRD's left, you can install cert-manager via it's helm chart. It has some options in case you need them, you read about them here, but in my case that wasn't needed.</p> <pre><code>helm install --name cert-manager --namespace default stable/cert-manager\n</code></pre>"},{"location":"blogs/k8s-lets-encrypt/#deploy-issuer","title":"Deploy Issuer","text":"<p>To be able to use a certificate we need to have a Certificate Issuer.</p> <p>If you remember from our <code>cert-manager</code>, there are two CRD's that can take this role:</p> <ul> <li>ClusterIssuer: clusterissuers.certmanager.k8s.io</li> <li>Issuer: issuers.certmanager.k8s.io</li> </ul> <p>Both issuer type can use two ways of providing the proof of ownership, either by <code>dns-01</code> or <code>http-01</code>.</p> <p>We'll be using the <code>http-01</code> method, for the <code>dns-01</code> method, refer to the cert-manager documenation.</p>"},{"location":"blogs/k8s-lets-encrypt/#clusterissuer","title":"ClusterIssuer","text":"<p>As the resource <code>Kind</code> implies, a <code>ClusterIssuer</code> is a cluster-wide resource and not bound to a specific namespace.</p> <pre><code>apiVersion: certmanager.k8s.io/v1alpha1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # The ACME server URL\n    server: https://acme-staging.api.letsencrypt.org/directory\n    # Email address used for ACME registration\n    email: user@example.com\n    # Name of a secret used to store the ACME account private key\n    privateKeySecretRef:\n      name: letsencrypt-staging\n    # Enable the HTTP-01 challenge provider\n    http01: {}\n</code></pre>"},{"location":"blogs/k8s-lets-encrypt/#issuer","title":"Issuer","text":"<p>Not everyone wants a cluster-wide resource, and not everyone has the rights to install something elsewhere than their namespace.</p> <p>I prefer having as much as possible tied to a namespace - either a team or an application - I will use this type.</p> <pre><code>apiVersion: certmanager.k8s.io/v1alpha1\nkind: Issuer\nmetadata:\n  name: myapp-letsencrypt-staging\n  namespace: myapp\nspec:\n  acme:\n    # The ACME server URL\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    # Email address used for ACME registration\n    email: myadmin@myapp.com\n    # Name of a secret used to store the ACME account private key\n    privateKeySecretRef:\n      name: myapp-letsencrypt-staging\n    # Enable the HTTP-01 challenge provider\n    http01: {}\n</code></pre> <p>There's a few things to note here:</p> <ul> <li>server: this refers to the server executing the ACME test, in this case: Let's Encrypt Staging (with the v2 API)</li> <li>email: this will be the account it will use for registering the certificate</li> <li>privateKeySecretRef: this is the Kubernetes <code>secret</code> resource in which the privateKey will be stored, just in case you need or want to remove it</li> </ul>"},{"location":"blogs/k8s-lets-encrypt/#deploy-certificate-resource","title":"Deploy Certificate Resource","text":"<p>Next up is our <code>Certificate</code> resource, this is where <code>cert-manager</code> will store our certificate details to be used by our application.</p> <p>In case you forgot, this is one of the three CRD's provided by cert-manager.</p> <pre><code>apiVersion: certmanager.k8s.io/v1alpha1\nkind: Certificate\nmetadata:\n  name: myapp-example-com\n  namespace: myapp\nspec:\n  secretName: myapp-tls\n  dnsNames:\n  - myapp.example.com\n  acme:\n    config:\n    - http01:\n        ingressClass: nginx\n      domains:\n      - myapp.example.com\n  issuerRef:\n    name: myapp-letsencrypt-staging\n    kind: Issuer\n</code></pre> <p>The things to note here:</p> <ul> <li>name: so far I've found it a naming convention to write the domain name where <code>-</code> replaces the <code>.</code>'s.</li> <li>secretName: the name of the Kubernetes secret that will house the certificate and certificate key</li> <li>dnsNames: you can specify more than one name, in our case just a single one, should match <code>acme.config.domains</code></li> <li>acme.config: this defines the configuration for how the ownership proof should be done, this should match the method defined in the <code>Issuer</code></li> <li>issuerRef: in good Kubernetes fashion, we reference the <code>Issuer</code> that should issue our certificate, the name and kind should match our Issue resource</li> </ul>"},{"location":"blogs/k8s-lets-encrypt/#confirm-resources","title":"Confirm Resources","text":"<p>We have defined our set of resources that should create our valid - though not trusted, as it is staging - certificate. Before we use it, we should confirm our secret and our certificate are both valid.</p> <pre><code>kubectl describe certificate myapp-example-com --namespace myapp\n</code></pre> <p>The response includes the latest status, which looks like this:</p> <pre><code>Status:\n  Acme:\n    Order:\n      URL:  https://acme-staging-v02.api.letsencrypt.org/acme/order/705.../960...\n  Conditions:\n    Last Transition Time:  2018-10-02T21:17:34Z\n    Message:               Certificate issued successfully\n    Reason:                CertIssued\n    Status:                True\n    Type:                  Ready\n</code></pre> <p>Next up is our secret, containing the actual certificate and the certificate key.</p> <pre><code>kubectl describe secret myapp-tls --namespace myapp\n</code></pre> <p>Which results in something like this:</p> <pre><code>Name:         myapp-tls\nNamespace:    myapp\nLabels:       certmanager.k8s.io/certificate-name=myapp-example-com\nAnnotations:  certmanager.k8s.io/alt-names: myapp.example.com\n              certmanager.k8s.io/common-name: myapp.example.com\n              certmanager.k8s.io/issuer-kind: Issuer\n              certmanager.k8s.io/issuer-name: myapp-letsencrypt-staging\n\nType:  kubernetes.io/tls\n\nData\n====\ntls.crt:  3797 bytes\ntls.key:  1679 bytes\n</code></pre>"},{"location":"blogs/k8s-lets-encrypt/#use-certificate-to-enable-https","title":"Use certificate to enable https","text":"<p>Assuming the secret and the certificate are correct, we can use them to enable https on our web app.</p> <p>We put the tls certificate on the ingress of the application, in this case, Nginx, which assumes the following about the app:</p> <ul> <li>it has a deployment or stateful set</li> <li>it has a service which provides and endpoint to one or more instances</li> <li>it has an nginx ingress which points to the service</li> </ul>"},{"location":"blogs/k8s-lets-encrypt/#deployment","title":"Deployment","text":"<pre><code>kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: myapp\n  namespace: myapp\n  labels:\n    app: myapp\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp\n        image: caladreas/catnip-master\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n</code></pre> <p>You should always include livenessprobe, resource limits and so on. But for the sake of brevity, these are omitted!</p>"},{"location":"blogs/k8s-lets-encrypt/#service","title":"Service","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: myapp\n  namespace: myapp\n  labels:\n    app: myapp\nspec:\n  selector:\n    app: myapp\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8087\n    protocol: TCP\n</code></pre>"},{"location":"blogs/k8s-lets-encrypt/#ingress-for-issuer","title":"Ingress for Issuer","text":"<pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: myapp\n  namespace: myapp\n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"\n    ingress.kubernetes.io/ssl-redirect: \"true\"\n    certmanager.k8s.io/issuer: myapp-letsencrypt-staging\n    certmanager.k8s.io/acme-challenge-type: http01\nspec:\n  rules:\n  - host: myapp.example.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: myapp\n          servicePort: 80\n  tls:\n  - hosts:\n    - myapp.example.com\n    secretName: myapp-tls\n</code></pre>"},{"location":"blogs/k8s-lets-encrypt/#ingress-for-clusterissuer","title":"Ingress for ClusterIssuer","text":"<pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: myapp\n  namespace: myapp\n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"\n    ingress.kubernetes.io/ssl-redirect: \"true\"\n    certmanager.k8s.io/cluster-issuer: letsencrypt-staging\n    certmanager.k8s.io/acme-challenge-type: http01\nspec:\n  rules:\n  - host: myapp.example.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: myapp\n          servicePort: 80\n  tls:\n  - hosts:\n    - myapp.example.com\n    secretName: myapp-tls\n</code></pre>"},{"location":"blogs/k8s-lets-encrypt/#further-resources","title":"Further resources","text":"<ul> <li>How Does Let's Encrypt Work</li> <li>Tutorial that inspired this page</li> <li>Amazone EKS Ingress Guide</li> <li>Kuberetes EKS Ingress and TLS</li> <li>How To Configure LTS for Nginx Ingress</li> </ul>"},{"location":"blogs/kubernetes-post-install/","title":"Kubernetes Post Install","text":"<p>What to do after you've installed your Kubernetes cluster, whether that was EKS via eksctl or GKE via gcloud.</p> <ul> <li>make network more secure with encryption<ul> <li>weavenet for example</li> </ul> </li> <li>install package manager<ul> <li>install helm &amp; tiller</li> </ul> </li> <li>use nginx for ssl termination together with Let's Encrypt<ul> <li>install nginx</li> <li>install cert-manager</li> </ul> </li> </ul>"},{"location":"blogs/kubernetes-post-install/#helm-tiller","title":"Helm &amp; Tiller","text":"<p>Helm is the defacto standard package manager for Kubernetes.</p> <p>Its current iteration is version 2, which has a client component - Helm - and a serverside component, Tiller.</p> <p>There's a problem with that, due this setup with Helm and Tiller, Tiller is aking to a cluster admin. This isn't very secure and there are several ways around that.</p> <ul> <li>JenkinsX: its binary (<code>jx</code>) can install helm charts without using Tiller. It generates the kubernetes resource files and installs these directly</li> <li>custom RBAC setup: you can also setup RBAC in such a way that every separate namespace gets its own Tiller, limiting the reach of any Tiller</li> </ul>"},{"location":"blogs/kubernetes-post-install/#tiller-custom-rbac-example","title":"Tiller Custom RBAC Example","text":""},{"location":"blogs/kubernetes-post-install/#namespaces","title":"Namespaces","text":"<pre><code>kind: Namespace\napiVersion: v1\nmetadata:\n  name: sre\n---\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: dev1\n---\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: dev2\n</code></pre>"},{"location":"blogs/kubernetes-post-install/#service-accounts","title":"Service Accounts","text":"<pre><code>kind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: tiller\n  namespace: dev1\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: tiller\n  namespace: dev2\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: helm\n  namespace: sre\n</code></pre>"},{"location":"blogs/kubernetes-post-install/#roles","title":"Roles","text":"<pre><code>kind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: tiller-manager\n  namespace: dev1\nrules:\n- apiGroups: [\"\", \"batch\", \"extensions\", \"apps\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: tiller-manager\n  namespace: dev2\nrules:\n- apiGroups: [\"\", \"batch\", \"extensions\", \"apps\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: helm-clusterrole\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods/portforward\"]\n    verbs: [\"create\"]\n  - apiGroups: [\"\"]\n    resources: [\"pods\"]\n    verbs: [\"list\", \"get\"]\n</code></pre>"},{"location":"blogs/kubernetes-post-install/#rolebindings","title":"RoleBindings","text":"<pre><code>kind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: tiller-binding\n  namespace: dev1\nsubjects:\n- kind: ServiceAccount\n  name: tiller\n  namespace: dev1\nroleRef:\n  kind: Role\n  name: tiller-manager\n  apiGroup: rbac.authorization.k8s.io\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: tiller-binding\n  namespace: dev2\nsubjects:\n- kind: ServiceAccount\n  name: tiller\n  namespace: dev2\nroleRef:\n  kind: Role\n  name: tiller-manager\n  apiGroup: rbac.authorization.k8s.io\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: helm-clusterrolebinding\nroleRef:\n  kind: ClusterRole\n  apiGroup: rbac.authorization.k8s.io\n  name: helm-clusterrole\nsubjects:\n  - kind: ServiceAccount\n    name: helm\n    namespace: sre\n</code></pre>"},{"location":"blogs/kubernetes-post-install/#install-tiller","title":"Install Tiller","text":"<pre><code>helm init --service-account tiller --tiller-namespace dev1\nhelm init --service-account tiller --tiller-namespace dev2\n</code></pre>"},{"location":"blogs/kubernetes-post-install/#create-kubeconfig-for-helm-client","title":"Create KubeConfig for Helm client","text":"<pre><code># Find the secret associated with the Service Account\nSECRET=$(kubectl -n sre get sa helm -o jsonpath='{.secrets[].name}')\n\n# Retrieve the token from the secret and decode it\nTOKEN=$(kubectl get secrets -n sre $SECRET -o jsonpath='{.data.token}' | base64 -D)\n\n# Retrieve the CA from the secret, decode it and write it to disk\nkubectl get secrets -n sre $SECRET -o jsonpath='{.data.ca\\.crt}' | base64 -D &gt; ca.crt\n\n# Retrieve the current context\nCONTEXT=$(kubectl config current-context)\n\n# Retrieve the cluster name\nCLUSTER_NAME=$(kubectl config get-contexts $CONTEXT --no-headers=true | awk '{print $3}')\n\n# Retrieve the API endpoint\nSERVER=$(kubectl config view -o jsonpath=\"{.clusters[?(@.name == \\\"${CLUSTER_NAME}\\\")].cluster.server}\")\n\n# Set up variables\nKUBECONFIG_FILE=config USER=helm CA=ca.crt\n\n# Set up config\nkubectl config set-cluster $CLUSTER_NAME \\\n    --kubeconfig=$KUBECONFIG_FILE \\\n    --server=$SERVER \\\n    --certificate-authority=$CA \\\n    --embed-certs=true\n\n# Set token credentials\nkubectl config set-credentials \\\n    $USER \\\n    --kubeconfig=$KUBECONFIG_FILE \\\n    --token=$TOKEN\n\n# Set context entry\nkubectl config set-context \\\n    $USER \\\n    --kubeconfig=$KUBECONFIG_FILE \\\n    --cluster=$CLUSTER_NAME \\\n    --user=$USER\n\n# Set the current-context\nkubectl config use-context $USER \\\n    --kubeconfig=$KUBECONFIG_FILE\n</code></pre>"},{"location":"blogs/kubernetes-post-install/#helm-install","title":"Helm Install","text":"<pre><code>helm install \\\n    --name prometheus \\\n    stable/prometheus \\\n    --tiller-namespace dev1 \\\n    --kubeconfig config \\\n    --namespace dev1 \\\n    --set rbac.create=false\n\nNAME:   prometheus\nLAST DEPLOYED: Sun Oct 28 16:22:46 2018\nNAMESPACE: dev1\nSTATUS: DEPLOYED\n</code></pre> <pre><code>helm install --name grafana \\\n    stable/grafana \\\n    --tiller-namespace dev2 \\\n    --kubeconfig config \\\n    --namespace dev2 \\\n    --set rbac.pspEnabled=false \\\n    --set rbac.create=false\n\nNAME:   grafana\nLAST DEPLOYED: Sun Oct 28 16:25:18 2018\nNAMESPACE: dev2\nSTATUS: DEPLOYED\n</code></pre>"},{"location":"blogs/kubernetes-post-install/#references","title":"References","text":"<ul> <li>https://medium.com/@elijudah/configuring-minimal-rbac-permissions-for-helm-and-tiller-e7d792511d10</li> <li>https://medium.com/virtuslab/think-twice-before-using-helm-25fbb18bc822</li> <li>https://jenkins-x.io/architecture/helm3/</li> <li>https://gist.github.com/innovia/fbba8259042f71db98ea8d4ad19bd708#file-kubernetes_add_service_account_kubeconfig-sh</li> </ul>"},{"location":"blogs/kubernetes-sso-keycloak/","title":"Single Sign On on Kubernetes with Keycloak","text":"<p>This article is about setting up an Apache Keycloak<sup>1</sup> instance for Single Sign-On<sup>2</sup> (SSO) on Kubernetes.</p> <p>Important</p> <p>This guide is created to help you show how you can do this from a technical point of view. If you are in an enterprise, please consult your in-house - if available - or external security professionals before running tools such as Keycloak in production.</p>"},{"location":"blogs/kubernetes-sso-keycloak/#goal","title":"Goal","text":"<p>The goal is to show why using Keycloak as SSO can be valuable and guide on how to do so.</p> <p>For what's in the <code>How</code>, look at the next paragraph: <code>Steps</code>.</p> <p>At the outset, this article is not about comparing SSO solutions or about claiming Keycloak is the best. Its a solution, and if you're interested in using this, it helps you with the how.</p> <p>Below are some links to alternatives as reference, but are not discussed.</p>"},{"location":"blogs/kubernetes-sso-keycloak/#audience","title":"Audience","text":"<p>The expected audience are those who are using Kubernetes<sup>3</sup> and are:</p> <ul> <li>looking to host a SSO solution themselves</li> <li>want to use LDAP<sup>4</sup> as <code>User Federation</code> for Keycloak</li> <li>want to setup a SSO with Jenkins<sup>5</sup> and/or SonarQube<sup>6</sup></li> </ul>"},{"location":"blogs/kubernetes-sso-keycloak/#steps","title":"Steps","text":"<p>These are the steps we will execute.</p> <ul> <li>configure an LDAP server with a test data set</li> <li>package LDAP server in a Docker container and run it as an kubernetes <code>Deployment</code><ul> <li>you might think, why not a <code>StatefulSet</code>, but our LDAP will have static data</li> <li>and the idea of this setup, is that we do not control LDAP, meaning, we should expect LDAP's data to be immutable</li> </ul> </li> <li>install Keycloak with Helm<sup>8</sup><ul> <li>we use Helm 2, as Helm 3<sup>9</sup> is still in beta at the time of writing (September 2019)</li> </ul> </li> <li>configure Keycloak to use LDAP for <code>User Federation</code></li> <li>install Jenkins and SonarQube with Helm<sup>8</sup></li> <li>configure SSO with Keycloak in Jenkins</li> <li>configure SSO with Keycloak in SonarQube</li> </ul>"},{"location":"blogs/kubernetes-sso-keycloak/#ldap","title":"LDAP","text":"<p>We will use OpenDJ<sup>7</sup> as LDAP implementation. There are many alternatives out there, feel free to use those. But for this guide, we will use OpenDJ's community edition, it works well and is easy to configure.</p> <p>We need the following:</p> <ul> <li>configured with a test data set including users and groups</li> <li>Docker container image definition</li> <li>Kubernetes <code>Deployment</code> definition to run</li> <li>Kubernetes <code>Service</code> definition to access a stable address</li> </ul>"},{"location":"blogs/kubernetes-sso-keycloak/#test-data-set","title":"Test Data Set","text":"example.ldiff <p>This a full example.</p> <p>The different parts will be explained below.</p> <pre><code>dn: dc=example,dc=com\nobjectclass: top\nobjectclass: domain\ndc: example\n\ndn: ou=People, dc=example,dc=com\nobjectclass: top\nobjectclass: organizationalunit\nou: People\naci: (targetattr=\"*||+\")(version 3.0; acl \"IDM Access\"; allow (all)\nuserdn=\"ldap:///uid=idm,ou=Administrators,dc=example,dc=com\";)\n\ndn: uid=cptjack, ou=People, dc=example,dc=com\ncn: cpt. Jack Sparrow\nsn: Sparrow\ngivenname: Jack\nobjectclass: top\nobjectclass: person\nobjectclass: organizationalPerson\nobjectclass: inetOrgPerson\nou: Operations\nou: People\nl: Caribbean\nuid: cptjack\nmail: jack@example.com\ntelephonenumber: +421 910 123456\nfacsimiletelephonenumber: +1 408 555 1111\nroomnumber: 666\nuserpassword: MyAwesomePassword\n\ndn: uid=djones, ou=People, dc=example,dc=com\ncn: Davy Jones\nsn: Jones\ngivenname: Davy\nobjectclass: top\nobjectclass: person\nobjectclass: organizationalPerson\nobjectclass: inetOrgPerson\nou: Operations\nou: People\nl: Caribbean\nuid: djones\nmail: d.jones@example.com\ntelephonenumber: +421 910 382735\nfacsimiletelephonenumber: +1 408 555 1112\nroomnumber: 112\nuserpassword: MyAwesomePassword\n\ndn: ou=Groups, dc=example,dc=com\nobjectclass: top\nobjectclass: organizationalunit\nou: Groups\naci: (targetattr=\"*||+\")(version 3.0; acl \"IDM Access\"; allow (all)\nuserdn=\"ldap:///uid=idm,ou=Administrators,dc=example,dc=com\";)\n\ndn: cn=Pirates,ou=Groups,dc=example,dc=com\nobjectclass: top\nobjectclass: groupOfUniqueNames\ncn: Pirates\nou: Groups\nuniquemember: uid=cptjack, ou=People, dc=example,dc=com\nuniquemember: uid=djones, ou=People, dc=example,dc=com\ndescription: Arrrrr!\n\ndn: cn=Catmins,ou=Groups,dc=example,dc=com\nobjectclass: top\nobjectclass: groupOfUniqueNames\ncn: Catmins\nou: Groups\nuniquemember: uid=djones, ou=People, dc=example,dc=com\ndescription: Purrrr!\n\ndn: ou=Administrators, dc=example,dc=com\nobjectclass: top\nobjectclass: organizationalunit\nou: Administrators\n\ndn: uid=idm, ou=Administrators,dc=example,dc=com\nobjectclass: top\nobjectclass: person\nobjectclass: organizationalPerson\nobjectclass: inetOrgPerson\nuid: idm\ncn: IDM Administrator\nsn: IDM Administrator\ndescription: Special LDAP acccount used by the IDM\nto access the LDAP data.\nou: Administrators\nuserPassword: MySecretAdminPassword\nds-privilege-name: unindexed-search\nds-privilege-name: password-reset\n</code></pre>"},{"location":"blogs/kubernetes-sso-keycloak/#basics","title":"Basics","text":"<p>We need some basic meta data for our server.  The two most important elements being:</p> <ul> <li>the base <code>dn</code> (Distinguished Name), where all of our data lives<ul> <li>build up out of <code>dc</code> (domainComponent) elements, representing a domain name, usually companyName.com</li> </ul> </li> <li>the <code>dn</code> where our user data will live and which user owns it (the <code>userdn</code> assigned in the <code>aci</code>)</li> </ul> <pre><code>dn: dc=example,dc=com\nobjectclass: top\nobjectclass: domain\ndc: example\n\ndn: ou=People, dc=example,dc=com\nobjectclass: top\nobjectclass: organizationalunit\nou: People\naci: (targetattr=\"*||+\")(version 3.0; acl \"IDM Access\"; allow (all)\n  userdn=\"ldap:///uid=idm,ou=Administrators,dc=example,dc=com\";)\n</code></pre>"},{"location":"blogs/kubernetes-sso-keycloak/#user-entries","title":"User Entries","text":"<p>After these two main metadata <code>dn</code> elements, we have all the user entries.</p> <p>We start with the identifier of the entry, designated by the <code>dn</code>, followed by attributes and classifications. While LDAP stands for Lightweight, it has a lot of attributes which are not easily understood because they are commonly used by their two-letter acronym only. Below we explain a few of the attributes, but there are more complete lists available<sup>10</sup>.</p> <ul> <li>dn: the full identifier of the data entry, in reverse tree</li> <li>uid: the unique id of the user within the organizational structure</li> <li>cn: Common Name, generally how humans would identify this resource</li> <li>l: Location</li> <li>ou: Organizational Unit, these commonly represent departments</li> <li>objectclass: defines a object hierarchy, derived from Object Oriented Programming<ul> <li>top: is commonly the root object class, every other object inheriting from there</li> <li>object classes are used to create typed objects which have their own schema</li> <li>this allows you to define what values are required and which are optional</li> <li>object classes are also used in search filters, so you can distinguish different value types (users, groups)</li> </ul> </li> </ul> <p>Organizational Tree</p> <pre><code>.\n\u251c\u2500\u2500 com\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 example\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 Operations\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 People\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 jwick\n</code></pre> <pre><code>dn: uid=jwick, ou=People, dc=example,dc=com\ncn: John Wick\nsn: Wick\ngivenname: John\nobjectclass: top\nobjectclass: person\nobjectclass: organizationalPerson\nobjectclass: inetOrgPerson\nou: Operations\nou: People\nl: New York\nuid: jwick\nmail: jwick@example.com\ntelephonenumber: +1 408 555 1236\nfacsimiletelephonenumber: +1 408 555 4323\nroomnumber: 233\nuserpassword: myawesomepassword\n</code></pre>"},{"location":"blogs/kubernetes-sso-keycloak/#group","title":"Group","text":"<p>We first create the <code>OU</code> housing our Groups, including which object classes it represents.</p> <p>Then we can attach groups to this <code>OU</code> by creating <code>dn</code>'s under it. There are two flavors of groups, we have <code>groupOfNames</code> and <code>groupOfUniqueNames</code>, I'm sure you understand what the difference is.</p> <p>We explain wich kind of group we are by the <code>objectclass: groupOfUniqueNames</code>, and include a list of members. In the case of <code>groupOfNames</code> we use <code>member: &lt;member dn&gt;</code>, in our case we user <code>uniquemember: uid=cptjack, ou=People, dc=example,dc=com</code>. To make it a list, add more than one entry, each with a unique value.</p> <pre><code>dn: ou=Groups, dc=example,dc=com\nobjectclass: top\nobjectclass: organizationalunit\nou: Groups\naci: (targetattr=\"*||+\")(version 3.0; acl \"IDM Access\"; allow (all)\n  userdn=\"ldap:///uid=idm,ou=Administrators,dc=example,dc=com\";)\n\ndn: cn=Pirates,ou=Groups,dc=example,dc=com\nobjectclass: top\nobjectclass: groupOfUniqueNames\ncn: Pirates\nou: Groups\nuniquemember: uid=cptjack, ou=People, dc=example,dc=com\nuniquemember: uid=will, ou=People, dc=example,dc=com\nuniquemember: uid=djones, ou=People, dc=example,dc=com\ndescription: Arrrrr!\n</code></pre>"},{"location":"blogs/kubernetes-sso-keycloak/#administrator","title":"Administrator","text":"<p>We have to configure our main administrator, as we've already given it a lot of privileges to manage the other resources (via the <code>aci</code> rules). As usual, we start with defining a <code>ou</code> that houses our <code>Administrators</code>.</p> <p>In this case, we give it two more special permissions via <code>ds-privilege-name</code>. So this user can also reset passwords, just in case.</p> <pre><code>dn: ou=Administrators, dc=example,dc=com\nobjectclass: top\nobjectclass: organizationalunit\nou: Administrators\n\ndn: uid=idm, ou=Administrators,dc=example,dc=com\nobjectclass: top\nobjectclass: person\nobjectclass: organizationalPerson\nobjectclass: inetOrgPerson\nuid: idm\ncn: IDM Administrator\nsn: IDM Administrator\ndescription: Special LDAP acccount used by the IDM\n  to access the LDAP data.\nou: Administrators\nuserPassword: MySecretAdminPassword\nds-privilege-name: unindexed-search\nds-privilege-name: password-reset\n</code></pre>"},{"location":"blogs/kubernetes-sso-keycloak/#configure-opendj","title":"Configure OpenDJ","text":"<p>One of the reasons I like OpenDJ is because it is very easy to operate. It has many tools to help you manage the server. Some are separate tools that interact with a running server, others interact with the configure while the server is offline.</p> <p>We have to do two configuration actions, 1) we intialize it with our root <code>dn</code>, which port to use and so on, 2) we add our test data set, so we have our groups and users to work with.</p> <pre><code>/opt/opendj/setup --cli \\\n    -p 1389 \\\n    --ldapsPort 1636 \\\n    --enableStartTLS \\\n    --generateSelfSignedCertificate \\\n    --baseDN dc=example,dc=com \\\n    -h localhost \\\n    --rootUserDN \"$ROOT_USER_DN\" \\\n    --rootUserPassword \"$ROOT_PASSWORD\" \\\n    --acceptLicense \\\n    --no-prompt \\\n    --doNotStart\n</code></pre> <pre><code>/opt/opendj/bin/import-ldif \\\n    --includeBranch dc=example,dc=com \\\n    --backendID userRoot \\\n    --offline \\\n    --ldifFile example.ldiff\n</code></pre>"},{"location":"blogs/kubernetes-sso-keycloak/#dockerfile","title":"Dockerfile","text":"<p>For completeness I will also include my Docker image.</p> <p>We use Tini<sup>11</sup> order to manage the process of OpenDJ nicely, even in the face of being shutdown. For more information, read my article on Docker Graceful Shutdown.</p> <p>We use a <code>jre</code> as we only need to run Java, so no need for a JDK. We use the OpenJDK distribution from <code>Azul</code><sup>15</sup> in order to avoid possible violations of Oracle's License<sup>12</sup>. We use Azul's Alpine<sup>13</sup> based image as it is much smaller than those based of full-fledged OS's. This makes the image smaller in size on disk and in memory and reduces the attack vector and recommended by the likes of Docker and Snyk<sup>14</sup>.</p> <p>We download the latest version available which, as of this writing in September 2019, is <code>4.4.3</code> which you can retrieve from the OpenDJ Community's <code>releases</code> page on GitHub<sup>13</sup>.</p> <pre><code>FROM azul/zulu-openjdk-alpine:8u222-jre\n\nLABEL authors=\"Joost van der Griendt &lt;joostvdg@gmail.com&gt;\"\nLABEL version=\"0.2.0\"\nLABEL description=\"OpenDJ container\"\n\nWORKDIR /opt\nEXPOSE 1389 1636 4444\n\nENV CHANGE_DATE='20190916-2100'\nENV JAVA_HOME /usr/lib/jvm/zulu-8\nENV OPENDJ_JAVA_HOME /usr/lib/jvm/zulu-8\nENV VERSION=4.4.3\nENV ROOT_USER_DN='cn=admin'\nENV ROOT_PASSWORD='changeme'\nRUN apk add --no-cache tini\n\nENTRYPOINT [\"/sbin/tini\", \"-vv\",\"-g\",\"-s\", \"--\"]\nCMD [\"/opt/opendj/bin/start-ds\", \"--nodetach\"]\n\nRUN wget --quiet \\\n    https://github.com/OpenIdentityPlatform/OpenDJ/releases/download/$VERSION/opendj-$VERSION.zip &amp;&amp; \\\n    unzip opendj-$VERSION.zip &amp;&amp; \\\n    rm -r opendj-$VERSION.zip\n\nRUN /opt/opendj/setup --cli \\\n    -p 1389 \\\n    --ldapsPort 1636 \\\n    --enableStartTLS \\\n    --generateSelfSignedCertificate \\\n    --baseDN dc=example,dc=com \\\n    -h localhost \\\n    --rootUserDN \"$ROOT_USER_DN\" \\\n    --rootUserPassword \"$ROOT_PASSWORD\" \\\n    --acceptLicense \\\n    --no-prompt \\\n    --doNotStart\n\nADD Example.ldif /var/tmp/example.ldiff\n# RUN /opt/opendj/bin/import-ldif --help\nRUN /opt/opendj/bin/import-ldif --includeBranch dc=example,dc=com --backendID userRoot --offline --ldifFile /var/tmp/example.ldiff\n</code></pre>"},{"location":"blogs/kubernetes-sso-keycloak/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>In production you might want to run LDAP in a <code>StatefulSet</code> and give it some permanent storage. But in this guide the goal of LDAP is to show how to use it with Keycloak and we stick to a <code>Deployment</code> as it is easier.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opendj4\n  labels:\n    app: opendj4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: opendj4\n  template:\n    metadata:\n      labels:\n        app: opendj4\n    spec:\n      containers:\n      - name: opendj4\n        image: caladreas/opendj:4.4.3-1\n        ports:\n        - containerPort: 1389\n          name: ldap\n        resources:\n          requests:\n            memory: \"250Mi\"\n            cpu: \"50m\"\n          limits:\n            memory: \"500Mi\"\n            cpu: \"250m\"\n</code></pre>"},{"location":"blogs/kubernetes-sso-keycloak/#kubernetes-service","title":"Kubernetes Service","text":"<p>When we use a <code>Deployment</code> our container instance will have a generated name and a new ip address on every (re-)start. So we use a <code>Service</code> to create a stable endpoint, which means that we will now access our LDAP server via our service: <code>ldap://opendj4:389</code>.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: opendj4\n  name: opendj4\nspec:\n  ports:\n    - name: http\n      port: 389\n      targetPort: 1389\n      protocol: TCP\n  selector:\n    app: opendj4\n</code></pre> <p>Caution</p> <p>If you deploy the <code>Service</code> and <code>Deployment</code> in a different namespace than where you want to access them from, you will have to add the namespace to the access url.</p> <p>If you've configured them in namespace <code>ldap</code>, the access url becomes <code>ldap://opendj4.ldap:389</code>.</p>"},{"location":"blogs/kubernetes-sso-keycloak/#homegrown-helm-chart-with-github","title":"Homegrown Helm chart with GitHub","text":"<p>You can also package the above explained Docker Image + Kubernetes Yaml definition as a Helm package in a GitHub repository<sup>16</sup>.</p> <p>So if you do not want to use any of the above, feel free to use my personal Helm Repository<sup>17</sup>.</p> <pre><code>helm repo add joostvdg https://raw.githubusercontent.com/joostvdg/helm-repo/master/\nhelm repo update\n</code></pre> <pre><code>helm install joostvdg/opendj4 --name ldap --namespace ldap\n</code></pre> <p>The service, retrieved by <code>kubectl get service</code> now has a different name.</p> <pre><code>NAME           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE\nldap-opendj4   ClusterIP   10.100.19.29     &lt;none&gt;        389/TCP   105s\n</code></pre> <p>So now we access the LDAP server via <code>ldap://ldap-opendj4:389</code>.</p>"},{"location":"blogs/kubernetes-sso-keycloak/#keycloak","title":"Keycloak","text":""},{"location":"blogs/kubernetes-sso-keycloak/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Helm installed</li> <li>TLS certificate</li> </ul> <p>Using a tool as Keycloak to do SSO well, feels wrong without using TLS certificates. So I wholeheartedly recommend configuring Keycloak with a Domain name and TLS Certificate.</p> <p>For the TLS certificate, you use Let's Encrypt<sup>20</sup> with Cert Manager<sup>19</sup>, if you unsure how to proceed in Kubernetes read my guide on Let's Encrypt on Kubernetes.</p>"},{"location":"blogs/kubernetes-sso-keycloak/#install-via-helm","title":"Install Via Helm","text":"<pre><code>helm repo add codecentric https://codecentric.github.io/helm-charts\nhelm repo update\n</code></pre> <pre><code>kubectl apply -f keycloak-certificate.yaml\n</code></pre> <pre><code>helm install --name keycloak codecentric/keycloak -f keycloak-values.yaml\n</code></pre> <p>Note</p> <p>Make sure you replace the dns name <code>keycloak.my.domain.com</code> with your own domain. If you do not have a domain, you can use `nip.io<sup>18</sup>.</p> <p>keycloak-certificate.yaml</p> <pre><code>apiVersion: certmanager.k8s.io/v1alpha1\nkind: Certificate\nmetadata:\nname: keycloak.my.domain.com\nspec:\nsecretName: tls-keycloak\ndnsNames:\n- keycloak.my.domain.com\nacme:\n    config:\n    - http01:\n        ingressClass: nginx\n    domains:\n    - keycloak.my.domain.com\nissuerRef:\n    name: letsencrypt-prod\n    kind: ClusterIssuer\n</code></pre> <p>keycloak-values.yaml</p> <pre><code>keycloak:\n  password: notsosecret\n  ingress:\n    enabled: true\n    path: /\n    annotations: \n    kubernetes.io/ingress.class: nginx\n    kubernetes.io/tls-acme: \"true\"\n    ingress.kubernetes.io/affinity: cookie\n    hosts:\n    - keycloak.my.domain.com\n    tls:\n    - hosts:\n        - keycloak.my.domain.com\n    secretName: tls-keycloak\n</code></pre>"},{"location":"blogs/kubernetes-sso-keycloak/#ldap-as-user-federation","title":"LDAP As User Federation","text":"<p>Assuming you have Keycloak running now, login with the admin user, <code>keycloak</code>, and the password you set in the <code>keycloak-values.yaml</code>.</p> <p>In the left hand menu, you can select <code>User Federation</code>, this is where we can add <code>ldap</code> and <code>kerberos</code> providers. As Keycloak supports multiple sources, these will be listed by their priority (the smaller the number, the higher). The federations are consulted in the order according to their priority.</p>"},{"location":"blogs/kubernetes-sso-keycloak/#create-ldap","title":"Create LDAP","text":"<p>Let's create the new LDAP provider, select the <code>Add provider</code> dropdown - top right - and choose <code>ldap</code>.</p> <p>You will now see a whole list of values to fill in, don't worry, many can be kept as default. And to be sure, I will list them all here. For more information, each field has a little question mark, hover over it.</p> <p>Info</p> <p>When you've filled in the <code>Connection URL</code>, you should test the configuration with the <code>Test connection</code> button. The same is true for when you've configured the fields <code>Users DN</code>, <code>Bind Type</code>, <code>Bind DN</code>, <code>Bind Credential</code> with the button <code>Test authentication</code>.</p> <ul> <li>Console Display Name: display name, should be a name that tells you what this provider is</li> <li>Priority: the priority of this provider, will be used for the order that the federation is accessed</li> <li>Import Users: wether or not to import the users, I just leave this to <code>On</code> as to cache the users</li> <li>Import Users: <code>READ_ONLY</code>, in this guide we assume the LDAP server is not under your control, so read only</li> <li>Vendor: <code>Other</code></li> <li>Username LDAP attribute: <code>uid</code></li> <li>RDN LDAP attribute: <code>uid</code></li> <li>UUID LDAP attribute: <code>entryUUID</code></li> <li>User Object Classes: <code>inetOrgPerson, organizationalPerson</code></li> <li>Connection URL: <code>ldap://opendj4:389</code></li> <li>Users DN: <code>dc=example,dc=com</code></li> <li>Bind Type: <code>simple</code></li> <li>Bind DN: <code>uid=idm, ou=Administrators,dc=example,dc=com</code></li> <li>Bind Credential: <code>secret</code> -&gt; or what ever you've set it in the <code>example.diff</code></li> <li>Custom User LDAP Filter: ``</li> <li>Search Scope: <code>Subtree</code></li> <li>Validate Password Policy: <code>OFF</code></li> <li>Use Truststore SPI: <code>Only for ldap</code></li> <li>Connection Pooling: <code>ON</code></li> <li>Connection Timeout: ``</li> <li>Read Timeout: ``</li> <li>Pagination: <code>ON</code></li> </ul> <p>Sync Settings:</p> <ul> <li>Cache Policy: 1000</li> <li>Periodic Full Sync: <code>On</code></li> <li>Full Sync Period: <code>604800</code></li> <li>Periodic Changed Users Sync: <code>On</code></li> <li>Changed Users Sync Period: <code>86400</code></li> </ul> <p>Cache Settings:</p> <ul> <li>Cache Policy: <code>DEFAULT</code></li> </ul> <p></p>"},{"location":"blogs/kubernetes-sso-keycloak/#add-group-mapping","title":"Add Group Mapping","text":"<p>If the value is not mentioned, the default value should be fine.</p> <p>Some values listed here are default, but listed all the same.</p> <ul> <li>Name: <code>groups</code></li> <li>Mapper Type: <code>group-ldap-mapper</code></li> <li>LDAP Groups DN: <code>ou=Groups, dc=example,dc=com</code></li> <li>Group Name LDAP Attribute: <code>cn</code></li> <li>Group Object Classes: <code>groupOfUniqueNames</code></li> <li>Membership LDAP Attribute: <code>uniquemember</code></li> <li>User Groups Retrieve Strategy: <code>LOAD_GROUPS_BY_MEMBER_ATTRIBUTE</code></li> </ul> <p>When you hit <code>Save</code>, you can synchronize the groups to Keycloak - if you don't need to, it will confirm the configuration works. Hit the <code>Sync LDAP Groups To Keycloak</code> button, and on top there should be a temporary banner stating how many groups were synchronized (if all categories are <code>0</code>, something is wrong).</p> <p>Note</p> <p>Everytime you change a value, you first have to save the page before you can synchronize again.</p> <p></p>"},{"location":"blogs/kubernetes-sso-keycloak/#mix-ldap-users-with-other-sources","title":"Mix LDAP Users With Other Sources","text":"<p>One of the reasons for this guide to exist, is to be able to encapsulate an LDAP over which you have no control and add additional accounts and groups. There's multiple ways forward here, you can use <code>Identity Providers</code> and <code>User Federation</code> to create more sources of user accounts.</p> <p>Perhaps the simplest way is to manage these extra accounts in Keycloak itself. It has its own User database and Groups database. In addition to that, it allows you to assign users created in Keycloak to be a member of a group derived from LDAP - if you've synched them.</p>"},{"location":"blogs/kubernetes-sso-keycloak/#ui","title":"UI","text":"<p>We can use the UI in Keycloak to manage Users and Groups.</p> <p>We initiate this by going to the <code>Users</code> view and hit <code>Add user</code>.</p> <p></p> <p>We can then fill in all the details of the User.</p> <p></p>"},{"location":"blogs/kubernetes-sso-keycloak/#rest-api","title":"REST API","text":"<p>Keycloak has a rich REST API with good decent documentation<sup>21</sup>. The thing missing is some examples for how to use them correctly.</p> <p>I recommend using HTTPie<sup>28</sup> rather than cUrl, as it is easier to use for these more complex calls.</p>"},{"location":"blogs/kubernetes-sso-keycloak/#get-token","title":"Get Token","text":"<p>Warning</p> <p>The Bearer Token is only valid for a short period of time. If you wait too long, you will get <code>401 unauthorized</code>.</p> <p>The default REALM is <code>master</code>, and <code>URL</code> is the base URL of Keycloak.</p> httpiecurl <pre><code>#include &lt;stdio.h&gt;\n\nhttp --form POST \\\n \"${URL}/auth/realms/${REALM}/protocol/openid-connect/token\" \\\n  username=\"keycloak\"  \\\n  password=\"${PASS}\"   \\\n  client_id=\"admin-cli\"\\\n  grant_type=\"password\"\n</code></pre> <pre><code>curl ${URL}/auth/realms/${REALM}/protocol/openid-connect/token -u keycloak:${PASS}\n</code></pre>"},{"location":"blogs/kubernetes-sso-keycloak/#get-users","title":"Get Users","text":"httpiehttpie - get usercurl <pre><code>http \"${KEYCLOAK_URL}/auth/admin/realms/${REALM}/users\" \\\n  \"Authorization: Bearer $TOKEN\"\n</code></pre> <pre><code>http \"${KEYCLOAK_URL}/auth/admin/realms/${REALM}/users/${userId}\" \\\n  \"Authorization: Bearer $TOKEN\"\n</code></pre> <pre><code>curl -v ${KEYCLOAK_URL}/auth/admin/realms/${REALM}/users \\\n  -H \"Authorization: Bearer $TOKEN\" | jq\n</code></pre>"},{"location":"blogs/kubernetes-sso-keycloak/#create-user","title":"Create User","text":"httpiecurl <pre><code>http POST \"${KEYCLOAK_URL}/auth/admin/realms/${REALM}/users\" \\\n    \"Authorization: Bearer $TOKEN\" \\\n    credentials:=\"[{\\\"value\\\" : \\\"mypass\\\", \\\"type\\\": \\\"password\\\" }]\" \\\n    email=\"user@example.com\" \\\n    firstName=\"hannibal\" \\\n    lastName=\"lecter\" \\\n    username=\"hlecter\" \\\n    groups:='[\"Robots\"]' \\\n    emailVerified:=true \\\n    enabled:=true\n</code></pre> <pre><code>curl -v ${KEYCLOAK_URL}/auth/admin/realms/${REALM}/users \\\n  -H \"Authorization: Bearer $TOKEN\" | jq\n</code></pre>"},{"location":"blogs/kubernetes-sso-keycloak/#verify","title":"Verify","text":"<p>To verify everything is working as it should, you can go to the <code>Groups</code> and <code>Users</code> pages within the <code>Manage</code> menu (left hand side).</p> <p>With Groups you have <code>View all groups</code>, which should have the groups from LDAP and any group you have created within Keycloak - if not yet, you can do so here as well.</p> <p>To view details of a Group, double click the name - it's not obvious you can do so - and you will go to the details page. Here you can also see the members of the Group.</p> <p>You can do the same with the Users menu item. By default the page is empty, if your list of users is not too large, click on <code>View all users</code>. You can view the details page of a user by clicking the link of the User ID. Within the Groups tab you can add the user to more groups, this should contain all the groups known to Keycloak - both Keycloak internal and from LDAP.</p>"},{"location":"blogs/kubernetes-sso-keycloak/#sso-with-jenkins","title":"SSO with Jenkins","text":"<p>To configure Jenkins to use Keycloak we have two plugins at our disposal, OpenId Connect(<code>oic-auth</code>)<sup>22</sup> and Keycloak<sup>23</sup>. While the Keycloak plugin is easier to configure for authentication, I found it difficult to configure groups. As I feel the group management is mandatory we're going with the OpenId Connect plugin.</p> <p>You can install plugins in Jenkins via the UI<sup>24</sup>, via the new Jenkins CLI<sup>25</sup>, or via the <code>values.yaml</code> when installing via the Helm Chart<sup>27</sup>.</p> <p>Important</p> <p>Install the OpenId Connect plugin before configuring the next parts, and restart your Jenkins instance for best results.</p> <p>In order to configure OpenId Connect with Jenkins, it is the easiest to use the <code>well-known</code> endpoint url. This endpoint contains all the configuration information the plugin needs to configure itself.</p> <p>Usually, this is <code>${KEYCLOAK_URL}/auth/realms/${REALM}/.well-known/openid-configuration</code>.</p> <p>From Keycloak's perspective we have to register Jenkins as a Client.</p>"},{"location":"blogs/kubernetes-sso-keycloak/#keycloak-client","title":"Keycloak Client","text":"<p>We go to the <code>Clients</code> screen in Keycloak and hit the <code>Create</code> button.</p> <p>In the next screen, we need to supply three values:</p> <ul> <li>Client ID: the name of your client, <code>jenkins</code> would be good example</li> <li>Client Protocol: <code>openid-connect</code> is recommended</li> <li>Root URL: the main url of your installation, for example <code>https://jenkins.my.domain.com</code></li> </ul>"},{"location":"blogs/kubernetes-sso-keycloak/#main-settings","title":"Main Settings","text":"<p>Once we hit save, we get a details view.</p> <p>We have to change some values here.</p> <ul> <li>Access Type: we need a Client ID and Client Secret, we only get this when we select <code>confidential</code></li> <li>Valid Redirect URIs: confirm this is <code>${yourJenkinsURL}/*</code></li> </ul> <p></p> <p>Once you hit save, you get a new Tab in the Details screen of this Client. It is called <code>Credentials</code>, and here you can see the Client <code>Secret</code> which we will need to enter in Jenkins.</p> <p></p>"},{"location":"blogs/kubernetes-sso-keycloak/#group-mappings","title":"Group Mappings","text":"<p>Just as we had to add the Group mapping to the LDAP configuration, we will need to configure the Group mapping in the Client.</p> <p>If we go into the Client details, we see there's a Tab called <code>Mappers</code>. We create a new Mapping here by hitting the <code>Create</code> button.</p> <p>Give the mapping a name and then select the Mapper Type <code>Group Membership</code>.</p> <p>The Token Claim Name is important as well, we use this in our Jenkins configuration, give it a descriptive name such as <code>group-membership</code>.</p> <p></p>"},{"location":"blogs/kubernetes-sso-keycloak/#configure-via-ui","title":"Configure Via UI","text":"<p>In Jenkins we go to <code>Manage Jenkins</code> -&gt; <code>Configure Global Security</code> and here we select <code>Login with Openid Connect</code> in the <code>Security Realm</code> block.</p> <ul> <li>Client id: the client we've configured in Keycloak, if you've followed this guide, it should be <code>jenkins</code></li> <li>Client secret: the secret of the client configured in Keycloak, if you've lost it, go back to <code>Keycloak</code> -&gt; <code>Clients</code> -&gt; <code>jenkins</code> -&gt; <code>Credentials</code> and copy the value in the field <code>Secret</code>.</li> </ul> <p>We now get a <code>Configuration mode</code> block where we can select either <code>Automatic configuration</code> on <code>Manual configuration</code>. We select <code>Automatic</code> and enter our <code>Well-known</code> configuration endpoint URL from Keycloak we've written down earlier. If you don't remember, the format is usually this: <code>${KEYCLOAK_URL}/auth/realms/${REALM}/.well-known/openid-configuration</code>.</p> <ul> <li>User name field name: <code>preferred_username</code></li> <li>Full name field name: <code>name</code></li> <li>Email field name: <code>email</code></li> <li>Groups field name: <code>group-membership</code></li> </ul> <p>Note</p> <p>The field Groups field name refers back to the Token Claim Name we configured in Keycloak within our Client's Mapping for Group Membership.</p> <p></p>"},{"location":"blogs/kubernetes-sso-keycloak/#configure-via-configuration-as-code","title":"Configure Via Configuration-as-Code","text":"<p>We can use the amazing Jenkins Configuration-as-Code<sup>26</sup> to make sure our SSO configuration is configured out-of-the-box!</p> <p>In order to avoid the having to store the Client ID and Client Secret, we're going to create these as Kubernetes Secrets first.</p> <p>There's more ways to create these, but this is to keep it simple.</p> create generic kubernetes secret<pre><code>kubectl create secret generic oic-auth \\\n  --from-literal=clientID=\"${CLIENT_ID}\" \\\n  --from-literal=clientSecret=\"${CLIENT_SECRET}\" \\\n  --from-literal=keycloakUrl=${keycloakUrl} \\\n  --namespace jenkins\n</code></pre> <p>JCASC-config.yaml</p> <pre><code>  jenkins:\n    securityRealm:\n    oic:\n      clientId: \"${clientID}\"\n      clientSecret: \"${clientSecret}\"\n      wellKnownOpenIDConfigurationUrl: \"${keycloakUrl}/auth/realms/master/.well-known/openid-configuration\"\n      tokenServerUrl: \"${keycloakUrl}auth/realms/master/protocol/openid-connect/token\"\n      authorizationServerUrl: \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/auth\"\n      userInfoServerUrl: \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/userinfo\"\n      userNameField: \"preferred_username\"\n      fullNameFieldName: \"name\"\n      emailFieldName: \"email\"\n      groupsFieldName: \"group-membership\"\n      scopes: \"web-origins address phone openid offline_access profile roles microprofile-jwt email\"\n      disableSslVerification: false\n      logoutFromOpenidProvider: true\n      endSessionUrl: \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/logout\"\n      postLogoutRedirectUrl: \"\"\n      escapeHatchEnabled: false\n      escapeHatchSecret: \"\"\n</code></pre> complete-jenkins-helm-values.yaml <pre><code>master:\n  ingress:\n    enabled: true\n    hostName: jenkins.my.doamain.com\n  csrf:\n    defaultCrumbIssuer:\n      enabled: true\n      proxyCompatability: true\n  cli: false\n  installPlugins:\n    - kubernetes:latest\n    - kubernetes-credentials:latest\n    - workflow-aggregator:latest\n    - workflow-job:latest\n    - credentials-binding:latest\n    - git:latest\n    - blueocean:latest\n    - prometheus:latest\n    - matrix-auth:latest\n    - keycloak:latest\n    - oic-auth:latest\n  JCasC:\n    enabled: true\n    pluginVersion: \"1.30\"\n    configScripts:\n      welcome-message: |\n        jenkins:\n          systemMessage: Welcome, this Jenkins is configured and managed as code.\n      ldap-settings: |\n        jenkins:\n          securityRealm:\n            oic:\n              clientId: \"${clientID}\"\n              clientSecret: \"${clientSecret}\"\n              wellKnownOpenIDConfigurationUrl: \"${keycloakUrl}/auth/realms/master/.well-known/openid-configuration\"\n              tokenServerUrl: \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/token\"\n              authorizationServerUrl: \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/auth\"\n              userInfoServerUrl: \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/userinfo\"\n              userNameField: \"preferred_username\"\n              fullNameFieldName: \"name\"\n              emailFieldName: \"email\"\n              groupsFieldName: \"group-membership\"\n              scopes: \"web-origins address phone openid offline_access profile roles microprofile-jwt email\"\n              disableSslVerification: false\n              logoutFromOpenidProvider: true\n              endSessionUrl: \"${keycloakUrl}/auth/realms/master/protocol/openid-connect/logout\"\n              postLogoutRedirectUrl: \"\"\n              escapeHatchEnabled: false\n              escapeHatchSecret: \"\"\n      matrix-auth: |\n        jenkins:\n          authorizationStrategy:\n            globalMatrix:\n              permissions:\n                - \"Overall/Read:authenticated\"\n                - \"Overall/Administer:barbossa\"\n                - \"Overall/Administer:Catmins\"\n                - \"Overall/Administer:Robots\"\n                - \"Overall/Administer:/Catmins\"\n                - \"Overall/Administer:/Robots\"\npersistence:\n  enabled: true\n  volumes:\n  - name: oic-auth-clientid\n    secret:\n      secretName: oic-auth\n      items:\n        - key: clientID\n          path: clientID\n  - name: oic-auth-clientsecret\n    secret:\n      secretName: oic-auth\n      items:\n        - key: clientSecret\n          path: clientSecret\n  - name: oic-auth-keycloakurl\n    secret:\n      secretName: oic-auth\n      items:\n        - key: keycloakUrl\n          path: keycloakUrl\n  mounts:\n    - name: oic-auth-clientid\n      mountPath: /run/secrets/clientID\n      subPath: clientID\n    - name: oic-auth-clientsecret\n      mountPath: /run/secrets/clientSecret\n      subPath: clientSecret\n    - name: oic-auth-keycloakurl\n      mountPath: /run/secrets/keycloakUrl\n      subPath: keycloakUrl\n</code></pre>"},{"location":"blogs/kubernetes-sso-keycloak/#use-keycloak-groups-in-jenkins","title":"Use Keycloak Groups In Jenkins","text":"<p>Warning</p> <p>Unfortuantely, there is one caveat about using Keycloak as intermediary between LDAP and Jenkins.</p> <p>Groups do not come across the same as before, they're now prefixed with <code>/</code>. I'm sure it is down to a misconfiguration on my end, so please let me know how to resolve that if you figure it out. </p> <p>This means, that a LDAP group called Catmins will have to be used in Jenkins via <code>/Catmins</code> in <code>Matrix</code>, <code>Project Matrix</code> or other authorization schemes.</p>"},{"location":"blogs/kubernetes-sso-keycloak/#verify_1","title":"Verify","text":"<p>To verify, take the leap of faith to save the configuration, logout (top right) and then log back in. If everything goes well, you will be redirected to Keycloak, once successfully logged in, redirected back to Jenkins!</p> <p>Note</p> <p>One thing to remark, that if you're logged into Jenkins and you want to see the Groups, you can select your User (top right, click on your name). </p> <p>Alternatively, you can go to <code>${JENKINS_URL}/whoAmI</code> (notice the capital casing of the 'A' and 'I', it is required).</p>"},{"location":"blogs/kubernetes-sso-keycloak/#references","title":"References","text":"<ol> <li> <p>Apache Keycloak Home \u21a9</p> </li> <li> <p>Wikipedia Definition on Single sign-on \u21a9</p> </li> <li> <p>Kubernetes Home \u21a9</p> </li> <li> <p>Lightweight Directory Access Protocol (LDAP) \u21a9</p> </li> <li> <p>Jenkins Home \u21a9</p> </li> <li> <p>SonarQube Home \u21a9</p> </li> <li> <p>OpenDJ Community Edition \u21a9</p> </li> <li> <p>Helm - Package Manager for Kubernetes \u21a9\u21a9</p> </li> <li> <p>Helm 3 Beta \u21a9</p> </li> <li> <p>Common Used LDAP Attributes Explained \u21a9</p> </li> <li> <p>tini - process manager \u21a9</p> </li> <li> <p>Overops Article on Java License in Docker images \u21a9</p> </li> <li> <p>Alpine Docker Image \u21a9\u21a9</p> </li> <li> <p>Snyk - 10 docker image security best practices \u21a9</p> </li> <li> <p>Azul OpenJDK Alpine Image \u21a9</p> </li> <li> <p>Hosting Helm Private Repository from GitHub \u21a9</p> </li> <li> <p>Joost van der Griendt's Helm Repository \u21a9</p> </li> <li> <p>Nip Io - Dead simple wildcard DNS for any IP Address \u21a9</p> </li> <li> <p>Cert Manager \u21a9</p> </li> <li> <p>Let's Encrypt - Let's Encrypt the world! \u21a9</p> </li> <li> <p>Keycloak Admin REST API \u21a9</p> </li> <li> <p>Jenkins OpenId Connect Plugin \u21a9</p> </li> <li> <p>Jenkins Keycloak Plugin \u21a9</p> </li> <li> <p>Jenkins Plugin Management \u21a9</p> </li> <li> <p>Jenkins JCLI \u21a9</p> </li> <li> <p>Jenkins Configuration As Code \u21a9</p> </li> <li> <p>Jenkins Helm Chart \u21a9</p> </li> <li> <p>HTTPie http commandline interface \u21a9</p> </li> </ol>"},{"location":"blogs/monitor-jenkins-on-k8s/","title":"Monitor Jenkins on Kubernetes","text":""},{"location":"blogs/monitor-jenkins-on-k8s/#additional-metrics","title":"Additional Metrics","text":"<ul> <li>metrics-diskusage</li> <li>disk-usage</li> </ul>"},{"location":"blogs/monitor-jenkins-on-k8s/#next-steps","title":"Next Steps","text":""},{"location":"blogs/monitor-jenkins-on-k8s/#replace-node-builds","title":"Replace Node Builds","text":"<ul> <li>make them a single metric, and calculate builds per label</li> </ul>"},{"location":"blogs/sso-azure-ad/","title":"Azure AD &amp; CloudBees Core","text":"<p>In this article we're going to configure Azure AD as Single-Sign-Solution for CloudBees Core. The configuration rests on three points; 1) Azure AD, 2) Jenkins' SAML plugin, and 3) CloudBees Core's Role Base Access Control or RBAC for short.</p> <p>Important</p> <p>Currently there's a discrepency between the TimeToLive (TTL) of the RefreshToken in Azure AD and the default configuration in the SAML plugin.</p> <p>This creates the problem that you can be logged in, in Azure, but when going to Jenkins you get the Logged Out page. The only way to log back in, is to logout in Azure and back in.</p> <p>The reason is as follows:</p> <p>The max lifetime of the Access Token in Azure AD seems to be 24 hours where the refresh token can live for a maximum of 14 days (if the access token expires the refresh token is used to try to obtain a new access token). The Jenkins setting in Configure Global Security &gt; SAML Identity Provider Settings &gt; Maximum Authentication Lifetime is 24 hours (86400 in seconds) upping this to 1209600 (which is 14 days in seconds/the max lifetime of the Refresh Token).</p> <p>The recommended resolution is to set <code>Maximum Authentication Lifetime</code> to the same value of your RefreshToken TTL. If you haven't changed this in Azure AD, it is <code>1209600</code>.</p> <p><code>Manage Jenkins</code> -&gt; <code>Configure Global Security</code> &gt; <code>SAML Identity Provider Settings</code> &gt; <code>Maximum Authentication Lifetime</code> = <code>1209600</code></p>"},{"location":"blogs/sso-azure-ad/#prerequisites","title":"Prerequisites","text":"<p>Before we start, there are some requirements.</p> <ul> <li>a running CloudBees Core Operations Center instance</li> <li>this instance is accessible via <code>https.</code><ul> <li>if you do not have a valid certificate and you're running on Kubernetes take a look at my cert-manager guide</li> <li>Let's Encrypt can now also work with nip.io addresses</li> </ul> </li> <li>active Azure subscription</li> <li>have an Azure subscription Administrator on hand</li> </ul>"},{"location":"blogs/sso-azure-ad/#configure-azure","title":"Configure Azure","text":"<p>Warning</p> <p>We use <code>https://cloudbees-core.example.com</code> as the example URL for CloudBees Core. Please replace the domain to your actual domain in all the examples!</p>"},{"location":"blogs/sso-azure-ad/#steps-to-execute","title":"Steps to execute","text":"<p>We do the following steps on the Azure side.</p> <ul> <li>create the Azure Active Directory</li> <li>create users and groups</li> <li>create App Registration<ul> <li>URL: <code>https://cloudbees-core.example.com/cjoc/securityRealm/finishLogin</code></li> <li>replace <code>example.com</code> with your domain, https is required! </li> </ul> </li> <li>update manifest (for groups) <ul> <li>change: <code>\"groupMembershipClaims\": null,</code> (usually line 11)</li> <li>to: <code>\"groupMembershipClaims\": \"SecurityGroup\",</code></li> </ul> </li> <li>create SP ID / App ID URI</li> <li>grant admin consent</li> </ul> <p>Info</p> <p>If you use the Azure AD plugin, you also create a client secret.</p>"},{"location":"blogs/sso-azure-ad/#information-to-note-down","title":"Information To Note Down","text":"<p>The following information is unique to your installation, so you need to record them as you go along.</p> <ul> <li><code>App ID URI</code></li> <li><code>Object ID</code>'s of Users and Groups you want to give rights</li> <li><code>Federation Metadata Document</code> Endpoint<ul> <li>Azure AD -&gt; App Registrations -&gt;  -&gt; Endpoints (circular icon on top) <li>you can use either the URL or the document contents</li> <li>make sure the URL contains the Tenant ID of your Azure Active Directory</li> <li>URL example: <code>https://login.microsoftonline.com/${TENANT_ID}/federationmetadata/2007-06/federationmetadata.xml</code></li> <li>You can find your Tenant ID in <code>Azure Active Directory</code> -&gt; <code>Properties</code> -&gt; <code>Directory ID</code> (different name, same ID)</li>"},{"location":"blogs/sso-azure-ad/#visual-guide","title":"Visual Guide","text":"<p>Below is a visual guide with screenshots.</p> <p>Pay attention to these hints in the screenshots.</p> <ul> <li>red: this is the main thing</li> <li>orange: this is how we got to the current page/view</li> <li>blue: while you're in this screen, there might be other things you could do</li> </ul>"},{"location":"blogs/sso-azure-ad/#create-new-app-registration","title":"Create New App Registration","text":"<p>If you have an Azure account, you have an Azure Active Directory (Azure AD) pre-created.</p> <p>This guide assumes you have an Azure AD ready to use.</p> <p>That means the next step is to create an Application Registration.</p> <p></p> <p>Give the registration a useful name, select who can authenticate and the <code>redirect URL</code>.</p> <p>This URL needs to be configured and MUST be the actual URL of your CloudBees Core installation.</p> <p></p> <p>Important</p> <p>To have Client Masters as a fallback for login, incase Operations Center is down, you have to add another redirect URL for each Master.</p> <p>Azure AD -&gt; App Registrations -&gt;  -&gt; Authentication -&gt; Web -&gt; <code>https://example.com/teams-cat/securityRealm/finishLogin</code>"},{"location":"blogs/sso-azure-ad/#app-registration-data-to-write-down","title":"App Registration Data To Write Down","text":"<p>Depending on the plugin you're going to use (SAML or Azure AD) you need information from your Azure AD / App Registration.</p> <ul> <li>Tentant ID</li> <li>Object ID</li> <li>Client ID</li> <li>Federation Metadata Document <ul> <li>you can use the document XML content or the URL</li> </ul> </li> </ul> <p></p> <p>Click on the <code>Endpoints</code> button to open the side-bar with the links.</p> <p></p>"},{"location":"blogs/sso-azure-ad/#app-id","title":"App ID","text":"<p>We need the <code>App ID</code> - even if the SAML plugin doesn't mention it.</p> <p>Azure generates an <code>APP ID</code> URI for you. You can also use CloudBees Core's URL as the URI. The URI is shown when there is an error, so it recommended to use a value you can recognize.</p> <p>Info</p> <p><code>App ID</code> must match in both Azure AD (set as <code>App ID URI</code>) and the SAML plugin (set as <code>Entity ID</code>) configuration in Jenkins. So write it down.</p> <p></p> <p></p>"},{"location":"blogs/sso-azure-ad/#api-permissions","title":"API Permissions","text":"<p>Of course, things wouldn't be proper IAM/LDAP/AD without giving some permissions.</p> <p>If we want to retrieve group information and other fields, we need to be able to read the Directory information.</p> <p></p> <p>You find the Directory information via the <code>Microsoft Graph</code> api button.</p> <p></p> <p>We select <code>Application Permissions</code> and then check <code>Directory.Read.All</code>. We don't need to write.</p> <p></p> <p>The Permissions have changed, so we require an Administrator account to consent with the new permissions.</p> <p></p>"},{"location":"blogs/sso-azure-ad/#update-manifest","title":"Update Manifest","text":"<p>As with the permissions, the default <code>Manifest</code> doesn't give us all the information we want.</p> <p>We want the groups so we can configure RBAC, and thus we have to set the <code>groupMembershipsClaims</code> claim attribute.</p> <p></p> <p>We change the <code>null</code> to <code>\"SecurityGroup\"</code>. Please consult the Microsoft docs (see reference below) for other options.</p> <p>We can download, edit, and upload the manifest file. Alternatively, we can edit inline and hit save on top.</p> <p></p>"},{"location":"blogs/sso-azure-ad/#retrieve-group-object-id","title":"Retrieve Group Object ID","text":"<p>If we want to assign Azure AD groups to groups or roles in Jenkins' RBAC, we need to use the <code>Object ID</code>'s.</p> <p>Each Group and User has an <code>Object ID</code>, which have a handy <code>Copy this</code> button on the end of the value box!</p> <p></p>"},{"location":"blogs/sso-azure-ad/#configure-jenkins","title":"Configure Jenkins","text":"<p>We now get to the point where we configure Jenkins. The SAML plugin is opensource, and thus it's configuration can also be used for Jenkins or CloudBees Jenkins Distribution.</p>"},{"location":"blogs/sso-azure-ad/#steps","title":"Steps","text":"<p>Here are the steps we perform to configure Azure AD as the Single-Sign-On solution. </p> <ul> <li>install the SAML plugin<ul> <li>I assume you know how to install plugins, so we skip this</li> <li>if you don't know Read the Managing Plugins Guide</li> </ul> </li> <li>configure saml 2.0 in Jenkins</li> <li>setup groups (RBAC)<ul> <li>administrators -&gt; admin group</li> <li>browsers -&gt; all other groups</li> </ul> </li> </ul>"},{"location":"blogs/sso-azure-ad/#visual-guide_1","title":"Visual Guide","text":"<p>Below is a visual guide with screenshots. Please pay attention to the hints in the screenshots.</p> <ul> <li>Red: this is the main thing</li> <li>Orange: this is how we got to the current page/view</li> <li>Blue: while you're in this screen, there might be other things you could do</li> </ul>"},{"location":"blogs/sso-azure-ad/#configure-security","title":"Configure Security","text":"<p>To go to Jenkins' security configuration, follow this route:</p> <ul> <li>login with an Admin user</li> <li>go to the <code>Operations Center</code></li> <li><code>Manage Jenkins</code> -&gt; <code>Global Security Configuration</code></li> </ul>"},{"location":"blogs/sso-azure-ad/#configure-rbac","title":"Configure RBAC","text":"<p>The SAML plugin configuration pollutes the screen with fields.</p> <p>My advice is to enable RBAC first.</p> <p>If you haven't got any groups/roles yet, I recommend using the <code>Typical Initial Setup</code> from the dropdown. The logged-in user is automatically registered as administrator. So if your Azure AD configuration doesn't work, this user can still manage Jenkins.</p> <p></p> <p>Important</p> <p>Make sure you know the credentials of the current admin user.</p> <p>It will automatically be added to the <code>Administrators</code> group, and it will be your go-to account when you mess up the SAML configuration and you have to reset security. </p> <p>For how to reset the security configuration, see the <code>For When You Mess Up</code> paragraph.</p>"},{"location":"blogs/sso-azure-ad/#configure-saml","title":"Configure SAML","text":"<p>Select <code>SAML 2.0</code> from the <code>Security Realm</code> options.</p> <p>Here we first supply our <code>Federation Metadata Document</code> content or it's URL.</p> <p>Each option - document content or URL - has its own <code>Validate ...</code> button, hit it and confirm it says <code>Success</code>.</p> <p></p> <p></p> <p>Info</p> <p>You can leave <code>Displayname</code> empty, which gives you the default naming scheme from Azure AD. I think this is ugly, as it amounts to something like <code>${EMAIL_ADDRESS}_${AD_DOMAIN}_${AZURE_CORP_DOMAIN}</code>. There are other options, I've settled for <code>givenname</code>, as there isn't a <code>fullname</code> by default, and well, I prefer <code>Joost</code> to a long hard to recognize string.</p>"},{"location":"blogs/sso-azure-ad/#fields","title":"Fields","text":"<ul> <li>Displayname: <code>http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname</code></li> <li>Group: <code>http://schemas.microsoft.com/ws/2008/06/identity/claims/groups</code></li> <li>Username: <code>http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name</code></li> <li>Email: <code>http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress</code></li> <li>SP Entity ID:  the <code>App ID</code> URI you configured in Azure AD (hidden behind <code>Advanced Configuration</code>)</li> </ul>"},{"location":"blogs/sso-azure-ad/#configure-rbac-groups","title":"Configure RBAC Groups","text":"<p>Tip</p> <p>Once Azure AD is configured, and it works, you can configure groups for RBAC just as you're used to.</p> <p>Both for classic RBAC and Team Masters.</p> <p>Just make sure you use the Azure AD <code>Object ID</code>'s of the groups to map them.</p> <p>Bonus tip, add every Azure AD group to <code>Browsers</code>, so you can directly map their groups to Team Master roles without problems. </p> <p></p> <p></p>"},{"location":"blogs/sso-azure-ad/#xml-config","title":"XML Config","text":"<pre><code>  &lt;useSecurity&gt;true&lt;/useSecurity&gt;\n  &lt;authorizationStrategy class=\"nectar.plugins.rbac.strategy.RoleMatrixAuthorizationStrategyImpl\"/&gt;\n &lt;securityRealm class=\"org.jenkinsci.plugins.saml.SamlSecurityRealm\" plugin=\"saml@1.1.2\"&gt;\n    &lt;displayNameAttributeName&gt;http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname&lt;/displayNameAttributeName&gt;\n    &lt;groupsAttributeName&gt;http://schemas.microsoft.com/ws/2008/06/identity/claims/groups&lt;/groupsAttributeName&gt;\n    &lt;maximumAuthenticationLifetime&gt;86400&lt;/maximumAuthenticationLifetime&gt;\n    &lt;emailAttributeName&gt;http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress&lt;/emailAttributeName&gt;\n    &lt;usernameCaseConversion&gt;none&lt;/usernameCaseConversion&gt;\n    &lt;usernameAttributeName&gt;http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name&lt;/usernameAttributeName&gt;\n    &lt;binding&gt;urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect&lt;/binding&gt;\n    &lt;advancedConfiguration&gt;\n      &lt;forceAuthn&gt;false&lt;/forceAuthn&gt;\n      &lt;spEntityId&gt;https://cloudbees-core.kearos.net&lt;/spEntityId&gt;\n    &lt;/advancedConfiguration&gt;\n    &lt;idpMetadataConfiguration&gt;\n      &lt;xml&gt;&lt;/xml&gt;\n      &lt;url&gt;https://login.microsoftonline.com/95b46e09-0307-488b-a6fc-1d2717ba9c49/federationmetadata/2007-06/federationmetadata.xml&lt;/url&gt;\n      &lt;period&gt;5&lt;/period&gt;\n    &lt;/idpMetadataConfiguration&gt;\n  &lt;/securityRealm&gt;\n  &lt;disableRememberMe&gt;false&lt;/disableRememberMe&gt;\n</code></pre>"},{"location":"blogs/sso-azure-ad/#logout-url","title":"Logout URL","text":"<p>Depending on the requirements, you may want to specify a logout url in the SAML configuration to log you completely out of SAML, not just Core.</p> <p>An example <code>https://login.windows.net/&lt;tenant_id_of_your_app&gt;/oauth2/logout?post_logout_redirect_uri=&lt;logout_URL_of_your_app&gt;/logout</code></p>"},{"location":"blogs/sso-azure-ad/#for-when-you-mess-up","title":"For When You Mess Up","text":"<p>This is the default config for security in CloudBees Core.</p> <p>This file is in <code>${JENKINS_HOME}/config.xml</code>, the XML tags we want to look at are quite near the top.</p> <pre><code> &lt;useSecurity&gt;true&lt;/useSecurity&gt;\n  &lt;authorizationStrategy class=\"hudson.security.FullControlOnceLoggedInAuthorizationStrategy\"&gt;\n    &lt;denyAnonymousReadAccess&gt;true&lt;/denyAnonymousReadAccess&gt;\n  &lt;/authorizationStrategy&gt;\n  &lt;securityRealm class=\"hudson.security.HudsonPrivateSecurityRealm\"&gt;\n    &lt;disableSignup&gt;true&lt;/disableSignup&gt;\n    &lt;enableCaptcha&gt;false&lt;/enableCaptcha&gt;\n  &lt;/securityRealm&gt;\n  &lt;disableRememberMe&gt;false&lt;/disableRememberMe&gt;\n</code></pre>"},{"location":"blogs/sso-azure-ad/#on-cloudbees-core-modern-kubernetes","title":"On CloudBees Core Modern / Kubernetes","text":"<p>To rectify a failed configuration, execute the following steps:</p> <ol> <li>exec into the <code>cjoc-0</code> container: <code>kubectl exec -ti cjoc-0 -- bash</code></li> <li>open <code>config.xml</code>: <code>vi /var/jenkins_home/config.xml</code></li> <li>replace conflicting lines with the above snippet</li> <li>save the changes</li> <li>exit the container: <code>exit</code></li> <li>kill the pod: <code>kubectl delete po cjoc-0</code></li> </ol> <p>Tip</p> <p>For removing a whole line, stay in \"normal\" mode, and press <code>d d</code> (two times the <code>d</code> key). To add the new lines, go into insert mode by pressing the <code>i</code> key. Go back to \"normal\" mode by pressing the <code>esc</code> key. Then, save and quit, by writing: <code>:wq</code> followed by <code>enter</code>.</p>"},{"location":"blogs/sso-azure-ad/#references","title":"References","text":"<ul> <li>CloudBees Guide on Azure AD for Core SSO(outdated)</li> <li>SAML Plugin Docs for Azure AD (outdated)</li> <li>Microsoft Doc for Azure AD Tokens</li> <li>Microsoft Doc for Azure AD Optional Tokens</li> <li>Microsoft Doc for Azure AD Custom Tokens</li> <li>Alternative Azure AD Plugin (very new)</li> </ul> <p>Info</p> <p>Currently, there is a limitation which requires you to use the <code>Object ID</code>'s which make searching groups and people less than ideal.  When the Alternative Azure AD Plugin is approved this may provide a more satisfactory solution.</p>"},{"location":"blogs/teams-automation/","title":"Core Modern Teams Automation","text":"<p>CloudBees Core on Modern (meaning, on Kubernetes) has two main types of Jenkins Masters, a Managed Master, and a Team Master. In this article, we're going to automate the creation and management of the Team Masters.</p> <p>Hint</p> <p>If you do not want to read any of the code here, or just want to take a look at the end result - pipeline wise - you can find working examples on GitHub.</p> <ul> <li>Template Repository - creates a new team template and a PR to the GitOps repository</li> <li>GitOps Repository - applies GitOps principles to manage the CloudBees Core Team Masters</li> </ul>"},{"location":"blogs/teams-automation/#goals","title":"Goals","text":"<p>Just automating the creation of a Team Master is relatively easy, as this can be done via the Client Jar. So we're going to set some additional goals to create a decent challenge.</p> <ul> <li>GitOps: I want to be able to create and delete Team Masters by managing configuration in a Git repository </li> <li>Configuration-as-Code: as much of the configuration as possible should be stored in the Git repository</li> <li>Namespace: one of the major reasons for Team Masters to exist is to increase (Product) Team autonomy, which in Kubernetes should correspond to a <code>namespace</code>. So I want each Team Master to be in its own Namespace!</li> <li>Self-Service: the solution should cater to (semi-)autonomous teams and lower the workload of the team managing CloudBees Core. So requesting a Team Master should be doable by everyone</li> </ul>"},{"location":"blogs/teams-automation/#before-we-start","title":"Before We Start","text":"<p>Some assumptions need to be taken care off before we start.</p> <ul> <li>Kubernetes cluster in which you are <code>ClusterAdmin</code><ul> <li>if you don't have one yet, there are guides on this elsewhere on the site</li> </ul> </li> <li>your cluster has enough capacity (at least two nodes of 4gb memory)</li> <li>your cluster has CloudBees Core Modern installed<ul> <li>if you don't have this yet look at one of the guides on this site</li> <li>or look at the guides on CloudBees.com</li> </ul> </li> <li>have administrator access to CloudBees Core Cloud Operations Center</li> </ul> <p>Code Examples</p> <p>The more extensive Code Examples, such as Kubernetes Yaml files, are collapsed by default. You can open them by clicking on them. On the right, the code snippet will have a <code>[ ]</code> copy icon. Below is an example.</p> Code Snippet Example <p>Here's a code snippet.</p> <pre><code>pipeline {\n    agent any\n    stages {\n        stage('Hello') {\n            steps {\n                echo 'Hello World!'\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"blogs/teams-automation/#bootstrapping","title":"Bootstrapping","text":"<p>All right, so we want to use GitOps and to process the changes we need a Pipeline that can be triggered by a Webhook. I believe in everything as code - except Secrets and such - which includes the Pipeline.</p> <p>Unfortunately, Operations Center cannot run such pipelines. To get over this hurdle, we will create a special <code>Ops</code> Team Master. This Master will be configured to be able to Manage the other Team Masters for us.</p> <p>Log into your Operations Center with a user that has administrative access.</p>"},{"location":"blogs/teams-automation/#create-api-token","title":"Create API Token","text":"<p>Create a new API Token for your administrator user by clicking on the user's name - top right corner. Select the <code>Configuration</code> menu on the left and then you should see a section where you can <code>Create a API Token</code>. This Token will disappear, so write it down.</p>"},{"location":"blogs/teams-automation/#get-configure-client-jar","title":"Get &amp; Configure Client Jar","text":"<p>Replace the values marked by <code>&lt; ... &gt;</code>. The Operations Center URL should look like this: <code>http://cbcore.mydomain.com/cjoc</code>.</p> <p>Setup the connection variables.</p> <pre><code>OC_URL=&lt;your operations center url&gt;\n</code></pre> <pre><code>USR=&lt;your username&gt;\nTKN=&lt;api token&gt;\n</code></pre> <p>Download the Client Jar.</p> <pre><code>curl ${OC_URL}/jnlpJars/jenkins-cli.jar -o jenkins-cli.jar\n</code></pre>"},{"location":"blogs/teams-automation/#create-alias-test","title":"Create Alias &amp; Test","text":"<pre><code>alias cboc=\"java -jar jenkins-cli.jar -noKeyAuth -auth ${USR}:${TKN} -s ${OC_URL}\"\n</code></pre> <pre><code>cboc version\n</code></pre>"},{"location":"blogs/teams-automation/#create-team-ops","title":"Create Team Ops","text":"<p>As the tasks of the Team Masters for managing Operations are quite specific and demand special rights, I'd recommend putting this in its own <code>namespace</code>. To do so properly, we need to configure a few things.</p> <ul> <li>allows Operations Center access to this namespace (so it can create the Team Master)</li> <li>give the <code>ServiceAccount</code> the permissions to create <code>namespace</code>'s for the other Team Masters</li> <li>add config map for the Jenkins Agents</li> <li>temporarily change Operations Center's operating Namespace (where it will spawn resources in)</li> <li>use the CLI to create the <code>team-ops</code> Team Master</li> <li>reset Operations Center's operating Namespace</li> </ul>"},{"location":"blogs/teams-automation/#update-create-kubernetes-namespaces","title":"Update &amp; Create Kubernetes Namespaces","text":""},{"location":"blogs/teams-automation/#create-team-ops-namespace","title":"Create Team Ops Namespace","text":"<pre><code>kubectl apply -f team-ops-namespace.yaml\n</code></pre> team-ops-namespace.yaml <p>This creates the <code>team-ops</code> namespace including all the resources required such as <code>ResourceQuota</code>, <code>ServiceAccount</code> and so on.</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: team-ops\n\n---\n\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: resource-quota\n  namespace: team-ops\nspec:\n  hard:\n    pods: \"20\"\n    requests.cpu: \"4\"\n    requests.memory: 6Gi\n    limits.cpu: \"5\"\n    limits.memory: 10Gi\n    services.loadbalancers: \"0\"\n    services.nodeports: \"0\"\n    persistentvolumeclaims: \"10\"\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins\n  namespace: team-ops\n\n---\n\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: pods-all\n  namespace: team-ops\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/exec\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/log\"]\n  verbs: [\"get\",\"list\",\"watch\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: jenkins\n  namespace: team-ops\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: pods-all\nsubjects:\n- kind: ServiceAccount\n  name: jenkins\n  namespace: team-ops\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: cjoc\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: master-management\nsubjects:\n- kind: ServiceAccount\n  name: jenkins\n  namespace: team-ops\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  name: create-namespaces\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"serviceaccounts\", \"rolebindings\", \"roles\", \"resourcequotas\", \"namespaces\"]\n  verbs: [\"create\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"configmaps\", \"rolebindings\", \"roles\", \"resourcequotas\", \"namespaces\"]\n  verbs: [\"create\",\"get\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"events\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"persistentvolumeclaims\", \"pods\", \"pods/exec\", \"services\", \"statefulsets\", \"ingresses\", \"extensions\"]\n  verbs: [\"create\", \"delete\", \"get\", \"list\", \"patch\", \"update\", \"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/log\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"list\"]\n- apiGroups: [\"apps\"]\n  resources: [\"statefulsets\"] \n  verbs: [\"create\", \"delete\", \"get\", \"list\", \"patch\", \"update\", \"watch\"]\n- apiGroups: [\"extensions\"]\n  resources: [\"ingresses\"]\n  verbs: [\"create\", \"delete\", \"get\", \"list\", \"patch\", \"update\", \"watch\"]\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: ops-namespace\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: create-namespaces\nsubjects:\n- kind: ServiceAccount\n  name: jenkins\n  namespace: team-ops\n</code></pre>"},{"location":"blogs/teams-automation/#update-operation-center-serviceaccount","title":"Update Operation Center ServiceAccount","text":"<p>The <code>ServiceAccount</code> under which Operation Center runs, only has rights in it's own <code>namespace</code>. Which means it cannot create our Team Ops Master. Below is the <code>.yaml</code> file for Kubernetes and the command to apply it.</p> <p>Warning</p> <p>I assume you're using the default <code>cloudbees-core</code> as per Cloudbees' documentation. If this is not the case, change the last line, <code>namespace: cloudbees-core</code> with the namespace your Operation Center runs in.</p> <pre><code>kubectl apply -f patch-oc-serviceaccount.yaml -n team-ops\n</code></pre> patch-oc-serviceaccount.yaml <p>This patches the existing Operation Center's ServiceAccount to also have the correct rights in the <code>team-ops</code> namespace.</p> <pre><code>kind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: master-management\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/exec\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/log\"]\n  verbs: [\"get\",\"list\",\"watch\"]\n- apiGroups: [\"apps\"]\n  resources: [\"statefulsets\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"services\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"persistentvolumeclaims\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"extensions\"]\n  resources: [\"ingresses\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"events\"]\n  verbs: [\"get\",\"list\",\"watch\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: cjoc\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: master-management\nsubjects:\n- kind: ServiceAccount\n  name: cjoc\n  namespace: cloudbees-core\n</code></pre>"},{"location":"blogs/teams-automation/#jenkins-agent-configmap","title":"Jenkins Agent ConfigMap","text":"<pre><code>kubectl apply -f jenkins-agent-config-map.yaml -n team-ops\n</code></pre> jenkins-agent-config-map.yaml <p>Creates the Jenkins Agent ConfigMap, which contains the information the Jenkins Agent - within a PodTemplate - uses to connect to the Jenkins Master.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: jenkins-agent\ndata:\n  jenkins-agent: |\n    #!/usr/bin/env sh\n    # The MIT License\n    #\n    #  Copyright (c) 2015, CloudBees, Inc.\n    #\n    #  Permission is hereby granted, free of charge, to any person obtaining a copy\n    #  of this software and associated documentation files (the \"Software\"), to deal\n    #  in the Software without restriction, including without limitation the rights\n    #  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    #  copies of the Software, and to permit persons to whom the Software is\n    #  furnished to do so, subject to the following conditions:\n    #\n    #  The above copyright notice and this permission notice shall be included in\n    #  all copies or substantial portions of the Software.\n    #\n    #  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    #  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    #  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    #  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    #  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    #  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n    #  THE SOFTWARE.\n    # Usage jenkins-slave.sh [options] -url http://jenkins [SECRET] [AGENT_NAME]\n    # Optional environment variables :\n    # * JENKINS_TUNNEL : HOST:PORT for a tunnel to route TCP traffic to jenkins host, when jenkins can't be directly accessed over network\n    # * JENKINS_URL : alternate jenkins URL\n    # * JENKINS_SECRET : agent secret, if not set as an argument\n    # * JENKINS_AGENT_NAME : agent name, if not set as an argument\n    if [ $# -eq 1 ]; then\n        # if `docker run` only has one arguments, we assume user is running alternate command like `bash` to inspect the image\n        exec \"$@\"\n    else\n        # if -tunnel is not provided try env vars\n        case \"$@\" in\n            *\"-tunnel \"*) ;;\n            *)\n            if [ ! -z \"$JENKINS_TUNNEL\" ]; then\n                TUNNEL=\"-tunnel $JENKINS_TUNNEL\"\n            fi ;;\n        esac\n        if [ -n \"$JENKINS_URL\" ]; then\n            URL=\"-url $JENKINS_URL\"\n        fi\n        if [ -n \"$JENKINS_NAME\" ]; then\n            JENKINS_AGENT_NAME=\"$JENKINS_NAME\"\n        fi  \n        if [ -z \"$JNLP_PROTOCOL_OPTS\" ]; then\n            echo \"Warning: JnlpProtocol3 is disabled by default, use JNLP_PROTOCOL_OPTS to alter the behavior\"\n            JNLP_PROTOCOL_OPTS=\"-Dorg.jenkinsci.remoting.engine.JnlpProtocol3.disabled=true\"\n        fi\n        # If both required options are defined, do not pass the parameters\n        OPT_JENKINS_SECRET=\"\"\n        if [ -n \"$JENKINS_SECRET\" ]; then\n            case \"$@\" in\n                *\"${JENKINS_SECRET}\"*) echo \"Warning: SECRET is defined twice in command-line arguments and the environment variable\" ;;\n                *)\n                OPT_JENKINS_SECRET=\"${JENKINS_SECRET}\" ;;\n            esac\n        fi\n\n        OPT_JENKINS_AGENT_NAME=\"\"\n        if [ -n \"$JENKINS_AGENT_NAME\" ]; then\n            case \"$@\" in\n                *\"${JENKINS_AGENT_NAME}\"*) echo \"Warning: AGENT_NAME is defined twice in command-line arguments and the environment variable\" ;;\n                *)\n                OPT_JENKINS_AGENT_NAME=\"${JENKINS_AGENT_NAME}\" ;;\n            esac\n        fi\n        SLAVE_JAR=/usr/share/jenkins/slave.jar\n        if [ ! -f \"$SLAVE_JAR\" ]; then\n            tmpfile=$(mktemp)\n            if hash wget &gt; /dev/null 2&gt;&amp;1; then\n                wget -O \"$tmpfile\" \"$JENKINS_URL/jnlpJars/slave.jar\"\n            elif hash curl &gt; /dev/null 2&gt;&amp;1; then\n                curl -o \"$tmpfile\" \"$JENKINS_URL/jnlpJars/slave.jar\"\n            else\n                echo \"Image does not include $SLAVE_JAR and could not find wget or curl to download it\"\n                return 1\n            fi\n            SLAVE_JAR=$tmpfile\n        fi\n        #TODO: Handle the case when the command-line and Environment variable contain different values.\n        #It is fine it blows up for now since it should lead to an error anyway.\n        exec java $JAVA_OPTS $JNLP_PROTOCOL_OPTS -cp $SLAVE_JAR hudson.remoting.jnlp.Main -headless $TUNNEL $URL $OPT_JENKINS_SECRET $OPT_JENKINS_AGENT_NAME \"$@\"\n    fi\n</code></pre>"},{"location":"blogs/teams-automation/#create-initial-master","title":"Create Initial Master","text":"<p>To make it easier to change the <code>namespace</code> if needed, its extracted out from the command.</p> <pre><code>OriginalNamespace=cloudbees-core\n</code></pre> <p>This script changes the Operations Center's operating <code>namespace</code>, creates a Team Master with the name <code>ops</code>, and then resets the namespace.</p> <pre><code>cboc groovy = &lt; configure-oc-namespace.groovy team-ops\ncboc teams ops --put &lt; team-ops.json\ncboc groovy = &lt; configure-oc-namespace.groovy $OriginalNamespace\n</code></pre> team-ops.json <p>This <code>json</code> file that describes a team. By default there are three roles defined on a team, <code>TEAM_ADMIN</code>, <code>TEAM_MEMBER</code>, and <code>TEAM_GUEST</code>. Don't forget to change the <code>id</code>'s to Group ID's from your Single-Sign-On solution.</p> <pre><code>{\n    \"version\" : \"1\",\n    \"data\": {\n        \"name\": \"ops\",\n        \"displayName\": \"Operations\",\n        \"provisioningRecipe\": \"basic\",\n        \"members\": [{\n            \"id\": \"Catmins\",\n            \"roles\": [\"TEAM_ADMIN\"]\n        },\n        {\n            \"id\": \"Pirates\",\n            \"roles\": [\"TEAM_MEMBER\"]\n        },\n        {\n            \"id\": \"Continental\",\n            \"roles\": [\"TEAM_GUEST\"]\n        }\n        ],\n        \"icon\": {\n            \"name\": \"hexagons\",\n            \"color\": \"#8d7ec1\"\n        }\n    }\n}\n</code></pre> configure-oc-namespace.groovy <p>This is a Jenkins Configuration or System Groovy script. It will change the <code>namespace</code> Operation Center uses to create resources. You can change this in the UI by going to <code>Operations Center</code> -&gt; <code>Manage Jenkins</code> -&gt; <code>System Configuration</code> -&gt; <code>Master Provisioning</code> -&gt; <code>Namespace</code>.</p> <pre><code>import hudson.*\nimport hudson.util.Secret;\nimport hudson.util.Scrambler;\nimport hudson.util.FormValidation;\nimport jenkins.*\nimport jenkins.model.*\nimport hudson.security.*\n\nimport com.cloudbees.masterprovisioning.kubernetes.KubernetesMasterProvisioning\nimport com.cloudbees.masterprovisioning.kubernetes.KubernetesClusterEndpoint\n\nprintln \"=== KubernetesMasterProvisioning Configuration - start\"\n\nprintln \"== Retrieving main configuration\"\ndef descriptor = Jenkins.getInstance().getInjector().getInstance(KubernetesMasterProvisioning.DescriptorImpl.class)\ndef namespace = this.args[0]\n\ndef currentKubernetesClusterEndpoint =  descriptor.getClusterEndpoints().get(0)\nprintln \"= Found current endpoint\"\nprintln \"= \" + currentKubernetesClusterEndpoint.toString()\ndef id = currentKubernetesClusterEndpoint.getId()\ndef name = currentKubernetesClusterEndpoint.getName()\ndef url = currentKubernetesClusterEndpoint.getUrl()\ndef credentialsId = currentKubernetesClusterEndpoint.getCredentialsId()\n\nprintln \"== Setting Namspace to \" + namespace\ndef updatedKubernetesClusterEndpoint = new KubernetesClusterEndpoint(id, name, url, credentialsId, namespace)\ndef clusterEndpoints = new ArrayList&lt;KubernetesClusterEndpoint&gt;()\nclusterEndpoints.add(updatedKubernetesClusterEndpoint)\ndescriptor.setClusterEndpoints(clusterEndpoints)\n\nprintln \"== Saving Jenkins configuration\"\ndescriptor.save()\n\nprintln \"=== KubernetesMasterProvisioning Configuration - finish\"\n</code></pre>"},{"location":"blogs/teams-automation/#configure-team-ops-master","title":"Configure Team Ops Master","text":"<p>Now that we've created the Operations Team Master (Team Ops), we can configure it. </p> <p>The Pipelines we need will require credentials, we describe them below. </p> <ul> <li>githubtoken_token: GitHub API Token only, credentials type <code>Secret Text</code>  (for the PR pipeline)</li> <li>githubtoken: GitHub username and API Token</li> <li>jenkins-api: Username and API Token for Operations Center. Just like the one we used for Client Jar.</li> </ul> <p>We also need to have a Global Pipeline Library defined by the name <code>github.com/joostvdg/jpl-core</code>. This, as the name suggests, should point to <code>https://github.com/joostvdg/jpl-core.git</code>.</p>"},{"location":"blogs/teams-automation/#create-gitops-pipeline","title":"Create GitOps Pipeline","text":"<p>In total, we need two repositories and two or three Pipelines. You can either use my CLI Docker Image or roll your own. I will proceed as if you will create your own.</p> <ul> <li>CLI Image Pipeline: this will create a CLI Docker Image that is used to talk to Operations Center via the Client Jar (CLI)</li> <li>PR Pipeline: I like the idea of Self-Service, but in order to keep things in check, you might want to provide that via a PullRequest (PR) rather than a direct write to the Master branch. This is also Repository One, as I prefer having each pipeline in their own Repository, but you don't need to.</li> <li>Main Pipeline: will trigger on a commit to the Master branch and create the new team. I'll even throw in a free manage your Team Recipes for free as well.</li> </ul>"},{"location":"blogs/teams-automation/#create-cli-image-pipeline","title":"Create CLI Image Pipeline","text":"<p>In a Kubernetes cluster, you should not build with Docker directly, use an in-cluster builder such as Kaniko or Buildah. </p> <p>You can read more about the why and how elsewhere on this site.</p> <p>Tip</p> <p>If you do not want to create your own, you can re-use my images.</p> <p>There should be one available for every recent version of CloudBees Core that will work with your Operations Center. The images are available in DockerHub at caladreas/cbcore-cli</p>"},{"location":"blogs/teams-automation/#kaniko-configuration","title":"Kaniko Configuration","text":"<p>Kaniko uses a Docker Image to build your Docker Image in cluster. It does however need to directly communicate to your Docker Registry. This requires a Kubernetes <code>Secret</code> of type <code>docker-registry</code>.</p> <p>How you can do this and more, you can read on the CloudBees Core Docs.</p>"},{"location":"blogs/teams-automation/#pipeline","title":"Pipeline","text":"<p>Now that you have Kaniko configured, you can use this Pipeline to create your own CLI Images.</p> <p>Caution</p> <p>Make sure you replace the environment variables with values that make sense to you.</p> <ul> <li>CJOC_URL internal url in Kubernets, usually <code>http://cjoc.&lt;namespace&gt;/cjoc</code></li> <li>REGISTRY : index.docker.io = DockerHub</li> <li>REPO: docker repository name</li> <li>IMAGE: docker image name</li> </ul> Jenkinsfile <p>Jenkins Declarative Pipeline for the CLI Image geberation.</p> <pre><code>pipeline {\n    agent {\n        kubernetes {\n        //cloud 'kubernetes'\n        label 'test'\n        yaml \"\"\"\nkind: Pod\nmetadata:\n  name: test\nspec:\n  containers:\n  - name: curl\n    image: byrnedo/alpine-curl\n    command:\n    - cat\n    tty: true\n    resources:\n      requests:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor:debug\n    imagePullPolicy: Always\n    command:\n    - /busybox/cat\n    tty: true\n    resources:\n      requests:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n    volumeMounts:\n      - name: jenkins-docker-cfg\n        mountPath: /root\n  volumes:\n  - name: jenkins-docker-cfg\n    projected:\n      sources:\n      - secret:\n          name: docker-credentials\n          items:\n            - key: .dockerconfigjson\n              path: .docker/config.json\n\"\"\"\n        }\n    }\n    environment {\n        CJOC_URL    = 'http://cjoc.cloudbees-core/cjoc'\n        CLI_VERSION = ''\n        REGISTRY    = 'index.docker.io'\n        REPO        = 'caladreas'\n        IMAGE       = 'cbcore-cli'\n    }\n    stages {\n        stage('Download CLI') {\n            steps {\n                container('curl') {\n                    sh 'curl --version'\n                    sh 'echo ${CJOC_URL}/jnlpJars/jenkins-cli.jar'\n                    sh 'curl ${CJOC_URL}/jnlpJars/jenkins-cli.jar --output jenkins-cli.jar'\n                    sh 'ls -lath'\n                }\n            }\n        }\n        stage('Prepare') {\n            parallel {\n                stage('Verify CLI') {\n                    environment {\n                        CREDS   = credentials('jenkins-api')\n                        CLI     = \"java -jar jenkins-cli.jar -noKeyAuth -s ${CJOC_URL} -auth\"\n                    }\n                    steps {\n                        sh 'echo ${CLI}'\n                        script {\n                            CLI_VERSION = sh returnStdout: true, script: '${CLI} ${CREDS} version'\n                        }\n                        sh 'echo ${CLI_VERSION}'\n                    }\n                }\n                stage('Prepare Dockerfile') {\n                    steps {\n                        writeFile encoding: 'UTF-8', file: 'Dockerfile', text: \"\"\"FROM mcr.microsoft.com/java/jre-headless:8u192-zulu-alpine\nWORKDIR /usr/bin\nADD jenkins-cli.jar .\nRUN pwd\nRUN ls -lath\n\"\"\"\n                    }\n                }\n            }\n        }\n        stage('Build with Kaniko') {\n            environment { \n                PATH = \"/busybox:/kaniko:$PATH\"\n                TAG  = \"${CLI_VERSION}\"\n            }\n            steps {\n                sh 'echo image fqn=${REGISTRY}/${REPO}/${IMAGE}:${TAG}'\n                container(name: 'kaniko', shell: '/busybox/sh') {\n                    sh '''#!/busybox/sh\n                    /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:${TAG}\n                    /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:latest\n                    '''\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"blogs/teams-automation/#pr-pipeline","title":"PR Pipeline","text":"<p>Caution</p> <p>The PR Pipeline example builds upon the GitHub API, if you're not using GItHub, you will have to figure out another way to make the PR.</p>"},{"location":"blogs/teams-automation/#tools-used","title":"Tools Used","text":"<ul> <li>yq: commandline tool for processing Yaml files</li> <li>jq commandline tool for pressing Json files </li> <li>Kustomize templating tool for Kubernetes Yaml, as of Kubernetes <code>1.13</code>, this is part of the Client (note, your server can be older, don't worry!)</li> <li>Hub commandline client for GitHub</li> </ul>"},{"location":"blogs/teams-automation/#repository-layout","title":"Repository Layout","text":"<ul> <li>folder: <code>team-master-template</code><ul> <li>with file <code>simple.json</code></li> </ul> </li> <li>folder: <code>namespace-creation</code><ul> <li>with folder: <code>kustomize</code> this contains the Kustomize configuration</li> </ul> </li> </ul> Simple.json <p>This is a template for the team JSON definition.</p> <pre><code>{\n    \"version\" : \"1\",\n    \"data\": {\n        \"name\": \"NAME\",\n        \"displayName\": \"DISPLAY_NAME\",\n        \"provisioningRecipe\": \"RECIPE\",\n        \"members\": [\n            {\n                \"id\": \"ADMINS\",\n                \"roles\": [\"TEAM_ADMIN\"]\n            },\n            {\n                \"id\": \"MEMBERS\",\n                \"roles\": [\"TEAM_MEMBER\"]\n            },\n            {\n                \"id\": \"GUESTS\",\n                \"roles\": [\"TEAM_GUEST\"]\n            }\n        ],\n        \"icon\": {\n            \"name\": \"ICON\",\n            \"color\": \"HEX_COLOR\"\n        }\n    }\n}\n</code></pre>"},{"location":"blogs/teams-automation/#kustomize-configuration","title":"Kustomize Configuration","text":"<p>Kustomize is a tool for template Kubernetes YAML definitions, which is what we need here. However, only for the <code>namespace</code> creation &amp; configuration. So if you don't want to do that, you can skip this.</p> <p>The Kustomize configuration has two parts, a folder called <code>team-example</code> with a <code>kustomization.yaml</code>. This will be what we configure to generate a new yaml definition. The main template is in the folder <code>base</code>, where the entrypoint will be again <code>kustomization.yaml</code>. This time, the <code>kustomization.yaml</code> will link to all the template files we need.</p> <p>As posting all these yaml files again is a bit much, I'll link to my example repo. Feel free to fork it instead: cb-team-gitops-template</p> <ul> <li>configmap.yaml: the Jenkins Agent ConfigMap</li> <li>namespace.yaml: the new namespace</li> <li>resource-quota.yaml: resource quota's for the namespace</li> <li>role-binding-cjoc.yaml: a role binding for the CJOC ServiceAccount, so it create create the new Master in the new <code>namespace</code></li> <li>role-binding.yaml: the role binding for the <code>jenkins</code> ServiceAccount, which allows the new Master to create and manage Pods (for PodTemplates)</li> <li>role-cjoc.yaml: the role for CJOC for the ability to create a Master in the new Namspace</li> <li>role.yaml: the role for the <code>jenkins</code> ServiceAccount for the new Master</li> <li>service-account.yaml: the ServiceAccount, <code>jenkins</code>, used by the new Master</li> </ul>"},{"location":"blogs/teams-automation/#pipeline_1","title":"Pipeline","text":"<p>The Pipeline will do the following:</p> <ul> <li>capture input parameters to be used to customize the Team Master</li> <li>update the Kustomize template to make sure every resource is correct for the new namespace (<code>teams-&lt;name of team&gt;</code>)</li> <li>execute Kustomize to generate a single <code>yaml</code> file that defines the configuration for the new Team Masters' namespace</li> <li>process the <code>simple.json</code> to generate a <code>team.json</code> file for the new Team Master for use with the Jenkins CLI</li> <li>checkout your GIT_REPO that contains your team definitions</li> <li>create a new PR to your GIT_REPO for the new team</li> </ul> Jenkinsfile <p>Variables to update:</p> <ul> <li>GIT_REPO: the GitHub repository in which the Team Definitions are stored</li> <li>RESET_NAMESPACE: the namespace Operations Center should use as default</li> </ul> <pre><code>  pipeline {\n      agent {\n          kubernetes {\n          label 'team-automation'\n          yaml \"\"\"\n  kind: Pod\n  spec:\n    containers:\n    - name: hub\n      image:  caladreas/hub\n      command: [\"cat\"]\n      tty: true\n      resources:\n        requests:\n          memory: \"50Mi\"\n          cpu: \"150m\"\n        limits:\n          memory: \"50Mi\"\n          cpu: \"150m\"\n    - name: kubectl\n      image: bitnami/kubectl:latest\n      command: [\"cat\"]\n      tty: true\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n      resources:\n        requests:\n          memory: \"50Mi\"\n          cpu: \"100m\"\n        limits:\n          memory: \"150Mi\"\n          cpu: \"200m\"\n    - name: yq\n      image: mikefarah/yq\n      command: ['cat']\n      tty: true\n      resources:\n        requests:\n          memory: \"50Mi\"\n          cpu: \"100m\"\n        limits:\n          memory: \"50Mi\"\n          cpu: \"100m\"\n    - name: jq\n      image: colstrom/jq\n      command: ['cat']\n      tty: true\n      resources:\n        requests:\n          memory: \"50Mi\"\n          cpu: \"100m\"\n        limits:\n          memory: \"50Mi\"\n          cpu: \"100m\"\n\n  \"\"\"\n          }\n      }\n      libraries {\n          lib('github.com/joostvdg/jpl-core')\n      }\n      options {\n          disableConcurrentBuilds() // we don't want more than one at a time\n          checkoutToSubdirectory 'templates' // we need to do two checkouts\n          buildDiscarder logRotator(artifactDaysToKeepStr: '', artifactNumToKeepStr: '', daysToKeepStr: '5', numToKeepStr: '5') // always clean up\n      }\n      environment {\n          envGitInfo          = ''\n          RESET_NAMESPACE     = 'jx-production'\n          TEAM_BASE_NAME      = ''\n          NAMESPACE_TO_CREATE = ''\n          DISPLAY_NAME        = ''\n          TEAM_RECIPE         = ''\n          ICON                = ''\n          ICON_COLOR_CODE     = ''\n          ADMINS_ROLE         = ''\n          MEMBERS_ROLE        = ''\n          GUESTS_ROLE         = ''\n          RECORD_LOC          = ''\n          GIT_REPO                 = ''\n      }\n      stages {\n          stage('Team Details') {\n              input {\n                  message \"Please enter the team details.\"\n                  ok \"Looks good, proceed\"\n                  parameters {\n                      string(name: 'Name', defaultValue: 'hex', description: 'Please specify a team name')\n                      string(name: 'DisplayName', defaultValue: 'Hex', description: 'Please specify a team display name')\n                      choice choices: ['joostvdg', 'basic', 'java-web'], description: 'Please select a Team Recipe', name: 'TeamRecipe'\n                      choice choices: ['anchor', 'bear', 'bowler-hat', 'briefcase', 'bug', 'calculator', 'calculatorcart', 'clock', 'cloud', 'cloudbees', 'connect', 'dollar-bill', 'dollar-symbol', 'file', 'flag', 'flower-carnation', 'flower-daisy', 'help', 'hexagon', 'high-heels', 'jenkins', 'key', 'marker', 'monocle', 'mustache', 'office', 'panther', 'paw-print', 'teacup', 'tiger', 'truck'], description: 'Please select an Icon', name: 'Icon'\n                      string(name: 'IconColorCode', defaultValue: '#CCCCCC', description: 'Please specify a valid html hexcode for the color (https://htmlcolorcodes.com/)')\n                      string(name: 'Admins', defaultValue: 'Catmins', description: 'Please specify a groupid or userid for the TEAM_ADMIN role')\n                      string(name: 'Members', defaultValue: 'Pirates', description: 'Please specify a groupid or userid for the TEAM_MEMBER role')\n                      string(name: 'Guests', defaultValue: 'Continental', description: 'Please specify a groupid or userid for the TEAM_GUEST role')\n                  }\n              }\n              steps {\n                  println \"Name=${Name}\"\n                  println \"DisplayName=${DisplayName}\"\n                  println \"TeamRecipe=${TeamRecipe}\"\n                  println \"Icon=${Icon}\"\n                  println \"IconColorCode=${IconColorCode}\"\n                  println \"Admins=${Admins}\"\n                  println \"Members=${Members}\"\n                  println \"Guests=${Guests}\"\n                  script {\n                      TEAM_BASE_NAME      = \"${Name}\"\n                      NAMESPACE_TO_CREATE = \"cb-teams-${Name}\"\n                      DISPLAY_NAME        = \"${DisplayName}\"\n                      TEAM_RECIPE         = \"${TeamRecipe}\"\n                      ICON                = \"${Icon}\"\n                      ICON_COLOR_CODE     = \"${IconColorCode}\"\n                      ADMINS_ROLE         = \"${Admins}\"\n                      MEMBERS_ROLE        = \"${Members}\"\n                      GUESTS_ROLE         = \"${Guests}\"\n                      RECORD_LOC          = \"templates/teams/${Name}\"\n                      sh \"mkdir -p ${RECORD_LOC}\"\n                  }\n              }\n          }\n          stage('Create Team Config') {\n              environment {\n                  BASE        = 'templates/namespace-creation/kustomize'\n                  NAMESPACE   = \"${NAMESPACE_TO_CREATE}\"\n                  RECORD_LOC  = \"templates/teams/${TEAM_BASE_NAME}\"\n              }\n              parallel {\n                  stage('Namespace') {\n                      steps {\n                          container('yq') {\n                              sh 'yq w -i ${BASE}/base/role-binding.yaml subjects[0].namespace ${NAMESPACE}'\n                              sh 'yq w -i ${BASE}/base/namespace.yaml metadata.name ${NAMESPACE}'\n                              sh 'yq w -i ${BASE}/team-example/kustomization.yaml namespace ${NAMESPACE}'\n                          }\n                          container('kubectl') {\n                              sh '''\n                                  kubectl kustomize ${BASE}/team-example &gt; ${RECORD_LOC}/team.yaml\n                                  cat ${RECORD_LOC}/team.yaml\n                              '''\n                          }\n                      }\n                  }\n                  stage('Team Master JSON') {\n                      steps {\n                          container('jq') {\n                              sh \"\"\"jq \\\n                              '.data.name = \"${TEAM_BASE_NAME}\" |\\\n                              .data.displayName = \"${DISPLAY_NAME}\" |\\\n                              .data.provisioningRecipe = \"${TEAM_RECIPE}\" |\\\n                              .data.icon.name = \"${ICON}\" |\\\n                              .data.icon.color = \"${ICON_COLOR_CODE}\" |\\\n                              .data.members[0].id = \"${ADMINS_ROLE}\" |\\\n                              .data.members[1].id = \"${MEMBERS_ROLE}\" |\\\n                              .data.members[2].id = \"${GUESTS_ROLE}\"'\\\n                              templates/team-master-template/simple.json &gt; ${RECORD_LOC}/team.json\n                              \"\"\"\n                          }\n                          sh 'cat ${RECORD_LOC}/team.json'\n                      }\n                  }\n              }\n          }\n          stage('Create PR') {\n              when { branch 'master'}\n              environment {\n                  RECORD_OLD_LOC  = \"templates/teams/${TEAM_BASE_NAME}\"\n                  RECORD_LOC      = \"teams/${TEAM_BASE_NAME}\"\n                  PR_CHANGE_NAME  = \"add_team_${TEAM_BASE_NAME}\"\n              }\n              steps {\n                  container('hub') {\n                      dir('cb-team-gitops') {\n                          script {\n                              envGitInfo = git \"${GIT_REPO}\"\n                          }\n                          sh 'git checkout -b ${PR_CHANGE_NAME}'\n                          sh 'ls -lath ../${RECORD_OLD_LOC}'\n                          sh 'cp -R ../${RECORD_OLD_LOC} ./teams'\n                          sh 'ls -lath'\n                          sh 'ls -lath teams/'\n\n                          gitRemoteConfigByUrl(envGitInfo.GIT_URL, 'githubtoken_token') // must be a API Token ONLY -&gt; secret text\n                          sh '''\n                          git config --global user.email \"jenkins@jenkins.io\"\n                          git config --global user.name \"Jenkins\"\n                          git add ${RECORD_LOC}\n                          git status\n                          git commit -m \"add team ${TEAM_BASE_NAME}\"\n                          git push origin ${PR_CHANGE_NAME}\n                          '''\n\n\n                          // has to be indented like that, else the indents will be in the pr description\n                          writeFile encoding: 'UTF-8', file: 'pr-info.md', text: \"\"\"Add ${TEAM_BASE_NAME}\n  \\n\n  This pr is automatically generated via CloudBees.\\\\n\n  \\n\n  The job: ${env.JOB_URL}\n                      \"\"\"\n\n                          // TODO: unfortunately, environment {}'s credentials have fixed environment variable names\n                          // TODO: in this case, they need to be EXACTLY GITHUB_PASSWORD and GITHUB_USER\n                          script {\n                              withCredentials([usernamePassword(credentialsId: 'githubtoken', passwordVariable: 'GITHUB_PASSWORD', usernameVariable: 'GITHUB_USER')]) {\n                                  sh \"\"\"\n                                  set +x\n                                  hub pull-request --force -F pr-info.md -l '${TEAM_BASE_NAME}' --no-edit\n                                  \"\"\"\n                              }\n                          }\n                      }\n                  }\n              }\n          }\n      }\n  }\n</code></pre>"},{"location":"blogs/teams-automation/#main-pipeline","title":"Main Pipeline","text":"<p>The main Pipeline should be part of a repository. The Repository should look like this:</p> <ul> <li><code>recipes</code> (folder)<ul> <li><code>recipes.json</code> -&gt; current complete list of CloudBees Core Team Recipes definition</li> </ul> </li> <li><code>teams</code> (folder)<ul> <li>folder per team<ul> <li><code>team.json</code> -&gt; CloudBees Core Team definition</li> <li><code>team.yaml</code> -&gt; Kubernetes YAML definition of the <code>namespace</code> and all its resources</li> </ul> </li> </ul> </li> </ul>"},{"location":"blogs/teams-automation/#process","title":"Process","text":"<p>The pipeline can be a bit hard to grasp, so let me break it down into individual steps.</p> <p>We have the following stages:</p> <ul> <li><code>Create Team</code> - which is broken into sub-stages via the sequential stages feature.         * <code>Parse Changelog</code>         * <code>Create Namespace</code>         * <code>Change OC Namespace</code>         * <code>Create Team Master</code></li> <li><code>Test CLI Connection</code></li> <li><code>Update Team Recipes</code></li> </ul>"},{"location":"blogs/teams-automation/#notable-statements","title":"Notable Statements","text":"disableConcurrentBuilds <p>We change the <code>namespace</code> of Operation Center to a different value only for the duration of creating this master. This is something that should probably be part of the Team Master creation, but as it is a single configuration option for all that Operation Center does, we need to be careful. By ensuring we only run one build concurrently, we reduce the risk of this blowing up in our face.</p> <pre><code>options {\n    disableConcurrentBuilds()\n}\n</code></pre> when { } <p>The When Directive allows us to creating effective conditions for when a stage should be executed.</p> <p>The snippet below shows the use of a combination of both the <code>branch</code> and <code>changeset</code> built-in filters. <code>changeset</code> looks at the commit being build and validates that there was a change in that file path.</p> <pre><code>    when { allOf { branch 'master'; changeset \"teams/**/team.*\" } }\n</code></pre> post { always { } } <p>The Post Directive allows us to run certain commands after the main pipeline has run depending on the outcome (compared or not to the previous outcome). In this case, we want to make sure we reset the <code>namespace</code> used by Operations Center to the original value.</p> <p>By using <code>post { always {} }</code>, it will ALWAYS run, regardless of the status of the pipeline. So we should be safe.</p> <pre><code>post {\n    always {\n        container('cli') {\n            sh '${CLI} ${CREDS} groovy = &lt; resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE}'\n        }\n    }\n}\n</code></pre> stages { stage { parallel { stage() { stages { stage {  <p>Oke, you might've noticed this massive indenting depth and probably have some questions. </p> <p>By combining sequential stages with parallel stages we can create a set of stages that will be executed in sequence but can be controlled by a single <code>when {}</code> statement whether or not they get executed. </p> <p>This prevents mistakes being made in the condition and accidentally running one or other but not all the required steps.</p> <pre><code>    stages {\n        stage('Create Team') {\n            parallel {\n                stage('Main') {\n                    stages {\n                        stage('Parse Changelog') {\n</code></pre> changetSetData &amp; container('jpb') {} <p>Alright, so even if we know a team was added in <code>/teams/&lt;team-name&gt;</code>, we still don't know the following two things: 1) what is the name of this team, 2) was this team changed or deleted?</p> <p>So we have to process the changelog to be able to answer these questions as well. There are different ways of getting the changelog and parsing it. I've written one you can do on ANY machine, regardless of Jenkins by leveraging <code>Git</code> and my own custom binary (<code>jpb</code> -&gt; Jenkins Pipeline Binary). The code for my binary is at GitHub: github.com/joostvdg/jpb.</p> <p>An alternative approach is described by CloudBees Support here, which leverages Jenkins groovy powers.</p> <pre><code>COMMIT_INFO = \"${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT}\"\ndef changeSetData = sh returnStdout: true, script: \"git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO}\"\nchangeSetData = changeSetData.replace(\"\\n\", \"\\\\n\")\ncontainer('jpb') {\n    changeSetFolders = sh returnStdout: true, script: \"/usr/bin/jpb/bin/jpb GitChangeListToFolder '${changeSetData}' 'teams/'\"\n    changeSetFolders = changeSetFolders.split(',')\n}\n</code></pre>"},{"location":"blogs/teams-automation/#files","title":"Files","text":"recipes.json <p>The default Team Recipes that ships with CloudBees Core Modern.</p> <pre><code>    {\n        \"version\": \"1\",\n        \"data\": [{\n            \"name\": \"basic\",\n            \"displayName\": \"Basic\",\n            \"description\": \"The minimalistic setup.\",\n            \"plugins\": [\"bluesteel-master\", \"cloudbees-folders-plus\", \"cloudbees-jsync-archiver\", \"cloudbees-monitoring\", \"cloudbees-nodes-plus\", \"cloudbees-ssh-slaves\", \"cloudbees-support\", \"cloudbees-workflow-template\", \"credentials-binding\", \"email-ext\", \"git\", \"git-client\", \"github-branch-source\", \"github-organization-folder\", \"infradna-backup\", \"ldap\", \"mailer\", \"operations-center-analytics-reporter\", \"operations-center-cloud\", \"pipeline-model-definition\", \"ssh-credentials\", \"wikitext\", \"workflow-aggregator\", \"workflow-cps-checkpoint\"],\n            \"default\": true\n        }, {\n            \"name\": \"java-web\",\n            \"displayName\": \"Java &amp; Web Development\",\n            \"description\": \"The essential tools to build, release and deploy Java Web applications including integration with Maven, Gradle and Node JS.\",\n            \"plugins\": [\"bluesteel-master\", \"cloudbees-folders-plus\", \"cloudbees-jsync-archiver\", \"cloudbees-monitoring\", \"cloudbees-nodes-plus\", \"cloudbees-ssh-slaves\", \"cloudbees-support\", \"cloudbees-workflow-template\", \"credentials-binding\", \"email-ext\", \"git\", \"git-client\", \"github-branch-source\", \"github-organization-folder\", \"infradna-backup\", \"ldap\", \"mailer\", \"operations-center-analytics-reporter\", \"operations-center-cloud\", \"pipeline-model-definition\", \"ssh-credentials\", \"wikitext\", \"workflow-aggregator\", \"workflow-cps-checkpoint\", \"config-file-provider\", \"cloudbees-aws-cli\", \"cloudbees-cloudfoundry-cli\", \"findbugs\", \"gradle\", \"jira\", \"junit\", \"nodejs\", \"openshift-cli\", \"pipeline-maven\", \"tasks\", \"warnings\"],\n            \"default\": false\n        }]\n    }\n</code></pre> Jenkinsfile <p>This is the pipeline that will process the commit to the repository and, if it detects a new team is created will apply the changes.</p> <p>Variables to overwrite:</p> <ul> <li>GIT_REPO: the https url to the Git Repository your GitOps code/configuration is stored</li> <li>RESET_NAMESPACE: the <code>namespace</code> your Operation Center normally operates in</li> <li>CLI: this command depends on the namespace Operation Center is in (<code>http://&lt;service name&gt;.&lt;namespace&gt;/cjoc</code>)</li> </ul> <pre><code>pipeline {\n    agent {\n        kubernetes {\n            label 'jenkins-agent'\n            yaml '''\napiVersion: v1\nkind: Pod\nspec:\n  serviceAccountName: jenkins\n  containers:\n  - name: cli\n    image: caladreas/cbcore-cli:2.176.2.3\n    imagePullPolicy: Always\n    command:\n    - cat\n    tty: true\n    resources:\n      requests:\n        memory: \"50Mi\"\n        cpu: \"150m\"\n      limits:\n        memory: \"50Mi\"\n        cpu: \"150m\"\n  - name: kubectl\n    image: bitnami/kubectl:latest\n    command: [\"cat\"]\n    tty: true\n    resources:\n      requests:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"150Mi\"\n        cpu: \"200m\"\n  - name: yq\n    image: mikefarah/yq\n    command: ['cat']\n    tty: true\n    resources:\n      requests:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n  - name: jpb\n    image: caladreas/jpb\n    command:\n    - cat\n    tty: true\n    resources:\n      requests:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n  securityContext:\n    runAsUser: 1000\n    fsGroup: 1000\n'''\n        }\n    }\n    options {\n        disableConcurrentBuilds()\n        buildDiscarder logRotator(artifactDaysToKeepStr: '', artifactNumToKeepStr: '', daysToKeepStr: '5', numToKeepStr: '5')\n    }\n    environment {\n        RESET_NAMESPACE     = 'cloudbees-core'\n        CREDS               = credentials('jenkins-api')\n        CLI                 = \"java -jar /usr/bin/jenkins-cli.jar -noKeyAuth -s http://cjoc.cloudbees-core/cjoc -auth\"\n        COMMIT_INFO         = ''\n        TEAM                = ''\n        GIT_REPO            = ''\n    }\n    stages {\n        stage('Create Team') {\n            when { allOf { branch 'master'; changeset \"teams/**/team.*\" } }\n            parallel {\n                stage('Main') {\n                    stages {\n                        stage('Parse Changelog') {\n                            steps {\n                                // Alternative approach: https://support.cloudbees.com/hc/en-us/articles/217630098-How-to-access-Changelogs-in-a-Pipeline-Job-\n                                // However, that runs on the master, JPB runs in an agent!\n                                script {\n                                    scmVars = git \"${GIT_REPO}\"\n                                    COMMIT_INFO = \"${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT}\"\n                                    def changeSetData = sh returnStdout: true, script: \"git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO}\"\n                                    changeSetData = changeSetData.replace(\"\\n\", \"\\\\n\")\n                                    container('jpb') {\n                                        changeSetFolders = sh returnStdout: true, script: \"/usr/bin/jpb/bin/jpb GitChangeListToFolder '${changeSetData}' 'teams/'\"\n                                        changeSetFolders = changeSetFolders.split(',')\n                                    }\n                                    if (changeSetFolders.length &gt; 0) {\n                                        TEAM = changeSetFolders[0]\n                                        TEAM = TEAM.trim()\n                                        // to protect against a team being removed\n                                        def exists = fileExists \"teams/${TEAM}/team.yaml\"\n                                        if (!exists) {\n                                            TEAM = ''\n                                        }\n                                    } else {\n                                        TEAM = ''\n                                    }\n                                    echo \"Team that changed: |${TEAM}|\"\n                                }\n                            }\n                        }\n                        stage('Create Namespace') {\n                            when { expression { return !TEAM.equals('') } }\n                            environment {\n                                NAMESPACE   = \"cb-teams-${TEAM}\"\n                                RECORD_LOC  = \"teams/${TEAM}\"\n                            }\n                            steps {\n                                container('kubectl') {\n                                    sh '''\n                                        cat ${RECORD_LOC}/team.yaml\n                                        kubectl apply -f ${RECORD_LOC}/team.yaml\n                                    '''\n                                }\n                            }\n                        }\n                        stage('Change OC Namespace') {\n                            when { expression { return !TEAM.equals('') } }\n                            environment {\n                                NAMESPACE   = \"cb-teams-${TEAM}\"\n                            }\n                            steps {\n                                container('cli') {\n                                    sh 'echo ${NAMESPACE}'\n                                    script {\n                                        def response = sh encoding: 'UTF-8', label: 'create team', returnStatus: true, script: '${CLI} ${CREDS} groovy = &lt; resources/bootstrap/configure-oc-namespace.groovy ${NAMESPACE}'\n                                        println \"Response: ${response}\"\n                                    }\n                                }\n                            }\n                        }\n                        stage('Create Team Master') {\n                            when { expression { return !TEAM.equals('') } }\n                            environment {\n                                TEAM_NAME = \"${TEAM}\"\n                            }\n                            steps {\n                                container('cli') {\n                                    println \"TEAM_NAME=${TEAM_NAME}\"\n                                    sh 'ls -lath'\n                                    sh 'ls -lath teams/'\n                                    script {\n                                        def response = sh encoding: 'UTF-8', label: 'create team', returnStatus: true, script: '${CLI} ${CREDS} teams ${TEAM_NAME} --put &lt; \"teams/${TEAM_NAME}/team.json\"'\n                                        println \"Response: ${response}\"\n                                    }\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n        stage('Test CLI Connection') {\n            steps {\n                container('cli') {\n                    script {\n                        def response = sh encoding: 'UTF-8', label: 'retrieve version', returnStatus: true, script: '${CLI} ${CREDS} version'\n                        println \"Response: ${response}\"\n                    }\n                }\n            }\n        }\n        stage('Update Team Recipes') {\n            when { allOf { branch 'master'; changeset \"recipes/recipes.json\" } }\n            steps {\n                container('cli') {\n                    sh 'ls -lath'\n                    sh 'ls -lath recipes/'\n                    script {\n                        def response = sh encoding: 'UTF-8', label: 'update team recipe', returnStatus: true, script: '${CLI} ${CREDS} team-creation-recipes --put &lt; \"recipes/recipes.json\"'\n                        println \"Response: ${response}\"\n                    }\n                }\n            }\n        }\n    }\n    post {\n        always {\n            container('cli') {\n                sh '${CLI} ${CREDS} groovy = &lt; resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE}'\n            }\n        }\n    }\n}        \n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/","title":"Alerts With Alertmanager","text":""},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#list-of-alerts","title":"List Of Alerts","text":"<ul> <li>health score &lt; 0.95</li> <li>ingress performance </li> <li>vm heap usage ratio &gt; 80%</li> <li>file descriptor &gt; 75%</li> <li>job queue &gt; 10 over x minutes</li> <li>job success ratio &lt; 50%</li> <li>master executor count &gt; 0</li> <li>good http request ratio &lt; 90%</li> <li>offline nodes &gt; 5 over 30 minutes</li> <li>healtcheck duration &gt; 0.002</li> <li> <p>plugin updates available &gt; 10</p> </li> <li> <p>An alert that triggers if any of the health reports are failing</p> </li> <li>An alert that triggers if the file descriptor usage on the master goes above 80%</li> <li><code>vm.file.descriptor.ratio</code> -&gt; <code>vm_file_descriptor_ratio</code></li> <li>An alert that triggers if the JVM heap memory usage is over 80% for more than a minute</li> <li><code>vm.memory.heap.usage</code> -&gt; <code>vm_memory_heap_usage</code></li> <li>An alert that triggers if the 5 minute average of HTTP/404 responses goes above 10 per minute for more than five minutes</li> <li><code>http.responseCodes.badRequest</code> -&gt; <code>http_responseCodes_badRequest</code></li> </ul>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#alert-manager-configuration","title":"Alert Manager Configuration","text":"<p>We can configure Alert Manager via the Prometheus Helm Chart.</p> <p>All the configuration elements below are part of the <code>prom-values.yaml</code> we used to when installing Prometheus via Helm.</p>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#get-slack-endpoint","title":"Get Slack Endpoint","text":"<p>There are many ways to get the Alerts out, for all options you can read the Prometheus documentation. </p> <p>In this guide, I've chosen to use slack, as I find it convenient personally. Slack has a guide on creating webhooks, once you've created an <code>App</code> you can retrieve an endpoint which you can use directly in the Alertmanager configuration.</p>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#alerts-configuration","title":"Alerts Configuration","text":"<p>We configure the alerts within Prometheus itself via a <code>ConfigMap</code>. We configure the body of the alert configuration file via <code>serverFiles</code>.<code>alerts</code>.<code>groups</code> and <code>serverFiles</code>.<code>rules</code>. </p> <p>We can have a list of rules and a list of groups of rules. For more information how you can configure these rules, consult the Prometheus documentation.</p> <pre><code>serverFiles:\n  alerts:\n    groups:\n    # alerts come here\n  rules: {}\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#alert-example","title":"Alert Example","text":"<p>Below is an example of an Alert. We have the following fields:</p> <ul> <li>alert: the name of the alert</li> <li>expr: the query that should evaluate to <code>true</code> or <code>false</code></li> <li>for (optional): duration of the expressions equating to <code>true</code> before it fires</li> <li>labels(optional): you can add key-value pairs to encode more information on the alert, you can use this to select different receiver (e.g., email vs. slack, or different slack channels)</li> <li>annotations: we're expected to fill in <code>summary</code> and <code>description</code> as shown below, they will header and body of the alert</li> </ul> <pre><code>- alert: JenkinsTooManyJobsQueued\n  expr: sum(jenkins_queue_size_value) &gt; 5\n  for: 1m\n  labels:\n    severity: notify\n  annotations:\n    summary: \" {{ $labels.app_kubernetes_io_instance }} too many jobs queued\"\n    description: \"{{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs stuck in the queue\"\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#alertmanager-configuration","title":"Alertmanager Configuration","text":"<p>We use Alertmanager for what to do with alerts once they happen. We configure this in the same <code>prom-values.yaml</code> file, in this under <code>alertmanagerFiles</code>.<code>alertmanager.yml</code>.</p> <p>We can create different routes that match on labels or other values. For simplicity sake - this guide is not on Alertmanager's capabilities - we stick to the most straightforward example without any such matching or grouping. For more information on configuring routes, please read the Prometheus configuration documentation.</p> <pre><code>alertmanagerFiles:\n  alertmanager.yml:\n    global: {}\n    route:\n      group_by: [alertname, app_kubernetes_io_instance]\n      receiver: default\n    receivers:\n    - name: default\n      slack_configs:\n      - api_url: '&lt;REPLACE_WITH_YOUR_SLACK_API_ENDPOINT&gt;'\n        username: 'Alertmanager'\n        channel: '#notify'\n        send_resolved: true\n        title: \"{{ .CommonAnnotations.summary }} \" \n        text: \"{{ .CommonAnnotations.description }} {{ .CommonLabels.app_kubernetes_io_instance}} \"\n        title_link: http://my-prometheus.com/alerts\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#group-alerts","title":"Group Alerts","text":"<p>You can group alerts if they are similar or the same with different trigger values (warning vs critical).</p> <pre><code>serverFiles:\n  alerts:\n    groups:\n    - name: healthcheck\n      rules:\n      - alert: JenkinsHealthScoreToLow\n        # alert info\n      - alert: JenkinsTooSlowHealthCheck\n        # alert info\n    - name: jobs\n      rules:\n      - alert: JenkinsTooManyJobsQueued\n        # alert info\n      - alert: JenkinsTooManyJobsStuckInQueue\n        # alert info\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#the-alerts","title":"The Alerts","text":"<p>Behold, my awesome - eh, simple example - alerts. These are by no means the best alerts to create and are by no means alerts you should directly put into production. Please see them as examples to learn from!</p> <p>Caution</p> <p>One thing to note especially, the values for the <code>exp</code> and <code>for</code> are generally set very low. This is intentional, so they are easy to copy past and test. They should be relatively easy to trigger so you can learn about the relationship between the situation in your master and the alert firing.</p>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#too-many-jobs-queued","title":"Too Many Jobs Queued","text":"<p>If there are too many Jobs queued in the Jenkins Master. This event fires if there's more than <code>10</code> jobs in the queue for at least 10 minutes.</p> <pre><code>- alert: JenkinsTooManyJobsQueued\n  expr: sum(jenkins_queue_size_value) &gt; 10\n  for: 10m\n  labels:\n    severity: notify\n  annotations:\n    summary: \" {{ $labels.app_kubernetes_io_instance }} too many jobs queued\"\n    description: \"{{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs stuck in the queue\"\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#jobs-stuck-in-queue","title":"Jobs Stuck In Queue","text":"<p>Sometimes Jobs depend on other Jobs, which means they're not just in the queue, they're stuck in the queue.</p> <pre><code>- alert: JenkinsTooManyJobsStuckInQueue\n  expr: sum(jenkins_queue_stuck_value) by (app_kubernetes_io_instance) &gt; 5\n  for: 5m\n  labels:\n    severity: notify\n  annotations:\n    summary: \" {{ $labels.app_kubernetes_io_instance }} too many jobs queued\"\n    description: \" {{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs in queue\"\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#jobs-waiting-too-long-to-start","title":"Jobs Waiting Too Long To Start","text":"<p>If Jobs are generally waiting a long time to start, waiting for a build agent to be available or otherwise, we want to know. This value is not very useful - although not completely useless - if you only have PodTemplates as build agents. When you use PodTemplates, this value is the time between the job being scheduled and when the Pod is scheduled in Kubernetes.</p> <pre><code>- alert: JenkinsWaitingTooMuchOnJobStart\n  expr: sum (jenkins_job_waiting_duration) by (app_kubernetes_io_instance) &gt; 0.05\n  for: 1m\n  labels:\n    severity: notify\n  annotations:\n    summary: \"{{ $labels.app_kubernetes_io_instance }} waits too long for jobs\"\n    description: \"{{ $labels.app_kubernetes_io_instance }} is waiting on average {{ $value }} seconds to start a job\"\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#health-score-1","title":"health score &lt; 1","text":"<p>By default, each Jenkins Master has a health check consisting out of four values. Some plugins will add an entry, such as the CloudBees ElasticSearch Reporter for CloudBees Core. This values range from 0-1, and likely will show <code>0.25</code>, <code>0.50</code>, <code>0.75</code> and <code>1</code> as values.</p> <pre><code>- alert: JenkinsHealthScoreToLow\n  expr: sum(jenkins_health_check_score) by (app_kubernetes_io_instance) &lt; 1\n  for: 5m\n  labels:\n    severity: notify\n  annotations:\n    summary: \" {{ $labels.app_kubernetes_io_instance }} has a to low health score\"\n    description: \" {{ $labels.app_kubernetes_io_instance }} a health score lower than 100%\"\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#ingress-too-slow","title":"Ingress Too Slow","text":"<p>This alert looks at the ingress controller request duration. It fires if the request duration in 0.25 seconds or faster is not achieved for the 95% percentile.</p> <pre><code>- alert: AppTooSlow\n  expr: sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{le=\"0.25\"}[5m])) by (ingress) / sum(rate(nginx_ingress_controller_request_duration_seconds_count[5m])) by (ingress) &lt; 0.95\n  for: 5m\n  labels:\n    severity: notify\n  annotations:\n    summary: \"Application - {{ $labels.ingress }} - is too slow\"\n    description: \" {{ $labels.ingress }} - More then 5% of requests are slower than 0.25s\"\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#http-requests-too-slow","title":"HTTP Requests Too Slow","text":"<p>These are the HTTP requests in Jenkins' webserver itself. We should hold this by must stricter standards than the Ingress controller - which goes through many more layers.</p> <pre><code>- alert: JenkinsTooSlow\n  expr: sum(http_requests{quantile=\"0.99\"} ) by (app_kubernetes_io_instance) &gt; 1\n  for: 3m\n  labels:\n    severity: notify\n  annotations:\n    summary: \"{{ $labels.app_kubernetes_io_instance }} is too slow\"\n    description: \"{{ $labels.app_kubernetes_io_instance }}  More then 1% of requests are slower than 1s (request time: {{ $value }})\"\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#too-many-plugin-updates","title":"Too Many Plugin Updates","text":"<p>I always prefer having my instance up-to-date, don't you? So why not send an alert if there's more than X number of plugins waiting for an update.</p> <pre><code>- alert: JenkinsTooManyPluginsNeedUpate\n  expr: sum(jenkins_plugins_withUpdate) by (app_kubernetes_io_instance) &gt; 3\n  for: 1m\n  labels:\n    severity: notify\n  annotations:\n    summary: \" {{ $labels.app_kubernetes_io_instance }} too many plugins updates\"\n    description: \" {{ $labels.app_kubernetes_io_instance }} has {{ $value }} plugins that require an update\"\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#file-descriptor-ratio-40","title":"File Descriptor Ratio &gt; 40%","text":"<p>According to CloudBees' documentation, the File Descriptor Ratio should not exceed 40%.</p> <p>Warning</p> <p>I don't truly know the correct value level of this metric. So wether this should be <code>0.0040</code> or <code>0.40</code> I'm not sure. Also, does this make sense in Containers with remote storage? So before you put this in production, please re-evaluate this!</p> <pre><code>- alert: JenkinsToManyOpenFiles\n  expr: sum(vm_file_descriptor_ratio) by (app_kubernetes_io_instance) &gt; 0.040\n  for: 5m\n  labels:\n    severity: notify\n  annotations:\n    summary: \" {{ $labels.app_kubernetes_io_instance }} has a to many open files\"\n    description: \" {{ $labels.app_kubernetes_io_instance }} instance has used {{ $value }} of available open files\"\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#job-success-ratio-50","title":"Job Success Ratio &lt; 50%","text":"<p>Please, please do not use Job success ratios to punish people. But if it is at all possible - which it almost certainly is - keep a respectable level of success. When practicing Continuous Integration, a broken build is a stop the world event, fix it before moving on. </p> <p>100% success rate should be strived for. It is ok, not to achieve it, yet, one should be as close as possible and not let broken builds rot.</p> <pre><code>- alert: JenkinsTooLowJobSuccessRate\n  expr: sum(jenkins_runs_success_total) by (app_kubernetes_io_instance) / sum(jenkins_runs_total_total) by (app_kubernetes_io_instance) &lt; 0.5\n  for: 5m\n  labels:\n    severity: notify\n  annotations:\n    summary: \"{{$labels.app_kubernetes_io_instance}} has a too low job success rate\"\n    description: \"{{$labels.app_kubernetes_io_instance}} instance has less than 50% of jobs being successful\"\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#offline-nodes-5-over-10-minutes","title":"Offline nodes &gt; 5 over 10 minutes","text":"<p>Having nodes offline for quite some time is usually a bad sign. It can be a static agent that can be enabled or reconnect at will, so it isn't bad on its own. Having multiple offline for a long period is likely an issue somewhere, though.</p> <pre><code>- alert: JenkinsTooManyOfflineNodes\n  expr: sum(jenkins_node_offline_value) by (app_kubernetes_io_instance) &gt; 5\n  for: 10m\n  labels:\n    severity: notify\n  annotations:\n    summary: \"{{ $labels.app_kubernetes_io_instance }} has a too many offline nodes\"\n    description: \"{{ $labels.app_kubernetes_io_instance }} has {{ $value }} nodes that are offline for some time (5 minutes)\" \n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#healtcheck-duration-0002","title":"healtcheck duration &gt; 0.002","text":"<p>The health check within Jenkins is talking to itself. This means it is generally really fast. We should be very very strict here, if Jenkins start having trouble measuring its own health, it is a first sign of trouble.</p> <pre><code>- alert: JenkinsTooSlowHealthCheck\n  expr: sum(jenkins_health_check_duration{quantile=\"0.999\"})\n    by (app_kubernetes_io_instance) &gt; 0.001\n  for: 1m\n  labels:\n    severity: notify\n  annotations:\n    summary: \" {{ $labels.app_kubernetes_io_instance }} responds too slow to health check\"\n    description: \" {{ $labels.app_kubernetes_io_instance }} is responding too slow to the regular health check\"\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#gc-throughput-too-low","title":"GC ThroughPut Too Low","text":"<p>Ok, here I am on thin ice. I'm not a JVM expert, so this is just an inspiration. I do not know what would be a reasonable value for triggering an alert here. I'd say, test it!</p> <pre><code>- alert: JenkinsTooManyPluginsNeedUpate\n  expr: 1 - sum(vm_gc_G1_Young_Generation_time)by (app_kubernetes_io_instance)  /  sum (vm_uptime_milliseconds) by (app_kubernetes_io_instance) &lt; 0.99\n  for: 30m\n  labels:\n    severity: notify\n  annotations:\n    summary: \"{{ $labels.instance }} too low GC throughput\"\n    description: \"{{ $labels.instance }} has too low Garbage Collection throughput\"\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#vm-heap-usage-ratio-70","title":"vm heap usage ratio &gt; 70%","text":"<p>According to the CloudBees guide on tuning the JVM - which redirects to Oracle - the ration of JVM Heap memory usage should not exceed about 60%. So if we get over 70% for quite some time, expect trouble. As with any of these values, please do not take my word on it, and understand it yourself.</p> <pre><code>- alert: JenkinsVMMemoryRationTooHigh\n  expr: sum(vm_memory_heap_usage) by (app_kubernetes_io_instance) &gt; 0.70\n  for: 3m\n  labels:\n    severity: notify\n  annotations:\n    summary: \"{{$labels.app_kubernetes_io_instance}} too high memory ration\"\n    description: \"{{$labels.app_kubernetes_io_instance}} has a too high VM memory ration\"\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#uptime-less-than-two-hours","title":"Uptime Less Than Two Hours","text":"<p>I absolutely love servers that have excellent uptime. Running services in Containers makes that a thing of the past, such a shame. Still, I'd like my applications - such as Jenkins - to be up for reasonable lengths of time.</p> <p>In this case we can get notifications on Masters that have restart - for example, when OOMKilled by Kubernetes. We also get an alert when a new Master is created, which if there's selfservice involved is a nice bonus.</p> <pre><code>- alert: JenkinsNewOrRestarted\n  expr: sum(vm_uptime_milliseconds) by (app_kubernetes_io_instance) / 3600000 &lt; 2\n  for: 3m\n  labels:\n    severity: notify\n  annotations:\n    summary: \" {{ $labels.app_kubernetes_io_instance }} has low uptime\"\n    description: \" {{ $labels.app_kubernetes_io_instance }} has low uptime and was either restarted or is a new instance (uptime: {{ $value }} hours)\"\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/alerts/#full-example","title":"Full Example","text":"<pre><code>server:\n  ingress:\n    enabled: true\n    annotations:\n      ingress.kubernetes.io/ssl-redirect: \"false\"\n      nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n  resources:\n    limits:\n      cpu: 100m\n      memory: 1000Mi\n    requests:\n      cpu: 10m\n      memory: 500Mi\nalertmanager:\n  ingress:\n    enabled: true\n    annotations:\n      ingress.kubernetes.io/ssl-redirect: \"false\"\n      nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n  resources:\n    limits:\n      cpu: 10m\n      memory: 20Mi\n    requests:\n      cpu: 5m\n      memory: 10Mi\nkubeStateMetrics:\n  resources:\n    limits:\n      cpu: 10m\n      memory: 50Mi\n    requests:\n      cpu: 5m\n      memory: 25Mi\nnodeExporter:\n  resources:\n    limits:\n      cpu: 10m\n      memory: 20Mi\n    requests:\n      cpu: 5m\n      memory: 10Mi\npushgateway:\n  resources:\n    limits:\n      cpu: 10m\n      memory: 20Mi\n    requests:\n      cpu: 5m\n      memory: 10Mi\n\nserverFiles:\n  alerts:\n    groups:\n    - name: jobs\n      rules:\n      - alert: JenkinsTooManyJobsQueued\n        expr: sum(jenkins_queue_size_value) &gt; 5\n        for: 1m\n        labels:\n          severity: notify\n        annotations:\n          summary: \" {{ $labels.app_kubernetes_io_instance }} too many jobs queued\"\n          description: \"{{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs stuck in the queue\"\n      - alert: JenkinsTooManyJobsStuckInQueue\n        expr: sum(jenkins_queue_stuck_value) by (app_kubernetes_io_instance) &gt; 5\n        for: 1m\n        labels:\n          severity: notify\n        annotations:\n          summary: \" {{ $labels.app_kubernetes_io_instance }} too many jobs queued\"\n          description: \" {{ $labels.app_kubernetes_io_instance }} has {{ $value }} jobs in queue\"\n      - alert: JenkinsWaitingTooMuchOnJobStart\n        expr: sum (jenkins_job_waiting_duration) by (app_kubernetes_io_instance) &gt; 0.05\n        for: 1m\n        labels:\n          severity: notify\n        annotations:\n          summary: \"{{ $labels.app_kubernetes_io_instance }} waits too long for jobs\"\n          description: \"{{ $labels.app_kubernetes_io_instance }} is waiting on average {{ $value }} seconds to start a job\"\n      - alert: JenkinsTooLowJobSuccessRate\n        expr: sum(jenkins_runs_success_total) by (app_kubernetes_io_instance) / sum(jenkins_runs_total_total) by (app_kubernetes_io_instance) &lt; 0.60\n        for: 1m\n        labels:\n          severity: notify\n        annotations:\n          summary: \" {{ $labels.app_kubernetes_io_instance }} has a too low job success rate\"\n          description: \" {{ $labels.app_kubernetes_io_instance }} instance has {{ $value }}% of jobs being successful\"\n    - name: uptime\n      rules:\n      - alert: JenkinsNewOrRestarted\n        expr: sum(vm_uptime_milliseconds) by (app_kubernetes_io_instance) / 3600000 &lt; 2\n        for: 3m\n        labels:\n          severity: notify\n        annotations:\n          summary: \" {{ $labels.app_kubernetes_io_instance }} has low uptime\"\n          description: \" {{ $labels.app_kubernetes_io_instance }} has low uptime and was either restarted or is a new instance (uptime: {{ $value }} hours)\"\n    - name: plugins\n      rules:\n      - alert: JenkinsTooManyPluginsNeedUpate\n        expr: sum(jenkins_plugins_withUpdate) by (app_kubernetes_io_instance) &gt; 3\n        for: 1m\n        labels:\n          severity: notify\n        annotations:\n          summary: \" {{ $labels.app_kubernetes_io_instance }} too many plugins updates\"\n          description: \" {{ $labels.app_kubernetes_io_instance }} has {{ $value }} plugins that require an update\"\n    - name: jvm\n      rules:\n      - alert: JenkinsToManyOpenFiles\n        expr: sum(vm_file_descriptor_ratio) by (app_kubernetes_io_instance) &gt; 0.040\n        for: 5m\n        labels:\n          severity: notify\n        annotations:\n          summary: \" {{ $labels.app_kubernetes_io_instance }} has a to many open files\"\n          description: \" {{ $labels.app_kubernetes_io_instance }} instance has used {{ $value }} of available open files\"\n      - alert: JenkinsVMMemoryRationTooHigh\n        expr: sum(vm_memory_heap_usage) by (app_kubernetes_io_instance) &gt; 0.70\n        for: 3m\n        labels:\n          severity: notify\n        annotations:\n          summary: \"{{$labels.app_kubernetes_io_instance}} too high memory ration\"\n          description: \"{{$labels.app_kubernetes_io_instance}} has a too high VM memory ration\"\n      - alert: JenkinsTooManyPluginsNeedUpate\n        expr: 1 - sum(vm_gc_G1_Young_Generation_time)by (app_kubernetes_io_instance)  /  sum (vm_uptime_milliseconds) by (app_kubernetes_io_instance) &lt; 0.99\n        for: 30m\n        labels:\n          severity: notify\n        annotations:\n          summary: \"{{ $labels.instance }} too low GC throughput\"\n          description: \"{{ $labels.instance }} has too low Garbage Collection throughput\"\n    - name: web\n      rules:\n      - alert: JenkinsTooSlow\n        expr: sum(http_requests{quantile=\"0.99\"} ) by (app_kubernetes_io_instance) &gt; 1\n        for: 3m\n        labels:\n          severity: notify\n        annotations:\n          summary: \"{{ $labels.app_kubernetes_io_instance }} is too slow\"\n          description: \"{{ $labels.app_kubernetes_io_instance }}  More then 1% of requests are slower than 1s (request time: {{ $value }})\"\n      - alert: AppTooSlow\n        expr: sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{le=\"0.25\"}[5m])) by (ingress) / sum(rate(nginx_ingress_controller_request_duration_seconds_count[5m])) by (ingress) &lt; 0.95\n        for: 5m\n        labels:\n          severity: notify\n        annotations:\n          summary: \"Application - {{ $labels.ingress }} - is too slow\"\n          description: \" {{ $labels.ingress }} - More then 5% of requests are slower than 0.25s\"\n    - name: healthcheck\n      rules:\n      - alert: JenkinsHealthScoreToLow\n        expr: sum(jenkins_health_check_score) by (app_kubernetes_io_instance) &lt; 1\n        for: 5m\n        labels:\n          severity: notify\n        annotations:\n          summary: \" {{ $labels.app_kubernetes_io_instance }} has a to low health score\"\n          description: \" {{ $labels.app_kubernetes_io_instance }} a health score lower than 100%\"\n      - alert: JenkinsTooSlowHealthCheck\n        expr: sum(jenkins_health_check_duration{quantile=\"0.999\"})\n          by (app_kubernetes_io_instance) &gt; 0.001\n        for: 1m\n        labels:\n          severity: notify\n        annotations:\n          summary: \" {{ $labels.app_kubernetes_io_instance }} responds too slow to health check\"\n          description: \" {{ $labels.app_kubernetes_io_instance }} is responding too slow to the regular health check\"\n    - name: nodes\n      rules:\n      - alert: JenkinsTooManyOfflineNodes\n        expr: sum(jenkins_node_offline_value) by (app_kubernetes_io_instance) &gt; 3\n        for: 1m\n        labels:\n          severity: notify\n        annotations:\n          summary: \"{{ $labels.app_kubernetes_io_instance }} has a too many offline nodes\"\n          description: \"{{ $labels.app_kubernetes_io_instance }} has {{ $value }} nodes that are offline for some time (5 minutes)\"\nalertmanagerFiles:\n  alertmanager.yml:\n    global: {}\n    route:\n      group_by: [alertname, app_kubernetes_io_instance]\n      receiver: default\n    receivers:\n    - name: default\n      slack_configs:\n      - api_url: '&lt;REPLACE_WITH_YOUR_SLACK_API_URL&gt;'\n        username: 'Alertmanager'\n        channel: '#notify'\n        send_resolved: true\n        title: \"{{ .CommonAnnotations.summary }} \" \n        text: \"{{ .CommonAnnotations.description }} {{ .CommonLabels.app_kubernetes_io_instance}} \"\n        title_link: http://my-prometheus.com/alerts\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/cloudbees/","title":"CloudBees Core","text":"<ul> <li>add prometheus plugin to team-recipe</li> <li>update CJOC's Master Provisioning with prometheus annotations</li> </ul> <pre><code>apiVersion: \"apps/v1\"\nkind: \"StatefulSet\"\nspec:\n  template:\n    metadata:\n      annotations:\n        prometheus.io/path: /${name}/prometheus\n        prometheus.io/port: \"8080\"\n        prometheus.io/scrape: \"true\"\n      labels:\n        app.kubernetes.io/component: Managed-Master\n        app.kubernetes.io/instance: ${name}\n        app.kubernetes.io/managed-by: CloudBees-Core-Cloud-Operations-Center\n        app.kubernetes.io/name: ${name}\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/dashboard/","title":"Grafana Dashboard","text":""},{"location":"blogs/monitor-jenkins-on-k8s/dashboard/#example-screenshot","title":"Example Screenshot","text":""},{"location":"blogs/monitor-jenkins-on-k8s/dashboard/#grafana-variables","title":"Grafana Variables","text":"<ul> <li>cluster<ul> <li>type: <code>datasource</code></li> <li>datasource type: <code>prometheus</code></li> </ul> </li> <li>node<ul> <li>type: <code>query</code></li> <li>query: <code>label_values(kube_node_info{component=\"kube-state-metrics\"}, node)</code></li> <li>label: <code>K8S Node</code></li> <li>multivalue</li> <li>include all</li> </ul> </li> <li>namespace<ul> <li>type: <code>query</code></li> <li>query: <code>label_values(jenkins_health_check_duration, kubernetes_namespace)</code></li> <li>label: <code>Namespace</code></li> <li>multivalue</li> <li>include all</li> </ul> </li> <li>instance<ul> <li>type: <code>query</code></li> <li>query: <code>label_values(jenkins_health_check_duration, app_kubernetes_io_instance)</code></li> <li>label: <code>Master</code></li> <li>multivalue</li> <li>include all</li> </ul> </li> </ul>"},{"location":"blogs/monitor-jenkins-on-k8s/dashboard/#dashboard-json","title":"Dashboard json","text":"<pre><code>{\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": \"-- Grafana --\",\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations &amp; Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n  \"description\": \"Dashboard for when you are using multiple Jenkins Masters\",\n  \"editable\": true,\n  \"gnetId\": null,\n  \"graphTooltip\": 1,\n  \"id\": 9,\n  \"iteration\": 1565906968208,\n  \"links\": [],\n  \"panels\": [\n    {\n      \"collapsed\": false,\n      \"gridPos\": {\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 0\n      },\n      \"id\": 20,\n      \"panels\": [],\n      \"title\": \"Performance\",\n      \"type\": \"row\"\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"decimals\": 3,\n      \"description\": \"The ratio of ok (200) request out of all requests.\",\n      \"fill\": 1,\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 0,\n        \"y\": 1\n      },\n      \"id\": 36,\n      \"legend\": {\n        \"alignAsTable\": true,\n        \"avg\": false,\n        \"current\": true,\n        \"max\": false,\n        \"min\": false,\n        \"rightSide\": true,\n        \"show\": true,\n        \"total\": false,\n        \"values\": true\n      },\n      \"lines\": true,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"options\": {},\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"sum(http_responseCodes_ok_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) \\nby (app_kubernetes_io_instance) / \\nsum(http_requests_count{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) \\nby (app_kubernetes_io_instance)\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [\n        {\n          \"colorMode\": \"warning\",\n          \"fill\": true,\n          \"line\": true,\n          \"op\": \"lt\",\n          \"value\": 0.991,\n          \"yaxis\": \"left\"\n        },\n        {\n          \"colorMode\": \"critical\",\n          \"fill\": true,\n          \"line\": true,\n          \"op\": \"lt\",\n          \"value\": 0.981,\n          \"yaxis\": \"left\"\n        }\n      ],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"Good HTTP Request Ratio\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"decimals\": 3,\n          \"format\": \"percentunit\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": \"1\",\n          \"min\": \"0.95\",\n          \"show\": true\n        },\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": false\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"decimals\": 0,\n      \"description\": \"Http Server Errors (500)\",\n      \"fill\": 1,\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 5,\n        \"x\": 6,\n        \"y\": 1\n      },\n      \"id\": 38,\n      \"legend\": {\n        \"alignAsTable\": true,\n        \"avg\": false,\n        \"current\": true,\n        \"hideEmpty\": true,\n        \"hideZero\": false,\n        \"max\": false,\n        \"min\": false,\n        \"rightSide\": true,\n        \"show\": true,\n        \"total\": false,\n        \"values\": true\n      },\n      \"lines\": true,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"options\": {},\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"http_responseCodes_serverError_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [\n        {\n          \"colorMode\": \"warning\",\n          \"fill\": true,\n          \"line\": true,\n          \"op\": \"gt\",\n          \"value\": 1,\n          \"yaxis\": \"left\"\n        }\n      ],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"Server Errors\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"fill\": 1,\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 11,\n        \"y\": 1\n      },\n      \"id\": 42,\n      \"interval\": \"1m\",\n      \"legend\": {\n        \"avg\": false,\n        \"current\": false,\n        \"max\": false,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": false\n      },\n      \"lines\": true,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"options\": {},\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"sum (jenkins_job_waiting_duration{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\",\n          \"format\": \"time_series\",\n          \"instant\": false,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"Job Wait Duration\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"s\",\n          \"label\": null,\n          \"logBase\": 2,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": false\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"fill\": 1,\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 17,\n        \"y\": 1\n      },\n      \"id\": 44,\n      \"interval\": \"1m\",\n      \"legend\": {\n        \"avg\": false,\n        \"current\": false,\n        \"hideEmpty\": false,\n        \"hideZero\": false,\n        \"max\": false,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": false\n      },\n      \"lines\": true,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"options\": {},\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"sum(jenkins_job_building_duration{quantile=\\\"0.5\\\",kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})\\nby (app_kubernetes_io_instance) / 60\",\n          \"format\": \"time_series\",\n          \"instant\": false,\n          \"interval\": \"\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"p50-{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        },\n        {\n          \"expr\": \"sum(jenkins_node_builds{quantile=\\\"0.999\\\",kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})\\nby (app_kubernetes_io_instance) / 60\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"p999-{{app_kubernetes_io_instance}}\",\n          \"refId\": \"B\"\n        }\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"Avg Build Duration Minutes\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"m\",\n          \"label\": null,\n          \"logBase\": 2,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"decimals\": 1,\n          \"format\": \"m\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": false\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"description\": \"How long the health check takes to complete at the 99th percentile.\\nHigher numbers signify problems\",\n      \"fill\": 1,\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 7,\n        \"x\": 0,\n        \"y\": 7\n      },\n      \"id\": 18,\n      \"interval\": \"1m\",\n      \"legend\": {\n        \"avg\": false,\n        \"current\": false,\n        \"hideEmpty\": true,\n        \"hideZero\": true,\n        \"max\": false,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": false\n      },\n      \"lines\": true,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"nullPointMode\": \"null as zero\",\n      \"options\": {},\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"sum(jenkins_health_check_duration{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\", quantile=\\\"0.99\\\"}) \\n    by (app_kubernetes_io_instance)\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"Health Check Duration (99%)\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"description\": \"The 99% percentile of HTTP Requests handled by Jenkins masters.\",\n      \"fill\": 1,\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 8,\n        \"x\": 7,\n        \"y\": 7\n      },\n      \"id\": 52,\n      \"legend\": {\n        \"avg\": false,\n        \"current\": false,\n        \"max\": false,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": false\n      },\n      \"lines\": true,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"options\": {},\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"sum(http_requests{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\",quantile=\\\"0.99\\\"} ) by (app_kubernetes_io_instance)\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"HTTP Request Duration (99%)\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"description\": \"Shows performance of Ingress Controller connection that lasts longer than 250 milliseconds\",\n      \"fill\": 1,\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 8,\n        \"x\": 15,\n        \"y\": 7\n      },\n      \"id\": 32,\n      \"legend\": {\n        \"avg\": false,\n        \"current\": false,\n        \"max\": false,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": false\n      },\n      \"lines\": true,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"options\": {},\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"sum(rate(\\n  nginx_ingress_controller_request_duration_seconds_bucket{\\n    le=\\\"0.25\\\",\\n    namespace=~\\\"$namespace\\\",\\n   ingress=~\\\"jenkins*\\\"\\n  }[5m]\\n)) \\nby (ingress) / \\nsum(rate(\\n  nginx_ingress_controller_request_duration_seconds_count{\\n   namespace=~\\\"$namespace\\\",\\n   ingress=~\\\"jenkins*\\\"\\n  }[5m]\\n)) \\nby (ingress) \",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{ingress}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [\n        {\n          \"colorMode\": \"custom\",\n          \"fill\": false,\n          \"fillColor\": \"rgba(50, 116, 217, 0.2)\",\n          \"line\": true,\n          \"lineColor\": \"#B877D9\",\n          \"op\": \"gt\",\n          \"value\": 0.5,\n          \"yaxis\": \"left\"\n        },\n        {\n          \"colorMode\": \"warning\",\n          \"fill\": false,\n          \"fillColor\": \"rgba(50, 116, 217, 0.2)\",\n          \"line\": true,\n          \"lineColor\": \"rgba(31, 96, 196, 0.6)\",\n          \"op\": \"gt\",\n          \"value\": 1.5,\n          \"yaxis\": \"left\"\n        },\n        {\n          \"colorMode\": \"critical\",\n          \"fill\": false,\n          \"line\": true,\n          \"op\": \"gt\",\n          \"value\": 3,\n          \"yaxis\": \"left\"\n        }\n      ],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"Ingress Perfomance\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"collapsed\": false,\n      \"gridPos\": {\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 13\n      },\n      \"id\": 12,\n      \"panels\": [],\n      \"title\": \"General Info\",\n      \"type\": \"row\"\n    },\n    {\n      \"cacheTimeout\": null,\n      \"colorBackground\": true,\n      \"colorPostfix\": false,\n      \"colorValue\": false,\n      \"colors\": [\n        \"#F2495C\",\n        \"#FFCB7D\",\n        \"#5794F2\"\n      ],\n      \"description\": \"Amount of Masters healthy\",\n      \"format\": \"none\",\n      \"gauge\": {\n        \"maxValue\": 100,\n        \"minValue\": 0,\n        \"show\": false,\n        \"thresholdLabels\": false,\n        \"thresholdMarkers\": true\n      },\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 4,\n        \"x\": 0,\n        \"y\": 14\n      },\n      \"id\": 26,\n      \"interval\": null,\n      \"links\": [],\n      \"mappingType\": 1,\n      \"mappingTypes\": [\n        {\n          \"name\": \"value to text\",\n          \"value\": 1\n        },\n        {\n          \"name\": \"range to text\",\n          \"value\": 2\n        }\n      ],\n      \"maxDataPoints\": 100,\n      \"nullPointMode\": \"connected\",\n      \"nullText\": null,\n      \"options\": {},\n      \"pluginVersion\": \"6.2.4\",\n      \"postfix\": \"\",\n      \"postfixFontSize\": \"50%\",\n      \"prefix\": \"\",\n      \"prefixFontSize\": \"50%\",\n      \"rangeMaps\": [\n        {\n          \"from\": \"null\",\n          \"text\": \"N/A\",\n          \"to\": \"null\"\n        }\n      ],\n      \"sparkline\": {\n        \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n        \"full\": false,\n        \"lineColor\": \"rgb(31, 120, 193)\",\n        \"show\": false\n      },\n      \"tableColumn\": \"\",\n      \"targets\": [\n        {\n          \"expr\": \"sum(jenkins_health_check_score{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": \"0,1\",\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Number of Masters\",\n      \"type\": \"singlestat\",\n      \"valueFontSize\": \"200%\",\n      \"valueMaps\": [\n        {\n          \"op\": \"=\",\n          \"text\": \"N/A\",\n          \"value\": \"null\"\n        }\n      ],\n      \"valueName\": \"current\"\n    },\n    {\n      \"cacheTimeout\": null,\n      \"columns\": [\n        {\n          \"text\": \"Avg\",\n          \"value\": \"avg\"\n        }\n      ],\n      \"description\": \"Dropwizard based Health Score derived from other metrics\",\n      \"fontSize\": \"100%\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 5,\n        \"x\": 4,\n        \"y\": 14\n      },\n      \"id\": 40,\n      \"links\": [],\n      \"options\": {},\n      \"pageSize\": 5,\n      \"pluginVersion\": \"6.2.4\",\n      \"scroll\": true,\n      \"showHeader\": true,\n      \"sort\": {\n        \"col\": 0,\n        \"desc\": true\n      },\n      \"styles\": [\n        {\n          \"alias\": \"Score\",\n          \"colorMode\": \"row\",\n          \"colors\": [\n            \"#5794F2\",\n            \"#FF9830\",\n            \"#F2495C\"\n          ],\n          \"decimals\": 0,\n          \"pattern\": \"/.*/\",\n          \"thresholds\": [\n            \"90\",\n            \"95\"\n          ],\n          \"type\": \"number\",\n          \"unit\": \"percentunit\"\n        }\n      ],\n      \"targets\": [\n        {\n          \"expr\": \"jenkins_health_check_score{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\",\n          \"format\": \"time_series\",\n          \"instant\": true,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Health Score\",\n      \"transform\": \"timeseries_aggregations\",\n      \"type\": \"table\"\n    },\n    {\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 14,\n        \"x\": 9,\n        \"y\": 14\n      },\n      \"id\": 46,\n      \"links\": [],\n      \"options\": {\n        \"fieldOptions\": {\n          \"calcs\": [\n            \"last\"\n          ],\n          \"defaults\": {\n            \"decimals\": 1,\n            \"max\": 100,\n            \"min\": 0,\n            \"title\": \"\",\n            \"unit\": \"percent\"\n          },\n          \"mappings\": [],\n          \"override\": {},\n          \"thresholds\": [\n            {\n              \"color\": \"red\",\n              \"index\": 0,\n              \"value\": null\n            },\n            {\n              \"color\": \"purple\",\n              \"index\": 1,\n              \"value\": 50\n            },\n            {\n              \"color\": \"blue\",\n              \"index\": 2,\n              \"value\": 75\n            }\n          ],\n          \"values\": false\n        },\n        \"orientation\": \"auto\",\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"pluginVersion\": \"6.2.4\",\n      \"targets\": [\n        {\n          \"expr\": \"sum(jenkins_runs_success_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})\\nby (app_kubernetes_io_instance) /\\nsum(jenkins_runs_total_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})\\nby (app_kubernetes_io_instance) * 100\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Job Success Ratio\",\n      \"type\": \"gauge\"\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"decimals\": 0,\n      \"description\": \"Amount of Jobs Currenty in the Queue\",\n      \"fill\": 1,\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 7,\n        \"x\": 0,\n        \"y\": 20\n      },\n      \"id\": 30,\n      \"legend\": {\n        \"alignAsTable\": false,\n        \"avg\": false,\n        \"current\": false,\n        \"hideEmpty\": false,\n        \"hideZero\": true,\n        \"max\": false,\n        \"min\": false,\n        \"rightSide\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": false\n      },\n      \"lines\": true,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"options\": {},\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"sum(jenkins_queue_size_value{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [\n        {\n          \"colorMode\": \"critical\",\n          \"fill\": false,\n          \"line\": true,\n          \"op\": \"gt\",\n          \"value\": 10,\n          \"yaxis\": \"left\"\n        },\n        {\n          \"colorMode\": \"warning\",\n          \"fill\": false,\n          \"line\": true,\n          \"op\": \"gt\",\n          \"value\": 5,\n          \"yaxis\": \"left\"\n        }\n      ],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"Job Queue\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"cacheTimeout\": null,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"decimals\": 0,\n      \"fill\": 1,\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 8,\n        \"x\": 7,\n        \"y\": 20\n      },\n      \"hideTimeOverride\": false,\n      \"id\": 50,\n      \"interval\": \"\",\n      \"legend\": {\n        \"alignAsTable\": true,\n        \"avg\": false,\n        \"current\": false,\n        \"hideEmpty\": true,\n        \"hideZero\": true,\n        \"max\": true,\n        \"min\": false,\n        \"rightSide\": true,\n        \"show\": true,\n        \"total\": false,\n        \"values\": true\n      },\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"options\": {},\n      \"percentage\": false,\n      \"pluginVersion\": \"6.2.4\",\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"sum(increase(jenkins_runs_total_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}[24h])) by (app_kubernetes_io_instance)\",\n          \"format\": \"time_series\",\n          \"interval\": \"\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}-total\",\n          \"refId\": \"A\"\n        },\n        {\n          \"expr\": \"sum(increase(jenkins_runs_failure_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}[24h])) by (app_kubernetes_io_instance)\",\n          \"format\": \"time_series\",\n          \"interval\": \"\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}-failed\",\n          \"refId\": \"B\"\n        },\n        {\n          \"expr\": \"sum(increase(jenkins_runs_aborted_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}[24h])) by (app_kubernetes_io_instance)\",\n          \"format\": \"time_series\",\n          \"interval\": \"\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}-aborted\",\n          \"refId\": \"C\"\n        }\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": \"12h\",\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"Runs Per Day\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"decimals\": 0,\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"decimals\": 0,\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"description\": \"Active Build Runs\",\n      \"fill\": 1,\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 8,\n        \"x\": 15,\n        \"y\": 20\n      },\n      \"id\": 56,\n      \"legend\": {\n        \"avg\": false,\n        \"current\": false,\n        \"max\": false,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": false\n      },\n      \"lines\": true,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"options\": {},\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"sum(jenkins_executor_in_use_history{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"Build Runs\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"cards\": {\n        \"cardPadding\": null,\n        \"cardRound\": null\n      },\n      \"color\": {\n        \"cardColor\": \"#F2495C\",\n        \"colorScale\": \"sqrt\",\n        \"colorScheme\": \"interpolateSpectral\",\n        \"exponent\": 0.8,\n        \"max\": null,\n        \"min\": 0,\n        \"mode\": \"spectrum\"\n      },\n      \"dataFormat\": \"tsbuckets\",\n      \"description\": \"Heatmap of when Jobs are scheduled\",\n      \"gridPos\": {\n        \"h\": 7,\n        \"w\": 23,\n        \"x\": 0,\n        \"y\": 26\n      },\n      \"heatmap\": {},\n      \"hideTimeOverride\": false,\n      \"hideZeroBuckets\": false,\n      \"highlightCards\": true,\n      \"id\": 58,\n      \"interval\": \"5m\",\n      \"legend\": {\n        \"show\": true\n      },\n      \"links\": [],\n      \"options\": {},\n      \"reverseYBuckets\": false,\n      \"targets\": [\n        {\n          \"expr\": \"sum(increase(jenkins_runs_total_total{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}[2h])) by (app_kubernetes_io_instance)\",\n          \"format\": \"heatmap\",\n          \"instant\": false,\n          \"interval\": \"\",\n          \"intervalFactor\": 10,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"timeFrom\": \"168h\",\n      \"timeShift\": null,\n      \"title\": \"Job Runs Heatmap\",\n      \"tooltip\": {\n        \"show\": true,\n        \"showHistogram\": false\n      },\n      \"type\": \"heatmap\",\n      \"xAxis\": {\n        \"show\": true\n      },\n      \"xBucketNumber\": null,\n      \"xBucketSize\": null,\n      \"yAxis\": {\n        \"decimals\": 0,\n        \"format\": \"short\",\n        \"logBase\": 1,\n        \"max\": null,\n        \"min\": \"0\",\n        \"show\": true,\n        \"splitFactor\": null\n      },\n      \"yBucketBound\": \"auto\",\n      \"yBucketNumber\": null,\n      \"yBucketSize\": null\n    },\n    {\n      \"columns\": [\n        {\n          \"text\": \"Current\",\n          \"value\": \"current\"\n        }\n      ],\n      \"description\": \"Jenkins Master Plugin Count\",\n      \"fontSize\": \"100%\",\n      \"gridPos\": {\n        \"h\": 5,\n        \"w\": 7,\n        \"x\": 0,\n        \"y\": 33\n      },\n      \"id\": 14,\n      \"interval\": \"1m\",\n      \"links\": [],\n      \"options\": {},\n      \"pageSize\": null,\n      \"pluginVersion\": \"6.2.4\",\n      \"scroll\": true,\n      \"showHeader\": true,\n      \"sort\": {\n        \"col\": 0,\n        \"desc\": true\n      },\n      \"styles\": [\n        {\n          \"alias\": \"Time\",\n          \"dateFormat\": \"YYYY-MM-DD HH:mm:ss\",\n          \"pattern\": \"Time\",\n          \"type\": \"date\"\n        },\n        {\n          \"alias\": \"Master\",\n          \"colorMode\": \"row\",\n          \"colors\": [\n            \"#5794F2\",\n            \"#FF9830\",\n            \"#F2495C\"\n          ],\n          \"decimals\": 0,\n          \"link\": false,\n          \"pattern\": \"/.*/\",\n          \"thresholds\": [\n            \"95\",\n            \"130\"\n          ],\n          \"type\": \"number\",\n          \"unit\": \"short\"\n        }\n      ],\n      \"targets\": [\n        {\n          \"expr\": \"jenkins_plugins_active{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\",\n          \"format\": \"time_series\",\n          \"instant\": true,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Plugin Count\",\n      \"transform\": \"timeseries_to_rows\",\n      \"type\": \"table\"\n    },\n    {\n      \"columns\": [\n        {\n          \"text\": \"Current\",\n          \"value\": \"current\"\n        }\n      ],\n      \"description\": \"Amount of plugins that are available for updating\",\n      \"fontSize\": \"100%\",\n      \"gridPos\": {\n        \"h\": 5,\n        \"w\": 8,\n        \"x\": 7,\n        \"y\": 33\n      },\n      \"id\": 22,\n      \"links\": [],\n      \"options\": {},\n      \"pageSize\": null,\n      \"scroll\": true,\n      \"showHeader\": true,\n      \"sort\": {\n        \"col\": 0,\n        \"desc\": true\n      },\n      \"styles\": [\n        {\n          \"alias\": \"Time\",\n          \"dateFormat\": \"YYYY-MM-DD HH:mm:ss\",\n          \"pattern\": \"Time\",\n          \"type\": \"date\"\n        },\n        {\n          \"alias\": \"\",\n          \"colorMode\": \"row\",\n          \"colors\": [\n            \"#5794F2\",\n            \"#FF9830\",\n            \"#F2495C\"\n          ],\n          \"decimals\": 0,\n          \"pattern\": \"/.*/\",\n          \"thresholds\": [\n            \"3\",\n            \"10\"\n          ],\n          \"type\": \"number\",\n          \"unit\": \"short\"\n        }\n      ],\n      \"targets\": [\n        {\n          \"expr\": \"sum(jenkins_plugins_withUpdate{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\",\n          \"format\": \"time_series\",\n          \"instant\": true,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Plugin Updates Available\",\n      \"transform\": \"timeseries_aggregations\",\n      \"type\": \"table\"\n    },\n    {\n      \"columns\": [\n        {\n          \"text\": \"Current\",\n          \"value\": \"current\"\n        },\n        {\n          \"text\": \"Max\",\n          \"value\": \"max\"\n        }\n      ],\n      \"description\": \"Jenkins Master Job Count\",\n      \"fontSize\": \"100%\",\n      \"gridPos\": {\n        \"h\": 5,\n        \"w\": 8,\n        \"x\": 15,\n        \"y\": 33\n      },\n      \"id\": 16,\n      \"links\": [],\n      \"options\": {},\n      \"pageSize\": null,\n      \"scroll\": true,\n      \"showHeader\": true,\n      \"sort\": {\n        \"col\": 0,\n        \"desc\": true\n      },\n      \"styles\": [\n        {\n          \"alias\": \"Time\",\n          \"dateFormat\": \"YYYY-MM-DD HH:mm:ss\",\n          \"pattern\": \"Time\",\n          \"type\": \"date\"\n        },\n        {\n          \"alias\": \"\",\n          \"colorMode\": \"row\",\n          \"colors\": [\n            \"#F2495C\",\n            \"#FF9830\",\n            \"#5794F2\"\n          ],\n          \"decimals\": 0,\n          \"pattern\": \"/.*/\",\n          \"thresholds\": [\n            \"1\"\n          ],\n          \"type\": \"number\",\n          \"unit\": \"short\"\n        }\n      ],\n      \"targets\": [\n        {\n          \"expr\": \"jenkins_job_count_value{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\",\n          \"format\": \"time_series\",\n          \"instant\": true,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Job Count\",\n      \"transform\": \"timeseries_aggregations\",\n      \"type\": \"table\"\n    },\n    {\n      \"columns\": [\n        {\n          \"text\": \"Current\",\n          \"value\": \"current\"\n        }\n      ],\n      \"description\": \"Counts offline build nodes that are connected to this master\",\n      \"fontSize\": \"100%\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 7,\n        \"x\": 0,\n        \"y\": 38\n      },\n      \"id\": 24,\n      \"links\": [],\n      \"options\": {},\n      \"pageSize\": null,\n      \"scroll\": true,\n      \"showHeader\": true,\n      \"sort\": {\n        \"col\": 0,\n        \"desc\": true\n      },\n      \"styles\": [\n        {\n          \"alias\": \"Time\",\n          \"dateFormat\": \"YYYY-MM-DD HH:mm:ss\",\n          \"pattern\": \"Time\",\n          \"type\": \"date\"\n        },\n        {\n          \"alias\": \"\",\n          \"colorMode\": \"row\",\n          \"colors\": [\n            \"#5794F2\",\n            \"#B877D9\",\n            \"#F2495C\"\n          ],\n          \"decimals\": 2,\n          \"pattern\": \"/.*/\",\n          \"thresholds\": [\n            \"1\",\n            \"3\"\n          ],\n          \"type\": \"number\",\n          \"unit\": \"short\"\n        }\n      ],\n      \"targets\": [\n        {\n          \"expr\": \"sum(jenkins_node_offline_value{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\",\n          \"format\": \"time_series\",\n          \"instant\": true,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Offline Nodes\",\n      \"transform\": \"timeseries_aggregations\",\n      \"type\": \"table\"\n    },\n    {\n      \"cacheTimeout\": null,\n      \"columns\": [\n        {\n          \"text\": \"Current\",\n          \"value\": \"current\"\n        }\n      ],\n      \"description\": \"Uptime in hours\",\n      \"fontSize\": \"100%\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 8,\n        \"x\": 7,\n        \"y\": 38\n      },\n      \"id\": 6,\n      \"links\": [],\n      \"options\": {},\n      \"pageSize\": null,\n      \"pluginVersion\": \"6.2.4\",\n      \"scroll\": true,\n      \"showHeader\": true,\n      \"sort\": {\n        \"col\": null,\n        \"desc\": false\n      },\n      \"styles\": [\n        {\n          \"alias\": \"Time\",\n          \"dateFormat\": \"YYYY-MM-DD HH:mm:ss\",\n          \"pattern\": \"Time\",\n          \"type\": \"date\"\n        },\n        {\n          \"alias\": \"\",\n          \"colorMode\": \"row\",\n          \"colors\": [\n            \"#F2495C\",\n            \"#FF9830\",\n            \"#5794F2\"\n          ],\n          \"decimals\": 0,\n          \"pattern\": \"/.*/\",\n          \"thresholds\": [\n            \"1\",\n            \"24\"\n          ],\n          \"type\": \"number\",\n          \"unit\": \"short\"\n        }\n      ],\n      \"targets\": [\n        {\n          \"expr\": \"vm_uptime_milliseconds{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"} / 3600000 \",\n          \"format\": \"time_series\",\n          \"instant\": true,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Uptime\",\n      \"transform\": \"timeseries_aggregations\",\n      \"type\": \"table\"\n    },\n    {\n      \"columns\": [\n        {\n          \"text\": \"Current\",\n          \"value\": \"current\"\n        }\n      ],\n      \"description\": \"The current Master executor count, masters should not have executors, so only 0 is green.\",\n      \"fontSize\": \"100%\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 8,\n        \"x\": 15,\n        \"y\": 38\n      },\n      \"id\": 34,\n      \"interval\": \"\",\n      \"links\": [],\n      \"options\": {},\n      \"pageSize\": null,\n      \"scroll\": true,\n      \"showHeader\": true,\n      \"sort\": {\n        \"col\": 0,\n        \"desc\": true\n      },\n      \"styles\": [\n        {\n          \"alias\": \"Time\",\n          \"dateFormat\": \"YYYY-MM-DD HH:mm:ss\",\n          \"pattern\": \"Time\",\n          \"type\": \"date\"\n        },\n        {\n          \"alias\": \"\",\n          \"colorMode\": \"row\",\n          \"colors\": [\n            \"#F2495C\",\n            \"#FF9830\",\n            \"#5794F2\"\n          ],\n          \"decimals\": 2,\n          \"pattern\": \"/.*/\",\n          \"thresholds\": [\n            \"0\",\n            \"0\",\n            \"1\"\n          ],\n          \"type\": \"number\",\n          \"unit\": \"short\"\n        }\n      ],\n      \"targets\": [\n        {\n          \"expr\": \"sum(jenkins_executor_count_value{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\",\n          \"format\": \"time_series\",\n          \"instant\": true,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Master Executor Count\",\n      \"transform\": \"timeseries_aggregations\",\n      \"type\": \"table\"\n    },\n    {\n      \"collapsed\": false,\n      \"gridPos\": {\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 44\n      },\n      \"id\": 2,\n      \"panels\": [],\n      \"title\": \"JVM Metrics\",\n      \"type\": \"row\"\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"fill\": 1,\n      \"gridPos\": {\n        \"h\": 7,\n        \"w\": 10,\n        \"x\": 0,\n        \"y\": 45\n      },\n      \"id\": 48,\n      \"legend\": {\n        \"avg\": false,\n        \"current\": false,\n        \"max\": false,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": false\n      },\n      \"lines\": true,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"options\": {},\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"1 - sum(vm_gc_G1_Young_Generation_time{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"})by (app_kubernetes_io_instance) \\n/ \\nsum (vm_uptime_milliseconds{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\",\n          \"format\": \"time_series\",\n          \"instant\": false,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [\n        {\n          \"colorMode\": \"warning\",\n          \"fill\": true,\n          \"line\": true,\n          \"op\": \"lt\",\n          \"value\": 0.998,\n          \"yaxis\": \"left\"\n        },\n        {\n          \"colorMode\": \"critical\",\n          \"fill\": true,\n          \"line\": true,\n          \"op\": \"lt\",\n          \"value\": 0.98,\n          \"yaxis\": \"left\"\n        }\n      ],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"JVM GC Throughput\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"decimals\": 5,\n          \"format\": \"percentunit\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": \"1\",\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": false\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"description\": \"Ratio of JVM Memory used\",\n      \"fill\": 1,\n      \"gridPos\": {\n        \"h\": 7,\n        \"w\": 13,\n        \"x\": 10,\n        \"y\": 45\n      },\n      \"id\": 10,\n      \"legend\": {\n        \"alignAsTable\": false,\n        \"avg\": false,\n        \"current\": false,\n        \"max\": false,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": false\n      },\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"options\": {},\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"sum(vm_memory_heap_usage{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}) by (app_kubernetes_io_instance)\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [\n        {\n          \"colorMode\": \"critical\",\n          \"fill\": true,\n          \"line\": true,\n          \"op\": \"gt\",\n          \"value\": 0.75,\n          \"yaxis\": \"left\"\n        }\n      ],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"Memory Ratio\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"percentunit\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": false\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"decimals\": 4,\n      \"description\": \"JVM CPU Load\",\n      \"fill\": 1,\n      \"gridPos\": {\n        \"h\": 7,\n        \"w\": 10,\n        \"x\": 0,\n        \"y\": 52\n      },\n      \"id\": 4,\n      \"legend\": {\n        \"alignAsTable\": true,\n        \"avg\": false,\n        \"current\": true,\n        \"hideEmpty\": true,\n        \"hideZero\": true,\n        \"max\": true,\n        \"min\": false,\n        \"rightSide\": true,\n        \"show\": false,\n        \"total\": false,\n        \"values\": true\n      },\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"connected\",\n      \"options\": {},\n      \"percentage\": false,\n      \"pointradius\": 1,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": true,\n      \"targets\": [\n        {\n          \"expr\": \"vm_cpu_load{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\",\n          \"format\": \"time_series\",\n          \"instant\": false,\n          \"intervalFactor\": 5,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"CPU Load\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"short\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"short\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": false\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"decimals\": null,\n      \"description\": \"JVM Memory usage\",\n      \"fill\": 2,\n      \"gridPos\": {\n        \"h\": 7,\n        \"w\": 13,\n        \"x\": 10,\n        \"y\": 52\n      },\n      \"id\": 8,\n      \"legend\": {\n        \"alignAsTable\": true,\n        \"avg\": false,\n        \"current\": true,\n        \"max\": true,\n        \"min\": false,\n        \"rightSide\": true,\n        \"show\": true,\n        \"total\": false,\n        \"values\": true\n      },\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"options\": {},\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"vm_memory_total_used{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"Memory\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 1,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"description\": \"Lists memory usage of the Pod vs. Kubernetes Requests\",\n      \"fill\": 1,\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 10,\n        \"x\": 0,\n        \"y\": 59\n      },\n      \"id\": 54,\n      \"legend\": {\n        \"avg\": false,\n        \"current\": false,\n        \"max\": false,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": false\n      },\n      \"lines\": true,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"nullPointMode\": \"connected\",\n      \"options\": {},\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"sum (label_join(container_memory_usage_bytes{\\n    container_name=\\\"jenkins\\\",\\n    namespace=~\\\"$namespace\\\"\\n  }, \\n  \\\"pod\\\", \\n  \\\",\\\", \\n  \\\"pod_name\\\"\\n)) by (pod) / \\nsum (kube_pod_container_resource_requests_memory_bytes { \\n        container=\\\"jenkins\\\",\\n        namespace=~\\\"$namespace\\\"\\n    }\\n) by (pod)\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{pod}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"Memory Usage vs. Request\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"description\": \"The ratio of open file descriptors\\nSee: https://support.cloudbees.com/hc/en-us/articles/204246140-Too-many-open-files\",\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 5,\n        \"x\": 10,\n        \"y\": 59\n      },\n      \"id\": 28,\n      \"links\": [],\n      \"options\": {\n        \"displayMode\": \"basic\",\n        \"fieldOptions\": {\n          \"calcs\": [\n            \"last\"\n          ],\n          \"defaults\": {\n            \"max\": 1,\n            \"min\": 0,\n            \"unit\": \"percentunit\"\n          },\n          \"mappings\": [],\n          \"override\": {},\n          \"thresholds\": [\n            {\n              \"color\": \"blue\",\n              \"index\": 0,\n              \"value\": null\n            },\n            {\n              \"color\": \"orange\",\n              \"index\": 1,\n              \"value\": 60\n            },\n            {\n              \"color\": \"red\",\n              \"index\": 2,\n              \"value\": 80\n            }\n          ],\n          \"values\": false\n        },\n        \"orientation\": \"horizontal\"\n      },\n      \"targets\": [\n        {\n          \"expr\": \"vm_file_descriptor_ratio{kubernetes_namespace=~\\\"$namespace\\\", app_kubernetes_io_instance=~\\\"$instance\\\"}\",\n          \"format\": \"time_series\",\n          \"instant\": true,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"{{app_kubernetes_io_instance}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"File Descriptor Ratio\",\n      \"type\": \"bargauge\"\n    }\n  ],\n  \"refresh\": \"30s\",\n  \"schemaVersion\": 18,\n  \"style\": \"dark\",\n  \"tags\": [\n    \"Jenkins\",\n    \"Prometheus\"\n  ],\n  \"templating\": {\n    \"list\": [\n      {\n        \"current\": {\n          \"text\": \"Prometheus\",\n          \"value\": \"Prometheus\"\n        },\n        \"hide\": 0,\n        \"includeAll\": false,\n        \"label\": null,\n        \"multi\": false,\n        \"name\": \"cluster\",\n        \"options\": [],\n        \"query\": \"prometheus\",\n        \"refresh\": 1,\n        \"regex\": \"\",\n        \"skipUrlSync\": false,\n        \"type\": \"datasource\"\n      },\n      {\n        \"allValue\": null,\n        \"current\": {\n          \"text\": \"All\",\n          \"value\": [\n            \"$__all\"\n          ]\n        },\n        \"datasource\": \"Prometheus\",\n        \"definition\": \"label_values(jenkins_health_check_duration, app_kubernetes_io_instance)\",\n        \"hide\": 0,\n        \"includeAll\": true,\n        \"label\": \"Master\",\n        \"multi\": true,\n        \"name\": \"instance\",\n        \"options\": [],\n        \"query\": \"label_values(jenkins_health_check_duration, app_kubernetes_io_instance)\",\n        \"refresh\": 2,\n        \"regex\": \"\",\n        \"skipUrlSync\": false,\n        \"sort\": 1,\n        \"tagValuesQuery\": \"\",\n        \"tags\": [],\n        \"tagsQuery\": \"\",\n        \"type\": \"query\",\n        \"useTags\": false\n      },\n      {\n        \"allValue\": null,\n        \"current\": {\n          \"text\": \"All\",\n          \"value\": \"$__all\"\n        },\n        \"datasource\": \"Prometheus\",\n        \"definition\": \"label_values(jenkins_health_check_duration, kubernetes_namespace)\",\n        \"hide\": 0,\n        \"includeAll\": true,\n        \"label\": \"Namespace\",\n        \"multi\": true,\n        \"name\": \"namespace\",\n        \"options\": [],\n        \"query\": \"label_values(jenkins_health_check_duration, kubernetes_namespace)\",\n        \"refresh\": 2,\n        \"regex\": \"\",\n        \"skipUrlSync\": false,\n        \"sort\": 1,\n        \"tagValuesQuery\": \"\",\n        \"tags\": [],\n        \"tagsQuery\": \"\",\n        \"type\": \"query\",\n        \"useTags\": false\n      },\n      {\n        \"allValue\": null,\n        \"current\": {\n          \"text\": \"All\",\n          \"value\": \"$__all\"\n        },\n        \"datasource\": \"Prometheus\",\n        \"definition\": \"label_values(kube_node_info{component=\\\"kube-state-metrics\\\"}, node)\",\n        \"hide\": 0,\n        \"includeAll\": true,\n        \"label\": \"K8S Node\",\n        \"multi\": true,\n        \"name\": \"node\",\n        \"options\": [],\n        \"query\": \"label_values(kube_node_info{component=\\\"kube-state-metrics\\\"}, node)\",\n        \"refresh\": 1,\n        \"regex\": \"\",\n        \"skipUrlSync\": false,\n        \"sort\": 5,\n        \"tagValuesQuery\": \"\",\n        \"tags\": [],\n        \"tagsQuery\": \"\",\n        \"type\": \"query\",\n        \"useTags\": false\n      },\n      {\n        \"allValue\": null,\n        \"current\": {\n          \"text\": \"jx-production\",\n          \"value\": \"jx-production\"\n        },\n        \"datasource\": \"Prometheus\",\n        \"definition\": \"label_values(jenkins_health_check_duration, kubernetes_namespace)\",\n        \"hide\": 0,\n        \"includeAll\": false,\n        \"label\": null,\n        \"multi\": false,\n        \"name\": \"Test\",\n        \"options\": [],\n        \"query\": \"label_values(jenkins_health_check_duration, kubernetes_namespace)\",\n        \"refresh\": 2,\n        \"regex\": \"\",\n        \"skipUrlSync\": false,\n        \"sort\": 1,\n        \"tagValuesQuery\": \"\",\n        \"tags\": [],\n        \"tagsQuery\": \"\",\n        \"type\": \"query\",\n        \"useTags\": false\n      }\n    ]\n  },\n  \"time\": {\n    \"from\": \"now-6h\",\n    \"to\": \"now\"\n  },\n  \"timepicker\": {\n    \"refresh_intervals\": [\n      \"5s\",\n      \"10s\",\n      \"30s\",\n      \"1m\",\n      \"5m\",\n      \"15m\",\n      \"30m\",\n      \"1h\",\n      \"2h\",\n      \"1d\"\n    ],\n    \"time_options\": [\n      \"5m\",\n      \"15m\",\n      \"1h\",\n      \"6h\",\n      \"12h\",\n      \"24h\",\n      \"2d\",\n      \"7d\",\n      \"30d\"\n    ]\n  },\n  \"timezone\": \"\",\n  \"title\": \"Jenkins Masters\",\n  \"uid\": \"8Z9-POHWz\",\n  \"version\": 9\n}\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/install/","title":"Install Components for Monitoring","text":"<p>This chapter is about installing all the tools we need for completing this guide. If you already have these tools installed, feel free to skip the actual installations. However, do make sure to confirm you have a compatible configuration.</p> <p>Important</p> <p>This guide is written during August/September 2019, during which Helm 3 entered Beta. This guide assumes Helm 2, be mindful of the Helm version you are running!</p>"},{"location":"blogs/monitor-jenkins-on-k8s/install/#prepare","title":"Prepare","text":"<p>First, we make sure we have hostnames for our services, including Prometheus, Alertmanager, and Grafana.</p> <pre><code>export DOMAIN=\n</code></pre> <pre><code>export PROM_ADDR=mon.${DOMAIN}\nexport AM_ADDR=alertmanager.${DOMAIN}\nexport GRAFANA_ADDR=\"grafana.${DOMAIN}\"\n</code></pre> <p>Then we create a namespace to host the monitoring tools.</p> <pre><code>kubectl create namespace mon\nkubens mon\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/install/#install-prometheus-alertmanager","title":"Install Prometheus &amp; Alertmanager","text":"<p>By default, the Helm chart of Prometheus installs Alertmanager as well. To access the UI of Alertmanager, we also set its Ingress' hostname.</p> <pre><code>helm upgrade -i prometheus \\\n stable/prometheus \\\n --namespace mon \\\n --version 7.1.3 \\\n --set server.ingress.hosts={$PROM_ADDR} \\\n --set alertmanager.ingress.hosts={$AM_ADDR} \\\n -f prom-values.yaml\n</code></pre> <p>Use the below command to wait for the deployment of Prometheus to be completed.</p> <pre><code>kubectl -n mon \\\n rollout status \\\n deploy prometheus-server\n</code></pre> prom-values.yaml <p>Below is an example helm <code>values.yaml</code> for Prometheus. It shows how to set resources limits and request, some alerts, and how to configure sending these alerts to Slack.</p> <pre><code>server:\n    ingress:\n    enabled: true\n    annotations:\n        ingress.kubernetes.io/ssl-redirect: \"false\"\n        nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n    resources:\n    limits:\n        cpu: 100m\n        memory: 1000Mi\n    requests:\n        cpu: 10m\n        memory: 500Mi\nalertmanager:\n    ingress:\n    enabled: true\n    annotations:\n        ingress.kubernetes.io/ssl-redirect: \"false\"\n        nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n    resources:\n    limits:\n        cpu: 10m\n        memory: 20Mi\n    requests:\n        cpu: 5m\n        memory: 10Mi\nkubeStateMetrics:\n    resources:\n    limits:\n        cpu: 10m\n        memory: 50Mi\n    requests:\n        cpu: 5m\n        memory: 25Mi\nnodeExporter:\n    resources:\n    limits:\n        cpu: 10m\n        memory: 20Mi\n    requests:\n        cpu: 5m\n        memory: 10Mi\npushgateway:\n    resources:\n    limits:\n        cpu: 10m\n        memory: 20Mi\n    requests:\n        cpu: 5m\n        memory: 10Mi\nserverFiles:\n    alerts:\n    groups:\n    - name: nodes\n        rules:\n        - alert: JenkinsToManyJobsQueued\n        expr: sum(jenkins_queue_size_value) &gt; 5\n        for: 3m\n        labels:\n            severity: notify\n        annotations:\n            summary: Jenkins to many jobs queued\n            description: A Jenkins instance is failing a health check\nalertmanagerFiles:\n    alertmanager.yml:\n    global: {}\n    route:\n        group_wait: 10s\n        group_interval: 5m\n        receiver: slack\n        repeat_interval: 3h\n        routes:\n        - receiver: slack\n        repeat_interval: 5d\n        match:\n            severity: notify\n            frequency: low\n    receivers:\n    - name: slack\n        slack_configs:\n        - api_url: \"XXXXXXXXXX\"\n        send_resolved: true\n        title: \"{{ .CommonAnnotations.summary }}\"\n        text: \"{{ .CommonAnnotations.description }}\"\n        title_link: http://example.com\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/install/#install-grafana","title":"Install Grafana","text":"<p>!!!    note     At the time of writing (September 2019) we cannot use the latest version of the Grafana helm chart.</p> <pre><code>* https://github.com/helm/charts/pull/15702\n* https://github.com/helm/charts/issues/15725\n</code></pre> <p>We install Grafana in the same namespace as Prometheus and Alertmanager.</p> <pre><code>helm upgrade -i grafana stable/grafana \\\n --version 3.5.5 \\\n --namespace mon \\\n --set ingress.hosts=\"{$GRAFANA_ADDR}\" \\\n --values grafana-values.yaml\n</code></pre> <pre><code>kubectl -n mon rollout status deployment grafana\n</code></pre> <p>Once the deployment is rolled out, we can either directly open the Grafana UI or echo the address and copy &amp; paste it.</p> <pre><code>echo \"http://$GRAFANA_ADDR\"\n</code></pre> <pre><code>open \"http://$GRAFANA_ADDR\"\n</code></pre> <p>By default, the Grafana helm chart generates a password for you, with the command below you can retrieve it.</p> <pre><code>kubectl -n mon \\\n get secret grafana \\\n -o jsonpath=\"{.data.admin-password}\" \\\n | base64 --decode; echo\n</code></pre> <pre><code>open \"https://grafana.com/dashboards\"\n</code></pre> grafana-values.yaml <p>Below is an example configuration for a helm <code>values.yaml</code>, which also includes some useful dashboards by default.  We've also configured a default Datasource, pointing to the Prometheus installed earlier.</p> <pre><code>ingress:\n  enabled: true\npersistence:\n  enabled: true\n  accessModes:\n  - ReadWriteOnce\n  size: 1Gi\nresources:\n  limits:\n    cpu: 20m\n    memory: 50Mi\n  requests:\n    cpu: 5m\n    memory: 25Mi\ndatasources:\n  datasources.yaml:\n    apiVersion: 1\n    datasources:\n    - name: Prometheus\n      type: prometheus\n      url: http://prometheus-server\n      access: proxy\n      isDefault: true\ndashboardProviders:\n  dashboardproviders.yaml:\n    apiVersion: 1\n    providers:\n    - name: 'Default'\n      orgId: 1\n      folder: 'default'\n      type: file\n      disableDeletion: true\n      editable: true\n      options:\n        path: /var/lib/grafana/dashboards/default\ndashboards:\n  default:\n    Costs-Pod:\n      gnetId: 6879\n      revision: 1\n      datasource: Prometheus\n    Costs:\n      gnetId: 8670\n      revision: 1\n      datasource: Prometheus\n    Summary:\n      gnetId: 8685\n      revision: 1\n      datasource: Prometheus\n    Capacity:\n      gnetId: 5228\n      revision: 6\n      datasource: Prometheus\n    Deployments:\n      gnetId: 8588\n      revision: 1\n      datasource: Prometheus\n    Volumes:\n      gnetId: 6739\n      revision: 1\n      datasource: Prometheus\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/install/#install-jenkins","title":"Install Jenkins","text":"<p>Now that we've taken care of the monitoring tools, we can install Jenkins. We start by creating a namespace for Jenkins to land in.</p> <pre><code>kubectl create namespace jenkins\nkubens jenkins\n</code></pre> <p>There are many ways of installing Jenkins. There is a very well maintained Helm chart, which is well suited for what we want to achieve.</p> <p>!!!  note     It is recommended to spread teams and applications across Jenkins masters rather than put everything into a single instance. So in this guide we create two identical Jenkins Masters, each with a unique hostname, to simulate this and show that the alerts and dashboards work for one or more Jenkins masters.</p> <p>Although the Helm chart is a very good starting point, we still need a <code>values.yaml</code> file to configure a few things.</p>"},{"location":"blogs/monitor-jenkins-on-k8s/install/#helm-values-explained","title":"Helm Values Explained","text":"<p>Let's explain some of the values:</p> <ul> <li><code>installPlugins</code>: we want <code>blueocean</code> for a more beautiful Pipeline UI and <code>prometheus</code> to expose the metrics in a Prometheus format</li> <li><code>resources</code>: always specify your resources, if these are wrong, our monitoring alerts and dashboard should help use tweak these values</li> <li><code>javaOpts</code>: for some reason, the default configuration doesn't have the recommended JVM and Garbage Collection configuration, so we have to specify this, see CloudBees' JVM Troubleshoot Guide for more details</li> <li><code>ingress</code>: because I believe every publicly available service should only be accessible via TLS, we have to configure TLS and certmanager annotations (as we're using Certmanager to manage our certificate)</li> <li><code>podAnnotations</code>: the default metrics endpoint that Prometheus scrapes from is <code>/metrics</code>, unfortunately, the by default included Metrics Plugin exposes the metrics on that endpoint in the wrong format. This means we have to inform Prometheus how to retrieve the metrics</li> </ul> <p>Make sure both <code>jenkins-values.X.yaml</code> and <code>jenkins-certificate.X.yaml</code> are created according to the template files below. Replace the X for each master, if you want three, you'll have <code>.1.yaml</code>, <code>.2.yaml</code> and <code>.3.yaml</code> for each of the files. Replace the <code>&lt;ReplaceWithYourDNS&gt;</code> with your DNS Host name and the <code>X</code> with the appropriate number.</p> <p>For example, if your host name is <code>example.com</code>, you will have the following:</p> <pre><code> hostName: jenkins1.example.com\n tls:\n - secretName: tls-jenkins-1\n hosts:\n - jenkins1.example.com\n</code></pre> <p>Use this file as the starting point for each of the masters. I would recommend making your changes in this file first and then make two copies and update the <code>X</code> value with <code>1</code> and <code>2</code> respectively.</p> jenkins-values.X.yaml <pre><code>master:\n  serviceType: ClusterIP\n  installPlugins:\n    - blueocean:1.17.0\n    - prometheus:2.0.0\n    - kubernetes:1.17.2\n  resources:\n    requests:\n      cpu: \"250m\"\n      memory: \"1024Mi\"\n    limits:\n      cpu: \"1000m\"\n      memory: \"2048Mi\"\n  javaOpts: \"-XX:+AlwaysPreTouch -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+ParallelRefProcEnabled -XX:+DisableExplicitGC -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\"\n  ingress:\n    enabled: true\n    hostName: jenkinsX.&lt;ReplaceWithYourDNS&gt;\n    tls:\n      - secretName: tls-jenkins-X\n        hosts:\n          - jenkinsX.&lt;ReplaceWithYourDNS&gt;\n    annotations:\n      certmanager.k8s.io/cluster-issuer: \"letsencrypt-prod\"\n      kubernetes.io/ingress.class: nginx\n      kubernetes.io/tls-acme: \"false\"\n      nginx.ingress.kubernetes.io/proxy-body-size: 50m\n      nginx.ingress.kubernetes.io/proxy-request-buffering: \"off\"\n      nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n  podAnnotations:\n    prometheus.io/path: /prometheus\n    prometheus.io/port: \"8080\"\n    prometheus.io/scrape: \"true\"\nagent:\n  enabled: true\nrbac:\n  create: true\n</code></pre> <p>If you want to use TLS for Jenkins, this is an example Certificate. If you don't already have <code>certmanager</code> configured, take a look at my guide on leveraging Let's Encrypt in Kubernetes.</p> jenkins-certificate.X.yaml <pre><code>apiVersion: certmanager.k8s.io/v1alpha1\nkind: Certificate\nmetadata:\n  name: jenkinsX.&lt;ReplaceWithYourDNS&gt;\nspec:\n  secretName: tls-jenkins-X\n  dnsNames:\n  - jenkinsX.&lt;ReplaceWithYourDNS&gt;\n  acme:\n    config:\n    - http01:\n        ingressClass: nginx\n      domains:\n      - jenkinsX.&lt;ReplaceWithYourDNS&gt;\n  issuerRef:\n    name: letsencrypt-prod\n    kind: ClusterIssuer\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/install/#master-one","title":"Master One","text":"<p>Assuming you've created unique Helm values files for both Master One and Master Two, we can start with creating the first one.</p> <pre><code>helm upgrade -i jenkins \\\n stable/jenkins \\\n --namespace jenkins\\\n -f jenkins-values.1.yaml\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/install/#apply-certificate","title":"Apply Certificate","text":"<p>If you have the certificate, apply it to the cluster.</p> <pre><code>kubectl apply -f jenkins-certificate.1.yaml\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/install/#wait-for-rollout","title":"Wait for rollout","text":"<p>If you want to wait for the Jenkins deployment to be completed, use the following command.</p> <pre><code>kubectl -n jenkins rollout status deployment jenkins1\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/install/#retrieve-password","title":"Retrieve Password","text":"<p>The Jenkins Helm chart also generates a admin password for you. See the command below on how to retrieve it.</p> <pre><code>printf $(kubectl get secret --namespace jenkins jenkins1 -o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode);echo\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/install/#master-two","title":"Master Two","text":"<p>Let's create Master Two as well, same deal as before. The commands are here for convenience, so you can use the <code>[]</code> in the top right to copy and paste easily.</p> <pre><code>helm upgrade -i jenkins2 \\\n stable/jenkins \\\n --namespace jenkins\\\n -f jenkins-values.2.yaml\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/install/#wait-for-rollout_1","title":"Wait for rollout","text":"<pre><code>kubectl -n jenkins rollout status deployment jenkins2\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/install/#apply-certificate_1","title":"Apply Certificate","text":"<pre><code>kubectl apply -f jenkins-certificate.2.yaml\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/install/#retrieve-password_1","title":"Retrieve Password","text":"<pre><code>printf $(kubectl get secret --namespace jenkins jenkins2 -o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode);echo\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/introduction/","title":"Introduction","text":"<p>This is a guide on monitoring Jenkins on Kubernetes with Prometheus and Grafana. There are other solutions out there, and if you want to learn those, I suggest to look elsewhere. If you're interested in diving in Jenkins Metrics and how to make sense of them with Prometheus and Grafana, read on!</p>"},{"location":"blogs/monitor-jenkins-on-k8s/introduction/#what-we-will-do","title":"What We Will Do","text":"<p>The outline of the steps to take is below. Each has its own page, so if you feel you have</p> <ul> <li>create a Kubernetes cluster</li> <li>configure the cluster (e.g. Helm)</li> <li>install Prometheus and Grafana</li> <li>install one or more Jenkins instances</li> <li>get metrics from running Jenkins instance(s)</li> <li>have queries for understanding the state and performance of the Jenkins instance(s)</li> <li>have a dashboard to aid debugging an issue or determine new alerts</li> <li>have alerts that fire when (potential) problematic conditions occur</li> <li>get metrics from Jenkins Pipelines</li> </ul>"},{"location":"blogs/monitor-jenkins-on-k8s/introduction/#resources","title":"Resources","text":"<p>The list below is both a shout out to the resources I learned from and as a reference for you if you want to learn more.</p> <ul> <li>https://go.cloudbees.com/docs/solutions/jvm-troubleshooting/</li> <li>https://go.cloudbees.com/docs/cloudbees-documentation/devoptics-user-guide/run_insights/</li> <li>https://medium.com/@eng.mohamed.m.saeed/monitoring-jenkins-with-grafana-and-prometheus-a7e037cbb376</li> <li>https://stackoverflow.com/questions/52230653/graphite-jenkins-job-level-metrics</li> <li>https://towardsdatascience.com/jenkins-events-logs-and-metrics-7c3e8b28962b</li> <li>https://github.com/nvgoldin/jenkins-graphite</li> <li>https://www.weave.works/blog/promql-queries-for-the-rest-of-us/</li> <li>https://medium.com/quiq-blog/prometheus-relabeling-tricks-6ae62c56cbda</li> <li>https://docs.google.com/presentation/d/1gtqEfTKM3oLr1N9zjAeXtOcS1eAQS--Xz0D4hwlo_KQ/edit#slide=id.g5bbd4fcccc_10_10</li> <li>https://go.cloudbees.com/docs/cloudbees-documentation/devoptics-user-guide/security_privacy/#_verification_of_connection_to_the_devoptics_service</li> <li>https://sysdig.com/blog/golden-signals-kubernetes/</li> <li>https://stackoverflow.com/questions/47141967/how-to-use-the-selected-period-of-time-in-a-query/47173828#47173828</li> <li>https://www.robustperception.io/rate-then-sum-never-sum-then-rate</li> <li>https://www.innoq.com/en/blog/prometheus-counters/</li> <li>https://www.robustperception.io/dont-put-the-value-in-alert-labels</li> <li>https://blog.pvincent.io/2017/12/prometheus-blog-series-part-5-alerting-rules/</li> <li>https://docs.google.com/document/d/199PqyG3UsyXlwieHaqbGiWVa8eMWi8zzAn0YfcApr8Q/edit</li> <li>https://wiki.jenkins.io/display/JENKINS/Metrics+Plugin</li> <li>https://www.robustperception.io/controlling-the-instance-label</li> <li>https://www.robustperception.io/target-labels-are-for-life-not-just-for-christmas</li> <li>https://prometheus.io/docs/alerting/notifications/</li> <li>https://piotrminkowski.wordpress.com/2017/08/29/visualizing-jenkins-pipeline-results-in-grafana/</li> <li>https://medium.com/@jotak/designing-prometheus-metrics-72dcff88c2e5</li> <li>https://github.com/prometheus/pushgateway</li> <li>https://github.com/prometheus/client_golang</li> <li>https://blog.pvincent.io/2017/12/prometheus-blog-series-part-2-metric-types/</li> <li>https://prometheus.io/docs/concepts/jobs_instances/</li> </ul>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/","title":"Metrics","text":"<p>Jenkins is a Java Web application, in this case running in Kubernetes. Let's categorize the metrics we want to look at and deal with each group individually.</p> <ul> <li><code>JVM</code>: the JVM metrics are exposed, we should leverage this for particular queries and alerts</li> <li><code>Jenkins Configuration</code>: the default configuration exposes some configuration elements, a few of these have strong recommended values, such as <code>Master Executor Slots</code> (should always be 0)</li> <li><code>Jenkins Usage</code>: jobs running in Jenkins, or Jobs not running in Jenkins can also tell us about (potential) problems</li> <li><code>Web</code>: although it is not the primary function of Jenkins, web access gives hints about performance trends</li> <li><code>Pod Metrics</code>: any generic metric from a Kubernetes Pod perspective can be helpful to look at</li> </ul> <p>Kubernetes Labels</p> <p>This guide assumes you install Jenkins via the Helm chart as explained elsewhere in the guide. This means it assumes the Jenkins instances all have the label <code>app.kubernetes.io/instance</code>. In Prometheus, this becomes <code>app_kubernetes_io_instance</code>. </p> <p>If you install your applications (such as Jenkins) in other ways, either change the queries presented here accordingly or add the label.</p>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#types-of-metrics-to-evaluate","title":"Types of Metrics to evaluate","text":"<p>We are in the age where SRE, DevOps Engineer, and Platform Engineer are hyped terms. Hyped they may be, there is a good reason people are making noise about monitoring. A lot is written about which kinds of metrics to observe and which to ignore. There's enough written about this - including Viktor Farcic's excellent DevOps Toolkit 2.5 - so we skip diving into these. In case you haven't read anything about it, let's briefly look at the types of metrics.</p> <ul> <li><code>Latency</code>: response times of your application, in our case, both external access via Ingress and internal access. We can measure latency on internal access via Jenkins' own metrics, which also has percentile information (for example, p99)</li> <li><code>Errors</code>: we can take a look at network errors such as HTTP 500, which we get straight from Jenkins' webserver (Netty) and at failed jobs</li> <li><code>Traffic</code>: the number of connections to our service, in our case we have web traffic and jobs running, both we get from Jenkins</li> <li><code>Saturation</code>: how much the system is used compared to the available resources, core resources such as CPU and Memory primarily depend on your Kubernetes Nodes. However, we can take a look at the Pod's limits vs. requests and Jenkins' job wait time - which roughly translates to saturation</li> </ul>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#jvm-metrics","title":"JVM Metrics","text":"<p>We have some basic JVM metrics, such as CPU and Memory usage, and uptime. </p> <p>Uptime</p> <p>In the age of containers,  <code>uptime</code> is not a very useful or sexy metric to observe. I include it because we can use <code>uptime</code> as a proxy metric. For example, if a service never goes beyond a particular value - Prometheus records Max values - it can signify a problem elsewhere.</p> <pre><code>vm_cpu_load\n</code></pre> <pre><code>(vm_memory_total_max - vm_memory_total_used) / vm_memory_total_max * 100.0\n</code></pre> <pre><code>vm_uptime_milliseconds\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#garbage-collection","title":"Garbage Collection","text":"<p>For fine-tuning the JVM's garbage collection for Jenkins, there are two central guides from CloudBees. Which also explain the <code>JVM_OPTIONS</code> in the <code>jenkins-values.yaml</code> we used for the Helm installation.</p> <ul> <li>Guide On Preparing Jenkins For Support</li> <li>JVM Troubleshooting Guide </li> </ul> <p>The second article contains much information on how to analyze the Garbage Collection logs and metrics. To process the data thoroughly requires experts with specially designed tools. I am not such an expert, nor is this the document to guide you through this. Summarizing the two guides: measure core metrics and Garbage Collection Throughput. If you need more, consult experts.</p> <p>Garbage Collection Throughput</p> <pre><code>1 -sum(\n    rate(\n        vm_gc_G1_Young_Generation_time[5m]\n    )\n) by (app_kubernetes_io_instance) \n/ \nsum (\n    vm_uptime_milliseconds\n) by (app_kubernetes_io_instance)\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#check-for-too-many-open-files","title":"Check for too many open files","text":"<p>When looking at the CloudBees guide on tuning performance on Linux, one of the main things to look are core metrics (Memory and CPU) and Open Files. There's even an explicit guide on monitoring the number of open files.</p> <pre><code>vm_file_descriptor_ratio\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#jenkins-config-metrics","title":"Jenkins Config Metrics","text":"<p>Some of the metrics are derived from the configuration of a Jenkins Master.</p>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#plugins","title":"Plugins","text":"<p>While Jenkins' extensive community is often praised for the number of plugins created and maintained, the plugins are also a big source of risk. You probably want to set a baseline and determine a value for when to send an alert.</p> <pre><code>jenkins_plugins_active\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#jenkins-build-nodes","title":"Jenkins Build Nodes","text":"<p>Jenkins should never build on a master, always on a node or agent.</p> <pre><code>jenkins_executor_count_value\n</code></pre> <p>You might use static agents or, while we're in Kubernetes, only have dynamic agents. Either way, having nodes offline for a while signifies a problem. Maybe the Node configuration is wrong, or the PodTemplate has a mistake, or maybe your ServiceAccount doesn't have the correct permissions.</p> <pre><code>jenkins_node_offline_value\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#jenkins-usage-metrics","title":"Jenkins Usage Metrics","text":"<p>Most of Jenkins' metrics relate to its usage, though. Think about metrics regarding HTTP request duration, number server errors (HTTP 500), and all the metrics related to builds.</p>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#builds-per-day","title":"Builds Per Day","text":"<pre><code>sum(increase(jenkins_runs_total_total[24h])) by (app_kubernetes_io_instance)\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#job-duration","title":"Job duration","text":"<pre><code>default_jenkins_builds_last_build_duration_milliseconds\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#job-count","title":"Job Count","text":"<pre><code>jenkins_job_count_value\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#jobs-in-queue","title":"Jobs in Queue","text":"<p>If a Jenkins master is overloaded, it is likely to fall behind building jobs that are scheduled. Jenkins observes the duration a job spends in the queue (<code>jenkins_job_queuing_duration</code>) and the current queue size (<code>jenkins_queue_size_value</code>).</p> <pre><code>jenkins_job_queuing_duration\n</code></pre> <pre><code>sum(jenkins_queue_size_value) by (app_kubernetes_io_instance)\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#web-metrics","title":"Web Metrics","text":"<p>As Jenkins is also a web application, it makes to look at its HTTP related metrics as well. </p> <p>Route Of External Traffic</p> <p>It is important to note that the HTTP traffic of user interaction with Jenkins when running in Kubernetes can contain quite a lot of layers. Problems can arise in any of these layers, so it is crucial to monitor traffic to a service on multiple layers to speed debug time. Tracing is a great solution but out of scope for this guide.</p>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#http-requests","title":"HTTP Requests","text":"<p>The 99<sup>th</sup> percentile of HTTP Requests handled by Jenkins masters. </p> <pre><code>sum(http_requests{quantile=\"0.99\"} ) by (app_kubernetes_io_instance)\n</code></pre> <p>99<sup>th</sup> percentile</p> <p>We look at percentiles because average times are not very helpful. For more information on why this is so, please consult the Google SRE book which is free online.</p>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#health-check-duration","title":"Health Check Duration","text":"<p>How long the health check takes to complete at the 99<sup>th</sup> percentile. Higher numbers signify problems.</p> <pre><code>sum(rate(jenkins_health_check_duration{ quantile=\"0.99\"}[5m])) \n by (app_kubernetes_io_instance)\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#ingress-performance","title":"Ingress Performance","text":"<p>In this case, we look at the metrics of the Nginx Ingress Controller. If you use a different controller, rewrite the query to a sensible alternative.</p> <pre><code>sum(rate(\n nginx_ingress_controller_request_duration_seconds_bucket{\n le=\"0.25\"\n }[5m]\n)) \nby (ingress) /\nsum(rate(\n nginx_ingress_controller_request_duration_seconds_count[5m]\n)) \nby (ingress)\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#number-of-good-request-vs-request","title":"Number of Good Request vs. Request","text":"<pre><code>sum(http_responseCodes_ok_total) \n by (kubernetes_pod_name) / \nsum(http_requests_count) \n by (kubernetes_pod_name)\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#pod-metrics","title":"Pod Metrics","text":"<p>These metrics are purely related to the Kubernetes Pods. They are as such, applicable to more applications than just Jenkins.</p>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#cpu-usage","title":"CPU Usage","text":"<pre><code>sum(rate(\n container_cpu_usage_seconds_total{\n container_name=\"jenkins*\"\n }[5m]\n)) \nby (pod_name)\n</code></pre> <p>Query Filters</p> <p>In this case, we filter on those containers with name <code>jenkins*</code>, which means any container whose name has jenkins as the prefix. If you want to have more than one prefix or suffix, you can use <code>||</code>. </p> <p>So, if you would want to combine Jenkins with, let's say, <code>prometheus</code>, you will get the following.</p> <pre><code>container_name=\"jenkins*||prometheus*\"\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#oversubscription-of-pod-memory","title":"Oversubscription of Pod memory","text":"<p>While <code>requests</code> are not meant to be binding, if you think your application requests around 1GB and it is using well over 3GB, something is off. Either you are too naive and should update the <code>requests</code>, or something is wrong, and you need to take action.</p> <pre><code>sum (label_join(container_memory_usage_bytes{\n container_name=\"jenkins\"\n }, \n \"pod\", \n \",\", \n \"pod_name\"\n)) by (pod) / \nsum (kube_pod_container_resource_requests_memory_bytes { \n container=\"jenkins\"\n }\n) by (pod)\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#devoptics-metrics","title":"DevOptics Metrics","text":"<p>CloudBees made parts of its DevOptics product free. This product contains - amongst other things - a feature set called Run Insights. This is a monitoring solution where your Jenkins Master uploads its metrics to the CloudBees service, and you get a dashboard with many of the same things already discussed.</p> <p>You might not want to leverage this free service but like some of its dashboard features. I've tried to recreate some of these - in a minimal fashion.</p>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#active-runs","title":"Active Runs","text":"<p>To know how many current builds there are, we can watch the executors that are in use.</p> <pre><code>sum(jenkins_executor_in_use_history) by (app_kubernetes_io_instance)\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#idle-executors","title":"Idle Executors","text":"<p>When using Kubernetes' Pods as an agent, the only idle executors we'll have are Pods that are done with their build and in the process of being terminated. Not very useful, but in case you want to know how:</p> <pre><code>sum(jenkins_executor_free_history) by (app_kubernetes_io_instance)\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#average-time-waiting-to-start","title":"Average Time Waiting to Start","text":"<p>With Kubernetes PodTemplates we cannot calculate this. The only wait time we get is the one that is between queue'ing a job and requesting the Pod, which isn't very meaningful.</p>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#completed-runs-per-day","title":"Completed Runs Per Day","text":"<pre><code>sum(increase(jenkins_runs_total_total[24h])) by (app_kubernetes_io_instance)\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/metrics/#average-time-to-complete","title":"Average Time to complete","text":"<pre><code>sum(jenkins_job_building_duration) by (app_kubernetes_io_instance) /\n sum(jenkins_job_building_duration_count) by (app_kubernetes_io_instance)\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/","title":"Track Metrics of Pipelines","text":""},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#get-data-from-jobs","title":"Get Data From Jobs","text":"<ul> <li>Use Prometheus Push Gateway<ul> <li>via shared lib</li> </ul> </li> <li>JX sh step -&gt; tekton -&gt; write interceptor</li> </ul>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#configure-prometheus-push-gateway","title":"Configure Prometheus Push Gateway","text":"<ul> <li>Make sure it is enabled in the <code>prometheus</code> helm chart</li> </ul> <pre><code>pushgateway:\n  enabled: true\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#identification-data","title":"Identification Data","text":"<ul> <li>canonical FQN:<ul> <li>application ID</li> <li>source URI</li> </ul> </li> </ul>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#questions-to-answer","title":"Questions to answer","text":""},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#metrics-to-gather","title":"Metrics to gather","text":"<ul> <li>test coverage</li> <li>shared libraries used</li> <li>duration of stages</li> <li>duration of job</li> <li>status of job</li> <li>status of stage</li> <li>time-to-fix</li> <li>git source</li> <li>node label</li> <li>languages (github languages parser)</li> </ul>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#test-coverage","title":"Test Coverage","text":"<p>Send a <code>Gauge</code> with coverage as value.</p> <p>Potential Labels:</p> <ul> <li>Application ID</li> <li>Source URI</li> <li>Job</li> <li>Instance</li> <li>RunId</li> </ul>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#send-metric-from-jenkins-pipeline","title":"Send Metric From Jenkins Pipeline","text":""},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#bash","title":"Bash","text":""},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#simple","title":"Simple","text":"<pre><code>echo \"some_metric 3.14\" | curl --data-binary @- http://pushgateway.example.org:9091/metrics/job/some_job\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#complex","title":"Complex","text":"<pre><code>cat &lt;&lt;EOF | curl --data-binary @- http://pushgateway.example.org:9091/metrics/job/some_job/instance/some_instance\n  # TYPE some_metric counter\n  some_metric{label=\"val1\"} 42\n  # TYPE another_metric gauge\n  # HELP another_metric Just an example.\n  another_metric 2398.283\n  EOF\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#delete-by-instance","title":"Delete by instance","text":"<pre><code>curl -X DELETE http://pushgateway.example.org:9091/metrics/job/some_job/instance/some_instance\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#delete-by-prometheus-job","title":"Delete by (prometheus) job","text":"<pre><code>curl -X DELETE http://pushgateway.example.org:9091/metrics/job/some_job\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#go-lang-client","title":"Go lang client","text":""},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#queries","title":"Queries","text":""},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#total-stages-duration-in-seconds","title":"Total Stages Duration In Seconds","text":"<pre><code>sum(jenkins_pipeline_run_stages_hist_sum) by (jobName, runId) / 1000\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#coverage-metric","title":"Coverage Metric","text":"<pre><code>jenkins_pipeline_run_test_coverage\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#last-push-of-metric","title":"Last Push Of Metric","text":"<pre><code>time() - push_time_seconds\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#stage-statusses","title":"Stage Statusses","text":"<pre><code>m\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#_1","title":"??","text":"<pre><code>(sum(jenkins_pipeline_run_hist_sum) by (jobName) / 1000) / \n    sum(jenkins_pipeline_run_hist_count) by  (jobName)\n</code></pre> <pre><code>sum(jenkins_pipeline_run_hist_sum) by (jobName, runId)\n</code></pre> <pre><code>sum(jenkins_pipeline_run_hist_count) by (instance, appId)\n</code></pre> <pre><code>sum(jenkins_pipeline_run_stages_hist_sum) by (instance, jobName, runId)\n</code></pre> <pre><code>sum(jenkins_pipeline_run_hist_count  offset 3d) by (jobName) \n</code></pre> <pre><code>sum(jenkins_pipeline_run_hist_count) by (jobName) \n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#success-rate-of-stages","title":"Success Rate of Stages","text":"<pre><code> count(jenkins_pipeline_run_hist_sum{ result=\"SUCCESS\"}) by (jobName, runId) /  count(jenkins_pipeline_run_hist_sum) by (jobName, runId)\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#github-autostatus","title":"Github Autostatus","text":"<ul> <li>install influxDB<ul> <li>configure influxDB into Grafana as Datasource</li> </ul> </li> <li>install plugin in Jenkins<ul> <li>Plugin: https://plugins.jenkins.io/github-autostatus</li> <li>configure in jenkins config to us influxdb</li> </ul> </li> <li>import dashboard into Grafana<ul> <li><code>5786</code></li> <li><code>SELECT \"jobtime\", \"buildnumber\", \"passed\", \"branch\", \"buildurl\" FROM \"job\" WHERE (\"owner\" = 'joostvdg') AND $timeFilter GROUP BY \"repo\"</code></li> </ul> </li> </ul>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#things-to-look-at","title":"Things to look at","text":"<ul> <li>Scraping of Gateway means metrics are retrieved more often than they are created<ul> <li>you can reduce the error by creating a rewrite rule</li> <li>https://www.robustperception.io/aggregating-across-batch-job-runs-with-push_time_seconds</li> </ul> </li> <li>Counter does not aggregate<ul> <li>https://stackoverflow.com/questions/50923880/prometheus-intrumentation-for-distributed-accumulated-batch-jobs</li> <li>if you want aggregation, use Prometheus Aggregation Gateway from Weaveworks</li> <li>https://github.com/weaveworks/prom-aggregation-gateway</li> </ul> </li> <li></li> </ul>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#pipeline-example-curl","title":"Pipeline Example - Curl","text":"<pre><code>pipeline {\n    agent {\n        kubernetes {\n        label 'jpb-mon'\n        yaml \"\"\"\nkind: Pod\nmetadata:\n  labels:\n    build: prom-test\nspec:\n  containers:\n  - name: curl\n    image: byrnedo/alpine-curl\n    command: [\"cat\"]\n    tty: true\n  - name: golang\n    image: golang:1.9\n    command: [\"cat\"]\n    tty: true\n\"\"\"\n        }\n    }\n    environment {\n        CREDS           = credentials('api')\n        TEST_COVERAGE   = ''\n        PROM_URL        = 'http://prometheus-pushgateway.obs:9091/metrics/job/devops25'\n    }\n    stages {\n        stage('Checkout') {\n            steps {\n                git 'https://github.com/joostvdg/go-demo-5.git'\n            }\n        }\n        stage('Prepare') {\n            steps {\n                container('golang') {\n                    sh 'go get -d -v -t'\n                }\n            }\n        }\n        stage('Build &amp; Test') {\n            steps {\n                container('golang') {\n                    script {\n                        sh 'go build'\n                        def coverage = sh encoding: 'UTF-8', label: 'go test', returnStdout: true, script: 'go test --cover -v ./... --run UnitTest | grep coverage:'\n                        coverage = coverage.trim()\n                        coverage = coverage.replace('coverage: ', '')\n                        coverage = coverage.replace('% of statements', '')\n                        TEST_COVERAGE = \"${coverage}\"\n                        println \"coverage=${coverage}\"\n                    }\n                    sh 'ls -lath'\n                }\n            }\n        }\n        stage('Push Metrics') {\n            environment {\n                COVERAGE = \"${TEST_COVERAGE}\"\n            }\n            steps {\n                println \"COVERAGE=${COVERAGE}\"\n                container('curl') {\n                    sh 'echo \"TEST_COVERAGE=${COVERAGE}\"'\n                    sh 'echo \"PROM_URL=${PROM_URL}\"'\n                    sh 'echo \"BUILD_ID=${BUILD_ID}\"'\n                    sh 'echo \"JOB_NAME=${JOB_NAME}\"'\n                    sh 'echo \"jenkins_pipeline_run_test_coverage{instance=\\\"$JENKINS_URL\\\",jobName=\\\"$JOB_NAME\\\", run=\\\"$BUILD_ID\\\"} ${TEST_COVERAGE}\" | curl --data-binary @- ${PROM_URL}'\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#pipeline-example-cli","title":"Pipeline Example - CLI","text":"<p>The tool Jenkins Pipeline Binary - Monitoring will retrieve the Stages Nodes from Jenkins and translate them to Gauges in Prometheus.</p> <pre><code>pipeline {\n    agent {\n        kubernetes {\n        label 'jpb-mon'\n        yaml \"\"\"\nkind: Pod\nmetadata:\n  labels:\n    build: prom-test-4\nspec:\n  containers:\n  - name: jpb\n    image: caladreas/jpb-mon:0.23.0\n    command: ['/bin/jpb-mon', 'sleep', '--sleep', '3m']\n    tty: true\n    resources:\n      requests:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n\"\"\"\n        }\n    }\n    environment {\n        CREDS = credentials('api')\n    }\n    stages {\n        stage('Test1') {\n            steps {\n                print 'Hello World'\n            }\n        }\n        stage('Test2') {\n            environment {\n                MASTER = 'jenkins1'\n            }\n            steps {\n                sh 'echo \"Hello World!\"'\n            }\n        }\n    }\n    post {\n        always {\n            container('jpb') {\n                sh \"/bin/jpb-mon get-run --verbose --host ${JENKINS_URL} --job ${JOB_NAME} --run ${BUILD_ID} --username ${CREDS_USR} --password ${CREDS_PSW} --push\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/pipeline/#resources","title":"Resources","text":"<ul> <li>https://stackoverflow.com/questions/37009906/access-stage-results-in-workflow-pipeline-plugin</li> <li>https://github.com/jenkinsci/blueocean-plugin/tree/master/blueocean-rest#get-pipeline-run-nodes</li> <li>https://github.com/jenkinsci/pipeline-model-definition-plugin/wiki/Getting-Started</li> </ul>"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/","title":"Prepare Environment","text":"<p>This is a guide on monitoring Jenkins on Kubernetes, which makes it rather handy to have a Kubernetes cluster at hand.</p> <p>There are many ways to create a Kubernetes cluster, below is a guide on creating a cluster with Google Cloud's GKE.</p> <p>Elsewhere on this site, there are alternatives, such as Azure's AKS and AWS's EKS.</p>"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#things-to-do","title":"Things To Do","text":"<ul> <li>create a cluster</li> <li>install and configure Helm<ul> <li>for easily installing other applications on the cluster</li> </ul> </li> <li>install and configure Certmanager<ul> <li>for managing TLS certificates with Let's Encrypt</li> </ul> </li> </ul>"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#create-gke-cluster","title":"Create GKE Cluster","text":"<p>Enough talk about what we should be doing, let's create the cluster!</p>"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#prerequisites","title":"Prerequisites","text":"<ul> <li>gcloud command-line utility</li> <li>Google Cloud account that is activated</li> </ul>"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#variables","title":"Variables","text":"<p>Variables we need for the <code>gcloud</code> create cluster command. To make it easy to copy and paste the command.</p> <pre><code>K8S_VERSION=1.13.7-gke.8\nREGION=europe-west4\nCLUSTER_NAME=&lt;your cluster name&gt;\nPROJECT_ID=&lt;your google project id&gt;\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#query-available-versions","title":"Query available versions","text":"<p>If you want to see which versions are available in your Google Cloud Region, set the <code>REGION</code> variable and execute the command below.</p> <p>The list you get back will contain two lists, one for <code>worker nodes</code> and one for <code>master nodes</code>. Only the versions for <code>master nodes</code> can be used to create a cluster.</p> <pre><code>gcloud container get-server-config --region $REGION\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#create-cluster","title":"Create Cluster","text":"<pre><code>gcloud container clusters create ${CLUSTER_NAME} \\\n    --region ${REGION} \\\n    --cluster-version ${K8S_VERSION} \\\n    --num-nodes 2 --machine-type n1-standard-2 \\\n    --addons=HorizontalPodAutoscaling \\\n    --min-nodes 2 --max-nodes 3 \\\n    --enable-autoupgrade \\\n    --enable-autoscaling \\\n    --enable-network-policy \\\n    --labels=purpose=practice\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#set-clusteradmin","title":"Set ClusterAdmin","text":"<p>For some later commands, such as Helm, we need to be ClusterAdmin.</p> <pre><code>kubectl create clusterrolebinding \\\n    cluster-admin-binding \\\n    --clusterrole cluster-admin \\\n    --user $(gcloud config get-value account)\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#install-ingress-controller","title":"Install Ingress Controller","text":"<p>An ingress controller is what allows you to access the applications you install on your Kubernetes cluster from the outside. We need to do this for the tools we will use. So we need to install an ingress controller. Any will do, but <code>ingress-nginx</code> (based on the widely use <code>nginx</code> application) is the most commonly used. </p> <pre><code>kubectl apply \\\n    -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/1cd17cd12c98563407ad03812aebac46ca4442f2/deploy/mandatory.yaml\n</code></pre> <pre><code>kubectl apply \\\n    -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/1cd17cd12c98563407ad03812aebac46ca4442f2/deploy/provider/cloud-generic.yaml\n</code></pre> <p>For exposing our applications to the outside, we need to have a valid DNS name. For that, we need to have the IP address of our LoadBalancer. The command below retrieves that address. If it is empty, wait a few minutes and try again.</p> <pre><code>export LB_IP=$(kubectl -n ingress-nginx \\\n    get svc ingress-nginx \\\n    -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\n\necho $LB_IP\n</code></pre> <p>Tip</p> <p>If you don't get an address back, check to see if your ingress controller has a service and that the service has an <code>EXTERNAL</code> IP address. <pre><code>kubectl get svc -n ingress-nginx  -o wide\n</code></pre></p> <p>The response should look something like this:</p> <pre><code>NAME            TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)                      AGE\ningress-nginx   LoadBalancer   10.48.14.43   34.90.67.21   80:32762/TCP,443:31389/TCP   21d\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#install-helm","title":"Install Helm","text":"<p>Helm is a, or the, package manager for Kubernetes. We will use it to install the other applications. Read more here.</p> <pre><code>kubectl create \\\n    -f https://raw.githubusercontent.com/vfarcic/k8s-specs/master/helm/tiller-rbac.yml \\\n    --record --save-config\n</code></pre> <p>Now that we've installed Helm, we can initialize the server component via a <code>helm init</code>.</p> <pre><code>helm init --service-account tiller\n</code></pre> <p>And now we wait.</p> <pre><code>kubectl -n kube-system \\\n    rollout status deploy tiller-deploy\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#install-cert-manager","title":"Install Cert-Manager","text":"<p>Cert-manager will help users automate installing TLS Certificates. Read more about cert-manager here.</p> <p>This creates the cert-manager specific resource definitions, also call <code>CustomerResourceDefinitions</code> or ***CRD***s. </p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8/deploy/manifests/00-crds.yaml\n</code></pre> <p>Due to how cert-manager works, it is best installed into its own namespace. There's a chicken and egg problem because it needs a Root Certificate Authority (or, RootCA) to exist, but every Certificate needs to be validated against this Certificate. Which is why we add the special label.</p> <pre><code>kubectl create namespace cert-manager\nkubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true\n</code></pre> <p>Now that the CRD's and the namespace are ready, we can install cert-manager. Well, almost. The Helm <code>Chart</code> - that is how we call Helm packages - is in another Castle, eh, Helm Repository. So we first have to tell Helm where to get the Chart.</p> <pre><code>helm repo add jetstack https://charts.jetstack.io\nhelm repo update\n</code></pre> <p>We can now install cert-manager via Helm!</p> <pre><code>helm install \\\n    --name cert-manager \\\n    --namespace cert-manager \\\n    --version v0.8.0 \\\n    jetstack/cert-manager\n</code></pre>"},{"location":"blogs/monitor-jenkins-on-k8s/prepare/#configure-clusterissuer","title":"Configure ClusterIssuer","text":"<p>Cert-manager can leverage Let's Encrypt to generate valid certificates. We need to instruct cert-manager which service to use, we do that by creating a <code>ClusterIssuer</code> resource.</p> <pre><code>kubectl apply -f cluster-issuer.yaml\n</code></pre> cluster-issuer.yaml <p>Don't forget to replace <code>&lt;replacewith your email address&gt;</code> with an actual email address you can access.</p> <pre><code>apiVersion: certmanager.k8s.io/v1alpha1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    email: &lt;replacewith your email address&gt;\n    server: https://acme-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    http01: {}\n</code></pre>"},{"location":"certificates/lets-encrypt-k8s/","title":"Let's Encrypt for Kubernetes","text":"<p>Let's Encrypt is a free, automated and open Certificate Authority. The kind of service you need if you want to have a secure website with https - yes I know that requires more than that - and it's now more straightforward to use than ever.</p> <p>This about using Let's Encrypt for generating a certificate for your service on Kubernetes. There are several ways to do this, with more or less automation, cluster-wide or namespace bound or with a DNS or HTTP validation check.</p> <p>I'll choose the route that was the easiest for me, and then I'll briefly look at the other options.</p>"},{"location":"certificates/lets-encrypt-k8s/#prerequisites","title":"Prerequisites","text":"<p>There are some prerequisites required, that are best discussed on their own. So we will continue with the assumption that you have these in order.</p> <ul> <li>valid Class A or CNAME domain name</li> <li>kubernetes cluster<ul> <li>with ingress controller (such as nginx)</li> <li>with helm and tiller installed in the cluster</li> </ul> </li> <li>web application</li> </ul>"},{"location":"certificates/lets-encrypt-k8s/#steps","title":"Steps","text":"<p>The steps to take to get a web application to get a certificate from Let's Encrypt are the following.</p> <ul> <li>install cert-manager from the official helm chart</li> <li>deploy a <code>Issuer</code> resource</li> <li>deploy a certificate resource</li> <li>confirm certificate and secret are created/filled</li> <li>use in web app</li> </ul>"},{"location":"certificates/lets-encrypt-k8s/#install-cert-manager","title":"Install Cert Manager","text":"<p>For more details on Cert Manager, I recommend reading their introduction.</p> <p>In essence, it's a tool that helps you initiate a certificate request with a service such as Let's Encrypt.</p> <p>You can install it via Helm, and it's meant to be installed only once per cluster.  The once per cluster restriction is due to the usage of Custom Resource Definitions (CRD's) which will block any (re-)installation.</p> <p>To confirm if there are any CRD's from cert-manager, you can issue the following command.</p> <pre><code>kubectl get customresourcedefinitions.apiextensions.k8s.io\n</code></pre> <p>The CRD's belonging to cert-manager are the following:</p> <ul> <li>certificates.certmanager.k8s.io</li> <li>clusterissuers.certmanager.k8s.io</li> <li>issuers.certmanager.k8s.io</li> </ul> <p>You will notice later, that we will use these CRD's for getting our certificate, so keep them in mind. To remove them in case of a re-install, you can issue the command below.</p> <pre><code>kubectl delete customresourcedefinitions.apiextensions.k8s.io \\\n    certificates.certmanager.k8s.io \\\n    clusterissuers.certmanager.k8s.io \\\n    issuers.certmanager.k8s.io\n</code></pre> <p>When you're sure there are no CRD's left, you can install cert-manager via it's helm chart. It has some options in case you need them, you read about them here, but in my case that wasn't needed.</p> <pre><code>helm install --name cert-manager --namespace default stable/cert-manager\n</code></pre>"},{"location":"certificates/lets-encrypt-k8s/#deploy-issuer","title":"Deploy Issuer","text":"<p>To be able to use a certificate we need to have a Certificate Issuer.</p> <p>If you remember from our <code>cert-manager</code>, there are two CRD's that can take this role:</p> <ul> <li>ClusterIssuer: clusterissuers.certmanager.k8s.io</li> <li>Issuer: issuers.certmanager.k8s.io</li> </ul> <p>Both issuer type can use two ways of providing the proof of ownership, either by <code>dns-01</code> or <code>http-01</code>.</p> <p>We'll be using the <code>http-01</code> method, for the <code>dns-01</code> method, refer to the cert-manager documenation.</p>"},{"location":"certificates/lets-encrypt-k8s/#clusterissuer","title":"ClusterIssuer","text":"<p>As the resource <code>Kind</code> implies, a <code>ClusterIssuer</code> is a cluster-wide resource and not bound to a specific namespace.</p> <pre><code>apiVersion: certmanager.k8s.io/v1alpha1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # The ACME server URL\n    server: https://acme-staging.api.letsencrypt.org/directory\n    # Email address used for ACME registration\n    email: user@example.com\n    # Name of a secret used to store the ACME account private key\n    privateKeySecretRef:\n      name: letsencrypt-staging\n    # Enable the HTTP-01 challenge provider\n    http01: {}\n</code></pre>"},{"location":"certificates/lets-encrypt-k8s/#issuer","title":"Issuer","text":"<p>Not everyone wants a cluster-wide resource, and not everyone has the rights to install something elsewhere than their namespace.</p> <p>I prefer having as much as possible tied to a namespace - either a team or an application - I will use this type.</p> <pre><code>apiVersion: certmanager.k8s.io/v1alpha1\nkind: Issuer\nmetadata:\n  name: myapp-letsencrypt-staging\n  namespace: myapp\nspec:\n  acme:\n    # The ACME server URL\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    # Email address used for ACME registration\n    email: myadmin@myapp.com\n    # Name of a secret used to store the ACME account private key\n    privateKeySecretRef:\n      name: myapp-letsencrypt-staging\n    # Enable the HTTP-01 challenge provider\n    http01: {}\n</code></pre> <p>There's a few things to note here:</p> <ul> <li>server: this refers to the server executing the ACME test, in this case: Let's Encrypt Staging (with the v2 API)</li> <li>email: this will be the account it will use for registering the certificate</li> <li>privateKeySecretRef: this is the Kubernetes <code>secret</code> resource in which the privateKey will be stored, just in case you need or want to remove it</li> </ul>"},{"location":"certificates/lets-encrypt-k8s/#deploy-certificate-resource","title":"Deploy Certificate Resource","text":"<p>Next up is our <code>Certificate</code> resource, this is where <code>cert-manager</code> will store our certificate details to be used by our application.</p> <p>In case you forgot, this is one of the three CRD's provided by cert-manager.</p> <pre><code>apiVersion: certmanager.k8s.io/v1alpha1\nkind: Certificate\nmetadata:\n  name: myapp-example-com\n  namespace: myapp\nspec:\n  secretName: myapp-tls\n  dnsNames:\n  - myapp.example.com\n  acme:\n    config:\n    - http01:\n        ingressClass: nginx\n      domains:\n      - myapp.example.com\n  issuerRef:\n    name: myapp-letsencrypt-staging\n    kind: Issuer\n</code></pre> <p>The things to note here:</p> <ul> <li>name: so far I've found it a naming convention to write the domain name where <code>-</code> replaces the <code>.</code>'s.</li> <li>secretName: the name of the Kubernetes secret that will house the certificate and certificate key</li> <li>dnsNames: you can specify more than one name, in our case just a single one, should match <code>acme.config.domains</code></li> <li>acme.config: this defines the configuration for how the ownership proof should be done, this should match the method defined in the <code>Issuer</code></li> <li>issuerRef: in good Kubernetes fashion, we reference the <code>Issuer</code> that should issue our certificate, the name and kind should match our Issue resource</li> </ul>"},{"location":"certificates/lets-encrypt-k8s/#confirm-resources","title":"Confirm Resources","text":"<p>We have defined our set of resources that should create our valid - though not trusted, as it is staging - certificate. Before we use it, we should confirm our secret and our certificate are both valid.</p> <pre><code>kubectl describe certificate myapp-example-com --namespace myapp\n</code></pre> <p>The response includes the latest status, which looks like this:</p> <pre><code>Status:\n  Acme:\n    Order:\n      URL:  https://acme-staging-v02.api.letsencrypt.org/acme/order/705.../960...\n  Conditions:\n    Last Transition Time:  2018-10-02T21:17:34Z\n    Message:               Certificate issued successfully\n    Reason:                CertIssued\n    Status:                True\n    Type:                  Ready\n</code></pre> <p>Next up is our secret, containing the actual certificate and the certificate key.</p> <pre><code>kubectl describe secret myapp-tls --namespace myapp\n</code></pre> <p>Which results in something like this:</p> <pre><code>Name:         myapp-tls\nNamespace:    myapp\nLabels:       certmanager.k8s.io/certificate-name=myapp-example-com\nAnnotations:  certmanager.k8s.io/alt-names: myapp.example.com\n              certmanager.k8s.io/common-name: myapp.example.com\n              certmanager.k8s.io/issuer-kind: Issuer\n              certmanager.k8s.io/issuer-name: myapp-letsencrypt-staging\n\nType:  kubernetes.io/tls\n\nData\n====\ntls.crt:  3797 bytes\ntls.key:  1679 bytes\n</code></pre>"},{"location":"certificates/lets-encrypt-k8s/#use-certificate-to-enable-https","title":"Use certificate to enable https","text":"<p>Assuming the secret and the certificate are correct, we can use them to enable https on our web app.</p> <p>We put the tls certificate on the ingress of the application, in this case, Nginx, which assumes the following about the app:</p> <ul> <li>it has a deployment or stateful set</li> <li>it has a service which provides and endpoint to one or more instances</li> <li>it has an nginx ingress which points to the service</li> </ul>"},{"location":"certificates/lets-encrypt-k8s/#deployment","title":"Deployment","text":"<pre><code>kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: myapp\n  namespace: myapp\n  labels:\n    app: myapp\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp\n        image: caladreas/catnip-master\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n</code></pre> <p>You should always include livenessprobe, resource limits and so on. But for the sake of brevity, these are omitted!</p>"},{"location":"certificates/lets-encrypt-k8s/#service","title":"Service","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: myapp\n  namespace: myapp\n  labels:\n    app: myapp\nspec:\n  selector:\n    app: myapp\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8087\n    protocol: TCP\n</code></pre>"},{"location":"certificates/lets-encrypt-k8s/#ingress","title":"Ingress","text":"<pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: myapp\n  namespace: myapp\n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"\n    ingress.kubernetes.io/ssl-redirect: \"true\"\n    certmanager.k8s.io/issuer-kind: Issuer\n    certmanager.k8s.io/issuer-name: myapp-letsencrypt-staging\nspec:\n  rules:\n  - host: myapp.example.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: myapp\n          servicePort: 80\n  tls:\n  - hosts:\n    - myapp.example.com\n    secretName: myapp-tls\n</code></pre>"},{"location":"certificates/lets-encrypt-k8s/#further-resources","title":"Further resources","text":"<ul> <li>How Does Let's Encrypt Work</li> <li>Tutorial that inspired this page</li> <li>Amazone EKS Ingress Guide</li> <li>Kuberetes EKS Ingress and TLS</li> <li>How To Configure LTS for Nginx Ingress</li> </ul>"},{"location":"cloud/automation-pulumi/","title":"Pulumi","text":"<p>Just pointing to the documentation and GitHub repository is a bit boring and most of all, lazy. So I've gone through the trouble to create a demo project that shows what Pulumi can do.</p> <p>One of the things to keep in mind is that Pulumi keeps its state in a remote location by default - Pulumi's SaaS service. Much like using an S3 bucket to store your Terraform state.</p> <p>The goal of the demo is to show what you can do with Pulumi. My personal opinion is that Pulumi is excellent for managing entire systems as code. From public cloud infrastructure to public open source resources right down to your applications and their configurations.</p> <p>The system that I've written with Pulumi is a CI/CD system running in a GKE cluster. The system contains Jenkins, Artifactory and a custom LDAP server.</p> <p>To make it interesting, we'll use different aspects of Pulumi to create the system. We will create the GKE cluster via Pulumi's wrapper for the <code>gcloud</code> cli - in a similar way as JenkinsX does -, install Jenkins and Artifactory via Helm and install the LDAP server via Pulumi's Kubernetes resource classes.</p>"},{"location":"cloud/automation-pulumi/#steps-taken","title":"Steps taken","text":"<ul> <li>For more info Pulumi.io</li> <li>install: <code>brew install pulumi</code></li> <li>clone demo: <code>git clone https://github.com/demomon/pulumi-demo-1</code></li> <li>init stack: <code>pulumi stack init demomon-pulumi-demo-1</code><ul> <li>connect to GitHub</li> </ul> </li> <li>set kubernetes config <code>pulumi config set kubernetes:context gke_ps-dev-201405_europe-west4_joostvdg-reg-dec18-1</code></li> <li><code>pulumi config set isMinikube false</code></li> <li>install npm resources: <code>npm install</code> (I've used the TypeScript demo's)</li> <li><code>pulumi config set username administrator</code></li> <li><code>pulumi config set password 3OvlgaockdnTsYRU5JAcgM1o --secret</code></li> <li><code>pulumi preview</code></li> <li>incase pulumi loses your stack: <code>pulumi stack select demomon-pulumi-demo-1</code></li> <li><code>pulumi destroy</code></li> </ul> <p>Based on the following demo's:</p> <ul> <li>https://pulumi.io/quickstart/kubernetes/tutorial-exposed-deployment.html</li> <li>https://github.com/pulumi/examples/tree/master/kubernetes-ts-jenkins</li> </ul>"},{"location":"cloud/automation-pulumi/#artifactory-via-helm","title":"Artifactory via Helm","text":"<p>To install the helm charts of Artifactory using Helm, we will first need to add JFrog's repository.</p> <ul> <li><code>helm repo add jfrog https://charts.jfrog.io</code></li> <li><code>helm repo update</code></li> </ul>"},{"location":"cloud/automation-pulumi/#gke-cluster","title":"GKE Cluster","text":"<p>Below is the code for the cluster.</p> <pre><code>import * as gcp from \"@pulumi/gcp\";\nimport * as k8s from \"@pulumi/kubernetes\";\nimport * as pulumi from \"@pulumi/pulumi\";\nimport { nodeCount, nodeMachineType, password, username } from \"./gke-config\";\n\nexport const k8sCluster = new gcp.container.Cluster(\"gke-cluster\", {\n    name: \"joostvdg-dec-2018-pulumi\",\n    initialNodeCount: nodeCount,\n    nodeVersion: \"latest\",\n    minMasterVersion: \"latest\",\n    nodeConfig: {\n        machineType: nodeMachineType,\n        oauthScopes: [\n            \"https://www.googleapis.com/auth/compute\",\n            \"https://www.googleapis.com/auth/devstorage.read_only\",\n            \"https://www.googleapis.com/auth/logging.write\",\n            \"https://www.googleapis.com/auth/monitoring\"\n        ],\n    },\n});\n</code></pre>"},{"location":"cloud/automation-pulumi/#gke-config","title":"GKE Config","text":"<p>As you could see, we import variables from a configuration file <code>gke-config</code>.</p> <pre><code>import { Config } from \"@pulumi/pulumi\";\nconst config = new Config();\nexport const nodeCount = config.getNumber(\"nodeCount\") || 3;\nexport const nodeMachineType = config.get(\"nodeMachineType\") || \"n1-standard-2\";\n// username is the admin username for the cluster.\nexport const username = config.get(\"username\") || \"admin\";\n// password is the password for the admin user in the cluster.\nexport const password = config.require(\"password\");\n</code></pre>"},{"location":"cloud/automation-pulumi/#kubeconfig","title":"Kubeconfig","text":"<p>As you probably will want to install other things (Helm charts, services) inside this cluster, we need to make sure we get the <code>kubeconfig</code> file of our cluster - which is yet to be created. Below is the code - courtesy of Pulumi's GKE Example - that generates and exports the Kubernetes Client Configuration.</p> <p>This client configuration can then be used by Helm charts or other Kubernetes services. Pulumi will then also understand it depends on the cluster and create/update it first before moving on to the others.</p> <pre><code>// Manufacture a GKE-style Kubeconfig. Note that this is slightly \"different\" because of the way GKE requires\n// gcloud to be in the picture for cluster authentication (rather than using the client cert/key directly).\nexport const k8sConfig = pulumi.\n    all([ k8sCluster.name, k8sCluster.endpoint, k8sCluster.masterAuth ]).\n    apply(([ name, endpoint, auth ]) =&gt; {\n        const context = `${gcp.config.project}_${gcp.config.zone}_${name}`;\n        return `apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: ${auth.clusterCaCertificate}\n    server: https://${endpoint}\n  name: ${context}\ncontexts:\n- context:\n    cluster: ${context}\n    user: ${context}\n  name: ${context}\ncurrent-context: ${context}\nkind: Config\npreferences: {}\nusers:\n- name: ${context}\n  user:\n    auth-provider:\n      config:\n        cmd-args: config config-helper --format=json\n        cmd-path: gcloud\n        expiry-key: '{.credential.token_expiry}'\n        token-key: '{.credential.access_token}'\n      name: gcp\n`;\n    });\n\n// Export a Kubernetes provider instance that uses our cluster from above.\nexport const k8sProvider = new k8s.Provider(\"gkeK8s\", {\n    kubeconfig: k8sConfig,\n});\n</code></pre>"},{"location":"cloud/automation-pulumi/#pulumi-gcp-config","title":"Pulumi GCP Config","text":"<ul> <li>https://github.com/pulumi/examples/blob/master/gcp-ts-gke/README.md</li> </ul> <pre><code>export GCP_PROJECT=...\nexport GCP_ZONE=europe-west4-a\nexport CLUSTER_PASSWORD=...\nexport GCP_SA_NAME=...\n</code></pre> <p>Make sure you have a Google SA (Service Account) by that name first, as you can read here. For me it worked best to NOT set any environment variables mentioned. They invairably caused authentication or authorization issues. Just make sure the SA account and it's credential file (see below) are authorized and the <code>gcloud</code> cli works.</p> <pre><code>gcloud iam service-accounts keys create gcp-credentials.json \\\n    --iam-account ${GCP_SA_NAME}@${GCP_PROJECT}.iam.gserviceaccount.com\ngcloud auth activate-service-account --key-file gcp-credentials.json\ngcloud auth application-default login\n</code></pre> <pre><code>pulumi config set gcp:project ${GCP_PROJECT}\npulumi config set gcp:zone ${GCP_ZONE}\npulumi config set password --secret ${CLUSTER_PASSWORD}\n</code></pre>"},{"location":"cloud/automation-pulumi/#post-cluster-creation","title":"Post Cluster Creation","text":"<pre><code>gcloud container clusters get-credentials joostvdg-dec-2018-pulumi\nkubectl create clusterrolebinding cluster-admin-binding  --clusterrole cluster-admin  --user $(gcloud config get-value account)\n</code></pre>"},{"location":"cloud/automation-pulumi/#install-failed","title":"Install failed","text":"<p>Failed to install <code>kubernetes:rbac.authorization.k8s.io:Role         artifactory-artifactory</code>.</p> <p>Probably due to missing rights, so probably have to execute the admin binding before the helm charts.</p> <pre><code>error: Plan apply failed: roles.rbac.authorization.k8s.io \"artifactory-artifactory\" is forbidden: attempt to grant extra privileges: ...\n</code></pre>"},{"location":"cloud/automation-pulumi/#helm-charts","title":"Helm Charts","text":"<p>Using Pulumi to install a Helm Chart feels a bit like adding layers of wrapping upon wrapping. The power of Pulumi becomes visible when using more than one related service on the same cluster - for example a SDLC Tool Chain.</p> <p>This example application installs two helm charts, Jenkins and Artifactory, on a GKE cluster that is also created and managed by Pulumi.</p> <p>Below is an example of installing a Helm chart of Jenkins, where we provide the Kubernetes config from the GKE cluster as Provider. This way, Pulumi knows it must install the helm chart in that GKE cluster and not in the current Kubeconfig.</p> <pre><code>import { k8sProvider, k8sConfig } from \"./gke-cluster\";\n\nconst jenkins = new k8s.helm.v2.Chart(\"jenkins\", {\n    repo: \"stable\",\n    version: \"0.25.1\",\n    chart: \"jenkins\",\n    }, { \n        providers: { kubernetes: k8sProvider }\n    }\n);\n</code></pre>"},{"location":"cloud/automation-pulumi/#deployment-service","title":"Deployment &amp; Service","text":"<p>First, make sure you have an interface for the configuration arguments.</p> <pre><code>export interface LdapArgs {\n    readonly name: string,\n    readonly imageName: string,\n    readonly imageTag: string\n}\n</code></pre> <p>Then, create a exportable Pulumi resource class that can be reused.</p> <pre><code>export class LdapInstallation extends pulumi.ComponentResource {\n    public readonly deployment: k8s.apps.v1.Deployment;\n    public readonly service: k8s.core.v1.Service;\n\n    // constructor\n}\n</code></pre> <p>Inside the constructor placehold we will create a constructor method. It will do all the configuration we need to do for this resource, in this case a Kubernetes Service and Deployment.</p> <pre><code>constructor(args: LdapArgs) {\n    super(\"k8stypes:service:LdapInstallation\", args.name, {});\n    const labels = { app: args.name };\n    const name = args.name\n}\n</code></pre> <p>First Kubernetes resource to create is a container specification for the Deployment.</p> <pre><code>const container: k8stypes.core.v1.Container = {\n    name,\n    image: args.imageName + \":\" + args.imageTag,\n    resources: {\n        requests: { cpu: \"100m\", memory: \"200Mi\" },\n        limits: { cpu: \"100m\", memory: \"200Mi\" },\n    },\n    ports: [{\n            name: \"ldap\",containerPort: 1389,\n        },\n    ]\n};\n</code></pre> <p>As the configuration arguments can be any TypeScript type, you can allow people to override entire segments (such as Resources). Which you would do as follows:</p> <pre><code>    resources: args.resources || {\n        requests: { cpu: \"100m\", memory: \"200Mi\" },\n        limits: { cpu: \"100m\", memory: \"200Mi\" },\n    },\n</code></pre> <p>The Deployment and Service construction are quite similar.</p> <pre><code>this.deployment = new k8s.apps.v1.Deployment(args.name, {\n    spec: {\n        selector: { matchLabels: labels },\n        replicas: 1,\n        template: {\n            metadata: { labels: labels },\n            spec: { containers: [ container ] },\n        },\n    },\n},{ provider: cluster.k8sProvider });\n</code></pre> <pre><code>this.service = new k8s.core.v1.Service(args.name, {\n    metadata: {\n        labels: this.deployment.metadata.apply(meta =&gt; meta.labels),\n    },\n    spec: {\n        ports: [{\n                name: \"ldap\", port: 389, targetPort: \"ldap\" , protocol: \"TCP\"\n            },\n        ],\n        selector: this.deployment.spec.apply(spec =&gt; spec.template.metadata.labels),\n        type: \"ClusterIP\",\n    },\n}, { provider: cluster.k8sProvider });\n</code></pre>"},{"location":"cloudbees/cb-previews/","title":"CloudBees Previews","text":""},{"location":"cloudbees/cbc-eks/","title":"CloudBees Core on AWS EKS","text":"<p>The basics of installing CloudBees Core on EKS can found in the CloudBees Core  Install Guide. For details around the architecture and possibilities for TLS termination (L4 or L7 with ELB), see the CloudBees Core EKS Manual Install guide.</p> <p>This guide is here to do the installation with TLS on the Ingress Controller and certificates managed by Let's Encrypt. It will also go beyond the installation and continue with configuration of the Operations Center and create some Masters.</p> <p>Note</p> <p>This guide is originally written during 2019, when CloudBees Core didn't have a helm chart yet. If you prefer a Helm install, please refer to CloudBees Core EKS Helm Install guide.</p>"},{"location":"cloudbees/cbc-eks/#create-eks-cluster","title":"Create EKS Cluster","text":"<p>See my guide on creating a EKS cluster with EKSCTL , which is the recommended solution with regards to Kubernetes on AWS.</p>"},{"location":"cloudbees/cbc-eks/#certmanager","title":"Certmanager","text":"<pre><code>echo \"apiVersion: certmanager.k8s.io/v1alpha1\nkind: Certificate\nmetadata:\n  name: cloudbeescore-kearos-net\n  namespace: cje\nspec:\n  secretName: cjoc-tls-prd\n  dnsNames:\n  - cloudbees-core.kearos.net\n  acme:\n    config:\n    - http01:\n        ingressClass: nginx\n      domains:\n      - cloudbees-core.kearos.net\n  issuerRef:\n    name: letsencrypt-prd\n    kind: ClusterIssuer\" &gt; cjoc-cert.yml\neks apply -f cjoc-cert.yml\n</code></pre>"},{"location":"cloudbees/cbc-eks/#create-namespace-cje","title":"Create Namespace CJE","text":"<pre><code>echo \"apiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    name: cje\n  name: cje\" &gt; cje-namespace.yaml\n\neks create  -f cje-namespace.yaml\neks config set-context $(eks config current-context) --namespace=cje\n</code></pre>"},{"location":"cloudbees/cbc-eks/#cb-core-install","title":"CB Core Install","text":"<p>Download from downloads.cloudbees.com</p>"},{"location":"cloudbees/cbc-eks/#configure-dns","title":"Configure DNS","text":"<pre><code>DOMAIN_NAME=cloudbees-core.kearos.net\nsed -e s,cje.example.com,$DOMAIN_NAME,g &lt; cloudbees-core.yml &gt; tmp &amp;&amp; mv tmp cloudbees-core.yml\n</code></pre> <p>Configure k8s yaml file:</p> <ul> <li>add <code>certmanager.k8s.io/cluster-issuer: letsencrypt-prd</code> to cjoc ingress's <code>metadata.annotations</code></li> <li>add <code>secretName: cjoc-tls-prd</code> to cjoc ingress' <code>spec.tls.host[0]</code></li> <li>confirm cjoc ingress's host and tls host is <code>cloudbees-core.kearos.net</code></li> </ul>"},{"location":"cloudbees/cbc-eks/#install","title":"Install","text":"<pre><code>eks apply -f cloudbees-core.yml\neks rollout status sts cjoc\n</code></pre>"},{"location":"cloudbees/cbc-eks/#retrieve-initial-password","title":"Retrieve initial password","text":"<pre><code>eks exec cjoc-0 -- cat /var/jenkins_home/secrets/initialAdminPassword\n</code></pre>"},{"location":"cloudbees/cbc-eks/#jenkins-cli","title":"Jenkins CLI","text":"<pre><code>export CJOC_URL=https://cloudbees-core.kearos.net/cjoc/\nhttp --download ${CJOC_URL}/jnlpJars/jenkins-cli.jar --verify false\n</code></pre> <pre><code>export USR=jvandergriendt\nexport TKN=11b1016f80ddbb8a35bcbb5389599f7881\n</code></pre> <pre><code>alias cbc=\"java -jar jenkins-cli.jar -noKeyAuth -auth ${USR}:${TKN} -s ${CJOC_URL}\"\ncbc teams\n</code></pre>"},{"location":"cloudbees/cbc-eks/#create-team-cat","title":"Create team CAT","text":"<pre><code>cbc teams cat --put &lt; team-cat.json\n</code></pre>"},{"location":"cloudbees/cbc-eks/#use-efs","title":"Use EFS","text":"<p>https://go.cloudbees.com/docs/cloudbees-core/cloud-reference-architecture/kubernetes-efs/</p> <ul> <li>Create EFS in AWS<ul> <li>performance: general purpose</li> <li>throughput: provisioned, 160mb/s</li> <li>encrypted: yes</li> </ul> </li> </ul> <pre><code>EFS_KEY_ARN=arn:aws:kms:eu-west-1:324005994172:key/4bfd8d70-c7de-4e7a-ab83-10792be5daaa\n</code></pre>"},{"location":"cloudbees/cbc-eks/#destroy-cluster","title":"Destroy cluster","text":""},{"location":"cloudbees/cbc-eks/#external-client-the-hard-way","title":"External Client - The Hard Way","text":"<ul> <li>create a new master (hat)</li> <li>confirm remoting works on expected port</li> <li>50000+n, where <code>n</code> is incremental count of number of masters</li> <li>for example, if <code>hat</code> is the first new \"team\", it will be <code>50001</code></li> <li>create a new node</li> <li>external-agent</li> <li>launch via java webstart</li> <li>download client jar</li> <li>confirm port is NOT accessable</li> <li>open port on LB</li> <li>confirm port is open</li> </ul>"},{"location":"cloudbees/cbc-eks/#open-port-on-lb","title":"Open Port on LB","text":"<pre><code>export DOMAIN_NAME=cloudbees-core.example.com\nexport TEAM_NAME=hat\nexport MASTER_NAME=teams-${TEAM_NAME}\nexport USR=\nexport PSS=\n</code></pre>"},{"location":"cloudbees/cbc-eks/#test-port","title":"Test Port","text":""},{"location":"cloudbees/cbc-eks/#get-remoting-port","title":"Get Remoting Port","text":"<pre><code>http --print=hH --auth $USR:$PSS https://$DOMAIN_NAME/$MASTER_NAME/ | grep X-Jenkins-CLI-Port\n</code></pre>"},{"location":"cloudbees/cbc-eks/#configure-config-map","title":"Configure Config Map","text":"<p>If you already configured tcp-services before, you will need to retrieve the current configmap using kubectl get configmap tcp-services -n ingress-nginx -o yaml &gt; tcp-services.yaml and edit it accordingly</p> <pre><code>kubectl get configmap tcp-services -n ingress-nginx -o yaml &gt; tcp-services.yaml\n</code></pre> <p>Else:</p> <pre><code>export JNLP_MASTER_PORT=50001\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tcp-services\n  namespace: ingress-nginx\ndata:\n  $JNLP_MASTER_PORT: \\\"$NAMESPACE/$MASTER_NAME:$JNLP_MASTER_PORT:PROXY\\\"\n</code></pre> <p>For example:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tcp-services\n  namespace: ingress-nginx\ndata:\n  50001: \"default/teams-hat:50001:PROXY\"\n</code></pre>"},{"location":"cloudbees/cbc-eks/#create-patch-for-deployment-ingress","title":"Create Patch for Deployment (ingress)","text":"<pre><code>spec:\n  template:\n    spec:\n      containers:\n        - name: nginx-ingress-controller\n          ports:\n          - containerPort: $JNLP_MASTER_PORT\n            name: $JNLP_MASTER_PORT-tcp\n            protocol: TCP\n</code></pre> <p>Example:</p> <pre><code>spec:\n  template:\n    spec:\n      containers:\n        - name: nginx-ingress-controller\n          ports:\n          - containerPort: 50001\n            name: 50001-tcp\n            protocol: TCP\n</code></pre>"},{"location":"cloudbees/cbc-eks/#create-patch-for-service-ingress","title":"Create Patch for Service (ingress)","text":"<pre><code>spec:\n  ports:\n  - name: $JNLP_MASTER_PORT-tcp\n    port: $JNLP_MASTER_PORT\n    protocol: TCP\n    targetPort: $JNLP_MASTER_PORT-tcp\n</code></pre> <p>Example:</p> <pre><code>spec:\n  ports:\n  - name: 50001-tcp\n    port: 50001\n    protocol: TCP\n    targetPort: 50001-tcp\n</code></pre>"},{"location":"cloudbees/cbc-eks/#apply-patches","title":"Apply patches","text":"<pre><code>export NGINX_POD=$(kubectl get deployment -l app.kubernetes.io/name=ingress-nginx -n ingress-nginx -o jsonpath=\"{.items[0].metadata.name}\")\nkubectl apply -f tcp-services.yaml\nkubectl patch deployment ${NGINX_POD} -n ingress-nginx -p \"$(cat deployment-patch.yaml)\"\nkubectl patch service ingress-nginx -n ingress-nginx -p \"$(cat service-patch.yaml)\"\nkubectl annotate -n ingress-nginx service/ingress-nginx  service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout=\"3600\" --overwrite\n</code></pre>"},{"location":"cloudbees/cbc-eks/#delete-cluster","title":"Delete cluster","text":"<pre><code>aws cloudformation delete-stack --stack-name ${STACK_NAME} --region ${REGION} --profile ${PROFILE}\n</code></pre>"},{"location":"cloudbees/cbc-gke-helm/","title":"Install CloudBees Core On GKE","text":""},{"location":"cloudbees/cbc-gke-helm/#prerequisite","title":"Prerequisite","text":"<p>Have a GKE cluster in which you're <code>ClusterAdmin</code>.</p> <p>Don't have one yet? Read here how to create one!</p>"},{"location":"cloudbees/cbc-gke-helm/#prepare","title":"Prepare","text":"<pre><code>kubectl create namespace cloudbees-core\n</code></pre> <pre><code>helm repo add cloudbees https://charts.cloudbees.com/public/cloudbees\nhelm repo update\n</code></pre> <pre><code>kubens cloudbees-core\n</code></pre> <p>Info</p> <p><code>kubense</code> is a subcommand of the kubecontext tool.</p>"},{"location":"cloudbees/cbc-gke-helm/#install-clusterissuercert","title":"Install ClusterIssuer/Cert","text":"<p>This assumes you have Cert-Manager installed.</p> <pre><code>kubectl apply -f clusterissuer.yaml\nkubectl apply -f certificate.yaml\n</code></pre>"},{"location":"cloudbees/cbc-gke-helm/#clusterissueryaml","title":"clusterissuer.yaml","text":"<p>Make sure to replace <code>&lt;REPLACE_WITH_YOUR_EMAIL_ADDRESS&gt;</code> with your own email address.</p> <p>Let's Encrypt will use this to register the certificate and will notify you there when it expires.</p> <pre><code>apiVersion: certmanager.k8s.io/v1alpha1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    email: &lt;REPLACE_WITH_YOUR_EMAIL_ADDRESS&gt;\n    server: https://acme-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    http01: {}\n</code></pre>"},{"location":"cloudbees/cbc-gke-helm/#certificateyaml","title":"certificate.yaml","text":"<pre><code>apiVersion: certmanager.k8s.io/v1alpha1\nkind: Certificate\nmetadata:\n  name: &lt;MyHostName&gt;\n  namespace: cloudbees-core\nspec:\n  secretName: tls-cloudbees-core-kearos-net\n  dnsNames:\n  - cloudbees-core.kearos.net\n  acme:\n    config:\n    - http01:\n        ingressClass: nginx\n      domains:\n      - cloudbees-core.kearos.net\n  issuerRef:\n    name: letsencrypt-prod\n    kind: ClusterIssuer\n</code></pre>"},{"location":"cloudbees/cbc-gke-helm/#install-with-valuesyaml","title":"Install with values.yaml","text":"<pre><code>helm install --name cloudbees-core \\\n    -f cloudbees-core-values.yaml \\\n    --namespace=cloudbees-core \\\n    cloudbees/cloudbees-core\n</code></pre> <pre><code>kubectl rollout status statefulset cjoc\n</code></pre> <pre><code>kubectl get po cjoc-0\n</code></pre> <pre><code>kubectl logs -f cjoc-0\n</code></pre> <pre><code>stern cjoc\n</code></pre>"},{"location":"cloudbees/cbc-gke-helm/#get-initial-password","title":"Get Initial Password","text":"<pre><code>kubectl -n cloudbees-core exec cjoc-0 cat /var/jenkins_home/secrets/initialAdminPassword\n</code></pre>"},{"location":"cloudbees/cbc-gke-helm/#valuesyaml","title":"values.yaml","text":"<pre><code># A helm example values file for standard kubernetes install.\n# An nginx-ingress controller is not installed and ssl isn't installed.\n# Install an nginx-ingress controller\nnginx-ingress:\n  Enabled: false\n\nOperationsCenter:\n  # Set the HostName for the Operation Center\n  HostName: 'cloudbees-core.kearos.net'\n  # Setting ServiceType to ClusterIP creates ingress\n  ServiceType: ClusterIP\n  CSRF:\n    ProxyCompatibility: true\n  Ingress:\n    Annotations:\n      certmanager.k8s.io/cluster-issuer: \"letsencrypt-prod\"\n      kubernetes.io/ingress.class: nginx\n      kubernetes.io/tls-acme: \"false\"\n      nginx.ingress.kubernetes.io/app-root: https://$best_http_host/cjoc/teams-check/\n      nginx.ingress.kubernetes.io/proxy-body-size: 50m\n      nginx.ingress.kubernetes.io/proxy-request-buffering: \"off\"\n      nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    tls:\n    ## Set this to true in order to enable TLS on the ingress record\n      Enable: true\n      # Create a certificate kubernetes and use it here.\n      SecretName: tls-cloudbees-core-kearos-net\n      Host: cloudbees-core.kearos.net\n</code></pre>"},{"location":"cloudbees/cbc-gke-helm/#core-post-install","title":"Core Post Install","text":""},{"location":"cloudbees/cbc-gke-helm/#setup-api-token","title":"Setup API Token","text":"<p>Go to <code>http://&lt;MyHostName&gt;/cjoc</code>, login with your admin user.</p> <p>Click on the user's name (top right corner) -&gt; <code>Configure</code> -&gt; <code>Generate Token</code>.</p> <p>Warning</p> <p>You will see this token only once, so copy it and store it somewhere.</p>"},{"location":"cloudbees/cbc-gke-helm/#get-cli","title":"Get CLI","text":"<pre><code>export CJOC_URL=https://&lt;MyHostName&gt;/cjoc/\nhttp --download ${CJOC_URL}/jnlpJars/jenkins-cli.jar --verify false\n</code></pre>"},{"location":"cloudbees/cbc-gke-helm/#alias-cli","title":"Alias CLI","text":"<pre><code>USR=admin\nTKN=\n</code></pre> <pre><code>alias cboc=\"java -jar jenkins-cli.jar -noKeyAuth -auth ${USR}:${TKN} -s ${CJOC_URL}\"\n</code></pre> <pre><code>cboc version\n</code></pre>"},{"location":"cloudbees/cbc-gke-helm/#for-more-cli","title":"For More CLI","text":"<p>Go to <code>http://&lt;MyHostName&gt;/cjoc/cli</code></p>"},{"location":"cloudbees/cbc-post-install-tips/","title":"CloudBees Core Post Install Tips","text":""},{"location":"cloudbees/cbc-team-namespace/","title":"Team Master In Alternative Namespace","text":""},{"location":"cloudbees/cbc-team-namespace/#goal","title":"Goal","text":"<p>The goal of this document is to show how to create a Team Master with CloudBees Core Modern in a different [Kubernetes] Namespace than where the Operations Center resides.</p>"},{"location":"cloudbees/cbc-team-namespace/#audience","title":"Audience","text":"<p>For anyone working with CloudBees Core Modern as an Administrator or Cluster Administrator.</p>"},{"location":"cloudbees/cbc-team-namespace/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>working installation of CloudBees Core Modern</li> <li>kubectl access to the cluster with sufficient rights to set permissions</li> <li>Helm installed</li> <li>CloudBees Helm Chart configured</li> <li>Kubectx installed</li> </ul>"},{"location":"cloudbees/cbc-team-namespace/#prepare-helm","title":"Prepare Helm","text":"<ul> <li><code>helm repo add cloudbees https://charts.cloudbees.com/public/cloudbees</code></li> <li><code>helm repo update</code></li> </ul>"},{"location":"cloudbees/cbc-team-namespace/#process","title":"Process","text":"<ul> <li>create &amp; configure the new Namespace</li> <li>configure additional Kubernetes endpoint in CJOC</li> <li>create Team Master via Team UI using new Kubernetes Endpoint</li> </ul> <p>Important</p> <p>Unlike a Managed Master, we cannot choose the Namespace in which we want to create the Team Master. Not in the Teams UI nor in the Jenkins CLI.  To achieve our goal, we leverage the Kubernetes Endpoint configuration with Master Provisioning. </p>"},{"location":"cloudbees/cbc-team-namespace/#configure-new-namespace","title":"Configure New Namespace","text":"<p>If we want the Operations Center to create our Team Master in a different Namespace, we have to create this namespace first.</p> <p>That isn't enough; we also have to configure this Namespace so that Operations Center has the permissions to create resources. Additionally, we need to make sure that the new Team Master can create build agents (via PodTemplates) in the new Namespace as well.</p>"},{"location":"cloudbees/cbc-team-namespace/#create-namespace","title":"Create Namespace","text":"<p>First, we create the Namespace via <code>kubectl</code>.</p> <pre><code>NAMESPACE=\n</code></pre> <pre><code>kubectl create namespace $NAMESPACE\n</code></pre> <p>This sets the new Namespace as default.</p> <pre><code>kubens $NAMESPACE\n</code></pre> <p>Note</p> <p><code>kubens</code> is part of the kubectx tool.</p>"},{"location":"cloudbees/cbc-team-namespace/#create-helm-values-file","title":"Create Helm Values File","text":"<p>To configure the Namespace with everything the Operations Center and the to-be-created Team Master need, we can leverage the CloudBees Core Helm Chart. </p> <p>The Helm chart has a built-in feature to generate the Namespace configuration for a secondary namespace. To do so, we need to set two values at least:</p> <ul> <li>Master.OperationsCenterNamespace=<code>${NAMESPACE}</code></li> <li>OperationsCenter.Enabled=<code>false</code></li> </ul> <p>In yaml form (<code>namespace-values.yaml</code>):</p> <pre><code>Master:\n OperationsCenterNamespace: cloudbees-core\n\nOperationsCenter:\n Enabled: false\n</code></pre> <p>Note</p> <p>Make sure that <code>OperationsCenterNamespace</code> is the Namespace your Operations Center is configured in. By default, it should be <code>cloudbees-core</code>.</p>"},{"location":"cloudbees/cbc-team-namespace/#fetch-helm-chart","title":"Fetch Helm Chart","text":"<p>We then have to retrieve the Helm Chart itself, so Helm can use it for templating.</p> <pre><code>helm fetch \\\n --repo https://charts.cloudbees.com/public/cloudbees \\\n --version 3.8.0+a0d07461ae1c \\\n cloudbees-core\n</code></pre> <p>Note</p> <p>Make sure to change the version to reflect the version you've downloaded.</p>"},{"location":"cloudbees/cbc-team-namespace/#create-namespace-configuration","title":"Create Namespace Configuration","text":"<p>We have the values and the Chart. We can now let Helm create the configuration via <code>helm template</code>. </p> <pre><code>helm template cloudbees-core-namespace \\\n cloudbees-core-3.8.0+a0d07461ae1c.tgz \\\n -f namespace-values.yaml \\\n --namespace ${NAMESPACE} \\\n &gt; cloudbees-core-namespace.yml\n</code></pre> <p>!!!    note     Make sure to change the Chart filename to reflect the version you've downloaded.</p>"},{"location":"cloudbees/cbc-team-namespace/#apply-namespace-configuration","title":"Apply Namespace Configuration","text":"<p>Now that we have the complete configuration file of the Namespace, we can apply it via <code>kubectl apply -f</code>.</p> <pre><code>kubectl apply -f cloudbees-core-namespace.yml --namespace ${NAMESPACE}\n</code></pre>"},{"location":"cloudbees/cbc-team-namespace/#configure-kubernetes-endpoint-in-operations-center","title":"Configure Kubernetes Endpoint In Operations Center","text":"<p>Now that we have the Namespace configured, we can create a new Kubernetes Endpoint definition in Operations Center.</p> <p>Go to <code>Operations Center</code> -&gt; <code>Manage Jenkins</code> -&gt; <code>Configure System</code> -&gt; <code>Kubernetes Master Provisioning</code> and click <code>Add</code>.</p> <p>Here we configure the endpoint. We change the namespace only, and stay within the same cluster, so we leave the following fields blank:</p> <ul> <li><code>API endpoint URL</code></li> <li><code>Credentials</code></li> <li><code>Server Certificate</code></li> </ul> <p>You have to fill in the fields <code>Display Name</code> and <code>Namespace</code>, I'd recommend using the same value for both, the namespace we just created and configured.</p> <p>We also have to fill in the field <code>Jenkins URL</code>, we can take the base name from the default endpoint (should be <code>http://cjoc.cloudbees-core.svc.cluster.local/cjoc</code>). We then have to add the namespace of where Operations Center is in, to the URL. Which in my case is, is in <code>cloudbees-core</code>. The end result being: <code>http://cjoc.cloudbees-core.svc.cluster.local/cjoc</code>.</p> <p>Info</p> <p>Make sure to hit the <code>Validate</code> button to ensure the configuration works.</p> <p></p>"},{"location":"cloudbees/cbc-team-namespace/#create-team-master-in-alternative-namespace","title":"Create Team Master In Alternative Namespace","text":"<p>First, open the Teams UI.</p> <p></p> <p>Second, start the New Team Wizard.</p> <p></p> <p>And finally, select your new endpoint.</p> <p></p>"},{"location":"cloudbees/cbci-casc/","title":"Configuration As Code","text":"<p>What we will do is leverage CloudBees CI's(CBCI) Configuration as Code (CasC) to automate as much of the CBCI installation as possible.</p>"},{"location":"cloudbees/cbci-casc/#solution","title":"Solution","text":"<p>CBCI Modern only!</p> <p>This solution is for CloudBees CI Modern, on a recent Kubernetes cluster.</p>"},{"location":"cloudbees/cbci-casc/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster</li> <li>Helm</li> <li>Helmfile</li> <li>Helm diff plugin</li> <li>Kubectl with access to the Kubernetes cluster</li> <li>CloudBees CI license</li> </ul>"},{"location":"cloudbees/cbci-casc/#install-helm-diff","title":"Install Helm diff","text":"<pre><code>helm plugin install https://github.com/databus23/helm-diff\n</code></pre>"},{"location":"cloudbees/cbci-casc/#steps-to-take","title":"Steps to take","text":"<ul> <li>Bootstrap the Kubernetes cluster</li> <li>Setup Helm configuration (we'll use Helmfile)</li> <li>Setup Configuration as Code for the Operations Center</li> <li>Setup Configuration as Code for Controllers</li> </ul>"},{"location":"cloudbees/cbci-casc/#tools-included","title":"Tools Included","text":"<ul> <li>nginx-ingress: our Ingress Controller for accessing web applications in the cluster</li> <li>external-dns: manages the DNS entries in our Google CloudDNS Zone, so each Ingress resource gets its own unique DNS entry, this also enables us to generate specfic certificates via cert-manager</li> <li>cert-manager: manages certificate requests, so all our web applications can use TLS</li> <li>openldap: an LDAP server, for having an actual representative authentication realm for CBCI</li> <li>We use Geek-Cookbook's version.</li> <li>prometheus: we use Prometheus to collect metrics from the resources in the cluster</li> <li>grafana: we use Grafana to turn the metrics from Prometheus into viewable dashboards<ul> <li>the dashboard that is installed comes from here</li> </ul> </li> </ul>"},{"location":"cloudbees/cbci-casc/#bootstrap-kubernetes-cluster","title":"Bootstrap Kubernetes Cluster","text":""},{"location":"cloudbees/cbci-casc/#create-namespaces","title":"Create Namespaces","text":"<p>We need a namespace for nginx-ingress, cert-manager, and CloudBees CI.</p> <pre><code>kubectl create namespace cbci\nkubectl create namespace cert-manager\nkubectl create namespace nginx-ingress\n</code></pre>"},{"location":"cloudbees/cbci-casc/#configure-cert-manager-namespace","title":"Configure cert-manager namespace","text":"<p>Cert Manager will perform some validations on Ingress resources. It cannot do that on its own Ingress resource, so we label the <code>cert-manager</code> namespace so Cert Manager ignores itself.</p> <pre><code>kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true\n</code></pre>"},{"location":"cloudbees/cbci-casc/#external-dns-config","title":"External DNS Config","text":"<p>In my case, I am using the External DNS Controller with Google Cloud and a Cloud DNS Zone.</p> <p>For this I have created a GCP Service Account with a credentials file (the JSON file). If you want both <code>cert-manager</code> and <code>nginx-ingress</code> to directly use the Cloud DNS configuration to bypass the more lenghty alternatives (such as http verification) you need to supply them with the GCP Service Account.</p> <p>Read here for more on using the External DNS Controller with GCP.</p> <p>For other options, please refer to the External DNS Controller docs, which has guides for all the supported environments.</p> Cert ManagerNginx Ingress Controller <pre><code>kubectl -n cert-manager create secret generic external-dns-gcp-sa --from-file=credentials.json\n</code></pre> <pre><code>kubectl -n nginx-ingress create secret generic external-dns-gcp-sa --from-file=credentials.json \n</code></pre>"},{"location":"cloudbees/cbci-casc/#configure-cbci-namespace","title":"Configure CBCI namespace","text":"<p>We need to configure the LDAP password secret, so our CasC for OC bundle can configure LDAP while JCasC translates the placeholder to the actual password.</p> <p>This should be the same value as the <code>ldapGlobalPassword</code> in the <code>helmfile-values.yaml</code>.</p> <pre><code>kubectl create secret generic ldap-manager-pass --from-literal=pass=${LDAP_GLOBAL_PASSWORD} --namespace cbci\n</code></pre>"},{"location":"cloudbees/cbci-casc/#helmfile-configuration","title":"Helm(file) Configuration","text":""},{"location":"cloudbees/cbci-casc/#helmfile-layout","title":"Helmfile Layout","text":"<p>The files that matter to Helmfile are the following.</p> helmfile directory structure<pre><code>.\n\u251c\u2500\u2500 helmfile-values.yaml\n\u251c\u2500\u2500 helmfile.yaml\n\u2514\u2500\u2500 values\n    \u2514\u2500\u2500 *.yaml.gotmpl\n</code></pre>"},{"location":"cloudbees/cbci-casc/#helmfileyaml","title":"Helmfile.yaml","text":"<p>The <code>helmfile.yaml</code> file has several sections.</p> <p>Let's look at each section separately, before I share the whole file.</p> <p>We start with the environments.  In this case, I have just one environments, <code>default</code>, but you can choose to have more, opting to have seperate value files for Staging and Production for example.</p> helmfile.yaml (enviroments)<pre><code>environments:\n  default: # (1)\n    values:\n    - helmfile-values.yaml \n</code></pre> <ol> <li>the default environment is chosen if you do not choose an environment</li> </ol> <p>As Helmfile will interact with Helm for us, we can properly manage our Helm repositories. If we give Helmfile a list of Helm repositories, it will make to update them prior to any installation, so you don't have to worry about that.</p> helmfile.yaml (repositories)<pre><code>repositories:\n- name: stable\n  url: https://charts.helm.sh/stable\n- name: cloudbees\n  url: https://charts.cloudbees.com/public/cloudbees\n- name: jetstack\n  url: https://charts.jetstack.io\n- name: bitnami\n  url: https://charts.bitnami.com/bitnami\n- name: geek-cookbook\n  url: https://geek-cookbook.github.io/charts/\n- name: grafana\n  url: https://grafana.github.io/helm-charts\n- name: prometheus-community\n  url: https://prometheus-community.github.io/helm-charts\n</code></pre> <p>Another thing we cannot steer if Helmfile does the interaction with Helm for us, are Helms flags. Some of these flags might be important for you, I've chosen to set these.</p> helmfile.yaml (defaults)<pre><code>helmDefaults:\n  wait: true # (1)\n  timeout: 600 # (2)\n  historyMax: 25 # (3)\n  createNamespace: true # (4)\n</code></pre> <ol> <li>We will wait on the resources becoming ready before creating the next. This ensures our dependencies are honored.</li> <li>I personally always set an explicit timeout, so it is easy to spot if we hit a timeout. The timeout refers to how long we wait for the resources to be ready.</li> <li>How many update versions Hel tracks. I like to be able to rollback and have a bit of history.</li> <li>Some namespaces we created in the bootstrap, the rest should get created when required. This setting will make sure that any Helm installation in a new namespace, will have it created.</li> </ol> <p>Next up are the releases. These are the Helm chart releases. For the latest versions and the configurable properties, I recommend using ArtifactHub.io.</p> <p>Releases need a name, chart, version, and values. The chart, is a combination of the source repository (how you named it) and the chart name in that repository. In our case, this would be <code>cloudbees</code>, because I called the CloudBees Helm repository that, and then <code>/cloudbees-core</code>. While the product has been renamed, the Helm chart has kept the CloudBees Core name.</p> <p>Another thing you can see, is the <code>needs</code> list. This tells Helmfile to install those releases (by name) before installing this one.</p> helmfile.yaml (releases)<pre><code>releases:\n- name: cbci\n  namespace: cbci\n  chart: cloudbees/cloudbees-core # (1)\n  version: '3.37.2+7390bf58e3ab' \n  values:\n  - values/cbci.yaml.gotmpl  # (2)\n  needs: # (3)\n  - nginx-ingress\n  - external-dns\n  - cert-manager\n  - ldap\n\n- name: cm-cluster-issuer\n  namespace: cert-manager\n  chart: incubator/raw # (4)\n  values:\n  - values/cluster-issuer.yaml.gotmpl\n  needs:\n  - cbci\n  - cert-manager\n</code></pre> <ol> <li>The name of a Chart, <code>&lt;Repository Name&gt;/&lt;Chart Name&gt;</code></li> <li>The values to use for this Helm installation. In this case we're specifying a Go template, signified by the <code>yaml.gotmpl</code> extension.</li> <li>Informs Helmfile there is a dependency relationship between this Release and others, making sure it install them in the proper order.</li> <li><code>incubator/raw</code> lets you include templated files directly, without having a Helm release</li> </ol> <p>Helmfile supports various ways of supplying the Helm values. In this example I'm using a Go template which lets us template the Helm chart installations. By using a template values file, we can re-use values accross Helm charts to ensure that if two or more Charts reference the same value, we can guarantee it is the same.</p> <p>The second release, <code>cm-cluster-issuer</code> is a file that is in the same repository as the Helmfile configuration. This is why the chart is listed as <code>incubator/raw</code>, it lets you include templated Kubernetes manifests directly, without creating a Helm release.</p> Full Helmfile helmfile.yaml<pre><code>environments:\n  default:\n    values:\n    - helmfile-values.yaml\n\nrepositories:\n- name: stable\n  url: https://charts.helm.sh/stable\n- name: cloudbees\n  url: https://charts.cloudbees.com/public/cloudbees\n- name: jetstack\n  url: https://charts.jetstack.io\n- name: bitnami\n  url: https://charts.bitnami.com/bitnami\n- name: geek-cookbook\n  url: https://geek-cookbook.github.io/charts/\n- name: grafana\n  url: https://grafana.github.io/helm-charts  \n- name: prometheus-community\n  url: https://prometheus-community.github.io/helm-charts\n\nhelmDefaults:\n  wait: true\n  timeout: 600\n  historyMax: 25\n  createNamespace: true  \n\nreleases:\n- name: cbci\n  namespace: cbci\n  chart: cloudbees/cloudbees-core \n  version: 3.37.2+7390bf58e3ab \n  values:\n  - values/cbci.yaml.gotmpl\n  needs:\n  - nginx-ingress\n  - external-dns\n  - cert-manager\n  - ldap\n\n- name: nginx-ingress\n  namespace: nginx-ingress\n  chart: bitnami/nginx-ingress-controller\n  version: 7.6.21\n  values:\n  - values/nginx-ingress.yaml.gotmpl\n\n- name: external-dns\n  namespace: nginx-ingress\n  chart: bitnami/external-dns\n  version: 5.4.8\n  values:\n  - values/external-dns.yaml.gotmpl\n\n- name: cert-manager\n  namespace: cert-manager\n  chart: jetstack/cert-manager\n  version: 1.5.3\n  values:\n  - values/cert-manager.yaml.gotmpl\n  needs:\n  - nginx-ingress\n\n- name: cm-cluster-issuer\n  namespace: cert-manager\n  chart: incubator/raw\n  values:\n  - values/cluster-issuer.yaml.gotmpl\n  needs:\n  - cbci\n  - cert-manager\n\n- name: ldap\n  namespace: cbci\n  chart: geek-cookbook/openldap\n  version: 1.2.4\n  values:\n  - values/ldap.yaml.gotmpl\n\n- name: prometheus\n  namespace: mon\n  chart: prometheus-community/prometheus\n  version: 14.8.1\n  values:\n  - values/prom.yaml.gotmpl\n\n- name: grafana\n  namespace: mon\n  chart: grafana/grafana\n  version: 6.16.11\n  values:\n  - values/grafana.yaml.gotmpl\n  needs:\n  - prometheus\n</code></pre>"},{"location":"cloudbees/cbci-casc/#helmfile-valuesyaml","title":"Helmfile-values.yaml","text":"<p>As stated in the previous section, I have opted for using templated Helm values files. This lets me add placeholder values, which I can replace with values via Helmfile.</p> <p>In the <code>environments</code> section, I referenced the file <code>helmfile-values.yaml</code> for the default, and only, environment. So let's take a look at this file.</p> <p>There are mostly passwords in there, for Grafana and LDAP. There are also two values related to the External DNS Controller configuration,  <code>googleProject</code> and <code>googleASSecret</code>.</p> <p>Feel free to remove these, if you're not using GCP or you're not using the External DNS Controller.</p> helmfile-values.yaml<pre><code>adminEmail: # (1)\ngoogleProject: # (2)\ngoogleASSecret: external-dns-gcp-sa # (3)\nldapGlobalPassword: \nldapUser1Password: \nldapUser2Password: \nldapUser3Password: \ngrafanaUser: \ngrafanaPass: \n</code></pre> <ol> <li>the admin email address used for the Cluster Issuer, and will receive notifications from Cert Manager for certificate expirations</li> <li>the Google Project id where the Cloud DNS Zone resides</li> <li>the name of the Kubernetes secret containing the GCP Service Account JSON file</li> </ol>"},{"location":"cloudbees/cbci-casc/#values-files","title":"Values Files","text":"<p>I won't list each of them here, they are all available in my CloudBees CI CasC repo on GitHub.</p> <p>The Cluster Issuer is an optional resource. I personally always prefer having an automated DNS setup and HTTPS with matching Certificates (e.g., no wildcard certificates).</p> <p>Optional</p> <p>Only use this if you are using GCP, the External DNS Controller, and Cert Manager.</p> values/cluster-issuer.yaml.gotmpl<pre><code>resources:\n  - apiVersion: cert-manager.io/v1\n    kind: ClusterIssuer\n    metadata:\n      name: letsencrypt-prod\n    spec:\n      acme:\n        email: {{ .Values.adminEmail }}\n        server: https://acme-v02.api.letsencrypt.org/directory\n        privateKeySecretRef:\n          name: letsencrypt-prod\n        solvers: # (1)\n        - selector: {} \n          dns01:\n            cloudDNS:\n              project: {{ .Values.googleProject }}\n              serviceAccountSecretRef:\n                name: {{ .Values.googleASSecret }}\n                key: credentials.json\n</code></pre> <ol> <li>An empty 'selector' means that this solver matches all domains</li> </ol> <p>The most important one of couse, is the Helm values for the CloudBees CI installation. It already contains some secret sauce that will help us with synchronizing the CasC for OC Bundle.</p> values/cbci.yaml.gotmpl<pre><code>OperationsCenter:\n  HostName: {{ .Values.cbciHostname }}\n  CSRF:\n    ProxyCompatibility: true\n  Annotations: # (1)\n    prometheus.io/path: \"/prometheus\"\n    prometheus.io/port: \"8080\"\n    prometheus.io/scrape: \"true\"\n  Ingress:\n    Annotations:\n      cert-manager.io/cluster-issuer: letsencrypt-prod # (2)\n      kubernetes.io/tls-acme: 'true'\n      kubernetes.io/ingress.class: nginx\n    tls:\n      Enable: true\n      Host: {{ .Values.cbciHostname }}\n      SecretName: my-cbci-tls-secret\n  JavaOpts: # (3)\n    -Djenkins.install.runSetupWizard=false\n    -Dcore.casc.config.bundle=/var/jenkins_config/oc-casc-bundle-from-git/cloudbees-ci-casc/casc-for-oc/bundle\n  ContainerEnv: # (4)\n    - name: LDAP_MANAGER_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: ldap-manager-pass\n          key: pass\n  ExtraContainers: # (5)\n  - name: git-sync\n    image: gcr.io/k8s-staging-git-sync/git-sync@sha256:866599ca98bcde1404b56152d8601888a5d3dae7fc21665155577d607652aa09\n    args:\n      - --repo=https://github.com/joostvdg/cloudbees-ci-casc\n      - --branch=main\n      - --depth=1\n      - --wait=20\n      - --root=/git\n    volumeMounts:\n      - name: content-from-git\n        mountPath: /git\n\n  ExtraVolumes: # (6)\n  - name: content-from-git\n    emptyDir: {}\n  ExtraVolumeMounts: # (7)\n  - name: content-from-git\n    mountPath: /var/jenkins_config/oc-casc-bundle-from-git\n    readOnly: true\n\nAgents: # (8)\n  Enabled: true\n  SeparateNamespace:\n    Enabled: true\n    Name: ci-agents\n    Create: true\n\nHibernation: # (9)\n  Enabled: true\n</code></pre> <ol> <li>OC Pod Annotations for Prometheus, so our Prometheus installation can scrape the Metrics from the OC</li> <li>Annotations for the Ingress resource, so we get a valid certificate from Cert Manager via our referenced Certificate Issuer</li> <li>Disable the Installation Wizard, and tell the OC where it can find its CasC Bundle</li> <li>Map the LDAP password secret as an environment variable, so JCasC can interpolate it</li> <li>Define a sidecar container with the Git Sync</li> <li>Define additional Pod volumes</li> <li>Define additional OC Container Volume Mounts</li> <li>Let CloudBees CI run Kubernetes agent in a separate namespace</li> <li>Enable the CloudBees CI Hibernation feature</li> </ol> <p>Some of the things we're doing in this Helm file configuration:</p> <ul> <li>OC Pod Annotations for Prometheus, so our Prometheus installation can scrape the Metrics from the OC</li> <li>Annotations for the Ingress resource, so we get a valid certificate from Cert Manager via our referenced Certificate Issuer</li> <li>Tell the OC where it can find its CasC Bundle</li> <li>Define a sidecar container with the Git Sync with additional volume mounts (more at git-sync-for-casc-oc-bundle) </li> <li>Let CloudBees CI run Kubernetes agent in a separate namespace</li> <li>Enable the CloudBees CI Hibernation feature</li> </ul>"},{"location":"cloudbees/cbci-casc/#install-via-helmfile","title":"Install Via Helmfile","text":"<p>To verify the Helm and Helmfile configuration is correct, you can run the validate command.</p> <pre><code>helmfile lint\n</code></pre> <p>The output will be something like this:</p> <p>Success</p> <pre><code>Adding repo cloudbees https://charts.cloudbees.com/public/cloudbees\n\"cloudbees\" has been added to your repositories\n\nFetching cloudbees/cloudbees-core\n\nLinting release=cbci, chart=/var/folders/f_/mpwdv8s16r7_zt43r3r7k20w0000gn/T/helmfile2581490205/cbci/cbci/cloudbees/cloudbees-core/3.37.2+7390bf58e3ab/cloudbees-core\n==&gt; Linting /var/folders/f_/mpwdv8s16r7_zt43r3r7k20w0000gn/T/helmfile2581490205/cbci/cbci/cloudbees/cloudbees-core/3.37.2+7390bf58e3ab/cloudbees-core\n[WARNING] templates/cjoc-ingress.yaml: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress\n[WARNING] templates/managed-master-hibernation-monitor-ingress.yaml: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress\n\n1 chart(s) linted, 0 chart(s) failed\n</code></pre> <p>Assuming all your configuration is valid, you can now safely apply it to the cluster.</p> <pre><code>helmfile apply\n</code></pre> <p>The output of that depends on your charts and their versions, and you will see the post-install messages of each chart. If, like me, you've configured the option <code>helmDefaults.wait</code>, then there will be pauses in the process, as Helmfile waits for the Helm install to finish before moving to the next.</p>"},{"location":"cloudbees/cbci-casc/#casc-for-oc","title":"CasC for OC","text":"<p>At the time of writing this (November 2021), Configuration as Code (CasC) for the Operations Center (OC) is still a Preview feature.</p> <p>But aside from a few wrinkles that are about to be ironed out, CasC for OC is ready for prime time. Where applicable, I will mention the wrinkle and the workaround if required.</p> <p>We will look at the following things:</p> <ul> <li>How to make OC use CasC</li> <li>How to structure the CasC Bundle</li> <li>How to configure some common items</li> <li>How to keep the Bundle up to date</li> <li>How to manage Items (such as Jobs, Folders, Controllers)</li> </ul> <p>For all the details, my full working CasC for OC example is on GitHub.</p> <p>What is a Bundle</p> <p>CloudBees CI extends Jenkins Configuration as Code (JCasC) with additional configuration files, for a full Configuration as Code experience. This collection of files, is called a Bundle. A Bundle has a fixed structure.</p> <p>For more information, please read creating a OC Bundle in the official documentation.</p>"},{"location":"cloudbees/cbci-casc/#make-oc-use-casc","title":"Make OC use CasC","text":"<p>Warning</p> <p>There is a chicken and egg problem; unless you have a wildcard license, you need to start OC with the wizard, and either retrieve a valid license based on its <code>Instance ID</code>, or request a trial license. </p> <p>From then on, you can use CasC to configure the OC, including the License (if you want).</p> <p>To make OC use a CasC we need three things:</p> <ol> <li>have a valid CasC Bundle</li> <li>have an OC with the CasC plugins installed</li> <li>tell OC where its Bundle is</li> </ol> <p>We will explore the first point in the next section. For the second point, we can use CasC to install those plugins and that ability is there by default. So luckily, we can ignore the chicken and egg problem there.</p> <p>To tell OC to use a Bundle, we set a Java property with the directory where it can find the Bundle. We can set Java properties in the Helm chart, via the <code>OperationsCenter.JavaOpts</code> property. The property to set, is <code>-Dcore.casc.config.bundle=</code>, as you can see in the example below.</p> values/cbci.yaml.gotmpl<pre><code>OperationsCenter:\n  # other config ignored for example\n  JavaOpts:\n    -Djenkins.install.runSetupWizard=false\n    -Dcore.casc.config.bundle=/var/jenkins_config/oc-casc-bundle-from-git/cloudbees-ci-casc/casc-for-oc/bundle\n\n  # other config ignored for example\n  ExtraVolumes: # (6)\n  - name: casc-oc-volume\n    configMap:\n      name: casc-oc\n  ExtraVolumeMounts: # (7)\n  - name: casc-oc-volume\n    mountPath: /var/jenkins_config/oc-casc-bundle\n    readOnly: true\n</code></pre> <p>Help</p> <p>If you're wondering how we configure that sidecar container and why, we'll dive into this, in the section: Keep Bundle up-to-date with Git Sync.</p> <p>In this example, we use a sidecar container to synchronize the OC Bundle to a directory. Due to the structure of that repository, the Bundle is accessible at <code>/var/jenkins_config/oc-casc-bundle-from-git/cloudbees-ci-casc/casc-for-oc/bundle</code>. </p>"},{"location":"cloudbees/cbci-casc/#bundle-structure","title":"Bundle Structure","text":"<p>Below is a snippet of my CasC for OC Bundle.</p> <pre><code>.\n\u251c\u2500\u2500 bundle.yaml\n\u251c\u2500\u2500 items\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 controllers\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 blue.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yellow.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 items.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 jobs\n\u251c\u2500\u2500 jenkins.yaml\n\u251c\u2500\u2500 plugins.yaml\n\u2514\u2500\u2500 rbac.yaml\n</code></pre> <p>The main items here are as follows:</p> <ul> <li><code>bundle.yaml</code>: the Bundle descriptor, informs CasC about the contents of this Bundle</li> <li><code>items/</code>: folder with with subfolders and items in their separate files</li> <li><code>jenkins.yaml</code>: the main configuration, essentially leveraging JCasC</li> <li><code>plugins.yaml</code>: list of plugins to install, limited to CAP plugins</li> <li><code>rbac.yaml</code>: where we configure Role Based Access Control, global roles and global groups</li> </ul> <p>RBAC Configuration</p> <p>The <code>rbac.yaml</code> contains the global configuration. Configuration for folders or Controllers is defined in their respective item manifest files. We'll explore these later.</p> <p>Let's take a look at some of these files.</p> bundle.yaml<pre><code>apiVersion: \"1\"\nid: \"jenkins\"\ndescription: \"OC Bundle\"\nversion: \"1.12\" # (1)\nplugins:\n  - \"plugins.yaml\"\njcasc:\n  - \"jenkins.yaml\"\nrbac:\n  - \"rbac.yaml\"\nitems: # (2)\n  - \"items/\"\n</code></pre> <ol> <li>updating the version speeds up the detection of a bundle change</li> <li>we can read configuration from a directory and its subdirectories</li> </ol> <p>The only mandatory files are this <code>bundle.yaml</code> and the <code>jcasc</code> section with one or more files. You can either use a named file, like I've done, or use point it to a folder. CasC will read all the files in the directory and its sub-directories.</p> <p>We leverage that with our <code>items</code> section. We'll explore this in more depth in the Managing Items section.</p>"},{"location":"cloudbees/cbci-casc/#configure-oc-via-casc","title":"Configure OC via CasC","text":"<p>To configure OC via a Bundle, let's explore some of the files by which you can do so. Due to the more detailed nature, we'll explore Items and the LDAP configuration separately.</p> <p>Let's start with the plugins.</p> plugin.yaml<pre><code>plugins:\n- {id: notification-api}\n- {id: operations-center-notification}\n- {id: cloudbees-prometheus}\n# skipping some items to reduce example size\n</code></pre> <p>Info</p> <p>At the time of writing, the CasC plugins are restructured. So while the example at some point in time used the following two plugins; <code>cloudbees-casc-api</code> and <code>cloudbees-casc-server</code>.</p> <p>They are likely different by the time you're reading this. Please look at the official docs for the appropriate plugins for your release. </p> <p>If you look at the file in my example repository, you'll fine it has ~108 plugins. For the sake of the example, I won't list all of them here.</p> <p>For plugins in the CAP we do not have to list the versions, as CAP manages those for us. So we only list the plugin id of the plugins we want. If for some reason the plugin cannot be installed, there will be an explicit warning in the logs.</p> <p>Warning</p> <p>When adding plugins, the instance must always be restarted for the change to take effect.</p> <p>Up next is the RBAC configuration, if you're not using CloudBees' RBAC with CloudBees CI yet, I strongly urge you to reconsider!</p> rbac.yaml<pre><code>removeStrategy:\n  rbac: SYNC\n\nroles:\n- name: authenticated\n- permissions:\n  - hudson.model.Hudson.Administer\n  name: administer\n- name: anonymous\n- filterable: 'true'\n  permissions:\n  - hudson.model.Item.Read\n  - hudson.model.Item.Discover\n  - hudson.model.Hudson.Read\n  - hudson.model.View.Read\n  name: browse\ngroups:\n- members:\n    external_groups:\n      - Administrators\n    users:\n      - admin\n      - jvandergriendt\n  roles:\n  - name: administer\n    grantedAt: current\n  name: Administrators\n- roles:\n  - name: develop\n    grantedAt: current\n  name: Developers\n  members:\n    external_groups:\n      - BlueAdmins\n      - GreenAdmins\n- roles:\n  - name: browse\n    grantedAt: current\n  name: Browsers\n  members:\n    external_groups:\n      - Blue\n      - Green\n</code></pre> <p>There are three main sections in my exmaple.</p> <ul> <li><code>removeStrategy</code>: determines how differences between the CasC configuration and current state is handled</li> <li><code>roles</code>: defines the global roles, these are applied to the OC and all Controllers (except those that opt out of global RBAC)</li> <li><code>groups</code>: these are the global groups, active from the root from OC all the way down. Which is why we give some external groups the <code>browse</code>role, so they can see the items in OC they might be administrator of (such as a Controller)</li> </ul> <p>For the sake of brevity, I have excluded some of the roles.  These are the roles that are generated when you first initialize RBAC and choose <code>typical initial setup</code>.</p> <p>If you're not sure how the define these, I recommend exploring the options via the UI first, and then exporting the configuration.</p> <p>Warning</p> <p>The <code>removeStrategy</code> is required, please read the official docs for the available options.</p>"},{"location":"cloudbees/cbci-casc/#jenkinsyaml","title":"Jenkins.yaml","text":"<p>In the <code>jcasc</code> section of the Bundle manifest (<code>bundle.yaml</code>), we define the configuration of the OC. Essentially, anything you can configure via one of the pages of the <code>Manage Jenkins</code> screen, can be defined via CasC.</p> <p>Again, for the sake of brevity, I'll exclude some details that aren't important to discuss.</p> jenkins.yaml<pre><code>jenkins:\n  authorizationStrategy: \"cloudBeesRoleBasedAccessControl\"\n  crumbIssuer:\n    standard:\n      excludeClientIPFromCrumb: true\n  primaryView:\n    masters:\n      columns:\n      - \"jobName\"\n      - \"status\"\n      - \"weather\"\n      - \"masterConfigurationStaleViewColumn\"\n      - \"jenkinsVersionViewColumn\"\n      - \"cascViewColumn\"\n      - \"manageMaster\"\n      - \"totalJobsViewColumn\"\n      - \"queueSizeViewColumn\"\n      - \"listSelectionColumn\"\n      jobFilters:\n      - \"connectedMasterViewFilter\"\n      name: \"Controllers\"\n      recurse: true\n  securityRealm:\n    ldap: #  we'll dive into this below\ncloudBeesCasCServer:\n  defaultBundle: \"mc1\"\n  visibility: true\nbeekeeper:\n  enabled: true\n\nmasterprovisioning:\n  kubernetes:\n    clusterEndpoints:\n    # details are explored below\n</code></pre> <p>Danger</p> <p>If you want to manage the RBAC configuration via CasC, the <code>authorizationStrategy</code> must be set to <code>\"cloudBeesRoleBasedAccessControl\"</code>.</p> <p>You can see that this file can quickly become quite large and unwieldy. So you can break it up into different files, and then in the <code>bundle.yaml</code> point to a folder with all the file instead.</p> <p>In case you've used either CasC or JCasC before, this is exactly the same, with the only difference that the OC might not support some plugins and has some plugins which do not exist for Controllers.</p> <p>When in doubt, configure it via the UI first and then export the CasC configuration.</p>"},{"location":"cloudbees/cbci-casc/#ldap-configuration","title":"LDAP Configuration","text":"<p>Let's take a look at the LDAP configuration snippet. The configuration as based on the LDAP we've configured via the Helmfile configuration earlier in this guide.</p> jenkins.yaml (jenkins.securityRealm)<pre><code>jenkins:\n  securityRealm:\n    ldap:\n      configurations:\n      - displayNameAttributeName: \"cn\"\n        groupMembershipStrategy:\n          fromGroupSearch:\n            filter: \"member={0}\"\n        groupSearchBase: \"ou=Groups\"\n        inhibitInferRootDN: false\n        managerDN: \"cn=admin,dc=example,dc=org\"\n        managerPasswordSecret: ${LDAP_MANAGER_PASSWORD}\n        rootDN: \"dc=example,dc=org\"\n        server: \"ldap://ldap-openldap:389\"\n        userSearchBase: \"ou=People\"\n      disableMailAddressResolver: false\n      groupIdStrategy: \"caseInsensitive\"\n      userIdStrategy: \"caseInsensitive\"  \n</code></pre> <p>The password has been set to <code>${LDAP_MANAGER_PASSWORD}</code>, which will be automatically interpreted by JCasC, read here how and what options there are.</p> <p>I've opted for leveraging an environment variable. This environment variable comes from a Kubernetes secret by the name <code>ldap-manager-pass</code>, we created in the Configure CBCI namespace section.</p> values/cbci.yaml.gotmpl<pre><code>OperationsCenter:\n  # other config excluded for brevity\n  ContainerEnv: \n    - name: LDAP_MANAGER_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: ldap-manager-pass\n          key: pass\n</code></pre>"},{"location":"cloudbees/cbci-casc/#controller-provisioning","title":"Controller Provisioning","text":"<p>Bug</p> <p>While CloudBees CI stands for inclusive naming and works hard to remove any outdated terminology, sometimes they missed. Expect <code>masterprovisioning</code> to be renamed to <code>controllerprovisioning</code>.</p> <p>One of the great things about CloudBees CI Modern is its ability of creating and managing Controllers on Kubernetes. To control the properties of the Kubernetes resources, there is the <code>masterprovisioning</code>.</p> <p>I've configured just the default settings, where the only notable thing is the <code>-Dorg.csanchez...defaultImage=cloudbees/cloudbees-core-agent:2.303.3.3</code> property. Which will have to be updated if you upgrade CloudBees CI.</p> <p>Warning</p> <p>As we're using YAML in YAML and have multiple layers of interpretation, we have to exclude the <code>${name}</code> variables, so the annotations get the correct Controller name. To escape values, such as the <code>$</code> in this case, we use the <code>^</code>, so <code>${name}</code> becomes <code>^${name}</code>.</p> <p>Remember the configuration for Prometheus? Well, if we want our Controller's metrics to be scrapable by Prometheus, we need to ensure they have the proper annotations as well. So I've included the annotations in the <code>yaml</code> section, so any new Controller automatically has them.</p> jenkins.yaml (masterprovisioning)<pre><code>masterprovisioning:\n  kubernetes:\n    clusterEndpoints:\n    - id: \"default\"\n      ingressClass: \"nginx\"\n      jenkinsUrl: \"http://cjoc.cbci.svc.cluster.local/cjoc/\"\n      name: \"kubernetes\"\n    cpus: \"1.0\"\n    disk: 50\n    fsGroup: \"1000\"\n    globalJavaOptions: \"-Djenkins.model.Jenkins.crumbIssuerProxyCompatibility=true\\\n      \\ -Dorg.csanchez.jenkins.plugins.kubernetes.pipeline.PodTemplateStepExecution.defaultImage=cloudbees/cloudbees-core-agent:2.303.3.3\\\n      \\ -Dcom.cloudbees.jenkins.plugins.kube.ServiceAccountFilter.defaultServiceAccount=jenkins-agents\\\n      \\ -Dcom.cloudbees.networking.useSubdomain=false -Dcom.cloudbees.networking.protocol=\\\"\\\n      https\\\" -Dcom.cloudbees.networking.hostname=\\\"ci.cbci-pm.beescloud.com\\\" -Dcom.cloudbees.networking.port=443\\\n      \\ -Dcom.cloudbees.networking.operationsCenterName=\\\"cjoc\\\"\"\n    javaOptions: \"-XshowSettings:vm -XX:+AlwaysPreTouch -XX:+UseG1GC -XX:+DisableExplicitGC\\\n      \\ -XX:+ParallelRefProcEnabled -XX:+UseStringDeduplication -Dhudson.slaves.NodeProvisioner.initialDelay=0\"\n    livenessInitialDelaySeconds: 300\n    livenessPeriodSeconds: 12\n    livenessTimeoutSeconds: 12\n    memory: 3072\n    readinessFailureThreshold: 100\n    readinessInitialDelaySeconds: 30\n    readinessTimeoutSeconds: 5\n    terminationGracePeriodSeconds: 1200\n    yaml: |-\n      apiVersion: \"apps/v1\"\n      kind: \"StatefulSet\"\n      spec:\n        template:\n          metadata:\n            annotations:\n              prometheus.io/path: \"/^${name}/prometheus\"\n              prometheus.io/port: \"8080\"\n              prometheus.io/scrape: \"true\"\n            labels:\n              app.kubernetes.io/component: Managed-Controller\n              app.kubernetes.io/instance: \"^${name}\"\n              app.kubernetes.io/managed-by: CloudBees-CI-Cloud-Operations-Center\n              app.kubernetes.io/name: \"^${name}\"\n</code></pre>"},{"location":"cloudbees/cbci-casc/#managing-items","title":"Managing Items","text":"<p>As you've seen, we've configured the Bundle to collect Item manifests from the <code>items</code> folder.</p> <p>CasC will automatically read all the files in the specified folder and its subfolders.</p> <p>In my example Bundle, the items folder looks like this:</p> <pre><code>.\n\u251c\u2500\u2500 controllers\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 blue.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cyan.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 green.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 purple.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 yellow.yaml\n\u251c\u2500\u2500 items.yaml\n\u2514\u2500\u2500 jobs\n    \u2514\u2500\u2500 controller-casc-sync.yaml\n</code></pre> <p>In the <code>items.yaml</code> you can define any item you need, such as Folders. I've separated Controllers and Jobs from the main file.</p> <p>This way, if someone wants to add a new Controller or a new Job, all they have to do is add a new file into the folder and synchronise the Bundle to OC.</p> <p>Mandatory Boilerplate</p> <p>In the current iteration of CasC, each file in the Items directory hierarchy, needs some boilerplate configuration, else it throws an error.</p> <pre><code>removeStrategy:\n  rbac: SYNC\n  items: NONE\nitems:\n</code></pre>"},{"location":"cloudbees/cbci-casc/#managed-controller","title":"Managed Controller","text":"<p>You might have picked it up already, but yes, you can create and managed Managed Controllers via OC's CasC Bundle!</p> <p>Below is about as minimal an example as it can get. It configures some basic properties such as the name and description, and some more specific items. Such as the RBAC configuration (via <code>groups:</code>), which CasC Bundle it should use, and its Kubernetes configuration (overriding or extending <code>masterprovisioning</code> from the OC).</p> <p>Created but not provisioned</p> <p>In the CloudBees CI releases prior to December 2021, Managed Controllers get created, but do not get provisioned. This means their respective Kubernetes resources do not get created automatically.</p> <p>You can create a Cluster Operations Job, use the UI, or the CLI to save and restart the Controllers to create their Kubernetes resources.</p> items/controllers/cyan.yaml<pre><code>removeStrategy:\n  rbac: SYNC\n  items: NONE\n\nitems:\n- kind: managedController\n  displayName: 'Cyan'\n  name: mc-cyan\n  description: 'Cyan Controller (Managed by CasC)'\n  groups:\n  - members:\n      users:\n      - bluedev\n      - blueadmin\n    name: Blues\n  properties:\n  - configurationAsCode:\n      bundle: purple\n  configuration:\n    kubernetes:\n      domain: cyan\n</code></pre>"},{"location":"cloudbees/cbci-casc/#keep-bundle-up-to-date-with-git-sync","title":"Keep Bundle up-to-date with Git Sync","text":"<p>There are various ways you can get a CasC Bundle to the OC.</p> <p>For example, you can store the YAML files in a Kubernetes ConfigMap, and then mount that ConfigMap into the OC's volume.</p> <p>If you have a flat Bundle file hierarchy, or you have a transformation pipeline that creates a customizable output a ConfigMap is an option.</p> <p>Tip</p> <p>If you do want to use a ConfigMap, this is how you configure the Helm values, assuming you CasC Bundle's ConfigMap is called <code>casc-oc</code>.</p> <pre><code>ExtraVolumes: \n- name: casc-oc-volume\n  configMap:\n    name: casc-oc\nExtraVolumeMounts: \n- name: casc-oc-volume\n  mountPath: /var/jenkins_config/oc-casc-bundle\n  readOnly: true\n</code></pre> <p>I've opted to go another route, synchronizing my CasC Bundle directly from Git into a (Kubernetes) Volume. To do so, I've used Git Sync, a tool from the Kubernetes maintainers for exactly this purpose.</p> <p>So what we want to achieve, is that when I make a change in my Bundle in Git, it is automatically updated in the OC. To do so, we will add a sidecar container to the OC via the Helm values and some volume mounts, and thats all we need.</p> <p>I've based my solution based on the Git Sync documentation example for raw Kubernetes.</p> <p>Warning</p> <p>The example uses a Container repository that is either not accessible, or does not contain the Git Sync images.</p> <p>After some digging around, I found the <code>3.x</code> version images in a GCR; <code>gcr.io/k8s-staging-git-sync/git-sync:v3.3.4__linux_amd64</code>. </p> <p>For those who want to use the hash id instead of the tag, that tag is the following <code>git-sync@sha256:866599ca98bcde1404b56152d8601888a5d3dae7fc21665155577d607652aa09</code>.</p> <p>While not documented, there is an option in the CBCI Helm chart to add a side car container. You do so via <code>OperationsCenter.ExtraContainers</code> which is a list item.</p> <p>This container needs a name and an image.  For the sake of Git Sync, we need to supply it with arguments which tell it which repository to synchronize, how, and where to store it.</p> <p>My repository is public, so it does not need credentials, if yours does, all the common solutions are supported, as per the docs. Do verify the actual options the version you use supports, as it seems some flags were renamed between the version <code>4.x</code> that is the current <code>main</code> branch and the <code>3.4</code> version I'm using.</p> <p>We add a Volume Mount to this sidecar container, so it can store the Git checkout on a Volume that is also usable by OC. We can add volumes via the <code>OperationsCenter.ExtraVolumes</code> property, which is also a list item.</p> <p>Then to make sure the OC can read from this Volume, we also mount that same Volume via the <code>OperationsCenter.ExtraVolumeMounts</code> property. The <code>mountPath</code> is what is the basis of the <code>-Dcore.casc.config.bundle</code> flag we set, to tell OC where it can find its Bundle.</p> values/cbci.yaml.gotmpl<pre><code>  ExtraContainers: \n  - name: git-sync\n    image: gcr.io/k8s-staging-git-sync/git-sync@sha256:866599ca98bcde1404b56152d8601888a5d3dae7fc21665155577d607652aa09\n    args:\n      - --repo=https://github.com/joostvdg/cloudbees-ci-casc\n      - --branch=main\n      - --depth=1\n      - --wait=20\n      - --root=/git\n    volumeMounts:\n      - name: content-from-git\n        mountPath: /git\n\n  ExtraVolumes: \n  - name: content-from-git\n    emptyDir: {}\n  ExtraVolumeMounts: \n  - name: content-from-git\n    mountPath: /var/jenkins_config/oc-casc-bundle-from-git\n    readOnly: true\n</code></pre> <p>Speaking of the flag <code>-Dcore.casc.config.bundle</code>, to save you some of the trouble I had, lets take a good look at the path I've specified. Essentially, the Git Sync checks out the repository into a folder with the same name.</p> <p>The path starts from the <code>mountPath</code>, which is <code>/var/jenkins_config/oc-casc-bundle-from-git</code>. My Bundle's repository is called <code>cloudbees-ci-casc</code>, which we'll add next, and then the path to my Bundle within that repository.</p> values/cbci.yaml.gotmpl<pre><code>  JavaOpts:\n    -Djenkins.install.runSetupWizard=false\n    -Dcore.casc.config.bundle=/var/jenkins_config/oc-casc-bundle-from-git/cloudbees-ci-casc/casc-for-oc/bundle\n</code></pre>"},{"location":"cloudbees/cbci-casc/#casc-for-controllers","title":"CasC for Controllers","text":"<p>What is a Bundle</p> <p>CloudBees CI extends Jenkins Configuration as Code (JCasC) with additional configuration files, for a full Configuration as Code experience. This collection of files, is called a Bundle. A Bundle has a fixed structure.</p> <p>For more information, please read creating a Controller Bundle in the official documentation.</p> <p>As a Controller Bundle is essentially the same as the one for the OC, I'll limit myself to two things. One, how you can get those Bundles into the OC and usable by Controllers, and two, how to leverage the inheritance.</p>"},{"location":"cloudbees/cbci-casc/#synchronize-controller-bundles-to-oc","title":"Synchronize Controller Bundles to OC","text":""},{"location":"cloudbees/cbci-casc/#steps-to-synchronize-bundles","title":"Steps to Synchronize Bundles","text":"<ul> <li>install a git client on Operations Center</li> <li>for example: <code>github-branch-source</code></li> <li>create a Freestyle job</li> <li>check out from your repository with the casc Bundles</li> <li>use the <code>Synchronize bundles from workspace with internal storage</code> build step</li> <li>create a Controller and select an available Bundle</li> </ul>"},{"location":"cloudbees/cbci-casc/#update-bundle-configuration","title":"Update Bundle Configuration","text":"<p>If you're not sure what you'd want to configure in the bundle, or which plugins you really need.</p> <p>You can first create a Managed Controller how you want it to be. Then export its CasC configuration by the built-in   <code>casc-exporter</code>.</p> <p>You do this, by going to the following URL <code>&lt;controllerUrl&gt;/core-casc-export</code>.</p>"},{"location":"cloudbees/cbci-casc/#freestyle-job-on-oc","title":"Freestyle Job - On OC","text":"<p>URL to checkout: <code>https://github.com/joostvdg/cloudbees-ci-casc.git</code> Use the <code>Synchronize bundles from workspace with internal storage</code> build step.</p> <p>Note: this only works if the Bundles are at the top level</p> <p></p> CasC Sync Job as CasC Item casc-sync-job-item.yaml<pre><code>- kind: freeStyle\n  displayName: casc-sync-new\n  name: casc-sync-new\n  disabled: false\n  description: 'My CasC Bundle Synchronization job'\n  concurrentBuild: false\n  builders:\n  - casCBundlesSyncBuildStep: {}\n  blockBuildWhenUpstreamBuilding: false\n  blockBuildWhenDownstreamBuilding: false\n  scm:\n    gitSCM:\n      userRemoteConfigs:\n      - userRemoteConfig:\n          url: https://github.com/joostvdg/cloudbees-ci-casc.git\n      branches:\n      - branchSpec:\n          name: '*/main'\n  scmCheckoutStrategy:\n    standard: {}\n</code></pre>"},{"location":"cloudbees/cbci-casc/#bundle-inheritance","title":"Bundle Inheritance","text":"<p>You can have Bundles inherit from each other. You do so by using the parent property in a Bundle descriptor (<code>bundle.yaml</code>).</p> <p>For example, let's take a look at my community2 Bundle.</p> bundle.yaml<pre><code>apiVersion: \"1\"\nversion: \"1.0\"\nid: \"community2\"\ndescription: \"Shared Bundle\"\nparent: \"community1\"\njcasc:\n  - \"jenkins.yaml\"\nplugins:\n  - \"plugins.yaml\"\n</code></pre> <p>This means the Bundle now extends the <code>community1</code> Bundle. When you use <code>community2</code>, you essentially get all the files of both <code>community1</code> and <code>community2</code>.</p> <p>Inheritance Limitation</p> <p>At this time of writing, Bundles cannot override configuration they inherit and the configuration must be complimentary. Meaning, once you specify the <code>jenkins.systemMessage</code>, the entire inheritance chain has to use the same message.</p> <p>What does work, which may not be obvious, is list items. Such as SharedLibraries, PodTemplates, and Tool definitions. Meaning, if you define two SharedLibraries in Bundle A, one SharedLibrary in Bundle B, the Controller using Bundle B, will have three SharedLibraries defined.</p> <p>Another caveeat to take note of, is that the last found Plugin Catalog is used. As a Plugin Catalog essentially just lists plugins that can be installed, I suggest you define one Plugin Catalog at the root of your hierarchy, and any plugins you need after, you add them to that same one Plugin Catalog.</p> <p>This is what I've done with the <code>community</code> chain of Bundles. The Bundle <code>community1</code> contains a Plugin Catalog, with all the Tier 3 plugins all downstream Bundles need.</p> <ul> <li>Bundle <code>community1</code> sets up a basic configuration, including a Plugin Catalog listing common Tier 3 (Community) plugins</li> <li>Bundle <code>community2</code> installs a chunk of these plugins, and sets a system message</li> <li>Bubdle <code>purple</code> install an additional community plugin (also defined in the Plugin Catalog) and has Controllers specific items (jobs)</li> </ul> <pre><code>.\n\u251c\u2500\u2500 community1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bundle.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 jenkins\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jenkins.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 podtemplate-golang.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 podtemplate-maven-jdk17.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 shared-libraries.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 plugin-catalog.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 plugins.yaml\n\u251c\u2500\u2500 community2 (inherits community 1)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bundle.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 jenkins.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 plugins.yaml\n\u2514\u2500\u2500 purple (inherits community 2)\n \u00a0\u00a0 \u251c\u2500\u2500 bundle.yaml\n \u00a0\u00a0 \u251c\u2500\u2500 items\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 pipeline-example.yaml\n \u00a0\u00a0 \u2514\u2500\u2500 plugins.yaml\n</code></pre>"},{"location":"cloudbees/multi-cluster-eks/","title":"CloudBees CI - Multi Cluster EKS","text":"<ul> <li>https://medium.com/@Joachim8675309/building-eks-with-eksctl-799eeb3b0efd</li> </ul> <p>To create and configure cluster one, we execute the following steps:</p> <ol> <li>Create Cluster via EKSCTL</li> <li>Create Route53 Hosted Zone</li> <li>Install Bootstrapping services (certmanager, ingress controller, external dns)</li> <li>Install CloudBees CI</li> </ol>"},{"location":"cloudbees/multi-cluster-eks/#1-create-cluster-via-eksctl","title":"1. Create Cluster via EKSCTL","text":"<p>For some tasks, you need create IAM access for the workloads.</p> <p>There are various ways to do this, I urge you to explore those that are suitable to you.</p> <p>For simplicity, you can let EKSCTL generate them for you on the node instances. Be careful, as this means any Pod on these nodes can use the access.</p> <p>For production clusters, you either want Service Account IAM access, or multiple node pools where workloads are restricted to node pools related to their level of required access.</p> <p>Read here more information on <code>eksctl</code> and IAM policies.</p> <p>Note</p> <p>When using this example for Cluster Two, make sure to change the <code>metadata.name</code>.</p> <p>cluster.yaml</p> <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: cluster-jvandergriendt\n  region: us-east-1\n  version: \"1.15\"\n  tags: \n    CreatedBy: jvandergriendt\n\nnodeGroups:\n  - name: ng1-masters\n    instanceType: m5.large\n    minSize: 3\n    maxSize: 8\n    volumeSize: 100\n    volumeType: gp2\n    labels:\n      nodegroup-type: masters\n    iam:\n      withAddonPolicies:\n      autoScaler: true\n      externalDNS: true\n      certManager: true\n      ebs: true\n\navailabilityZones: [\"us-east-1d\", \"us-east-1f\"]\n</code></pre> Setup ProfileCreate Cluster <pre><code>AWS_PROFILE=cloudbees-iam\nAWS_ROLE_ARN=\n</code></pre> <pre><code>eksctl create cluster  --config-file cluster.yaml --profile cloudbees-eks\n</code></pre>"},{"location":"cloudbees/multi-cluster-eks/#2-create-route53-hosted-zone","title":"2. Create Route53 Hosted Zone","text":"<ul> <li>via the UI: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/CreatingHostedZone.html</li> <li>Via CLI: https://docs.aws.amazon.com/cli/latest/reference/route53/create-hosted-zone.html</li> </ul>"},{"location":"cloudbees/multi-cluster-eks/#3-install-bootstrapping-services","title":"3. Install Bootstrapping services","text":""},{"location":"cloudbees/multi-cluster-eks/#nginx","title":"Nginx","text":"Add Helm Repo <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n</code></pre> <p>=== \"Update Helm repositories     <pre><code>helm repo update\n</code></pre></p> Helm Install Cluster OneHelm Install Cluster Two <pre><code>helm install ingress ingress-nginx/ingress-nginx -f ingress-values-c1.yaml\n</code></pre> <pre><code>helm install ingress ingress-nginx/ingress-nginx -f ingress-values-c2.yaml\n</code></pre> <p>ingress-values-c1.yaml</p> <pre><code>service:\n    omitClusterIP: true\ncontroller:\n  tcp:\n    configMapNamespace: default\n  autoscaling:\n    enabled: true\n    minReplicas: 3\n    maxReplicas: 7\n  publishService:\n    enabled: true\n  metrics:\n    enabled: true\ntcp: \n  50000: \"cloudbees-ci/cjoc:50000\"\n</code></pre> <p>ingress-values-c2.yaml</p> <pre><code>service:\n    omitClusterIP: true\ncontroller:\n  tcp:\n    configMapNamespace: default\n  autoscaling:\n    enabled: true\n    minReplicas: 3\n    maxReplicas: 7\n  publishService:\n    enabled: true\n  metrics:\n    enabled: true\n</code></pre>"},{"location":"cloudbees/multi-cluster-eks/#external-dns","title":"External DNS","text":"<ul> <li>https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws.md</li> <li>https://github.com/helm/charts/tree/master/stable/external-dns</li> <li>https://github.com/bitnami/charts/tree/master/bitnami/external-dns</li> <li>https://github.com/bitnami/charts/tree/master/bitnami/external-dns#tutorials</li> </ul> Add Helm Chart RepoUpdate Helm ReposHelm Install w/ valuesHelm Install w/ set <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\n</code></pre> <pre><code>helm repo update\n</code></pre> <pre><code>helm install exdns bitnami/external-dns -f exdns-values.yaml\n</code></pre> <pre><code>helm install my-release \\\n  --set provider=aws \\\n  --set aws.zoneType=public \\\n  --set txtOwnerId=HOSTED_ZONE_IDENTIFIER \\\n  --set domainFilters[0]=HOSTED_ZONE_NAME \\\n  bitnami/external-dns\n</code></pre> <p>exdns-values.yaml</p> <pre><code>provider: aws\ndomainFilters:\n  - \"eks.kearos.net\"\nmetrics:\n  enabled: true\naws:\n  zoneType: public\ntxtOwnerId: HOSTED_ZONE_IDENTIFIER\n</code></pre>"},{"location":"cloudbees/multi-cluster-eks/#apply-policy","title":"Apply Policy","text":"<p>Caution</p> <p>If you have already configured the policies in the <code>cluster.yaml</code>, you do not need to do this step.</p> <p>The documentation warns not to use this policy, as it gives every Pod in the cluster the ability to update Route53 records of the specified hostedzone (by default all, <code>*</code>).</p> <pre><code>aws iam put-role-policy  --region us-east-1 --profile cloudbees-eks \\\n    --role-name ${ROLE} \\\n    --policy-name eks-jvandergriendt1-ExdnsPolicy \\\n    --policy-document file://exdns-policy.json\n</code></pre> <p>exdns-policy.json</p> <ul> <li>https://awspolicygen.s3.amazonaws.com/policygen.html</li> </ul> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Stmt1591221999371\",\n      \"Action\": [\n        \"route53:ChangeResourceRecordSets\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"arn:aws:route53:::hostedzone/*\"\n    },\n    {\n      \"Sid\": \"Stmt1591222034162\",\n      \"Action\": [\n        \"route53:ListHostedZones\",\n        \"route53:ListResourceRecordSets\",\n        \"route53:GetChange\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre>"},{"location":"cloudbees/multi-cluster-eks/#certmanager","title":"Certmanager","text":"Add Helm RepoHelm Repo UpdateInstall CRDsCreate NamespaceHelm InstallInstall ClusterIssuer <pre><code>helm repo add jetstack https://charts.jetstack.io\n</code></pre> <pre><code>helm repo update\n</code></pre> <pre><code>kubectl apply \\\n   --validate=false \\\n   -f https://github.com/jetstack/cert-manager/releases/download/v0.15.1/cert-manager.crds.yaml\n</code></pre> <pre><code># certmanager expects to run in `cert-manager` namespace\nkubectl create namespace cert-manager\n</code></pre> <pre><code>helm install cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  -f certmanager-values.yaml\n</code></pre> <pre><code>kubectl apply -f cluster-issuer.yaml\n</code></pre> <p>certmanager-values.yaml</p> <pre><code>prometheus:\n  enabled: true\ningressShim:\n  defaultIssuerName: letsencrypt-prod\n  defaultIssuerKind: ClusterIssuer\n</code></pre> <p>cluster-issuer.yaml</p> <ul> <li>https://cert-manager.io/docs/configuration/acme/dns01/route53/</li> </ul> <pre><code>apiVersion: cert-manager.io/v1alpha2\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    email: joostvdg@gmail.com\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    server: https://acme-v02.api.letsencrypt.org/directory\n    solvers:\n    # example: cross-account zone management for example.com\n    # this solver uses ambient credentials (i.e. inferred from the environment or EC2 Metadata Service)\n    # to assume a role in a different account\n    - selector:\n        dnsZones:\n          - \"example.com\"\n      dns01:\n        route53:\n          region: us-east-1\n          hostedZoneID: DIKER8JEXAMPLE \n</code></pre>"},{"location":"cloudbees/multi-cluster-eks/#4-install-cloudbees-ci","title":"4. Install CloudBees CI","text":"Create NamespaceHelm Install <pre><code>kubectl create namespace cloudbees-ci\n</code></pre> <pre><code>helm install cloudbees-ci cloudbees/cloudbees-core \\\n   -f cloudbees-ci-values.yaml \\\n   --namespace cloudbees-ci \\\n    --version 3.14.0+ebfb4625ad50\n</code></pre> <p>cloudbees-ci-values.yaml</p> <pre><code>OperationsCenter:\n  HostName: ci.eks.kearos.net\n  Ingress:\n    Annotations:\n      cert-manager.io/cluster-issuer: letsencrypt-prod\n    tls:\n      Enable: true\n      Host: ci.eks.kearos.net\n      SecretName: tls-ci-eks-kearos-net-p\n  ServiceType: ClusterIP\n</code></pre>"},{"location":"cloudbees/multi-cluster-eks/#prepare-cluster-two-access","title":"Prepare Cluster Two Access","text":"<p>First, follow step 1, 2 and 3 in the previous chapter but now for the second cluster.</p> <p>Then, come back and continue with the next steps below.</p>"},{"location":"cloudbees/multi-cluster-eks/#create-prepare-namespace","title":"Create &amp; Prepare Namespace","text":"Create NamespaceFetch Helm ChartGenerate Namespace ManifestsApply Namespace Manifests <pre><code>kubectl create namespace masters\n</code></pre> <pre><code>helm fetch cloudbees/cloudbees-core --untar\n</code></pre> <pre><code>helm template cloudbees-core-masters-masters cloudbees-core \\\n   --namespace masters \\\n   -f namespace-masters-values.yaml \\\n   &gt; namespace-masters.yaml\n</code></pre> <pre><code>kubectl apply -f namespace-masters.yaml \\\n  --namespace masters\n</code></pre> <p>Caution</p> <p>Make sure you are executing the steps on Cluster Two!</p> <p>namespace-masters-values.yaml</p> <pre><code>OperationsCenter:\n    Enabled: false\nMaster:\n    Enabled: true\n    OperationsCenterNamespace: masters\nAgents:\n    Enabled: true\n</code></pre>"},{"location":"cloudbees/multi-cluster-eks/#create-managed-master-in-both-clusters","title":"Create Managed Master in both clusters","text":"<ol> <li>Configure Operations Center with Access To EKS Cluster Two</li> <li>Create second Kubernetes endpoint configuration</li> <li>Create Managed Master in Cluster One</li> <li>Create Managed Master in Cluster Two</li> </ol>"},{"location":"cloudbees/multi-cluster-eks/#configure-operations-center-with-access-to-eks-cluster-two","title":"Configure Operations Center with Access To EKS Cluster Two","text":"<p>This is complicated, there are various ways of getting access. These all depend on IAM controls and the policies that exist within your organization.</p>"},{"location":"cloudbees/multi-cluster-eks/#aws-iam-authenticator","title":"AWS IAM Authenticator","text":"<p>For more information, you can read this PR on the Jenkins Kubernetes Plugin. And here for downloading the <code>aws-iam-authenticator</code> binary for the target platform.</p> <p>A set of steps that can work is the following:</p> <pre><code>curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.16.8/2020-04-16/bin/linux/amd64/aws-iam-authenticator\n</code></pre> <pre><code>chmod +x ./aws-iam-authenticator\n</code></pre> <pre><code>kubectl cp aws-iam-authenticator cjoc-0:/var/jenkins_home/bin\n</code></pre> <p>Then, you have to update the StatefulSet of Operations Center to include <code>/var/jenkins_home/bin</code> in the <code>PATH</code>.</p> <p>You can add the snippet below to the <code>cloudbees-ci.yaml</code> and run a <code>helm upgrade cloudbees-ci ...</code> command (the same as the <code>helm install</code> but replace <code>install</code> with <code>upgrade</code>).</p> <pre><code>ContainerEnv:\n  - name: PATH\n    value: \"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/var/jenkins_home/bin\"\n</code></pre> <p>Then, you create a <code>kubeconfig</code> file with the information for the EKS cluster, see below. And add this as a <code>secret file</code> credential to Operations Center's credentials. Use this credential in the <code>Kubernetes Master Provision</code>'s Endpoint for Cluster Two.</p> <p>kubeconfig</p> <pre><code>clusters:\n- cluster:\n    certificate-authority-data: &lt;redacted&gt;\n    server: &lt;redacted&gt;\n  name: &lt;redacted&gt;\ncontexts:\n- context:\n    cluster: &lt;redacted&gt;\n    user: &lt;redacted&gt;\n  name: &lt;redacted&gt;\ncurrent-context: &lt;redacted&gt;\nkind: Config\npreferences: {}\nusers:\n- name: &lt;redacted&gt;\n  user:\n    exec:\n      apiVersion: client.authentication.k8s.io/v1alpha1\n      env:\n        - name: AWS_ACCESS_KEY\n          value: &lt;redacted&gt;\n        - name: AWS_SECRET_ACCESS_KEY\n          value: &lt;redacted&gt;\n      args:\n      - token\n      - -i\n      - eks-cluster-name\n      command: aws-iam-authenticator\n</code></pre>"},{"location":"cloudbees/multi-cluster-eks/#create-second-kubernetes-endpoint-configuration","title":"Create second Kubernetes endpoint configuration","text":"<p>Now we create a second Kubernetes Endpoint in the cluster.</p> <p>The fields to fill in:</p> <ol> <li>API endpoint URL: the cluster endpoint of Cluster Two</li> <li>Display Name: display name so you know which cluster you create a Master in</li> <li>Credentials: the credentials you created for IAM access to Cluster Two</li> <li>Server Certificate: the server certificate of Cluster Two</li> <li>Namespace: the namespace you have prepared, in our case <code>masters</code></li> <li>Master URL Pattern: the DNS pattern for Cluster Two, in our case <code>https://*.two.domain.com</code></li> <li>Ingress Class: unless you used a different Ingress than in this guide, set to <code>nginx</code></li> <li>Jenkins URL: the external URL of the Operations Center</li> </ol> <p>Info</p> <p>If you don't know the API Endpoint or the Server Certificate, you can retrieve this from your <code>~/.kube/config</code> file. Assuming you have access to Cluster Two, you will the details of the cluster there.</p> <p>For the Server Certificate, you have to Base64 decode this. You can do so on mac/linux by echo + pipe into <code>base64</code>.</p> <p>For example, on a mac:</p> <pre><code>echo \"kjhKJSDH13123\" | base64 -D\n</code></pre> <p></p>"},{"location":"cloudbees/multi-cluster-eks/#update-master-provisioning","title":"Update Master Provisioning","text":"<ul> <li><code>Manage Jenkins</code> -&gt; <code>Configure System</code> -&gt; <code>Kubernetes Master Provisioning</code> -&gt; <code>Advanced</code> -&gt; <code>YAML</code></li> </ul> <p>YAML field</p> <pre><code>apiVersion: \"extensions/v1beta1\"\nkind: \"Ingress\"\nmetadata:\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  tls:\n  - hosts:\n    - ${host}\n    secretName: ${secretName}\n</code></pre>"},{"location":"cloudbees/multi-cluster-eks/#cleanup","title":"Cleanup","text":"<ul> <li>https://eksworkshop.com/920_cleanup/eksctl/</li> </ul> <p>For each cluster, we can now run the <code>eksctl</code> cleanup.</p> <pre><code>eksctl delete cluster -f cluster.yaml --wait\n</code></pre> <p>Important</p> <p>If you attached any policies to the nodes, you have to remove them first!</p> <p>Else you get this error:</p> <pre><code>AWS::IAM::Role/NodeInstanceRole: DELETE_FAILED \u2013\u00a0\"Cannot delete entity, must delete policies first. (Service: AmazonIdentityManagement; Status Code: 409;\n</code></pre> <p>To do so, you can use <code>aws iam delete-role-policy</code>.</p> <pre><code>aws iam delete-role-policy --region ${REGION} --profile ${PROFILE} \\\n    --role-name ${ROLE_NAME} \\\n    --policy-name ${CLUSTER_NAME}-ExdnsPolicy \n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/","title":"CloudBees CI - Multi Cluster GKE","text":"<p>Imagine you have a whole range of departments and development teams. Preferably you want to serve them with a standardized SDA(Software Delivery Automation) platform, but at the same time, make sure they pay for their usage.</p> <p>I don't think this is too far fetched or something terrible. I think it makes sense. In this light, CloudBees Core now supports running on multiple Kubernetes clusters.</p> <p>In this guide, we're going to explore how to run CloudBees Core on two GKE cluster at the same time. And how to deal with the details of getting the namespace configured, certificates, Kubernetes API tokens and so on and so forth.</p>"},{"location":"cloudbees/multi-cluster-gke/#prerequisites","title":"Prerequisites","text":"<ul> <li>The ability to create two GKE clusters</li> <li>CloudDNS enabled</li> <li>valid domain name</li> </ul>"},{"location":"cloudbees/multi-cluster-gke/#steps-to-do","title":"Steps to do","text":"<p>High over, we have to do the following steps:</p> <ul> <li>create and configure GKE cluster One<ul> <li>create GCP service account for accessing CloudDNS<ul> <li>this is per GCP Project, if cluster two is in another project, repeat this</li> </ul> </li> <li>install Operations Center</li> </ul> </li> <li>create and configure GKE cluster Two<ul> <li>create GCP service account for accessing GKE</li> </ul> </li> <li>create a Managed Master in both clusters<ul> <li>configure Operations Center</li> <li>create managed masters</li> </ul> </li> </ul>"},{"location":"cloudbees/multi-cluster-gke/#create-and-configure-cluster-one","title":"Create and Configure Cluster One","text":"<p>The easiest way to create and manage a cluster is with Terraform.</p> <p>Note</p> <p>I will use <code>GCP_PROJECT_ONE</code> and <code>one.domain.com</code> as placeholders for the Google Project ID and domain name respectively. You will need to change these to reflect your configuration.</p> <p>The assumption in this guide is that the clusters are in two different projects. This doesn't have to be so be, so feel free to use the same project id and domain name for both clusters.</p> <p>To create and configure cluster one, we execute the following steps:</p> <ol> <li>Create Cluster With Terraform</li> <li>Create Cloud DNS Zone</li> <li>Install Bootstrapping services (certmanager, ingress controller, external dns)</li> <li>Install CloudBees CI</li> <li>Configure External Access</li> </ol>"},{"location":"cloudbees/multi-cluster-gke/#1-create-cluster-with-terraform","title":"1. Create Cluster With Terraform","text":"<p>We create four files:</p> <ul> <li><code>main.tf</code> containing the provider information</li> <li><code>variables.tf</code> containg the variables (and their defaults)</li> <li><code>cluster.tf</code> cluster definition</li> <li><code>nodepool.tf</code>houses our node pool definition, currently one, but should be easy to add more if desired</li> </ul>"},{"location":"cloudbees/multi-cluster-gke/#main","title":"Main","text":"<p>main.tf</p> <pre><code>terraform {\n  required_version = \"~&gt; 0.12\"\n}\n\n# https://www.terraform.io/docs/providers/google/index.html\nprovider \"google\" {\n  version   = \"~&gt; 2.18.1\"\n  project   = var.project\n  region    = var.location\n}\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#variables","title":"Variables","text":"<p>variables.tf</p> <pre><code>variable \"project\" { }\n\nvariable \"name\" {\n  description = \"The name of the cluster (required)\"\n  default     = \"jvandergriendt_cluster_one\"\n}\n\nvariable \"description\" {\n  description = \"The description of the cluster\"\n  default     = \"CloudBees CI environment for jvandergriendt\"\n}\n\nvariable \"location\" {\n  description = \"The location to host the cluster\"\n  default     = \"europe-west4\"\n}\n\nvariable \"cluster_master_version\" {\n  description = \"The minimum kubernetes version for the master nodes\"\n  default     = \"1.15.9-gke.8\"\n}\n\nvariable \"np1count\" {\n  description = \"The initial Node Count for NodePool 2\"\n  default     = 1\n}\n\nvariable \"np1machinetype\" {\n  description = \"GCP Machine Type for Nodepool 1\"\n  default     = \"e2-standard-2\"\n}\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#cluster","title":"Cluster","text":"<p>cluster.tf</p> <pre><code>resource \"google_container_cluster\" \"primary\" {\n  name        = var.name\n  location    = var.location\n\n  # We can't create a cluster with no node pool defined, but we want to only use\n  # separately managed node pools. So we create the smallest possible default\n  # node pool and immediately delete it.\n  remove_default_node_pool  = true\n  initial_node_count        = 1\n  resource_labels           = {\n    environment = \"development\"\n    created-by  = \"terraform\"\n    cb-owner    = \"professional-services\"\n  }\n\n  # Configuration options for the NetworkPolicy feature.\n  network_policy {\n    # Whether network policy is enabled on the cluster. Defaults to false.\n    # In GKE this also enables the ip masquerade agent\n    # https://cloud.google.com/kubernetes-engine/docs/how-to/ip-masquerade-agent\n    enabled = true\n\n    # The selected network policy provider. Defaults to PROVIDER_UNSPECIFIED.\n    provider = \"CALICO\"\n  }\n}\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#nodepools","title":"Nodepools","text":"<p>nodepool.tf</p> <pre><code>resource \"google_container_node_pool\" \"nodepool1\" {\n  name       = \"pool1\"\n  location   = var.location\n  cluster    = google_container_cluster.primary.name\n  node_count = var.np1count\n\n  management {\n    auto_repair  = true\n    auto_upgrade = true\n  }\n\n  autoscaling {\n    min_node_count = 2\n    max_node_count = 5\n  }\n\n  node_config {\n    machine_type = var.np1machinetype\n    oauth_scopes = [\n      \"https://www.googleapis.com/auth/compute\",\n      \"https://www.googleapis.com/auth/devstorage.read_only\",\n      \"https://www.googleapis.com/auth/logging.write\",\n      \"https://www.googleapis.com/auth/monitoring\",\n      \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\",\n      \"https://www.googleapis.com/auth/cloud-platform\"\n    ]\n  }\n}\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#execute-terraform-steps","title":"Execute Terraform Steps","text":"Terraform initTerraform planTerraform apply <pre><code>terraform init\n</code></pre> <pre><code>terraform plan -out plan.out\n</code></pre> <pre><code>terraform apply \"plan.out\"\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#2-create-cloud-dns-zone","title":"2. Create Cloud DNS Zone","text":""},{"location":"cloudbees/multi-cluster-gke/#gcloud","title":"Gcloud","text":"<pre><code>gcloud dns managed-zones create one-domain-com --dns-name one.domain.com. \\\n      --description \"Example DNS zone\"\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#terraform","title":"Terraform","text":"<p>main.tf</p> <pre><code>resource \"google_dns_managed_zone\" \"one-domain-com\" {\n  name        = \"one-domain-com\"\n  dns_name    = \"one.domain.com.\"\n  description = \"Example DNS zone\"\n  labels = {\n    foo = \"bar\"\n  }\n}\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#create-gcp-service-account-for-accessing-cloud-dns","title":"Create GCP Service Account for accessing Cloud DNS","text":""},{"location":"cloudbees/multi-cluster-gke/#ui","title":"UI","text":"<p>In GCP's IAM/service Account screen, create a new Service Account.</p> <p>Give the service account two roles:</p> <ul> <li>Project Viewer</li> <li>DNS Admin</li> </ul> <p>Once done, create a JSON key and download this. Rename the file to <code>credentials.json</code>.</p>"},{"location":"cloudbees/multi-cluster-gke/#gcloud_1","title":"Gcloud","text":"Create SAGive SA Project PermissionGive CloudDNS PermissionGenerate JSON Key <pre><code>gcloud iam service-accounts create sa-name \\\n    --description=\"sa-description\" \\\n    --display-name=\"sa-display-name\"\n</code></pre> <pre><code>gcloud projects add-iam-policy-binding my-project-123 \\\n    --member serviceAccount:my-sa-123@my-project-123.iam.gserviceaccount.com \\\n    --role roles/viewer\n</code></pre> <pre><code>gcloud projects add-iam-policy-binding my-project-123 \\\n    --member serviceAccount:my-sa-123@my-project-123.iam.gserviceaccount.com \\\n    --role roles/dns.admin\n</code></pre> <pre><code>gcloud iam service-accounts keys create ~key.json \\\n  --iam-account &lt;YOUR-SA-NAME&gt;&gt;@project-id.iam.gserviceaccount.com\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#configure-credentials-in-cluster","title":"Configure Credentials In Cluster","text":"<p>Create the Kubernetes namespace <code>bootstrap</code> and add the credentials to the namespace.</p> <pre><code>kubectl create namespace bootstrap\n</code></pre> <pre><code>kubectl create secret generic external-dns-gcp-sa --from-file=credentials.json -n bootstrap\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#3-install-bootstrapping","title":"3. Install Bootstrapping","text":""},{"location":"cloudbees/multi-cluster-gke/#nginx-ingress","title":"Nginx Ingress","text":"<p>Important</p> <p>We assume Cluster One will be the cluster hosting Operations Center.</p> <p>This means Cluster One needs to expose the port <code>50000</code> via the LoadBalancer. Nginx can do this automatically, if we add a <code>tcp</code> configuration in the <code>values.yaml</code>.</p> <p>Cluster Two should not get this configuration as it will throw errors and polute your Nginx Ingress Controller logs. So be mindful that there are two different files and two different commands.</p> Add Helm Repo <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n</code></pre> <p>=== \"Update Helm repositories     <pre><code>helm repo update\n</code></pre></p> Helm Install Cluster OneHelm Install Cluster Two <pre><code>helm install ingress ingress-nginx/ingress-nginx -f ingress-values-c1.yaml\n</code></pre> <pre><code>helm install ingress ingress-nginx/ingress-nginx -f ingress-values-c2.yaml\n</code></pre> <p>ingress-values-c1.yaml</p> <pre><code>service:\n    omitClusterIP: true\ncontroller:\n  tcp:\n    configMapNamespace: default\n  autoscaling:\n    enabled: true\n    minReplicas: 3\n    maxReplicas: 7\n  publishService:\n    enabled: true\n  metrics:\n    enabled: true\ntcp: \n  50000: \"cloudbees-ci/cjoc:50000\"\n</code></pre> <p>ingress-values-c2.yaml</p> <pre><code>service:\n    omitClusterIP: true\ncontroller:\n  tcp:\n    configMapNamespace: default\n  autoscaling:\n    enabled: true\n    minReplicas: 3\n    maxReplicas: 7\n  publishService:\n    enabled: true\n  metrics:\n    enabled: true\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#certmanager","title":"Certmanager","text":"Install RepositoryUpdate Helm RepositoriesInstall Certmanager chart <pre><code>helm repo add jetstack https://charts.jetstack.io\n</code></pre> <pre><code>helm repo update\n</code></pre> <pre><code>helm install certmanager jetstack/cert-manager \\\n  -f certmanager-values.yaml \\\n  -n bootstrap \\\n   --version v0.15.1 \\\n   --set installCRDs=true\n</code></pre> <p>certmanager-values.yaml</p> <pre><code>prometheus:\n  enabled: true\ningressShim:\n  defaultIssuerName: letsencrypt-prod\n  defaultIssuerKind: ClusterIssuer\n</code></pre> <p>cluster-issuer.yaml</p> <pre><code>apiVersion: cert-manager.io/v1alpha2\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    email: joostvdg@gmail.com\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    server: https://acme-v02.api.letsencrypt.org/directory\n    solvers:\n    - dns01:\n        clouddns:\n          project: GCP_PROJECT_ONE\n          serviceAccountSecretRef:\n            key: credentials.json\n            name: external-dns-gcp-sa\n</code></pre> <pre><code>kubectl apply -f cluster-issuer.yaml\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#external-dns-controller","title":"External DNS Controller","text":"<p>extdns-values.yaml</p> <pre><code>provider: google\ndomainFilters:\n  - \"one.domain.com\"\nmetrics:\n  enabled: true\ngoogle:\n  project: GCP_PROJECT_ONE\n  serviceAccountSecret: external-dns-gcp-sa\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#4-install-cloudbees-ci","title":"4. Install CloudBees CI","text":"<p>cloudbees-ci-values.yaml</p> <pre><code>OperationsCenter:\n  HostName: ci.one.domain.com\n  Ingress:\n    tls:\n      Enable: true\n      Host: ci.dev.jxgke.kearos.net\n      SecretName: tls-dev-jxgke-kearos-net-p\n  ServiceType: ClusterIP\n</code></pre> Add Helm RepoUpdate Helm ReposCreate NamespaceHelm Install <pre><code>helm repo add cloudbees https://charts.cloudbees.com/public/cloudbees\n</code></pre> <pre><code>helm repo update\n</code></pre> <pre><code>kubectl create namespace cloudbees-ci\n</code></pre> <pre><code>helm install cloudbees-ci cloudbees/cloudbees-core \\\n  -f cloudbees-ci-values.yaml \\\n  --namespace cloudbees-ci \\\n  --version 3.14.0+ebfb4625ad50\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#5-manually-configure-external-access","title":"5. Manually Configure External Access","text":"<p>Caution</p> <p>This configuration is only required if you do not use the Nginx Ingress Helm Chart.</p> <p>If you do, this is already configured for you in the Nginx Ingress paragraph earlier.</p> <p>If you're curious how to configure this manually, read on!</p> <p>To allow Master to connect back to Operations Center from another cluster, two ports must be open, <code>443</code> (or <code>80</code> if no TLS) and <code>50000</code> (if you've kept the default).</p> <p>For this we enable <code>tcp-services</code> and configure the <code>50000</code> port on the deployment and service (Kubernetes resources) of our Nginx Ingress.</p> <p>How to change the Nginx Ingress configuration depends on how you've installed it. For sake of brevity, I will simply edit the resources via a <code>kubectl edit</code> (hint: don't do this in production).</p>"},{"location":"cloudbees/multi-cluster-gke/#tcp-services-configmap","title":"TCP Services ConfigMap","text":"<p>To enable the TCP services, we need to take three steps:</p> <ul> <li>create a config map with the configuration</li> <li>enable tcp services in the deployment and point to the configmap</li> <li>update the service to listen to port <code>50000</code></li> </ul> <p>nginx-config-map.yaml</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tcp-services\n  namespace: kube-system\ndata:\n  50000: cloudbees-ci/cjoc:50000\"\n</code></pre> <p>Info</p> <p><code>cloudbees-ci</code> is the namespace my CJOC runs in.</p> <p>Change this if your CJOC runs elsewhere.</p> <pre><code>kubectl apply -f nginx-config-map.yaml\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#update-nginx-deployment","title":"Update Nginx Deployment","text":"<p>Once this is done, we can edit the controller's deployment.</p> <p>We have to make two changes:</p> <ul> <li>add the tcp-services coniguration</li> <li>add the port <code>50000</code> configuration</li> </ul> <pre><code>kubectl -n ${NAMESPACE} edit deployment nginx-ingress-controller\n</code></pre> <p>Among the container args, we add <code>--tcp-services-configmap</code>.</p> <pre><code>    spec:\n      containers:\n      - args:\n        - /nginx-ingress-controller\n        ...\n        - --tcp-services-configmap=kube-system/tcp-services\n</code></pre> <p>Next, we edit the same deployment, and add the tcp port.</p> <p>We will add the 50000 to the container ports.</p> <pre><code>        - containerPort: 50000\n          name: jnlp\n          protocol: TCP\n</code></pre> <p>It will then look something like this.</p> <pre><code>        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        - containerPort: 50000\n          name: jnlp\n          protocol: TCP\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#update-nginx-service","title":"Update Nginx Service","text":"<p>Last but not least, we also have to update the Nginx Ingress service resource.</p> <pre><code>kubectl -n $NAMESPACE edit svc nginx-ingress-controller\n</code></pre> <p>We add a similar port definition.</p> <pre><code>  - name: jnlp\n    nodePort: 31559\n    port: 50000\n    protocol: TCP\n    targetPort: jnlp\n</code></pre> <p>Which will then look something like this (clusterIp and nodePort's will be different).</p> <pre><code>spec:\n  clusterIP: .....\n  externalTrafficPolicy: Cluster\n  ports:\n  - name: http\n    nodePort: 31406\n    port: 80\n    protocol: TCP\n    targetPort: http\n  - name: https\n    nodePort: 30391\n    port: 443\n    protocol: TCP\n    targetPort: https\n  - name: jnlp\n    nodePort: 31559\n    port: 50000\n    protocol: TCP\n    targetPort: jnlp\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#create-and-configure-clustertwo","title":"Create and Configure ClusterTwo","text":"<p>Creating Cluster Two is similar to the creation of Cluster One. We re-use some steps of the Creation Cluster One segment, and below are the new steps. Do ensure to replace the placeholders for the cluster name, project id, and domain with the appropriate values for Cluster Two.</p> <p>To create and configure Cluster Two, we execute the following steps:</p> <ol> <li>Create Cluster With Terraform see step 1</li> <li>Create Cloud DNS Zone see step 2</li> <li>Install Bootstrapping services see step 3</li> <li>Create GCP service account for accessing GKE</li> <li>Configure Receiving Namespace</li> </ol>"},{"location":"cloudbees/multi-cluster-gke/#4-create-gcp-service-account-for-accessing-gke","title":"4. Create GCP service account for accessing GKE","text":""},{"location":"cloudbees/multi-cluster-gke/#ui_1","title":"UI","text":"<p>Open the Hamburger Menu on the top left, and select <code>IAM &amp; Admin</code> \u2192 <code>Service Accounts</code> -&gt; <code>CREATE SERVICE ACCOUNT</code> (top middle).</p> <p>Give it a descriptive name and two roles:</p> <ul> <li><code>Project / Viewer</code></li> <li><code>Kubernetes Engine Cluster Admin</code> (or, <code>Kubernetes Engine Developer</code>)</li> </ul> <p></p> <p>Then hit the <code>+ CREATE KEY</code> button, and select (and download) the JSON key.</p> <p></p>"},{"location":"cloudbees/multi-cluster-gke/#gcloud_2","title":"Gcloud","text":"<p>If you prefer to create the SA via Gcloud CLI:</p> Create SAGive SA Project PermissionGive GKE PermissionGenerate JSON Key <pre><code>gcloud iam service-accounts create sa-name \\\n    --description=\"sa-description\" \\\n    --display-name=\"sa-display-name\"\n</code></pre> <pre><code>gcloud projects add-iam-policy-binding my-project-123 \\\n    --member serviceAccount:my-sa-123@my-project-123.iam.gserviceaccount.com \\\n    --role roles/viewer\n</code></pre> <pre><code>gcloud projects add-iam-policy-binding my-project-123 \\\n    --member serviceAccount:my-sa-123@my-project-123.iam.gserviceaccount.com \\\n    --role roles/container.clusterAdmin # or roles/container.developer\n</code></pre> <pre><code>gcloud iam service-accounts keys create ~key.json \\\n  --iam-account &lt;YOUR-SA-NAME&gt;&gt;@project-id.iam.gserviceaccount.com\n</code></pre> <p>Read Google Cloud's documentation on Roles for further information.</p>"},{"location":"cloudbees/multi-cluster-gke/#5-configure-receiving-namespace","title":"5. Configure Receiving Namespace","text":""},{"location":"cloudbees/multi-cluster-gke/#prepare-namespace","title":"Prepare Namespace","text":"Create NamespaceFetch Helm ChartGenerate Namespace ManifestsApply Namespace Manifests <pre><code>kubectl create namespace masters\n</code></pre> <pre><code>helm fetch cloudbees/cloudbees-core --untar\n</code></pre> <pre><code>helm template cloudbees-core-masters-masters cloudbees-core \\\n   --namespace masters \\\n   -f namespace-masters-values.yaml \\\n   &gt; namespace-masters.yaml\n</code></pre> <pre><code>kubectl apply -f namespace-masters.yaml \\\n  --namespace masters\n</code></pre> <p>Caution</p> <p>Make sure you are executing the steps on Cluster Two!</p> <p>namespace-masters-values.yaml</p> <pre><code>OperationsCenter:\n    Enabled: false\nMaster:\n    Enabled: true\n    OperationsCenterNamespace: masters\nAgents:\n    Enabled: true\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#give-user-admin-rights","title":"Give User Admin Rights","text":"<p>I'm not sure if you can figure out the user id upfront. When you create a Master in this cluster, you'll likely see an error that <code>user 98389745879 has no permission</code> to create certain resources, such as the StatefulSet.</p> <p>Run the command below to give this user permission to create these Kubernetes resources.</p> <pre><code>kubectl create clusterrolebinding clusterrolebinding-name \\\n--clusterrole cluster-admin \\\n--user $USER_ID\n</code></pre> <p>If you don't know the user id of your GCP Service Account, go ahead with the next chapter. We refer back to this paragraph.</p>"},{"location":"cloudbees/multi-cluster-gke/#create-managed-master-in-both-clusters","title":"Create Managed Master in both clusters","text":"<ol> <li>Create CloudBees CI Credential with GCP Service Account</li> <li>Create second Kubernetes endpoint configuration</li> <li>Create Managed Master in Cluster One</li> <li>Create Managed Master in Cluster Two</li> </ol>"},{"location":"cloudbees/multi-cluster-gke/#create-cloudbees-ci-credential-with-gcp-service-account","title":"Create CloudBees CI Credential with GCP Service Account","text":"<p>This requires that you've created the GCP Service Account with access to GKE and generated the JSON Key.</p> <p>You can then upload this JSON Key as a credential to CloudBees CI via the Credentials kind <code>Google Service Account from private key</code>.</p> <p></p>"},{"location":"cloudbees/multi-cluster-gke/#create-second-kubernetes-endpoint-configuration","title":"Create second Kubernetes endpoint configuration","text":"<p>Now we create a second Kubernetes Endpoint in the cluster.</p> <p>The fields to fill in:</p> <ol> <li>API endpoint URL: the cluster endpoint of Cluster Two</li> <li>Display Name: display name so you know which cluster you create a Master in</li> <li>Credentials: the credentials we create in #Create CloudBees CI Credential with GCP Service Account</li> <li>Server Certificate: the server certificate of Cluster Two</li> <li>Namespace: the namespace you have prepared, in our case <code>masters</code></li> <li>Master URL Pattern: the DNS pattern for Cluster Two, in our case <code>https://*.two.domain.com</code></li> <li>Ingress Class: unless you used a different Ingress than in this guide, set to <code>nginx</code></li> <li>Jenkins URL: the external URL of the Operations Center</li> </ol> <p>Info</p> <p>If you don't know the API Endpoint or the Server Certificate, you can retrieve this from your <code>~/.kube/config</code> file. Assuming you have access to Cluster Two, you will the details of the cluster there.</p> <p>For the Server Certificate, you have to Base64 decode this. You can do so on mac/linux by echo + pipe into <code>base64</code>.</p> <p>For example, on a mac:</p> <pre><code>echo \"kjhKJSDH13123\" | base64 -D\n</code></pre> <p></p>"},{"location":"cloudbees/multi-cluster-gke/#create-managed-masters","title":"Create Managed Masters","text":"<p>To enable automatic TLS for the Masters in the clusters, you can either edit the Ingress when creating the Mater, or update the Kubernetes Master Provisioning before hand.</p>"},{"location":"cloudbees/multi-cluster-gke/#update-master-provisioning","title":"Update Master Provisioning","text":"<ul> <li><code>Manage Jenkins</code> -&gt; <code>Configure System</code> -&gt; <code>Kubernetes Master Provisioning</code> -&gt; <code>Advanced</code> -&gt; <code>YAML</code></li> </ul> <p>YAML field</p> <pre><code>apiVersion: \"extensions/v1beta1\"\nkind: \"Ingress\"\nmetadata:\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  tls:\n  - hosts:\n    - ${host}\n    secretName: ${secretName}\n</code></pre>"},{"location":"cloudbees/multi-cluster-gke/#creating-master","title":"Creating Master","text":"<p>In the top right corner, there's a button \"New Master\", push it and give your master a name.</p> <p>In the field <code>Cluster Endpoint</code>, select the one for Cluster Two. Confirm the URL has the correct domain and hit save.</p> <p></p> <p>Info</p> <p>The process for creating a Master in Cluster One and Cluster Two are the same. The only difference is the Kubenernetes endpoint used.</p>"},{"location":"cloudbees/multi-cluster-gke/#give-user-permissions-in-cluster-two","title":"Give User Permissions In Cluster Two","text":"<p>As discussed in the Create Cluster Two chapter, the paragraph #Give User Admin Rights explains how to do so.</p>"},{"location":"cloudbees/multi-cluster-temp/","title":"CloudBees Core On Multiple Clusters","text":"<ul> <li>https://docs.cloudbees.com/docs/cloudbees-core/latest/cloud-admin-guide/multiple-clusters</li> <li>https://docs.microsoft.com/en-us/azure/aks/ingress-static-ip</li> </ul>"},{"location":"cloudbees/multi-cluster-temp/#pre-requisites","title":"Pre-Requisites","text":"<ul> <li>GKE Cluster: Primary</li> <li>AKS Cluster: Secondary</li> </ul>"},{"location":"cloudbees/multi-cluster-temp/#configure-primary-cluster","title":"Configure Primary Cluster","text":"<ul> <li>create cluster</li> <li>retrieve account token</li> <li>retrieve Kubernetes API Endpoint</li> <li>retrieve Kubernetes Root CA<ul> <li>hint -&gt; <code>echo \" \" | base64 -D</code></li> </ul> </li> <li>install CloudBees Core via Helm (or Jenkins X)</li> </ul>"},{"location":"cloudbees/multi-cluster-temp/#configure-secondary-cluster","title":"Configure Secondary Cluster","text":"<ul> <li>https://docs.microsoft.com/en-us/azure/aks/ingress-tls</li> </ul>"},{"location":"cloudbees/multi-cluster-temp/#install-ingress-controller","title":"Install Ingress Controller","text":"<p>Create a namespace for your ingress resources</p> <p>Add the official stable repository</p> <pre><code>helm repo add stable https://kubernetes-charts.storage.googleapis.com/\n</code></pre> <p>Use Helm to deploy an NGINX ingress controller</p> <p>Create a values file, <code>ingress-values.yaml</code>.</p> <p>The reason is stated as this:</p> <p>Since version 0.22.1 of stable/nginx-ingress chart, ClusterRole and ClusterRoleBinding are not created automatically when the controller scope is enabled. They are required for this functionality to work. To use the controller scope feature, see the article Helm install of stable/nginx-ingress fails to deploy the Ingress Controller. </p> <pre><code>rbac:\n  create: true\ndefaultBackend:\n  enabled: false\ncontroller:\n  ingressClass: \"nginx\"\n  metrics:\n    enabled: \"true\"\n  replicaCount: 2\n  nodeSelector: \n    beta\\.kubernetes.io/os: linux \n  scope:\n    enabled: \"true\"\n    namespace: cbmasters\n  service:\n    externalTrafficPolicy: \"Cluster\"\n</code></pre> <pre><code>kubectl create namespace ingress-nginx\n</code></pre> <p>```bash tab=\"Helm V3\" helm install nginx-ingress stable/nginx-ingress \\     --namespace ingress-nginx \\     --values ingress-values.yaml \\     --version  1.29.6 <pre><code>```bash tab=\"Helm V2\"\nhelm install \\\n    --name nginx-ingress stable/nginx-ingress \\\n    --namespace ingress-nginx \\\n    --values ingress-values.yaml  \\\n    --version  1.29.6\n</code></pre></p>"},{"location":"cloudbees/multi-cluster-temp/#certmanager","title":"Certmanager","text":"<p>Install the CustomResourceDefinition resources separately</p> <pre><code>kubectl apply --validate=false \\\n    -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.13/deploy/manifests/00-crds.yaml\n</code></pre> <p>Label the ingress-basic namespace to disable resource validation</p> <pre><code>kubectl label namespace ingress-basic certmanager.k8s.io/disable-validation=true\n</code></pre> <p>Add the Jetstack Helm repository</p> <pre><code>helm repo add jetstack https://charts.jetstack.io\n</code></pre> <p>Update your local Helm chart repository cache</p> <pre><code>helm repo update\n</code></pre> <p>Create a <code>certmanager-values.yaml</code> file.</p> <pre><code>ingressShim:\n    defaultIssuerName: letsencrypt\n    defaultIssuerKind: ClusterIssuer\n</code></pre> <p>Install the cert-manager Helm chart.</p> <p>```bash tab=\"Helm V3\" helm install cert-manager \\   --namespace cert-manager \\   --version v0.13.0 \\   --values certmanager-values.yaml \\   jetstack/cert-manager <pre><code>```bash tab=\"Helm V2\"\nhelm install \\\n  --name cert-manager \\\n  --namespace cert-manager \\\n  --version v0.13.0 \\\n  --values certmanager-values.yaml \\\n  jetstack/cert-manager\n</code></pre></p>"},{"location":"cloudbees/multi-cluster-temp/#configure-certificate-issuer","title":"Configure Certificate Issuer","text":"<pre><code>apiVersion: cert-manager.io/v1alpha2\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: MY_EMAIL_ADDRESS\n    privateKeySecretRef:\n      name: letsencrypt\n    solvers:\n    - http01:\n        ingress:\n          class: nginx\n</code></pre> <pre><code>kubectl apply -f cluster-issuer.yaml --namespace ingress-basic\n</code></pre>"},{"location":"cloudbees/multi-cluster-temp/#configure-certificate","title":"Configure Certificate","text":"<p>Todo</p>"},{"location":"cloudbees/multi-cluster-temp/#prepare-receiving-namespace","title":"Prepare Receiving Namespace","text":"<pre><code>NAMESPACE=\n</code></pre> <pre><code>kubectl create namespace $NAMESPACE\n</code></pre> <p>Create a configuration for CloudBees Core Helm chart, called <code>cloudbees-values.yaml</code>.</p> <pre><code>OperationsCenter:\n    Enabled: false\nMaster:\n    Enabled: true\n    OperationsCenterNamespace: jx-staging\nAgents:\n    Enabled: true\n ```\n\n\n```bash\nhelm fetch \\\n --repo https://charts.cloudbees.com/public/cloudbees \\\n --version 3.8.0+a0d07461ae1c \\\n cloudbees-core\n</code></pre> <pre><code>helm template cloudbees-core-namespace \\\n cloudbees-core-3.8.0+a0d07461ae1c.tgz \\\n -f cloudbees-values.yaml \\\n --namespace ${NAMESPACE} \\\n &gt; cloudbees-core-namespace.yml\n</code></pre> <pre><code>kubectl apply -f cloudbees-core-namespace.yml --namespace ${NAMESPACE}\n</code></pre>"},{"location":"cloudbees/multi-cluster-temp/#configure-cloudbees-core","title":"Configure CloudBees Core","text":"<ul> <li>Client Secret -&gt; <code>secret text</code> -&gt; the token from Terraforms output</li> <li></li> </ul>"},{"location":"cloudbees/multi-cluster-temp/#configre-siodecar-injector","title":"Configre Siodecar Injector","text":"<pre><code>helm fetch \\\n --repo https://charts.cloudbees.com/public/cloudbees \\\n --version 2.0.1 \\\n    cloudbees-sidecar-injector\n</code></pre> <p>Untar the config, and update the configmap -&gt; <code>templates/configmap.yaml</code>. Set <code>requiresExplicitInjection</code> to true.</p> <pre><code>    requiresExplicitInjection: true\n</code></pre> <p>Generate the end result:</p> <pre><code>helm template cloudbees-sidecar-injector \\\n cloudbees-sidecar-injector \\\n --namespace  cloudbees-sidecar-injector\\\n &gt; cloudbees-sidecar-injector.yml\n</code></pre> <p>And apply the file.</p> <pre><code>kubectl apply -f cloudbees-sidecar-injector.yml\n</code></pre>"},{"location":"cloudbees/multi-cluster-temp/#debugging-ssl-issue","title":"Debugging SSL Issue","text":"<p>Add System properties: <pre><code>javax.net.ssl.trustStore=/etc/ssl/certs/java/cacerts\njavax.net.ssl.trustStorePassword=changeit\njavax.net.debug=SSL,trustmanager\n</code></pre></p> <p>Add annotation <code>com.cloudbees.sidecar-injector/inject: yes</code>, and change this in the master configuration.</p> <pre><code>apiVersion: \"apps/v1\"\nkind: \"StatefulSet\"\nspec:\n  template:\n    metadata:\n      annotations:\n        prometheus.io/path: /${name}/prometheus\n        prometheus.io/port: \"8080\"\n        prometheus.io/scrape: \"true\"\n        com.cloudbees.sidecar-injector/inject: yes\n      labels:\n        app.kubernetes.io/component: Managed-Master\n        app.kubernetes.io/instance: ${name}\n        app.kubernetes.io/managed-by: CloudBees-Core-Cloud-Operations-Center\n        app.kubernetes.io/name: ${name}\n</code></pre> <pre><code>trustStore is: /etc/ssl/certs/java/cacerts\ntrustStore type is: jks\ntrustStore provider is:\n</code></pre> <p>----- BEGIN CONNECTION DETAILS ----- H4sIAAAAAAAAAA3KQQ7CIBBA0bvMWqBMKLS9zTAi1lowMF0Z7y6bn7zkfyE3KgIbLC56Gx6sJhtn 5XBFFe0IYiA3zeviA8MN9vt4UZ0n+aGrvQefIp++GcO1Jd2F8l6yzkfSR6JWuy5JDL8qG/j9ATek aGdwAAAA ----- END CONNECTION DETAILS -----</p> <p>Problem is, Jenkins uses an outdated SSL library (openSSL) that doesn't support SNI (servername). This cause Nginx Ingress Controller to return an invalid certificate: <code>Issuer: CN=Kubernetes Ingress Controller Fake Certificate, O=Acme Co</code>. As this is not a CA cert, it cannot be directly trusted.</p>"},{"location":"cloudbees/multi-cluster-temp/#minica-solution","title":"MiniCA Solution","text":"<p>The gist:  * create custom CA with minica * generate wildcard certificate for primary domain * set wildcard cert as <code>default ssl certificate</code>         * <code>- --default-ssl-certificate=default/cloudbees-core.kearos.net-tls</code> * add custom CA to <code>cacerts</code> truststore and <code>ca-certificates.cert</code> * update <code>ca-bundles</code> configmap solution with sidecar injector</p> <pre><code>minica --domains kearos.net\n</code></pre> <pre><code>cat minica.pem &gt;&gt; ca-certificates.crt\nkeytool -import -noprompt -keystore cacerts -file minica.pem -storepass changeit -alias kearos-net;\n</code></pre> <pre><code>kubectl create configmap --from-file=ca-certificates.crt,cacerts ca-bundles\n</code></pre> <pre><code>kubectl create secret tls tls-fake-kearos-net --key ./kearos.net/key.pem --cert ./kearos.net/cert.pem --namespace default\n</code></pre> <p><code>kubectl edit deployment nginx-ingress-controller</code></p> <pre><code>spec:\n    containers:\n    - args:\n        - /nginx-ingress-controller\n        - --configmap=$(POD_NAMESPACE)/nginx-configuration\n        - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n        - --udp-services-configmap=$(POD_NAMESPACE)/udp-services\n        - --publish-service=$(POD_NAMESPACE)/ingress-nginx\n        - --annotations-prefix=nginx.ingress.kubernetes.io\n        - --default-ssl-certificate=default/tls-fake-kearos-net\n</code></pre> <pre><code>kubectl create configmap --from-file=ca-certificates.crt,cacerts ca-bundles\n</code></pre> <p>Change <code>cjoc-0</code> ingress:</p> <p>Add</p> <pre><code>      - backend:\n          serviceName: cjoc\n          servicePort: 80\n        path: /\n</code></pre> <p>Remove:</p> <pre><code>metadata:\n  annotations:\n    nginx.ingress.kubernetes.io/app-root: https://$best_http_host/cjoc/teams-check/\n</code></pre>"},{"location":"cloudbees/multi-cluster-temp/#fix-port-50000-issue","title":"Fix Port 50000 issue","text":"<pre><code># nginx-config-map.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tcp-services\n  namespace: kube-system\ndata:\n  50000: \"jx-staging/cjoc:50000\"\n</code></pre> <pre><code>kubectl apply -f nginx-config-map.yaml\n</code></pre> <pre><code>- containerPort: 50000\n    name: jnlp\n    protocol: TCP\n</code></pre> <p>And in its args, add <code>--tcp-services-configmap</code> and point to the tcp-services configmap you created.</p> <pre><code>args: \n    ...\n   - --tcp-services-configmap=kube-system/tcp-services\n</code></pre> <pre><code>kubectl edit -n kube-system deployment jxing-nginx-ingress-default-backend\n</code></pre> <ul> <li>https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/</li> </ul> <pre><code>- name: jnlp\n    port: 50000\n    protocol: TCP\n    targetPort: jnlp\n</code></pre> <pre><code>kubectl edit -n kube-system svc jxing-nginx-ingress-controller\n</code></pre>"},{"location":"cloudbees/multi-cluster-temp/#ticket-for-core-v2","title":"Ticket for Core V2","text":"<p>In the 2.204 Operations Center image, the location of the truststore (<code>cacerts</code>) and the ca certificate bundle ( <code>ca-certificates.crt</code>) has changed. In general there isn't a direct issue, but we mention it in a lot of places. This should be checked, propably tested and communicated to docs.</p>"},{"location":"cloudbees/multi-cluster-temp/#kbs","title":"KBs","text":"<ul> <li>https://support.cloudbees.com/hc/en-us/articles/360018267271</li> <li>https://support.cloudbees.com/hc/en-us/articles/360018094412-Deploy-Self-Signed-Certificates-in-Masters-and-Agents-Custom-Location-</li> </ul>"},{"location":"cloudbees/multi-cluster-temp/#core-docs","title":"Core Docs","text":"<ul> <li>https://docs.cloudbees.com/docs/cloudbees-core/latest/cloud-admin-guide/kubernetes-self-signed-certificates</li> </ul>"},{"location":"cloudbees/multi-cluster-temp/#sidecar-injector","title":"Sidecar Injector","text":"<ul> <li>https://github.com/cloudbees/sidecar-injector/blob/master/charts/cloudbees-sidecar-injector/README.md#create-a-certificate-bundle</li> <li>https://github.com/cloudbees/sidecar-injector/blob/c965497a51bc68f6dc6df8e9aef2403819f7902f/charts/cloudbees-sidecar-injector/values.yaml</li> </ul>"},{"location":"cloudbees/multi-cluster/","title":"Multi-Cluster (OLD/AKS)","text":"<p>Imagine you have a whole range of departments and development teams. Preferably you want to serve them with a standardized SDA(Software Delivery Automation) platform, but at the same time, make sure they pay for their usage.</p> <p>I don't think this is too far fetched or something terrible. I think it makes sense. In this light, CloudBees Core now supports running on multiple Kubernetes clusters. </p> <p>In this guide, we're going to explore how to run CloudBees Core on a GKE and AKS cluster at the same time. And how to deal with the details of getting the namespace configured, certificates, Kubernetes API tokens and so on and so forth.</p>"},{"location":"cloudbees/multi-cluster/#prerequisites","title":"Prerequisites","text":"<p>First off, we need a [GKE cluster] (/kubernetes/distributions/gke-terraform/) and a AKS cluster.</p> <p>Next, we make the GKE cluster the default one. Here we will install CloudBees Core. You can either install CloudBees Core directly or via Jenkins X.</p> <p>You're also welcome to follow the CloudBees documentation.</p> <p>The expected state is that you access Operations Center via a proper DNS name with TLS enabled, served via Nginx Ingress. And the AKS cluster is ready to be used - but empty.</p> <p>Some further assumptions: you have admin rights on both clusters the TLS certificate is from Let's Encrypt via Certmanager the AKS cluster is empty You can break these assumptions, but that means you have to change the examples to reflect your situation.</p>"},{"location":"cloudbees/multi-cluster/#steps-to-do","title":"Steps to do","text":"<p>High over, we have to do the following steps:</p> <ul> <li>tweak the GKE cluster</li> <li>configure the AKS cluster</li> <li>configure Operations Center</li> <li>create a Managed Master</li> </ul>"},{"location":"cloudbees/multi-cluster/#tweak-gke-cluster","title":"Tweak GKE Cluster","text":"<p>At this time of writing, Jenkins is suffering from an outdated OpenSSL library. This version doesn't support SNI, which is what the Nginx Ingress Controller relies on.</p> <p>When a Client Master (Managed or not) connects to the Operations Center via the Nginx Controller without SNI, the controller will not route you to the correct path. Because of this, when TLS is enabled, you will get the default controller's TLS certificate. As this certificate, by default, is an invalid one, you get TLS handshake errors.</p> <p>We have to rectify this. There are many ways to do this. For example, you make the TLS certificate for Operations Center the default TLS certificate. While this might not be nice for other applications, it will suit our case.</p> <p>Info</p> <p>If there are many other applications in the same cluster, consider having more than one Nginx Ingress Controller.</p> <p>Via the configuration parameters <code>controller.scope.enabled</code> and <code>controller.scope.namespace</code> , you can limit the controller to a single namespace. This way, you can configure the default TLS certificate only for the Ingress Controller that manages the Operations Center's namespace \u2014 limiting the effect of the workaround.</p> <p>In addition to the change for the default certificate, we also need to make sure external Masters can connect to the Operations Center. They require two ports to be open, <code>443</code> (or <code>80</code> if no TLS) and <code>50000</code> (if you've kept the default). </p> <p>For this we enable <code>tcp-services</code> and configure the <code>50000</code> port on the deployment and service (Kubernetes resources).</p> <p>How to change the Nginx Ingress configuration depends on how you've installed it. For sake of brevity, I will simply edit the resources via a <code>kubectl edit</code> (hint: don't do this in production).</p>"},{"location":"cloudbees/multi-cluster/#tcp-services","title":"TCP Services","text":"<p>To enable the TCP services, we need to do two things: * create a config map with the configuration * enable tcp services in the deployment and point to the configmap</p> <pre><code># nginx-config-map.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n name: tcp-services\n namespace: kube-system\ndata:\n 50000: \"&lt;CJOC_NAMESPACE&gt;/cjoc:50000\"\n</code></pre> <p>Info</p> <p>Do replace <code>CJOC_NAMESPACE</code> with the actual namespace!</p> <pre><code>kubectl apply -f nginx-config-map.yaml\n</code></pre> <p>Once this is done, we can edit the controller's deployment.</p> <p>We have to make two changes: * add the tcp-services coniguration * add the port <code>50000</code> configuration</p> <p><pre><code>kubectl -n ${NAMESPACE} edit deployment nginx-ingress-controller\n</code></pre> Among the container args, we add <code>--tcp-services-configmap</code>.</p> <pre><code>    spec:\n      containers:\n      - args:\n        - /nginx-ingress-controller\n        ...\n        - --tcp-services-configmap=kube-system/tcp-services\n</code></pre> <p>Next, we edit the same deployment, and add the tcp port.</p> <p>We will add the 50000 to the container ports.</p> <pre><code>        - containerPort: 50000\n          name: jnlp\n          protocol: TCP\n</code></pre> <p>It will then look something like this.</p> <pre><code>        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        - containerPort: 50000\n          name: jnlp\n          protocol: TCP\n</code></pre> <p>Last but not least, we also have to update the Nginx Ingress service  resource.</p> <pre><code>kubectl -n $NAMESPACE edit svc nginx-ingress-controller\n</code></pre> <p>We add a similar port definition.</p> <pre><code>  - name: jnlp\n    nodePort: 31559\n    port: 50000\n    protocol: TCP\n    targetPort: jnlp\n</code></pre> <p>Which will then look something like this (clusterIp and nodePort's will be different).</p> <pre><code>spec:\n  clusterIP: .....\n  externalTrafficPolicy: Cluster\n  ports:\n  - name: http\n    nodePort: 31406\n    port: 80\n    protocol: TCP\n    targetPort: http\n  - name: https\n    nodePort: 30391\n    port: 443\n    protocol: TCP\n    targetPort: https\n  - name: jnlp\n    nodePort: 31559\n    port: 50000\n    protocol: TCP\n    targetPort: jnlp\n</code></pre>"},{"location":"cloudbees/multi-cluster/#set-default-tls-certificate","title":"Set Default TLS Certificate","text":"<p>The first thing you'll have to do is to find out the name of the secret container the certificate. If you've created it yourself, great, else this is a way.</p> <pre><code>kubectl get certificate -A\n</code></pre> <p>The output should look like this:</p> <pre><code>NAMESPACE    NAME                           READY   SECRET                         AGE\njx-staging   tls-staging-gke-kearos-net-p   True    tls-staging-gke-kearos-net-p   4d\njx           tls-dev-gke-kearos-net-p       True    tls-dev-gke-kearos-net-p       15d\n</code></pre> <p>It should tell you which secret belongs to which certificate and in which namespace it resides. Note down both the secret name and the namespace.</p> <p>Info</p> <p>Hint, the <code>-A</code> flag means <code>--all-namespaces</code> but then with less typing.</p> <p><pre><code>kubectl -n ${NAMESPACE} edit deployment nginx-ingress-controller\n</code></pre> Among the container args, <code>- --default-ssl-certificate=&lt;namespace&gt;/&lt;secretName&gt;</code>.</p> <p>And it should then look something like this:</p> <pre><code>    spec:\n      containers:\n      - args:\n        - /nginx-ingress-controller\n    ...\n        - --default-ssl-certificate=default/tls-fake-kearos-net\n</code></pre>"},{"location":"cloudbees/multi-cluster/#configure-aks-cluster","title":"Configure AKS Cluster","text":"<p>We need to do the following steps:</p> <ul> <li>retrieve the Kubernetes API endpoint</li> <li>retrieve the Kubernetes API endpoint's certificate</li> <li>retrieve the service account token</li> <li>install an Ingress controller</li> <li>create &amp; configure namespace for Managed Master(s)</li> <li>configure CloudBees Sidecar Injector</li> </ul>"},{"location":"cloudbees/multi-cluster/#gather-kubernetes-credential-certificate","title":"Gather Kubernetes Credential &amp; Certificate","text":"<p>If you haven't already retrieved the Kubernetes credentials for your AKS cluster, you can do so via the <code>az</code> or AzureCLI.</p> <p><pre><code>az aks get-credentials --resource-group ${AKS_RESOURCE_GROUP} --name ${AKS_CLUSTER_NAME}\n</code></pre> All three parts that we need, the Kubernetes API endpoint,  Kubernetes API endpoint's certificate, Service Account Token will be added to our <code>.kubeconfig</code>. In my case, it is located at <code>~/.kube/config</code>.</p> <ul> <li>Kubernetes API endpoint: <code>cluster.server</code></li> <li>Kubernetes API endpoint's certificate: <code>cluster.certificate-authority-data</code></li> <li>Service Account Token: <code>users.user.token</code></li> </ul> <p>Mind you, the certificate is Base64 encoded. To decode, you can usually use a command line tool. </p> <p>```bash tab=\"macOs\" echo \"LS0tLS1C....LS0tCg==\" | base64 -D <pre><code>The decoded thing should look like this:\n\n```bash\n-----BEGIN CERTIFICATE-----\nMIIEyTCCArGgAwIBAgIQK0sOS0aRjfZPYLM1TaRQMjANBgkqhkiG9w0BAQsFADAN\n..........\nkKrPcnzV0gRdWNGNoJtRh9EGtKDP1VZUBiwdH44=\n-----END CERTIFICATE-----\n</code></pre></p> <p>Save this into a <code>.pem</code> file, we're going to need it. Let's call it <code>aks-kubernetes-api-server.pem</code>.</p>"},{"location":"cloudbees/multi-cluster/#install-an-ingress-controller","title":"Install an Ingress controller","text":"<p>We're going to use Helm to install the Nginx Ingres Controller.</p> <p>Let's start by creating a namespace for your ingress resources</p> <pre><code>kubectl create namespace ingress-nginx\n</code></pre> <p>Add the official stable repository, just to be sure.</p> <pre><code>helm repo add stable https://kubernetes-charts.storage.googleapis.com/\n</code></pre> <p>And update the helm repos.</p> <pre><code>helm repo update\n</code></pre> <p>Create a values file, <code>ingress-values.yaml</code>, we will need to change some values.</p> <pre><code>rbac:\n create: true\ndefaultBackend:\n enabled: false\ncontroller:\n ingressClass: \"nginx\"\n metrics:\n   enabled: \"true\"\n replicaCount: 2\n nodeSelector: \n   beta\\.kubernetes.io/os: linux \n scope:\n   enabled: \"true\"\n   namespace: cbmasters\n service:\n   externalTrafficPolicy: \"Cluster\"\n</code></pre> <p>```bash tab=\"Helm V3\" helm install nginx-ingress stable/nginx-ingress \\  --namespace ingress-nginx \\  --values ingress-values.yaml \\  --version 1.29.6 <pre><code>```bash tab=\"Helm V2\"\nhelm install \\\n --name nginx-ingress stable/nginx-ingress \\\n --namespace ingress-nginx \\\n --values ingress-values.yaml \\\n --version 1.29.6\n</code></pre></p>"},{"location":"cloudbees/multi-cluster/#create-configure-namespace","title":"Create &amp; Configure Namespace","text":"<p>We're going to use a feature from the official CloudBees Core helm chart. We start by creating the namespace that will house our Managed Master(s).</p> <pre><code>NAMESPACE=\n</code></pre> <pre><code>kubectl create namespace $NAMESPACE\n</code></pre> <p>Create a configuration for CloudBees Core Helm chart, called <code>cloudbees-values.yaml</code>. The namespace of the Operations Center doesn't really matter, it is in a different cluster anyway.</p> <pre><code>OperationsCenter:\n Enabled: false\nMaster:\n Enabled: true\n OperationsCenterNamespace: cloudbees-core\nAgents:\n Enabled: true\n</code></pre> <p>To be able to generate the Kubernetes resources files, we first have to use <code>helm fetch</code> to retrieve the chart.</p> <pre><code>helm fetch \\\n --repo https://charts.cloudbees.com/public/cloudbees \\\n --version 3.8.0+a0d07461ae1c \\\n cloudbees-core\n</code></pre> <p>Now we can generate the Kubernetes resource definitions and store them as a single yaml file.</p> <pre><code>helm template cloudbees-core-namespace \\\n cloudbees-core-3.8.0+a0d07461ae1c.tgz \\\n -f cloudbees-values.yaml \\\n --namespace ${NAMESPACE} \\\n &gt; cloudbees-core-namespace.yml\n</code></pre> <p>Which we then apply.</p> <pre><code>kubectl apply -f cloudbees-core-namespace.yml --namespace ${NAMESPACE}\n</code></pre>"},{"location":"cloudbees/multi-cluster/#configure-cloudbees-sidecar-injector","title":"Configure CloudBees Sidecar Injector","text":"<p>Unfortunately, the Let's Encrypt Root CA certificates are not yet in the trust stores of our images. So we have to ensure they end up in the Masters we run in this (AKS) cluster, so they do not run into SSL errors. You can find the certificates on the  certificates page of letsencrypt.org.</p> <p>Make sure to download and save the PEM of <code>ISRGRoot X1 (self-signed)</code>, <code>Let's Encrypt Authority X3 (IdenTrust Cross-signed)</code>, and <code>Let's Encrypt Authority X3 (Signed By ISRG Root X1)</code>.</p> <p>Next, we have to add them to a bundle and a truststore, we can then put into a ConfigMap.</p> <p>We first have retrieve the current bundle and truststore. We can get them from Operations Center via a <code>kubectl cp</code>. The process is described in the CloudBees Cloud Admin Guide. Unfortunately, it seems as of CloudBees Core 2.204.+ the location has changed. The new locations are below. </p> <pre><code>kubectl cp cjoc-0:etc/pki/ca-trust/extracted/java/cacerts ca-bundle/cacerts\nkubectl cp cjoc-0:etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem ca-bundle/ca-certificates.crt\n</code></pre> <p>Now that we have the files to start with, we can add our Let's Encrypt Root and Intermediary certificates.</p> <p>This assumes we're in the folder <code>ca-bundle</code>.</p> <pre><code>cat valid-isrgrootx1-letsencrypt-org.pem &gt;&gt; ca-certificates.crt\nkeytool -import -noprompt -keystore cacerts -file valid-isrgrootx1-letsencrypt-org.pem -storepass changeit -alias letsencrypt;\n</code></pre> <p>We also have to add the <code>aks-kubernetes-api-server.pem</code> to ensure we can talk to the Kubernetes API Endpoint.</p> <pre><code>cat aks-kubernetes-api-server.pem &gt;&gt; ca-certificates.crt\nkeytool -import -noprompt -keystore cacerts -file aks-kubernetes-api-server.pem -storepass changeit -alias kubeapi;\n</code></pre> <p>Info</p> <p>If you use custom or self-signed certificates, the process is the same. Simply substitute the Let's Encrypt certificates mentioned by your own / your provider's.</p> <p>This assumes we're in the folder <code>ca-bundle</code>.</p> <pre><code>kubectl create configmap --from-file=ca-certificates.crt,cacerts ca-bundles\n</code></pre> <p>Now we can focus on installing the CloudBees Sidecar Injector. It is a tool that creates a Sidecar to inject the certificate bundle and truststore from the Configmap into containers.</p> <p>As before, we first fetch chart. Unfortunately, the value we need to change is not parameterized, so we have to un tar the config. We can let the Helm fetch do that via the <code>--untar</code> flag, followed by the  <code>--untardir &lt;folderName&gt;</code> flag.</p> <pre><code>helm fetch \\\n --repo https://charts.cloudbees.com/public/cloudbees \\\n --version 2.0.1 \\\n--untar \\\n--untardir cloudbees-sidecar-injector\\\n cloudbees-sidecar-injector\n</code></pre> <p>Tip</p> <p>If you haven't done so already, configure the CloudBees Helm repo by the following commands.</p> <pre><code>helm repo add cloudbees https://charts.cloudbees.com/public/cloudbees\nhelm repo update\n</code></pre> <p>Update the configmap, in the file<code>templates/configmap.yaml</code>, set <code>requiresExplicitInjection</code> to true.</p> <pre><code> requiresExplicitInjection: true\n</code></pre> <p>Generate the end result via <code>helm template</code>.</p> <pre><code>helm template cloudbees-sidecar-injector \\\n cloudbees-sidecar-injector \\\n --namespace cloudbees-sidecar-injector\\\n &gt; cloudbees-sidecar-injector.yml\n</code></pre> <p>And apply the file.</p> <pre><code>kubectl apply -f cloudbees-sidecar-injector.yml\n</code></pre>"},{"location":"cloudbees/multi-cluster/#configure-operations-center","title":"Configure Operations Center","text":"<p>We have to configure two things in Operations Center. First, we create the AKS endpoint, and then we configure the Master template by adding a YAML snippet.</p> <p>We go to <code>Operations Center</code> -&gt; <code>Manage Jenkins</code> -&gt; <code>Configure System</code> -&gt; <code>Kubernetes Master Provisioning</code>.</p>"},{"location":"cloudbees/multi-cluster/#configure-endpoint","title":"Configure Endpoint","text":"<p>Every field is essential here.</p> <ul> <li>API endpoint URL: the Kubernetes API server url, you should have this saved somewhere (e.g. <code>~/.kube/config</code>)</li> <li>Display Name: the name by which you can refer to this endpoint</li> <li>Credentials: credentials of the Service Account that can access the Kubernetes API, you should have this saved somewhere (e.g. <code>~/.kube/config</code>, <code>users.user.token</code>)</li> <li>Server Certificate: the certificate of the Kubernetes API Server of the target cluster, copy paste the contents of <code>aks-kubernetes-api-server.pem</code></li> <li>Namespace: the namespace we created on the AKS cluster which is configured to be used by Operations Center</li> <li>Master URL Pattern: assuming that your domain name for the other cluster is different, make sure you have a pattern in here <code>http://ake.mydomain.com/*/</code> ensure all Master will have that host name and the Master name will replace the <code>*</code></li> <li>Jenkins URL: the URL of this Operations Center, you can leave this blank</li> </ul>"},{"location":"cloudbees/multi-cluster/#master-template","title":"Master Template","text":"<p>We can open up the Master template configuration by hitting the  <code>Advanced</code> button in the Kubernetes Master Provisioning block.</p> <p></p> <p>We have to add the annotation <code>com.cloudbees.sidecar-injector/inject: yes</code> to ensure the Masters will receive the certificates from the CloudBees Sidecar Injector - so they can talk to Operations Center.</p> <p>The full snippet becomes this:</p> <pre><code>apiVersion: \"apps/v1\"\nkind: \"StatefulSet\"\nspec:\n  template:\n    metadata:\n      annotations:\n        com.cloudbees.sidecar-injector/inject: yes\n</code></pre> <p></p> <p>And then hit the <code>Save</code> button at the bottom to persist our changes.</p>"},{"location":"cloudbees/multi-cluster/#create-a-managed-master","title":"Create a Managed Master","text":"<p>To create a Managed Master, we go to the home page of the Operations Center. If you're unsure, go to <code>&lt;cjocHost&gt;/cjoc/</code> or click on the top left breadcrumb <code>Jenkins</code>.</p> <p>On the top right, hit the button <code>New Master</code>.</p> <p></p> <p>Give the Master an appropriate name and hit <code>Go</code>.</p> <p>Tip</p> <p>Sometimes the button is not clickable. Click next to the button with your mouse to make the text field lose focus. It should now be clickable.</p> <p></p> <p>The first thing we have to change, is the <code>Cluster endpoint</code>. This should now be a dropdown with <code>kubernetes</code> and <code>Azure</code> (or whatever you've named your endpoint). Select the one that points to your other cluster, in my case, <code>Azure</code>.</p> <p></p> <p>We have one more change to make. We have to tell the JVM where our truststore is with the additional certificates. To ensure we can contact the Operations Center and the Kubernetes API.</p> <p>We do this by supplying the <code>javax.net.ssl.trustStore</code> and <code>javax.net.ssl.trustStorePassword</code> properties in the <code>System Properties</code> field. Each should be on its own line. If you're wondering what the location is, this is where the CloudBees Sidecar Injector will place the truststore.</p> <pre><code>javax.net.ssl.trustStore=/etc/ssl/certs/java/cacerts\njavax.net.ssl.trustStorePassword=changeit\n</code></pre> <p>If you run into SSL or truststore issues -not trust issues, I cannot help you with those - you can enable debug logging by adding the following property.</p> <pre><code>javax.net.debug=SSL,trustmanager\n</code></pre> <p></p> <p>Hit the <code>Save</code> button at the bottom, and you should be good to go!</p>"},{"location":"cloudbees/sso-azure-ad/","title":"Azure AD &amp; CloudBees Core","text":"<p>In this article we're going to configure Azure AD as Single-Sign-Solution for CloudBees Core. The configuration rests on three points; 1) Azure AD, 2) Jenkins' SAML plugin, and 3) CloudBees Core's Role Base Access Control or RBAC for short.</p> <p>Important</p> <p>Currently there's a discrepency between the TimeToLive (TTL) of the RefreshToken in Azure AD and the default configuration in the SAML plugin.</p> <p>This creates the problem that you can be logged in, in Azure, but when going to Jenkins you get the Logged Out page. The only way to log back in, is to logout in Azure and back in.</p> <p>The reason is as follows:</p> <p>The max lifetime of the Access Token in Azure AD seems to be 24 hours where the refresh token can live for a maximum of 14 days (if the access token expires the refresh token is used to try to obtain a new access token). The Jenkins setting in Configure Global Security &gt; SAML Identity Provider Settings &gt; Maximum Authentication Lifetime is 24 hours (86400 in seconds) upping this to 1209600 (which is 14 days in seconds/the max lifetime of the Refresh Token).</p> <p>The recommended resolution is to set <code>Maximum Authentication Lifetime</code> to the same value of your RefreshToken TTL. If you haven't changed this in Azure AD, it is <code>1209600</code>.</p> <p><code>Manage Jenkins</code> -&gt; <code>Configure Global Security</code> &gt; <code>SAML Identity Provider Settings</code> &gt; <code>Maximum Authentication Lifetime</code> = <code>1209600</code></p>"},{"location":"cloudbees/sso-azure-ad/#prerequisites","title":"Prerequisites","text":"<p>Before we start, there are some requirements.</p> <ul> <li>a running CloudBees Core Operations Center instance</li> <li>this instance is accessible via <code>https.</code><ul> <li>if you do not have a valid certificate and you're running on Kubernetes take a look at my cert-manager guide</li> <li>Let's Encrypt can now also work with nip.io addresses</li> </ul> </li> <li>active Azure subscription</li> <li>have an Azure subscription Administrator on hand</li> </ul>"},{"location":"cloudbees/sso-azure-ad/#configure-azure","title":"Configure Azure","text":"<p>Warning</p> <p>We use <code>https://cloudbees-core.example.com</code> as the example URL for CloudBees Core. Please replace the domain to your actual domain in all the examples!</p>"},{"location":"cloudbees/sso-azure-ad/#steps-to-execute","title":"Steps to execute","text":"<p>We do the following steps on the Azure side.</p> <ul> <li>create the Azure Active Directory</li> <li>create users and groups</li> <li>create App Registration<ul> <li>URL: <code>https://cloudbees-core.example.com/cjoc/securityRealm/finishLogin</code></li> <li>replace <code>example.com</code> with your domain, https is required! </li> </ul> </li> <li>update manifest (for groups) <ul> <li>change: <code>\"groupMembershipClaims\": null,</code> (usually line 11)</li> <li>to: <code>\"groupMembershipClaims\": \"SecurityGroup\",</code></li> </ul> </li> <li>create SP ID / App ID URI</li> <li>grant admin consent</li> </ul> <p>Info</p> <p>If you use the Azure AD plugin, you also create a client secret.</p>"},{"location":"cloudbees/sso-azure-ad/#information-to-note-down","title":"Information To Note Down","text":"<p>The following information is unique to your installation, so you need to record them as you go along.</p> <ul> <li><code>App ID URI</code></li> <li><code>Object ID</code>'s of Users and Groups you want to give rights</li> <li><code>Federation Metadata Document</code> Endpoint<ul> <li>Azure AD -&gt; App Registrations -&gt;  -&gt; Endpoints (circular icon on top) <li>you can use either the URL or the document contents</li> <li>make sure the URL contains the Tenant ID of your Azure Active Directory</li> <li>URL example: <code>https://login.microsoftonline.com/${TENANT_ID}/federationmetadata/2007-06/federationmetadata.xml</code></li> <li>You can find your Tenant ID in <code>Azure Active Directory</code> -&gt; <code>Properties</code> -&gt; <code>Directory ID</code> (different name, same ID)</li>"},{"location":"cloudbees/sso-azure-ad/#visual-guide","title":"Visual Guide","text":"<p>Below is a visual guide with screenshots.</p> <p>Pay attention to these hints in the screenshots.</p> <ul> <li>red: this is the main thing</li> <li>orange: this is how we got to the current page/view</li> <li>blue: while you're in this screen, there might be other things you could do</li> </ul>"},{"location":"cloudbees/sso-azure-ad/#create-new-app-registration","title":"Create New App Registration","text":"<p>If you have an Azure account, you have an Azure Active Directory (Azure AD) pre-created.</p> <p>This guide assumes you have an Azure AD ready to use.</p> <p>That means the next step is to create an Application Registration.</p> <p></p> <p>Give the registration a useful name, select who can authenticate and the <code>redirect URL</code>.</p> <p>This URL needs to be configured and MUST be the actual URL of your CloudBees Core installation.</p> <p></p> <p>Important</p> <p>To have Client Masters as a fallback for login, incase Operations Center is down, you have to add another redirect URL for each Master.</p> <p>Azure AD -&gt; App Registrations -&gt;  -&gt; Authentication -&gt; Web -&gt; <code>https://example.com/teams-cat/securityRealm/finishLogin</code>"},{"location":"cloudbees/sso-azure-ad/#app-registration-data-to-write-down","title":"App Registration Data To Write Down","text":"<p>Depending on the plugin you're going to use (SAML or Azure AD) you need information from your Azure AD / App Registration.</p> <ul> <li>Tentant ID</li> <li>Object ID</li> <li>Client ID</li> <li>Federation Metadata Document <ul> <li>you can use the document XML content or the URL</li> </ul> </li> </ul> <p></p> <p>Click on the <code>Endpoints</code> button to open the side-bar with the links.</p> <p></p>"},{"location":"cloudbees/sso-azure-ad/#app-id","title":"App ID","text":"<p>We need the <code>App ID</code> - even if the SAML plugin doesn't mention it.</p> <p>Azure generates an <code>APP ID</code> URI for you. You can also use CloudBees Core's URL as the URI. The URI is shown when there is an error, so it recommended to use a value you can recognize.</p> <p>Info</p> <p><code>App ID</code> must match in both Azure AD (set as <code>App ID URI</code>) and the SAML plugin (set as <code>Entity ID</code>) configuration in Jenkins. So write it down.</p> <p></p> <p></p>"},{"location":"cloudbees/sso-azure-ad/#api-permissions","title":"API Permissions","text":"<p>Of course, things wouldn't be proper IAM/LDAP/AD without giving some permissions.</p> <p>If we want to retrieve group information and other fields, we need to be able to read the Directory information.</p> <p></p> <p>You find the Directory information via the <code>Microsoft Graph</code> api button.</p> <p></p> <p>We select <code>Application Permissions</code> and then check <code>Directory.Read.All</code>. We don't need to write.</p> <p></p> <p>The Permissions have changed, so we require an Administrator account to consent with the new permissions.</p> <p></p>"},{"location":"cloudbees/sso-azure-ad/#update-manifest","title":"Update Manifest","text":"<p>As with the permissions, the default <code>Manifest</code> doesn't give us all the information we want.</p> <p>We want the groups so we can configure RBAC, and thus we have to set the <code>groupMembershipsClaims</code> claim attribute.</p> <p></p> <p>We change the <code>null</code> to <code>\"SecurityGroup\"</code>. Please consult the Microsoft docs (see reference below) for other options.</p> <p>We can download, edit, and upload the manifest file. Alternatively, we can edit inline and hit save on top.</p> <p></p>"},{"location":"cloudbees/sso-azure-ad/#retrieve-group-object-id","title":"Retrieve Group Object ID","text":"<p>If we want to assign Azure AD groups to groups or roles in Jenkins' RBAC, we need to use the <code>Object ID</code>'s.</p> <p>Each Group and User has an <code>Object ID</code>, which have a handy <code>Copy this</code> button on the end of the value box!</p> <p></p>"},{"location":"cloudbees/sso-azure-ad/#configure-jenkins","title":"Configure Jenkins","text":"<p>We now get to the point where we configure Jenkins. The SAML plugin is opensource, and thus it's configuration can also be used for Jenkins or CloudBees Jenkins Distribution.</p>"},{"location":"cloudbees/sso-azure-ad/#steps","title":"Steps","text":"<p>Here are the steps we perform to configure Azure AD as the Single-Sign-On solution. </p> <ul> <li>install the SAML plugin<ul> <li>I assume you know how to install plugins, so we skip this</li> <li>if you don't know Read the Managing Plugins Guide</li> </ul> </li> <li>configure saml 2.0 in Jenkins</li> <li>setup groups (RBAC)<ul> <li>administrators -&gt; admin group</li> <li>browsers -&gt; all other groups</li> </ul> </li> </ul>"},{"location":"cloudbees/sso-azure-ad/#visual-guide_1","title":"Visual Guide","text":"<p>Below is a visual guide with screenshots. Please pay attention to the hints in the screenshots.</p> <ul> <li>Red: this is the main thing</li> <li>Orange: this is how we got to the current page/view</li> <li>Blue: while you're in this screen, there might be other things you could do</li> </ul>"},{"location":"cloudbees/sso-azure-ad/#configure-security","title":"Configure Security","text":"<p>To go to Jenkins' security configuration, follow this route:</p> <ul> <li>login with an Admin user</li> <li>go to the <code>Operations Center</code></li> <li><code>Manage Jenkins</code> -&gt; <code>Global Security Configuration</code></li> </ul>"},{"location":"cloudbees/sso-azure-ad/#configure-rbac","title":"Configure RBAC","text":"<p>The SAML plugin configuration pollutes the screen with fields.</p> <p>My advice is to enable RBAC first.</p> <p>If you haven't got any groups/roles yet, I recommend using the <code>Typical Initial Setup</code> from the dropdown. The logged-in user is automatically registered as administrator. So if your Azure AD configuration doesn't work, this user can still manage Jenkins.</p> <p></p> <p>Important</p> <p>Make sure you know the credentials of the current admin user.</p> <p>It will automatically be added to the <code>Administrators</code> group, and it will be your go-to account when you mess up the SAML configuration and you have to reset security. </p> <p>For how to reset the security configuration, see the <code>For When You Mess Up</code> paragraph.</p>"},{"location":"cloudbees/sso-azure-ad/#configure-saml","title":"Configure SAML","text":"<p>Select <code>SAML 2.0</code> from the <code>Security Realm</code> options.</p> <p>Here we first supply our <code>Federation Metadata Document</code> content or it's URL.</p> <p>Each option - document content or URL - has its own <code>Validate ...</code> button, hit it and confirm it says <code>Success</code>.</p> <p></p> <p></p> <p>Info</p> <p>You can leave <code>Displayname</code> empty, which gives you the default naming scheme from Azure AD. I think this is ugly, as it amounts to something like <code>${EMAIL_ADDRESS}_${AD_DOMAIN}_${AZURE_CORP_DOMAIN}</code>. There are other options, I've settled for <code>givenname</code>, as there isn't a <code>fullname</code> by default, and well, I prefer <code>Joost</code> to a long hard to recognize string.</p>"},{"location":"cloudbees/sso-azure-ad/#fields","title":"Fields","text":"<ul> <li>Displayname: <code>http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname</code></li> <li>Group: <code>http://schemas.microsoft.com/ws/2008/06/identity/claims/groups</code></li> <li>Username: <code>http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name</code></li> <li>Email: <code>http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress</code></li> <li>SP Entity ID:  the <code>App ID</code> URI you configured in Azure AD (hidden behind <code>Advanced Configuration</code>)</li> </ul>"},{"location":"cloudbees/sso-azure-ad/#configure-rbac-groups","title":"Configure RBAC Groups","text":"<p>Tip</p> <p>Once Azure AD is configured, and it works, you can configure groups for RBAC just as you're used to.</p> <p>Both for classic RBAC and Team Masters.</p> <p>Just make sure you use the Azure AD <code>Object ID</code>'s of the groups to map them.</p> <p>Bonus tip, add every Azure AD group to <code>Browsers</code>, so you can directly map their groups to Team Master roles without problems. </p> <p></p> <p></p>"},{"location":"cloudbees/sso-azure-ad/#xml-config","title":"XML Config","text":"<pre><code>  &lt;useSecurity&gt;true&lt;/useSecurity&gt;\n  &lt;authorizationStrategy class=\"nectar.plugins.rbac.strategy.RoleMatrixAuthorizationStrategyImpl\"/&gt;\n &lt;securityRealm class=\"org.jenkinsci.plugins.saml.SamlSecurityRealm\" plugin=\"saml@1.1.2\"&gt;\n    &lt;displayNameAttributeName&gt;http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname&lt;/displayNameAttributeName&gt;\n    &lt;groupsAttributeName&gt;http://schemas.microsoft.com/ws/2008/06/identity/claims/groups&lt;/groupsAttributeName&gt;\n    &lt;maximumAuthenticationLifetime&gt;86400&lt;/maximumAuthenticationLifetime&gt;\n    &lt;emailAttributeName&gt;http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress&lt;/emailAttributeName&gt;\n    &lt;usernameCaseConversion&gt;none&lt;/usernameCaseConversion&gt;\n    &lt;usernameAttributeName&gt;http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name&lt;/usernameAttributeName&gt;\n    &lt;binding&gt;urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect&lt;/binding&gt;\n    &lt;advancedConfiguration&gt;\n      &lt;forceAuthn&gt;false&lt;/forceAuthn&gt;\n      &lt;spEntityId&gt;https://cloudbees-core.kearos.net&lt;/spEntityId&gt;\n    &lt;/advancedConfiguration&gt;\n    &lt;idpMetadataConfiguration&gt;\n      &lt;xml&gt;&lt;/xml&gt;\n      &lt;url&gt;https://login.microsoftonline.com/95b46e09-0307-488b-a6fc-1d2717ba9c49/federationmetadata/2007-06/federationmetadata.xml&lt;/url&gt;\n      &lt;period&gt;5&lt;/period&gt;\n    &lt;/idpMetadataConfiguration&gt;\n  &lt;/securityRealm&gt;\n  &lt;disableRememberMe&gt;false&lt;/disableRememberMe&gt;\n</code></pre>"},{"location":"cloudbees/sso-azure-ad/#logout-url","title":"Logout URL","text":"<p>Depending on the requirements, you may want to specify a logout url in the SAML configuration to log you completely out of SAML, not just Core.</p> <p>An example <code>https://login.windows.net/&lt;tenant_id_of_your_app&gt;/oauth2/logout?post_logout_redirect_uri=&lt;logout_URL_of_your_app&gt;/logout</code></p>"},{"location":"cloudbees/sso-azure-ad/#for-when-you-mess-up","title":"For When You Mess Up","text":"<p>This is the default config for security in CloudBees Core.</p> <p>This file is in <code>${JENKINS_HOME}/config.xml</code>, the XML tags we want to look at are quite near the top.</p> <pre><code> &lt;useSecurity&gt;true&lt;/useSecurity&gt;\n  &lt;authorizationStrategy class=\"hudson.security.FullControlOnceLoggedInAuthorizationStrategy\"&gt;\n    &lt;denyAnonymousReadAccess&gt;true&lt;/denyAnonymousReadAccess&gt;\n  &lt;/authorizationStrategy&gt;\n  &lt;securityRealm class=\"hudson.security.HudsonPrivateSecurityRealm\"&gt;\n    &lt;disableSignup&gt;true&lt;/disableSignup&gt;\n    &lt;enableCaptcha&gt;false&lt;/enableCaptcha&gt;\n  &lt;/securityRealm&gt;\n  &lt;disableRememberMe&gt;false&lt;/disableRememberMe&gt;\n</code></pre>"},{"location":"cloudbees/sso-azure-ad/#on-cloudbees-core-modern-kubernetes","title":"On CloudBees Core Modern / Kubernetes","text":"<p>To rectify a failed configuration, execute the following steps:</p> <ol> <li>exec into the <code>cjoc-0</code> container: <code>kubectl exec -ti cjoc-0 -- bash</code></li> <li>open <code>config.xml</code>: <code>vi /var/jenkins_home/config.xml</code></li> <li>replace conflicting lines with the above snippet</li> <li>save the changes</li> <li>exit the container: <code>exit</code></li> <li>kill the pod: <code>kubectl delete po cjoc-0</code></li> </ol> <p>Tip</p> <p>For removing a whole line, stay in \"normal\" mode, and press <code>d d</code> (two times the <code>d</code> key). To add the new lines, go into insert mode by pressing the <code>i</code> key. Go back to \"normal\" mode by pressing the <code>esc</code> key. Then, save and quit, by writing: <code>:wq</code> followed by <code>enter</code>.</p>"},{"location":"cloudbees/sso-azure-ad/#references","title":"References","text":"<ul> <li>CloudBees Guide on Azure AD for Core SSO(outdated)</li> <li>SAML Plugin Docs for Azure AD (outdated)</li> <li>Microsoft Doc for Azure AD Tokens</li> <li>Microsoft Doc for Azure AD Optional Tokens</li> <li>Microsoft Doc for Azure AD Custom Tokens</li> <li>Alternative Azure AD Plugin (very new)</li> </ul> <p>Info</p> <p>Currently, there is a limitation which requires you to use the <code>Object ID</code>'s which make searching groups and people less than ideal.  When the Alternative Azure AD Plugin is approved this may provide a more satisfactory solution.</p>"},{"location":"cloudbees/teams-automation/","title":"Core Modern Teams Automation","text":"<p>CloudBees Core on Modern (meaning, on Kubernetes) has two main types of Jenkins Masters, a Managed Master, and a Team Master. In this article, we're going to automate the creation and management of the Team Masters.</p> <p>Hint</p> <p>If you do not want to read any of the code here, or just want to take a look at the end result - pipeline wise - you can find working examples on GitHub.</p> <ul> <li>Template Repository - creates a new team template and a PR to the GitOps repository</li> <li>GitOps Repository - applies GitOps principles to manage the CloudBees Core Team Masters</li> </ul>"},{"location":"cloudbees/teams-automation/#goals","title":"Goals","text":"<p>Just automating the creation of a Team Master is relatively easy, as this can be done via the Client Jar. So we're going to set some additional goals to create a decent challenge.</p> <ul> <li>GitOps: I want to be able to create and delete Team Masters by managing configuration in a Git repository </li> <li>Configuration-as-Code: as much of the configuration as possible should be stored in the Git repository</li> <li>Namespace: one of the major reasons for Team Masters to exist is to increase (Product) Team autonomy, which in Kubernetes should correspond to a <code>namespace</code>. So I want each Team Master to be in its own Namespace!</li> <li>Self-Service: the solution should cater to (semi-)autonomous teams and lower the workload of the team managing CloudBees Core. So requesting a Team Master should be doable by everyone</li> </ul>"},{"location":"cloudbees/teams-automation/#before-we-start","title":"Before We Start","text":"<p>Some assumptions need to be taken care off before we start.</p> <ul> <li>Kubernetes cluster in which you are <code>ClusterAdmin</code><ul> <li>if you don't have one yet, there are guides on this elsewhere on the site</li> </ul> </li> <li>your cluster has enough capacity (at least two nodes of 4gb memory)</li> <li>your cluster has CloudBees Core Modern installed<ul> <li>if you don't have this yet look at one of the guides on this site</li> <li>or look at the guides on CloudBees.com</li> </ul> </li> <li>have administrator access to CloudBees Core Cloud Operations Center</li> </ul> <p>Code Examples</p> <p>The more extensive Code Examples, such as Kubernetes Yaml files, are collapsed by default. You can open them by clicking on them. On the right, the code snippet will have a <code>[ ]</code> copy icon. Below is an example.</p> Code Snippet Example <p>Here's a code snippet.</p> <pre><code>pipeline {\n    agent any\n    stages {\n        stage('Hello') {\n            steps {\n                echo 'Hello World!'\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"cloudbees/teams-automation/#bootstrapping","title":"Bootstrapping","text":"<p>All right, so we want to use GitOps and to process the changes we need a Pipeline that can be triggered by a Webhook. I believe in everything as code - except Secrets and such - which includes the Pipeline.</p> <p>Unfortunately, Operations Center cannot run such pipelines. To get over this hurdle, we will create a special <code>Ops</code> Team Master. This Master will be configured to be able to Manage the other Team Masters for us.</p> <p>Log into your Operations Center with a user that has administrative access.</p>"},{"location":"cloudbees/teams-automation/#create-api-token","title":"Create API Token","text":"<p>Create a new API Token for your administrator user by clicking on the user's name - top right corner. Select the <code>Configuration</code> menu on the left and then you should see a section where you can <code>Create a API Token</code>. This Token will disappear, so write it down.</p>"},{"location":"cloudbees/teams-automation/#get-configure-client-jar","title":"Get &amp; Configure Client Jar","text":"<p>Replace the values marked by <code>&lt; ... &gt;</code>. The Operations Center URL should look like this: <code>http://cbcore.mydomain.com/cjoc</code>.</p> <p>Setup the connection variables.</p> <pre><code>OC_URL=&lt;your operations center url&gt;\n</code></pre> <pre><code>USR=&lt;your username&gt;\nTKN=&lt;api token&gt;\n</code></pre> <p>Download the Client Jar.</p> <pre><code>curl ${OC_URL}/jnlpJars/jenkins-cli.jar -o jenkins-cli.jar\n</code></pre>"},{"location":"cloudbees/teams-automation/#create-alias-test","title":"Create Alias &amp; Test","text":"<pre><code>alias cboc=\"java -jar jenkins-cli.jar -noKeyAuth -auth ${USR}:${TKN} -s ${OC_URL}\"\n</code></pre> <pre><code>cboc version\n</code></pre>"},{"location":"cloudbees/teams-automation/#create-team-ops","title":"Create Team Ops","text":"<p>As the tasks of the Team Masters for managing Operations are quite specific and demand special rights, I'd recommend putting this in its own <code>namespace</code>. To do so properly, we need to configure a few things.</p> <ul> <li>allows Operations Center access to this namespace (so it can create the Team Master)</li> <li>give the <code>ServiceAccount</code> the permissions to create <code>namespace</code>'s for the other Team Masters</li> <li>add config map for the Jenkins Agents</li> <li>temporarily change Operations Center's operating Namespace (where it will spawn resources in)</li> <li>use the CLI to create the <code>team-ops</code> Team Master</li> <li>reset Operations Center's operating Namespace</li> </ul>"},{"location":"cloudbees/teams-automation/#update-create-kubernetes-namespaces","title":"Update &amp; Create Kubernetes Namespaces","text":""},{"location":"cloudbees/teams-automation/#create-team-ops-namespace","title":"Create Team Ops Namespace","text":"<pre><code>kubectl apply -f team-ops-namespace.yaml\n</code></pre> team-ops-namespace.yaml <p>This creates the <code>team-ops</code> namespace including all the resources required such as <code>ResourceQuota</code>, <code>ServiceAccount</code> and so on.</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: team-ops\n\n---\n\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: resource-quota\n  namespace: team-ops\nspec:\n  hard:\n    pods: \"20\"\n    requests.cpu: \"4\"\n    requests.memory: 6Gi\n    limits.cpu: \"5\"\n    limits.memory: 10Gi\n    services.loadbalancers: \"0\"\n    services.nodeports: \"0\"\n    persistentvolumeclaims: \"10\"\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins\n  namespace: team-ops\n\n---\n\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: pods-all\n  namespace: team-ops\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/exec\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/log\"]\n  verbs: [\"get\",\"list\",\"watch\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: jenkins\n  namespace: team-ops\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: pods-all\nsubjects:\n- kind: ServiceAccount\n  name: jenkins\n  namespace: team-ops\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: cjoc\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: master-management\nsubjects:\n- kind: ServiceAccount\n  name: jenkins\n  namespace: team-ops\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  name: create-namespaces\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"serviceaccounts\", \"rolebindings\", \"roles\", \"resourcequotas\", \"namespaces\"]\n  verbs: [\"create\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"configmaps\", \"rolebindings\", \"roles\", \"resourcequotas\", \"namespaces\"]\n  verbs: [\"create\",\"get\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"events\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"persistentvolumeclaims\", \"pods\", \"pods/exec\", \"services\", \"statefulsets\", \"ingresses\", \"extensions\"]\n  verbs: [\"create\", \"delete\", \"get\", \"list\", \"patch\", \"update\", \"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/log\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"list\"]\n- apiGroups: [\"apps\"]\n  resources: [\"statefulsets\"] \n  verbs: [\"create\", \"delete\", \"get\", \"list\", \"patch\", \"update\", \"watch\"]\n- apiGroups: [\"extensions\"]\n  resources: [\"ingresses\"]\n  verbs: [\"create\", \"delete\", \"get\", \"list\", \"patch\", \"update\", \"watch\"]\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: ops-namespace\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: create-namespaces\nsubjects:\n- kind: ServiceAccount\n  name: jenkins\n  namespace: team-ops\n</code></pre>"},{"location":"cloudbees/teams-automation/#update-operation-center-serviceaccount","title":"Update Operation Center ServiceAccount","text":"<p>The <code>ServiceAccount</code> under which Operation Center runs, only has rights in it's own <code>namespace</code>. Which means it cannot create our Team Ops Master. Below is the <code>.yaml</code> file for Kubernetes and the command to apply it.</p> <p>Warning</p> <p>I assume you're using the default <code>cloudbees-core</code> as per Cloudbees' documentation. If this is not the case, change the last line, <code>namespace: cloudbees-core</code> with the namespace your Operation Center runs in.</p> <pre><code>kubectl apply -f patch-oc-serviceaccount.yaml -n team-ops\n</code></pre> patch-oc-serviceaccount.yaml <p>This patches the existing Operation Center's ServiceAccount to also have the correct rights in the <code>team-ops</code> namespace.</p> <pre><code>kind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: master-management\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/exec\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/log\"]\n  verbs: [\"get\",\"list\",\"watch\"]\n- apiGroups: [\"apps\"]\n  resources: [\"statefulsets\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"services\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"persistentvolumeclaims\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"extensions\"]\n  resources: [\"ingresses\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"events\"]\n  verbs: [\"get\",\"list\",\"watch\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: cjoc\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: master-management\nsubjects:\n- kind: ServiceAccount\n  name: cjoc\n  namespace: cloudbees-core\n</code></pre>"},{"location":"cloudbees/teams-automation/#jenkins-agent-configmap","title":"Jenkins Agent ConfigMap","text":"<pre><code>kubectl apply -f jenkins-agent-config-map.yaml -n team-ops\n</code></pre> jenkins-agent-config-map.yaml <p>Creates the Jenkins Agent ConfigMap, which contains the information the Jenkins Agent - within a PodTemplate - uses to connect to the Jenkins Master.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: jenkins-agent\ndata:\n  jenkins-agent: |\n    #!/usr/bin/env sh\n    # The MIT License\n    #\n    #  Copyright (c) 2015, CloudBees, Inc.\n    #\n    #  Permission is hereby granted, free of charge, to any person obtaining a copy\n    #  of this software and associated documentation files (the \"Software\"), to deal\n    #  in the Software without restriction, including without limitation the rights\n    #  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    #  copies of the Software, and to permit persons to whom the Software is\n    #  furnished to do so, subject to the following conditions:\n    #\n    #  The above copyright notice and this permission notice shall be included in\n    #  all copies or substantial portions of the Software.\n    #\n    #  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    #  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    #  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    #  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    #  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    #  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n    #  THE SOFTWARE.\n    # Usage jenkins-slave.sh [options] -url http://jenkins [SECRET] [AGENT_NAME]\n    # Optional environment variables :\n    # * JENKINS_TUNNEL : HOST:PORT for a tunnel to route TCP traffic to jenkins host, when jenkins can't be directly accessed over network\n    # * JENKINS_URL : alternate jenkins URL\n    # * JENKINS_SECRET : agent secret, if not set as an argument\n    # * JENKINS_AGENT_NAME : agent name, if not set as an argument\n    if [ $# -eq 1 ]; then\n        # if `docker run` only has one arguments, we assume user is running alternate command like `bash` to inspect the image\n        exec \"$@\"\n    else\n        # if -tunnel is not provided try env vars\n        case \"$@\" in\n            *\"-tunnel \"*) ;;\n            *)\n            if [ ! -z \"$JENKINS_TUNNEL\" ]; then\n                TUNNEL=\"-tunnel $JENKINS_TUNNEL\"\n            fi ;;\n        esac\n        if [ -n \"$JENKINS_URL\" ]; then\n            URL=\"-url $JENKINS_URL\"\n        fi\n        if [ -n \"$JENKINS_NAME\" ]; then\n            JENKINS_AGENT_NAME=\"$JENKINS_NAME\"\n        fi  \n        if [ -z \"$JNLP_PROTOCOL_OPTS\" ]; then\n            echo \"Warning: JnlpProtocol3 is disabled by default, use JNLP_PROTOCOL_OPTS to alter the behavior\"\n            JNLP_PROTOCOL_OPTS=\"-Dorg.jenkinsci.remoting.engine.JnlpProtocol3.disabled=true\"\n        fi\n        # If both required options are defined, do not pass the parameters\n        OPT_JENKINS_SECRET=\"\"\n        if [ -n \"$JENKINS_SECRET\" ]; then\n            case \"$@\" in\n                *\"${JENKINS_SECRET}\"*) echo \"Warning: SECRET is defined twice in command-line arguments and the environment variable\" ;;\n                *)\n                OPT_JENKINS_SECRET=\"${JENKINS_SECRET}\" ;;\n            esac\n        fi\n\n        OPT_JENKINS_AGENT_NAME=\"\"\n        if [ -n \"$JENKINS_AGENT_NAME\" ]; then\n            case \"$@\" in\n                *\"${JENKINS_AGENT_NAME}\"*) echo \"Warning: AGENT_NAME is defined twice in command-line arguments and the environment variable\" ;;\n                *)\n                OPT_JENKINS_AGENT_NAME=\"${JENKINS_AGENT_NAME}\" ;;\n            esac\n        fi\n        SLAVE_JAR=/usr/share/jenkins/slave.jar\n        if [ ! -f \"$SLAVE_JAR\" ]; then\n            tmpfile=$(mktemp)\n            if hash wget &gt; /dev/null 2&gt;&amp;1; then\n                wget -O \"$tmpfile\" \"$JENKINS_URL/jnlpJars/slave.jar\"\n            elif hash curl &gt; /dev/null 2&gt;&amp;1; then\n                curl -o \"$tmpfile\" \"$JENKINS_URL/jnlpJars/slave.jar\"\n            else\n                echo \"Image does not include $SLAVE_JAR and could not find wget or curl to download it\"\n                return 1\n            fi\n            SLAVE_JAR=$tmpfile\n        fi\n        #TODO: Handle the case when the command-line and Environment variable contain different values.\n        #It is fine it blows up for now since it should lead to an error anyway.\n        exec java $JAVA_OPTS $JNLP_PROTOCOL_OPTS -cp $SLAVE_JAR hudson.remoting.jnlp.Main -headless $TUNNEL $URL $OPT_JENKINS_SECRET $OPT_JENKINS_AGENT_NAME \"$@\"\n    fi\n</code></pre>"},{"location":"cloudbees/teams-automation/#create-initial-master","title":"Create Initial Master","text":"<p>To make it easier to change the <code>namespace</code> if needed, its extracted out from the command.</p> <pre><code>OriginalNamespace=cloudbees-core\n</code></pre> <p>This script changes the Operations Center's operating <code>namespace</code>, creates a Team Master with the name <code>ops</code>, and then resets the namespace.</p> <pre><code>cboc groovy = &lt; configure-oc-namespace.groovy team-ops\ncboc teams ops --put &lt; team-ops.json\ncboc groovy = &lt; configure-oc-namespace.groovy $OriginalNamespace\n</code></pre> team-ops.json <p>This <code>json</code> file that describes a team. By default there are three roles defined on a team, <code>TEAM_ADMIN</code>, <code>TEAM_MEMBER</code>, and <code>TEAM_GUEST</code>. Don't forget to change the <code>id</code>'s to Group ID's from your Single-Sign-On solution.</p> <pre><code>{\n    \"version\" : \"1\",\n    \"data\": {\n        \"name\": \"ops\",\n        \"displayName\": \"Operations\",\n        \"provisioningRecipe\": \"basic\",\n        \"members\": [{\n            \"id\": \"Catmins\",\n            \"roles\": [\"TEAM_ADMIN\"]\n        },\n        {\n            \"id\": \"Pirates\",\n            \"roles\": [\"TEAM_MEMBER\"]\n        },\n        {\n            \"id\": \"Continental\",\n            \"roles\": [\"TEAM_GUEST\"]\n        }\n        ],\n        \"icon\": {\n            \"name\": \"hexagons\",\n            \"color\": \"#8d7ec1\"\n        }\n    }\n}\n</code></pre> configure-oc-namespace.groovy <p>This is a Jenkins Configuration or System Groovy script. It will change the <code>namespace</code> Operation Center uses to create resources. You can change this in the UI by going to <code>Operations Center</code> -&gt; <code>Manage Jenkins</code> -&gt; <code>System Configuration</code> -&gt; <code>Master Provisioning</code> -&gt; <code>Namespace</code>.</p> <pre><code>import hudson.*\nimport hudson.util.Secret;\nimport hudson.util.Scrambler;\nimport hudson.util.FormValidation;\nimport jenkins.*\nimport jenkins.model.*\nimport hudson.security.*\n\nimport com.cloudbees.masterprovisioning.kubernetes.KubernetesMasterProvisioning\nimport com.cloudbees.masterprovisioning.kubernetes.KubernetesClusterEndpoint\n\nprintln \"=== KubernetesMasterProvisioning Configuration - start\"\n\nprintln \"== Retrieving main configuration\"\ndef descriptor = Jenkins.getInstance().getInjector().getInstance(KubernetesMasterProvisioning.DescriptorImpl.class)\ndef namespace = this.args[0]\n\ndef currentKubernetesClusterEndpoint =  descriptor.getClusterEndpoints().get(0)\nprintln \"= Found current endpoint\"\nprintln \"= \" + currentKubernetesClusterEndpoint.toString()\ndef id = currentKubernetesClusterEndpoint.getId()\ndef name = currentKubernetesClusterEndpoint.getName()\ndef url = currentKubernetesClusterEndpoint.getUrl()\ndef credentialsId = currentKubernetesClusterEndpoint.getCredentialsId()\n\nprintln \"== Setting Namspace to \" + namespace\ndef updatedKubernetesClusterEndpoint = new KubernetesClusterEndpoint(id, name, url, credentialsId, namespace)\ndef clusterEndpoints = new ArrayList&lt;KubernetesClusterEndpoint&gt;()\nclusterEndpoints.add(updatedKubernetesClusterEndpoint)\ndescriptor.setClusterEndpoints(clusterEndpoints)\n\nprintln \"== Saving Jenkins configuration\"\ndescriptor.save()\n\nprintln \"=== KubernetesMasterProvisioning Configuration - finish\"\n</code></pre>"},{"location":"cloudbees/teams-automation/#configure-team-ops-master","title":"Configure Team Ops Master","text":"<p>Now that we've created the Operations Team Master (Team Ops), we can configure it. </p> <p>The Pipelines we need will require credentials, we describe them below. </p> <ul> <li>githubtoken_token: GitHub API Token only, credentials type <code>Secret Text</code>  (for the PR pipeline)</li> <li>githubtoken: GitHub username and API Token</li> <li>jenkins-api: Username and API Token for Operations Center. Just like the one we used for Client Jar.</li> </ul> <p>We also need to have a Global Pipeline Library defined by the name <code>github.com/joostvdg/jpl-core</code>. This, as the name suggests, should point to <code>https://github.com/joostvdg/jpl-core.git</code>.</p>"},{"location":"cloudbees/teams-automation/#create-gitops-pipeline","title":"Create GitOps Pipeline","text":"<p>In total, we need two repositories and two or three Pipelines. You can either use my CLI Docker Image or roll your own. I will proceed as if you will create your own.</p> <ul> <li>CLI Image Pipeline: this will create a CLI Docker Image that is used to talk to Operations Center via the Client Jar (CLI)</li> <li>PR Pipeline: I like the idea of Self-Service, but in order to keep things in check, you might want to provide that via a PullRequest (PR) rather than a direct write to the Master branch. This is also Repository One, as I prefer having each pipeline in their own Repository, but you don't need to.</li> <li>Main Pipeline: will trigger on a commit to the Master branch and create the new team. I'll even throw in a free manage your Team Recipes for free as well.</li> </ul>"},{"location":"cloudbees/teams-automation/#create-cli-image-pipeline","title":"Create CLI Image Pipeline","text":"<p>In a Kubernetes cluster, you should not build with Docker directly, use an in-cluster builder such as Kaniko or Buildah. </p> <p>You can read more about the why and how elsewhere on this site.</p> <p>Tip</p> <p>If you do not want to create your own, you can re-use my images.</p> <p>There should be one available for every recent version of CloudBees Core that will work with your Operations Center. The images are available in DockerHub at caladreas/cbcore-cli</p>"},{"location":"cloudbees/teams-automation/#kaniko-configuration","title":"Kaniko Configuration","text":"<p>Kaniko uses a Docker Image to build your Docker Image in cluster. It does however need to directly communicate to your Docker Registry. This requires a Kubernetes <code>Secret</code> of type <code>docker-registry</code>.</p> <p>How you can do this and more, you can read on the CloudBees Core Docs.</p>"},{"location":"cloudbees/teams-automation/#pipeline","title":"Pipeline","text":"<p>Now that you have Kaniko configured, you can use this Pipeline to create your own CLI Images.</p> <p>Caution</p> <p>Make sure you replace the environment variables with values that make sense to you.</p> <ul> <li>CJOC_URL internal url in Kubernets, usually <code>http://cjoc.&lt;namespace&gt;/cjoc</code></li> <li>REGISTRY : index.docker.io = DockerHub</li> <li>REPO: docker repository name</li> <li>IMAGE: docker image name</li> </ul> Jenkinsfile <p>Jenkins Declarative Pipeline for the CLI Image geberation.</p> <pre><code>pipeline {\n    agent {\n        kubernetes {\n        //cloud 'kubernetes'\n        label 'test'\n        yaml \"\"\"\nkind: Pod\nmetadata:\n  name: test\nspec:\n  containers:\n  - name: curl\n    image: byrnedo/alpine-curl\n    command:\n    - cat\n    tty: true\n    resources:\n      requests:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor:debug\n    imagePullPolicy: Always\n    command:\n    - /busybox/cat\n    tty: true\n    resources:\n      requests:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n    volumeMounts:\n      - name: jenkins-docker-cfg\n        mountPath: /root\n  volumes:\n  - name: jenkins-docker-cfg\n    projected:\n      sources:\n      - secret:\n          name: docker-credentials\n          items:\n            - key: .dockerconfigjson\n              path: .docker/config.json\n\"\"\"\n        }\n    }\n    environment {\n        CJOC_URL    = 'http://cjoc.cloudbees-core/cjoc'\n        CLI_VERSION = ''\n        REGISTRY    = 'index.docker.io'\n        REPO        = 'caladreas'\n        IMAGE       = 'cbcore-cli'\n    }\n    stages {\n        stage('Download CLI') {\n            steps {\n                container('curl') {\n                    sh 'curl --version'\n                    sh 'echo ${CJOC_URL}/jnlpJars/jenkins-cli.jar'\n                    sh 'curl ${CJOC_URL}/jnlpJars/jenkins-cli.jar --output jenkins-cli.jar'\n                    sh 'ls -lath'\n                }\n            }\n        }\n        stage('Prepare') {\n            parallel {\n                stage('Verify CLI') {\n                    environment {\n                        CREDS   = credentials('jenkins-api')\n                        CLI     = \"java -jar jenkins-cli.jar -noKeyAuth -s ${CJOC_URL} -auth\"\n                    }\n                    steps {\n                        sh 'echo ${CLI}'\n                        script {\n                            CLI_VERSION = sh returnStdout: true, script: '${CLI} ${CREDS} version'\n                        }\n                        sh 'echo ${CLI_VERSION}'\n                    }\n                }\n                stage('Prepare Dockerfile') {\n                    steps {\n                        writeFile encoding: 'UTF-8', file: 'Dockerfile', text: \"\"\"FROM mcr.microsoft.com/java/jre-headless:8u192-zulu-alpine\nWORKDIR /usr/bin\nADD jenkins-cli.jar .\nRUN pwd\nRUN ls -lath\n\"\"\"\n                    }\n                }\n            }\n        }\n        stage('Build with Kaniko') {\n            environment { \n                PATH = \"/busybox:/kaniko:$PATH\"\n                TAG  = \"${CLI_VERSION}\"\n            }\n            steps {\n                sh 'echo image fqn=${REGISTRY}/${REPO}/${IMAGE}:${TAG}'\n                container(name: 'kaniko', shell: '/busybox/sh') {\n                    sh '''#!/busybox/sh\n                    /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:${TAG}\n                    /kaniko/executor -f `pwd`/Dockerfile -c `pwd` --cleanup --cache=true --destination=${REGISTRY}/${REPO}/${IMAGE}:latest\n                    '''\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"cloudbees/teams-automation/#pr-pipeline","title":"PR Pipeline","text":"<p>Caution</p> <p>The PR Pipeline example builds upon the GitHub API, if you're not using GItHub, you will have to figure out another way to make the PR.</p>"},{"location":"cloudbees/teams-automation/#tools-used","title":"Tools Used","text":"<ul> <li>yq: commandline tool for processing Yaml files</li> <li>jq commandline tool for pressing Json files </li> <li>Kustomize templating tool for Kubernetes Yaml, as of Kubernetes <code>1.13</code>, this is part of the Client (note, your server can be older, don't worry!)</li> <li>Hub commandline client for GitHub</li> </ul>"},{"location":"cloudbees/teams-automation/#repository-layout","title":"Repository Layout","text":"<ul> <li>folder: <code>team-master-template</code><ul> <li>with file <code>simple.json</code></li> </ul> </li> <li>folder: <code>namespace-creation</code><ul> <li>with folder: <code>kustomize</code> this contains the Kustomize configuration</li> </ul> </li> </ul> Simple.json <p>This is a template for the team JSON definition.</p> <pre><code>{\n    \"version\" : \"1\",\n    \"data\": {\n        \"name\": \"NAME\",\n        \"displayName\": \"DISPLAY_NAME\",\n        \"provisioningRecipe\": \"RECIPE\",\n        \"members\": [\n            {\n                \"id\": \"ADMINS\",\n                \"roles\": [\"TEAM_ADMIN\"]\n            },\n            {\n                \"id\": \"MEMBERS\",\n                \"roles\": [\"TEAM_MEMBER\"]\n            },\n            {\n                \"id\": \"GUESTS\",\n                \"roles\": [\"TEAM_GUEST\"]\n            }\n        ],\n        \"icon\": {\n            \"name\": \"ICON\",\n            \"color\": \"HEX_COLOR\"\n        }\n    }\n}\n</code></pre>"},{"location":"cloudbees/teams-automation/#kustomize-configuration","title":"Kustomize Configuration","text":"<p>Kustomize is a tool for template Kubernetes YAML definitions, which is what we need here. However, only for the <code>namespace</code> creation &amp; configuration. So if you don't want to do that, you can skip this.</p> <p>The Kustomize configuration has two parts, a folder called <code>team-example</code> with a <code>kustomization.yaml</code>. This will be what we configure to generate a new yaml definition. The main template is in the folder <code>base</code>, where the entrypoint will be again <code>kustomization.yaml</code>. This time, the <code>kustomization.yaml</code> will link to all the template files we need.</p> <p>As posting all these yaml files again is a bit much, I'll link to my example repo. Feel free to fork it instead: cb-team-gitops-template</p> <ul> <li>configmap.yaml: the Jenkins Agent ConfigMap</li> <li>namespace.yaml: the new namespace</li> <li>resource-quota.yaml: resource quota's for the namespace</li> <li>role-binding-cjoc.yaml: a role binding for the CJOC ServiceAccount, so it create create the new Master in the new <code>namespace</code></li> <li>role-binding.yaml: the role binding for the <code>jenkins</code> ServiceAccount, which allows the new Master to create and manage Pods (for PodTemplates)</li> <li>role-cjoc.yaml: the role for CJOC for the ability to create a Master in the new Namspace</li> <li>role.yaml: the role for the <code>jenkins</code> ServiceAccount for the new Master</li> <li>service-account.yaml: the ServiceAccount, <code>jenkins</code>, used by the new Master</li> </ul>"},{"location":"cloudbees/teams-automation/#pipeline_1","title":"Pipeline","text":"<p>The Pipeline will do the following:</p> <ul> <li>capture input parameters to be used to customize the Team Master</li> <li>update the Kustomize template to make sure every resource is correct for the new namespace (<code>teams-&lt;name of team&gt;</code>)</li> <li>execute Kustomize to generate a single <code>yaml</code> file that defines the configuration for the new Team Masters' namespace</li> <li>process the <code>simple.json</code> to generate a <code>team.json</code> file for the new Team Master for use with the Jenkins CLI</li> <li>checkout your GIT_REPO that contains your team definitions</li> <li>create a new PR to your GIT_REPO for the new team</li> </ul> Jenkinsfile <p>Variables to update:</p> <ul> <li>GIT_REPO: the GitHub repository in which the Team Definitions are stored</li> <li>RESET_NAMESPACE: the namespace Operations Center should use as default</li> </ul> <pre><code>  pipeline {\n      agent {\n          kubernetes {\n          label 'team-automation'\n          yaml \"\"\"\n  kind: Pod\n  spec:\n    containers:\n    - name: hub\n      image:  caladreas/hub\n      command: [\"cat\"]\n      tty: true\n      resources:\n        requests:\n          memory: \"50Mi\"\n          cpu: \"150m\"\n        limits:\n          memory: \"50Mi\"\n          cpu: \"150m\"\n    - name: kubectl\n      image: bitnami/kubectl:latest\n      command: [\"cat\"]\n      tty: true\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n      resources:\n        requests:\n          memory: \"50Mi\"\n          cpu: \"100m\"\n        limits:\n          memory: \"150Mi\"\n          cpu: \"200m\"\n    - name: yq\n      image: mikefarah/yq\n      command: ['cat']\n      tty: true\n      resources:\n        requests:\n          memory: \"50Mi\"\n          cpu: \"100m\"\n        limits:\n          memory: \"50Mi\"\n          cpu: \"100m\"\n    - name: jq\n      image: colstrom/jq\n      command: ['cat']\n      tty: true\n      resources:\n        requests:\n          memory: \"50Mi\"\n          cpu: \"100m\"\n        limits:\n          memory: \"50Mi\"\n          cpu: \"100m\"\n\n  \"\"\"\n          }\n      }\n      libraries {\n          lib('github.com/joostvdg/jpl-core')\n      }\n      options {\n          disableConcurrentBuilds() // we don't want more than one at a time\n          checkoutToSubdirectory 'templates' // we need to do two checkouts\n          buildDiscarder logRotator(artifactDaysToKeepStr: '', artifactNumToKeepStr: '', daysToKeepStr: '5', numToKeepStr: '5') // always clean up\n      }\n      environment {\n          envGitInfo          = ''\n          RESET_NAMESPACE     = 'jx-production'\n          TEAM_BASE_NAME      = ''\n          NAMESPACE_TO_CREATE = ''\n          DISPLAY_NAME        = ''\n          TEAM_RECIPE         = ''\n          ICON                = ''\n          ICON_COLOR_CODE     = ''\n          ADMINS_ROLE         = ''\n          MEMBERS_ROLE        = ''\n          GUESTS_ROLE         = ''\n          RECORD_LOC          = ''\n          GIT_REPO                 = ''\n      }\n      stages {\n          stage('Team Details') {\n              input {\n                  message \"Please enter the team details.\"\n                  ok \"Looks good, proceed\"\n                  parameters {\n                      string(name: 'Name', defaultValue: 'hex', description: 'Please specify a team name')\n                      string(name: 'DisplayName', defaultValue: 'Hex', description: 'Please specify a team display name')\n                      choice choices: ['joostvdg', 'basic', 'java-web'], description: 'Please select a Team Recipe', name: 'TeamRecipe'\n                      choice choices: ['anchor', 'bear', 'bowler-hat', 'briefcase', 'bug', 'calculator', 'calculatorcart', 'clock', 'cloud', 'cloudbees', 'connect', 'dollar-bill', 'dollar-symbol', 'file', 'flag', 'flower-carnation', 'flower-daisy', 'help', 'hexagon', 'high-heels', 'jenkins', 'key', 'marker', 'monocle', 'mustache', 'office', 'panther', 'paw-print', 'teacup', 'tiger', 'truck'], description: 'Please select an Icon', name: 'Icon'\n                      string(name: 'IconColorCode', defaultValue: '#CCCCCC', description: 'Please specify a valid html hexcode for the color (https://htmlcolorcodes.com/)')\n                      string(name: 'Admins', defaultValue: 'Catmins', description: 'Please specify a groupid or userid for the TEAM_ADMIN role')\n                      string(name: 'Members', defaultValue: 'Pirates', description: 'Please specify a groupid or userid for the TEAM_MEMBER role')\n                      string(name: 'Guests', defaultValue: 'Continental', description: 'Please specify a groupid or userid for the TEAM_GUEST role')\n                  }\n              }\n              steps {\n                  println \"Name=${Name}\"\n                  println \"DisplayName=${DisplayName}\"\n                  println \"TeamRecipe=${TeamRecipe}\"\n                  println \"Icon=${Icon}\"\n                  println \"IconColorCode=${IconColorCode}\"\n                  println \"Admins=${Admins}\"\n                  println \"Members=${Members}\"\n                  println \"Guests=${Guests}\"\n                  script {\n                      TEAM_BASE_NAME      = \"${Name}\"\n                      NAMESPACE_TO_CREATE = \"cb-teams-${Name}\"\n                      DISPLAY_NAME        = \"${DisplayName}\"\n                      TEAM_RECIPE         = \"${TeamRecipe}\"\n                      ICON                = \"${Icon}\"\n                      ICON_COLOR_CODE     = \"${IconColorCode}\"\n                      ADMINS_ROLE         = \"${Admins}\"\n                      MEMBERS_ROLE        = \"${Members}\"\n                      GUESTS_ROLE         = \"${Guests}\"\n                      RECORD_LOC          = \"templates/teams/${Name}\"\n                      sh \"mkdir -p ${RECORD_LOC}\"\n                  }\n              }\n          }\n          stage('Create Team Config') {\n              environment {\n                  BASE        = 'templates/namespace-creation/kustomize'\n                  NAMESPACE   = \"${NAMESPACE_TO_CREATE}\"\n                  RECORD_LOC  = \"templates/teams/${TEAM_BASE_NAME}\"\n              }\n              parallel {\n                  stage('Namespace') {\n                      steps {\n                          container('yq') {\n                              sh 'yq w -i ${BASE}/base/role-binding.yaml subjects[0].namespace ${NAMESPACE}'\n                              sh 'yq w -i ${BASE}/base/namespace.yaml metadata.name ${NAMESPACE}'\n                              sh 'yq w -i ${BASE}/team-example/kustomization.yaml namespace ${NAMESPACE}'\n                          }\n                          container('kubectl') {\n                              sh '''\n                                  kubectl kustomize ${BASE}/team-example &gt; ${RECORD_LOC}/team.yaml\n                                  cat ${RECORD_LOC}/team.yaml\n                              '''\n                          }\n                      }\n                  }\n                  stage('Team Master JSON') {\n                      steps {\n                          container('jq') {\n                              sh \"\"\"jq \\\n                              '.data.name = \"${TEAM_BASE_NAME}\" |\\\n                              .data.displayName = \"${DISPLAY_NAME}\" |\\\n                              .data.provisioningRecipe = \"${TEAM_RECIPE}\" |\\\n                              .data.icon.name = \"${ICON}\" |\\\n                              .data.icon.color = \"${ICON_COLOR_CODE}\" |\\\n                              .data.members[0].id = \"${ADMINS_ROLE}\" |\\\n                              .data.members[1].id = \"${MEMBERS_ROLE}\" |\\\n                              .data.members[2].id = \"${GUESTS_ROLE}\"'\\\n                              templates/team-master-template/simple.json &gt; ${RECORD_LOC}/team.json\n                              \"\"\"\n                          }\n                          sh 'cat ${RECORD_LOC}/team.json'\n                      }\n                  }\n              }\n          }\n          stage('Create PR') {\n              when { branch 'master'}\n              environment {\n                  RECORD_OLD_LOC  = \"templates/teams/${TEAM_BASE_NAME}\"\n                  RECORD_LOC      = \"teams/${TEAM_BASE_NAME}\"\n                  PR_CHANGE_NAME  = \"add_team_${TEAM_BASE_NAME}\"\n              }\n              steps {\n                  container('hub') {\n                      dir('cb-team-gitops') {\n                          script {\n                              envGitInfo = git \"${GIT_REPO}\"\n                          }\n                          sh 'git checkout -b ${PR_CHANGE_NAME}'\n                          sh 'ls -lath ../${RECORD_OLD_LOC}'\n                          sh 'cp -R ../${RECORD_OLD_LOC} ./teams'\n                          sh 'ls -lath'\n                          sh 'ls -lath teams/'\n\n                          gitRemoteConfigByUrl(envGitInfo.GIT_URL, 'githubtoken_token') // must be a API Token ONLY -&gt; secret text\n                          sh '''\n                          git config --global user.email \"jenkins@jenkins.io\"\n                          git config --global user.name \"Jenkins\"\n                          git add ${RECORD_LOC}\n                          git status\n                          git commit -m \"add team ${TEAM_BASE_NAME}\"\n                          git push origin ${PR_CHANGE_NAME}\n                          '''\n\n\n                          // has to be indented like that, else the indents will be in the pr description\n                          writeFile encoding: 'UTF-8', file: 'pr-info.md', text: \"\"\"Add ${TEAM_BASE_NAME}\n  \\n\n  This pr is automatically generated via CloudBees.\\\\n\n  \\n\n  The job: ${env.JOB_URL}\n                      \"\"\"\n\n                          // TODO: unfortunately, environment {}'s credentials have fixed environment variable names\n                          // TODO: in this case, they need to be EXACTLY GITHUB_PASSWORD and GITHUB_USER\n                          script {\n                              withCredentials([usernamePassword(credentialsId: 'githubtoken', passwordVariable: 'GITHUB_PASSWORD', usernameVariable: 'GITHUB_USER')]) {\n                                  sh \"\"\"\n                                  set +x\n                                  hub pull-request --force -F pr-info.md -l '${TEAM_BASE_NAME}' --no-edit\n                                  \"\"\"\n                              }\n                          }\n                      }\n                  }\n              }\n          }\n      }\n  }\n</code></pre>"},{"location":"cloudbees/teams-automation/#main-pipeline","title":"Main Pipeline","text":"<p>The main Pipeline should be part of a repository. The Repository should look like this:</p> <ul> <li><code>recipes</code> (folder)<ul> <li><code>recipes.json</code> -&gt; current complete list of CloudBees Core Team Recipes definition</li> </ul> </li> <li><code>teams</code> (folder)<ul> <li>folder per team<ul> <li><code>team.json</code> -&gt; CloudBees Core Team definition</li> <li><code>team.yaml</code> -&gt; Kubernetes YAML definition of the <code>namespace</code> and all its resources</li> </ul> </li> </ul> </li> </ul>"},{"location":"cloudbees/teams-automation/#process","title":"Process","text":"<p>The pipeline can be a bit hard to grasp, so let me break it down into individual steps.</p> <p>We have the following stages:</p> <ul> <li><code>Create Team</code> - which is broken into sub-stages via the sequential stages feature.         * <code>Parse Changelog</code>         * <code>Create Namespace</code>         * <code>Change OC Namespace</code>         * <code>Create Team Master</code></li> <li><code>Test CLI Connection</code></li> <li><code>Update Team Recipes</code></li> </ul>"},{"location":"cloudbees/teams-automation/#notable-statements","title":"Notable Statements","text":"disableConcurrentBuilds <p>We change the <code>namespace</code> of Operation Center to a different value only for the duration of creating this master. This is something that should probably be part of the Team Master creation, but as it is a single configuration option for all that Operation Center does, we need to be careful. By ensuring we only run one build concurrently, we reduce the risk of this blowing up in our face.</p> <pre><code>options {\n    disableConcurrentBuilds()\n}\n</code></pre> when { } <p>The When Directive allows us to creating effective conditions for when a stage should be executed.</p> <p>The snippet below shows the use of a combination of both the <code>branch</code> and <code>changeset</code> built-in filters. <code>changeset</code> looks at the commit being build and validates that there was a change in that file path.</p> <pre><code>    when { allOf { branch 'master'; changeset \"teams/**/team.*\" } }\n</code></pre> post { always { } } <p>The Post Directive allows us to run certain commands after the main pipeline has run depending on the outcome (compared or not to the previous outcome). In this case, we want to make sure we reset the <code>namespace</code> used by Operations Center to the original value.</p> <p>By using <code>post { always {} }</code>, it will ALWAYS run, regardless of the status of the pipeline. So we should be safe.</p> <pre><code>post {\n    always {\n        container('cli') {\n            sh '${CLI} ${CREDS} groovy = &lt; resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE}'\n        }\n    }\n}\n</code></pre> stages { stage { parallel { stage() { stages { stage {  <p>Oke, you might've noticed this massive indenting depth and probably have some questions. </p> <p>By combining sequential stages with parallel stages we can create a set of stages that will be executed in sequence but can be controlled by a single <code>when {}</code> statement whether or not they get executed. </p> <p>This prevents mistakes being made in the condition and accidentally running one or other but not all the required steps.</p> <pre><code>    stages {\n        stage('Create Team') {\n            parallel {\n                stage('Main') {\n                    stages {\n                        stage('Parse Changelog') {\n</code></pre> changetSetData &amp; container('jpb') {} <p>Alright, so even if we know a team was added in <code>/teams/&lt;team-name&gt;</code>, we still don't know the following two things: 1) what is the name of this team, 2) was this team changed or deleted?</p> <p>So we have to process the changelog to be able to answer these questions as well. There are different ways of getting the changelog and parsing it. I've written one you can do on ANY machine, regardless of Jenkins by leveraging <code>Git</code> and my own custom binary (<code>jpb</code> -&gt; Jenkins Pipeline Binary). The code for my binary is at GitHub: github.com/joostvdg/jpb.</p> <p>An alternative approach is described by CloudBees Support here, which leverages Jenkins groovy powers.</p> <pre><code>COMMIT_INFO = \"${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT}\"\ndef changeSetData = sh returnStdout: true, script: \"git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO}\"\nchangeSetData = changeSetData.replace(\"\\n\", \"\\\\n\")\ncontainer('jpb') {\n    changeSetFolders = sh returnStdout: true, script: \"/usr/bin/jpb/bin/jpb GitChangeListToFolder '${changeSetData}' 'teams/'\"\n    changeSetFolders = changeSetFolders.split(',')\n}\n</code></pre>"},{"location":"cloudbees/teams-automation/#files","title":"Files","text":"recipes.json <p>The default Team Recipes that ships with CloudBees Core Modern.</p> <pre><code>    {\n        \"version\": \"1\",\n        \"data\": [{\n            \"name\": \"basic\",\n            \"displayName\": \"Basic\",\n            \"description\": \"The minimalistic setup.\",\n            \"plugins\": [\"bluesteel-master\", \"cloudbees-folders-plus\", \"cloudbees-jsync-archiver\", \"cloudbees-monitoring\", \"cloudbees-nodes-plus\", \"cloudbees-ssh-slaves\", \"cloudbees-support\", \"cloudbees-workflow-template\", \"credentials-binding\", \"email-ext\", \"git\", \"git-client\", \"github-branch-source\", \"github-organization-folder\", \"infradna-backup\", \"ldap\", \"mailer\", \"operations-center-analytics-reporter\", \"operations-center-cloud\", \"pipeline-model-definition\", \"ssh-credentials\", \"wikitext\", \"workflow-aggregator\", \"workflow-cps-checkpoint\"],\n            \"default\": true\n        }, {\n            \"name\": \"java-web\",\n            \"displayName\": \"Java &amp; Web Development\",\n            \"description\": \"The essential tools to build, release and deploy Java Web applications including integration with Maven, Gradle and Node JS.\",\n            \"plugins\": [\"bluesteel-master\", \"cloudbees-folders-plus\", \"cloudbees-jsync-archiver\", \"cloudbees-monitoring\", \"cloudbees-nodes-plus\", \"cloudbees-ssh-slaves\", \"cloudbees-support\", \"cloudbees-workflow-template\", \"credentials-binding\", \"email-ext\", \"git\", \"git-client\", \"github-branch-source\", \"github-organization-folder\", \"infradna-backup\", \"ldap\", \"mailer\", \"operations-center-analytics-reporter\", \"operations-center-cloud\", \"pipeline-model-definition\", \"ssh-credentials\", \"wikitext\", \"workflow-aggregator\", \"workflow-cps-checkpoint\", \"config-file-provider\", \"cloudbees-aws-cli\", \"cloudbees-cloudfoundry-cli\", \"findbugs\", \"gradle\", \"jira\", \"junit\", \"nodejs\", \"openshift-cli\", \"pipeline-maven\", \"tasks\", \"warnings\"],\n            \"default\": false\n        }]\n    }\n</code></pre> Jenkinsfile <p>This is the pipeline that will process the commit to the repository and, if it detects a new team is created will apply the changes.</p> <p>Variables to overwrite:</p> <ul> <li>GIT_REPO: the https url to the Git Repository your GitOps code/configuration is stored</li> <li>RESET_NAMESPACE: the <code>namespace</code> your Operation Center normally operates in</li> <li>CLI: this command depends on the namespace Operation Center is in (<code>http://&lt;service name&gt;.&lt;namespace&gt;/cjoc</code>)</li> </ul> <pre><code>pipeline {\n    agent {\n        kubernetes {\n            label 'jenkins-agent'\n            yaml '''\napiVersion: v1\nkind: Pod\nspec:\n  serviceAccountName: jenkins\n  containers:\n  - name: cli\n    image: caladreas/cbcore-cli:2.176.2.3\n    imagePullPolicy: Always\n    command:\n    - cat\n    tty: true\n    resources:\n      requests:\n        memory: \"50Mi\"\n        cpu: \"150m\"\n      limits:\n        memory: \"50Mi\"\n        cpu: \"150m\"\n  - name: kubectl\n    image: bitnami/kubectl:latest\n    command: [\"cat\"]\n    tty: true\n    resources:\n      requests:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"150Mi\"\n        cpu: \"200m\"\n  - name: yq\n    image: mikefarah/yq\n    command: ['cat']\n    tty: true\n    resources:\n      requests:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n  - name: jpb\n    image: caladreas/jpb\n    command:\n    - cat\n    tty: true\n    resources:\n      requests:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"50Mi\"\n        cpu: \"100m\"\n  securityContext:\n    runAsUser: 1000\n    fsGroup: 1000\n'''\n        }\n    }\n    options {\n        disableConcurrentBuilds()\n        buildDiscarder logRotator(artifactDaysToKeepStr: '', artifactNumToKeepStr: '', daysToKeepStr: '5', numToKeepStr: '5')\n    }\n    environment {\n        RESET_NAMESPACE     = 'cloudbees-core'\n        CREDS               = credentials('jenkins-api')\n        CLI                 = \"java -jar /usr/bin/jenkins-cli.jar -noKeyAuth -s http://cjoc.cloudbees-core/cjoc -auth\"\n        COMMIT_INFO         = ''\n        TEAM                = ''\n        GIT_REPO            = ''\n    }\n    stages {\n        stage('Create Team') {\n            when { allOf { branch 'master'; changeset \"teams/**/team.*\" } }\n            parallel {\n                stage('Main') {\n                    stages {\n                        stage('Parse Changelog') {\n                            steps {\n                                // Alternative approach: https://support.cloudbees.com/hc/en-us/articles/217630098-How-to-access-Changelogs-in-a-Pipeline-Job-\n                                // However, that runs on the master, JPB runs in an agent!\n                                script {\n                                    scmVars = git \"${GIT_REPO}\"\n                                    COMMIT_INFO = \"${scmVars.GIT_COMMIT} ${scmVars.GIT_PREVIOUS_COMMIT}\"\n                                    def changeSetData = sh returnStdout: true, script: \"git diff-tree --no-commit-id --name-only -r ${COMMIT_INFO}\"\n                                    changeSetData = changeSetData.replace(\"\\n\", \"\\\\n\")\n                                    container('jpb') {\n                                        changeSetFolders = sh returnStdout: true, script: \"/usr/bin/jpb/bin/jpb GitChangeListToFolder '${changeSetData}' 'teams/'\"\n                                        changeSetFolders = changeSetFolders.split(',')\n                                    }\n                                    if (changeSetFolders.length &gt; 0) {\n                                        TEAM = changeSetFolders[0]\n                                        TEAM = TEAM.trim()\n                                        // to protect against a team being removed\n                                        def exists = fileExists \"teams/${TEAM}/team.yaml\"\n                                        if (!exists) {\n                                            TEAM = ''\n                                        }\n                                    } else {\n                                        TEAM = ''\n                                    }\n                                    echo \"Team that changed: |${TEAM}|\"\n                                }\n                            }\n                        }\n                        stage('Create Namespace') {\n                            when { expression { return !TEAM.equals('') } }\n                            environment {\n                                NAMESPACE   = \"cb-teams-${TEAM}\"\n                                RECORD_LOC  = \"teams/${TEAM}\"\n                            }\n                            steps {\n                                container('kubectl') {\n                                    sh '''\n                                        cat ${RECORD_LOC}/team.yaml\n                                        kubectl apply -f ${RECORD_LOC}/team.yaml\n                                    '''\n                                }\n                            }\n                        }\n                        stage('Change OC Namespace') {\n                            when { expression { return !TEAM.equals('') } }\n                            environment {\n                                NAMESPACE   = \"cb-teams-${TEAM}\"\n                            }\n                            steps {\n                                container('cli') {\n                                    sh 'echo ${NAMESPACE}'\n                                    script {\n                                        def response = sh encoding: 'UTF-8', label: 'create team', returnStatus: true, script: '${CLI} ${CREDS} groovy = &lt; resources/bootstrap/configure-oc-namespace.groovy ${NAMESPACE}'\n                                        println \"Response: ${response}\"\n                                    }\n                                }\n                            }\n                        }\n                        stage('Create Team Master') {\n                            when { expression { return !TEAM.equals('') } }\n                            environment {\n                                TEAM_NAME = \"${TEAM}\"\n                            }\n                            steps {\n                                container('cli') {\n                                    println \"TEAM_NAME=${TEAM_NAME}\"\n                                    sh 'ls -lath'\n                                    sh 'ls -lath teams/'\n                                    script {\n                                        def response = sh encoding: 'UTF-8', label: 'create team', returnStatus: true, script: '${CLI} ${CREDS} teams ${TEAM_NAME} --put &lt; \"teams/${TEAM_NAME}/team.json\"'\n                                        println \"Response: ${response}\"\n                                    }\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n        stage('Test CLI Connection') {\n            steps {\n                container('cli') {\n                    script {\n                        def response = sh encoding: 'UTF-8', label: 'retrieve version', returnStatus: true, script: '${CLI} ${CREDS} version'\n                        println \"Response: ${response}\"\n                    }\n                }\n            }\n        }\n        stage('Update Team Recipes') {\n            when { allOf { branch 'master'; changeset \"recipes/recipes.json\" } }\n            steps {\n                container('cli') {\n                    sh 'ls -lath'\n                    sh 'ls -lath recipes/'\n                    script {\n                        def response = sh encoding: 'UTF-8', label: 'update team recipe', returnStatus: true, script: '${CLI} ${CREDS} team-creation-recipes --put &lt; \"recipes/recipes.json\"'\n                        println \"Response: ${response}\"\n                    }\n                }\n            }\n        }\n    }\n    post {\n        always {\n            container('cli') {\n                sh '${CLI} ${CREDS} groovy = &lt; resources/bootstrap/configure-oc-namespace.groovy ${RESET_NAMESPACE}'\n            }\n        }\n    }\n}        \n</code></pre>"},{"location":"devops/","title":"DevOps Assessment","text":"<p>How can I assess an organisation for what to do next.</p>"},{"location":"devops/#questions","title":"Questions","text":"<ul> <li>To which extent can your development teams request/create an environment on their own, without going through lengthy approval processes?</li> <li>To which extent can your development teams use pre-configured/ template tool sets (e.g. Jenkins master jobs, master POM etc) which they can extend and/or modify to their needs?</li> <li>To which extent can your developments teams deploy to any environment (including production)? If not, what do they lack: knowledge or passwords to higher environments?</li> <li>Does your system of record provide you tractability from idea to production?</li> <li>How tightly coupled are your key delivery pipeline tools? <ul> <li>Is it easy to replace them?</li> </ul> </li> <li>Do you have different release management activities based on application blocks?<ul> <li>Who is keeping it up-to-date?</li> </ul> </li> </ul>"},{"location":"devops/#maturity-model","title":"Maturity Model","text":""},{"location":"devops/#resources","title":"Resources","text":"<ul> <li>https://www.devon.nl/continuous-delivery-at-enterprise-level-pitfall-1/</li> <li>https://www.devon.nl/continuous-delivery-at-enterprise-level-pitfall-2/</li> <li>https://devops-research.com/</li> </ul>"},{"location":"devops/#references","title":"References","text":""},{"location":"devops/pipeline-model/","title":"Pipeline Modal","text":""},{"location":"devops/pipeline-model/#ideas","title":"Ideas","text":"<ul> <li>Continuous Integration</li> <li>Continuous Delivery</li> <li>Continuous Deployment</li> <li> <p>Progressive Delivery</p> </li> <li> <p>Continuous, Iterative, Cyclic</p> </li> <li>Small cycle: local dev, CI<ul> <li>goal: get it to work</li> </ul> </li> <li> <p>Large cycle: CD - CDP - ProgDY</p> <ul> <li>goal: deliver value to customers</li> </ul> </li> <li> <p>DevOps -&gt; single team managing both Small and Large cycle</p> </li> </ul>"},{"location":"devops/pipeline-model/#genericshared-concepts","title":"Generic/Shared Concepts","text":""},{"location":"devops/pipeline-model/#jenkins","title":"Jenkins","text":""},{"location":"devops/pipeline-model/#tekton","title":"Tekton","text":""},{"location":"devops/pipeline-model/#jenkins-x","title":"Jenkins X","text":""},{"location":"devops/pipeline-model/#flow","title":"Flow","text":""},{"location":"devops/pipeline-model/#concepts","title":"Concepts","text":"<ul> <li>Pipeline<ul> <li>Stage</li> <li>Task</li> <li>Pipeline</li> <li>Pipeline Run</li> <li>Entry Gate</li> <li>Exit Gate</li> </ul> </li> <li>Other<ul> <li>Server</li> <li>Application</li> <li>Resource</li> <li>Release</li> <li>Environment</li> <li>Application</li> <li>Application Deployment</li> <li>Microservice (seems tied to containers?)</li> <li>Microservice Deployment</li> <li>Production</li> <li>Bill-of-Materials</li> <li>Payload</li> <li>Process</li> <li>Approval</li> <li>Version</li> <li>Manifest</li> <li>Components</li> <li>Self-Service Catalog</li> </ul> </li> </ul>"},{"location":"devops/progressive-delivery/","title":"Progressive Delivery","text":""},{"location":"devops/progressive-delivery/#resources","title":"Resources","text":"<ul> <li>https://redmonk.com/jgovernor/2018/08/06/towards-progressive-delivery/</li> <li>https://chrisshort.tumblr.com/post/176701070950/recommended-read-towards-progressive-delivery</li> <li>https://dzone.com/articles/gitops-workflows-for-istio-canary-deployments</li> <li>https://launchdarkly.com/blog/progressive-delivery-a-history-condensed/</li> <li>https://tech.target.com/infrastructure/2018/06/20/enter-unimatrix.html</li> <li>https://dzone.com/articles/deployments-at-scale-using-kubernetes-and-launchda</li> <li>https://blog.csanchez.org/2019/01/22/progressive-delivery-in-kubernetes-blue-green-and-canary-deployments/</li> <li>https://blog.csanchez.org/2019/01/24/progressive-delivery-with-jenkins-x/</li> <li>https://blog.csanchez.org/2019/03/05/progressive-delivery-with-jenkins-x-automatic-canary-deployments/</li> <li>https://devblogs.microsoft.com/devops/configuring-your-release-pipelines-for-safe-deployments/</li> <li>https://www.linkedin.com/pulse/counting-down-zero-time-takes-launch-app-target-tom-kadlec-1/</li> </ul>"},{"location":"devops/sdm/","title":"Software Development Management","text":""},{"location":"devops/sdm/#related-concepts","title":"Related Concepts","text":"<ul> <li>Agile<ul> <li>scrum</li> <li>safe</li> <li>kanban</li> <li>xp</li> </ul> </li> <li>DevOps</li> <li>Lean</li> <li>Systems Thinking</li> <li>Observability</li> <li>SRE</li> <li>EmpathyOps</li> <li>DevSecOps</li> <li>InfoSec</li> <li>Pets vs. Cattle</li> <li>Value Streams</li> <li>Automation</li> <li>Self-service</li> <li>2-Pizza Teams</li> <li>Spotify Model</li> <li>Feature Flags</li> <li>Development Workflows<ul> <li>Trunk Based Development</li> <li>TDD / BDD</li> <li>DDD</li> </ul> </li> <li>Testing<ul> <li>Automated</li> <li>Exploratory</li> <li>Functional</li> <li>Penetration</li> <li>Static/Dynamic Code Analysis</li> <li>Policies</li> <li>Unit</li> <li>Blacbox</li> <li>...</li> </ul> </li> <li>Development Paradigms<ul> <li>Object Oriented</li> <li>Functional</li> <li>Procedural</li> <li>Imperative</li> <li>Declarative</li> </ul> </li> <li>Wardley Maps</li> <li>Undifferentiated Heavy Lifting</li> <li>Layers Of Abstraction</li> <li>Models<ul> <li>Three-Tier-Model</li> <li>MVC</li> <li>Enterprise Architecture Patterns</li> <li>Evolutionary Architecture</li> <li>Synchronous</li> <li>Asynchronous Or Reactive</li> <li>Streaming</li> <li>CQRS</li> <li>Event Sourcing</li> </ul> </li> <li>Premature Optimization</li> <li>Theory of Constraints</li> <li>Segregation Of Duties</li> <li>Feedback Cycles</li> <li>Center Of Excellence</li> <li>Developer Productivity Teams</li> <li>Shared Services</li> <li>Product vs Project</li> <li>Design Thinking</li> <li>Systems Thinking</li> <li>1-2-Many</li> <li>Evaluation Techniques<ul> <li>Blameless Postmortem</li> <li>Retrospective</li> <li>The Five Why's</li> <li>Root Cause Analysis</li> </ul> </li> <li>Behavioral Economics</li> </ul>"},{"location":"devops/sdm/#anatomy-of-a-toolchain","title":"Anatomy Of A Toolchain","text":""},{"location":"devops/sdm/#essential-processes","title":"Essential Processes","text":"<ul> <li>versioning</li> <li>dependency management</li> <li>application tracking</li> <li>release</li> <li>deployment</li> <li>testing<ul> <li>point of testing</li> <li>level of testing</li> <li>goal of testing</li> </ul> </li> <li>publish</li> </ul>"},{"location":"devops/sdm/#adoption","title":"Adoption","text":"<ul> <li>Top Buys</li> <li>Bottom Adopts</li> </ul>"},{"location":"devops/sdm/#workflow-cycles","title":"Workflow Cycles","text":""},{"location":"devops/sdm/#the-small-cycle","title":"The Small Cycle","text":"<ul> <li>design</li> <li>build</li> <li>test</li> <li>refactor</li> <li>document</li> <li>publish</li> <li>evaluate</li> </ul>"},{"location":"devops/sdm/#resources","title":"Resources","text":"<ul> <li>https://info.container-solutions.com/info.container-solutions.com/understanding-cloud-native-cs-google-events-thankyou-1</li> <li>https://unixism.net/2019/08/a-managers-guide-to-kubernetes-adoption/?utm_source=DevOps%27ish&amp;utm_campaign=8bd4b37eb5-DEVOPSISH_145&amp;utm_medium=email&amp;utm_term=0_eab566bc9f-8bd4b37eb5-46458919</li> <li>https://www.allthingsdistributed.com/2019/08/modern-applications-at-aws.html</li> <li>https://blog.gitprime.com/individual-contributor-to-manager-julie-zhuo/</li> <li>https://martinfowler.com/articles/cd4ml.html</li> <li>https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started</li> </ul>"},{"location":"docker/build-kit/","title":"Docker Build with Build-Kit","text":"<p>Instead of investing in improving docker image building via the Docker Client, Docker created a new API and client library.</p> <p>This library called BuildKit, is completely independent. With Docker 18.09, it is included in the Docker Client allowing anyone to use it as easily as the traditional <code>docker image build</code>.</p> <p>BuildKit is already used by some other tools, such as Buildah and IMG, and allows you to create custom DSL \"Frontends\". As long as the API of BuikdKit is adhered to, the resulting image will be OCI compliant.</p>"},{"location":"docker/build-kit/#how-to-use-it","title":"How To Use It","text":"<p>So further remarks below and how to use it.</p> <ul> <li>BuildKit</li> <li>In-Depth session Supercharged Docker Build with BuildKit</li> <li>Usable from Docker <code>18.09</code></li> <li>HighLights:<ul> <li>allows custom DSL for specifying image (BuildKit) to still be used with Docker client/daemon</li> <li>build cache for your own files during build, think Go, Maven, Gradle...</li> <li>much more optimized, builds less, quicker, with more cache in less time</li> <li>support mounts (cache) such as secrets, during build phase</li> </ul> </li> </ul> <pre><code># Set env variable to enable\n# Or configure docker's json config\nexport DOCKER_BUILDKIT=1\n</code></pre>"},{"location":"docker/build-kit/#example","title":"Example","text":"<p>```dockerfile</p>"},{"location":"docker/build-kit/#syntaxdockerdockerfileexperimental","title":"syntax=docker/dockerfile:experimental","text":""},{"location":"docker/build-kit/#_1","title":"Docker BuildKit","text":""},{"location":"docker/build-kit/#1-build-jar-with-maven","title":"1. BUILD JAR WITH MAVEN","text":"<p>FROM maven:3.6-jdk-8 as BUILD WORKDIR /usr/src COPY . /usr/src ! RUN --mount=type=cache,target=/root/.m2/  mvn clean package -e</p>"},{"location":"docker/build-kit/#_2","title":"Docker BuildKit","text":""},{"location":"docker/build-kit/#2-build-native-image-with-graal","title":"2. BUILD NATIVE IMAGE WITH GRAAL","text":"<p>FROM oracle/graalvm-ce:1.0.0-rc9 as NATIVE_BUILD WORKDIR /usr/src COPY --from=BUILD /usr/src/ /usr/src RUN ls -lath /usr/src/target/ COPY /docker-graal-build.sh /usr/src RUN ./docker-graal-build.sh RUN ls -lath</p>"},{"location":"docker/build-kit/#_3","title":"Docker BuildKit","text":""},{"location":"docker/build-kit/#3-build-docker-runtime-image","title":"3. BUILD DOCKER RUNTIME IMAGE","text":"<p>FROM alpine:3.8 CMD [\"jpc-graal\"] COPY --from=NATIVE_BUILD /usr/src/jpc-graal /usr/local/bin/ RUN chmod +x /usr/local/bin/jpc-graal</p>"},{"location":"docker/build-kit/#_4","title":"Docker BuildKit","text":""},{"location":"docker/graceful-shutdown/","title":"Gracefully Shutting Down Applications in Docker","text":"<p>I'm not sure about you, but I prefer it when my neighbors leave our shared spaces clean and don't take up parking spaces when they don't need them.</p> <p>Imagine you live in an apartment complex with the above-mentioned parking lot. Some tenants go away and never come back. If nothing is done to clean up after them - to reclaim their apartment and parking space - then after some time, more and more apartments are unavailable for no reason, and the parking lot fills with cars which belong to no one.</p> <p>Some tenants did not get a parking lot and are getting frustrated that none are becoming available. When they moved in, they were told that when others leave, they would be next in line. While they're waiting, they have to park outside the complex. Eventually, the entrance gets blocked and no one can enter or leave. The end result is a completely unlivable apartment block with trapped tenants - never to be seen or heard from again.</p> <p>If you agree with me that when a tenant leaves, they should clean the apartment and free the parking spot to make it ready for the next inhabitant; then please read on. We're going to dive into the equivalent of doing this with containers.</p> <p>We will explore running our containers with Docker (run, compose, swarm) and Kubernetes. Even if you use another way to run your containers, this article should provide you with enough insight to get you on your way.</p>"},{"location":"docker/graceful-shutdown/#the-case-for-graceful-shutdown","title":"The case for graceful shutdown","text":"<p>We're in an age where many applications are running in Docker containers across a multitude of clusters. These applications are then confronted with new concerns to tackle such as more moving parts, networking between these parts, remote storage and others. One significant way we defend ourselves against the perils of this distributed nature is to make our applications more robust - able to survive errors.</p> <p>However, even then there is still no guarantee your application is always up and running. So another concern we should tackle is how it responds when it needs to shut down. Where we can differentiate between an unexpected shutdown - we crashed - or an expected shutdown. On top of that, failing instead of trying to recover when something bad happens also adheres to \"fail fast\" - as strongly advocated by Michael Nygard in ReleaseIt.</p> <p>Shutting down can happen for a variety of reasons, in this post we dive into how to deal with an expected shutdown such as it being told to stop by an orchestrator such as Kubernetes.</p> <p>Containers can be purposefully shut down for a variety of reasons, including but not limited too:</p> <ul> <li>your application's health check fails</li> <li>your application consumed more resources than allowed</li> <li>the application is scaling down</li> </ul> <p>Just as cleaning up when leaving makes you a better tenant, having your application clean up connections, resources Moreover, the more tenants behaving in a good way increases the quality of living for all tenants. In our case, it improves the reliability and consistency of our cluster.</p> <p>Graceful shutdown is not unique to Docker, as it has been part of Linux's best practices for quite some years before Docker's existence. However, applying them to Docker container adds extra dimensions.</p>"},{"location":"docker/graceful-shutdown/#start-good-so-you-can-end-well","title":"Start Good So You Can End Well","text":"<p>When you sign up for an apartment, you probably have to sign a contract detailing your rights and obligations. The more you state explicitly, the easier it is to deal with bad behaving neighbors. The same is true when running processes; we should make sure that we set the rules, obligations, and expectations from the start.</p> <p>As we say in Dutch: a good beginning is half the work. We will start with how you can run a process in a container with a process that shuts down gracefully.</p> <p>There are many ways to start a process in a container. In this article, we look at processes started by commands defined in a Dockerfile. There are two ways to specify this:</p> <ul> <li>CMD: runs a command when the container gets started</li> <li>ENTRYPOINT: provides the location (entrypoint) from where commands get run when the container starts</li> </ul> <p>You need at least one ENTRYPOINT or CMD in a Dockerfile for it to be valid. They can be used in collaboration but they can do similar things.</p> <p>For more information on the details of these commands, read\u00a0Docker's docs on Entrypoint vs. CMD.</p>"},{"location":"docker/graceful-shutdown/#docker-shell-form-example","title":"Docker Shell form example","text":"<p>We start with the shell form and see if it can do what we want; begin in such a way, we can stop it nicely. Shell form means we define a shell command without any special format or keywords.</p> <p>Please create Dockerfile with the content that follows.</p> <pre><code>FROM ubuntu:18.04\nENTRYPOINT top -b\n</code></pre> <p>Then build an image and run a container.</p> <pre><code>docker image build --tag shell-form .\ndocker run --name shell-form --rm shell-form\n</code></pre> <p>The above command yields the following output.</p> <pre><code>top - 16:34:56 up 1 day,  5:15,  0 users,  load average: 0.00, 0.00, 0.00\nTasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0.4 us,  0.3 sy,  0.0 ni, 99.2 id,  0.1 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem :  2046932 total,   541984 free,   302668 used,  1202280 buff/cache\nKiB Swap:  1048572 total,  1042292 free,     6280 used.  1579380 avail Mem\n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n    1 root      20   0    4624    760    696 S   0.0  0.0   0:00.05 sh\n    6 root      20   0   36480   2928   2580 R   0.0  0.1   0:00.01 top\n</code></pre> <p>As you can see, two processes are running, sh and top. Meaning, that killing the process, with ctrl+c for example, terminates the sh process, but not\u00a0top. This happens because the sh process forked the top process, but the termination will only be send to PID 1 - in this case sh. As sh will not stop the top process for us it will continue running and leave the container alive.</p> <p>To kill this container, open a second terminal and execute the following command.</p> <pre><code>docker rm -f shell-form\n</code></pre> <p>Shell form doesn't do what we need. Starting a process with shell form will only lead us to the disaster of parking lots filling up unless there's a someone actively cleaning up.</p>"},{"location":"docker/graceful-shutdown/#docker-exec-form-example","title":"Docker exec form example","text":"<p>This leads us to the exec form. Hopefully, this gets us somewhere.</p> <p>The exec form is written as an array of parameters:\u00a0<code>ENTRYPOINT [\"top\", \"-b\"]</code></p> <p>To continue in the same line of examples, we will create a Dockerfile, build and run it.</p> <pre><code>FROM ubuntu:18.04\nENTRYPOINT [\"top\", \"-b\"]\n</code></pre> <p>Then build and run it.</p> <pre><code>docker image build --tag exec-form .\ndocker run --name exec-form --rm exec-form\n</code></pre> <p>This yields the following output.</p> <pre><code>top - 18:12:30 up 1 day,  6:53,  0 users,  load average: 0.00, 0.00, 0.00\nTasks:   1 total,   1 running,   0 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0.4 us,  0.3 sy,  0.0 ni, 99.2 id,  0.1 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem :  2046932 total,   535896 free,   307196 used,  1203840 buff/cache\nKiB Swap:  1048572 total,  1042292 free,     6280 used.  1574880 avail Mem\n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n    1 root      20   0   36480   2940   2584 R   0.0  0.1   0:00.03 top\n</code></pre> <p>Now we got something we can work with. If something would tell this Container to stop, it will tell our only running process so it is sure to reach the correct one!</p>"},{"location":"docker/graceful-shutdown/#gotchas","title":"Gotchas","text":"<p>Knowing we can use the exec form for our goal - gracefully shutting down our container - we can move on to the next part of our efforts. For the sake of imparting you with some hard learned lessons, we will explore two gotchas. They're optional, so you can also choose to skip to Make Sure Your Process Listens.</p>"},{"location":"docker/graceful-shutdown/#docker-exec-form-with-parameters","title":"Docker exec form with parameters","text":"<p>A caveat with the exec form is that it doesn't interpolate parameters.</p> <p>You can try the following:</p> <pre><code>FROM ubuntu:18.04\nENV PARAM=\"-b\"\nENTRYPOINT [\"top\", \"${PARAM}\"]\n</code></pre> <p>Then build and run it:</p> <pre><code>docker image build --tag exec-param .\ndocker run --name exec-form --rm exec-param\n</code></pre> <p>This should yield the following:</p> <pre><code>/bin/sh: 1: [top: not found\n</code></pre> <p>This is where Docker created a mix between the two styles.  It allows you to create an Entrypoint with a shell command - performing interpolation - but executing it as an exec form.  This can be done by prefixing the shell form, with, you guessed it, exec.</p> <pre><code>FROM ubuntu:18.04\nENV PARAM=\"-b\"\nENTRYPOINT exec \"top\" \"${PARAM}\"\n</code></pre> <p>Then build and run it:</p> <pre><code>docker image build --tag exec-param .\ndocker run --name exec-form --rm exec-param\n</code></pre> <p>This will return the exact same as if we would've run <code>ENTRYPOINT [\"top\", \"-b\"]</code>.</p> <p>Now you can also override the param, by using the environment variable flag.</p> <pre><code>docker image build --tag exec-param .\ndocker run --name exec-form --rm -e PARAM=\"help\" exec-param\n</code></pre> <p>Resulting in top's help string.</p>"},{"location":"docker/graceful-shutdown/#the-special-case-of-alpine","title":"The special case of Alpine","text":"<p>One of the main best practices for Dockerfiles, is to make them as small as possible.  The easiest way to do this is to start with a minimal image.  This is where Alpine Linux comes in. We will revisit out shell form example, but replace ubuntu with alpine.</p> <p>Create the following Dockerfile.</p> <pre><code>FROM alpine:3.8\nENTRYPOINT top -b\n</code></pre> <p>Then build and run it.</p> <pre><code>docker image build --tag exec-param .\ndocker run --name exec-form --rm -e PARAM=\"help\" exec-param\n</code></pre> <p>This yields the following output.</p> <pre><code>Mem: 1509068K used, 537864K free, 640K shrd, 126756K buff, 1012436K cached\nCPU:   0% usr   0% sys   0% nic 100% idle   0% io   0% irq   0% sirq\nLoad average: 0.00 0.00 0.00 2/404 5\n  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND\n    1     0 root     R     1516   0%   0   0% top -b\n</code></pre> <p>Aside from top's output looking a bit different, there is only one command.</p> <p>Alpine Linux helps us avoid the problem of shell form altogether!</p>"},{"location":"docker/graceful-shutdown/#make-sure-your-process-listens","title":"Make Sure Your Process Listens","text":"<p>It is excellent if your tenants are all signed up, know their rights and obligations. But you can't contact them when something happens, how will they ever know when to act?</p> <p>Translating that into our process. It starts and can be told to shut down, but does it process listen? Can it interpret the message it gets from Docker or Kubernetes? And if it does, can it relay the message correctly to its Child Processes? In order for your process to gracefully shutdown, it should know when to do so. As such, it should listen not only for itself but also on behalf of its children - yours never do anything wrong though!</p> <p>Some processes do, but many aren't designed to listen or tell their Children. They expect someone else to listen for them and tell them and their children - process managers.</p> <p>In order to listen to these signals, we can call in the help of others. We will look at two options.</p> <ul> <li>we let Docker manage the process and its children</li> <li>we use a process manager</li> </ul>"},{"location":"docker/graceful-shutdown/#let-docker-manage-it-for-us","title":"Let Docker manage it for us","text":"<p>If you're not using Docker to run or manage your containers, you should skip to Depend on a process manager.</p> <p>Docker has a build in feature, that it uses a lightweight process manager to help you.</p> <p>So if you're running your images with Docker itself, either directly or via Compose or Swarm, you're fine. You can use the init flag in your run command or your compose file.</p> <p>Please, note that the below examples require a certain minimum version of Docker.</p> <ul> <li>run - 1.13+</li> <li>compose (v 2.2) - 1.13.0+</li> <li>swarm (v 3.7) - 18.06.0+</li> </ul>"},{"location":"docker/graceful-shutdown/#with-docker-run","title":"With Docker Run","text":"<pre><code>docker run --rm -ti --init caladreas/dui\n</code></pre>"},{"location":"docker/graceful-shutdown/#with-docker-compose","title":"With Docker Compose","text":"<pre><code>version: '2.2'\nservices:\n    web:\n        image: caladreas/java-docker-signal-demo:no-tini\n        init: true\n</code></pre>"},{"location":"docker/graceful-shutdown/#with-docker-swarm","title":"With Docker Swarm","text":"<pre><code>version: '3.7'\nservices:\n    web:\n        image: caladreas/java-docker-signal-demo:no-tini\n        init: true\n</code></pre> <p>Relying on Docker does create a dependency on how your container runs. It only runs correctly in Docker-related technologies (run, compose, swarm) and only if the proper versions are available.</p> <p>Creating either a different experience for users running your application somewhere else or not able to meet the version requirements. So maybe another solution is to bake a process manager into your image and guarantee its behavior.</p>"},{"location":"docker/graceful-shutdown/#depend-on-a-process-manager","title":"Depend on a process manager","text":"<p>One of our goals for Docker images is to keep them small. We should look for a lightweight process manager. It does not have too many a whole machine worth or processes, just one and perhaps some children.</p> <p>Here we would like to introduce you to Tini, a lightweight process manager designed for this purpose.  It is a very successful and widely adopted process manager in the Docker world. So successful, that the before mentioned init flags from Docker are implemented by baking Tini into Docker.</p>"},{"location":"docker/graceful-shutdown/#debian-example","title":"Debian example","text":"<p>For brevity, the build process is excluded, and for image size, we use Debian slim instead of default Debian.</p> <pre><code>FROM debian:stable-slim\nENV TINI_VERSION v0.18.0\nADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini\nRUN chmod +x /tini\nENTRYPOINT [\"/tini\", \"-vv\",\"-g\", \"--\", \"/usr/bin/dui/bin/dui\",\"-XX:+UseCGroupMemoryLimitForHeap\", \"-XX:+UnlockExperimentalVMOptions\"]\nCOPY --from=build /usr/bin/dui-image/ /usr/bin/dui\n</code></pre>"},{"location":"docker/graceful-shutdown/#alpine-example","title":"Alpine example","text":"<p>Alpine Linux works wonders for Docker images, so to improve our lives, you can very easily install it if you want.</p> <pre><code>FROM alpine\nRUN apk add --no-cache tini\nENTRYPOINT [\"/sbin/tini\", \"-vv\",\"-g\",\"-s\", \"--\"]\nCMD [\"top -b\"]\n</code></pre>"},{"location":"docker/graceful-shutdown/#how-to-be-told-what-you-want-to-hear","title":"How To Be Told What You Want To Hear","text":"<p>You've made it this far; your tenets are reachable so you can inform them if they need to act. However, there's another problem lurking around the corner. Do they speak your language?</p> <p>Our process now starts knowing it can be talked to, it has someone who takes care of listening for it and its children. Now we need to make sure it can understand what it hears, it should be able to handle the incoming signals. We have two main ways of doing this.</p> <ul> <li>Handle signals as they come: we should make sure our process deal with the signals as they come</li> <li>State the signals we want: we can also tell up front, which signals we want to hear and put the burden of translation on our callers</li> </ul> <p>For more details on the subject of Signals and Docker, please read this excellent blog from Grigorii Chudnov.</p>"},{"location":"docker/graceful-shutdown/#handle-signals-as-they-come","title":"Handle signals as they come","text":"<p>Handling process signals depend on your application, programming language or framework.</p>"},{"location":"docker/graceful-shutdown/#state-the-signals-we-want","title":"State the signals we want","text":"<p>Sometimes your language or framework of choice, doesn't handle signals all that well. It might be very rigid in what it does with specific signals, removing your ability to do the right thing. Of course, not all languages or frameworks are designed with Docker container or Microservices in mind, are yet to catch up to this more dynamic environment.</p> <p>Luckily Docker and Kubernetes allow you to specify what signal too sent to your process.</p>"},{"location":"docker/graceful-shutdown/#docker-run","title":"Docker run","text":"<pre><code>docker run --rm -ti --init --stop-signal=SIGINT \\\n   caladreas/java-docker-signal-demo\n</code></pre>"},{"location":"docker/graceful-shutdown/#docker-composeswarm","title":"Docker compose/swarm","text":"<p>Docker's compose file format allows you to specify a stop signal.  This is the signal sent when the container is stopped in a normal fashion. Normal in this case, meaning <code>docker stop</code> or when docker itself determines it should stop the container.</p> <p>If you forcefully remove the container, for example with <code>docker rm -f</code> \u00a0it will directly kill the process, so don't do that.</p> <pre><code>version: '2.2'\nservices:\n    web:\n        image: caladreas/java-docker-signal-demo\n        stop_signal: SIGINT\n        stop_grace_period: 15s\n</code></pre> <p>If you run this with <code>docker-compose up</code> and then in a second terminal, stop the container, you will see something like this.</p> <pre><code>web_1  | HelloWorld!\nweb_1  | Shutdown hook called!\nweb_1  | We're told to stop early...\nweb_1  | java.lang.InterruptedException: sleep interrupted\nweb_1  |    at java.base/java.lang.Thread.sleep(Native Method)\nweb_1  |    at joostvdg.demo.signal@1.0/com.github.joostvdg.demo.signal.HelloWorld.printHelloWorld(Unknown Source)\nweb_1  |    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\nweb_1  |    at java.base/java.util.concurrent.FutureTask.run(Unknown Source)\nweb_1  |    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\nweb_1  |    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\nweb_1  |    at java.base/java.lang.Thread.run(Unknown Source)\nweb_1  | [DEBUG tini (1)] Passing signal: 'Interrupt'\nweb_1  | [DEBUG tini (1)] Received SIGCHLD\nweb_1  | [DEBUG tini (1)] Reaped child with pid: '7'\nweb_1  | [INFO  tini (1)] Main child exited with signal (with signal 'Interrupt')\n</code></pre>"},{"location":"docker/graceful-shutdown/#kubernetes","title":"Kubernetes","text":"<p>In Kubernetes we can make use of Container Lifecycle Hooks to manage how our container should be stopped.  We could, for example, send a SIGINT (interrupt) to tell our application to stop.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n    name: java-signal-demo\n    namespace: default\n    labels:\n        app: java-signal-demo\nspec:\n    replicas: 1\n    template:\n        metadata:\n            labels:\n                app: java-signal-demo\n        spec:\n            containers:\n            - name: main\n              image: caladreas/java-docker-signal-demo\n              lifecycle:\n                  preStop:\n                      exec:\n                          command: [\"killall\", \"java\" , \"-INT\"]\n            terminationGracePeriodSeconds: 60\n</code></pre> <p>When you create this as deployment.yml, create and delete it -\u00a0<code>kubectl apply -f deployment.yml</code>\u00a0/ <code>kubectl delete -f deployment.yml</code> - you will see the same behavior.</p>"},{"location":"docker/graceful-shutdown/#how-to-be-told-when-you-want-to-hear-it","title":"How To Be Told When You Want To Hear It","text":"<p>Our process now will now start knowing it will hear what it wants to hear. But we now have to make sure we hear it when we need to hear it. An intervention is excellent when you can still be saved, but it is a bit useless if you're already dead.</p>"},{"location":"docker/graceful-shutdown/#docker","title":"Docker","text":"<p>You can either configure your health check in your Dockerfile or  configure it in your docker-compose.yml for either compose or swarm.</p> <p>Considering only Docker can use the health check in your Dockerfile,   it is strongly recommended to have health checks in your application and document how they can be used.</p>"},{"location":"docker/graceful-shutdown/#kubernetes_1","title":"Kubernetes","text":"<p>In Kubernetes we have the concept of Container Probes. This allows you to configure whether your container is ready (readinessProbe) to be used and if it is still working as expected (livenessProbe).</p>"},{"location":"docker/graceful-shutdown/#examples","title":"Examples","text":"<p>How to actually listen to the signals and determine which one to use will depend on your programming language.</p> <p>There's three examples I have worked out, one for Go (lang) and two for Java: pojo and Spring Boot.</p>"},{"location":"docker/graceful-shutdown/#go","title":"Go","text":""},{"location":"docker/graceful-shutdown/#dockerfile","title":"Dockerfile","text":"<pre><code># build stage\nFROM golang:latest AS build-env\nRUN go get -v github.com/docker/docker/client/...\nRUN go get -v github.com/docker/docker/api/...\nADD src/ $GOPATH/flow-proxy-service-lister\nWORKDIR $GOPATH/flow-proxy-service-lister\nRUN go build -o main -tags netgo main.go\n\n# final stage\nFROM alpine\nENTRYPOINT [\"/app/main\"]\nCOPY --from=build-env /go/flow-proxy-service-lister/main /app/\nRUN chmod +x /app/main\n</code></pre>"},{"location":"docker/graceful-shutdown/#go-code-for-graceful-shutdown","title":"Go code for graceful shutdown","text":"<p>The following is a way for Go to shutdown a http server when receiving a termination signal.</p> <pre><code>func main() {\n    c := make(chan bool) // make channel for main &lt;--&gt; webserver communication\n    go webserver.Start(\"7777\", webserverData, c) // ignore the missing data\n\n    stop := make(chan os.Signal, 1) // make a channel that listens to is signals\n    signal.Notify(stop, syscall.SIGINT, syscall.SIGTERM) // we listen to some specific syscall signals\n\n    for i := 1; ; i++ { // this is still infinite\n        t := time.NewTicker(time.Second * 30) // set a timer for the polling\n        select {\n        case &lt;-stop: // this means we got a os signal on our channel\n            break // so we can stop\n        case &lt;-t.C:\n            // our timer expired, refresh our data\n            continue // and continue with the loop\n        }\n        break\n    }\n    fmt.Println(\"Shutting down webserver\") // if we got here, we have to inform the webserver to close shop\n    c &lt;- true // we do this by sending a message on the channel\n    if b := &lt;-c; b { // when we get true back, that means the webserver is doing with a graceful shutdown\n        fmt.Println(\"Webserver shut down\") // webserver is done\n    }\n    fmt.Println(\"Shut down app\") // we can close shop ourselves now\n}\n</code></pre>"},{"location":"docker/graceful-shutdown/#java-plain-docker-swarm","title":"Java plain (Docker Swarm)","text":"<p>This application is a Java 9 modular application, which can be found on github, github.com/joostvdg.</p>"},{"location":"docker/graceful-shutdown/#dockerfile_1","title":"Dockerfile","text":"<pre><code>FROM openjdk:9-jdk AS build\n\nRUN mkdir -p /usr/src/mods/jars\nRUN mkdir -p /usr/src/mods/compiled\n\nCOPY . /usr/src\nWORKDIR /usr/src\n\nRUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $(find src -name \"*.java\")\nRUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1.0 -C /usr/src/mods/compiled/joostvdg.dui.logging .\nRUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1.0 -C /usr/src/mods/compiled/joostvdg.dui.api .\nRUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1.0 -C /usr/src/mods/compiled/joostvdg.dui.client .\nRUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1.0  -e com.github.joostvdg.dui.server.cli.DockerApp\\\n    -C /usr/src/mods/compiled/joostvdg.dui.server .\n\nRUN rm -rf /usr/bin/dui-image\nRUN jlink --module-path /usr/src/mods/jars/:/${JAVA_HOME}/jmods \\\n    --add-modules joostvdg.dui.api \\\n    --add-modules joostvdg.dui.logging \\\n    --add-modules joostvdg.dui.server \\\n    --add-modules joostvdg.dui.client \\\n    --launcher dui=joostvdg.dui.server \\\n    --output /usr/bin/dui-image\n\nRUN ls -lath /usr/bin/dui-image\nRUN ls -lath /usr/bin/dui-image\nRUN /usr/bin/dui-image/bin/java --list-modules\n\nFROM debian:stable-slim\nLABEL authors=\"Joost van der Griendt &lt;joostvdg@gmail.com&gt;\"\nLABEL version=\"0.1.0\"\nLABEL description=\"Docker image for playing with java applications in a concurrent, parallel and distributed manor.\"\n# Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/\nENV TINI_VERSION v0.16.1\nADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini\nRUN chmod +x /tini\nENTRYPOINT [\"/tini\", \"-vv\",\"-g\", \"--\", \"/usr/bin/dui/bin/dui\"]\nENV DATE_CHANGED=\"20180120-1525\"\nCOPY --from=build /usr/bin/dui-image/ /usr/bin/dui\nRUN /usr/bin/dui/bin/java --list-modules\n</code></pre>"},{"location":"docker/graceful-shutdown/#handling-code","title":"Handling code","text":"<p>The code first initializes the server which and when started, creates the Shutdown Hook.</p> <p>Java handles certain signals in specific ways, as can be found in this table for linux. For more information, you can read the docs from Oracle. </p> <pre><code>public class DockerApp {\n    public static void main(String[] args) {\n        ServiceLoader&lt;Logger&gt; loggers = ServiceLoader.load(Logger.class);\n                Logger logger = loggers.findFirst().isPresent() ? loggers.findFirst().get() : null;\n                if (logger == null) {\n                    System.err.println(\"Did not find any loggers, quiting\");\n                    System.exit(1);\n                }\n                logger.start(LogLevel.INFO);\n\n                int pseudoRandom = new Random().nextInt(ProtocolConstants.POTENTIAL_SERVER_NAMES.length -1);\n                String serverName = ProtocolConstants.POTENTIAL_SERVER_NAMES[pseudoRandom];\n                int listenPort = ProtocolConstants.EXTERNAL_COMMUNICATION_PORT_A;\n                String multicastGroup = ProtocolConstants.MULTICAST_GROUP;\n\n                DuiServer distributedServer = DuiServerFactory.newDistributedServer(listenPort,multicastGroup , serverName, logger);\n\n                distributedServer.logMembership();\n\n                ExecutorService executorService = Executors.newFixedThreadPool(1);\n                executorService.submit(distributedServer::startServer);\n\n                long threadId = Thread.currentThread().getId();\n\n                Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; {\n                    System.out.println(\"Shutdown hook called!\");\n                    logger.log(LogLevel.WARN, \"App\", \"ShotdownHook\", threadId, \"Shutting down at request of Docker\");\n                    distributedServer.stopServer();\n                    distributedServer.closeServer();\n                    executorService.shutdown();\n                    try {\n                        Thread.sleep(100);\n                        executorService.shutdownNow();\n                        logger.stop();\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }));        \n    }\n}\n</code></pre>"},{"location":"docker/graceful-shutdown/#java-plain-kubernetes","title":"Java Plain (Kubernetes)","text":"<p>So far we've utilized the utilities from Docker itself in conjunction with it's native Docker Swarm orchestrator.</p> <p>Unfortunately, when it comes to popularity Kubernetes beats Swarm hands down.</p> <p>So this isn't complete if it doesn't also do graceful shutdown in Kubernetes. </p>"},{"location":"docker/graceful-shutdown/#in-dockerfile","title":"In Dockerfile","text":"<p>Our original file had to be changed, as Debian's Slim image doesn't actually contain the kill package. And we need a kill package, as we cannot instruct Kubernetes to issue a specific SIGNAL. Instead, we can issue a PreStop exec command, which we can utilise to execute a killall java -INT.</p> <p>The command will be specified in the Kubernetes deployment definition below.</p> <pre><code>FROM openjdk:9-jdk AS build\n\nRUN mkdir -p /usr/src/mods/jars\nRUN mkdir -p /usr/src/mods/compiled\n\nCOPY . /usr/src\nWORKDIR /usr/src\n\nRUN javac -Xlint:unchecked -d /usr/src/mods/compiled --module-source-path /usr/src/src $(find src -name \"*.java\")\nRUN jar --create --file /usr/src/mods/jars/joostvdg.dui.logging.jar --module-version 1.0 -C /usr/src/mods/compiled/joostvdg.dui.logging .\nRUN jar --create --file /usr/src/mods/jars/joostvdg.dui.api.jar --module-version 1.0 -C /usr/src/mods/compiled/joostvdg.dui.api .\nRUN jar --create --file /usr/src/mods/jars/joostvdg.dui.client.jar --module-version 1.0 -C /usr/src/mods/compiled/joostvdg.dui.client .\nRUN jar --create --file /usr/src/mods/jars/joostvdg.dui.server.jar --module-version 1.0  -e com.github.joostvdg.dui.server.cli.DockerApp\\\n    -C /usr/src/mods/compiled/joostvdg.dui.server .\n\nRUN rm -rf /usr/bin/dui-image\nRUN jlink --module-path /usr/src/mods/jars/:/${JAVA_HOME}/jmods \\\n    --add-modules joostvdg.dui.api \\\n    --add-modules joostvdg.dui.logging \\\n    --add-modules joostvdg.dui.server \\\n    --add-modules joostvdg.dui.client \\\n    --launcher dui=joostvdg.dui.server \\\n    --output /usr/bin/dui-image\n\nRUN ls -lath /usr/bin/dui-image\nRUN ls -lath /usr/bin/dui-image\nRUN /usr/bin/dui-image/bin/java --list-modules\n\nFROM debian:stable-slim\nLABEL authors=\"Joost van der Griendt &lt;joostvdg@gmail.com&gt;\"\nLABEL version=\"0.1.0\"\nLABEL description=\"Docker image for playing with java applications in a concurrent, parallel and distributed manor.\"\n# Add Tini - it is already included: https://docs.docker.com/engine/reference/commandline/run/\nENV TINI_VERSION v0.16.1\nADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini\nRUN chmod +x /tini\nENTRYPOINT [\"/tini\", \"-vv\",\"-g\", \"--\", \"/usr/bin/dui/bin/dui\"]\nENV DATE_CHANGED=\"20180120-1525\"\nRUN apt-get update &amp;&amp; apt-get install --no-install-recommends -y psmisc=22.* &amp;&amp; rm -rf /var/lib/apt/lists/*\nCOPY --from=build /usr/bin/dui-image/ /usr/bin/dui\nRUN /usr/bin/dui/bin/java --list-modules\n</code></pre>"},{"location":"docker/graceful-shutdown/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>So here we have the image's K8s Deployment descriptor.</p> <p>Including the Pod's lifecycle <code>preStop</code> with a exec style command. You should know by now why we prefer that.</p> <pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: dui-deployment\n  namespace: default\n  labels:\n    k8s-app: dui\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        k8s-app: dui\n    spec:\n      containers:\n        - name: master\n          image: caladreas/buming\n          ports:\n            - name: http\n              containerPort: 7777\n          lifecycle:\n            preStop:\n              exec:\n                command: [\"killall\", \"java\" , \"-INT\"]\n      terminationGracePeriodSeconds: 60\n</code></pre>"},{"location":"docker/graceful-shutdown/#java-spring-boot-1x","title":"Java Spring Boot (1.x)","text":"<p>This example is for Spring Boot 1.x, in time we will have an example for 2.x.</p> <p>This example is for the scenario of a Fat Jar with Tomcat as container [^8].</p>"},{"location":"docker/graceful-shutdown/#execute-example","title":"Execute example","text":"<pre><code>docker-compose build\n</code></pre> <p>Execute the following command:</p> <pre><code>docker run --rm -ti --name test spring-boot-graceful\n</code></pre> <p>Exit the application/container via <code>ctrl+c</code> and you should see the application shutting down gracefully.</p> <pre><code>2018-01-30 13:35:46.327  INFO 7 --- [       Thread-3] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@6e5e91e4: startup date [Tue Jan 30 13:35:42 GMT 2018]; root of context hierarchy\n2018-01-30 13:35:46.405  INFO 7 --- [       Thread-3] BootGracefulApplication$GracefulShutdown : Tomcat was shutdown gracefully within the allotted time.\n2018-01-30 13:35:46.408  INFO 7 --- [       Thread-3] o.s.j.e.a.AnnotationMBeanExporter        : Unregistering JMX-exposed beans on shutdown\n</code></pre>"},{"location":"docker/graceful-shutdown/#dockerfile_2","title":"Dockerfile","text":"<pre><code>FROM maven:3-jdk-8 AS build\nENV MAVEN_OPTS=-Dmaven.repo.local=/usr/share/maven/repository\nENV WORKDIR=/usr/src/graceful\nRUN mkdir $WORKDIR\nWORKDIR $WORKDIR\nCOPY pom.xml $WORKDIR\nRUN mvn -B -e org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline\nCOPY . $WORKSPACE\nRUN mvn -B -e clean verify\n\nFROM anapsix/alpine-java:8_jdk_unlimited\nLABEL authors=\"Joost van der Griendt &lt;joostvdg@gmail.com&gt;\"\nENV TINI_VERSION v0.16.1\nADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini\nRUN chmod +x /tini\nENTRYPOINT [\"/tini\", \"-vv\",\"-g\", \"--\"]\nENV DATE_CHANGED=\"20180120-1525\"\nCOPY --from=build /usr/src/graceful/target/spring-boot-graceful.jar /app.jar\nCMD [\"java\", \"-Xms256M\",\"-Xmx480M\", \"-Djava.security.egd=file:/dev/./urandom\", \"-jar\", \"/app.jar\"]\n</code></pre>"},{"location":"docker/graceful-shutdown/#docker-compose-file","title":"Docker compose file","text":"<pre><code>version: \"3.5\"\n\nservices:\n  web:\n    image: spring-boot-graceful\n    build: .\n    stop_signal: SIGINT\n</code></pre>"},{"location":"docker/graceful-shutdown/#java-handling-code","title":"Java handling code","text":"<pre><code>package com.github.joostvdg.demo.springbootgraceful;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\nimport org.apache.catalina.connector.Connector;\nimport org.apache.tomcat.util.threads.ThreadPoolExecutor;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer;\nimport org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer;\nimport org.springframework.boot.context.embedded.tomcat.TomcatConnectorCustomizer;\nimport org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory;\nimport org.springframework.context.ApplicationListener;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.event.ContextClosedEvent;\n\nimport java.util.concurrent.Executor;\nimport java.util.concurrent.TimeUnit;\n\n@SpringBootApplication\npublic class SpringBootGracefulApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(SpringBootGracefulApplication.class, args);\n    }\n\n    @Bean\n    public GracefulShutdown gracefulShutdown() {\n        return new GracefulShutdown();\n    }\n\n    @Bean\n    public EmbeddedServletContainerCustomizer tomcatCustomizer() {\n        return new EmbeddedServletContainerCustomizer() {\n\n            @Override\n            public void customize(ConfigurableEmbeddedServletContainer container) {\n                if (container instanceof TomcatEmbeddedServletContainerFactory) {\n                    ((TomcatEmbeddedServletContainerFactory) container)\n                            .addConnectorCustomizers(gracefulShutdown());\n                }\n\n            }\n        };\n    }\n\n    private static class GracefulShutdown implements TomcatConnectorCustomizer,\n            ApplicationListener&lt;ContextClosedEvent&gt; {\n\n        private static final Logger log = LoggerFactory.getLogger(GracefulShutdown.class);\n\n        private volatile Connector connector;\n\n        @Override\n        public void customize(Connector connector) {\n            this.connector = connector;\n        }\n\n        @Override\n        public void onApplicationEvent(ContextClosedEvent event) {\n            this.connector.pause();\n            Executor executor = this.connector.getProtocolHandler().getExecutor();\n            if (executor instanceof ThreadPoolExecutor) {\n                try {\n                    ThreadPoolExecutor threadPoolExecutor = (ThreadPoolExecutor) executor;\n                    threadPoolExecutor.shutdown();\n                    if (!threadPoolExecutor.awaitTermination(30, TimeUnit.SECONDS)) {\n                        log.warn(\"Tomcat thread pool did not shut down gracefully within \"\n                                + \"30 seconds. Proceeding with forceful shutdown\");\n                    } else {\n                        log.info(\"Tomcat was shutdown gracefully within the allotted time.\");\n                    }\n                }\n                catch (InterruptedException ex) {\n                    Thread.currentThread().interrupt();\n                }\n            }\n        }\n\n    }\n}\n</code></pre>"},{"location":"docker/graceful-shutdown/#example-with-docker-swarm","title":"Example with Docker Swarm","text":"<p>For now there's only an example with docker swarm, in time there will also be a Kubernetes example.</p> <p>Now that you can create Java applications packaged neatly in Docker images that support graceful shutdown, it would be nice to utilize.</p> <p>A good scenario would be a microservices architecture where services can come and go, but are registered in a service registry such as Eureka.</p> <p>Or a membership based protocol where members interact with each other and perhaps shard data.</p> <p>In these cases, of course the interactions are designed to be fault tolerant and discover faulty nodes on their own. But wouldn't it be better that if you knew you're going to quit, you inform the rest?</p> <p>We can reuse the <code>caladreas/buming</code> image and make it a docker swarm stack and run the service on every node. This way, we can easily see members coming and going and reduce the time to detect failure by notifying our peers of our impeding end.  </p>"},{"location":"docker/graceful-shutdown/#docker-swarm-cluster","title":"Docker swarm cluster","text":"<p>Setting up a docker swarm cluster is easy, but has some requirements:</p> <ul> <li>virtual box 4.x+</li> <li>docker-machine 1.12+</li> <li>docker 17.06+</li> </ul> <p>Warn</p> <p>Make sure this is the first and only virtualbox docker-machine VM being created/running, so that the ip range starts with 192.168.99.100</p> <pre><code>docker-machine create --driver virtualbox dui-1\ndocker-machine create --driver virtualbox dui-2\ndocker-machine create --driver virtualbox dui-3\n\neval \"$(docker-machine env dui-1)\"\nIP=192.168.99.100\ndocker swarm init --advertise-addr $IP\nTOKEN=$(docker swarm join-token -q worker)\n\neval \"$(docker-machine env dui-2)\"\ndocker swarm join --token ${TOKEN} ${IP}:2377\n\neval \"$(docker-machine env dui-3)\"\ndocker swarm join --token ${TOKEN} ${IP}:2377\n\neval \"$(docker-machine env dui-1)\"\ndocker node ls\n</code></pre>"},{"location":"docker/graceful-shutdown/#docker-swarm-network-and-multicast","title":"Docker swarm network and multicast","text":"<p>Unfortunately, docker swarm's swarm mode network overlay does not support multicast [<sup>9][</sup>10].</p> <p>Why is this a problem? Well, the application I use to test the graceful shutdown requires this, sorry.</p> <p>Luckily there is a very easy solution for this, its by using Weavenet's docker network plugin.</p> <p>Don't want to know about it or how you install it? Don't worry, just execute the script below.</p> <pre><code>#!/usr/bin/env bash\necho \"=&gt; Prepare dui-2\"\neval \"$(docker-machine env dui-2)\"\ndocker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions\ndocker plugin disable weaveworks/net-plugin:2.1.3\ndocker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST=1\ndocker plugin enable weaveworks/net-plugin:2.1.3\n\necho \"=&gt; Prepare dui-3\"\neval \"$(docker-machine env dui-3)\"\ndocker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions\ndocker plugin disable weaveworks/net-plugin:2.1.3\ndocker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST=1\ndocker plugin enable weaveworks/net-plugin:2.1.3\n\necho \"=&gt; Prepare dui-1\"\neval \"$(docker-machine env dui-1)\"\ndocker plugin install weaveworks/net-plugin:2.1.3 --grant-all-permissions\ndocker plugin disable weaveworks/net-plugin:2.1.3\ndocker plugin set weaveworks/net-plugin:2.1.3 WEAVE_MULTICAST=1\ndocker plugin enable weaveworks/net-plugin:2.1.3\ndocker network create --driver=weaveworks/net-plugin:2.1.3 --opt works.weave.multicast=true --attachable dui\n</code></pre>"},{"location":"docker/graceful-shutdown/#docker-stack","title":"Docker stack","text":"<p>Now to create a service that runs on every node it is the easiest to create a docker stack.</p>"},{"location":"docker/graceful-shutdown/#compose-file-docker-stackyml","title":"Compose file (docker-stack.yml)","text":"<pre><code>version: \"3.5\"\n\nservices:\n  dui:\n    image: caladreas/buming\n    build: .\n    stop_signal: SIGINT\n    networks:\n      - dui\n    deploy:\n      mode: global\nnetworks:\n  dui:\n    external: true\n</code></pre>"},{"location":"docker/graceful-shutdown/#create-stack","title":"Create stack","text":"<pre><code>docker stack deploy --compose-file docker-stack.yml buming\n</code></pre>"},{"location":"docker/graceful-shutdown/#execute-example_1","title":"Execute example","text":"<p>Now that we have a docker swarm cluster and a stack - which has a service running on every node - we can showcase the power of graceful shutdown in a cluster of dependent services.</p> <p>Confirm the service is running correctly on every node, first lets check our nodes.</p> <p><pre><code>eval \"$(docker-machine env dui-1)\"\ndocker node ls\n</code></pre> Which should look like this:</p> <pre><code>ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS\nf21ilm4thxegn5xbentmss5ur *   dui-1               Ready               Active              Leader\ny7475bo5uplt2b58d050b4wfd     dui-2               Ready               Active              \n6ssxola6y1i6h9p8256pi7bfv     dui-3               Ready               Active                            \n</code></pre> <p>Then check the service.</p> <pre><code>docker service ps buming_dui\n</code></pre> <p>Which should look like this.</p> <pre><code>ID                  NAME                                   IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\n3mrpr0jg31x1        buming_dui.6ssxola6y1i6h9p8256pi7bfv   dui:latest          dui-3               Running             Running 17 seconds ago                       \npfubtiy4j7vo        buming_dui.f21ilm4thxegn5xbentmss5ur   dui:latest          dui-1               Running             Running 17 seconds ago                       \nf4gjnmhoe3y4        buming_dui.y7475bo5uplt2b58d050b4wfd   dui:latest          dui-2               Running             Running 17 seconds ago                       \n</code></pre> <p>Now open a second terminal window. In window one, follow the service logs:</p> <pre><code>eval \"$(docker-machine env dui-1)\"\ndocker service logs -f buming_dui\n</code></pre> <p>In window two, go to a different node and stop the container.</p> <pre><code>eval \"$(docker-machine env dui-2)\"\ndocker ps\ndocker stop buming_dui.y7475bo5uplt2b58d050b4wfd.pnoui2x6elrz0tvkjz51njz94\n</code></pre> <p>In this case, you will see the other nodes receiving a leave notice and then the node stopping.</p> <pre><code>buming_dui.0.ryd8szexxku3@dui-3    | [Server-John D. Carmack]           [WARN]  [14:19:02.604011]   [16]    [Main]              Received membership leave notice from MessageOrigin{host='83918f6ad817', ip='10.0.0.7', name='Ken Thompson'}\nbuming_dui.0.so5m14sz8ksh@dui-1    | [Server-Alan Kay]                  [WARN]  [14:19:02.602082]   [16]    [Main]              Received membership leave notice from MessageOrigin{host='83918f6ad817', ip='10.0.0.7', name='Ken Thompson'}\nbuming_dui.0.pnoui2x6elrz@dui-2    | Shutdown hook called!\nbuming_dui.0.pnoui2x6elrz@dui-2    | [App]                              [WARN]  [14:19:02.598759]   [1] [ShotdownHook]      Shutting down at request of Docker\nbuming_dui.0.pnoui2x6elrz@dui-2    | [Server-Ken Thompson]              [INFO]  [14:19:02.598858]   [12]    [Main]               Stopping\nbuming_dui.0.pnoui2x6elrz@dui-2    | [Server-Ken Thompson]              [INFO]  [14:19:02.601008]   [12]    [Main]               Closing\n</code></pre>"},{"location":"docker/graceful-shutdown/#further-reading","title":"Further reading","text":"<ul> <li>Wikipedia page on reboots</li> <li>Microsoft about graceful shutdown</li> <li>Gracefully stopping docker containers</li> <li>What to know about Java and shutdown hooks</li> <li>https://www.weave.works/blog/docker-container-networking-multicast-fast/</li> <li>https://www.weave.works/docs/net/latest/install/plugin/plugin-how-it-works/</li> <li>https://www.weave.works/docs/net/latest/install/plugin/plugin-v2/</li> <li>https://www.auzias.net/en/docker-network-multihost/</li> <li>https://forums.docker.com/t/cannot-get-zookeeper-to-work-running-in-docker-using-swarm-mode/27109</li> <li>https://github.com/docker/libnetwork/issues/740</li> </ul>"},{"location":"docker/kubernetes/","title":"Kubernetes","text":""},{"location":"docker/kubernetes/#kubernetes-terminology","title":"Kubernetes terminology","text":""},{"location":"docker/kubernetes/#kubernetes-model","title":"Kubernetes model","text":""},{"location":"docker/kubernetes/#resources","title":"Resources","text":"<ul> <li>https://github.com/weaveworks/scope</li> <li>https://github.com/hjacobs/kube-ops-view</li> <li>https://coreos.com/tectonic/docs/latest/tutorials/sandbox/install.html</li> <li>https://github.com/kubernetes/dashboard</li> <li>https://blog.alexellis.io/you-need-to-know-kubernetes-and-swarm/</li> <li>https://kubernetes.io/docs/reference/kubectl/cheatsheet/</li> <li>https://blog.heptio.com/core-kubernetes-jazz-improv-over-orchestration-a7903ea92ca</li> </ul>"},{"location":"docker/multi-stage-builds/","title":"Docker Multi-Stage Builds","text":""},{"location":"docker/swarm/","title":"Docker Swarm (mode)","text":""},{"location":"java/","title":"Java","text":""},{"location":"java/#patternsanti-patterns","title":"Patterns/Anti-patterns","text":""},{"location":"java/#constants","title":"Constants","text":"<p>Use a class that cannot be instantiated for the use of constants.</p> <p>Using an interface is an anti-pattern because of what an interface implies.</p> <pre><code>/**\n * It should also be final, else we can extend this and create a constructor allowing us to instantiate it anyway.\n */\npublic final class Constants {\n    private Constants() {} // we should not instantiate this class\n\n    public static final String HELLO = \"WORLD\";\n    public static final int AMOUNT_OF_CONSTANTS = 2;\n}\n</code></pre>"},{"location":"java/#other-usefull-things","title":"Other usefull things","text":"<ul> <li>Random integer</li> </ul>"},{"location":"java/concurrency/","title":"Java Concurrency","text":""},{"location":"java/concurrency/#terminology","title":"Terminology","text":"Correctness <p>Correctness means that a class conforms to its specification.  A good specification defines invariants constraining an object\u2019s state  and postconditions describing the effects of its operations. <sup>6</sup></p> Thread Safe Class <p>a class is thread-safe when it continues to behave correctly when accessed from multiple threads</p> <p>No set of operations performed sequentially or concurrently on instances of a thread-safe class  can cause an instance to be in an invalid state. <sup>6</sup></p> Mutex <p>Every Java object can implicitly act as a lock for purposes of synchronization; these built-in locks are called intrinsic locks or monitor locks.  The lock is auto-matically acquired by the executing thread before entering a synchronized block and automatically released when control exits the synchronized block, whether by the normal control path or by throwing an exception out of the block.</p> <p>Intrinsic locks in Java act as mutexes (or mutual exclusion locks), which means that at most one thread may own the lock. When thread A attempts to acquire a lock held by thread B, A must wait, or block, until B releases it. If B never releases the lock, A waits forever. <sup>6</sup></p> Reentrant locks <p>When a thread requests a lock that is already held by another thread, the requesting thread blocks.  But because intrinsic locks are reentrant, if a thread tries to acquire a lock that it already holds, the request succeeds. </p> <p>Reentrancy means that locks are acquired on a per-thread rather than per-invocation basis. </p> <p>Reentrancy is implemented by associating with each lock an acquisition count and an owning thread.  When the count is zero, the lock is considered unheld. </p> <p>When a thread acquires a previously unheld lock, the JVM records the owner and sets the acquisition count to one. </p> <p>If that same thread acquires the lock again, the count is incremented, and when the owning thread exits the synchronized block,  the count is decremented. When the count reaches zero, the lock is released. <sup>6</sup></p> Liveness <p>In concurrent computing, liveness refers to a set of properties of concurrent systems,  that require a system to make progress despite the fact that its concurrently executing components (\"processes\")      may have to \"take turns\" in critical sections, parts of the program that cannot be simultaneously run by multiple processes.<sup>1</sup> </p> <p>Liveness guarantees are important properties in operating systems and distributed systems.<sup>2</sup></p> <p>A liveness property cannot be violated in a finite execution of a distributed system because the \"good\" event might only theoretically occur at some time after execution ends.  Eventual consistency is an example of a liveness property.<sup>3</sup> </p> <p>All properties can be expressed as the intersection of safety and liveness properties.<sup>4</sup></p> Volatile fields <p>When a field is declared volatile, the compiler and runtime are put on notice that this variable is shared  and that operations on it should not be reordered with other memory operations. </p> <p>Volatile variables are not cached in registers or in caches where they are hidden from other processors,  so a read of a volatile variable always returns the most recent write by any thread. <sup>6</sup></p> <p>You can use volatile variables only when all the following criteria are met:</p> <ul> <li>Writes to the variable do not depend on its current value, or you can ensure that only a single thread ever updates the value;</li> <li>The variable does not participate in invariants with other state variables;</li> <li>Locking is not required for any other reason while the variable is being accessed</li> </ul> Confinement <p>Confined objects must not escape their intended scope.  An object may be confined to a class instance (such as a private class member), a lexical scope (such as a local variable), or a thread (such as an object that is passed from method to method within a thread, but not supposed to be shared across threads). </p> <p>Objects don\u2019t escape on their own, of course\u2014they need help from the developer,  who assists by publishing the object beyond its intended scope. <sup>6</sup></p> Latch <p>Simply put, a CountDownLatch has a counter field, which you can decrement as we require.  We can then use it to block a calling thread until it\u2019s been counted down to zero.</p> <p>If we were doing some parallel processing, we could instantiate the CountDownLatch with  the same value for the counter as a number of threads we want to work across. </p> <p>Then, we could just call countdown() after each thread finishes,  guaranteeing that a dependent thread calling await() will block until the worker threads are finished. <sup>7</sup></p> Semaphore <p>In computer science, a semaphore is a variable or abstract data type used to control access to a common resource by multiple processes  in a concurrent system such as a multiprogramming operating system.</p> <p>A trivial semaphore is a plain variable that is changed (for example, incremented or decremented, or toggled)  depending on programmer-defined conditions. The variable is then used as a condition to control access to some system resource.</p> <p>A useful way to think of a semaphore as used in the real-world systems is as a record of how many units  of a particular resource are available, coupled with operations to adjust that record safely  (i.e. to avoid race conditions) as units are required or become free, and, if necessary,  wait until a unit of the resource becomes available. <sup>7</sup></p> Java Thread pools <p>There are several different types of Thread pools available.</p> <ul> <li> <p>FixedThreadPool: A fixed-size thread pool creates threads as tasks are submitted,      up to the maximum pool size, and then attempts to keep the pool     size constant (adding new threads if a thread dies due to an unexpected Exception ).</p> </li> <li> <p>CachedThreadPool: A cached thread pool has more flexibility to reap idle threads when the current size of the pool  exceeds the demand for processing, and to add new threads when demand increases, but places no bounds on the size of the pool.</p> </li> <li> <p>SingleThreadExecutor: A single-threaded executor creates a single worker thread to process tasks,      replacing it if it dies unexpectedly.      Tasks are guaranteed to be processed sequentially according to the order imposed by the task queue (FIFO, LIFO, priority order). 4</p> </li> <li> <p>ScheduledThreadPool: A fixed-size thread pool that supports delayed and periodic task execution, similar to Timer. <sup>6</sup></p> </li> </ul> Interrupt <p>Thread provides the interrupt method for interrupting a thread and for querying whether a thread has been interrupted.  Each thread has a boolean property that represents its interrupted status; interrupting a thread sets this status. Interruption is a cooperative mechanism. </p> <p>One thread cannot force another to stop what it is doing and do something else;  when thread A interrupts thread B, A is merely requesting that B stop what it is doing  when it gets to a convenient stopping point\u2014if it feels like it.</p> <p>When your code calls a method that throws InterruptedException , then your method is a blocking method too, and must have a plan for responding to inter- ruption. </p> <p>For library code, there are basically two choices:</p> <ul> <li> <p>Propagate the InterruptedException: This is often the most sensible policy if you can get away with it:      just propagate the InterruptedException to your caller.      This could involve not catching InterruptedException , or catching it and throwing it again after performing some brief activity-specific cleanup.</p> </li> <li> <p>Restore the interrupt: Sometimes you cannot throw InterruptedException , for instance when your code is part of a Runnable .      In these situations, you must catch InterruptedException and restore the interrupted status by calling interrupt on the current thread,      so that code higher up the call stack can see that an interrupt was issued. <sup>6</sup></p> </li> </ul>"},{"location":"java/concurrency/#patterns","title":"Patterns","text":""},{"location":"java/concurrency/#queue-deque","title":"Queue &amp; Deque","text":"<p>Queue &amp; Deque</p> <p>A Deque is a double-ended queue that allows efficient insertion and removal from both the head and the tail.  Implementations include ArrayDeque and LinkedBlockingDeque .</p> <p>Just as blocking queues lend themselves to the producer-consumer pattern, deques lend themselves to a related pattern called work stealing. </p> <p>A producer-consumer design has one shared work queue for all consumers;  in a work stealing design, every consumer has its own deque. </p> <p>If a consumer exhausts the work in its own deque, it can steal work from the tail of someone else\u2019s deque. </p> <p>Work stealing can be more scalable than a traditional producer-consumer design  because workers don\u2019t contend for a shared work queue; most of the time they access only their own deque, reducing contention. </p> <p>When a worker has to access another\u2019s queue, it does so from the tail rather than the head, further reducing contention. <sup>6</sup></p>"},{"location":"java/concurrency/#monitor-pattern","title":"Monitor pattern","text":""},{"location":"java/concurrency/#resources","title":"Resources","text":"<ul> <li>concurrency-patterns-monitor-object</li> <li>Wikipedia article on monitor pattern</li> <li>e-zest blog on monitor pattern java</li> </ul>"},{"location":"java/concurrency/#examples","title":"Examples","text":""},{"location":"java/concurrency/#confinement","title":"Confinement","text":"<p>PersonSet (below) illustrates how confinement and locking can work   together to make a class thread-safe even when its component state variables are not.    The state of PersonSet is managed by a HashSet , which is not thread-safe.</p> <p>But because mySet is private and not allowed to escape, the HashSet is confined to the PersonSet. </p> <p>The only code paths that can access mySet are addPerson and containsPerson , and each of these acquires the lock on the PersonSet. </p> <p>All its state is guarded by its intrinsic lock, making PersonSet thread-safe. <sup>6</sup></p> <pre><code>public class PersonSet {\n    @GuardedBy(\"this\")\n    private final Set&lt;Person&gt; mySet = new HashSet&lt;Person&gt;();\n\n    public synchronized void addPerson(Person p) {\n        mySet.add(p);\n    }\n\n    public synchronized boolean containsPerson(Person p) {\n        return mySet.contains(p);\n    }\n}\n</code></pre>"},{"location":"java/concurrency/#http-call-counter","title":"HTTP Call Counter","text":""},{"location":"java/concurrency/#unsafe-counter","title":"Unsafe Counter","text":"<pre><code>public class UnsafeCounter {\n    private long count = 0;\n\n    public long getCount() {\n        return count;\n    }\n\n    public void service() {\n        // do some work\n        try {\n            int pseudoRandom = new Random().nextInt(20);\n            Thread.sleep(pseudoRandom * 100);\n            ++count;\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n}\n</code></pre>"},{"location":"java/concurrency/#safe-counter","title":"Safe Counter","text":"<pre><code>public class SafeCounter {\n\n    private final AtomicLong count = new AtomicLong(0);\n\n    public long getCount() {\n        return count.get();\n    }\n\n    public void service() {\n        try {\n            int pseudoRandom = new Random().nextInt(20);\n            Thread.sleep(pseudoRandom * 100);\n            count.incrementAndGet();\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n}\n</code></pre>"},{"location":"java/concurrency/#caller","title":"Caller","text":"<pre><code>public class Server {\n    public void start(int port) throws Exception {\n        HttpServer server = HttpServer.create(new InetSocketAddress(port), 0);\n        UnsafeCounter unsafeCounter = new UnsafeCounter();\n        SafeCounter safeCounter = new SafeCounter();\n        server.createContext(\"/test\", new MyTestHandler(unsafeCounter, safeCounter));\n        server.createContext(\"/\", new MyHandler(unsafeCounter, safeCounter));\n        Executor executor = Executors.newFixedThreadPool(5);\n        server.setExecutor(executor); // creates a default executor\n        server.start();\n    }\n\n    static class MyTestHandler implements HttpHandler {\n        private UnsafeCounter unsafeCounter;\n        private SafeCounter safeCounter;\n\n        public MyTestHandler(UnsafeCounter unsafeCounter, SafeCounter safeCounter) {\n            this.unsafeCounter = unsafeCounter;\n            this.safeCounter = safeCounter;\n        }\n\n        @Override\n        public void handle(HttpExchange t) throws IOException {\n            safeCounter.service();\n            unsafeCounter.service();\n            System.out.println(\"Got a request on /test, counts so far:\"+ unsafeCounter.getCount() + \"::\" + safeCounter.getCount());\n            String response = \"This is the response\";\n            t.sendResponseHeaders(200, response.length());\n            try (OutputStream os = t.getResponseBody()) {\n                os.write(response.getBytes());\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"java/concurrency/#outcome","title":"Outcome","text":"<pre><code>Starting server on port 8080\nServer started\nGot a request on /, counts so far:2::1\nGot a request on /, counts so far:6::2\nGot a request on /, counts so far:6::3\nGot a request on /, counts so far:6::4\nGot a request on /, counts so far:6::5\nGot a request on /, counts so far:6::6\n</code></pre> <ol> <li> <p>Lamport, L. (1977). \"Proving the Correctness of Multiprocess Programs\". IEEE Transactions on Software Engineering (2): 125\u2013143. doi:10.1109/TSE.1977.229904.\u00a0\u21a9</p> </li> <li> <p>Lu\u00eds Rodrigues, Christian Cachin; Rachid Guerraoui (2010). Introduction to reliable and secure distributed programming (2. ed.). Berlin: Springer Berlin. pp. 22\u201324. ISBN 978-3-642-15259-7.\u00a0\u21a9</p> </li> <li> <p>Bailis, P.; Ghodsi, A. (2013). \"Eventual Consistency Today: Limitations, Extensions, and Beyond\". Queue. 11 (3): 20. doi:10.1145/2460276.2462076.\u00a0\u21a9</p> </li> <li> <p>Alpern, B.; Schneider, F. B. (1987). \"Recognizing safety and liveness\". Distributed Computing. 2 (3): 117. doi:10.1007/BF01782772.\u00a0\u21a9</p> </li> <li> <p>Liveness article Wikipedia \u21a9</p> </li> <li> <p>Java Concurrency in Practice / Brian Goetz, with Tim Peierls. . . [et al.] Concurrency in Practice \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Baeldung tutorial on CountDownLatch \u21a9\u21a9</p> </li> <li> <p>Wikipedia article on Semaphore \u21a9</p> </li> </ol>"},{"location":"java/ecosystem/","title":"Java Ecosysten","text":""},{"location":"java/java-modules/","title":"Java Modules","text":""},{"location":"java/networking/","title":"Java Networking","text":""},{"location":"java/networking/#general-remarks","title":"General Remarks","text":"<ul> <li>Network API works for IPv4 (32-bit adrressing) and IPv6 (128-bit addressing)</li> <li>Java only supports <code>TCP/IP</code> and <code>UDP/IP</code></li> </ul>"},{"location":"java/networking/#java-proxy-system-params","title":"Java proxy system params","text":"<ul> <li>socksProxyHost</li> <li>socksProxyPort</li> <li>http.proxySet</li> <li>http.proxyHost</li> <li>http.proxyPort</li> <li>https.proxySet</li> <li>https.proxyHost</li> <li>https.proxyPort</li> <li>ftpProxySet</li> <li>ftpProxyHost</li> <li>ftpProxyPort</li> <li>gopherProxySet </li> <li>gopherProxyHost</li> <li>gopherProxyPort </li> </ul>"},{"location":"java/networking/#special-ipv4-segments","title":"Special IPv4 segments","text":""},{"location":"java/networking/#internal","title":"Internal","text":"<ul> <li>10...* </li> <li>172.17.. - 172.31..</li> <li>192.168..</li> </ul>"},{"location":"java/networking/#local","title":"Local","text":"<ul> <li>127...*</li> </ul>"},{"location":"java/networking/#broadcast","title":"Broadcast","text":"<ul> <li>255.255.255.255     &gt; Packets sent to this address are received by all nodes on the local network, though they are not routed beyond the local network</li> </ul>"},{"location":"java/networking/#special-ipv6-segments","title":"Special IPv6 segments","text":""},{"location":"java/networking/#local_1","title":"Local","text":"<ul> <li>0:0:0:0:0:0:0:1 (or ::::::1 or ::1)</li> </ul>"},{"location":"java/streams/","title":"Java Streams","text":""},{"location":"java/streams/#try-with-resources","title":"Try-with-Resources","text":"<p>try with resources can be used with any object that implements the   Closeable interface, which includes almost every object you need to   dispose. So far, JavaMail Transport objects are the only exceptions   I\u2019ve encountered. Those still need to be disposed of explicitly.</p> <pre><code>public class Main {\n    public static void main(String[] args) {\n        try (OutputStream out = new FileOutputStream(\"/tmp/data.txt\")) {\n            // work with the output stream...\n        } catch (IOException ex) {\n            System.err.println(ex.getMessage());\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins/","title":"Jenkins","text":"<p>Cloudbees Study Guide</p>"},{"location":"jenkins/#tuning","title":"Tuning","text":"<p>Please read the following articles from Cloudbees:</p> <ul> <li>Prepare-Jenkins-for-support</li> <li>tuning-jenkins-gc-responsiveness-and-stability</li> <li>After-moving-a-job-symlinks-for-folders-became-actual-folders</li> <li>How-to-disable-the-weather-column-to-resolve-instance-slowness</li> <li>Accessing-graphs-on-a-Build-History-page-can-cause-Jenkins-to-become-unresponsive</li> <li>AutoBrowser-Feature-Can-Cause-Performance-Issues</li> <li>Disk-Space-Issue-after-upgrading-Branch-API-plugin</li> <li>JVM-Memory-settings-best-practice</li> </ul>"},{"location":"jenkins/#pipeline-as-code","title":"Pipeline as code","text":"<p>The default interaction model with Jenkins, historically, has been very web UI driven, requiring users to manually create jobs, then manually fill in the details through a web browser. This requires additional effort to create and manage jobs to test and build multiple projects, it also keeps the configuration of a job to build/test/deploy separate from the actual code being built/tested/deployed. This prevents users from applying their existing CI/CD best practices to the job configurations themselves.</p> <p>With the introduction of the Pipeline plugin, users now can implement a project\u2019s entire build/test/deploy pipeline in a Jenkinsfile and store that alongside their code, treating their pipeline as another piece of code checked into source control.</p> <p>We will dive into several things that come into play when writing Jenkins pipelines.</p> <ul> <li>Kind of Pipeline jobs</li> <li>Info about Pipeline DSL (a groovy DSL)</li> <li>Reuse pipeline DSL scripts</li> <li>Things to keep in mind</li> <li>Do's and Don't</li> </ul>"},{"location":"jenkins/#resources","title":"Resources","text":"<ul> <li>Pipeline Steps</li> <li>Pipeline Solution</li> <li>Pipeline as Code</li> <li>Dzone RefCard</li> </ul>"},{"location":"jenkins/#type-of-pipeline-jobs","title":"Type of pipeline jobs","text":"<ul> <li>Pipeline (inline)</li> <li>Pipeline (from SCM)</li> <li>Multi-Branch Pipeline</li> <li>GitHub Organization</li> <li>BitBucket Team/Project</li> <li>Gitea Organization</li> <li>GitLab Integration</li> </ul> <p>Danger</p> <p>When using the stash function keep in mind that the copying goes from where you are now to the master. When you unstash, it will copy the files from the master to where you are building.</p> <p>When your pipeline runs on a node and you stash and then unstash, it will copy the files from the node to the master and then back to the node. This can have a severe penalty on the performance of your pipeline when you are copying over a network.</p>"},{"location":"jenkins/#api","title":"API","text":"<p>Jenkins has an extensive API allowing you to retrieve a lot of information from the server.</p>"},{"location":"jenkins/#plugin","title":"Plugin","text":"<p>For this way you of course have to know how to write a plugin. There are some usefull resources to get started: * https://github.com/joostvdg/hello-world-jenkins-pipeline-plugin * https://wiki.jenkins-ci.org/display/JENKINS/Plugin+tutorial * https://jenkins.io/blog/2016/05/25/update-plugin-for-pipeline/</p>"},{"location":"jenkins/#dos-and-dont","title":"Do's and Don't","text":"<p>Aside from the Do's and Don'ts from Cloudbees, there are some we want to share.</p> <p>This changes the requirement for the component identifier property, as a job may only match a single group and a job listing in a group can only match a single. Thus the easiest way to make sure everything will stay unique (template names probably don\u2019t), is to make the component identifier property unique per file - let it use the name of the project.</p>"},{"location":"jenkins/#other-resources","title":"Other Resources","text":"<ul> <li>Configuration As Code</li> <li>Jenkins CLI - for managing Plugins</li> <li>Jenkinsfile Runner</li> <li>CICD With Jenkins On Docker Compose</li> <li>Jenkins Helm Chart</li> <li>Jenkins Operator</li> <li>Jenkins X</li> <li>CloudBees Jenkins Distribution</li> <li>CloudBees Jenkins X Distribution</li> </ul>"},{"location":"jenkins/jenkins-on-kubernetes/","title":"Jenkins on Kubernetes","text":""},{"location":"jenkins/jenkins-on-kubernetes/#goal","title":"Goal","text":"<p>Run Jenkins as best as we can on Kubernetes, taking as much advantage of the platform and ecosystem as we can.</p>"},{"location":"jenkins/jenkins-on-kubernetes/#steps-to-take","title":"Steps to take","text":"<ul> <li>install LDAP</li> <li>install and configure Apache Keycloak backed by LDAP</li> <li>install Hashicorp Vault</li> <li>install Jenkins</li> <li>configure Jenkins with Jenkins Configuration as Code</li> <li>verify we can use Kubernetes agents</li> <li>verify we can use GitHub integration</li> <li>expose Telemtry via OpenTelemetry<ul> <li>collect telemtry via Prometheus/Grafana</li> <li>collect telemtry via Tanzu Observability</li> </ul> </li> </ul>"},{"location":"jenkins/jenkins-on-kubernetes/#ldap","title":"LDAP","text":"<pre><code>helm repo add helm-openldap https://jp-gouin.github.io/helm-openldap/\n</code></pre> <pre><code>helm repo update\n</code></pre> <pre><code>helm upgrade --install ldap helm-openldap/openldap --namespace keycloak --values ldap-values.yaml --version 2.0.4\n</code></pre>"},{"location":"jenkins/jenkins-on-kubernetes/#keycloak","title":"Keycloak","text":"<pre><code>kubectl create namespace keycloak\n</code></pre> <pre><code>kubectl apply -f keycloak-httpproxy.yaml\n</code></pre> <pre><code>helm upgrade --install keycloak bitnami/keycloak --namespace keycloak --values keycloak-values.yaml --version 9.2.8\n</code></pre> <pre><code>kubectl logs -f -n keycloak keycloak-0\n</code></pre>"},{"location":"jenkins/jenkins-on-kubernetes/#keycloak-httpproxy","title":"Keycloak HTTPProxy","text":"<pre><code>export LB_IP=$(kubectl get svc -n tanzu-system-ingress envoy -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\n</code></pre> <pre><code>export KEYCLOAK_HOSTNAME=keycloak.${LB_IP}.nip.io\n</code></pre> <pre><code>cfssl gencert -ca ca.pem -ca-key ca-key.pem \\\n  -config cfssl.json \\\n  -profile=server \\\n  -cn=\"${KEYCLOAK_HOSTNAME}\" \\\n  -hostname=\"${KEYCLOAK_HOSTNAME},keycloak.keycloak.svc.cluster.local,keycloak,localhost\" \\\n   base-service-cert.json   | cfssljson -bare keycloak-server\n</code></pre>"},{"location":"jenkins/jenkins-on-kubernetes/#jenkins","title":"Jenkins","text":"<pre><code>helm repo add jenkins https://charts.jenkins.io\nhelm repo update\n</code></pre> <pre><code>kubectl create namespace jenkins\n</code></pre> <pre><code>DH_USER=\nDH_EMAIL=\nDH_PASS=\nNS=jenkins\n</code></pre> <pre><code>kubectl create secret docker-registry dockerhub-pull-secret \\\n  --docker-username=${DH_USER} \\\n  --docker-password=${DH_PASS} \\\n  --docker-email=${DH_EMAIL} \\\n  --namespace ${NS}\n</code></pre> <pre><code>GH_USER=\nGH_TOKEN=\nNS=jenkins\n</code></pre> <pre><code>kubectl create secret generic github-token \\\n  --from-literal=user=\"${GH_USER}\" \\\n  --from-literal=token=\"${GH_TOKEN}\" \\\n  --namespace ${NS}\n</code></pre> <pre><code>JENKINS_ADMIN_USER=joostvdg\nJENKINS_ADMIN_PASS=\n</code></pre> <pre><code>kubectl create secret generic jenkins-admin \\\n  --from-literal=user=\"${JENKINS_ADMIN_USER}\" \\\n  --from-literal=token=\"${JENKINS_ADMIN_PASS}\" \\\n  --namespace ${NS}\n</code></pre>"},{"location":"jenkins/jenkins-on-kubernetes/#install-jenkins-helm","title":"Install Jenkins Helm","text":"<pre><code>helm repo add jenkins https://charts.jenkins.io\nhelm repo update\n</code></pre> <pre><code>helm upgrade --install jenkins jenkins/jenkins --namespace jenkins --values jenkins-values.yaml\n</code></pre>"},{"location":"jenkins/jenkins-on-kubernetes/#jenkins-casc","title":"Jenkins CasC","text":"<p>...</p>"},{"location":"jenkins/jenkins-on-kubernetes/#jenkins-httpproxy","title":"Jenkins HTTPProxy","text":"<pre><code>export LB_IP=$(kubectl get svc -n tanzu-system-ingress envoy -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\n</code></pre> <pre><code>export JENKINS_HOSTNAME=jenkins.${LB_IP}.nip.io\n</code></pre> <pre><code>cfssl gencert -ca ca.pem -ca-key ca-key.pem \\\n  -config cfssl.json \\\n  -profile=server \\\n  -cn=\"${JENKINS_HOSTNAME}\" \\\n  -hostname=\"${JENKINS_HOSTNAME},jenkins.jenkins.svc.cluster.local,jenkins,localhost\" \\\n   base-service-cert.json   | cfssljson -bare jenkins-server\n</code></pre>"},{"location":"jenkins/jenkins-on-kubernetes/#kaniko-to-harbor","title":"Kaniko To Harbor","text":"<pre><code>HARBOR_SERVER=harbor.10.220.7.70.nip.io\nHARBOR_USER=\nHARBOR_PASS=\n</code></pre> <pre><code>kubectl create secret docker-registry harbor-registry-creds \\\n  --docker-username=${HARBOR_USER} \\\n  --docker-password=${HARBOR_PASS} \\\n  --docker-server=${HARBOR_SERVER} \\\n  --namespace jenkins\n</code></pre>"},{"location":"jenkins/jenkins-on-kubernetes/#checks-api","title":"Checks API","text":"<ul> <li>create Jenkins credentials for GitHub API</li> <li>ensure you have create a GitHub Server in Jenkins config</li> </ul> <pre><code>credentials:\n  system:\n    domainCredentials:\n    - credentials:\n      - string:\n          description: \"github token\"\n          id: \"githubtoken\"\n          scope: GLOBAL\n          secret: \"{AQAAABAAAAAwDgut+8oOqmwh4qHohWO09AxFPsz78EXLrtoC4QEFr9nY8orwx/mcLaS11G831IDmO8Ftxs2QockuYPJveteneQ==}\"\n      - usernamePassword:\n          description: \"github-credentials\"\n          id: \"github-credentials\"\n          password: \"{AQAAABAAAAAwutoG/5mM+UqCYohxLLogZ7Dpd8lyfWh9MeAbNybMplhycx4Z17h1WgzQPQn0lHAM+mfHsufBMuRTwl79rnfk9g==}\"\n          scope: GLOBAL\n          username: \"joostvdg\"\n</code></pre> <pre><code>  gitHubPluginConfig:\n    configs:\n    - credentialsId: \"githubtoken\"\n      name: \"GitHub\"\n    hookUrl: \"https://jenkins.10.220.7.70.nip.io/github-webhook/\"\n</code></pre>"},{"location":"jenkins/jenkins-on-kubernetes/#pipeline","title":"Pipeline","text":"<ul> <li>create credentials<ul> <li><code>githubtoken</code> of type secret text with the GitHub API Token</li> <li><code>github-credentials</code> username and password -&gt; github username and API Token</li> </ul> </li> <li>create GitHub Org Job<ul> <li>add plugin GitHub Branch Source?</li> </ul> </li> </ul>"},{"location":"jenkins/jenkins-on-kubernetes/#webhooks-behind-a-firewall","title":"Webhooks behind a firewall","text":"<ul> <li>https://www.jenkins.io/blog/2019/01/07/webhook-firewalls/</li> <li>https://developer.ibm.com/tutorials/deliver-your-webhooks-without-worrying-about-firewalls/</li> </ul> <pre><code>docker pull quay.io/schabrolles/smeeclient:stable --platform linux/amd64\ndocker tag quay.io/schabrolles/smeeclient:stable harbor.10.220.7.70.nip.io/test/smeeclient:stable\ndocker push harbor.10.220.7.70.nip.io/test/smeeclient:stable\n</code></pre> <ul> <li>SMEESOURCE</li> <li>HTTPTARGET</li> </ul> <pre><code>kubectl run smee-client -n jenkins --rm -i --tty --image harbor.10.220.7.70.nip.io/test/harbor.10.220.7.70.nip.io/test/smeeclient:stable  \\\n     --env=SMEESOURCE=\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: smee-client\n  namespace: jenkins\n  labels:\n    app: smee-client\nspec:\n  containers:\n  - name: smee-client\n    image: harbor.10.220.7.70.nip.io/test/smeeclient:stable\n    env:\n    - name: SMEESOURCE\n      value: \"https://smee.io/TVjVEDxMHHQeNPDl\"\n    - name: HTTPTARGET\n      value: \"http://jenkins:8080/github-webhook/\"\n</code></pre>"},{"location":"jenkins/jenkins-on-kubernetes/#thanos","title":"Thanos","text":"<pre><code>kubectl create namespace thanos\n</code></pre> <pre><code>helm upgrade --install thanos bitnami/thanos --namespace thanos --values thanos-values.yaml\n</code></pre>"},{"location":"jenkins/jenkins-on-kubernetes/#to","title":"TO","text":"<ul> <li>https://github.com/bdekany/wavefront-otel-auto-instrumentation</li> <li>https://artifacthub.io/packages/helm/bitnami/wavefront</li> <li>https://docs.wavefront.com/opentelemetry_tracing.html</li> </ul> <pre><code>kubectl create namespace wavefront\n</code></pre> <pre><code>TO_API_TOKEN=\nTO_NAMESPACE=\nCLUSTER_NAME=\n</code></pre> <pre><code>kubectl create secret generic to-api-token  --from-literal=api-token=\"${TO_API_TOKEN}\" --namespace ${TO_NAMESPACE}\n</code></pre> <pre><code>projectPacific:\n  enabled: true\nvspheretanzu:\n  enabled: true\nwavefront:\n  url: https://vmware.wavefront.com\n  existingSecret: to-api-token\ncollector:\n  useDaemonset: true\n  apiServerMetrics: true\n  cadvisor:\n    enabled: true\n  logLevel: info\n  discovery:\n    annotationExcludes: []\n  tags:\n    datacenter: vbc-h20-62\n    project: vbc-h20\n    owner: joostvdg\nkubeStateMetrics:\n  enabled: true\nproxy:\n  replicaCount: 2\n  zipkinPort: 9411\n  args: --traceZipkinListenerPorts 9411 --otlpGrpcListenerPorts 4317 --otlpHttpListenerPorts 4318\n</code></pre> <pre><code>kubectl -n wavefront patch svc wavefront-proxy --patch '{\"spec\": {\"ports\": [{\"name\":\"oltphttp\", \"port\": 4318, \"protocol\": \"TCP\"}, {\"name\":\"oltpgrpc\", \"port\": 4317, \"protocol\": \"TCP\"}]}}'\n</code></pre> <ul> <li>patch Proxy deployment</li> <li>patch Proxy service</li> <li>https://docs.wavefront.com/opentelemetry_tracing.html</li> <li>https://docs.wavefront.com/proxies_configuring.html#proxy-file-paths</li> <li>https://github.com/jenkinsci/opentelemetry-plugin</li> <li>https://github.com/bdekany/wavefront-otel-auto-instrumentation/blob/main/README.md</li> <li>https://github.com/bdekany/wavefront-otel-auto-instrumentation</li> </ul>"},{"location":"jenkins/jenkins-on-kubernetes/#vault","title":"Vault","text":"<ul> <li>https://learn.hashicorp.com/tutorials/vault/kubernetes-raft-deployment-guide?in=vault/kubernetes</li> <li>https://www.vaultproject.io/docs/platform/k8s/helm/configuration</li> <li>https://learn.hashicorp.com/tutorials/vault/kubernetes-raft-deployment-guide?in=vault/kubernetes#initialize-and-unseal-vault</li> <li>artifact hub: https://artifacthub.io/packages/helm/hashicorp/vault</li> </ul>"},{"location":"jenkins/jenkins-on-kubernetes/#vault-helm-install","title":"Vault Helm Install","text":"<pre><code>helm repo add hashicorp https://helm.releases.hashicorp.com\nhelm repo update\n</code></pre> <pre><code>kubectl create namespace vault\n</code></pre> <pre><code>NS=vault\nDH_USER=\nDH_EMAIL=\nDH_PASS=\n</code></pre> <pre><code>kubectl create secret docker-registry dockerhub-pull-secret \\\n  --docker-username=${DH_USER} \\\n  --docker-password=${DH_PASS} \\\n  --docker-email=${DH_EMAIL} \\\n  --namespace ${NS}\n</code></pre> <pre><code>KEYCLOAK_CLIENT_ID=\nKEYCLOAK_CLIENT_SECRET=\nKEYCLOAK_URL=\n</code></pre> <pre><code>kubectl create secret generic oic-auth \\\n  --from-literal=clientID=\"${KEYCLOAK_CLIENT_ID}\" \\\n  --from-literal=clientSecret=\"${KEYCLOAK_CLIENT_SECRET}\" \\\n  --from-literal=keycloakUrl=${KEYCLOAK_URL} \\\n  --namespace jenkins\n</code></pre> <pre><code>kubectl --namespace vault create secret tls tls-ca --cert ./tls-ca.cert --key ./tls-ca.key\n</code></pre> <pre><code>helm upgrade --install vault hashicorp/vault --namespace vault --values vault-values.yaml\n</code></pre>"},{"location":"jenkins/jenkins-on-kubernetes/#httpproxy","title":"HTTPProxy","text":"<pre><code>export LB_IP=$(kubectl get svc -n tanzu-system-ingress envoy -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\n</code></pre> <pre><code>export VAULT_HOSTNAME=vault.${LB_IP}.nip.io\n</code></pre> <pre><code>cfssl gencert -ca ca.pem -ca-key ca-key.pem \\\n  -config cfssl.json \\\n  -profile=server \\\n  -cn=\"${VAULT_HOSTNAME}\" \\\n  -hostname=\"${VAULT_HOSTNAME},vault.vault.svc.cluster.local,vault-ui.vault.svc.cluster.local,vault,vault-ui,localhost\" \\\n   base-service-cert.json   | cfssljson -bare vault-server\n</code></pre> <pre><code>cat vault-server-key.pem | base64\n</code></pre> <pre><code>cat vault-server.pem | base64\n</code></pre> <pre><code>kubectl apply -f vault-httpproxy.yaml\n</code></pre> <pre><code>kubectl get httpproxy -n vault\n</code></pre>"},{"location":"jenkins/jenkins-on-kubernetes/#unsealing","title":"Unsealing","text":"<pre><code>kubectl get pods --selector='app.kubernetes.io/name=vault' --namespace=' vault'\n</code></pre> <pre><code>kubectl exec --namespace vault --stdin=true --tty=true vault-0 -- vault operator init\n</code></pre>"},{"location":"jenkins/jenkins-on-kubernetes/#todo","title":"TODO","text":"<p>Things to improve on.</p> <ul> <li>make credentials come from Vault</li> <li>make credentials come from Kubernetes</li> <li>OpenTelemtry<ul> <li>Prometheus/Grafana/?</li> <li>TO?</li> </ul> </li> <li>Jobs via SeedJob from JobDSL?</li> </ul>"},{"location":"jenkins/jenkins-on-kubernetes/#references","title":"References","text":""},{"location":"jenkins-jobs/jenkins-jobs-builder/","title":"Jenkins Job Builder","text":"<p>The configuration setup of Jenkins Job Builder is composed of two main categories. Basic configuration and job configuration. Job configuration can be further split into several sub categories.</p>"},{"location":"jenkins-jobs/jenkins-jobs-builder/#basic-configuration","title":"Basic Configuration","text":"<p>In the basic configuration you will have to specify how the Jenkins Job Builder CLI can connect to the Jenkins instance you want to configure and how it should act.</p> <p>To use such a configuration file, you add --conf  to the CLI command. <p>Example: <pre><code>localhost.ini\n[job_builder]\nignore_cache=True\nkeep_descriptions=False\ninclude_path=.:scripts:~/git/\nrecursive=False\nexclude=.*:manual:./development\nallow_duplicates=False\n\n[jenkins]\n#user=jenkins\n#password=\nurl=http://localhost:8080/\n</code></pre> For more information see http://docs.openstack.org/infra/jenkins-job-builder/installation.html.</p>"},{"location":"jenkins-jobs/jenkins-jobs-builder/#job-configuration","title":"Job Configuration","text":"<p>The configuration for configuring the jobs consists of several distinct parts which can all be in the same file or can be distributed in their own respected files.</p> <p>These different parts can also be split into two different categories, those that are strictly linked within the configuration - via template matching - and those that are separate.</p> <p>Separate: * Macro\u2019s * Global defaults * Job configuration defaults * External configuration files</p> <p>Linked: * Templates * Groups * Projects * Job definitions</p> <p></p> <p>Here\u2019s a schematic representation on how they are linked. Exampe in YAML config: <pre><code>- job-template:\n    name: '{name}-{configComponentId}-ci'\n    description: 'CI Job of {configComponentId}'\n    &lt;&lt;: *config_job_defaults\n    builders:\n        - shell: 'jenkins-jobs test -r global/:definitions/ -o compiled/'\n- job-template:\n    name: '{name}-{configComponentId}-execute'\n    description: 'Executor Job of {configComponentId}'\n    &lt;&lt;: *config_job_defaults\n    builders:\n        - shell: 'jenkins-jobs --conf configuration/localhost.ini update definitions/'\n- job-group:\n    name: '{name}-config'\n    gitlab-user: 'jvandergriendt'\n    jobs:\n        - '{name}-{configComponentId}-ci':\n        - '{name}-{configComponentId}-execute':\n\n- project:\n    name: RnD-Config\n    jobs:\n        - '{name}-config':\n            configComponentId: JenkinsJobDefinitions\n</code></pre> The above will result in the following jobs: RnD-Config-JenkinsJobDefinitions-ci RnD-Config-JenkinsJobDefinitions-execute</p>"},{"location":"jenkins-jobs/jenkins-jobs-builder/#macros","title":"Macro\u2019s","text":"<p>Macro\u2019s are what the name implies, a group of related commands which can be invoked by the group. In Jenkins Job Builder this means you can define specific configurations for a component type (e.g. builders, paramters, publishes etc).</p> <p>A component has a name and a macro name. In general the component name is plural and the macro name is singular. As can be seen in the examples below.</p> <p>Here\u2019s an example: <pre><code># The 'add' macro takes a 'number' parameter and will creates a\n# job which prints 'Adding ' followed by the 'number' parameter:\n- builder:\n    name: add\n    builders:\n     - shell: \"echo Adding {number}\"\n\n# A specialized macro 'addtwo' reusing the 'add' macro but with\n# a 'number' parameter hardcoded to 'two':\n- builder:\n    name: addtwo\n    builders:\n     - add:\n        number: \"two\"\n\n# Glue to have Jenkins Job Builder to expand this YAML example:\n- job:\n    name: \"testingjob\"\n    builders:\n     # The specialized macro:\n     - addtwo\n     # Generic macro call with a parameter\n     - add:\n        number: \"ZERO\"\n     # Generic macro called without a parameter. Never do this!\n     # See below for the resulting wrong output :(\n     - add\n</code></pre></p> <p>To expand the schematic representation, you will get the following.</p> <pre><code>- builder:\n    name: test\n    builders:\n     - shell: \"jenkins-jobs test -r global/:definitions/ -o compiled/\"\n\n- builder:\n    name: update\n    builders:\n     - shell: \"jenkins-jobs --conf config.ini update -r global/:definitions/\"\n\n- job-template:\n    name: '{name}-{configComponentId}-ci'\n    &lt;&lt;: *config_job_defaults\n    builders:\n        - test\n\n- job-template:\n    name: '{name}-{configComponentId}-update'\n    &lt;&lt;: *config_job_defaults\n    builders:\n        - update\n</code></pre>"},{"location":"jenkins-jobs/jenkins-jobs-builder/#global-defaults","title":"Global defaults","text":"<p>Global defaults are defaults that should be global for the jobs you configure for a certain environment. It is the job counterpart of the basic configuration, usually containing variables for the specific environment. For example, url\u2019s, credential id\u2019s, JDK\u2019s etc.</p> <p>Example: <pre><code>global-defaults-localhost.yaml\n- defaults:\n    name: 'global'\n    flusso-gitlab-url: https://gitlab.flusso.nl\n    nexus-npm-url: http://localhost:8081/nexus/content/repositories/npm-internal\n    default-jdk: JDK 1.8\n    jenkinsJobsDefinitionJobName: RnD-Config-JenkinsJobDefinitions-ci\n    credentialsId: '4f0dfb96-a7b1-421c-a4ea-b6a154f91b08'\n</code></pre></p>"},{"location":"jenkins-jobs/jenkins-jobs-builder/#job-configuration-defaults","title":"Job configuration defaults","text":"<p>Job configuration defaults are nothing specific on their own. It refers to using a build in structure from YAML to create basic building blocks to be used by other configuration parts, usually the Templates.</p> <p>Example (definition): <pre><code>- config_job_defaults: &amp;config_job_defaults\n    name: 'config_job_defaults'\n    project-type: freestyle\n    disabled: false\n    logrotate:\n        daysToKeep: 7\n        numToKeep: 5\n        artifactDaysToKeep: -1\n        artifactNumToKeep: -1\n    jdk: '{default-jdk}'\n</code></pre> Example (usage): <pre><code>- job-template:\n    name: '{name}-{configComponentId}-ci'\n    &lt;&lt;: *config_job_defaults\n</code></pre></p>"},{"location":"jenkins-jobs/jenkins-jobs-builder/#templates","title":"Templates","text":"<p>Templates are used to define job templates. You define the entirety of the job using global defaults, configuration defaults and where useful refer to placeholders to be filled in by the other downstream configuration items.</p> <p>You can configure almost every plugin that is available for Jenkins, these are divided in subdivisions which reflect the Jenkins\u2019 job definition sections.</p> <p>For these subdivision and the available plugins see: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#modules</p> <p>For those plugins that are not supported, you can include the raw XML generated by the plugin. For how to do this, see: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#raw-config</p> <p>Example: <pre><code>- job-template:\n    name: '{name}-{configComponentId}-ci'\n    display-name: '{name}-{configComponentId}-ci'\n    description: 'CI Job of {configComponentId}'\n    &lt;&lt;: *config_job_defaults\n    builders:\n        - shell: 'jenkins-jobs test -r global/:definitions/ -o compiled/'\n    publishers:\n        - archive:\n            artifacts: '{filesToArchive_1}'\n            fingerprint: true\n        - archive:\n            artifacts: '{filesToArchive_2}'\n            fingerprint: true\n        - email:\n            notify-every-unstable-build: true\n            send-to-individuals: true\n</code></pre></p>"},{"location":"jenkins-jobs/jenkins-jobs-builder/#groups","title":"Groups","text":"<p>Groups are used to group together related components that require the same set of jobs. Where you can also specify a similar set of properties, for example, a different JDK to be used.</p> <p>The name property is mandatory and will be used to match Job definitions. The jobs property is also mandatory and will be used to match Templates for which a Job will be generated per matching Job definition.</p> <p>Example <pre><code>- job-group:\n    name: '{name}-gulp'\n    gitlab-user: 'jvandergriendt'\n    artifactId: '{gulpComponentId}'\n    jobs:\n        - '{name}-{gulpComponentId}-ci':\n        - '{name}-{gulpComponentId}-version':\n        - '{name}-{gulpComponentId}-sonar':\n        - '{name}-{gulpComponentId}-publish':\n        - '{name}-{gulpComponentId}-deploy-prep':\n        - '{name}-{gulpComponentId}-deploy':\n        - '{name}-{gulpComponentId}-acceptance':\n</code></pre></p>"},{"location":"jenkins-jobs/jenkins-jobs-builder/#projects","title":"Projects","text":"<p>Projects are used to list the actual Job definitions, which via grouping and Templates get generated, and can obviously be used to define jobs for a specific project.</p> <p>The name property is mandatory and will be passed along with a Job definition and is generally used to tie job definitions to Groups. <pre><code>- project:\n    name: RnD-Maven\n    jobs:\n        - '{name}-keep':\n            gulpComponentId: keep-backend\n            displayName: Keep-Backend\n</code></pre></p>"},{"location":"jenkins-jobs/jenkins-jobs-builder/#job-definitions","title":"Job definitions","text":"<p>Job definitions are what is all about. Although they are part of the Project configuration item I treat them separately.</p> <p>You list the jobs under a Project and start with the name of the Group it belongs to. After that, you should define at least a name component to be able to differentiate the different jobs you want. As can be seen in the above examples with the gulpComponentId.</p> <p>External configuration files Sometimes you run into the situation you want to use a multi-line configuration for a plugin, or a set of commands. Or, used at in different configurations or templates.</p> <p>Then you run into the situation that it is very difficult to manage in them neatly inside YAML configuration files. For this situation you are able to simply include a text file, via a native YAML construct. See: http://docs.openstack.org/infra/jenkins-job-builder/definition.html#module-jenkins_jobs.local_yaml</p> <p>For example <pre><code>- job:\n    name: test-job-include-raw-1\n    builders:\n      - shell:\n          !include-raw include-raw001-hello-world.sh\n      - shell:\n          !include-raw include-raw001-vars.sh\n</code></pre> </p>"},{"location":"jenkins-jobs/jenkins-jobs-builder/#usage","title":"Usage","text":"<p>The information to how you use the tool is very well explained in the documentation. See http://docs.openstack.org/infra/jenkins-job-builder/installation.html#running Automated maintenance If all the jobs you can administer are done via Jenkins Job Builder, you can start to automate the maintenance of these jobs.</p> <p>Simply make jobs that poll/push on the code base where you have your Jenkins Job Builder configuration files.</p> <p>Example <pre><code>- config_job_defaults: &amp;config_job_defaults\n    name: 'config_job_defaults'\n    project-type: freestyle\n    disabled: false\n    logrotate:\n        daysToKeep: 7\n        numToKeep: 5\n        artifactDaysToKeep: -1\n        artifactNumToKeep: -1\n    jdk: '{default-jdk}'\n    triggers:\n        - pollscm: \"H/15 * * * *\"\n    scm:\n        - git:\n            url: '{flusso-gitlab-url}/{gitlab-user}/{componentGitName}.git'\n            credentials-id: '{credentialsId}'\n    publishers:\n        - email:\n            notify-every-unstable-build: true\n            send-to-individuals: true\n\n- job-template:\n    name: '{name}-{configComponentId}-ci'\n    display-name: '{name}-{configComponentId}-ci'\n    description: 'CI Job of {configComponentId}'\n    &lt;&lt;: *config_job_defaults\n    builders:\n        - shell: 'jenkins-jobs test -r global/:definitions/ -o compiled/'\n    publishers:\n        - archive:\n            artifacts: '{filesToArchive_1}'\n            fingerprint: true\n        - archive:\n            artifacts: '{filesToArchive_2}'\n            fingerprint: true\n        - email:\n            notify-every-unstable-build: true\n            send-to-individuals: true\n\n- job-template:\n    name: '{name}-{configComponentId}-x'\n    display-name: '{name}-{configComponentId}-execute'\n    description: 'Executor Job of {configComponentId}, it will execute the update and delete old command'\n    &lt;&lt;: *config_job_defaults\n    builders:\n        - shell: 'jenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/'\n\n- job-group:\n    name: '{name}-config'\n    gitlab-user: 'jvandergriendt'\n    jobs:\n        - '{name}-{configComponentId}-ci':\n        - '{name}-{configComponentId}-x':\n\n- project:\n    name: RnD-Config\n    jobs:\n        - '{name}-config':\n            configComponentId: JenkinsJobDefinitions\n            componentGitName: jenkins-job-definitions\n            filesToArchive_1: scripts/*.sh\n            filesToArchive_2: maven/settings.xml\n</code></pre></p>"},{"location":"jenkins-jobs/jenkins-jobs-builder/#tips-trick","title":"Tips &amp; Trick","text":"<p>As the documentation is so extensive, it can sometimes be difficult to figure out what would be a good way to deal with some constructs. Component identifier property One important thing to keep in mind is that in order to create a whole set of jobs via the groups and templates it imperative to have a component* identifier property.</p> <p>This way you can define hundreds of jobs in a project, dozens of groups and dozens of templates and generate thousands of unique individual jobs. Scale does not actually matter in this case, if you have more than one job in a project you will need this property. If the jobs that will be generated will not differ the execution will fail.</p> <p>Bulk</p> <p>you can combine multiple files or even entire folder structures together in a single call. For example, if you manage all the jobs of a company or a department and configure them in separate files.</p> <p>For example <pre><code>jenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/\n</code></pre></p>"},{"location":"jenkins-jobs/jobdsl/","title":"Jenkins Job DSL","text":"<p>Jenkins is a wonderful system for managing builds, and people love using its UI to configure jobs. Unfortunately, as the number of jobs grows, maintaining them becomes tedious, and the paradigm of using a UI falls apart. Additionally, the common pattern in this situation is to copy jobs to create new ones, these \"children\" have a habit of diverging from their original \"template\" and consequently it becomes difficult to maintain consistency between these jobs.</p> <p>The Jenkins job-dsl-plugin attempts to solve this problem by allowing jobs to be defined with the absolute minimum necessary in a programmatic form, with the help of templates that are synced with the generated jobs. The goal is for your project to be able to define all the jobs they want to be related to their project, declaring their intent for the jobs, leaving the common stuff up to a template that were defined earlier or hidden behind the DSL.</p>"},{"location":"jenkins-jobs/jobdsl/#pipeline-with-folder-example","title":"Pipeline with folder example","text":"<pre><code>import hudson.model.*\nimport jenkins.model.*\n\ndef dslExamplesFolder = 'DSL-Examples'\ndef gitLabCredentialsId = 'joost-flusso-gitlab-ssh'\ndef gitLabUrl = 'git@gitlab.flusso.nl'\ndef gitLabNamespace = 'keep'\ndef gitLabProject = 'keep-api'\n\n\nif(!jenkins.model.Jenkins.instance.getItem(dslExamplesFolder)) {\n    //folder doesn't exist because item doesn't exist in runtime\n    //Therefore, create the folder.\n    folder(dslExamplesFolder) {\n        displayName('DSL Examples')\n        description('Folder for job dsl examples')\n    }\n}\n\ncreateMultibranchPipelineJob(gitLabCredentialsId, gitLabUrl, dslExamplesFolder, 'keep', 'keep-api')\ncreateMultibranchPipelineJob(gitLabCredentialsId, gitLabUrl, dslExamplesFolder, 'keep', 'keep-backend-spring')\ncreateMultibranchPipelineJob(gitLabCredentialsId, gitLabUrl, dslExamplesFolder, 'keep', 'keep-frontend')\n\ndef createMultibranchPipelineJob(def gitLabCredentialsId, def gitLabUrl, def folder, def gitNamespace, def project) {\n    multibranchPipelineJob(\"${folder}/${project}-mb\") {\n        branchSources {\n            git {\n                remote(\"${gitLabUrl}:${gitNamespace}/${project}.git\")\n                credentialsId(gitLabCredentialsId)\n            }\n        }\n        orphanedItemStrategy {\n            discardOldItems {\n                numToKeep(20)\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-jobs/jobdsl/#freestyle-maven-job","title":"Freestyle maven job","text":"<pre><code>def project = 'quidryan/aws-sdk-test'\ndef branchApi = new URL(\"https://api.github.com/repos/${project}/branches\")\ndef branches = new groovy.json.JsonSlurper().parse(branchApi.newReader())\nbranches.each {\n    def branchName = it.name\n    def jobName = \"${project}-${branchName}\".replaceAll('/','-')\n    job(jobName) {\n        scm {\n            git(\"git://github.com/${project}.git\", branchName)\n        }\n        steps {\n            maven(\"test -Dproject.name=${project}/${branchName}\")\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-jobs/jobdsl/#resources","title":"Resources","text":"<ul> <li>Tutorial</li> <li>Live Playground</li> <li>Main DSL Commands</li> <li>API Viewer</li> </ul>"},{"location":"jenkins-jobs/jobdsl/#other-references","title":"Other References","text":"<ul> <li>Talks and Blogs</li> <li>User Power Movies</li> <li>DZone article</li> <li>Testing DSL Scripts</li> </ul>"},{"location":"jenkins-pipeline/artifactory-integration/","title":"JFrog Jenkins Challenge","text":"<p>Visited the stand of JFrog where they had stories about two main products: Artifactory and X-Ray.</p> <p>For both there is a Challenge, an X-Ray Challenge and a Jenkins &amp; Artifactory Challenge.</p>"},{"location":"jenkins-pipeline/artifactory-integration/#jenkins-challenge","title":"Jenkins Challenge","text":"<p>The instructions for the Challenge were simply, follow what is stated in their GitHub repository and email a screenshot of the result.</p> <p>The instruction were as follows:</p> <ol> <li>Get an Artifactory instance (you can start a free trial on prem or in the cloud)</li> <li>Install Jenkins</li> <li>Install Artifactory Jenkins Plugin</li> <li>Add Artifactory credentials to Jenkins Credentials</li> <li>Create a new pipeline job</li> <li>Use the Artifactory Plugin DSL documentation to complete the following script:</li> </ol> <p>With a Scripted Pipeline as starting point:</p> <pre><code>node {\n    def rtServer\n    def rtGradle\n    def buildInfo\n    stage('Preparation') {\n        git 'https://github.com/jbaruch/gradle-example.git'\n        // create a new Artifactory server using the credentials defined in Jenkins \n        // create a new Gradle build\n        // set the resolver to the Gradle build to resolve from Artifactory\n        // set the deployer to the Gradle build to deploy to Artifactory\n        // declare that your gradle script does not use Artifactory plugin\n        // declare that your gradle script uses Gradle wrapper\n    }\n    stage('Build') {\n        //run the artifactoryPublish gradle task and collect the build info\n    }\n    stage('Publish Build Info') {\n        //collect the environment variables to build info\n        //publish the build info\n    }\n}\n</code></pre> <p>I don't like scripted, so I opted for Declarative with Jenkins in Kubernetes with the Jenkins Kubernetes plugin.</p> <p>Steps I took:</p> <ul> <li>get a trial license from the JFrog website</li> <li>install Artifactory<ul> <li>and copy in the license when prompted</li> <li>change admin password</li> <li>create local maven repo 'libs-snapshot-local'</li> <li>create remote maven repo 'jcenter' (default remote value is jcenter, so only have to set the name)</li> </ul> </li> <li>install Jenkins<ul> <li>Artifactory plugin</li> <li>Kubernetes plugin</li> </ul> </li> <li>add Artifactory username/password as credential in Jenkins</li> <li>create a gradle application (Spring boot via start.spring.io) which you can find here</li> <li>create a Jenkinsfile</li> </ul>"},{"location":"jenkins-pipeline/artifactory-integration/#installing-artifactory","title":"Installing Artifactory","text":"<p>I installed Artifactory via Helm. JFrog has their own Helm repository - of course, would weird otherwise tbh - and you have to add that first.</p> <pre><code>helm repo add jfrog https://charts.jfrog.io\nhelm install --name artifactory stable/artifactory\n</code></pre>"},{"location":"jenkins-pipeline/artifactory-integration/#jenkinsfile","title":"Jenkinsfile","text":"<p>This uses the Gradle wrapper - as per instructions in the challenge.</p> <p>So we can use the standard JNLP container, which is default, so <code>agent any</code> will do.</p> <pre><code>pipeline {\n    agent any\n    environment {\n        rtServer  = ''\n        rtGradle  = ''\n        buildInfo = ''\n        artifactoryServerAddress = 'http://..../artifactory'\n    }\n    stages {\n        stage('Test Container') {\n            steps {\n                container('gradle') {\n                    sh 'which gradle'\n                    sh 'uname -a'\n                    sh 'gradle -version'\n                }\n            }\n        }\n        stage('Checkout'){\n            steps {\n                git 'https://github.com/demomon/gradle-jenkins-challenge.git'\n            }\n        }\n        stage('Preparation') {\n            steps {\n                script{\n                    // create a new Artifactory server using the credentials defined in Jenkins \n                    rtServer = Artifactory.newServer url: artifactoryServerAddress, credentialsId: 'art-admin'\n\n                    // create a new Gradle build\n                    rtGradle = Artifactory.newGradleBuild()\n\n                    // set the resolver to the Gradle build to resolve from Artifactory\n                    rtGradle.resolver repo:'jcenter', server: rtServer\n\n                    // set the deployer to the Gradle build to deploy to Artifactory\n                    rtGradle.deployer repo:'libs-snapshot-local',  server: rtServer\n\n                    // declare that your gradle script does not use Artifactory plugin\n                    rtGradle.usesPlugin = false\n\n                    // declare that your gradle script uses Gradle wrapper\n                    rtGradle.useWrapper = true\n                }\n            }\n        }\n        stage('Build') {\n            steps {\n                script {\n                    //run the artifactoryPublish gradle task and collect the build info\n                    buildInfo = rtGradle.run buildFile: 'build.gradle', tasks: 'clean build artifactoryPublish'\n                }\n            }\n        }\n        stage('Publish Build Info') {\n            steps {\n                script {\n                    //collect the environment variables to build info\n                    buildInfo.env.capture = true\n                    //publish the build info\n                    rtServer.publishBuildInfo buildInfo\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/artifactory-integration/#jenkinsfile-without-gradle-wrapper","title":"Jenkinsfile without Gradle Wrapper","text":"<p>I'd rather not install the Gradle tool if I can just use a pre-build container with it.</p> <p>Unfortunately, to use it correctly with the Artifactory plugin and a Jenkins Kubernetes plugin, we need to do two things.</p> <ol> <li>create a <code>Gradle</code> Tool in the Jenkins master<ul> <li>because the Artifactory plugin expects a <code>Jenkins Tool</code> object, not a location</li> <li>Manage Jenkins -&gt; Global Tool Configuration -&gt; Gradle -&gt; Add</li> <li>As value supply <code>/usr</code>, the Artifactory build will add <code>/gradle/bin</code> to it automatically</li> </ul> </li> <li>set the user of build Pod to id <code>1000</code> explicitly<ul> <li>else the build will not be allowed to touch files in <code>/home/jenkins/workspace</code></li> </ul> </li> </ol> <pre><code>pipeline {\n    agent {\n        kubernetes {\n        label 'mypod'\n        yaml \"\"\"apiVersion: v1\nkind: Pod\nspec:\n  securityContext:\n    runAsUser: 1000\n    fsGroup: 1000\n  containers:\n  - name: gradle\n    image: gradle:4.10-jdk-alpine\n    command: ['cat']\n    tty: true\n\"\"\"\n        }\n    }\n    environment {\n        rtServer  = ''\n        rtGradle  = ''\n        buildInfo = ''\n        CONTAINER_GRADLE_TOOL = '/usr/bin/gradle'\n    }\n    stages {\n        stage('Test Container') {\n            steps {\n                container('gradle') {\n                    sh 'which gradle'\n                    sh 'uname -a'\n                    sh 'gradle -version'\n                }\n            }\n        }\n        stage('Checkout'){\n            steps {\n                // git 'https://github.com/demomon/gradle-jenkins-challenge.git'\n        checkout scm\n            }\n        }\n        stage('Preparation') {\n            steps {\n                script{\n                    // create a new Artifactory server using the credentials defined in Jenkins \n                    rtServer = Artifactory.newServer url: 'http://35.204.238.14/artifactory', credentialsId: 'art-admin'\n\n                    // create a new Gradle build\n                    rtGradle = Artifactory.newGradleBuild()\n\n                    // set the resolver to the Gradle build to resolve from Artifactory\n                    rtGradle.resolver repo:'jcenter', server: rtServer\n\n                    // set the deployer to the Gradle build to deploy to Artifactory\n                    rtGradle.deployer repo:'libs-snapshot-local',  server: rtServer\n\n                    // declare that your gradle script does not use Artifactory plugin\n                    rtGradle.usesPlugin = false\n\n                    // declare that your gradle script uses Gradle wrapper\n                    rtGradle.useWrapper = true\n                }\n            }\n        }\n        stage('Build') {\n            //run the artifactoryPublish gradle task and collect the build info\n            steps {\n                script {\n                    buildInfo = rtGradle.run buildFile: 'build.gradle', tasks: 'clean build artifactoryPublish'\n                }\n            }\n        }\n        stage('Publish Build Info') {\n            //collect the environment variables to build info\n            //publish the build info\n            steps {\n                script {\n                    buildInfo.env.capture = true\n                    rtServer.publishBuildInfo buildInfo\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/core-concepts/","title":"Core Concepts","text":"<p>Below are some core concepts to understand before building pipelines in Jenkins.</p> <ul> <li>Pipeline as Code</li> <li>Step</li> <li>Master vs Nodes</li> <li>Checkout</li> <li>Workspace</li> <li>Stage</li> <li>Sandbox and Script Security</li> <li>Java vs. Groovy</li> <li>Env (object)</li> <li>Stash &amp; archive</li> <li>Credentials</li> <li>Tools &amp; Build Environment</li> <li>Pipeline Syntax Page</li> </ul>"},{"location":"jenkins-pipeline/core-concepts/#terminology","title":"Terminology","text":"<p>The terminology used in this page is based upon the terms used by Cloudbees as related to Jenkins.</p> <p>If in doubt, please consult the Jenkins Glossary. </p>"},{"location":"jenkins-pipeline/core-concepts/#pipeline-as-code","title":"Pipeline as Code","text":""},{"location":"jenkins-pipeline/core-concepts/#step","title":"Step","text":"<p>A single task; fundamentally steps tell Jenkins what to do inside of a Pipeline or Project.</p> <p>Consider the following piece of pipeline code:</p> <pre><code>node {\n    timestamps {\n        stage ('My FIrst Stage') {\n            if (isUnix()) {\n                sh 'echo \"this is Unix!\"'\n            } else {\n                bat 'echo \"this is windows\"'\n            }\n        }\n    }\n}\n</code></pre> <p>The only execution that happens (almost) exclusively on the node (or build slave) are the isUnix(), sh and bat shell commands.</p> <p>Those specific tasks are the steps in pipeline code.</p>"},{"location":"jenkins-pipeline/core-concepts/#master-vs-nodes","title":"Master vs Nodes","text":"<p>There are many things to keep in mind about Pipelines in Jenkins.  By far the most important are those related to the distinction between Masters and Nodes.</p> <p>Aside from the points below, the key thing to keep in mind: Nodes (build slaves) are designed to executes task, Masters are not.</p> <ol> <li> <p>Except for the steps themselves, all of the Pipeline logic, the Groovy conditionals, loops, etc execute on the master. Whether simple or complex! Even inside a node block!</p> </li> <li> <p>Steps may use executors to do work where appropriate, but each step has a small on-master overhead too.</p> </li> <li> <p>Pipeline code is written as Groovy but the execution model is radically transformed at compile-time to Continuation Passing Style (CPS).</p> </li> <li> <p>This transformation provides valuable safety and durability guarantees for Pipelines, but it comes with trade-offs:</p> <ul> <li>Steps can invoke Java and execute fast and efficiently, but Groovy is much slower to run than normal.</li> <li>Groovy logic requires far more memory, because an object-based syntax/block tree is kept in memory.</li> </ul> </li> <li> <p>Pipelines persist the program and its state frequently to be able to survive failure of the master.</p> </li> </ol> <p>Source: Sam van Oort, Cloudbees Engineer</p>"},{"location":"jenkins-pipeline/core-concepts/#node","title":"Node","text":"<p>A machine which is part of the Jenkins environment and capable of executing Pipelines or Projects. Both the Master and Agents are considered to be Nodes.</p>"},{"location":"jenkins-pipeline/core-concepts/#master","title":"Master","text":"<p>The central, coordinating process which stores configuration, loads plugins, and renders the various user interfaces for Jenkins.</p>"},{"location":"jenkins-pipeline/core-concepts/#what-to-do","title":"What to do?","text":"<p>So, if Pipeline code can cause big loads on Master, what should we do than?</p> <ul> <li>Try to limit the use of logic in your groovy code</li> <li>Avoid blocking or I/O calls unless explicitly done on a slave via a Step</li> <li>If you need heavy processing, and there isn't a Step, create either a <ul> <li>plugin </li> <li>Shared Library</li> <li>Or use a CLI tool via a platform independent language, such as Java or Go</li> </ul> </li> </ul> <p>Tip</p> <p>If need to do any I/O, use a plugin or anything related to a workspace, you need a node. If you only need to interact with variables, for example for an input form, do this outside of a node block. See Pipeline Input for how that works.</p>"},{"location":"jenkins-pipeline/core-concepts/#workspace","title":"Workspace","text":"<p>A disposable directory on the file system of a Node where work can be done by a Pipeline or Project. Workspaces are typically left in place after a Build or Pipeline run completes unless specific Workspace cleanup policies have been put in place on the Jenkins Master.</p> <p>The key part of the glossary entry there is disposable directory. There are absolutely no guarantees about Workspaces in pipeline jobs.</p> <p>That said, what you should take care of:</p> <ul> <li>always clean your workspace before you start, you don't know the state of the folder you get</li> <li>always clean your workspace after you finish, this way you're less likely to run into problems in subsequent builds</li> <li>a workspace is a temporary folder on a single node's filesystem: so every time you use <code>node{}</code> you have a new workspace</li> <li>after your build is finish or leaving the node otherwise, your workspace should be considered gone: need something from? stash or archive it!</li> </ul>"},{"location":"jenkins-pipeline/core-concepts/#checkout","title":"Checkout","text":"<p>There are several ways to do a checkout in the Jenkins pipeline code.</p> <p>In the groovy DSL you can use the Checkout dsl command, svn shorthand or the git shorthand.</p> <pre><code>node {\n    stage('scm') {\n        git 'https://github.com/joostvdg/jishi'\n    }\n}\n</code></pre> <p>Danger</p> <p>If you use a pipeline from SCM, multi-branch pipeline or a derived job type, beware! Only the Jenkinsfile gets checked out. You still need to checkout the rest of your files yourself!</p> <p>Tip</p> <p>However, when using pipeline from SCM, multi-branch pipeline or a derived job type. You can use a shorthand: checkout scm. This checks out the scm defined in your job (where the Jenkinsfile came from).     <pre><code>node {\n    stage('scm') {\n        checkout scm\n    }\n}\n</code></pre></p>"},{"location":"jenkins-pipeline/core-concepts/#stage","title":"Stage","text":"<p>Stage is a step for defining a conceptually distinct subset of the entire Pipeline, for example: \"Build\", \"Test\", and \"Deploy\", which is used by many plugins to visualize or present Jenkins Pipeline status/progress.</p> <p>The stage \"step\" has a primary function and a secondary function.</p> <ul> <li>Its primary function is to define the visual boundaries between logically separable parts of the pipeline.<ul> <li>For example, you can define SCM, Build, QA, Deploy as stages to tell you where the build currently is or where it failed.</li> </ul> </li> <li>The secondary function is to provided a scope for variables.<ul> <li>Just like most programming languages, code blocks are a more than just syntactic sugar, they also limit the scope of variables.</li> </ul> </li> </ul> <pre><code>node {\n    stage('SCM') {\n        def myVar = 'abc'\n        checkout scm\n    }\n    stage('Build') {\n        sh 'mvn clean install'\n        echo myVar # will fail because the variable doesn't exist here\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/core-concepts/#stages-in-classic-view","title":"Stages in classic view","text":""},{"location":"jenkins-pipeline/core-concepts/#stages-in-blue-ocean-view","title":"Stages in Blue Ocean view","text":""},{"location":"jenkins-pipeline/core-concepts/#sandbox-and-script-security","title":"Sandbox and Script Security","text":"<p>In Jenkins some plugins - such as the pipeline plugin - allow you to write groovy code that gets executed on the master.</p> <p>This means you could run code on the master that accesses the host machine with the same rights as Jenkins. As is unsafe, Jenkins has some guards against this in the form the sandbox mode and the script security.</p> <p>When you create a pipeline job, you get a inline code editor by default.  If you're an administrator you get the option to turn the \"sandbox\" mode of.</p> <p>If you use a pipeline from SCM or any of the higher abstraction pipeline job types (Multibranch Pipeline, BitBucket Team) you are always running in sandbox mode.</p> <p>When you're in sandbox mode, your script will run past the script security.  This uses a whitelisting technique to block dangerous or undesired methods, but is does so in a very restrictive manner.</p> <p>It could be you're doing something that is safe but still gets blocked. An administrator can then go to the script approval page (under Jenkins Administration) and approve your script.</p> <p>For more details, please consult Script Security plugin page.</p>"},{"location":"jenkins-pipeline/core-concepts/#example-error","title":"Example error","text":"<pre><code>org.jenkinsci.plugins.scriptsecurity.sandbox.RejectedAccessException: unclassified staticMethod org.tmatesoft.svn.core.internal.io.dav.DAVRepositoryFactory create org.tmatesoft.svn.core.SVNURL\n    at org.jenkinsci.plugins.scriptsecurity.sandbox.groovy.SandboxInterceptor.onStaticCall(SandboxInterceptor.java:138)\n    at org.kohsuke.groovy.sandbox.impl.Checker$2.call(Checker.java:180)\n    at org.kohsuke.groovy.sandbox.impl.Checker.checkedStaticCall(Checker.java:177)\n    at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:91)\n    at com.cloudbees.groovy.cps.sandbox.SandboxInvoker.methodCall(SandboxInvoker.java:16)\n    at WorkflowScript.run(WorkflowScript:12)\n    at ___cps.transform___(Native Method)\n</code></pre> <p>Tip</p> <p>There are three ways to deal with these errors.</p> <ul> <li>go to manage jenkins \u2192 script approval and approve the script</li> <li>use a Shared Library</li> <li>use a CLI tool/script via a shell command to do what you need to do</li> </ul>"},{"location":"jenkins-pipeline/core-concepts/#java-vs-groovy","title":"Java vs. Groovy","text":"<p>The pipeline code has to be written in groovy and therefor can also use java code. Two big difference to note: </p> <ul> <li>the usage of double quoted string (gstring, interpreted) and single quoted strings (literal)<ul> <li><code>def abc = 'xyz' # is a literal</code></li> <li><code>echo '$abc' # prints $abc</code></li> <li><code>echo \"$abc\" # prints xyz</code></li> </ul> </li> <li>no use of ;</li> </ul> <p>Unfortunately, due to the way the Pipeline code is processed, many of the groovy features don't work or don't work as expected.</p> <p>Things like the lambda's and for-each loops don't work well and are best avoided. In these situations, it is best to keep to the standard syntax of Java.</p> <p>For more information on how the groovy is being processed, it is best to read the technical-design.</p>"},{"location":"jenkins-pipeline/core-concepts/#env-object","title":"Env (object)","text":"<p>The env object is an object that is available to use in any pipeline script.</p> <p>The env object allows you to store objects and variables to be used anywhere during the script. So things can be shared between nodes, the master and nodes and code blocks.</p> <p>Why would you want to use it? As in general, global variables are a bad practice. But if you need to have variables to be available through the execution on different machines (master, nodes) it is good to use this.</p> <p>Also the env object contains context variables, such as BRANCH_NAME, JOB_NAME and so one. For a complete overview, view the pipeline syntax page.</p> <p>Don't use the env object in functions, always feed them the parameters directly. Only use it in the \"pipeline flow\" and use it for the parameters of the methods.  </p> <pre><code>node {\n    stage('SCM') {\n        checkout scm \n    }\n    stage('Echo'){\n        echo \"Branch=$env.BRANCH_NAME\" // will print Branch=master\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/core-concepts/#stash-archive","title":"Stash &amp; archive","text":"<p>If you need to store files for keeping for later, there are two options available stash and archive.</p> <p>Both should be avoided as they cause heavy I/O traffic, usually between the Node and Master.</p> <p>For more specific information, please consult the Pipeline Syntax Page.</p>"},{"location":"jenkins-pipeline/core-concepts/#stash","title":"Stash","text":"<p>Stash allows you to copy files from the current workspace to a temp folder in the workspace in the master. If you're currently on a different machine it will copy them one by one over the network, keep this in mind.</p> <p>The files can only be retrieved during the pipeline execution and you can do so via the unstash command.</p> <pre><code>node('Machine1') {\n    stage('A') {\n        // generate some files\n        stash excludes: 'secret.txt', includes: '*.txt', name: 'abc'\n    }\n}\nnode('Machine2') {\n    stage('B') {\n        unstash 'abc'\n    }\n}\n</code></pre> <p>Saves a set of files for use later in the same build, generally on another node/workspace. Stashed files are not otherwise available and are generally discarded at the end of the build. Note that the stash and unstash steps are designed for use with small files. For large data transfers, use the External Workspace Manager plugin, or use an external repository manager such as Nexus or Artifactory.</p>"},{"location":"jenkins-pipeline/core-concepts/#archive-archiveartifacts","title":"Archive &amp; archiveArtifacts","text":"<p>Archives build output artifacts for later use. As of Jenkins 2.x, you may use the more configurable archiveArtifacts.</p> <p>With archive you can store a file semi-permanently in your job. Semi as the files will be overridden by the latest build.</p> <p>The files you archive will be stored in the Job folder on the master. </p> <p>One usecase is to save a log file from a build tool.</p> <pre><code>node {\n    stage('A') {\n\n        try {\n            // do some build\n        } finally {\n            // This step should not normally be used in your script. Consult the inline help for details.\n            archive excludes: 'useless.log', includes: '*.log'\n\n            // Use this instead, but only for permanent files, or external logfiles\n            archiveArtifacts allowEmptyArchive: true, artifacts: '*.log', excludes: 'useless.log', fingerprint: true, onlyIfSuccessful: true\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/core-concepts/#credentials","title":"Credentials","text":"<p>In many pipelines you will have to deal with external systems, requiring credentials.</p> <p>Jenkins has the Credentials API which you can also utilize in the pipeline.</p> <p>You can use do this via the Credentials and Credentials Binding plugins, the first is the core plugin the second provides the integration for the pipeline.</p> <p>The best way to generate the required code snippet, is to go to the pipeline syntax page, select withCredentials and configure what you need.</p> <pre><code>node {\n    stage('someRemoteCall') {\n        withCredentials([usernameColonPassword(credentialsId: 'someCredentialsId', variable: 'USRPASS')]) {\n            sh \"curl -u $env.USRPASS $URL\"\n        }\n    }\n}\n</code></pre> <p>For more examples, please consult Cloudbees' Injecting-Secrets-into-Jenkins-Build-Jobs blog post.</p>"},{"location":"jenkins-pipeline/core-concepts/#tools-build-environment","title":"Tools &amp; Build Environment","text":"<p>Jenkins would not be Jenkins without the direct support for the build tools, such as JDK's, SDK's, Maven, Ant what have you not.</p> <p>So, how do you use them in the pipeline?</p> <p>Unfortunately, this is a bit more cumbersome than it is in a freestyle (or legacy) job.</p> <p>You have to do two things:</p> <ol> <li>retrieve the tool's location via the tool DSL method</li> <li>set the environment variables to suit the tool</li> </ol> <pre><code>node {\n    stage('Maven') {\n        String jdk = tool name: 'jdk_8', type: 'jdk'\n        String maven = tool name: 'maven_3.5.0', type: 'maven'\n        withEnv([\"JAVA_HOME=$jdk\", \"PATH+MAVEN=${jdk}/bin:${maven}/bin\"]) {\n            sh 'mvn clean install' \n        }\n\n        // or in one go\n        withEnv([\"JAVA_HOME=${ tool 'jdk_8' }\", \"PATH+MAVEN=${tool 'maven_3.5.0'}/bin:${env.JAVA_HOME}/bin\"]) {\n            sh 'mvn clean install' \n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/core-concepts/#pipeline-syntax-page","title":"Pipeline Syntax Page","text":"<p>Soooo, do I always have to figure out how to write these code snippets?</p> <p>No, don't worry. You don't have to.</p> <p>At every pipeline job type there is a link called \"Pipeline Syntax\".</p> <p>This gives you a page with a drop down menu, from where you can select all the available steps.</p> <p>Once you select a step, you can use the UI to setup the step and then use the generate button to give you the correct syntax. </p>"},{"location":"jenkins-pipeline/declarative-pipeline/","title":"Declarative Pipeline","text":"<p>Declarative Pipeline is a relatively recent addition to Jenkins Pipeline [1] which presents a more simplified and opinionated syntax on top of the Pipeline sub-systems.</p> <p>All valid Declarative Pipelines must be enclosed within a pipeline block, for example:</p> <pre><code>pipeline {\n    /* insert Declarative Pipeline here */\n}\n</code></pre>"},{"location":"jenkins-pipeline/declarative-pipeline/#hello-world-example","title":"Hello World Example","text":"<pre><code>pipeline {\n    agent { docker 'python:3.5.1' }\n    stages {\n        stage('build') {\n            steps {\n                sh 'python --version'\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/declarative-pipeline/#mkdocs-build-example","title":"MKDocs Build Example","text":"<pre><code>pipeline {\n    agent none\n    options {\n        timeout(time: 10, unit: 'MINUTES')\n        timestamps()\n        buildDiscarder(logRotator(numToKeepStr: '5'))\n    }\n    stages {\n        stage('Prepare'){\n            agent { label 'docker' }\n            steps {\n                parallel (\n                    Clean: {\n                        deleteDir()\n                    },\n                    NotifySlack: {\n                        slackSend channel: 'cicd', color: '#FFFF00', message: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n                    }\n                )\n            }\n        }\n        stage('Checkout'){\n            agent { label 'docker' }\n            steps {\n                git credentialsId: '355df378-e726-4abd-90fa-e723c5c21ad5', url: 'git@gitlab.flusso.nl:CICD/ci-cd-docs.git'\n                script {\n                    env.GIT_COMMIT_HASH = sh returnStdout: true, script: 'git rev-parse --verify HEAD'\n                }\n            }\n        }\n        stage('Build Docs') {\n            agent {\n                docker {\n                    image \"caladreas/mkdocs-docker-build-container\"\n                    label \"docker\"\n                }\n            }\n            steps {\n                sh 'mkdocs build'\n            }\n        }\n        stage('Prepare Docker Image'){\n            agent { label 'docker' }\n            steps {\n                parallel (\n                    TestDockerfile: {\n                        script {\n                            def lintResult = sh returnStdout: true, script: 'docker run --rm -i lukasmartinelli/hadolint &lt; Dockerfile'\n                            if (lintResult.trim() == '') {\n                                println 'Lint finished with no errors'\n                            } else {\n                                println 'Error found in Lint'\n                                println \"${lintResult}\"\n                                currentBuild.result = 'UNSTABLE'\n                            }\n                        }\n                    }, // end test dockerfile\n                    BuildImage: {\n                        sh 'chmod +x build.sh'\n                        sh './build.sh'\n                    } \n                )\n            }\n            post {\n                success {\n                    sh 'chmod +x push.sh'\n                    sh './push.sh'\n                }\n            }\n        }\n        stage('Update Docker Container') {\n            agent { label 'docker' }\n            steps {\n                sh 'chmod +x container-update.sh'\n                sh \"./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH}\"\n            }\n        }\n    }\n    post {\n        success {\n            slackSend channel: 'cicd', color: '#00FF00', message: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n        }\n        failure {\n            slackSend channel: 'cicd', color: '#FF0000', message: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/declarative-pipeline/#resources","title":"Resources","text":"<ul> <li>Syntax Reference</li> <li>Getting started</li> <li>Notifications</li> </ul>"},{"location":"jenkins-pipeline/github-org/","title":"GitHub Organization Pipeline","text":""},{"location":"jenkins-pipeline/github-org/#add-webhook","title":"Add Webhook","text":"<ul> <li>https://dzone.com/articles/adding-a-github-webhook-in-your-jenkins-pipeline</li> <li><code>${JENKINS_URL}/github-webhook/</code></li> </ul>"},{"location":"jenkins-pipeline/global-shared-library/","title":"Global Shared Library","text":"<p>https://jenkins.io/doc/book/pipeline/shared-libraries/</p> <p>When you're making pipelines on Jenkins you will run into the situation that you will want to stay DRY. To share pipeline code there are several ways.</p> <ul> <li>SCM: Have a pipeline dsl script in a SCM and load it from there</li> <li>Plugin: A Jenkins plugin that you can call via the pipeline dsl</li> <li>Global Workflow Library: There is a global library for pipeline dsl scripts in the Jekins master Preferred solution</li> </ul> <p>Please read the documentation to get a basic idea.</p> <p>Danger</p> <p>When using a Global Library you will always have to import something from this library. This doesn't make sense when you online use functions (via the vars folder). In this case, you have to import nothing, which you do via: \"_\"</p> <pre><code>@Library('FlussoGlobal')\nimport nl.flusso.Utilities\n</code></pre> <pre><code>@Library('FlussoGlobal') _\n</code></pre>"},{"location":"jenkins-pipeline/global-shared-library/#library-directory-structure","title":"Library Directory structure","text":"<p>The directory structure of a shared library repository is as follows:</p> <pre><code>(root)\n +- src                     # Groovy source files\n |   +- org\n |       +- foo\n |           +- Bar.groovy  # for org.foo.Bar class\n +- vars\n |   +- foo.groovy          # for global 'foo' variable/function\n |   +- foo.txt             # help for 'foo' variable/function\n +- resources               # resource files (external libraries only)\n |   +- org\n |       +- foo\n |           +- bar.json    # static helper data for org.foo.Bar\n</code></pre> <p>The <code>src</code> directory should look like standard Java source directory structure. This directory is added to the classpath when executing Pipelines.</p> <p>The <code>vars</code> directory hosts scripts that define global variables accessible from Pipeline scripts. The basename of each <code>*.groovy</code> file should be a Groovy (~ Java) identifier, conventionally <code>camelCased</code>. The matching <code>*.txt</code>, if present, can contain documentation, processed through the system\u2019s configured markup formatter (so may really be HTML, Markdown, etc., though the <code>txt</code> extension is required).</p> <p>The Groovy source files in these directories get the same \u201cCPS transformation\u201d as your Pipeline scripts.</p> <p>A <code>resources</code> directory allows the <code>libraryResource</code> step to be used from an external library to load associated non-Groovy files. Currently this feature is not supported for internal libraries.</p> <p>Other directories under the root are reserved for future enhancements.</p>"},{"location":"jenkins-pipeline/global-shared-library/#configure-libraries-in-jenkins","title":"Configure libraries in Jenkins","text":"<p>The a Jenkins Master you can configure the Global Pipeline Libraries.</p> <p>You can find this in: Manage Jenkins -&gt; Configure System -&gt; Global Pipeline Libraries</p> <p>You can configure multiple libraries, where the there is a preference for Git repositories. You can select a default version (for example: the master branch), and either allow or disallow overrides to this.</p> <p>To be able to use a different version, you would use the @ in case of Git. <pre><code>@Library('FlussoGlobal@my-feature-branch')\n</code></pre> <p></p>"},{"location":"jenkins-pipeline/global-shared-library/#helloworld-example","title":"HelloWorld Example","text":"<ul> <li>Create Git repository (see below for structure)</li> <li>Configure this Git repository as an \"Global Pipeline Libraries\" entry</li> <li>Name: FlussoGlobal</li> <li>Default Version: master</li> <li>Modern SCM: git</li> <li>Project repository: git@gitlab.flusso.nl:CICD/jenkins-pipeline-library.git</li> <li>Create the resources you want in the git repository</li> <li>Use the library in a pipeline</li> </ul>"},{"location":"jenkins-pipeline/global-shared-library/#util-class-class-example","title":"Util Class (class) Example","text":"<pre><code>#!/usr/bin/groovy\n#/src/nl/flusso/Utilities.groovy\npackage nl.flusso\n\nimport java.io.Serializable\n\nclass Utilities implements Serializable {\n  def steps\n\n  Utilities(steps) {this.steps = steps}\n\n  def sayHello(String name) {\n    steps.sh \"echo $name\"\n  }\n}\n</code></pre> <pre><code>@Library('FlussoGlobal')\nimport nl.flusso.Utilities\n\ndef utils = new Utilities(steps)\n\nnode {\n    String name = 'Joost'\n    utils.sayHello(name)\n}\n</code></pre>"},{"location":"jenkins-pipeline/global-shared-library/#util-method-var-example","title":"Util method (var) Example","text":"<pre><code>#!/usr/bin/groovy\n#/vars/sayHello.groovy\ndef call(name) {\n    // you can call any valid step functions from your code, just like you can from Pipeline scripts\n    echo \"Hello world, ${name}\"\n}\n</code></pre> <pre><code>@Library('FlussoGlobal') _\nnode {\n    String name = 'Joost'\n    sayHello name\n}\n</code></pre>"},{"location":"jenkins-pipeline/global-shared-library/#combining-libraries","title":"Combining libraries","text":"<p>Lets say you want to want to have a core library and multiple specific libraries that utilize these. There are several to do this, we will show two.</p>"},{"location":"jenkins-pipeline/global-shared-library/#import-both","title":"Import both","text":"<p>One way is to explicitly import both libraries in the Jenkinsfile.</p> <pre><code>@Library(['github.com/joostvdg/jenkins-pipeline-lib','github.com/joostvdg/jenkins-pipeline-go']) _\n</code></pre> <p>Con:</p> <ul> <li>you have to import all the required libraries yourself</li> </ul> <p>Pro:</p> <ul> <li>you can specify the versions of each</li> </ul>"},{"location":"jenkins-pipeline/global-shared-library/#implicit-import-explicit-import","title":"Implicit Import + Explicit Import","text":"<p>You can also configure the core (in this case jenkins-pipeline-lib) as \"loaded implicitly\". This will make anything from this library available by default.</p> <p>Be careful with the naming of the vars though!</p> <p>The resulting Jenkinsfile would then be.</p> <pre><code>@Library('github.com/joostvdg/jenkins-pipeline-go') _\n</code></pre>"},{"location":"jenkins-pipeline/global-shared-library/#resources","title":"Resources","text":"<ul> <li>implement-reusable-function-call</li> </ul>"},{"location":"jenkins-pipeline/groovy-pipeline/","title":"Jenkins Pipelines","text":"<p>Warning</p> <p>This style of pipeline definition is deprecated. When possible, please use the declarative version.</p> <p>Jenkins Pipeline is a suite of plugins which supports implementing and integrating continuous delivery pipelines into Jenkins. Pipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines \"as code\" via the Pipeline DSL.</p> <p>There are two ways to create pipelines in Jenkins. Either via the Groovy DSL or via the Declarative pipeline.</p> <p>For more information about the declarative pipeline, read the next page.</p>"},{"location":"jenkins-pipeline/groovy-pipeline/#hello-world-example","title":"Hello World Example","text":"<pre><code>node {\n    timestamps {\n        stage ('My FIrst Stage') {\n            if (isUnix()) {\n                sh 'echo \"this is Unix!\"'\n            } else {\n                bat 'echo \"this is windows\"'\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/groovy-pipeline/#resources","title":"Resources","text":"<ul> <li>Getting started</li> <li>Best practices</li> <li>Best practices for scaling</li> <li>Possible Steps</li> </ul>"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/","title":"IDE Integration for Jenkins Pipeline DSL","text":""},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#supported-ides","title":"Supported IDE's","text":"<p>Currently only Jetbrain's Intelli J's IDEA is supported.</p> <p>This via a Groovy DSL file (.gdsl).</p>"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#configure-intelli-j-idea","title":"Configure Intelli J IDEA","text":"<p>Go to a Jenkins Pipeline job and open the Pipeline Syntax page.</p> <p>On the page in the left hand menu, you will see a link to download a Jenkins Master specific Groovy DSL file. Download this and save it into your project's workspace.</p> <p></p> <p>It will have to be part of your classpath, the easiest way to do this is to add the file as pipeline.gdsl in a/the src folder.</p> <p>For more information, you can read Steffen Gerbert's blog.</p>"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#remarks-from-kohsuke-kawaguchi","title":"Remarks from Kohsuke Kawaguchi","text":"<p>More effort in this space will be taken by Cloudbees. But the priority is low compared to other initiatives.</p>"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#integration-of-pipeline-library","title":"Integration of Pipeline Library","text":"<p>If you're using the Global Shared Libraries for sharing generic pipeline building blocks, it would be nice to have this awareness in your editor as well.</p> <p>One of the ways to do this, is to checkout the source code of this library and make sure it is compiled. In your editor (assuming Intelli J IDEA) you can then add the compiled classes as dependency (type: classes). This way, at least every class defined in your library is usable as a normal dependency would be. </p>"},{"location":"jenkins-pipeline/ide-integration-pipeline-dsl/#final-configuration-intelli-j-idea","title":"Final configuration Intelli J IDEA","text":""},{"location":"jenkins-pipeline/input/","title":"Jenkins Pipeline - Input","text":"<p>The Jenkins Pipeline has a plugin for dealing with external input. Generally it is used to gather user input (values or approval), but it also has a REST API for this.</p>"},{"location":"jenkins-pipeline/input/#general-info","title":"General Info","text":"<p>The Pipeline Input Step allows you to</p> <p>The plugin allows you to capture input in a variety of ways, but there are some gotcha's.</p> <ul> <li>If you have a single parameter, it will be returned as a single value</li> <li>If you have multiple parameters, it will be returned as a map</li> <li>The choices for the Choice parameter should be a single line, where values are separated with /n</li> <li>Don't use input within a node {}, as this will block an executor slot</li> <li>..</li> </ul>"},{"location":"jenkins-pipeline/input/#examples","title":"Examples","text":""},{"location":"jenkins-pipeline/input/#single-parameter","title":"Single Parameter","text":"<pre><code>def hello = input id: 'CustomId', message: 'Want to continue?', ok: 'Yes', parameters: [string(defaultValue: 'world', description: '', name: 'hello')]\n\nnode {\n    println \"echo $hello\"\n}\n</code></pre>"},{"location":"jenkins-pipeline/input/#multiple-parameters","title":"Multiple Parameters","text":"<pre><code>def userInput = input id: 'CustomId', message: 'Want to continue?', ok: 'Yes', parameters: [string(defaultValue: 'world', description: '', name: 'hello'), string(defaultValue: '', description: '', name: 'token')]\n\nnode {\n    def hello = userInput['hello']\n    def token = userInput['token']\n    println \"hello=$hello, token=$token\"\n}\n</code></pre>"},{"location":"jenkins-pipeline/input/#timeout-on-input","title":"Timeout on Input","text":"<pre><code>def userInput\n\ntimeout(time: 10, unit: 'SECONDS') {\n    println 'Waiting for input'\n    userInput = input id: 'CustomId', message: 'Want to continue?', ok: 'Yes', parameters: [string(defaultValue: 'world', description: '', name: 'hello'), string(defaultValue: '', description: '', name: 'token')]\n}\n</code></pre>"},{"location":"jenkins-pipeline/input/#rest-api","title":"REST API","text":"<p>There's a rest API for sending the input to a waiting input step. The format of the url: {JenkinsURL}/{JobURL}/{Build#}/input/{InputID}/submit.</p> <p>There are some things to keep in mind:</p> <ul> <li>If Jenkins has CSRF protection enabled, you need a Crumb (see below) for the requests</li> <li>Requests are send via POST</li> <li>For supplying values you need to have a JSON with the parameters with as json param</li> <li>You need to supply the proceed value: the value of the ok button, as proceed param</li> <li>You will have to fill in the input id, so it is best to configure a unique input id for the input steps you want to connect to from outside</li> </ul>"},{"location":"jenkins-pipeline/input/#examples_1","title":"Examples","text":"<pre><code>{\"parameter\":\n    [\n        {\"name\": \"hello\", \"value\": \"joost\"},\n        {\"name\": \"token\", \"value\": \"not a token\"}\n    ]\n}\n</code></pre> <p><pre><code># single parameter\ncurl --user $USER:$PASS -X POST -H \"Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33\" -d json='{\"parameter\": {\"name\": \"hello\", \"value\": \"joost\"}}' -d proceed='Yes' 'https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit'\n</code></pre> <pre><code># Multiple Parameters\ncurl --user $USER:$PASS -X POST -H \"Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33\" -d json='{\"parameter\": [{\"name\": \"hello\", \"value\": \"joost\"},{\"name\": \"token\", \"value\": \"not a token\"}]}' -d proceed='Yes' 'https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit'\n</code></pre></p>"},{"location":"jenkins-pipeline/input/#crumb-secured-jenkins","title":"Crumb (secured Jenkins)","text":"<p>If Jenkins is secured against CSRF (via Global Security: Prevent Cross Site Request Forgery exploits), any API call requires a Crumb. You can read more about it here.</p> <p>To get a valid crumb you have to send a crumb request as authenticated user.</p> <ul> <li>JSON: https://ci.flusso.nl/jenkins/crumbIssuer/api/json</li> <li>XML (parsed): https://ci.flusso.nl/jenkins/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,%22:%22,//crumb)</li> </ul>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/","title":"Parallel Pipeline","text":"<p>Building applications can be fun, but it can also cause a lot of wait time<sup>3</sup>.</p> <p>There are many ways to speed up builds, do fewer tests, get bigger and better hardware, or run some tasks in parallel.</p> <p>Jenkins Pipelines can do parallel stages for a while, even in the Declarative format<sup>1</sup>.</p> <p>Although doing parallel pipelines, Jenkins didn't become awesome until Sequential Stages <sup>2</sup>. We will dive into the magic of Sequential Stages, but first, let's start with building in parallel.</p>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#parallel-stages","title":"Parallel Stages","text":"<p>This is an elementary example. We have an application we want to build in Java 11 - latest LTS - and the latest version of Java - now Java 13.</p> <p>As both are running in their own containers, each can leverage its own resources - provided the underlying VM has them available.</p> <pre><code>pipeline {\n    agent {\n        kubernetes {\n            label 'jx-maven-lib'\n            yaml \"\"\"\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: maven11\n    image: maven:3-jdk-11\n    command: ['cat']\n    tty: true\n  - name: maven13\n    image: maven:3-jdk-13\n    command: ['cat']\n    tty: true\n\"\"\"\n        }\n    }\n    stages {\n        stage('Checkout') {\n            steps {\n                git 'https://github.com/joostvdg/jx-maven-lib.git'\n            }\n        }\n        stage('Run Tests') {\n            parallel {\n                stage('Java 11') {\n                    steps {\n                        container('maven11') {\n                            sh 'mvn -V -e -C verify'\n                        }\n                    }\n                }\n                stage('Java 13') {\n                    steps {\n                        container('maven13') {\n                            sh 'mvn -V -e -C -Pjava13 verify'\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#visualization","title":"Visualization","text":"<p>This will then look like this:</p> <p></p>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#sequential","title":"Sequential","text":"<p>Before we dive into all the benefits of Sequential Stages, let's look at how the syntax changes.</p> <p>With Parallel Stages, we can execute some steps in parallel, but with regards to visualizing individual steps, it is inferior.</p> <p>Sequential Stages allows us to add Stages in sequence within a Parallel step, which is why I usually call them Parallel Sequential stages.</p> <p>In summary, the syntax now becomes:</p> <pre><code>stages {\n    stage('Checkout') {\n        stage('Run Tests') {\n            parallel {\n                stage('Java 11') {\n                    stages {\n                        stage('Build') {\n                            steps {}\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n</code></pre> <p>The full example:</p> <pre><code>pipeline {\n    agent {\n        kubernetes {\n            label 'jx-maven-lib'\n            yaml \"\"\"\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: maven11\n    image: maven:3-jdk-11\n    command: ['cat']\n    tty: true\n  - name: maven13\n    image: maven:3-jdk-13\n    command: ['cat']\n    tty: true\n\"\"\"\n        }\n    }\n    stages {\n        stage('Checkout') {\n            steps {\n                git 'https://github.com/joostvdg/jx-maven-lib.git'\n            }\n        }\n        stage('Run Tests') {\n            parallel {\n                stage('Java 11') {\n                    stages {\n                        stage('Build') {\n                            steps {\n                                container('maven11') {\n                                    sh 'mvn -V -e -C package'\n                                }\n                            }\n                        }\n                        stage('Test') {\n                            steps {\n                                container('maven11') {\n                                    sh 'mvn -V -e -C test'\n                                }\n                            }\n                        }\n                    }\n                }\n                stage('Java 13') {\n                    stages {\n                        stage('Build') {\n                            steps {\n                                container('maven13') {\n                                    sh 'mvn -V -e -C -Pjava13 package'\n                                }\n                            }\n                        }\n                        stage('Test') {\n                            steps {\n                                container('maven13') {\n                                    sh 'mvn -V -e -C -Pjava13 test'\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#visualization_1","title":"Visualization","text":"<p>The first thing we notice is that we get more indenting and more <code>{ }</code>. But, we can also visualize independent stages within the parallel \"streams\".</p> <p>If you're thinking, but can't I now do much more with stage individual stages? You're right, and we'll dive into that next.</p> <p></p>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#sequential-with-separate-pods","title":"Sequential With Separate Pods","text":"<p>The biggest downside of the previous examples is that the Kubernetes Pod is always there, including all the containers we need.</p> <p>But what if some parallel tasks take much longer than others? It would be great if the other containers would be removed as soon if we're done with them.</p> <p>With Sequential stages, we can achieve this. We first set <code>agent none</code>, to make sure we don't have a Pod running from start to finish.</p> <p>This comes with a price, though, now every stage will need to have its own agent defined. Luckily, combining Parallel and Sequential stages, we can give each parallel \"stream\" an agent - a Pod - and have each sequential stage use this.</p> <p>In summary, we do this:</p> <ul> <li>Pipeline: no agent<ul> <li>parallel: Build<ul> <li>stream java 11: agent maven11</li> <li>stream java 13: agent maven13</li> </ul> </li> <li>parallel: Test<ul> <li>stream java 11: agent maven11<ul> <li>Functional Tests</li> <li>API Contract Tests</li> <li>Performance Tests</li> </ul> </li> <li>stream java 13: agent maven13<ul> <li>Functional Tests</li> <li>API Contract Tests</li> <li>Performance Tests</li> </ul> </li> </ul> </li> <li>Deploy</li> </ul> </li> </ul> <p>Another benefit is that each stage can leverage every <code>Declarative Directive</code><sup>4</sup>, such as <code>when { }</code><sup>5</sup>. In this example, we've used <code>when { branch 'master' }</code> to avoid executing steps when we're not on branch <code>master</code>.</p> <p>To extend this even further, we can now leverage both the dynamic Pod allocation and the When Directive. When combined with <code>beforeAgent true</code>, we won't even spin up the Pod, avoiding unnecessary resource consumption and waiting.</p> <pre><code>stage('Deploy') {\n    agent { ... }\n    when {\n        branch 'master'\n        beforeAgent true\n    }\n    steps { echo \"hello\" }\n}\n</code></pre> <p>The complete example now looks like this.</p> <pre><code>pipeline {\n    agent none\n    stages {\n        stage('Build') {\n            parallel {\n                stage('Java 11') {\n                    agent {\n                        kubernetes {\n                            label 'jxmavenlib-jdk11_build'\n                            containerTemplate {\n                                name 'maven11'\n                                image 'maven:3-jdk-11'\n                                ttyEnabled true\n                                command 'cat'\n                            }\n                        }\n                    }\n                    steps {\n                        container('maven11') {\n                            sh 'mvn -v'\n                        }\n                    }\n                }\n                stage('Java 13') {\n                    agent {\n                        kubernetes {\n                            label 'jxmavenlib-jdk13-build'\n                            containerTemplate {\n                                name 'maven13'\n                                image 'maven:3-jdk-13'\n                                ttyEnabled true\n                                command 'cat'\n                            }\n                        }\n                    }\n                    steps {\n                        container('maven13') {\n                            sh 'mvn -v'\n                        }\n                    }\n                }\n            }\n        }\n        stage('Test') {\n            parallel {\n                stage('Java 11') {\n                    agent {\n                        kubernetes {\n                            label 'jxmavenlib-jdk11-test'\n                            containerTemplate {\n                                name 'maven'\n                                image 'maven:3-jdk-11'\n                                ttyEnabled true\n                                command 'cat'\n                            }\n                        }\n                    }\n                    stages {\n                        stage('Functional Tests') {\n                            steps {\n                                echo 'Hello'\n                            }\n                        }\n                        stage('API Contract Tests') {\n                            steps {\n                                echo 'Hello'\n                            }\n                        }\n                        stage('Performance Tests') {\n                            when {\n                                branch 'master'\n                            }\n                            steps {\n                                echo 'Hello'\n                            }\n                        }\n                    }\n                }\n                stage('Java 13') {\n                    agent {\n                        kubernetes {\n                            label 'jxmavenlib-jdk13-test'\n                            containerTemplate {\n                                name 'mavenjdk11'\n                                image 'maven:3-jdk-13'\n                                ttyEnabled true\n                                command 'cat'\n                            }\n                        }\n                    }\n                    stages {\n                        stage('Functional Tests') {\n                            steps {\n                                echo 'Hello'\n                            }\n                        }\n                        stage('API Contract Tests') {\n                            steps {\n                                echo 'Hello'\n                            }\n                        }\n                        stage('Performance Tests') {\n                            when {\n                                branch 'master'\n                            }\n                            steps {\n                                echo 'Hello'\n                            }\n                        }\n                    }\n                }\n            }\n        }\n        stage('Deploy') {\n            agent {\n                kubernetes {\n                    label 'jxmavenlib-deployment'\n                    containerTemplate {\n                        name 'pl_deployment'\n                        image 'cloudbees/docker-java-with-docker-client'\n                        ttyEnabled true\n                        command 'cat'\n                    }\n                }\n            }\n            when {\n                branch 'master'\n                beforeAgent true\n            }\n            steps {\n                echo \"hello\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#visualization_2","title":"Visualization","text":"<p>When visualized, we can spot which stages are skipped due to the <code>when {}</code> Directives.</p> <p></p>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#sequential-with-optional-reusable-pods","title":"Sequential With Optional &amp; Reusable Pods","text":"<p>We're not done yet \u2014 one more step to take.</p> <p>While dynamically allocating Pods with containers sounds excellent, it comes at a cost. If you do something in the build phase and you need the data in the latest stages, you've now lost it - new Pod = new workspace.</p> <p>There are a few things you can do, but none of them pretty. You can use <code>stash</code><sup>6</sup> and <code>unstash</code>[\u02c67], but these can be very costly in terms of I/O performance and time.</p> <p>Alternatively, you can either look for externalizing your workspace or keep the Pod around for reuse.</p> <p>Important</p> <p>If you can run more than one build at the same time - concurrent builds - you run the risk of having builds claim Pods another build has done work in.</p> <p>To avoid builds reusing Pods from other runs, you can disable concurrent builds.</p> <pre><code>options {\n    disableConcurrentBuilds()\n}\n</code></pre> <p>Alternatively, you can encode the build number into the name:</p> <pre><code>agent {\n    kubernetes {\n        label \"jxmavenlib-jdk11-b${BUILD_NUMBER}\"\n        yaml ...\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#pod-reuse","title":"Pod Reuse","text":"<p>To reuse a Pod, we have to override some default values in the PodTemplate<sup>10</sup>.</p> <ul> <li><code>idleMinutes</code>: Allows the Pod to remain active for reuse until the configured number of minutes has passed since the last step was executed on it.</li> </ul> <p>The configuration below means the Pod can be idle for about 5 minutes before it gets deleted. Additionally, we changed the label not to include the phase name. Otherwise, we cannot get the same Pod. </p> <pre><code>agent {\n    kubernetes {\n        idleMinutes 5\n        label 'jxmavenlib-jdk11'\n        yaml \"\"\"\nspec:\n  containers:\n  - name: maven11\n    image: maven:3-jdk-11\n    command: ['cat']\n    tty: true\n\"\"\"\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#volume-for-externalizing-workspace","title":"Volume For Externalizing Workspace","text":"<p>There are various ways to externalize your workspace, you can leverage NFS<sup>8</sup> or Jenkins workspace related plugins<sup>9</sup>.</p> <p>One way we'll look at here is to leverage a <code>PersistedVolume</code> in Kubernetes - which could be of any kind, incl NFS<sup>8</sup>.</p> <p>We add a volume of type <code>persistentVolumeClaim</code> and point to an existing one by <code>claimName</code>.</p> <pre><code>    volumeMounts:\n      - name: build-cache\n        mountPath: /tmp/cache\n  volumes:\n    - name: build-cache\n      persistentVolumeClaim:\n        claimName: azure-managed-disk\n</code></pre> azure-managed-disk-pvc.yaml <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: azure-managed-disk\nspec:\n  accessModes:\n  - ReadWriteOnce\n  storageClassName: managed-premium\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#full-example","title":"Full Example","text":"<pre><code>pipeline {\n    agent none\n    options {\n        disableConcurrentBuilds()\n    }\n    stages {\n        stage('Build') {\n            parallel {\n                stage('Java 11') {\n                    agent {\n                        kubernetes {\n                            idleMinutes 5\n                            label \"jxmavenlib-jdk11-b${BUILD_NUMBER}\"\n                            yaml \"\"\"\nspec:\n  containers:\n  - name: maven11\n    image: maven:3-jdk-11\n    command: ['cat']\n    tty: true\n    volumeMounts:\n      - name: maven-cache\n        mountPath: /root/.m2/repository\n      - name: build-cache\n        mountPath: /tmp/cache\n  volumes:\n    - name: maven-cache\n      hostPath:\n        path: /tmp\n        type: Directory\n    - name: build-cache\n      persistentVolumeClaim:\n        claimName: azure-managed-disk\n\"\"\"\n                        }\n                    }\n                    steps {\n                        git 'https://github.com/joostvdg/jx-maven-lib.git'\n                        container('maven11') {\n                            sh 'mvn -V -e -C verify'\n                            sh 'cp -R target/ /tmp/cache/'\n                            sh 'ls -lath /tmp/cache/'\n                        }\n                    }\n                }\n                stage('Java 13') {\n                    agent {\n                        kubernetes {\n                            idleMinutes 5\n                            label \"jxmavenlib-jdk13-b${BUILD_NUMBER}\"\n                            yaml \"\"\"\nspec:\n  containers:\n  - name: maven13\n    image: maven:3-jdk-13\n    command: ['cat']\n    tty: true\n    volumeMounts:\n      - name: maven-cache\n        mountPath: /root/.m2/repository\n  volumes:\n    - name: maven-cache\n      hostPath:\n        path: /tmp\n        type: Directory\n\"\"\"\n                        }\n                    }\n                    steps {\n                        git 'https://github.com/joostvdg/jx-maven-lib.git'\n                        container('maven13') {\n                            sh 'mvn -V -e -C -Pjava13 verify'\n                        }\n                    }\n                }\n            }\n        }\n        stage('Test') {\n            parallel {\n                stage('Java 11') {\n                    agent {\n                        kubernetes {\n                            idleMinutes 5\n                            label \"jxmavenlib-jdk11-b${BUILD_NUMBER}\"\n                            yaml \"\"\"\nspec:\n  containers:\n  - name: maven11\n    image: maven:3-jdk-11\n    command: ['cat']\n    tty: true\n    volumeMounts:\n      - name: maven-cache\n        mountPath: /root/.m2/repository\n      - name: build-cache\n        mountPath: /tmp/cache\n  volumes:\n    - name: maven-cache\n      hostPath:\n        path: /tmp\n        type: Directory\n    - name: build-cache\n      persistentVolumeClaim:\n        claimName: azure-managed-disk\n\"\"\"\n                        }\n                    }\n                    stages {\n                        stage('Functional Tests') {\n                            steps {\n                                container('maven11') {\n                                    sh 'ls -lath /tmp/cache'\n                                    sh 'cp -R /tmp/cache/ .'\n                                    sh 'ls -lath'\n                                }\n                            }\n                        }\n                        stage('API Contract Tests') {\n                            steps {\n                                echo 'Hello'\n                            }\n                        }\n                        stage('Performance Tests') {\n                            when {\n                                branch 'master'\n                            }\n                            steps {\n                                echo 'Hello'\n                            }\n                        }\n                    }\n                }\n                stage('Java 13') {\n                    agent {\n                        kubernetes {\n                            idleMinutes 5\n                            label \"jxmavenlib-jdk13-b${BUILD_NUMBER}\"\n                            yaml \"\"\"\nspec:\n  containers:\n  - name: maven13\n    image: maven:3-jdk-13\n    command: ['cat']\n    tty: true\n    volumeMounts:\n      - name: maven-cache\n        mountPath: /root/.m2/repository\n  volumes:\n    - name: maven-cache\n      hostPath:\n        path: /tmp\n        type: Directory\n\"\"\"\n                        }\n                    }\n                    stages {\n                        stage('Functional Tests') {\n                            steps {\n                               container('maven13') {\n                                    sh 'ls -lath'\n                                }\n                            }\n                        }\n                        stage('API Contract Tests') {\n                            steps {\n                                echo 'Hello'\n                            }\n                        }\n                        stage('Performance Tests') {\n                            when {\n                                branch 'master'\n                            }\n                            steps {\n                                echo 'Hello'\n                            }\n                        }\n                    }\n                }\n            }\n        }\n        stage('Deploy') {\n            agent {\n                kubernetes {\n                    label 'jxmavenlib-deployment'\n                    containerTemplate {\n                        name 'pl_deployment'\n                        image 'cloudbees/docker-java-with-docker-client'\n                        ttyEnabled true\n                        command 'cat'\n                    }\n                }\n            }\n            when {\n                branch 'master'\n                beforeAgent true\n            }\n            steps {\n                echo \"hello\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#matrix","title":"Matrix","text":"<p>In late 2019, Jenkins released the <code>Matrix Build</code> feature. It extends the Parallel and Sequential features and allows you to create a Matrix of options for parallel execution.</p> <p>If you want to get a more in-depth look at this feature, I recommend reading Liam Newman's<sup>11</sup> excellent introductory blog post.</p> <p>When you've read this and Liam's posts, I recommend you go through the <sup>12</sup> test cases of the <code>Matrix</code> build feature on GitHub. They provide a wealth of information and worked-out examples.</p>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#what-is-it","title":"What Is It","text":"<p>The <code>Matrix</code> build feature extends the Parallel and Sequential features mentioned earlier in this article. What it allows you to do is to specify multiple sets of values (<code>axis</code>), and then execute each entry of the cartesian product<sup>13</sup>.</p> <p><code>Matrix</code> acts as a special stage type, and can include an <code>agent{}</code> and <code>stages{}</code> definition.</p> <pre><code>matrix {\n    axes { }\n    excludes {}\n    agent {}\n    stages {}\n}\n</code></pre>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#axis","title":"Axis","text":"<p>The mechanism for specifying the sets of values is called <code>axis</code>.</p> <p>You start by specifying the matrix build within a stage.</p> <pre><code>pipeline {\n    agent none\n    stages {\n        stage('matrix build') {\n            matrix { } \n        }\n    }\n}\n</code></pre> <p>Within <code>matrix</code> you specify the <code>axis</code> with one or more <code>axis</code>.</p> <pre><code>matrix {\n    axes {\n        axis {\n            name 'JDK_VERSION'\n            values '8','11', '13'\n        }\n        axis {\n            name 'JDK_TYPE'\n            values 'ibmjava','amazoncorretto', 'jdk'\n        }\n    }\n}\n</code></pre> <p>In this case, we will get these nine combinations:</p> <ul> <li>jdk + 8, jdk + 11, jdk + 13</li> <li>ibmjava + 8, ibmjava + 11, ibmjava + 13</li> <li>amazoncorretto + 8, amazoncorretto + 11, amazoncorretto + 13</li> </ul>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#exclusions","title":"Exclusions","text":"<p>Unfortunately, several of these are not valid combinations.  The <code>ibmjava</code> only has a JDK 8 version, and the default (open)JDK is the only one with a JDK 13 version (at this time).</p> <p>You can filter these out with a <code>when</code> directive, but that would pollute the pipeline view. Wouldn't it be better to ensure the invalid combinations aren't even considered?</p> <p>You can tell the <code>matrix</code> build feature just that, by setting <code>exclusions</code>.</p> <p>You start with an <code>exclusions {}</code> followed by one or more <code>exclusion{}</code> blocks.</p> <p>Let's exclude JDK 11 from IBM's java.</p> <pre><code>excludes {\n    exclude {\n        axis {\n            name 'JDK_VERSION'\n            values '11'\n        }\n        axis {\n            name 'JDK_TYPE'\n            values \"ibmjava\"\n        }\n    }\n}\n</code></pre> <p>Sometimes, you want to limit a combination to a single <code>axis</code>.  Meaning, exclude every combination of <code>x</code>, except for <code>y</code>.</p> <p>We can do just that, by specifying <code>notValues</code> in the exclusion. Let's ensure only <code>jdk</code> will be paired with <code>13</code>.</p> <pre><code>exclude {\n    axis {\n        name 'JDK_VERSION'\n        values '13'\n    }\n    axis {\n        name 'JDK_TYPE'\n        notValues 'jdk' // double negative, make sure we only do 13 with jdk\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#together-with-podtemplates","title":"Together With PodTemplates","text":"<p>As with previous examples of the Parallel and Sequential features, <code>Matrix</code> adds an extra dimension to using PodTemplates.</p> <p>Remember the structure of a <code>Matrix</code>?</p> <pre><code>matrix {\n    axes { }\n    excludes {}\n    agent {}\n    stages {}\n}\n</code></pre> <p>It means we can structure of the <code>Sequential</code> stages, by adding a <code>stages {}</code> block and a single <code>agent{}</code> block for those stages.</p> <p>Because the PodTemplate properties are variables, the values we put in there can be dynamic. This means we can leverage the <code>axis</code> values as input for which container images to use for our PodTemplate.</p> <pre><code>matrix {\n    axes {}\n    agent {\n        kubernetes {\n            label \"maven-${JDK_TYPE}-${JDK_VERSION}-test\"\n            containerTemplate {\n                name 'maven'\n                image \"maven:3-${JDK_TYPE}-${JDK_VERSION}\"\n                ttyEnabled true\n                command 'cat'\n            }\n        }\n    }\n    stages {}\n}\n</code></pre>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#visual-representation","title":"Visual Representation","text":"<p>In the visual representation, we can the benefits of the <code>Sequential</code> stages with Matrix. Each <code>axis</code> combination will have its own set of stages, which can also have their own <code>when{}</code> directives.</p> <p></p>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#complete-example","title":"Complete Example","text":"<pre><code>pipeline {\n    agent none\n    stages {\n        stage('Prepare') {\n            agent {\n                kubernetes {\n                    label \"maven\"\n                    containerTemplate {\n                        name 'maven'\n                        image \"maven\"\n                        ttyEnabled true\n                        command 'cat'\n                    }\n                }\n            }\n            steps {\n                println 'hello world!'\n                container('maven') {\n                    sh 'env'\n                }\n            }\n        }\n        stage('Test') {\n            matrix {\n                axes {\n                    axis {\n                        name 'JDK_VERSION'\n                        values '8','11', '13'\n                    }\n                    axis {\n                        name 'JDK_TYPE'\n                        values 'ibmjava','amazoncorretto', 'jdk'\n                    }\n                }\n                excludes {\n                    exclude {\n                        axis {\n                            name 'JDK_VERSION'\n                            values '13'\n                        }\n                        axis {\n                            name 'JDK_TYPE'\n                            notValues 'jdk' // double negative, make sure we only do 13 with jdk\n                        }\n                    }\n                    exclude {\n                        axis {\n                            name 'JDK_VERSION'\n                            values '11'\n                        }\n                        axis {\n                            name 'JDK_TYPE'\n                            values \"ibmjava\"\n                        }\n                    }\n                }\n                agent {\n                    kubernetes {\n                        label \"maven-${JDK_TYPE}-${JDK_VERSION}-test\"\n                        containerTemplate {\n                            name 'maven'\n                            image \"maven:3-${JDK_TYPE}-${JDK_VERSION}\"\n                            ttyEnabled true\n                            command 'cat'\n                        }\n                    }\n                }\n                stages {\n                    stage('Test Image') {\n                        steps {\n                            println \"Using Image: maven:3-${JDK_TYPE}-${JDK_VERSION}\"\n                        }\n                    }\n                    stage(\"Build\") {\n                        steps {\n                            sh 'uname -a'\n                            git 'https://github.com/joostvdg/jx-maven-lib.git'\n                            container('maven') {\n                                sh 'mvn clean verify --show-version --strict-checksums -e'\n                            }\n                        }\n                    }\n                    stage(\"IT Test\") {\n                        when {\n                            branch 'master'\n                        }\n                        steps {\n                            echo \"WE SHOULD NEVER GET HERE\"\n                        }\n                    }\n                }\n            }\n        }\n        stage('Publish') {\n            agent {\n                kubernetes {\n                    label \"maven\"\n                    containerTemplate {\n                        name 'maven'\n                        image \"maven\"\n                        ttyEnabled true\n                        command 'cat'\n                    }\n                }\n            }\n            steps {\n                println 'hello world!'\n                container('maven') {\n                    sh 'env'\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/jenkins-parallel-pipeline/#references","title":"References","text":"<ol> <li> <p>Jenkins Pipeline - Parallel Stages \u21a9</p> </li> <li> <p>Jenkins Pipeline - Introducing Sequential Stages \u21a9</p> </li> <li> <p>XKCD - Code's Compiling \u21a9</p> </li> <li> <p>Jenkins Pipeline Syntax \u21a9</p> </li> <li> <p>Jenkins Declarative Pipeline - When Directive \u21a9</p> </li> <li> <p>Jenkins Pipeline - Step Stash \u21a9</p> </li> <li> <p>Jenkins Pipeline - Step Unstash \u21a9</p> </li> <li> <p>Kubernetes NFS Storage Provisioner \u21a9\u21a9</p> </li> <li> <p>Jenkins External Workspace Manager Plugin \u21a9</p> </li> <li> <p>Jenkins Kubernetes Plugin - template values explained \u21a9</p> </li> <li> <p>Jenkins Blog Introducing Matrix Build Feature \u21a9</p> </li> <li> <p>GitHub Repository containing test examples of the Matrix Build Feature \u21a9</p> </li> <li> <p>Wikipedia - Cartesian Product \u21a9</p> </li> </ol>"},{"location":"jenkins-pipeline/job-types/","title":"Jenkins Pipeline Job Types","text":"<p>Jenkins Pipelines are written as a <code>Jenkinsfile</code> and can have either a <code>Declarative</code><sup>1</sup> or <code>Scripted</code><sup>2</sup> format. Whenever possible for as much as possible, always write your pipelines in <code>Declarative</code> format.</p> <p>How they are configured and run is a classic case of <code>it depends...</code>. This page goes into the job types available and when to use which and why.</p> <p>Caution</p> <p>This piece is full of personal opinions. While based on years of experience with writing Jenkins Pipelines and helping customers and clients with reviewing theirs, they're still just an opinion.</p>"},{"location":"jenkins-pipeline/job-types/#job-types","title":"Job Types","text":"<p>We classify the job types as follows:</p> <ul> <li>Something that isn't a pipeline job and you should never ever use (I'm looking at you <code>Freestyle</code>)</li> <li>Pipeline</li> <li>Multibranch Pipeline</li> <li>Organization Job (named after the most commonly used GitHub Organization Job)</li> </ul>"},{"location":"jenkins-pipeline/job-types/#pipeline","title":"Pipeline","text":"<p>A <code>Pipeline</code> job<sup>9</sup> is the most fundamental building block available. You can create a <code>New Item</code> in Jenkins and choose the type <code>Pipeline</code>.</p> <p>Here you can choose two different configurations, either you write the Pipeline inline or <code>from SCM</code>. Inline means that you write the Pipeline directly in the Job configuration itself. Where <code>from SCM</code> let's check out a <code>Jenkinsfile</code> from an SCM (such as GitHub).</p> <p>Aside from testing Snippets, you should never use this Job type. It is very limited in what it offers beyond running a Pipeline.</p> <p>Tip</p> <p>I use this Job type for testing sections of a more extensive Pipeline or new features. Once it works, I will commit it to an SCM source and use one of the other Job types.</p>"},{"location":"jenkins-pipeline/job-types/#multibranch-pipeline","title":"Multibranch Pipeline","text":"<p>Now, this is starting to look like something we can use!</p> <p>The Multibranch Pipeline<sup>3</sup> allows you to map a Pipeline to an SCM Repository. Where a Pipeline Job maps to a specific Branch (or Branch scheme) of a Repository, a Multibranch Pipeline Job targets the entire repository.</p> <p>This means that it scans the repository for its Branches. For each branch that meets the criteria you set, it creates a Pipeline Job targeting this specific branch.</p> <p>By default, the only criteria configured is the existence of a file called <code>Jenkinsfile</code>. However, you can add other criteria such as a Branch Naming Scheme - either based on Regex or Wildcards.</p>"},{"location":"jenkins-pipeline/job-types/#benefits-beyond-pipeline-job","title":"Benefits Beyond Pipeline Job","text":"<p>Aside from not having to manage Job configurations for branches, as Multibranch handles both creation and deletion, this job type has additional benefits. It automatically populates environment variables such as <code>BRANCH_NAME</code> and many other related to the Git Commit. This allows you to take full leverage of the <code>When</code><sup>10</sup> Directive.</p> <pre><code>pipeline {\n    stages {\n        stage('Only Run If Master Branch') {\n            when { branch 'master' }\n            steps {\n                echo 'This only runs iff we are in the master branch'\n            }\n        }\n    }\n}\n</code></pre> <p>To leverage the Multibranch Pipeline well, I recommend going through this excellent end-to-end tutorial<sup>4</sup>.</p>"},{"location":"jenkins-pipeline/job-types/#branch-sources","title":"Branch Sources","text":"<p>The Multibranch Pipeline Job lets you configure to wich SCM Repository it should map to. This can be merely pointing to a Git or SVN (please don't) repository, but more importantly, you can map it to specific <code>Branch Sources</code>.</p> <p>What are <code>Branch Sources</code>? These are specific Git providers that 1) give Jenkins more information, and 2) have their own unique handler plugins providing better integration with the Git provider.</p> <p>In most cases, this means you can also build Pull Requests and deal with Tags in a way that leverages Jenkins Pipeline Directives such as the already mentioned <code>When</code><sup>10</sup> directive.</p> <p>As of this writing - August 2019 - there are four such Branch Source plugins available; GitHub<sup>5</sup> (most mature), GitLab<sup>8</sup> (latest addition), Bitbucket<sup>6</sup>, and Gitea<sup>7</sup>.</p>"},{"location":"jenkins-pipeline/job-types/#organization-job","title":"Organization Job","text":"<p>I name these types of jobs after the first one that worked very well: GitHub Organization Job. This plugin is replaced by <code>GitHub Branch Source</code>, now the common name for the plugins providing this kind of integration.</p> <p>The goal of Organization Pipeline Job is to scan an entire namespace of Git repositories for branches containing <code>Jenkinsfile</code>. In the case of GitHub, this is called an <code>Organization</code>, in the case of Bitbucket cloud, this is a <code>Team</code> and so forth. Creating several benefits, the essential being, you create a single job within Jenkins and manage the Pipeline for every application in that Organization/Team/Whatever.</p> <p>Moreover, don't worry, there are ample settings to configure if you want to limit the job to specific branches or repositories.</p>"},{"location":"jenkins-pipeline/job-types/#benefits","title":"Benefits","text":"<ul> <li>reduced maintenance: a single job maintained in Jenkins takes care of all applications in an Organization/Team/... including creation and deletion of the repositories and branches!</li> <li>reduced complexity: due to the configuration working as a standardization, there are no special snowflakes, there's just one way a job gets configured, and that includes the naming</li> <li>visible coupling: a Pipeline is coupled to a repository, the job clearly shows this connection with links and icons</li> <li>manages status reporting: although this does depend on the specific integration, in general, every Pipeline automatically reports the build status of each commit to your SCM without requiring configuration</li> </ul>"},{"location":"jenkins-pipeline/job-types/#models","title":"Models","text":"<p>What people often fail to realize, is that the mentioned Pipeline Job Types are not on their own. They are part of a single model, where they build on top of each other. The <code>Pipeline</code> job being the smallest building block, then <code>Multibranch</code>, and last but not least, <code>Organization</code> job extends <code>Multibranch</code>.</p> <p>I've created some models to try and visualize this.</p>"},{"location":"jenkins-pipeline/job-types/#abstract","title":"Abstract","text":"<p>At the very abstract level, we see this babushka doll type of layering.</p> <p></p>"},{"location":"jenkins-pipeline/job-types/#basic","title":"Basic","text":"<p>Starting from the outer layer, we have a <code>GitHub Organization</code> job. In essence, it is a folder that contains <code>Multbranch Pipeline</code> jobs. A <code>Multibranch Pipeline</code> job is also a folder, and it contains <code>Pipeline</code> jobs.</p> <p></p>"},{"location":"jenkins-pipeline/job-types/#mapping","title":"Mapping","text":"<p>This layering is created by mapping the different types of jobs to corresponding resources in the SCM you are talking to. In the example of a <code>GitHub Organization</code> job, we're talking to a GitHub Organization. Jenkins scans each repository in that Organization, and per repository, scan each branch.  For each branch that meets the criteria - by default the presence of a <code>Jenkinsfile</code> - it creates the <code>Multibranch Pipeline</code> job for that repository.</p> <p>The <code>Multibranch Pipeline</code> job creates a <code>Pipeline</code> job for each branch that met the criteria.</p> <p>So as summary:</p> <ul> <li>GitHub Organization Job -&gt; GitHub Organization / Bitbucket Team / ...</li> <li>Multibranch Pipeline Job -&gt; Repository</li> <li>Pipeline Job -&gt; Branch</li> </ul> <p></p>"},{"location":"jenkins-pipeline/job-types/#references","title":"References","text":"<ol> <li> <p>Jenkins Declarative Pipeline Fundamentals \u21a9</p> </li> <li> <p>Jenkins Scripted Pipeline Fundamentals \u21a9</p> </li> <li> <p>Jenkins Pipeline Multibranch Plugin \u21a9</p> </li> <li> <p>Tutorial On Creating a Multibranch Pipeline \u21a9</p> </li> <li> <p>GitHub Branch Source Plugin \u21a9</p> </li> <li> <p>Bitbucket Branch Source Plugin \u21a9</p> </li> <li> <p>Integrate Jenkins And Gitea \u21a9</p> </li> <li> <p>GitLab Branch Source Plugin \u21a9</p> </li> <li> <p>Create Pipeline Job Via UI \u21a9</p> </li> <li> <p>Pipeline Syntax - When Directive \u21a9\u21a9</p> </li> </ol>"},{"location":"jenkins-pipeline/kaniko-pipelines/","title":"Kaniko Pipelines","text":"<p>Kaniko<sup>1</sup> is one of the recommended tools for building Docker images within Kubernetes, especially when you build them as part of a Jenkins Pipeline. I've written about why you should use Kaniko(or similar) tools, the rest assumes you want to use Kaniko within your pipeline.</p> <p>Quote</p> <p>Kaniko is a tool to build container images from a Dockerfile, inside a container or Kubernetes cluster.</p> <p>kaniko doesn't depend on a Docker daemon and executes each command within a Dockerfile completely in userspace. This enables building container images in environments that can't easily or securely run a Docker daemon, such as a standard Kubernetes cluster.</p> <p>For more examples for leveraging Kaniko when using Jenkins in Kubernetes, you can look at the documentation from CloudBees Core<sup>2</sup>.</p>"},{"location":"jenkins-pipeline/kaniko-pipelines/#pipeline-example","title":"Pipeline Example","text":"<p>Note</p> <p>The Kaniko logger uses ANSI Colors, which can be represented via the Jenkins ANSI Color Plugin.</p> <p>If you have the plugin installed, you can do something like the snipped below to render the colors.</p> <pre><code>container(name: 'kaniko', shell: '/busybox/sh') {\n    ansiColor('xterm') {\n        sh '''#!/busybox/sh\n        /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=${REGISTRY}/${REPOSITORY}/${IMAGE}\n        '''\n    }\n}\n</code></pre> <pre><code>pipeline {\n    agent {\n        kubernetes {\n            label 'kaniko'\n            yaml \"\"\"\nkind: Pod\nmetadata:\n  name: kaniko\nspec:\n  containers:\n  - name: golang\n    image: golang:1.12\n    command:\n    - cat\n    tty: true\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor:debug\n    imagePullPolicy: Always\n    command:\n    - /busybox/cat\n    tty: true\n    volumeMounts:\n      - name: jenkins-docker-cfg\n        mountPath: /kaniko/.docker\n  volumes:\n  - name: jenkins-docker-cfg\n    projected:\n      sources:\n      - secret:\n          name: registry-credentials\n          items:\n            - key: .dockerconfigjson\n              path: config.json\n\"\"\"\n        }\n    }\n    stages {\n        stage('Checkout') {\n            steps {\n                git 'https://github.com/joostvdg/cat.git'\n            }\n        }\n        stage('Build') {\n            steps {\n                container('golang') {\n                    sh './build-go-bin.sh'\n                }\n            }\n        }\n        stage('Make Image') {\n            environment {\n                PATH        = \"/busybox:$PATH\"\n                REGISTRY    = 'index.docker.io' // Configure your own registry\n                REPOSITORY  = 'caladreas'\n                IMAGE       = 'cat'\n            }\n            steps {\n                container(name: 'kaniko', shell: '/busybox/sh') {\n                    sh '''#!/busybox/sh\n                    /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=${REGISTRY}/${REPOSITORY}/${IMAGE}\n                    '''\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/kaniko-pipelines/#configuration","title":"Configuration","text":"<p>Kaniko relies on a docker secret for directly communicating to a Docker Registry. This can be supplied in various ways, but the most common is to create a Kubernetes Secret of type <code>docker-registry</code>.</p> <pre><code>kubectl create secret docker-registry registry-credentials \\  \n    --docker-username=&lt;username&gt;  \\\n    --docker-password=&lt;password&gt; \\\n    --docker-email=&lt;email-address&gt;\n</code></pre> <p>We can then mount it in the pod via <code>Volumes</code> (PodSpec level) and <code>volumeMounts</code> (Container level).</p> <pre><code>  volumes:\n  - name: jenkins-docker-cfg\n    projected:\n      sources:\n      - secret:\n          name: docker-credentials\n          items:\n            - key: .dockerconfigjson\n              path: config.json\n</code></pre>"},{"location":"jenkins-pipeline/kaniko-pipelines/#azure-acr","title":"Azure &amp; ACR","text":"<p>Of course, there always have to be difference between the Public Cloud Providers (AWS, Azure, Alibaba, GCP).</p> <p>In the case of Kaniko, its Azure that does things differently.</p> <p>Assuming you want to leverage Azure Container Registry (ACR), you're in Azure after all, you will have to do a few things differently.</p>"},{"location":"jenkins-pipeline/kaniko-pipelines/#create-acr","title":"Create ACR","text":"<p>You can use the Azure CLI<sup>5</sup> or an Configuration-As-Code Tool such as Terraform<sup>6</sup>.</p>"},{"location":"jenkins-pipeline/kaniko-pipelines/#azure-cli","title":"Azure CLI","text":"<p>First, create a resource group.</p> <pre><code>az group create --name myResourceGroup --location eastus\n</code></pre> <p>And then create the ACR.</p> <pre><code>az acr create --resource-group myResourceGroup --name myContainerRegistry007 --sku Basic\n</code></pre>"},{"location":"jenkins-pipeline/kaniko-pipelines/#terraform","title":"Terraform","text":"<p>We leverage the <code>azurerm</code> backend of terraform<sup>9</sup><sup>10</sup>.</p> <pre><code>resource \"azurerm_resource_group\" \"acr\" {\n    name     = \"${var.resource_group_name}-acr\"\n    location = \"${var.location}\"\n}\n\nresource \"azurerm_container_registry\" \"acr\" {\n    name                     = \"${var.container_registry_name}\"\n    resource_group_name      = \"${azurerm_resource_group.acr.name}\"\n    location                 = \"${azurerm_resource_group.k8s.location}\"\n    sku                      = \"Premium\"\n    admin_enabled            = false\n}\n</code></pre>"},{"location":"jenkins-pipeline/kaniko-pipelines/#configure-access-to-acr","title":"Configure Access to ACR","text":"<p>Now that we have an ACR, we need to be able to pull and images from and to the registry.</p> <p>This requires access credentials, which we can create in several ways, we'll explore via ServicePrinciple.</p>"},{"location":"jenkins-pipeline/kaniko-pipelines/#via-serviceprinciple-credentials","title":"Via ServicePrinciple Credentials","text":"<p>The commands below are taken from the Azure Container Registry documentation about authentication<sup>7</sup>.</p> <p>First, lets setup some values that are not derived from something.</p> <pre><code>EMAIL=me@example.com\nSERVICE_PRINCIPAL_NAME=acr-service-principal\nACR_NAME=myacrinstance\n</code></pre> <p>Second, we fetch the basic information about the registry we have. We need this information for the other commands.</p> <pre><code>ACR_LOGIN_SERVER=$(az acr show --name $ACR_NAME --query loginServer --output tsv)\nACR_REGISTRY_ID=$(az acr show --name $ACR_NAME --query id --output tsv)\n</code></pre> <p>Now we can create a ServicePrinciple with just the rights we need<sup>8</sup>.  In the case of Kaniko, we need Push and Pull rights, which are both captured in the role <code>acrpush</code>.</p> <pre><code>SP_PASSWD=$(az ad sp create-for-rbac --name http://$SERVICE_PRINCIPAL_NAME --role acrpush --scopes $ACR_REGISTRY_ID --query password --output tsv)\nCLIENT_ID=$(az ad sp show --id http://$SERVICE_PRINCIPAL_NAME --query appId --output tsv)\n</code></pre> <pre><code>kubectl create secret docker-registry registry-credentials --docker-server ${ACR_LOGIN_SERVER} --docker-username ${CLIENT_ID} --docker-password ${SP_PASSWD} --docker-email ${EMAIL}\n</code></pre>"},{"location":"jenkins-pipeline/kaniko-pipelines/#references","title":"References","text":"<ol> <li> <p>Kaniko GitHub \u21a9</p> </li> <li> <p>CloudBees Guide On Using Kaniko With CloudBees Core \u21a9</p> </li> <li> <p>Sail CI On Kaniko With Azure Container Registry \u21a9</p> </li> <li> <p>Create Azure Container Registry With Azure CLI \u21a9</p> </li> <li> <p>Azure CLI \u21a9</p> </li> <li> <p>Terraform \u21a9</p> </li> <li> <p>Azure Container Registry Authentication Documentation \u21a9</p> </li> <li> <p>Azure Container Registry Roles and Permissions \u21a9</p> </li> <li> <p>Terraform AzureRM Backend \u21a9</p> </li> <li> <p>Create AKS Cluster Via Terraform \u21a9</p> </li> <li> <p>Azure Container Registry Credentials Management \u21a9</p> </li> </ol>"},{"location":"jenkins-pipeline/podtemplate-dind/","title":"Docker-in-Docker With PodTemplates","text":"<p>First of all, doing Docker-In-Docker is a controversial practice to begin with<sup>1</sup>, creating just as many problems as it solves.</p> <p>Second, if you're in Kubernetes, one should not use Docker directly.</p> <p>Ok, all caveats aside, there can be legitimate reasons for wanting to run Docker containers directly even in Kubernetes.</p>"},{"location":"jenkins-pipeline/podtemplate-dind/#scenario-pipeline-framework-based-on-docker","title":"Scenario: Pipeline Framework Based On Docker","text":"<p>I created the solution in this article for a client. This client has created a internal framework around Jenkins Pipeline<sup>3</sup>.</p> <p>This pipeline processes the requirements off the pipeline run and spins up the appropriate containers in parallel. Running parallel container can also be done with PodTemplates via the Kubernetes Plugin, which was deemed the way forward.</p> <p>However, as one can expect, you do not rewrite a framework used by dozens of applications unless you're reasonably sure you can get everything to run as it should. In order to bridge this period of running the pipelines as is, while evaluating Jenkins on Kubernetes, we got to work on achieving Docker-in-Docker with Jenkins Kubernetes Plugin<sup>4</sup>.</p>"},{"location":"jenkins-pipeline/podtemplate-dind/#goal","title":"Goal","text":"<p>The goal of the exercise, is to prove we can do a single git clone, and spin up multiple containers re-using the original namespace.</p>"},{"location":"jenkins-pipeline/podtemplate-dind/#docker-socket","title":"Docker Socket","text":"<p>There's many ways to spin up Docker containers, and I would say the most common one is know as Docker-in-Docker or <code>dind</code>.</p> <p>I find this misleading, because most of the time the second container doesn't run inside the original container, but parallel to it. This is often achieved by mounting the docker socket as a volume - <code>/var/run/docker.sock</code> . I call this: <code>Docker-On-Docker</code>. Because I want to stay close to the term Docker-in-Docker, but also signify it is different.</p> <p>What Docker-In-Docker should be, is that we host a docker daemon in a docker container, which then hosts the new containers in itself.</p> <p>Before we go into the differences, let's discuss why it matters. Remember, the goal is to be able to re-use the git clone in the original container. The default container with Jenkins Kubernetes Plugin is the jnpl, which acts as an ephemeral agent. It mounts a <code>emptyDir{}</code> as temporary volume as its workspace, which means this folder only exists within the Pod (practically speaking).</p>"},{"location":"jenkins-pipeline/podtemplate-dind/#docker-on-docker","title":"Docker-On-Docker","text":"<p>Classic Docker-On-Docker, we have a VM which has a Docker Daemon. We have a Docker client container which mounts the docker socket. The docker client now pretends to talk to a local Docker Daemon, but this redirects to the VM's daemon instead.</p> <p>When we do a <code>docker run</code>, the daemon will spawn a new container next to us. While this is nice, and stays clear from virtulization inception, this new container - let's say, a <code>maven</code>container - cannot access our workspace. Any volume flag we give to our container with <code>-v {origin}:{target}</code> maps a Host directory to our new container, but not the workspace volume (purple) in our Pod.</p> <p>This is because the new container cannot access any volume inside the Pod. For this, we need true Docker-In-Docker.</p> <p></p>"},{"location":"jenkins-pipeline/podtemplate-dind/#jenkinsfile-example","title":"Jenkinsfile Example","text":"<pre><code>podTemplate(yaml: \"\"\"\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:1.11\n    command: ['cat']\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n\"\"\"\n  ) {\n\n  def image = \"jenkins/jnlp-slave\"\n  node(POD_LABEL) {\n    stage('Build Docker image') {\n      git 'https://github.com/jenkinsci/docker-jnlp-slave.git'\n      container('docker') {\n        sh \"docker build -t ${image} .\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"jenkins-pipeline/podtemplate-dind/#docker-in-docker","title":"Docker-In-Docker","text":"<p>True Docker-In-Docker means spinning up another container inside an existing container - as illustrated below.</p> <p>This has a big benefit. The container now has access to all the volumes inside the Pod. We can now do a git clone and use docker build on this workspace. In addition, we can also spin up a new container and give it the workspace as a volume and have it build with it.</p> <p></p>"},{"location":"jenkins-pipeline/podtemplate-dind/#pod-configuration","title":"Pod Configuration","text":"<p>The examples below contain some specific configuration elements required to make the magic happen. So let's explore each configuration item to see what it does and why we set it.</p>"},{"location":"jenkins-pipeline/podtemplate-dind/#client-container","title":"Client Container","text":"<p>The Docker Client has to know where the Docker Daemon is.</p> <p>We do this by setting the environment variable <code>DOCKER_HOST</code><sup>2</sup> to <code>tcp://localhost:2375</code>. If we put the Daemon container within the same Pod, we can use <code>localhost</code> and then the default port of the daemon, being <code>2375</code>.</p> <pre><code>env:\n  - name: DOCKER_HOST\n    value: tcp://localhost:2375\n</code></pre> <p>The client container will directly terminate unless we give it something to do. If we put it to sleep for a significant amount of time, it should be there to execute our every command!</p> <pre><code>command: ['sleep', '99d']\n</code></pre>"},{"location":"jenkins-pipeline/podtemplate-dind/#daemon-container","title":"Daemon Container","text":"<p>As of Docker 18.09 the Docker Daemon container can use TLS, as of 19.03 it is configured by default.<sup>2</sup> So either you work around this or get the certificates sorted.</p> <p>The easiest way around it, is to set the environment variable <code>DOCKER_TLS_CERTDIR</code> to <code>\"\"</code>, which will disable TLS.</p> <p>Caution</p> <p>Disabling TLS is at your OWN risk. Read the docs carefully for what this means<sup>2</sup>.</p> <pre><code>env:\n   - name: DOCKER_TLS_CERTDIR\n     value: \"\"\n</code></pre> <p>In order for the Docker Daemon to create containers, it needs the <code>privileged</code> flag set to true. This should be another warning to you, to be careful of what you do with it!</p> <pre><code>securityContext:\n  privileged: true\n</code></pre> <p>Last but not least, we would not want to store all the Docker Daemon data in the pods. We might as well leverage local storage or a some specific volume.</p> <p>The <code>volume</code> and <code>volumeMounts</code> configuration of the Daemon container ensures we can leverage the Docker build and image caches.</p> <pre><code>  volumeMounts:\n    - name: cache\n      mountPath: /var/lib/docker\nvolumes:\n  - name: cache\n    hostPath:\n      path: /tmp\n      type: Directory\n</code></pre> Docker Build <p>Now that we can directly use Docker to build, we can also leverage <code>buildkit</code><sup>7</sup>.</p> <p>Because we have the workspace available to us, we can directly start our docker build.</p> <pre><code>pipeline {\n    options {\n        disableConcurrentBuilds()\n    }\n    agent {\n        kubernetes {\n            label 'docker-in-docker-maven'\n            yaml \"\"\"\napiVersion: v1\nkind: Pod\nspec:\ncontainers:\n- name: docker-client\n  image: docker:19.03.1\n  command: ['sleep', '99d']\n  env:\n    - name: DOCKER_HOST\n      value: tcp://localhost:2375\n- name: docker-daemon\n  image: docker:19.03.1-dind\n  env:\n    - name: DOCKER_TLS_CERTDIR\n      value: \"\"\n  securityContext:\n    privileged: true\n  volumeMounts:\n      - name: cache\n        mountPath: /var/lib/docker\nvolumes:\n  - name: cache\n    hostPath:\n      path: /tmp\n      type: Directory\n\"\"\"\n        }\n    }\n    stages {\n        stage('Checkout') {\n            steps {\n                git 'https://github.com/jenkinsci/docker-jnlp-slave.git'\n            }\n        }\n        stage('Docker Build') {\n            steps {\n                container('docker-client') {\n                    sh 'docker version &amp;&amp; DOCKER_BUILDKIT=1 docker build --progress plain -t testing .'\n                }\n            }\n        }\n    }\n}\n</code></pre> Docker Run With Workspace <p>In this example we're going to spin up another container to run our build with maven.</p> <p>This means it needs our workspace - see below - but it will also download a lot of Maven depencencies. We want to make sure we can leverage a local Maven Repository in order to speed up builds.</p> <p>We can do so by mounting a volume - or a <code>hostPath</code> to our Docker Client container.</p> <pre><code>volumeMounts:\n  - name: cache\n  mountPath: /tmp/repository\n</code></pre> <p>We can then mount this into our new container via the Docker volume flag (<code>-v</code>).</p> <pre><code>-v /tmp/repository:/root/.m2/repository\n</code></pre> <p>Here we mount the workspace from our Jenkins Build.  Notice the double quotes - in the pipeline below - this means we interpolate our Jenkins Environment variables.</p> <pre><code>-v ${WORKSPACE}:/usr/src/mymaven \n</code></pre> <p>The <code>-w</code> flag means work directory, it makes sure our container works in the directory container our workspace.</p> <pre><code>-w /usr/src/mymaven\n</code></pre> <pre><code>pipeline {\n    options {\n        disableConcurrentBuilds()\n    }\n    agent {\n        kubernetes {\n            label 'docker-in-docker-maven'\n            yaml \"\"\"\napiVersion: v1\nkind: Pod\nspec:\ncontainers:\n- name: docker-client\n  image: docker:19.03.1\n  command: ['sleep', '99d']\n  env:\n    - name: DOCKER_HOST\n      value: tcp://localhost:2375\n  volumeMounts:\n    - name: cache\n      mountPath: /tmp/repository\n- name: docker-daemon\n  image: docker:19.03.1-dind\n  env:\n    - name: DOCKER_TLS_CERTDIR\n      value: \"\"\n  securityContext:\n    privileged: true\n  volumeMounts:\n    - name: cache\n      mountPath: /var/lib/docker\nvolumes:\n  - name: cache\n    hostPath:\n      path: /tmp\n      type: Directory\n\"\"\"\n        }\n    }\n    stages {\n        stage('Checkout') {\n            steps {\n                git 'https://github.com/joostvdg/jx-maven-lib.git'\n            }\n        }\n        stage('Build') {\n            steps {\n                container('docker-client') {\n                    sh \"docker run -v ${WORKSPACE}:/usr/src/mymaven -v /tmp/repository:/root/.m2/repository -w /usr/src/mymaven maven:3-jdk-11-slim mvn clean verify\"\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline/podtemplate-dind/#references","title":"References","text":"<ol> <li> <p>Using Docker-in-Docker - Jerome Petazzo \u21a9</p> </li> <li> <p>Docker images on Dockerhub \u21a9\u21a9\u21a9</p> </li> <li> <p>Jenkins Pipeline \u21a9</p> </li> <li> <p>Jenkins Kubernetes Plugin \u21a9</p> </li> <li> <p>Jenkins Kubernetes Plugin - Docker In Docker Example \u21a9</p> </li> <li> <p>Jenkins Kubernetes Plugin - Docker On Docker Example \u21a9</p> </li> <li> <p>Docker Build Enchancements - Buildkit \u21a9</p> </li> </ol>"},{"location":"jenkins-pipeline-examples/","title":"Jenkins Pipeline Examples","text":"<p>Please mind that these examples all assume the following:</p> <ul> <li>you have Jenkins 2.32+</li> <li>you have a recent set of pipeline plugins<ul> <li>Jenkins Pipeline Model</li> <li>Jenkins Blue Ocean</li> <li>Jenkins Pipeline Maven</li> <li>Build timeout plugin</li> <li>Credentials Binding</li> <li>Credentials</li> <li>Pipeline Multi-Branch</li> <li>SonarQube</li> <li>Timestamper</li> <li>Pipeline Supporting APIs</li> <li>Pipeline Shared Groovy Libraries</li> </ul> </li> </ul>"},{"location":"jenkins-pipeline-examples/docker-alternatives/","title":"Pipelines With Docker Alternatives","text":"<p>Building pipelines with Jenkins on Docker has been common for a while.</p> <p>But accessing the docker socket has always been a bit tricky. The easiest solution is directly mounting the docker socket of the host into your build container.</p> <p>However, this is a big security risk and something that is undesirable at best and, pretty dangerous at worst.</p> <p>When you're using an orchestrator such as Kubernetes - which is where Jenkins is currently best put to work - the problem gets worse. Not only do you open up security holes, you also mess up the schedular's insight into available resources, and mess with it's ability to keep your cluster in a correct state.</p> <p>In some sense, using docker directly in a Kubernetes cluster, is likened to running with scissors.</p>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#potential-alternatives","title":"Potential Alternatives","text":"<p>So, directly running docker containers via a docker engine inside your cluster is out. Let's look at some alternatives.</p> <ul> <li>Kubernetes Pod and External Node: the simple way out, use a cloud (such as EC2 Cloud) to provision a classic VM with docker engine as an agent and build there</li> <li>JIB: tool from Google, only works for Java applications and is supported directly from Gradle and Maven - Link</li> <li>Kaniko: tool from Google, works as 1-on-1 replacement for docker image build (except Windows Containers) - Link</li> <li>IMG: tool from Jess Frazelle to avoid building docker images with a root user involved Link</li> </ul>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#kubernetes-pod-and-external-node","title":"Kubernetes Pod and External Node","text":"<p>One of the most used cloud environments is AWS, so I created this solution with AWS's Amazon EC2 Plugin.</p> <p>Warning</p> <p>Unfortunately, you cannot combine the Kubernetes plugin with external nodes (none pod container nodes) in a <code>Declarative</code> pipeline. So you have to use <code>Scripted</code>.</p> <p>This can be done with various different cloud providers such as Digital Ocean, Google, AWS or Azure. This guide will use AWS, as it has the most mature <code>Jenkins Cloud</code> plugin.</p>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS Account with rights to create AMI's and run EC2 instances</li> <li>Packer</li> <li>Jenkins with Amazon EC2 Plugin installed</li> </ul>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#steps","title":"Steps","text":"<ul> <li>create AMI with Packer</li> <li>install and configure Amazon EC2 plugin</li> <li>create a test pipeline</li> </ul>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#create-ami-with-packer","title":"Create AMI with Packer","text":"<p>Packer needs to be able to access EC2 API's and be able to spin up an EC2 instance and create an AMI out of it.</p>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#aws-setup-for-packer","title":"AWS setup for Packer","text":"<p>You need to configure two things:</p> <ul> <li>account details for Packer to use</li> <li>security group where your EC2 instances will be running with<ul> <li>this security group needs to open port <code>22</code></li> <li>both Packer and Jenkins will use this for their connection</li> </ul> </li> </ul> <pre><code>export AWS_DEFAULT_REGION=eu-west-1\nexport AWS_ACCESS_KEY_ID=XXX\nexport AWS_SECRET_ACCESS_KEY=XXX\n</code></pre> <pre><code>aws ec2 --profile myAwsProfile create-security-group \\\n    --description \"For building Docker images\" \\\n    --group-name docker\n\n{\n    \"GroupId\": \"sg-08079f78cXXXXXXX\"\n}\n</code></pre> <p>Export the security group ID.</p> <pre><code>export SG_ID=sg-08079f78cXXXXXXX\necho $SG_ID\n</code></pre>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#enable-port-22","title":"Enable port 22","text":"<pre><code>aws ec2 \\\n    --profile myAwsProfile \\\n    authorize-security-group-ingress \\\n    --group-name docker \\\n    --protocol tcp \\\n    --port 22 \\\n    --cidr 0.0.0.0/0\n</code></pre>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#packer-ami-definition","title":"Packer AMI definition","text":"<p>Here's an example definition for Packer for a Ubuntu 18.04 LTS base image with JDK 8 (required by Jenkins) and Docker.</p> <pre><code>{\n    \"builders\": [{\n        \"type\": \"amazon-ebs\",\n        \"region\": \"eu-west-1\",\n        \"source_ami_filter\": {\n            \"filters\": {\n                \"virtualization-type\": \"hvm\",\n                \"name\": \"*ubuntu-bionic-18.04-amd64-server-*\",\n                \"root-device-type\": \"ebs\"\n            },\n            \"owners\": [\"679593333241\"],\n            \"most_recent\": true\n        },\n        \"instance_type\": \"t2.micro\",\n        \"ssh_username\": \"ubuntu\",\n        \"ami_name\": \"docker\",\n        \"force_deregister\": true\n    }],\n    \"provisioners\": [{\n        \"type\": \"shell\",\n        \"inline\": [\n            \"sleep 15\",\n            \"sudo apt-get clean\",\n            \"sudo apt-get update\",\n            \"sudo apt-get install -y apt-transport-https ca-certificates nfs-common\",\n            \"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\",\n            \"sudo add-apt-repository \\\"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\\"\",\n            \"sudo add-apt-repository -y ppa:openjdk-r/ppa\",\n            \"sudo apt-get update\",\n            \"sudo apt-get install -y docker-ce\",\n            \"sudo usermod -aG docker ubuntu\",\n            \"sudo apt-get install -y openjdk-8-jdk\",\n            \"java -version\",\n            \"docker version\"\n        ]\n    }]\n}\n</code></pre> <p>Build the new AMI with packer.</p> <pre><code>packer build docker-ami.json\nexport AMI=ami-0212ab37f84e418f4\n</code></pre>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#ec2-key-pair","title":"EC2 Key Pair","text":"<p>Create EC2 key pair, this will be used by Jenkins to connect to the instances via ssh.</p> <pre><code>aws ec2 --profile myAwsProfile create-key-pair \\\n    --key-name jenkinsec2 \\\n    | jq -r '.KeyMaterial' \\\n    &gt;jenkins-ec2-proton.pem\n</code></pre>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#ec2-cloud-configuration","title":"EC2 Cloud Configuration","text":"<p>In a Jenkins' master main configuration, you add a new <code>cloud</code>. In this case, we will use a <code>ec2-cloud</code> so we can instantiate our EC2 VM's with docker.</p> <ul> <li>use EC2 credentials for initial connection</li> <li>use key (<code>.pem</code>) for VM connection (jenkins &lt;&gt; agent)</li> <li>configure the following:<ul> <li>AMI: ami-0212ab37f84e418f4</li> <li>availability zone: eu-west-1a</li> <li>VPC SubnetID: subnet-aa54XXXX</li> <li>Remote user: ubuntu</li> <li>labels: docker ubuntu linux</li> <li>SecurityGroup Name: (the id) sg-08079f78cXXXXXXX</li> <li>public ip = true</li> <li>connect via public ip = true</li> </ul> </li> </ul>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#pipeline","title":"Pipeline","text":"<pre><code>@Library('jenkins-pipeline-library@master') _\n\ndef scmVars\ndef label = \"jenkins-slave-${UUID.randomUUID().toString()}\"\n\npodTemplate(\n        label: label,\n        yaml: \"\"\"\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: kubectl\n    image: vfarcic/kubectl\n    command: [\"cat\"]\n    tty: true\n\"\"\"\n) {\n    node(label) {\n        node(\"docker\") {\n            stage('SCM &amp; Prepare') {\n                scmVars = checkout scm\n            }\n            stage('Lint') {\n                dockerfileLint()\n            }\n            stage('Build Docker') {\n                sh \"docker image build -t demo:rc-1 .\"\n            }\n            stage('Tag &amp; Push Docker') {\n                IMAGE = \"${DOCKER_IMAGE_NAME}\"\n                TAG = \"${DOCKER_IMAGE_TAG}\"\n                FULL_NAME = \"${FULL_IMAGE_NAME}\"\n\n                withCredentials([usernamePassword(credentialsId: \"dockerhub\", usernameVariable: \"USER\", passwordVariable: \"PASS\")]) {\n                    sh \"docker login -u $USER -p $PASS\"\n                }\n                sh \"docker image tag ${IMAGE}:${TAG} ${FULL_NAME}\"\n                sh \"docker image push ${FULL_NAME}\"\n            }\n        } // end node docker\n        stage('Prepare Pod') {\n            // have to checkout on our kubernetes pod aswell\n            checkout scm\n        }\n        stage('Check version') {\n            container('kubectl') {\n                sh 'kubectl version'\n            }\n        }\n    } // end node random label\n} // end pod def\n</code></pre>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#maven-jib","title":"Maven JIB","text":"<p>If you use Java with either Gradle or Maven, you can use JIB to create docker image without requiring a docker client or docker engine.</p> <p>JIB will communicate with DockerHub and Google's container registry for sending image layers back and forth. The last layer will be created by the JIB plugin itself, adding your self-executing Jar to the layer and a <code>Entrypoint</code> with the correct flags.</p> <p>For more information about the runtime options, see either jib-maven-plugin documentation or jib-gradle-plugin.</p>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Java project build with Gradle or Maven</li> <li>Java project that can start it self, such as Spring Boot or Thorntail (previously Wildfly Swarm, from the JBoss family)</li> <li>able to build either gradle or maven applications</li> </ul> <p>The project used can be found at github.com/demomon/maven-spring-boot-demo.</p>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#steps_1","title":"Steps","text":"<ul> <li>configure the plugin for either Gradle or Maven</li> <li>build using an official docker image via the kubernetes pod template</li> </ul>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#pipeline_1","title":"Pipeline","text":"<p>Using a bit more elaborate pipeline example here.</p> <p>Using SonarCloud for static code analysis and then JIB to create a docker image. We could then use that image either directly in anything that runs docker or in the same cluster via a Helm Chart.</p> <pre><code>def scmVars\ndef tag\n\npipeline {\n    options {\n        buildDiscarder logRotator(artifactDaysToKeepStr: '5', artifactNumToKeepStr: '5', daysToKeepStr: '5', numToKeepStr: '5')\n    }\n    libraries {\n        lib('core@master')\n        lib('maven@master')\n    }\n    agent {\n        kubernetes {\n            label 'mypod'\n            defaultContainer 'jnlp'\n            yaml \"\"\"\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    some-label: some-label-value\nspec:\n  containers:\n  - name: maven\n    image: maven:3-jdk-11-slim\n    command:\n    - cat\n    tty: true\n\"\"\"\n        }\n    }\n    stages {\n        stage('Test versions') {\n            steps {\n                container('maven') {\n                    sh 'uname -a'\n                    sh 'mvn -version'\n                }\n            }\n        }\n        stage('Checkout') {\n            steps {\n                script {\n                    scmVars = checkout scm\n                }\n                gitRemoteConfigByUrl(scmVars.GIT_URL, 'githubtoken')\n                sh '''\n                git config --global user.email \"jenkins@jenkins.io\"\n                git config --global user.name \"Jenkins\"\n                '''\n            }\n        }\n        stage('Build') {\n            steps {\n                container('maven') {\n                    sh 'mvn clean verify -B -e'\n                }\n            }\n        }\n        stage('Version &amp; Analysis') {\n            parallel {\n                stage('Version Bump') {\n                    when { branch 'master' }\n                    environment {\n                        NEW_VERSION = gitNextSemverTagMaven('pom.xml')\n                    }\n                    steps {\n                        script {\n                            tag = \"${NEW_VERSION}\"\n                        }\n                        container('maven') {\n                            sh 'mvn versions:set -DnewVersion=${NEW_VERSION}'\n                        }\n                        gitTag(\"v${NEW_VERSION}\")\n                    }\n                }\n                stage('Sonar Analysis') {\n                    when {branch 'master'}\n                    environment {\n                        SONAR_HOST='https://sonarcloud.io'\n                        KEY='spring-maven-demo'\n                        ORG='demomon'\n                        SONAR_TOKEN=credentials('sonarcloud')\n                    }\n                    steps {\n                        container('maven') {\n                            sh '''mvn sonar:sonar \\\n                                -Dsonar.projectKey=${KEY} \\\n                                -Dsonar.organization=${ORG} \\\n                                -Dsonar.host.url=${SONAR_HOST} \\\n                                -Dsonar.login=${SONAR_TOKEN}\n                            '''\n                        }\n                    }\n                }\n            }\n        }\n        stage('Publish Artifact') {\n            when { branch 'master' }\n            environment {\n                DHUB=credentials('dockerhub')\n            }\n            steps {\n                container('maven') {\n                    // we should never come here if the tests have not run, as we run verify before\n                    sh 'mvn clean compile -B -e jib:build -Djib.to.auth.username=${DHUB_USR} -Djib.to.auth.password=${DHUB_PSW} -DskipTests'\n                }\n            }\n        }\n    }\n    post {\n        always {\n            cleanWs()\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#kaniko","title":"Kaniko","text":"<p>Google loves Kubernetes and Google prefers people building docker images in Kubernetes without Docker.</p> <p>As JIB is only available for Java projects, there's needs to be an alternative for any other usecase/programming language.</p> <p>The answer to that is Kaniko, a specialized Docker image to create Docker images.</p> <p>Kaniko isn't the most secure way to create docker images, it barely beats mounting a Docker Socket, or might even be worse if you ask others (such as Jess Frazelle). </p> <p>That said, it is gaining some traction being used by JenkinsX and having an example in Jenkins' Kubernetes Plugin.</p> <p>Info</p> <p>When building more than one image inside the kaniko container, make sure to use the <code>--cleanup</code> flag. So it cleans its temporary cache data before building the next image, as discussed in this google group.</p>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#prerequisites_2","title":"Prerequisites","text":""},{"location":"jenkins-pipeline-examples/docker-alternatives/#steps_2","title":"Steps","text":"<ul> <li>Create docker registry secret</li> <li>Configure pod container template</li> <li>Configure stage</li> </ul>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#create-docker-registry-secret","title":"Create docker registry secret","text":"<p>This is an example for DockerHub inside the <code>build</code> namespace.</p> <pre><code>kubectl create secret docker-registry -n build regcred \\\n    --docker-server=index.docker.io \\\n    --docker-username=myDockerHubAccount \\\n    --docker-password=myDockerHubPassword \\\n    --docker-email=myDockerHub@Email.com\n</code></pre>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#example-ppeline","title":"Example Ppeline","text":"<p>Warning</p> <p>Although multi-stage <code>Dockerfile</code>'s are supported, it did fail in my case. So I created a second Dockerfile which is only for running the application (<code>Dockerfile.run</code>).</p> <pre><code>pipeline {\n    agent {\n        kubernetes {\n            //cloud 'kubernetes'\n            label 'kaniko'\n            yaml \"\"\"\nkind: Pod\nmetadata:\n  name: kaniko\nspec:\n  containers:\n  - name: golang\n    image: golang:1.11\n    command:\n    - cat\n    tty: true\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor:debug\n    imagePullPolicy: Always\n    command:\n    - /busybox/cat\n    tty: true\n    volumeMounts:\n      - name: jenkins-docker-cfg\n        mountPath: /root\n      - name: go-build-cache\n        mountPath: /root/.cache/go-build\n      - name: img-build-cache\n        mountPath: /root/.local\n  volumes:\n  - name: go-build-cache\n    emptyDir: {}\n  - name: img-build-cache\n    emptyDir: {}\n  - name: jenkins-docker-cfg\n    projected:\n      sources:\n      - secret:\n          name: regcred\n          items:\n            - key: .dockerconfigjson\n              path: .docker/config.json\n\"\"\"\n        }\n    }\n    stages {\n        stage('Checkout') {\n            steps {\n                git 'https://github.com/joostvdg/cat.git'\n            }\n        }\n        stage('Build') {\n            steps {\n                container('golang') {\n                    sh './build-go-bin.sh'\n                }\n            }\n        }\n        stage('Make Image') {\n            environment {\n                PATH = \"/busybox:$PATH\"\n            }\n            steps {\n                container(name: 'kaniko', shell: '/busybox/sh') {\n                    sh '''#!/busybox/sh\n                    /kaniko/executor -f `pwd`/Dockerfile.run -c `pwd` --cache=true --destination=index.docker.io/caladreas/cat\n                    '''\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#img","title":"IMG","text":"<p><code>img</code> is the brainchild of Jess Frazelle, a prominent figure in the container space.</p> <p>The goal is to be the safest and best way to build OCI compliant images, as outlined in her blog building container images securely on kubernetes.</p>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#not-working-for-me-yet","title":"Not working (for me) yet","text":"<p>It does not seem to work for me on AWS's EKS. To many little details with relation to runc, file permissions and other configuration.</p> <p>For those who want to give it a spin, here are some resources to take a look at.</p> <ul> <li>https://blog.jessfraz.com/post/building-container-images-securely-on-kubernetes/</li> <li>https://github.com/genuinetools/img</li> <li>https://github.com/opencontainers/runc</li> <li>https://git.j3ss.co/genuinetools/img/+/d05b3e4e10cd0e3c074ffb03dc22d7bb6cde1e78</li> </ul>"},{"location":"jenkins-pipeline-examples/docker-alternatives/#pipeline-example","title":"Pipeline Example","text":"<pre><code>pipeline {\n    agent {\n        kubernetes {\n            label 'img'\n            yaml \"\"\"\nkind: Pod\nmetadata:\n  name: img\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/img: unconfined  \nspec:\n  containers:\n  - name: golang\n    image: golang:1.11\n    command:\n    - cat\n    tty: true\n  - name: img\n    workingDir: /home/jenkins\n    image: caladreas/img:0.5.1\n    imagePullPolicy: Always\n    securityContext:\n        rawProc: true\n        privileged: true\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n      - name: jenkins-docker-cfg\n        mountPath: /root\n  volumes:\n  - name: temp\n    emptyDir: {}\n  - name: jenkins-docker-cfg\n    projected:\n      sources:\n      - secret:\n          name: regcred\n          items:\n            - key: .dockerconfigjson\n              path: .docker/config.json\n\"\"\"\n        }\n    }\n    stages {\n        stage('Checkout') {\n            steps {\n                git 'https://github.com/joostvdg/cat.git'\n            }\n        }\n        stage('Build') {\n            steps {\n                container('golang') {\n                    sh './build-go-bin.sh'\n                }\n            }\n        }\n        stage('Make Image') {\n            steps {\n                container('img') {\n                    sh 'mkdir cache'\n                    sh 'img build -s ./cache -f Dockerfile.run -t caladreas/cat .'\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline-examples/docker-declarative/","title":"Docker Declarative Examples","text":"<pre><code>pipeline {\n    agent none\n    options {\n        timeout(time: 10, unit: 'MINUTES')\n        timestamps()\n        buildDiscarder(logRotator(numToKeepStr: '5'))\n    }\n    stages {\n        stage('Prepare'){\n            agent { label 'docker' }\n            steps {\n                parallel (\n                    Clean: {\n                        deleteDir()\n                    },\n                    NotifySlack: {\n                        slackSend channel: 'cicd', color: '#FFFF00', message: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n                    }\n                )\n            }\n        }\n        stage('Checkout'){\n            agent { label 'docker' }\n            steps {\n                git credentialsId: '355df378-e726-4abd-90fa-e723c5c21ad5', url: 'git@gitlab.flusso.nl:CICD/ci-cd-docs.git'\n                script {\n                    env.GIT_COMMIT_HASH = sh returnStdout: true, script: 'git rev-parse --verify HEAD'\n                }\n            }\n        }\n        stage('Build Docs') {\n            agent {\n                docker {\n                    image \"caladreas/mkdocs-docker-build-container\"\n                    label \"docker\"\n                }\n            }\n            steps {\n                sh 'mkdocs build'\n            }\n        }\n        stage('Prepare Docker Image'){\n            agent { label 'docker' }\n            steps {\n                parallel (\n                    TestDockerfile: {\n                        script {\n                            def lintResult = sh returnStdout: true, script: 'docker run --rm -i lukasmartinelli/hadolint &lt; Dockerfile'\n                            if (lintResult.trim() == '') {\n                                println 'Lint finished with no errors'\n                            } else {\n                                println 'Error found in Lint'\n                                println \"${lintResult}\"\n                                currentBuild.result = 'UNSTABLE'\n                            }\n                        }\n                    }, // end test dockerfile\n                    BuildImage: {\n                        sh 'chmod +x build.sh'\n                        sh './build.sh'\n                    } \n                )\n            }\n            post {\n                success {\n                    sh 'chmod +x push.sh'\n                    sh './push.sh'\n                }\n            }\n        }\n        stage('Update Docker Container') {\n            agent { label 'docker' }\n            steps {\n                sh 'chmod +x container-update.sh'\n                sh \"./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH}\"\n            }\n        }\n    }\n    post {\n        success {\n            slackSend channel: 'cicd', color: '#00FF00', message: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n        }\n        failure {\n            slackSend channel: 'cicd', color: '#FF0000', message: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline-examples/maven-declarative/","title":"Maven Declarative Examples","text":""},{"location":"jenkins-pipeline-examples/maven-declarative/#basics","title":"Basics","text":"<p>We have to wrap the entire script in <code>pipeline { }</code>, for it to be marked a declarative script.</p> <p>As we will be using different agents for different stages, we select none as the default.</p> <p>For house keeping, we add the <code>options{}</code> block, where we configure the following:</p> <ul> <li>timeout: make sure this jobs succeeds in 10 minutes, else just cancel it</li> <li>timestamps(): to make sure we have timestamps in our logs</li> <li>buildDiscarder(): this will make sure we will only keep the latest 5 builds </li> </ul> <pre><code>pipeline {\n    agent none\n    options {\n        timeout(time: 10, unit: 'MINUTES')\n        timestamps()\n        buildDiscarder(logRotator(numToKeepStr: '5'))\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline-examples/maven-declarative/#checkout","title":"Checkout","text":"<p>There are several ways to checkout the code.</p> <p>Let's assume our code is somewhere in a git repository.</p>"},{"location":"jenkins-pipeline-examples/maven-declarative/#full-checkout-command","title":"Full Checkout command","text":"<p>The main command for checking out is the Checkout command.</p> <p>It will look like this.</p> <pre><code>stage('SCM') {\n    checkout([\n        $class: 'GitSCM', \n        branches: [[name: '*/master']], \n        doGenerateSubmoduleConfigurations: false, \n        extensions: [], \n        submoduleCfg: [], \n        userRemoteConfigs: \n            [[credentialsId: 'MyCredentialsId', \n            url: 'https://github.com/joostvdg/keep-watching']]\n    ])\n}\n</code></pre>"},{"location":"jenkins-pipeline-examples/maven-declarative/#git-shorthand","title":"Git shorthand","text":"<p>Thats a lot of configuration for a simple checkout.</p> <p>So what if I'm just using the master branch of a publicly accessible repository (as is the case with GitHub)?</p> <pre><code>stage('SCM') {\n    git 'https://github.com/joostvdg/keep-watching'\n}\n</code></pre> <p>Or with a different branch and credentials:</p> <pre><code>stage('SCM') {\n    git credentialsId: 'MyCredentialsId', url: 'https://github.com/joostvdg/keep-watching'\n}\n</code></pre> <p>That's much better, but we can do even better.</p>"},{"location":"jenkins-pipeline-examples/maven-declarative/#scm-shorthand","title":"SCM shorthand","text":"<p>If you're starting this pipeline job via a SCM, you've already configured the SCM.</p> <p>So assuming you've configured a pipeline job with 'Jenkinsfile from SCM' or an abstraction job - such as Multibranch-Pipeline, GitHub Organization or BitBucket Team/Project - you can do this.</p> <pre><code>stage('SCM') {\n    checkout scm\n}\n</code></pre> <p>The <code>checkout scm</code> line will use the checkout command we've used in the first example together with the object scm.</p> <p>This scm object, will contain the SCM configuration of the Job and will be reused for checking out.</p> <p>Warning</p> <p>A pipeline job from SCM or abstraction, will only checkout your Jenkinsfile. You will always need to checkout the rest of your code if you want to build it. For that, just use <code>checkout scm</code> </p>"},{"location":"jenkins-pipeline-examples/maven-declarative/#different-agent-per-stage","title":"Different Agent per Stage","text":"<p>As you could see on the top, we've set agent to none.</p> <p>So for every stage we now need to tell it which agent to use - without it, the stage will fail.  </p>"},{"location":"jenkins-pipeline-examples/maven-declarative/#agent-any","title":"Agent any","text":"<p>If you don't care what node it comes on, you specify any.</p> <pre><code>stage('Checkout') {\n    agent any\n    steps {\n        git 'https://github.com/joostvdg/keep-watching'\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline-examples/maven-declarative/#agent-via-label","title":"Agent via Label","text":"<p>If you want build on a node with a specific label - here docker - you do so with <code>agent { label '&lt;LABEL&gt;' }</code>. </p> <pre><code>stage('Checkout') {\n    agent { label 'docker' }\n    steps {\n        git 'https://github.com/joostvdg/keep-watching'\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline-examples/maven-declarative/#docker-container-as-agent","title":"Docker Container as Agent","text":"<p>Many developers are using docker for their CI/CD builds.  So being able to use docker containers as build agents is a requirement these days.</p> <pre><code>stage('Maven Build') {\n    agent {\n        docker {\n            image 'maven:3-alpine'\n            label 'docker'\n            args  '-v /home/joost/.m2:/root/.m2'\n        }\n    }\n    steps {\n        sh 'mvn -B clean package'\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline-examples/maven-declarative/#cache-maven-repo","title":"Cache Maven repo","text":"<p>When you're using a docker build container, it will be clean every time.</p> <p>So if you want to avoid downloading the maven dependencies every build, you have to cache them.</p> <p>One way to do this, is to map a volume into the container so the container will use that folder instead. </p> <pre><code>stage('Maven Build') {\n    agent {\n        docker {\n            image 'maven:3-alpine'\n            label 'docker'\n            args  '-v /home/joost/.m2:/root/.m2'\n        }\n    }\n    steps {\n        sh 'mvn -B clean package'\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline-examples/maven-declarative/#post-stagebuild","title":"Post stage/build","text":"<p>The declarative pipeline allows for Post actions, on both stage and complete build level.</p> <p>For both types there are different post hooks you can use, such as success, failure.</p> <pre><code>stage('Maven Build') {\n    agent {\n        docker {\n            image 'maven:3-alpine'\n            label 'docker'\n            args  '-v /home/joost/.m2:/root/.m2'\n        }\n    }\n    steps {\n        sh 'mvn -B clean package'\n    }\n    post {\n        success {\n            junit 'target/surefire-reports/**/*.xml'\n        }\n    }\n}\n</code></pre> <pre><code>stages {\n    stage('Example') {\n        steps {\n            echo 'Hello World'\n        }\n    }\n}\npost {\n    always {\n        echo 'This will always run'\n    }\n    success {\n        echo 'SUCCESS!'\n    }\n    failure {\n        echo \"We Failed\"\n    }\n    unstable {\n        echo \"We're unstable\"\n    }\n    changed {\n        echo \"Status Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result]\"\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline-examples/maven-declarative/#entire-example","title":"Entire example","text":"<pre><code>pipeline {\n    agent none\n    options {\n        timeout(time: 10, unit: 'MINUTES')\n        timestamps()\n        buildDiscarder(logRotator(numToKeepStr: '5'))\n    }\n    stages {\n        stage('Example') {\n            steps {\n                echo 'Hello World'\n            }\n        }\n        stage('Checkout') {\n            agent { label 'docker' }\n            steps {\n                git 'https://github.com/joostvdg/keep-watching'\n            }\n        }\n        stage('Maven Build') {\n            agent {\n                docker {\n                    image 'maven:3-alpine'\n                    label 'docker'\n                    args  '-v /home/joost/.m2:/root/.m2'\n                }\n            }\n            steps {\n                sh 'mvn -B clean package'\n            }\n            post {\n                success {\n                    junit 'target/surefire-reports/**/*.xml'\n                }\n            }\n        }\n        stage('Docker Build') {\n            agent { label 'docker' }\n            steps {\n                sh 'docker build --tag=keep-watching-be .'\n            }\n        }\n    }\n    post {\n        always {\n            echo 'This will always run'\n        }\n        success {\n            echo 'SUCCESS!'\n        }\n        failure {\n            echo \"We Failed\"\n        }\n        unstable {\n            echo \"We're unstable\"\n        }\n        changed {\n            echo \"Status Changed: [From: $currentBuild.previousBuild.result, To: $currentBuild.result]\"\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins-pipeline-examples/maven-groovy-dsl/","title":"Maven Groovy DSL Example","text":"<pre><code>node {\n    timestamps {\n        timeout(time: 15, unit: 'MINUTES') {\n            deleteDir()\n            stage 'SCM'\n            //git branch: 'master', credentialsId: 'flusso-gitlab', url: 'https://gitlab.flusso.nl/keep/keep-backend-spring.git'\n            checkout scm\n\n            env.JAVA_HOME=\"${tool 'JDK 8 Latest'}\"\n            env.PATH=\"${env.JAVA_HOME}/bin:${env.PATH}\"\n            sh 'java -version'\n\n            try {\n                def gradleHome = tool name: 'Gradle Latest', type: 'hudson.plugins.gradle.GradleInstallation'\n                stage 'Build'\n\n                sh \"${gradleHome}/bin/gradle clean build javadoc\"\n                step([$class: 'CheckStylePublisher', canComputeNew: false, defaultEncoding: '', healthy: '', pattern: 'build/reports/checkstyle/main.xml', unHealthy: ''])\n                step([$class: 'JUnitResultArchiver', testResults: 'build/test-results/*.xml'])\n                step([$class: 'JavadocArchiver', javadocDir: 'build/docs/javadoc'])\n\n                stage 'SonarQube'\n                sh \"${gradleHome}/bin/gradle sonarqube -Dsonar.host.url=http://sonarqube5-instance:9000\"\n\n                stash 'workspace'\n            }  catch (err) {\n                archive 'build/**/*.html'\n                echo \"Caught: ${err}\"\n                currentBuild.result = 'FAILURE'\n            }\n        }\n    }\n}\n\nnode ('docker') {\n    timestamps {\n        timeout(time: 15, unit: 'MINUTES') {\n            deleteDir()\n            unstash 'workspace'\n\n            stage 'Build Docker image'\n            sh './build.sh'\n            def image = docker.image('keep-backend-spring-img')\n\n            stage 'Push Docker image'\n            try {\n                sh 'docker tag keep-backend-spring-img nexus.docker:18443/flusso/keep-backend-spring-img:latest'\n                sh 'docker push nexus.docker:18443/flusso/keep-backend-spring-img:latest'\n            }  catch (err) {\n                archive 'build/**/*.html'\n                echo \"Caught: ${err}\"\n                currentBuild.result = 'FAILURE'\n            }\n\n        }\n    }\n}\n</code></pre>"},{"location":"jenkinsx/aks-boot-core/","title":"Jenkins X On AKS With JX Boot &amp; CloudBees Core","text":"<p>The goal of the guide is the following:</p> <p>manage CloudBees Core on Modern via Jenkins X in its own environment/namespace.</p> <p>To make it more interesting, we add more variables in the mix in the form of \"requirements\".</p> <ul> <li>cluster must NOT run on GKE, Jenkins X works pretty well there and doesn't teach us much</li> <li>every exposed service MUST use TLS, no excuses</li> <li>we do not want to create a certificate for every service that uses TLS</li> <li>as much as possible must be Configuration-as-Code</li> </ul> <p>In conclusion:</p> <ul> <li>We use Terraform to manage the Kubernetes Cluster on AKS</li> <li>JX Boot to manage Jenkins X</li> <li>We use Google CloudDNS to manage the DNS<ul> <li>this enables us to validate an entire subdomain via Let's Encrypt in one go</li> </ul> </li> </ul> <p>Note</p> <p>Unfortunately, these are already quite a lot of requirements. The Vault integration on anywhere but GKE is not stable. So we cheat and use <code>local</code> storage for credentials, meaning we need to use <code>jx boot</code> every time to upgrade the cluster.</p> <p>We will come back to this!</p>"},{"location":"jenkinsx/aks-boot-core/#install-jenkins-x","title":"Install Jenkins X","text":"<p>First, install Jenkins X with jx boot on AKS.</p>"},{"location":"jenkinsx/aks-boot-core/#install-cloudbees-core","title":"Install CloudBees Core","text":"<p>In order to install CloudBees Core with TLS, we need the following:</p> <ul> <li>TLS configuration for the environment Core is landing in (see above on how)</li> <li>add CloudBees Core as a requirement to the <code>env/requirements.yaml</code></li> <li>add configuration for CloudBees Core to the <code>env/values.yaml</code></li> </ul>"},{"location":"jenkinsx/aks-boot-core/#requirementsyaml","title":"requirements.yaml","text":"<pre><code>dependencies:\n- name: exposecontroller\n  version: 2.3.89\n  repository: http://chartmuseum.jenkins-x.io\n  alias: expose\n- name: exposecontroller\n  version: 2.3.89\n  repository: http://chartmuseum.jenkins-x.io\n  alias: cleanup\n- name: cloudbees-core\n  version: 2.176.203\n  repository: https://charts.cloudbees.com/public/cloudbees\n  alias: cbcore\n</code></pre>"},{"location":"jenkinsx/aks-boot-core/#valuesyaml","title":"values.yaml","text":"<p>Important</p> <p>The value you've set for the <code>alias</code> in the requirements, is your entrypoint for the configuration in the <code>values.yaml</code>!</p> <p>Also, take care to change the following values to reflect your environment! * OperationsCenter.HostName * OperationsCenter.Ingress.tls.Host * OperationsCenter.Ingress.tls.SecretName</p> <pre><code>cbcore:\n  OperationsCenter:\n    CSRF:\n      ProxyCompatibility: true\n    HostName: cbcore.staging.aks.example.com\n    Ingress:\n      Annotations:\n        kubernetes.io/ingress.class: nginx\n        kubernetes.io/tls-acme: \"true\"\n        nginx.ingress.kubernetes.io/app-root: https://$best_http_host/cjoc/teams-check/\n        nginx.ingress.kubernetes.io/proxy-body-size: 50m\n        nginx.ingress.kubernetes.io/proxy-request-buffering: \"off\"\n        nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n      tls:\n        Enable: true\n        Host: cbcore.staging.aks.example.com\n        SecretName: tls-staging-aks-example-com-p\n    ServiceType: ClusterIP\n  nginx-ingress:\n    Enabled: false\n</code></pre>"},{"location":"jenkinsx/aks-boot-core/#resources","title":"Resources","text":"<ul> <li>https://cloud.google.com/iam/docs/creating-managing-service-account-keys#iam-service-account-keys-create-console</li> <li>https://medium.com/google-cloud/kubernetes-w-lets-encrypt-cloud-dns-c888b2ff8c0e</li> <li>https://support.google.com/domains/answer/3290309?hl=en-GB&amp;ref_topic=9018335</li> <li>https://thorsten-hans.com/how-to-use-private-azure-container-registry-with-kubernetes</li> <li>https://cloud.google.com/dns/docs/migrating</li> </ul>"},{"location":"jenkinsx/build-packs/","title":"Build Packs","text":"<p>In this guide we will look at what a Jenkins X Build Pack is, what you can use it for and how you can extend it.</p>"},{"location":"jenkinsx/build-packs/#what-is-a-build-pack","title":"What is a Build Pack","text":"<p>A build pack is a collection of resources that helps you build and deploy an application in Kubernetes with Jenkins X.</p> <p>There's three ways you can use a build pack:</p> <ul> <li>when you create a new project (<code>jx create quickstart</code>)</li> <li>when you import an existing project (<code>jx import</code>)</li> <li>referencing it in a <code>jenkins-x.yaml</code> pipeline file</li> </ul> <p>A build pack contains resources related to the pipeline, both a <code>Jenkinsfile</code> for static Jenkins and a <code>jenkins-x.yanl</code> for Jenkins X pipelines.</p> <p>In addition, it contains everything else required to get an application of a specific technology and framework to build, versioned, containerized, and deployed with Helm. This means it contains a <code>Dockerfile</code> and a Helm Chart at least.</p> <p>When importing, what you already have doesn't get replaced, so they're safe to apply to any existing applications.</p> <p>As with any technology, you can never capture the whole world. This means there are times when you need to extend the default Build Packs.</p>"},{"location":"jenkinsx/build-packs/#ways-to-extend","title":"Ways To Extend","text":"<p>Roughly speaking, there are three ways to extend the Build Packs functionality within Jenkins X.</p> <ul> <li>Customize: you can customize your Build Packs by replacing the local reference of Jenkins X to different repository<ul> <li>this makes especially sense for <code>jx create quickstart</code> and <code>jx import</code>, as these commands run locally</li> </ul> </li> <li>Extend Locally: you can locally (in your own repository) extend the build pack by changing the generated files (such as <code>jenkins-x.yaml</code>)</li> <li>Extend Globally: for the Jenkins X pipeline (<code>jenkins-x.yaml</code>), you can also extend it globally by chaining Build Pack references</li> </ul> <p>Important</p> <p>The focus of this guide is on extending globally!</p>"},{"location":"jenkinsx/build-packs/#customize","title":"Customize","text":"<p>To customize the Build Packs we have to do the following:</p> <ul> <li>fork the default Jenkins X Build Packs</li> <li>make your changes to this fork repository</li> <li>update your Jenkins X's local Build Pack reference to your fork</li> </ul> <p>To fork the repository, you can go here https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes.</p> <p>The changes you might want to make I cannot predict, so you're on your own there.</p> <p>To create a similar Build Pack, you can copy a whole folder, and only change the things that need to be different.</p> <p>And, last but not least, to tell Jenkins X to use a different Build Pack repositort, use the command below:</p> <p>Tip</p> <p>In order to make this a bit easier to use, I've assumed the following: </p> <ul> <li>you do a direct fork, and do not rename the repository</li> <li>you fork it on GitHub</li> </ul> <p>Set the <code>GH_USER</code> variable and then the next few commands should be easier to use.</p> <pre><code>GH_USER=\n</code></pre> <pre><code>jx edit buildpack \\\n    -u https://github.com/$GH_USER/jenkins-x-kubernetes \\\n    -r master \\\n    -b\n</code></pre> <p>Once you've use it to create a new quickstart or import an existing application, you should be able to see your Build Repository checked out by Jenkins X locally.</p> <pre><code>ls -1 ~/.jx/draft/packs/github.com/$GH_USER/jenkins-x-kubernetes/packs\n</code></pre> <p>For a whole tutorial, you can look at Viktor Farcic's blog technologyconversations.com.</p>"},{"location":"jenkinsx/build-packs/#extend-locally","title":"Extend Locally","text":"<p>To extend locally, we simply have to alter anything in your application's repository.</p> <p>We can be a bit more helpful, there are ways to extend the Jenkins X pipeline locally, not by writing your own, but by the special <code>override</code>syntax in your <code>jenkins-x.yaml</code> file.</p> <p>This would look like this:</p> <pre><code>pipelineConfig:\n  pipelines:\n    overrides:\n</code></pre> <p>Go to the Jenkins X Pipelines page for further details.</p>"},{"location":"jenkinsx/build-packs/#extend-globally","title":"Extend Globally","text":"<p>Extending the Build Pack globally has a very limited scope, it is only for the Jenkins X pipeline.</p> <p>However, as the pipeline is one of the - if not the - most important parts of Jenkins X. So I'd argue that it is very powerful despite its limited scope.</p> <p>So what we're going to do is the following:</p> <ul> <li>create a new repository</li> <li>set up the required structure in the repository</li> <li>create a pipeline extension</li> <li>configure an existing Jenkins X application to leverage our Build Pack</li> </ul> <p>Info</p> <p>Why would you want to globally extend the Build Packs? Because this allows you to store your extensions to the Jenkins X default pipelines in a way every application can reuse them, or even extend those.</p> <p>It allows you to define standard steps that every pipeline in your organization needs to execute, once and only once.</p>"},{"location":"jenkinsx/build-packs/#create-build-pack-repository","title":"Create Build Pack Repository","text":"<p>The minimal amount we need, is a folder called <code>packs</code>, inside which we need a few more things.</p> <ul> <li>an <code>imports.yaml</code> importing any and all Build Pack repositories we want to use<ul> <li>for example, the default <code>kubernetes</code> and <code>classic</code> packs from the Jenkins X Authors themselves</li> </ul> </li> <li>a folder, the name of your Build Pack, containing a <code>pipeline.yaml</code></li> </ul> <p>The structure will then look like this.</p> <pre><code>.\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 packs\n    \u251c\u2500\u2500 imports.yaml\n    \u2514\u2500\u2500 maven-joost\n        \u2514\u2500\u2500 pipeline.yaml\n</code></pre> <p>packs/imports.yaml</p> <p>Here we import the default Build Pack repositories, so our new pipelines can extends them using the Pipeline Extensions syntax.</p> <pre><code>modules:\n- name: classic\n  gitUrl: https://github.com/jenkins-x-buildpacks/jenkins-x-classic.git\n  gitRef: master\n- name: kubernetes\n  gitUrl: https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes.git\n  gitRef: master\n</code></pre>"},{"location":"jenkinsx/build-packs/#create-pipeline-extension","title":"Create Pipeline Extension","text":"<p>In order to extend the existing pipelines coming from other Build Packs, we have to set the <code>extends</code> configuration in the Build Pack's <code>pipeline.yaml</code>.  This file would reside in <code>packs/&lt;name-of-your-pack&gt;/pipeline.yaml</code>.</p> <p>In this case, we want to extend the <code>kubernetes</code> Build Pack repository's <code>maven-java11</code> syntax. We do this by filling <code>import</code> field with the name field from the repository listed in the <code>packs/imports.yaml</code> file.</p> <p>We then select a Build Pack's pipeline file by pointing to a <code>pipeline.yaml</code> file from the relative path of <code>packs/</code>.</p> <p>Lets say we want to use a different docker container as build agent, we would end up with this.</p> <p>packs/maven-joost/pipeline.yaml</p> <pre><code>extends:\n    import: kubernetes\n    file: maven-java11/pipeline.yaml\nagent:\n    label: jenkins-maven-java11\n    image: maven-java11\n    container: maven\n</code></pre>"},{"location":"jenkinsx/build-packs/#extending-the-pipeline-further","title":"Extending The Pipeline Further","text":"<p>If you want to further extend the pipeline, you can leverage the Jenkins X Pipeline syntax.</p> <p>For example, say you want to make sure your Pull Request builds run a SonarQube scan. You can add the step <code>sonar-scan-pr</code> to the <code>pullRequest</code> Pipeline, under the stage <code>build</code> as below.</p> <p>packs/maven-joost/pipeline.yaml</p> <pre><code>pipelines:\n    pullRequest:\n        build:\n            steps:\n            - name: sonar-scan-pr\n                command: sonar-scanner\n                image: newtmitch/sonar-scanner:3.0\n                dir: /workspace/source/\n                args:\n                - -Dsonar.projectName=...\n                - -Dsonar.projectKey=...\n                - -Dsonar.organization=...\n                - -Dsonar.sources=./src/main/java/\n                - -Dsonar.language=java\n                - -Dsonar.java.binaries=./target/classes\n                - -Dsonar.host.url=https://sonarcloud.io\n                - -Dsonar.login=${SONARCLOUD_TOKEN}\n</code></pre> <p>Go to the Jenkins X Pipelines page for further details.</p>"},{"location":"jenkinsx/build-packs/#use-new-build-pack","title":"Use New Build Pack","text":"<p>Once you have defined the pipeline in your Build Pack, you then specify the Build Pack you want to use in your application in the <code>jenkins-x.yaml</code> file.</p> <p>Important</p> <p>This section refers to your application's, not the Build Back's repository.</p> <p>You need to specify three parameters in order for Jenkins X to pick up your Build Pack and build up the effective pipeline from your Build Pack hierarchy.</p> <ul> <li>buildPack: the name of your Build Pack, e.g. the folder name in your Build Pack repository's <code>packs</code> folder that contains the <code>pipeline.yaml</code></li> <li>buildPackGitRef: the Git ref, e.g. tag, commit or branch name</li> <li>buildPackGitURL: the http(s) git URL</li> </ul> <p>jenkins-x.yaml</p> <pre><code>buildPack: maven-joost\nbuildPackGitRef: master\nbuildPackGitURL: https://github.com/joostvdg/jx-buildpacks.git\n</code></pre>"},{"location":"jenkinsx/builder-image/","title":"Create Jenkins X Builder Image","text":"<p>Jenkins X leverages Tekton pipelines to create a Kubernetes native Pipeline experience. Every step is run in its own container.</p> <p>People comonly start using Jenkins X via its pre-defined build packs. These build packs already have a default Container Image defined, and use some specific containers for certain specific tasks - such as Kaniko for building Container Images. We call these Container Images: Builders.</p>"},{"location":"jenkinsx/builder-image/#create-custom-builder","title":"Create Custom Builder","text":"<p>Sometimes you need to use a different container for a specific step. First, look at the available builder. If what you need does not exist yet, you will have to create one yourself.</p> <p>Jenkins X has a guide on how to create a custom Builder.</p> <p>In essence, you create a Container Image that extends from <code>gcr.io/jenkinsxio/builder-base:0.0.81</code>, includes your tools and packages of choice, and may or may not include the <code>jx</code> binary.</p> <p>What comes after, is your choice. You can add your Builder to Jenkins X's list of Builders, or directly use it in your Jenkins X Pipeline by FQN.</p> <p>The main difference, is that when you add the Builder to Jenkins X you can include default configuration for the entire Pod. Otherwise, you have to specify any unique configuration in the Jenkins X Pipeline where you use the image.</p>"},{"location":"jenkinsx/builder-image/#dockerfile-example","title":"Dockerfile Example","text":"<p>Dockerfile</p> <pre><code>FROM gcr.io/jenkinsxio/builder-base:0.0.81\n\nRUN yum install -y java-11-openjdk-devel &amp;&amp; yum update -y &amp;&amp; yum clean all\n\n# Maven\nENV MAVEN_VERSION 3.6.3\nRUN curl -f -L https://repo1.maven.org/maven2/org/apache/maven/apache-maven/$MAVEN_VERSION/apache-maven-$MAVEN_VERSION-bin.tar.gz | tar -C /opt -xzv\nENV M2_HOME /opt/apache-maven-$MAVEN_VERSION\nENV maven.home $M2_HOME\nENV M2 $M2_HOME/bin\nENV PATH $M2:$PATH\n\n# GraalVM\nARG GRAAL_VERSION=20.0.0\nENV GRAAL_CE_URL=https://github.com/graalvm/graalvm-ce-builds/releases/download/vm-${GRAAL_VERSION}/graalvm-ce-java11-linux-amd64-${GRAAL_VERSION}.tar.gz\nARG INSTALL_PKGS=\"gzip\"\n\nENV GRAALVM_HOME /opt/graalvm\nENV JAVA_HOME /opt/graalvm\n\nRUN yum install -y ${INSTALL_PKGS} &amp;&amp; \\\n    ### Installation of GraalVM\n    mkdir -p ${GRAALVM_HOME} &amp;&amp; \\\n    cd ${GRAALVM_HOME} &amp;&amp; \\\n    curl -fsSL ${GRAAL_CE_URL} | tar -xzC ${GRAALVM_HOME} --strip-components=1  &amp;&amp; \\\n    ### Cleanup     \n    yum clean all &amp;&amp; \\\n    rm -f /tmp/graalvm-ce-amd64.tar.gz &amp;&amp; \\\n    rm -rf /var/cache/yum\n    ###\n\nENV PATH $GRAALVM_HOME/bin:$PATH\nRUN gu install native-image\n\n# jx\nENV JX_VERSION 2.1.30\nRUN curl -f -L https://github.com/jenkins-x/jx/releases/download/v${JX_VERSION}/jx-linux-amd64.tar.gz | tar xzv &amp;&amp; \\\nmv jx /usr/bin/\n\nCMD [\"mvn\",\"-version\"]\n</code></pre>"},{"location":"jenkinsx/builder-image/#usage","title":"Usage","text":"<p>The example above is my Jenkins X Builder for Maven + JDK 11 + GraalVM. My Dockerhub ID is <code>caladreas</code>, and the image is <code>jx-builder-graalvm-maven-jdk11</code>. </p>"},{"location":"jenkinsx/builder-image/#override-default-container","title":"Override Default Container","text":"<p>To use this Container Image as the default container:</p> <p>jenkins-x.yml</p> <pre><code>buildPack:  maven-java11\npipelineConfig:\n  agent:\n    image: caladreas/jx-builder-graalvm-maven-jdk11:v0.7.0\n</code></pre>"},{"location":"jenkinsx/builder-image/#override-step-container","title":"Override Step Container","text":"<p>jenkins-x.yml</p> <pre><code>buildPack:  maven-java11\npipelineConfig:\n  pipelines:\n    overrides:\n      - pipeline: pullRequest\n        stage: build\n        name: mvn-install\n        steps:\n          - name: mvn-deploy\n            image: caladreas/jx-builder-graalvm-maven-jdk11:v0.9.0\n</code></pre>"},{"location":"jenkinsx/buildpack/","title":"Build Packs","text":"<p>There are multiple ways to create your own buildpack for Jenkins X.</p> <ul> <li>Start from a working example: either create a quickstart project or import your existing application. Make the build and promotions work and then create a new buildpack by making the same changes (parameterized where applicable) to a copy of the buildpack you started from.</li> </ul>"},{"location":"jenkinsx/buildpack/#start-from-a-working-example","title":"Start from a working example","text":"<p>We're going to build a buildpack for the following application:</p> <ul> <li>Micronaut framework</li> <li>build with Gradle</li> <li>with a Redis datastore</li> <li>with a TLS certificate for the ingress (https)</li> </ul>"},{"location":"jenkinsx/buildpack/#create-micronaut-application","title":"Create Micronaut application","text":"<ul> <li>create application via Micronaut CLI</li> <li>add a controller</li> <li>enable default healthendpoint</li> <li>import application with Jenkins X</li> <li>update helm chart: change healtcheck endpoint</li> <li>update helm chart: add dependency on Redis</li> <li>update values: set redis to not use a password</li> </ul> <pre><code>mn create-app example.micronaut.complete --features=kotlin,spek,tracing-jaeger,redis-lettuce\n</code></pre> <pre><code>jx import\n</code></pre>"},{"location":"jenkinsx/buildpack/#secrets","title":"Secrets","text":"<pre><code>helm repo add soluto https://charts.soluto.io\nhelm repo update\n</code></pre> <pre><code>helm upgrade --install kamus soluto/kamus\n</code></pre>"},{"location":"jenkinsx/custom-domain/","title":"Custom Domain","text":""},{"location":"jenkinsx/custom-domain/#at-creation-time","title":"At Creation Time","text":""},{"location":"jenkinsx/custom-domain/#after-creation","title":"After Creation","text":""},{"location":"jenkinsx/custom-domain/#changing-domain","title":"Changing Domain","text":""},{"location":"jenkinsx/hello-world/","title":"Hello World Demo","text":""},{"location":"jenkinsx/hello-world/#create-gke-jenkins-x-cluster","title":"Create GKE + Jenkins X cluster","text":"<pre><code>export JX_CLUSTER_NAME=joostvdg\nexport JX_ENV_PREFIX=joostvdg\nexport JX_ADMIN_PSS=vXDzpiaVAthneXJR355J7PBT\nexport JX_DOMAIN=jx.kearos.net\nexport JX_GIT_USER=joostvdg\nexport JX_API_TOKEN=61edcbf6507d31b3f2fe811baa82aa6de33db001\nexport JX_ORG=demomon\nexport JX_GCE_PROJECT=ps-dev-201405\nexport JX_K8S_REGION=europe-west4\nexport JX_K8S_ZONE=europe-west4-a\nexport GKE_NODE_LOCATIONS=${REGION}-a,${REGION}-b\nexport JX_K8S_VERSION=\n</code></pre>"},{"location":"jenkinsx/hello-world/#get-supported-k8s-versions","title":"Get supported K8S versions","text":"<p>```bash tab=\"Zonal\" gcloud container get-server-config --zone ${JX_K8S_ZONE} <pre><code>``` bash tab=\"Regional\"\ngcloud container get-server-config --region ${JX_K8S_REGION}\n</code></pre></p> <pre><code>export JX_K8S_VERSION=1.11.7-gke.4\n</code></pre>"},{"location":"jenkinsx/hello-world/#create-regional-cluster-w-domain","title":"Create regional cluster w/ Domain","text":"<p>Currently only possible if you create a regional cluster first and then install jx.</p> <pre><code>gcloud container clusters create ${JX_CLUSTER_NAME} \\\n    --region ${JX_K8S_REGION} --node-locations ${GKE_NODE_LOCATIONS} \\\n    --cluster-version ${JX_K8S_VERSION} \\\n    --enable-pod-security-policy \\\n    --enable-network-policy \\\n    --num-nodes 2 --machine-type n1-standard-2 \\\n    --min-nodes 2 --max-nodes 3 \\\n    --enable-autoupgrade \\\n    --enable-autoscaling \\\n    --labels=owner=jvandergriendt,purpose=practice\n</code></pre> <pre><code>jx install\n</code></pre>"},{"location":"jenkinsx/hello-world/#create-zonal-cluster-w-domain","title":"Create zonal cluster w/ Domain","text":"<pre><code>jx create cluster gke \\\n    --cluster-name=\"${JX_CLUSTER_NAME}\" \\\n    --default-admin-password=\"${JX_ADMIN_PSS}\" \\\n    --domain=\"${JX_DOMAIN}\" \\\n    --kubernetes-version=\"${JX_K8S_VERSION}\" \\\n    --machine-type='n1-standard-2' \\\n    --max-num-nodes=3 \\\n    --min-num-nodes=2 \\\n    --project-id=${JX_GCE_PROJECT} \\\n    --zone=\"${JX_K8S_ZONE}\" \\\n    --kaniko=true \\\n    --skip-login\n</code></pre>"},{"location":"jenkinsx/hello-world/#reinstall","title":"Reinstall","text":"<pre><code>jx install \\\n    --default-environment-prefix=$JX_ENV_PREFIX \\\n    --git-api-token=$JX_API_TOKEN \\\n    --git-username=$JX_GIT_USER \\\n    --environment-git-owner=$JX_GIT_USER \\\n    --default-admin-password=\"${JX_ADMIN_PSS}\" \\\n    --domain=\"${JX_DOMAIN}\"\n</code></pre>"},{"location":"jenkinsx/hello-world/#configure-domain","title":"Configure Domain","text":"<p>Once the cluster is up and the Jenkins X basics are installed, jx will prompt us about missing an ingress controller.</p> <pre><code>? No existing ingress controller found in the kube-system namespace, shall we install one? Yes\n</code></pre> <p>Reply yes, and in a little while, you will see the following message:</p> <pre><code>You can now configure your wildcard DNS jx.kearos.net to point to 35.204.0.182\nnginx ingress controller installed and configured\n</code></pre> <p>We can now go to our Domain configuration and set <code>*.jx.${DomainName}</code> to the ip listed.</p> <p>If you're using Google Domains by any chance, you create an <code>A</code> class record for <code>*.jx</code> with ip <code>35.204.0.182</code>.</p> <p>Unfortunately, that's not enough, as the ingress resources created by jx after will have a different IP address. So we have to add a second IP address to your Class A record.</p> <p>Still assuming GKE, you can retrieve the second IP address as follows:</p> <pre><code>INGRESS_IP=$(kubectl get ing chartmuseum -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\necho $INGRESS_IP\n</code></pre> <p>To test the domain, you can do the following:</p> <pre><code>CM_URL=$(kubectl get ing chartmuseum -o jsonpath=\"{.spec.rules[0].host}\")\n</code></pre> <p>``` bash tab=\"Curl\" curl $CM_URL <pre><code>``` bash tab=\"Httpie\"\nhttp $CM_URL\n</code></pre></p>"},{"location":"jenkinsx/hello-world/#configure-tls","title":"Configure TLS","text":"<p>If we have a proper domain configured and working, we can also enable TLS. As we've not done so at the start, we will have to update the Ingress configuration.</p> <p>For all the options for updating the Ingress configuration, use the command below.</p> <pre><code>jx upgrade ingress --help\n</code></pre> <p>To configure TLS for our ingresses, we need TLS certificates. Jenkins X does this via Let's Encrypt, which in Kubernetes is easily done via Certmanager.</p> <p>The command we will issue will ask us if we want to install <code>CertManager</code>, and then delete all existing ingress resources and recreate them with the certificate. Unfortunately, when in Batch mode (<code>-b</code>) it does not install <code>CertManager</code> nor is there an option to force it in this case.</p> <ul> <li>When asked if we want to delete and recreate the existing ingress rules, say yes (<code>y</code>).</li> <li>Select the expose type, which should be <code>Ingress</code> (route is for OpenShift).</li> <li>Confirm your domain -&gt; do not change it, as this upgrade does NOT change your domain configuration everywhere and you will end up with a broken system</li> <li>Say yes to cluster wide TLS</li> <li>If you're certain your Domain works, select the <code>production</code> LetEncrypt configuration, else choose <code>staging</code> for tests</li> <li>Confirm your email address and the summary</li> <li>Agree with installing <code>CertManager</code></li> <li>Do Not agree with updating the webhooks (see below)</li> </ul> <pre><code>jx upgrade ingress --cluster --verbose\n</code></pre> <p>Warning</p> <p>There's currently a bug with changing the webhooks via this command; see issue #3115 It somehow can only select a different GitHub user than the current one, which makes no sense for an UPDATE. So we must update the webhooks manually ourselves!</p>"},{"location":"jenkinsx/hello-world/#manually-update-webhooks","title":"Manually update webhooks","text":"<p>Due to issue #3115 we need to manually update our webhooks for the environment repositories.</p> <p>If you're not sure where your environment repositories are, you can retrieve them with the command below:</p> <pre><code>js get env\n</code></pre> <p>Open each environment repository, go to the settings tabs (top right), open the webhooks menu (on the left), and edit the webhook. Simply change the <code>http://</code> to <code>https://</code> and save.</p> <p>Warning</p> <p>If you've selected the <code>staging</code> configuration for Let's Encrypt, you have set the SSL configuration to <code>Disable (not recommended)</code>.</p>"},{"location":"jenkinsx/hello-world/#create-options","title":"Create options","text":"<ul> <li><code>--buildpack='': The name of the build pack to use for the Team</code></li> <li><code>--vault</code></li> <li><code>--helm3=false: Use helm3 to install Jenkins X which does not use Tiller</code></li> <li><code>--kaniko=false</code></li> <li><code>--urltemplate='': For ingress; exposers can set the urltemplate to expose</code></li> </ul>"},{"location":"jenkinsx/hello-world/#addons","title":"Addons","text":"<pre><code>create addon ambassador Create an ambassador addon\ncreate addon anchore Create the Anchore addon for verifying container images\ncreate addon cloudbees Create the CloudBees app for Kubernetes (a web console for working with CI/CD, Environments and GitOps)\ncreate addon flagger Create the Flagger addon for Canary deployments\ncreate addon gitea  Create a Gitea addon for hosting Git repositories\ncreate addon istio  Create the Istio addon for service mesh\ncreate addon knative-build Create the knative build addon\ncreate addon kubeless Create a kubeless addon for hosting Git repositories\ncreate addon owasp-zap Create the OWASP Zed Attack Proxy addon for dynamic security checks against running apps\ncreate addon pipeline-events Create the pipeline events addon\ncreate addon prometheus Creates a prometheus addon\ncreate addon prow   Create a Prow addon\ncreate addon sso    Create a SSO addon for Single Sign-On\ncreate addon vault-operator Create an vault-operator addon for Hashicorp Vault\n</code></pre>"},{"location":"jenkinsx/hello-world/#upgrade","title":"Upgrade","text":"<pre><code>jx upgrade --help\n</code></pre> <pre><code>upgrade addons Upgrades any Addons added to Jenkins X if there are any new releases available\nupgrade apps Upgrades any Apps to the latest release\nupgrade binaries Upgrades the command line binaries (like helm or eksctl) - if there are new versions available\nupgrade cli Upgrades the command line applications - if there are new versions available\nupgrade cluster Upgrades the Kubernetes master to the specified version\nupgrade extensions Upgrades the Jenkins X extensions available to this Jenkins X install if there are new versions available\nupgrade ingress Upgrades Ingress rules\nupgrade platform Upgrades the Jenkins X platform if there is a new release available\n</code></pre>"},{"location":"jenkinsx/hello-world/#go-lang-example","title":"Go lang example","text":"<pre><code>jx create quickstartjc\n</code></pre> <ul> <li>select <code>golang-http</code></li> </ul>"},{"location":"jenkinsx/hello-world/#promote","title":"Promote","text":"<pre><code>APP=jx-go-demo-5\nVERSION=0.0.2\n</code></pre> <pre><code>jx promote ${APP} --version $VERSION --env production -b\n</code></pre> <pre><code>jx get apps\n</code></pre>"},{"location":"jenkinsx/hello-world/#spring-boot-example","title":"Spring Boot Example","text":"<pre><code>jx create spring -d web -d actuator\n</code></pre>"},{"location":"jenkinsx/hello-world/#serverless","title":"Serverless","text":"<pre><code>jx create terraform gke \\\n    --vault='true' \\\n    --cluster=\"${JX_CLUSTER_NAME}\"=gke \\\n    --gke-project-id=${JX_GCE_PROJECT} \\\n    --prow \\\n    --skip-login\n</code></pre>"},{"location":"jenkinsx/hello-world/#demo-show-jx-stuff","title":"Demo - Show JX Stuff","text":""},{"location":"jenkinsx/hello-world/#gitops","title":"GitOps","text":"<pre><code>Get environments: jx get environments\nWatch pipeline activity via:    jx get activity -f golang-http -w\nBrowse the pipeline log via:    jx get build logs demomon/golang-http/master\nOpen the Jenkins console via    jx console\nYou can list the pipelines via: jx get pipelines\nWhen the pipeline is complete:  jx get applications\n</code></pre>"},{"location":"jenkinsx/hello-world/#build-it","title":"Build It","text":"<ul> <li>explain build packs</li> <li><code>jx create quickstart</code></li> <li>show Jenkinsfile</li> <li>open application<ul> <li><code>jx get applications</code></li> </ul> </li> <li>create new branch <code>git checkout -b wip</code></li> <li>change main.go</li> <li>commit <code>git -a -m \"better message\"</code></li> <li>push <code>git push remote wip</code></li> <li>open PR page</li> <li>explain tide</li> <li>add cat picture: <code>/meow</code></li> <li>test it<ul> <li>add comment <code>/test this</code></li> <li>open logs <code>jx logs -k</code></li> </ul> </li> <li>open PR environment</li> <li>approve the change<ul> <li><code>/approve</code></li> <li>or add <code>approved</code> label</li> </ul> </li> <li>open logs for next step</li> <li>promote: <code>jx promote myapp --version 1.2.3 --env production</code></li> <li>promote: <code>jx promote ${APP} --version ${VERSION} --env production</code></li> </ul>"},{"location":"jenkinsx/hybrid/","title":"Jenkins X Hybrid TLS","text":"<p>Jenkins X Hybrid TLS is a configuration of Jenkins X using both Static Jenkins and Jenkins X Serverless with Tekton within the same cluster. As the TLS suffix hints at, it also uses TLS for both installations to make sure all the services and your applications are accessible via https with a valid certificate.</p>"},{"location":"jenkinsx/hybrid/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>GCP account<ul> <li>with active subscription</li> <li>with an active project with which you are authenticated</li> </ul> </li> <li><code>gcloud</code> CLI</li> <li>Jenkins X CLI <code>jx</code></li> <li>httpie or curl</li> </ul>"},{"location":"jenkinsx/hybrid/#steps","title":"Steps","text":"<ul> <li>create JX cluster in GKE with static Jenkins<ul> <li>without Nexus</li> </ul> </li> <li>create Go (lang) quickstart</li> <li>configure TLS</li> <li>install Serverless Jenkins X in the same cluster</li> <li>create Spring Boot Quickstart</li> <li>configure TLS for Serverless namespaces only</li> <li>re-install Jenkins X with Nexus</li> </ul>"},{"location":"jenkinsx/hybrid/#static","title":"Static","text":""},{"location":"jenkinsx/hybrid/#prepare","title":"Prepare","text":""},{"location":"jenkinsx/hybrid/#variables","title":"Variables","text":"<pre><code>CLUSTER_NAME=#name of your cluster\nPROJECT=#name of your GCP project\nREGION=#GCP region to install cluster in\nGITHUB_USER=#your GitHub Username\nGITHUB_TOKEN=#GitHub apitoken\n</code></pre>"},{"location":"jenkinsx/hybrid/#myvaluesyaml","title":"myvalues.yaml","text":"<p>We're going to use a demo application based on Go, so we don't need Nexus.</p> <p>To configure Jenkins X to skip Nexus' installation, create the file <code>myvalues.yaml</code> with the following contents:</p> <pre><code>nexus:\n  enabled: false\ndocker-registry:\n  enabled: true\n</code></pre>"},{"location":"jenkinsx/hybrid/#install-jx","title":"Install JX","text":"<p>Make sure you execute this command where you have the <code>myvalues.yaml</code> file.</p> <pre><code>jx create cluster gke \\\n    --cluster-name ${CLUSTER_NAME} \\\n    --project-id ${PROJECT} \\\n    --region ${REGION} \\\n    --machine-type n1-standard-2 \\\n    --min-num-nodes 1 \\\n    --max-num-nodes 2 \\\n    --default-admin-password=admin \\\n    --default-environment-prefix jx-rocks \\\n    --git-provider-kind github \\\n    --git-username ${GITHUB_USER} \\\n    --git-provider-kind github \\\n    --git-api-token ${GITHUB_TOKEN} \\\n    --batch-mode\n</code></pre>"},{"location":"jenkinsx/hybrid/#go-quickstart","title":"Go Quickstart","text":"<pre><code>jx create quickstart \\\n    -l go --org ${GITHUB_USER} \\\n    --project-name jx-static-go \\\n    --import-mode=Jenkinsfile \\\n    --deploy-kind default \\\n    -b\n</code></pre>"},{"location":"jenkinsx/hybrid/#watch-activity","title":"Watch activity","text":"<p>You can either go to Jenkins and watch the job there: <code>jx console</code> or watch in your console via <code>jx get activity</code>.</p> <pre><code>jx get activity -f jx-static-go -w\n</code></pre> <p>Once the build completes, you should see something like the line below, you can test the application.</p> <pre><code>Promoted                          28m5s    1m41s Succeeded  Application is at: http://jx-static-go.jx-staging.34.90.105.15.nip.io\n</code></pre>"},{"location":"jenkinsx/hybrid/#test-application","title":"Test application","text":"<p>To confirm the application is running in the staging environment:</p> <pre><code>jx get applications\n</code></pre> <p>Which should show something like this:</p> <pre><code>APPLICATION  STAGING PODS URL\njx-static-go 0.0.1   1/1  http://jx-static-go.jx-staging.${LIB_IP}.nip.io\n</code></pre> <pre><code>LB_IP=$(kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\n</code></pre> <pre><code>http jx-static-go.jx-staging.${LB_IP}.nip.io\n</code></pre> <p>Which should show the following:</p> <pre><code>HTTP/1.1 200 OK\nConnection: keep-alive\nContent-Length: 43\nContent-Type: text/plain; charset=utf-8\nDate: Thu, 13 Jun 2019 12:17:39 GMT\nServer: nginx/1.15.8\n\nHello from:  Jenkins X golang http example\n</code></pre>"},{"location":"jenkinsx/hybrid/#configure-tls","title":"Configure TLS","text":"<p>Make sure you have two things:</p> <ul> <li>the address of your LoadBalancer (see below how to retrieve this)</li> <li>a Domain name with a quick and easy DNS configuration (incl. wildcard support)</li> </ul>"},{"location":"jenkinsx/hybrid/#retrieve-loadbalancer-address","title":"Retrieve LoadBalancer address","text":"<pre><code>LB_IP=$(kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\n</code></pre>"},{"location":"jenkinsx/hybrid/#configure-dns","title":"Configure DNS","text":"<p>Go to your Domain provider of choice, if you don't have one, consider Google Domains for 12 Euro per year. They might no be the cheapest, but the service is great and works quick - changes like we're about to do, take a few minutes to be effectuated.</p> <p>Configure the following wildcards to direct to your LoadBalancer's IP address:</p> <ul> <li><code>*.jx</code> - type A</li> <li><code>*.jx-staging</code> - type A</li> <li><code>*.jx-production</code> - type A</li> <li><code>*.serverless</code> - type A (for the serverless section)</li> </ul>"},{"location":"jenkinsx/hybrid/#upgrade-ingress","title":"Upgrade Ingress","text":"<p>To configure TLS inside Jenkins X, we make use of Let's Encrypt and cert-manager.</p> <p>To get Jenkins X to configure TLS, we use the <code>jx upgrade ingress</code> command.</p> <pre><code>DOMAIN=#your domain name\n</code></pre> <pre><code>jx upgrade ingress \\\n    --cluster true \\\n    --domain $DOMAIN\n</code></pre> <p>Info</p> <p>To be sure, the Domain name above should the base hostname only. Any resource within your JX installation will automatically get the following domain name: <code>{name}.{namespace}.{DOMAIN}</code>. For example, if your domain is <code>example.com</code> Jenkins will become <code>jenkins.jx.example.com</code>.</p>"},{"location":"jenkinsx/hybrid/#test-applications","title":"Test applications","text":"<p>Confirm your application now has a https protocol.</p> <pre><code>jx get applications\n</code></pre> <pre><code>http https://jx-static-go.jx-staging.${DOMAIN}\n</code></pre>"},{"location":"jenkinsx/hybrid/#serverless","title":"Serverless","text":""},{"location":"jenkinsx/hybrid/#prepare_1","title":"Prepare","text":"<p>The values for <code>INGRESS_NS</code> and <code>INGRESS_DEP</code> are the default based on the static install created above. If your ingress controller namespace and/or deployment have different names, replace the values.</p> <p>For the <code>LB_IP</code>, we're also assuming default names and namespaces.</p> <pre><code>PROVIDER=gke\nLB_IP=$(kubectl get svc -n kube-system jxing-nginx-ingress-controller -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\nDOMAIN_SUFFIX=#your domain name\nDOMAIN=serverless.${DOMAIN_SUFFIX}\nINGRESS_NS=kube-system\nINGRESS_DEP=jxing-nginx-ingress-controller\nINSTALL_NS=cdx\nPROJECT=#your GCP project\n</code></pre> <p>Info</p> <p>We're going to use the <code>cdx</code> namespace, this will create namespaces such as <code>cdx</code> and <code>cdx-staging</code>. In order to avoid having to register every environment in at our DNS provider, we will use an additional domain prefix <code>serverless</code>. Making the domain <code>serverless.{DOMAIN}</code> and the JX components <code>{name}.cdx.serverless.{DOMAIN}</code>.</p>"},{"location":"jenkinsx/hybrid/#install-serverless-jx","title":"Install Serverless JX","text":"<pre><code>jx install \\\n    --provider $PROVIDER \\\n    --external-ip $LB_IP \\\n    --domain $DOMAIN \\\n    --default-admin-password=admin \\\n    --ingress-namespace $INGRESS_NS \\\n    --ingress-deployment $INGRESS_DEP \\\n    --default-environment-prefix tekton \\\n    --git-provider-kind github \\\n    --namespace ${INSTALL_NS} \\\n    --prow \\\n    --docker-registry gcr.io \\\n    --docker-registry-org $PROJECT \\\n    --tekton \\\n    --kaniko \\\n    -b\n</code></pre>"},{"location":"jenkinsx/hybrid/#spring-boot-quickstart","title":"Spring Boot Quickstart","text":""},{"location":"jenkinsx/hybrid/#create-quickstart","title":"Create quickstart","text":"<pre><code>jx create spring -d web -d actuator \\\n    --group com.example \\\n    --artifact jx-spring-boot-demo \\\n    -b\n</code></pre> <pre><code>cd jx-spring-boot-demo\n</code></pre>"},{"location":"jenkinsx/hybrid/#add-controller","title":"Add controller","text":"<p>Assuming you kept the group the same, you should find a folder <code>src/main/java/com/example/jxspringbootdemo</code> containing a file, <code>DemoApplication.java</code>.</p> <p>We're going to have to add two files to the same folder:</p> <ul> <li><code>Greeting.java</code></li> <li><code>GreetingController.java</code></li> </ul>"},{"location":"jenkinsx/hybrid/#greeting","title":"Greeting","text":"<pre><code>package com.example.jxspringbootdemo;\n\npublic class Greeting {\n\n    private final long id;\n    private final String content;\n\n    public Greeting(long id, String content) {\n        this.id = id;\n        this.content = content;\n    }\n\n    public long getId() {\n        return id;\n    }\n\n    public String getContent() {\n        return content;\n    }\n}\n</code></pre>"},{"location":"jenkinsx/hybrid/#greetingcontroller","title":"GreetingController","text":"<pre><code>package com.example.jxspringbootdemo;\n\nimport java.util.concurrent.atomic.AtomicLong;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RequestParam;\nimport org.springframework.web.bind.annotation.RestController;\n\n@RestController\npublic class GreetingController {\n\n    private static final String template = \"Hello, %s!\";\n    private final AtomicLong counter = new AtomicLong();\n\n    @RequestMapping(\"/greeting\")\n    public Greeting greeting(@RequestParam(value=\"name\", defaultValue=\"World\") String name) {\n        return new Greeting(counter.incrementAndGet(),\n                            String.format(template, name));\n    }\n}\n</code></pre>"},{"location":"jenkinsx/hybrid/#test-application_1","title":"Test application","text":"<pre><code>jx get activity -f jx-cdx-spring-boot-demo-1 -w\n</code></pre>"},{"location":"jenkinsx/hybrid/#re-install-with-nexus","title":"Re-Install with Nexus","text":""},{"location":"jenkinsx/hybrid/#myvaluesyaml_1","title":"myvalues.yaml","text":"<p>Our application didn't work because now we have an application that depends on a Maven repository. We have to \"re-install\" Jenkins X, to have it install Nexus for us in the <code>cdx</code> namespace.</p> <pre><code>nexus:\n  enabled: true\ndocker-registry:\n  enabled: true\n</code></pre>"},{"location":"jenkinsx/hybrid/#install","title":"Install","text":"<p>Make sure you execute this command where you have the <code>myvalues.yaml</code> file.</p> <pre><code>jx install \\\n    --provider $PROVIDER \\\n    --external-ip $LB_IP \\\n    --domain serverless.$DOMAIN \\\n    --default-admin-password=admin \\\n    --ingress-namespace $INGRESS_NS \\\n    --ingress-deployment $INGRESS_DEP \\\n    --default-environment-prefix tekton \\\n    --git-provider-kind github \\\n    --namespace ${INSTALL_NS} \\\n    --prow \\\n    --docker-registry gcr.io \\\n    --docker-registry-org $PROJECT \\\n    --tekton \\\n    --kaniko \\\n    -b\n</code></pre>"},{"location":"jenkinsx/hybrid/#test-application_2","title":"Test Application","text":"<p>To trigger a new build, make a change - for example to the <code>README.md</code> and push it.</p> <pre><code>jx get activity -f jx-cdx-spring-boot-demo-1 -w\n</code></pre> <pre><code>http jx-cdx-spring-boot-demo-1.cdx-staging.serverless.${DOMAIN}/greeting\n</code></pre>"},{"location":"jenkinsx/hybrid/#configure-tls_1","title":"Configure TLS","text":"<pre><code>jx upgrade ingress --domain $DOMAIN --namespaces cdx,cdx-staging\n</code></pre>"},{"location":"jenkinsx/hybrid/#re-test-application","title":"Re-test application","text":"<pre><code>ORG=#the GitHub user or organisation your application is in\n</code></pre> <pre><code>jx update webhooks --repo=jx-cdx-spring-boot-demo-1 --org=${ORG}\njx get applications\njx get activity -f jx-cdx-spring-boot-demo-1 -w\nhttp https://jx-cdx-spring-boot-demo-1.cdx-staging.serverless.${DOMAIN}/greeting\n</code></pre>"},{"location":"jenkinsx/intro/","title":"Introduction to Jenkins X","text":""},{"location":"jenkinsx/jx-pipelines/","title":"Jenkins X Pipelines","text":""},{"location":"jenkinsx/jx-pipelines/#verify-changes","title":"Verify Changes","text":"<pre><code>jx step syntax effective\n</code></pre>"},{"location":"jenkinsx/jx-pipelines/#adding-steps","title":"Adding Steps","text":""},{"location":"jenkinsx/jx-pipelines/#add-step-default","title":"Add Step Default","text":"<p>Info</p> <p>By default, adding a step to a Pipeline's stage will cause it to end up at the end of the stage.</p> <pre><code>buildPack: maven-java11\npipelineConfig:\n  pipelines:\n    pullRequest:\n      build:\n        steps:\n        - command: sonar-scanner\n          image: fabiopotame/sonar-scanner-cli # newtmitch/sonar-scanner for JDK 10+?\n          dir: /workspace/source/\n          args:\n           - -Dsonar.projectName=...\n           - -Dsonar.projectKey=...\n           - -Dsonar.organization=...\n           - -Dsonar.sources=./src/main/java/\n           - -Dsonar.language=java\n           - -Dsonar.java.binaries=./target/classes\n           - -Dsonar.host.url=https://sonarcloud.io\n           - -Dsonar.login=...\n</code></pre>"},{"location":"jenkinsx/jx-pipelines/#add-step-at-specific-place","title":"Add Step At Specific Place","text":"<p>If we want it explicity after or before a specific step, we have to \"select\" the <code>step</code> within a <code>stage</code> of a pipeline first.</p> <p>We do that as follows:</p> <pre><code>pipelineConfig:\n  pipelines:\n    overrides:\n      - pipeline: [pipeline name: release, feature, pullRequest]\n        stage: [stage name]\n        name: [step name]\n</code></pre> <p>And then state of we want to replace the step (via <code>type: replace</code>), or execute it before or after the \"selected\" step, via the field <code>type</code>.</p> <pre><code>pipelineConfig:\n  pipelines:\n    overrides:\n      - name: mvn-deploy\n        pipeline: release\n        stage: build\n        step:\n          name: sonar\n          command: sonar-scanner\n          image: fabiopotame/sonar-scanner-cli # newtmitch/sonar-scanner for JDK 10+?\n          dir: /workspace/source/\n          args:\n           - -Dsonar.projectName=...\n           - -Dsonar.projectKey=...\n           - -Dsonar.organization=...\n           - -Dsonar.sources=./src/main/java/\n           - -Dsonar.language=java\n           - -Dsonar.java.binaries=./target/classes\n           - -Dsonar.host.url=https://sonarcloud.io\n           - -Dsonar.login=...\n        type: after\n</code></pre>"},{"location":"jenkinsx/jx-pipelines/#overriding-steps","title":"Overriding Steps","text":"<pre><code>pipelineConfig:\n  pipelines:\n    release:\n      setup:\n        preSteps:\n        - sh: echo BEFORE BASE SETUP\n        steps:\n        - sh: echo AFTER BASE SETUP\n      build:\n        replace: true\n        steps:\n        - sh: mvn clean deploy -Pmyprofile\n          comment: this command is overridden from the base pipeline\n</code></pre>"},{"location":"jenkinsx/jx-pipelines/#meta-pipelinerun-always-for-every-pipeline","title":"Meta pipeline/run always for every pipeline","text":""},{"location":"jenkinsx/jx-pipelines/#jx-pipeline-converter","title":"JX Pipeline Converter","text":"<p><code>JX Pipeline Converter</code> plugin for Jenkins X to assist in converting from the legacy Jenkinsfile-based pipelines to the modern jenkins-x.yml-based pipelines.</p>"},{"location":"jenkinsx/jx-pipelines/#install","title":"Install","text":"<p>```bash tab=\"macos\" curl -L https://github.com/jenkins-x/jx-convert-jenkinsfile/releases/download/$(curl --silent https://api.github.com/repos/jenkins-x/jx-convert-jenkinsfile/releases/latest | jq -r '.tag_name')/jx-convert-jenkinsfile-darwin-amd64.tar.gz | tar xzv  sudo mv jx-convert-jenkinsfile /usr/local/bin <pre><code>```bash tab=\"linux\"\ncurl -L https://github.com/jenkins-x/jx-convert-jenkinsfile/releases/download/$(curl --silent https://api.github.com/repos/jenkins-x/jx-convert-jenkinsfile/releases/latest | jq -r '.tag_name')/jx-convert-jenkinsfile-linux-amd64.tar.gz | tar xzv \nsudo mv jx-convert-jenkinsfile /usr/local/bin\n</code></pre></p> <p>Caution</p> <p>It seems some shells will escape the <code>(</code> in the above command.</p> <p>Make sure the command reads <code>.../download/$(curl --silent...</code>.</p>"},{"location":"jenkinsx/jx-pipelines/#usage","title":"Usage","text":"<pre><code>jx convert jenkinsfile\n</code></pre>"},{"location":"jenkinsx/jx-pipelines/#loops","title":"Loops","text":"<pre><code>buildPack: go\npipelineConfig:\n  pipelines:\n    overrides:\n    - pipeline: release\n      # This is new\n      stage: build\n      name: make-build\n      steps:\n      - loop:\n          variable: GOOS\n          values:\n          - darwin\n          - linux\n          - windows\n          steps:\n          - name: build\n            command: CGO_ENABLED=0 GOOS=\\${GOOS} GOARCH=amd64 go build -o bin/jx-go-loops_\\${GOOS} main.go\n</code></pre> <p>Borrowed from Viktor Farcic's blog on Jenkins X Pipelines.</p>"},{"location":"jenkinsx/jx-pipelines/#replace-whole-pipeline","title":"Replace Whole Pipeline","text":"<p>If you want to write your pipeline from scratch, you can specify you do not use a Build Pack.</p> <pre><code>buildPack: none\npipelineConfig:\n  pipelines:\n    release:\n      pipeline:\n        agent:\n          image: busybox\n        stages:\n          - name: ci\n            steps:\n              - name: echo-version\n                image: mvn\n                command: mvn version\n</code></pre>"},{"location":"jenkinsx/jx-pipelines/#parallelization","title":"Parallelization","text":"<pre><code>pipelineConfig:\n  pipelines:\n    release:\n      pipeline:\n        stages:\n          - name: \"Parallelsss\"\n            agent:\n              image: maven\n            parallel:\n              - name: \"Parallel1\"\n                agent:\n                  image: maven\n                steps:\n                  - command: echo\n                    args:\n                      - test one a\n                  - command: sleep\n                    args:\n                      - \"60\"\n              - name: \"Parallel2\"\n                agent:\n                  image: maven\n                steps:\n                  - command: echo\n                    args:\n                      - test two a\n                  - command: sleep\n                    args:\n                      - \"60\"\n</code></pre> <p>Caution</p> <p>Each parallel stage will have its own Pod and thus they do not share the same workspace!</p> <p>Which in the activity log will look like this:</p> <pre><code>joostvdg/jx-spring-boot-11/master #13                      1m58s          Running\n  meta pipeline                                            1m58s      49s Succeeded\n    Credential Initializer T6bkh                           1m58s       0s Succeeded\n    Working Dir Initializer 7qqhg                          1m58s       0s Succeeded\n    Place Tools                                            1m58s       1s Succeeded\n    Git Source Meta Joostvdg Jx Spring Boot 11 Bj2lh       1m57s       7s Succeeded \n    Git Merge                                              1m50s       0s Succeeded\n    Merge Pull Refs                                        1m50s       0s Succeeded\n    Create Effective Pipeline                              1m50s       8s Succeeded\n    Create Tekton Crds                                     1m42s      33s Succeeded\n  Parallelsss                                               1m8s          Running\n  Parallelsss / Parallel1                                   1m8s     1m7s Succeeded\n    Credential Initializer H2lh9                            1m8s       0s Succeeded\n    Working Dir Initializer Nlk6b                           1m8s       0s Succeeded\n    Place Tools                                             1m8s       2s Succeeded\n    Git Source Joostvdg Jx Spring Boot 11 Mast 96j7m        1m6s       4s Succeeded \n    Git Merge                                               1m2s       1s Succeeded\n    Step2                                                   1m1s       0s Succeeded\n    Step3                                                   1m1s     1m0s Succeeded\n  Parallelsss / Parallel2                                   1m8s          Running\n    Credential Initializer Shc5t                            1m8s       0s Succeeded\n    Working Dir Initializer Tsx5j                           1m8s       2s Succeeded\n    Place Tools                                             1m6s       1s Succeeded\n    Git Source Joostvdg Jx Spring Boot 11 Mast 8dbcl        1m5s       5s Succeeded\n    Git Merge                                               1m0s       1s Succeeded\n    Step2                                                    59s       0s Succeeded\n</code></pre>"},{"location":"jenkinsx/jx-pipelines/#other-resources","title":"Other Resources","text":"<ul> <li>https://jenkins-x.io/docs/concepts/jenkins-x-pipelines/</li> <li>https://jenkins-x.io/docs/reference/pipeline-syntax-reference/</li> <li>https://jenkins-x.io/docs/reference/components/build-packs//#pipeline-extension-model</li> <li>https://technologyconversations.com/2019/06/30/overriding-pipelines-stages-and-steps-and-implementing-loops-in-jenkins-x-pipelines/</li> <li>https://docs.cloudbees.com/docs/cloudbees-jenkins-x-distribution/latest/pipelines/#_extending_pipelines</li> </ul>"},{"location":"jenkinsx/jxboot-aks/","title":"Jenkins X with Boot on AKS","text":"<p>This guide is about installing Jenkins X via <code>jx boot</code> on AKS, including leveraging Google's CloudDNS for dynamic DNS configuration (for ingress).</p>"},{"location":"jenkinsx/jxboot-aks/#create-aks-cluster","title":"Create AKS Cluster","text":"<p>Either create a cluster via AKS Terraform (recommended) or via AKS CLI.</p>"},{"location":"jenkinsx/jxboot-aks/#install-jenkins-x","title":"Install Jenkins X","text":""},{"location":"jenkinsx/jxboot-aks/#boot-config","title":"Boot Config","text":"<p>Make a fork of the jenkins-x-boot-config repository and clone it.</p> <pre><code>GH_USER=\n</code></pre> <pre><code>git clone https://github.com/${GH_USER}/jenkins-x-boot-config.git\ncd jenkins-x-boot-config\n</code></pre> <p>Changes to make:</p> <ul> <li>provider from <code>gke</code> to <code>aks</code></li> <li>set domain</li> <li>set clustername</li> <li>set external dns (see below)</li> <li>set repository value for each environments (not dev) as below</li> </ul> <pre><code>- key: staging\n  repository: environment-jx-aks-staging\n</code></pre>"},{"location":"jenkinsx/jxboot-aks/#external-dns","title":"External DNS","text":"<p>Using Google CloudDNS:</p> <ul> <li>login to the GCP account you want to use</li> <li>enable CloudDNS API by going to it</li> <li>create a CloudDNS zone for your subdomain<ul> <li>if the main domain is <code>example.com</code> -&gt; <code>aks.example.com</code></li> <li>once created, you get <code>NS</code> entries, copy these (usualy in the form <code>ns-cloud-X{1-4}.googledomains.com</code></li> </ul> </li> <li>in your Domain's DNS configuration, map your subdomain to these <code>NS</code> entries</li> <li>create a service account that can use CloudDNS API</li> <li>add the Google Project to which the Service Account belongs to: <code>jx-requirements.yaml</code> and <code>values.yaml</code></li> <li>export the <code>json</code> configuration file<ul> <li>rename the file to <code>credentials.json</code></li> </ul> </li> <li>create secret a secret in Kubernetes<ul> <li><code>kubectl create secret generic external-dns-gcp-sa --from-file=credentials.json</code></li> </ul> </li> <li>fix external dns values template -&gt; <code>systems/external-dns/values.tmpl.yaml</code><ul> <li>add <code>project: \"{{ .Requirements.cluster.project }}\"</code> to <code>external-dns</code>.<code>google</code></li> </ul> </li> </ul> <p>Important</p> <p>You have to create the secret <code>external-dns-gcp-sa</code> in every namespace you set up TLS via the <code>dns01</code> challenge.</p> jx-requirements.yaml <p>We're omitting the default values as much as possible, such as the <code>dev</code> and <code>production</code> environments.</p> <pre><code>cluster:\nenvironmentGitOwner: &lt;YOUR GITHUB ACCOUNT&gt;\ngitKind: github\ngitName: github\ngitServer: https://github.com\nnamespace: jx\nproject: your-google-project\nprovider: aks\nenvironments:\n- ingress:\n    domain: staging.aks.example.com\n    externalDNS: true\n    namespaceSubDomain: \"\"\n    tls:\n    email: &lt;YOUR EMAIL ADDRESS&gt;\n    enabled: true\n    production: true\nkey: staging\nrepository: environment-jx-aks-staging\ngitops: true\ningress:\ndomain: aks.example.com\nexternalDNS: true\nnamespaceSubDomain: -jx.\ntls:\n    email: &lt;YOUR EMAIL ADDRESS&gt;\n    enabled: true\n    production: true\nkaniko: true\nsecretStorage: local\n</code></pre> values.yaml <pre><code>cluster:\n    projectID: your-google-project\n</code></pre>"},{"location":"jenkinsx/jxboot-aks/#tls-config","title":"TLS Config","text":"<p>Update the <code>jx-requirements.yaml</code>, make sure <code>ingress</code> configuration is correct:</p> <pre><code>ingress:\n  domain: aks.example.com\n  externalDNS: true\n  namespaceSubDomain: -jx.\n  tls:\n    email: admin@example.com\n    enabled: true\n    production: true\n</code></pre> <p>If all is done correctly with the CloudDNS configuration, the external dns will contain all the entries of the <code>jx</code> services (such as hook, chartmuseum) and certmanager will be able to verify the domain with Let's Encrypt.</p>"},{"location":"jenkinsx/jxboot-aks/#docker-registry-config","title":"Docker Registry Config","text":"<p>values.yaml</p> <pre><code>jenkins-x-platform:\n    dockerRegistry: myacr.azurecr.io\n</code></pre> <p>This was not enough, added it to the values template: <code>env/jenkins-x-platform/values.tmpl.yaml</code></p> <pre><code>dockerRegistry: myacr.azurecr.io\n</code></pre>"},{"location":"jenkinsx/jxboot-aks/#tls-for-application-in-environment","title":"TLS For Application In Environment","text":"<ul> <li>create issuer</li> <li>create certificate</li> </ul> <p>Note</p> <p>This implies you need to run <code>jx boot</code> at least once before working on your environment configuration!</p> <p>Easiest way I found, was to copy the yaml from the issuer and certificate in the <code>jx</code> namespace. You then remove the unnecesary elements, those generated by Kubernetes itself (such as creation date, status, etc).</p> <p>You have to change the domain name and hosts values, as they should now point to the subdomain corresponding to this environment (unless its production). Once the files are good, you add them to your environment. You do so, by adding them to the templates folder -&gt; <code>env/templates</code>.</p> <pre><code>.\n\u251c\u2500\u2500 Jenkinsfile\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 env\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Chart.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 requirements.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 templates\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 certificate.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 issuer.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 values.yaml\n\u2514\u2500\u2500 jenkins-x.yml\n</code></pre> <pre><code>kubectl -n jx get issuer letsencrypt-prod -o yaml\n</code></pre> <pre><code>kubectl -n jx get certificate tls-&lt;unique to your cluster&gt;-p -o yaml\n</code></pre> issuer.yaml <p>The end result should look like this:</p> <pre><code>apiVersion: certmanager.k8s.io/v1alpha1\nkind: Issuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    email: admin@example.com\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    server: https://acme-v02.api.letsencrypt.org/directory\n    solvers:\n    - dns01:\n        clouddns:\n          project: your-google-project\n          serviceAccountSecretRef:\n            key: credentials.json\n            name: external-dns-gcp-sa\n      selector:\n        dnsNames:\n        - '*.staging.aks.example.com'\n        - staging.aks.example.com\n</code></pre> certificate.yaml <pre><code>apiVersion: certmanager.k8s.io/v1alpha1\nkind: Certificate\nmetadata:\n    name: tls-staging-aks-example-com-p\nspec:\n    commonName: '*.staging.aks.example.com'\n    dnsNames:\n    - '*.staging.aks.example.com'\n    issuerRef:\n        name: letsencrypt-prod\n    secretName: tls-staging-aks-example-com-p\n</code></pre>"},{"location":"jenkinsx/jxboot/","title":"Jenkins X Boot","text":""},{"location":"jenkinsx/jxboot/#what-is-jenkins-x-boot","title":"What Is Jenkins X Boot","text":"<p>Jenkins X Boot is a way to install Jenkins X via the principles of Configuration-as-Code and GitOps.</p> <p>What it does, is to run an initial pipeline that creates a <code>dev</code> environment repository.  This repository contains all the information in order to install Jenkins X.</p> <p>Once the <code>dev</code> environment (in Kubernetes) and it's backing git repository are created, it can run a pipeline to install the rest of Jenkins X.</p>"},{"location":"jenkinsx/jxboot/#process","title":"Process","text":"<ol> <li>You start with a configuration repository (boot config)<ul> <li>if you don't have one, Jenkins X will first create one for you</li> </ul> </li> <li>From there, Jenkins X will create the <code>dev</code> environment repository</li> <li>Next Jenkins X leverages the repository to create the <code>dev</code> environment in your Kubernetes cluster</li> <li>The pipeline of this dev environment will checkout the dev repository and install Jenkins X</li> </ol>"},{"location":"jenkinsx/jxboot/#architecture","title":"Architecture","text":""},{"location":"jenkinsx/jxboot/#cloudbees-jenkins-x-distribution","title":"CloudBees Jenkins X Distribution","text":"<p>CloudBees Jenkins X Distribution (or <code>CJXD</code>) is a version of Jenkins X maintained and supported by CloudBees. It is often abreviated to <code>CJXD</code> for convenience.</p> <p>There are three main differences between regular Jenkins X and CloudBees Jenkins X:</p> <ul> <li>there is more focus on stabilty and less on features, the release cadence is therefor once a month</li> <li>this distribution allows you to get paid support from CloudBees</li> <li>it contains a Web UI</li> </ul> <p>As Jenkins X, this is also free (as in beer) software.</p>"},{"location":"jenkinsx/jxboot/#get-cjxd","title":"Get CJXD","text":"<p>To use CJXD, you have to download the CloudBees distribution rather than relying on installes such as Homebrew or Chocolatey.</p> <p>Before you can then leverage the additional features included - such as the UI - you have to set the profile to CloudBees. To do so, run <code>jx profile cloudbees</code>.</p>"},{"location":"jenkinsx/jxboot/#initialize","title":"Initialize","text":"<ul> <li>install jx cli binary</li> <li>create a Kubernetes cluster</li> <li>initialize <code>jx</code> via jx boot -&gt; <code>jx boot</code></li> <li>update <code>jx-requirements.yml</code> of your boot configuration  repository</li> <li></li> </ul>"},{"location":"jenkinsx/jxboot/#install-on-gke","title":"Install On GKE","text":"<p>First, install a Kubernetes Cluster on Google Cloud GKE. Either directly via Google's gcloud CLI or via Terraform.</p>"},{"location":"jenkinsx/jxboot/#things-to-know","title":"Things To Know","text":""},{"location":"jenkinsx/jxboot/#single-service-account","title":"Single Service Account","text":"<p>Currently - as of November 2019 - Jenkins X with <code>jx boot</code> on GKE still uses the default service account of the account used to create the cluster.</p> <p>This is not the ideal situation, and is being worked upon. You can track the progress via GitHub issue #5856.</p>"},{"location":"jenkinsx/jxboot/#useful-commands","title":"Useful Commands","text":"<p>Restart From Specific Step</p> <p>If you do not want to redo the entire <code>jx boot</code> process, you can start from a specific step.</p> <p>For example, if you changed the envionments configuration, you can start with the step <code>install-env</code>.</p> <pre><code>jx boot --start-step install-env\n</code></pre> <p>Confirm Jenkins X Status</p> <pre><code>jx status\n</code></pre>"},{"location":"jenkinsx/jxboot/#add-cjxd-ui","title":"Add CJXD UI","text":"<p>If you use the CloudBees Jenkins X Distribution (CJXD), you can also install a web UI.</p> <pre><code>jx add app jx-app-ui --version=0.1.2\n</code></pre> <p>This will make a PR, which, when merged launches a Master Promotion build.</p> <pre><code>jx get activities --watch --filter environment-dev\n</code></pre> <pre><code>jx get build log\n</code></pre> <pre><code>jx ui -p 8081\n</code></pre>"},{"location":"jenkinsx/jxboot/#add-environment","title":"Add Environment","text":"<p>As of this writing - November 2019 - adding a new environment to Jenkins X with Config as Code is a bit tedious.</p> <ul> <li>https://jenkins-x.io/docs/managing-jx/faq/boot/#how-do-i-add-new-environments</li> </ul>"},{"location":"jenkinsx/jxboot/#create-environment","title":"Create Environment","text":"<ul> <li>copy existing resources into new (<code>env/templates/</code>) see <code>Get Resources</code></li> <li>update values to suit new environment</li> <li>execute <code>jx boot</code></li> <li>configure tls</li> <li>cloud dns zone</li> <li>external dns secret</li> <li>dns domain forward</li> <li>add cert and issuer to env repo</li> <li>if the environment has a separate domain, edit <code>exdns-external-dns</code> deployment</li> <li><code>kubectl edit deployment -n jx exdns-external-dns</code></li> </ul>"},{"location":"jenkinsx/jxboot/#get-resources","title":"Get Resources","text":"<pre><code>kubectl get env staging -oyaml &gt; env/templates/cb.yaml\nkubectl get sr joostvdg-env-cjxd-staging -oyaml &gt; env/templates/cb-sr.yaml\n</code></pre>"},{"location":"jenkinsx/jxboot/#configure-tls","title":"Configure TLS","text":""},{"location":"jenkinsx/jxboot/#configure-certificate-issuer","title":"Configure Certificate &amp; Issuer","text":"<p>If you want TLS certificates for your new Environment, you will configure this yourself.</p> <p>The easiest way is to copy the current <code>CertificateIssuer</code> and <code>Certificate</code> from <code>jx</code> namespace and modify the data.</p> <pre><code>kubectl get issuer -n jx letsencrypt-prod -oyaml &gt; env/templates/issuer.yaml\nkubectl get cert -n jx tls-dev-cjxd-kearos-net-p -oyaml &gt; env/templates/certificate.yaml\n</code></pre> <ul> <li>Rename the namespace to your namespace</li> <li>Rename <code>domain</code> and <code>host</code> names to the new (sub-)domain</li> <li>Remove <code>status</code> segment</li> <li>Remove kubernetes managed fields (<code>uuid</code>, timestamps, etc)</li> </ul> <pre><code>git add env/\ngit commit -m \"add certs\"\n</code></pre> <pre><code>git push\njx get activities -w\n</code></pre>"},{"location":"jenkinsx/jxboot/#create-cloud-dns-zone","title":"Create Cloud DNS Zone","text":"<p>If you leverage Cloud DNS and your new environment has a unique sub-domain, you have to create a new zone.</p> <p>A zone in Google's CloudDNS has three values:</p> <ul> <li>name: the name of the configuration item in GCP</li> <li>description: the description of the configuration item</li> <li>DNS Name: the actual DNS name</li> </ul> <pre><code>ZONE_NAME=cb-cjxd-kearos-net\nDESCRIPTION=\"joostvdg - cb env for cjxd\"\nDNS_NAME=cb.cjxd.kearos.net\n</code></pre> <pre><code>gcloud dns managed-zones create ${ZONE_NAME} --description=${DESCRIPTION} --dns-name=${DNS_NAME}\n</code></pre> <p>Once you've created the new zone, you will have to update your Domain provider to point the subdomain to the DNS servers of this new Zone.</p>"},{"location":"jenkinsx/jxboot/#configure-clouddns-secrets","title":"Configure CloudDNS Secrets","text":"<p>If you leverage CloudDNS, you will also have to copy over the CloudDNS configuration from the <code>jx</code> namespace to your new Environment's namespace.</p> <pre><code>kubectl get secret -n jx exdns-external-dns-token-cq5mv -oyaml &gt; exdns-external-dns-token-env-cb.yaml\nkubectl get secret -n jx external-dns-gcp-sa -oyaml &gt; external-dns-gcp-sa-env-cb.yaml\n</code></pre> <ul> <li>Rename the namespace to your namespace</li> <li>remove <code>status</code> segment</li> <li>remove kubernetes managed fields (<code>uuid</code>, timestamps, etc)</li> </ul> <pre><code>kubectl apply -f exdns-external-dns-token-env-cb.yaml\nkubectl apply -f external-dns-gcp-sa-env-cb.yaml\n</code></pre>"},{"location":"jenkinsx/jxboot/#confirm-certificate-works","title":"Confirm Certificate Works","text":"<pre><code>kubectl get cert -n cloudbees\n</code></pre>"},{"location":"jenkinsx/jxboot/#common-issues","title":"Common Issues","text":""},{"location":"jenkinsx/jxboot/#imagepullbackup","title":"ImagePullBackup","text":"<pre><code>events:\n  Type     Reason     Age                   From                                         Message\n  ----     ------     ----                  ----                                         -------\n  Normal   Scheduled  18m                   default-scheduler                            Successfully assigned jx-staging/jx-jx-qs-spring-boot-6-58b75446b4-pkd7x to gke-joost-cjxd-pool2-54e21b2f-hlhd\n  Normal   Pulling    16m (x4 over 18m)     kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd  Pulling image \"gcr.io/ps-dev-201405/jx-qs-spring-boot-6:0.0.1\"\n  Warning  Failed     16m (x4 over 18m)     kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd  Failed to pull image \"gcr.io/ps-dev-201405/jx-qs-spring-boot-6:0.0.1\": rpc error: code = Unknown desc = Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication\n  Warning  Failed     16m (x4 over 18m)     kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd  Error: ErrImagePull\n  Normal   BackOff    8m24s (x42 over 18m)  kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd  Back-off pulling image \"gcr.io/ps-dev-201405/jx-qs-spring-boot-6:0.0.1\"\n  Warning  Failed     3m18s (x64 over 18m)  kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd  Error: ImagePullBackOff\n</code></pre> <p>Then you're missing scopes in your GKE Node's.</p> <pre><code>resource \"google_container_node_pool\" \"nodepool2\" {\n  ... \n  node_config {\n    machine_type = \"n2-standard-2\"\n    oauth_scopes = [\n      \"https://www.googleapis.com/auth/compute\",\n      \"https://www.googleapis.com/auth/devstorage.read_only\",\n      \"https://www.googleapis.com/auth/logging.write\",\n      \"https://www.googleapis.com/auth/monitoring\",\n    ]\n  }\n  ...\n}\n</code></pre>"},{"location":"jenkinsx/jxboot/#unable-to-enable-dns-api","title":"Unable To Enable DNS API","text":"<pre><code>valid: there is a Secret: external-dns-gcp-sa in namespace: jx\nerror: unable to enable 'dns' api: failed to run 'gcloud services list --enabled --project XXXXXX' command in directory '', output: 'ERROR: (gcloud.services.list) User [XXXXXX857-compute@developer.gserviceaccount.com] does not have permission to access project [XXXXXX] (or it may not exist): Request had insufficient authentication scopes.'\n</code></pre> <pre><code>valid: there is a Secret: external-dns-gcp-sa in namespace: jx\nerror: unable to enable 'dns' api: failed to run 'gcloud services list --enabled --project GCP PROJECT B' command in directory '', output: 'ERROR: (gcloud.services.list) User [389413650857-compute@developer.gserviceaccount.com] does not have permission to access project [GCP PROJECT B] (or it may not exist): Request had insufficient authentication scopes.'\n\nPipeline failed on stage 'release' : container 'step-create-install-values'. The execution of the pipeline has stopped.\n</code></pre> <ul> <li>Have to enable \"Service Usage API\": https://console.developers.google.com/apis/api/serviceusage.googleapis.com/overview?project= <li>node pools (for example in terraform) need access to the security scope <code>https://www.googleapis.com/auth/cloud-platform</code></li> <pre><code>oauth_scopes = [\n    \"https://www.googleapis.com/auth/cloud-platform\"\n]\n</code></pre>"},{"location":"jenkinsx/jxboot/#certmanager-complaining-about-the-wrong-domain","title":"Certmanager complaining about the wrong domain","text":"<p>In case Cert-Manager is complaining that while validating <code>x.y.example.com</code> it cannot find <code>example.com</code>.</p> <p>See: https://github.com/jetstack/cert-manager/issues/1507</p> <pre><code>I1104 09:13:33.884549       1 base_controller.go:187] cert-manager/controller/challenges \"level\"=0 \"msg\"=\"syncing item\" \"key\"=\"cloudbees/tls-cb-cjxd-kearos-net-p-2383487961-0\"\nI1104 09:13:33.884966       1 dns.go:104] cert-manager/controller/challenges/Present \"level\"=0 \"msg\"=\"presenting DNS01 challenge for domain\" \"dnsName\"=\"cloudbees.cjxd.kearos.net\" \"domain\"=\"cloudbees.cjxd.kearos.net\" \"resource_kind\"=\"Challenge\" \"resource_name\"=\"tls-cb-cjxd-kearos-net-p-2383487961-0\" \"resource_namespace\"=\"cloudbees\" \"type\"=\"dns-01\"\nE1104 09:13:34.141122       1 base_controller.go:189] cert-manager/controller/challenges \"msg\"=\"re-queuing item  due to error processing\" \"error\"=\"No matching GoogleCloud domain found for domain kearos.net.\" \"key\"=\"cloudbees/tls-cb-cjxd-kearos-net-p-2383487961-0\"\n</code></pre>"},{"location":"jenkinsx/jxboot/#reset-installation","title":"Reset Installation","text":""},{"location":"jenkinsx/jxboot/#remove-environment-repos","title":"Remove Environment Repos","text":"<ul> <li>go to git and remove environment repo</li> <li>clear jx boot config folder</li> <li>clear cloud dns config (if used)</li> <li>clear requirements from env repo's if re-used</li> </ul>"},{"location":"jenkinsx/jxboot/#recreate-git-token","title":"Recreate Git Token","text":"<p>If for some reason the git token is invalid, you can recreate it with the commands below.</p> <pre><code>jx delete git token -n github &lt;yourUserName&gt;\njx create git token -n github &lt;yourUserName&gt;\n</code></pre>"},{"location":"jenkinsx/jxboot/#references","title":"References","text":"<ul> <li>Jenkins X - Managing FAQ</li> <li>Jenkins X - Using FAQ</li> <li>Jenkins X - Pipeline FAQ</li> <li>Jenkins X - Boot FAQ</li> <li>Jenkins X - Configure CloudDNS</li> <li>Jenkins X - Add New Environment</li> </ul>"},{"location":"jenkinsx/kubernetes-days/","title":"Jenkins X - Kubernetes Days","text":""},{"location":"jenkinsx/kubernetes-days/#prepare","title":"Prepare","text":"<pre><code>asciinema rec first.cast\n</code></pre> <pre><code>asciinema play -i 2 first.cast\n</code></pre> <ul> <li><code>-i</code> -&gt; play back with max of 2 seconds of idleness</li> <li><code>-s</code> -&gt; play back with double speed</li> </ul>"},{"location":"jenkinsx/kubernetes-days/#process","title":"Process","text":"<ul> <li>Create cluster</li> <li>Create quickstart </li> <li>Gitops</li> <li>Promotion</li> <li>Pr</li> <li><code>jx boot</code></li> </ul>"},{"location":"jenkinsx/kubernetes-days/#commands","title":"Commands","text":"<pre><code>asciinema rec jx-k8s-days-00-logo.cast\n</code></pre> <pre><code>  ______  __        ______    __    __   _______        .__   __.      ___   .___________. __  ____    ____  _______                     \n /      ||  |      /  __  \\  |  |  |  | |       \\       |  \\ |  |     /   \\  |           ||  | \\   \\  /   / |   ____|                    \n|  ,----'|  |     |  |  |  | |  |  |  | |  .--.  |______|   \\|  |    /  ^  \\ `---|  |----`|  |  \\   \\/   /  |  |__                       \n|  |     |  |     |  |  |  | |  |  |  | |  |  |  |______|  . `  |   /  /_\\  \\    |  |     |  |   \\      /   |   __|                      \n|  `----.|  `----.|  `--'  | |  `--'  | |  '--'  |      |  |\\   |  /  _____  \\   |  |     |  |    \\    /    |  |____                     \n \\______||_______| \\______/   \\______/  |_______/       |__| \\__| /__/     \\__\\  |__|     |__|     \\__/     |_______|                    \n\n                                      ______  __       ___ ______  _______                                                               \n                                     /      ||  |     /  //      ||       \\                                                              \n                                    |  ,----'|  |    /  /|  ,----'|  .--.  |                                                             \n                                    |  |     |  |   /  / |  |     |  |  |  |                                                             \n                                    |  `----.|  |  /  /  |  `----.|  '--'  |                                                             \n                                     \\______||__| /__/    \\______||_______/                                                              \n\n____    __    ____  __  .___________. __    __               __   _______ .__   __.  __  ___  __  .__   __.      _______.      ___   ___ \n\\   \\  /  \\  /   / |  | |           ||  |  |  |             |  | |   ____||  \\ |  | |  |/  / |  | |  \\ |  |     /       |      \\  \\ /  / \n \\   \\/    \\/   /  |  | `---|  |----`|  |__|  |             |  | |  |__   |   \\|  | |  '  /  |  | |   \\|  |    |   (----`       \\  V  /  \n  \\            /   |  |     |  |     |   __   |       .--.  |  | |   __|  |  . `  | |    &lt;   |  | |  . `  |     \\   \\            &gt;   &lt;   \n   \\    /\\    /    |  |     |  |     |  |  |  |       |  `--'  | |  |____ |  |\\   | |  .  \\  |  | |  |\\   | .----)   |          /  .  \\  \n    \\__/  \\__/     |__|     |__|     |__|  |__|        \\______/  |_______||__| \\__| |__|\\__\\ |__| |__| \\__| |_______/          /__/ \\__\\ \n</code></pre> <pre><code>asciinema play jx-k8s-days-00-logo.cast\n</code></pre>"},{"location":"jenkinsx/kubernetes-days/#create-cluster","title":"Create Cluster","text":"<pre><code>export NAMESPACE=cd\nexport PROJECT=\n</code></pre> <pre><code>asciinema rec jx-k8s-days-01-create.cast\n</code></pre> <pre><code>jx create cluster gke -n jx-rocks -p $PROJECT -r us-east1 \\\n    -m n1-standard-4 --min-num-nodes 1 --max-num-nodes 2 \\\n    --default-admin-password=admin \\\n    --default-environment-prefix jx-rocks --git-provider-kind github \\\n    --namespace $NAMESPACE --prow --tekton\n</code></pre> <pre><code>asciinema play -i 1 -s 4 jx-k8s-days-01-create.cast\n</code></pre>"},{"location":"jenkinsx/kubernetes-days/#create-quickstart-go","title":"Create QuickStart Go","text":"<pre><code>asciinema rec jx-k8s-days-02-quickstart.cast\n</code></pre> <pre><code>export APP_NAME=jx-k8s-days-go-02\n</code></pre> <pre><code>jx create quickstart --filter golang-http --project-name ${APP_NAME} --batch-mode\n</code></pre> <pre><code>ls -l ${APP_NAME}\n</code></pre> <pre><code>jx get activity -f ${APP_NAME} -w\n</code></pre> <pre><code>jx get pipelines\n</code></pre> <pre><code>jx get applications -e staging\n</code></pre> <pre><code>http \"http://${APP_NAME}.cd-staging.35.185.41.106.nip.io\"\n</code></pre> <pre><code>jx get build logs -f ${APP_NAME}  # Cancel with ctrl+c\n</code></pre> <pre><code>cd ${APP_NAME}\nvim main.go\n</code></pre> <pre><code>jx get activity -f ${APP_NAME} -w\n</code></pre> <pre><code>jx get applications -e staging\n</code></pre> <pre><code>http \"http://${APP_NAME}.cd-staging.35.185.41.106.nip.io\"\n</code></pre> <pre><code>asciinema play -i 2 -s 2 jx-k8s-days-02-quickstart.cast\n</code></pre>"},{"location":"jenkinsx/kubernetes-days/#import-project","title":"Import Project","text":"<pre><code> __  .___  ___. .______     ______   .______     .___________.    __________   ___  __       _______.___________..__   __.   _______ \n|  | |   \\/   | |   _  \\   /  __  \\  |   _  \\    |           |   |   ____\\  \\ /  / |  |     /       |           ||  \\ |  |  /  _____|\n|  | |  \\  /  | |  |_)  | |  |  |  | |  |_)  |   `---|  |----`   |  |__   \\  V  /  |  |    |   (----`---|  |----`|   \\|  | |  |  __  \n|  | |  |\\/|  | |   ___/  |  |  |  | |      /        |  |        |   __|   &gt;   &lt;   |  |     \\   \\       |  |     |  . `  | |  | |_ | \n|  | |  |  |  | |  |      |  `--'  | |  |\\  \\----.   |  |        |  |____ /  .  \\  |  | .----)   |      |  |     |  |\\   | |  |__| | \n|__| |__|  |__| | _|       \\______/  | _| `._____|   |__|        |_______/__/ \\__\\ |__| |_______/       |__|     |__| \\__|  \\______| \n\n     ___      .______   .______    __       __    ______     ___   .___________. __    ______   .__   __.                            \n    /   \\     |   _  \\  |   _  \\  |  |     |  |  /      |   /   \\  |           ||  |  /  __  \\  |  \\ |  |                            \n   /  ^  \\    |  |_)  | |  |_)  | |  |     |  | |  ,----'  /  ^  \\ `---|  |----`|  | |  |  |  | |   \\|  |                            \n  /  /_\\  \\   |   ___/  |   ___/  |  |     |  | |  |      /  /_\\  \\    |  |     |  | |  |  |  | |  . `  |                            \n /  _____  \\  |  |      |  |      |  `----.|  | |  `----./  _____  \\   |  |     |  | |  `--'  | |  |\\   |                            \n/__/     \\__\\ | _|      | _|      |_______||__|  \\______/__/     \\__\\  |__|     |__|  \\______/  |__| \\__|                            \n</code></pre> <pre><code>asciinema rec jx-k8s-days-03-import.cast\n</code></pre> <pre><code>git clone https://github.com/joostvdg/go-demo-6.git\n</code></pre> <pre><code>cd go-demo-6\ngit checkout orig\ngit merge -s ours master --no-edit\ngit checkout master\ngit merge orig\nrm -rf charts\nls -lath\n</code></pre> <pre><code>git push\n\njx import --batch-mode\n</code></pre> <pre><code>jx get activities --filter go-demo-6 --watch\n</code></pre> <pre><code>jx get applications\n</code></pre> <pre><code>kubectl --namespace cd-staging logs -l app=jx-go-demo-6\n</code></pre> <pre><code>echo \"dependencies:\n- name: mongodb\n  alias: go-demo-6-db\n  version: 5.3.0\n  repository:  https://kubernetes-charts.storage.googleapis.com\n  condition: db.enabled\" &gt; charts/go-demo-6/requirements.yaml\n</code></pre> <pre><code>cat charts/go-demo-6/requirements.yaml\n</code></pre> <pre><code>vim charts/go-demo-6/templates/deployment.yaml\n</code></pre> <pre><code>        env:\n        - name: DB\n          value: {{ template \"fullname\" . }}-db\n</code></pre> <pre><code>vim charts/go-demo-6/values.yaml\n</code></pre> <pre><code>probePath: /demo/hello?health=true\n</code></pre> <pre><code>git status\ngit add charts/\ngit commit -m \"add db dependency\"\ngit push\n</code></pre> <pre><code>jx get activities --filter go-demo-6 --watch\n</code></pre> <pre><code>jx get applications\n</code></pre> <pre><code>http http://go-demo-6.cd-staging.35.185.41.106.nip.io/demo/hello\n</code></pre> <pre><code>jx delete application\n</code></pre> <pre><code>rm -rf go-demo-6\n</code></pre> <pre><code>jx get applications\n</code></pre> <pre><code>asciinema play -i 2 -s 2 jx-k8s-days-03-import.cast\n</code></pre>"},{"location":"jenkinsx/kubernetes-days/#preview-environments","title":"Preview Environments","text":"<pre><code>APP_NAME=jx-k8s-days-go-01\n</code></pre> <pre><code>asciinema rec jx-k8s-days-04-preview.cast\n</code></pre> <pre><code>jx get applications\n</code></pre> <pre><code>cd ${APP_NAME}\n</code></pre> <pre><code>git checkout -b my-new-pr-3\n</code></pre> <pre><code>vim main.go\n</code></pre> <pre><code>git status\ngit add main.go\ngit commit -m \"change message\"\ngit push --set-upstream origin my-new-pr-3\n</code></pre> <pre><code>jx create pullrequest \\\n    --title \"My PR\" \\\n    --body \"This is the text that describes the PR\" \\\n    --batch-mode\n</code></pre> <pre><code>open pr link\n</code></pre> <pre><code>jx get previews\n</code></pre> <pre><code>http ..\n</code></pre> <pre><code>add /lgtm to pr\nmerge pr\n</code></pre> <pre><code>jx get activity --filter jx-k8s-day-go-01 --watch\n</code></pre> <pre><code>jx get applications\n</code></pre> <pre><code>git checkout master\ngit pull\ncd ..\n</code></pre> <pre><code>jx get previews\njx gc previews\njx get previews\n</code></pre> <pre><code>asciinema play -i 2 -s 2 jx-k8s-days-04-preview.cast\n</code></pre>"},{"location":"jenkinsx/kubernetes-days/#env","title":"Env","text":"<pre><code>jx create environment\n</code></pre>"},{"location":"jenkinsx/kubernetes-days/#reel","title":"Reel","text":"<pre><code>for VARIABLE in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17\ndo\n    asciinema play jx-k8s-days-00-logo.cast\n    asciinema play -i 2 -s 3 jx-k8s-days-01-create.cast\n    asciinema play -i 2 -s 1 jx-k8s-days-02-quickstart.cast\n    asciinema play -i 2 -s 1 jx-k8s-days-03-import.cast\n    asciinema play -i 2 -s 2 jx-k8s-days-04-preview.cast\ndone\n</code></pre>"},{"location":"jenkinsx/lighthouse-bitbucket/","title":"Jenkins X - Lighthouse &amp; Bitbucket","text":"<p>This guide is about using Jenkins X with Lighthouse<sup>1</sup> as webhook manager and Bitbucket for the environment repositories<sup>2</sup>.</p>"},{"location":"jenkinsx/lighthouse-bitbucket/#run-bitbucket-on-kubernetes","title":"Run Bitbucket on Kubernetes","text":"<p>Unfortunately, Atlassian doesn't have an officially supported Bitbucket for Kubernetes.</p> <p>So I've taken the courtesy of creating my own basic configuration - read, not production ready.</p> <p>service.yaml</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nlabels:\n    app: bitbucket\nname: bitbucket\nnamespace: default\nspec:\nports:\n- name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\nselector:\n    app: bitbucket\nsessionAffinity: None\ntype: ClusterIP\n</code></pre> <p>ingress.yaml</p> <p>I've taken the assumption that your cluster supports Ingress resources (even if its an OpenShift cluster).</p> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: bitbucket\n  namespace: default\nspec:\n  rules:\n  - host: bitbucket.openshift.example.com\n    http:\n      paths:\n      - backend:\n          serviceName: bitbucket\n          servicePort: 80\n</code></pre> <p>stateful-set.yaml</p> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: bitbucket\n  namespace: default\nspec:\n  serviceName: \"bitbucket\"\n  replicas: 1\n  selector:\n    matchLabels:\n      app: bitbucket\n  template:\n    metadata:\n      labels:\n        app: bitbucket\n    spec:\n      containers:\n      - name: bitbucket\n        image: atlassian/bitbucket-server:7.0.0\n        ports:\n        - containerPort: 7990\n          name: http\n        - containerPort: 7999\n          name: web\n        volumeMounts:\n        - name: data\n          mountPath: /var/atlassian/application-data/bitbucket\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>"},{"location":"jenkinsx/lighthouse-bitbucket/#jx-boot-configuration","title":"JX Boot Configuration","text":"<p>We use <code>jx boot</code><sup>3</sup> to install Jenkins X.  If we want to use Bitbucket for the environment repositories, we have to use Lighthouse<sup>1</sup><sup>4</sup>.</p> <p>In order to jx to install correctly, we have configure several parameters in the <code>jx-requirements.yml</code> with specific values. See the docs for all the possible values<sup>5</sup>.</p> <ul> <li>webhook: lighthouse:  we have to set the webhook manager to <code>lighthouse</code>, as Prow only works with GitHub</li> <li>environmentGitOwner: jx: the project in Bitbucket where the repositories need to be created</li> <li>gitKind: bitbucketserver: the <code>kind</code> of git server, in this case <code>bitbucketserver</code>, because <code>bitbucket</code> refers to Bitbucket Cloud</li> <li>gitName: bs: the name for our gitserver configuration</li> <li>gitServer: http://bitbucket.openshift.example.com: the url to our Bitbucket Server</li> <li>registry: docker.io: when not using a Public Cloud provider, you have to specify the docker registry URL, in this case, Dockerhub (which is <code>docker.io</code>)</li> <li>dockerRegistryOrg: caladreas:  when the docker registry owner - in my case, <code>caladreas</code>- is different from the git repository owner, you have to specify this via <code>dockerRegistryOrg</code></li> </ul> <p>We also have to set the storage for at least the logs. If we do not configure the storage for our logs, they will be assumed to be written to github pages of our application. That is, regardless of where our application resides. So, if you use anything other than GitHub (cloud), you have to configure the logs storage.</p> <p>The easiest solution, is to create a seperate repository for the build logs in your Bitbucket Server project.</p> <pre><code>    storage:\n      logs:\n        enabled: true\n        url: \"http://bitbucket.openshift.example.com/scm/jx/build-logs.git\"\n</code></pre> <p>If you have forgotten to set the storage before the installation, you can rectify this afterwards via the <code>jx edit storage</code> command.</p> <pre><code>jx edit storage -c logs --git-url http://bitbucket.openshift.kearos.net/scm/jx/build-logs.git  --git-branch master\n</code></pre> jx-requirements.yml <pre><code>bootConfigURL: https://github.com/jenkins-x/jenkins-x-boot-config.git\ncluster:\n  clusterName: rhos11\n  devEnvApprovers:\n  - jvandergriendt\n  environmentGitOwner: jx\n  gitKind: bitbucketserver\n  gitName: bs\n  gitServer: http://bitbucket.openshift.example.com\n  namespace: jx\n  provider: kubernetes\n  registry: docker.io\n  dockerRegistryOrg: caladreas\nenvironments:\n- ingress:\n    domain: openshift.example.com\n    namespaceSubDomain: -jx.\n  key: dev\n  repository: environment-rhos11-dev\n- ingress:\n    domain: staging.openshift.example.com\n    namespaceSubDomain: \"\"\n  key: staging\n  repository: env-rhos311-staging\n- key: production\n  repository: env-rhos311-prod\ngitops: true\ningress:\n  domain: openshift.example.com\n  namespaceSubDomain: -jx.\nkaniko: true\nrepository: nexus\nsecretStorage: local\nstorage:\n  logs:\n    enabled: true\n    url: \"http://bitbucket.openshift.example.com/scm/jx/build-logs.git\"\nversionStream:\n  ref: v1.0.361\n  url: https://github.com/jenkins-x/jenkins-x-versions.git\nwebhook: lighthouse\n</code></pre>"},{"location":"jenkinsx/lighthouse-bitbucket/#bitbucket-api-token","title":"Bitbucket API Token","text":"<p>To authenticate with Bitbucket server, Jenkins X needs a API token of a user that has admin permissions.</p> <p>First, create this user API token in Bitbucket. You can do so, via <code>Manage Account</code>(top right menu) -&gt; <code>Personal access tokens</code> -&gt; <code>Create a token</code> (top right).</p> <p>Then use the <code>jx create token addon</code><sup>6</sup> command to create the API token for Bitbucket server. Make sure to use the same <code>--name &lt;NAME&gt;</code>, as the <code>gitName</code> in your <code>jx-requirements.yml</code> file.</p> <p>Creates a new User Token for an Addon service</p> <p>For example, lets create the token for my configuration:</p> <pre><code>jx create token addon --name bs --url http://bitbucket.openshift.example.com  --api-token &lt;API_TOKEN&gt; &lt;USER&gt;\n</code></pre> <p>This should give the following response.</p> <pre><code>Created user &lt;USER&gt; API Token for addon server bs at http://bitbucket.openshift.example.com\n</code></pre>"},{"location":"jenkinsx/lighthouse-bitbucket/#installation","title":"Installation","text":"<p>Before running the Jenkins X installation with <code>jx boot</code>, make sure you meet the pre-requisites.</p>"},{"location":"jenkinsx/lighthouse-bitbucket/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Kubernetes cluster</li> <li>cluster admin access to Kubernetes cluster</li> <li>Bitbucket server</li> <li>Project in Bitbucket server</li> <li>API token in Bitbucket server</li> <li>API token for Jenkins X in the Kubernetes cluster</li> </ul> <p>Once these are met, we can install Jenkins X via <code>jx boot</code><sup>3</sup>.</p>"},{"location":"jenkinsx/lighthouse-bitbucket/#issue-with-controllerbuild","title":"Issue with controllerbuild","text":"<p>A potential issue you can run into, is that the deployment <code>jenkins-x-controllerbuild</code> fails to come up.</p> <pre><code>could not lock config file //.gitconfig: Permission denied: failed to run 'git config --global --add user.name jenkins-x-bot' command in directory '',\n</code></pre> <p>The issue here, seems to be some missing configuration, as the the two <code>/</code>'s in <code>//.gitconfig</code>, give the idea there's supposed to be some folder defined.</p> <p>A way to solve this, is to ensure we have a home folder git can write into, and tell git where this home folder is.</p> <p>The image seems to set its working directory to <code>/home/jenkins</code>, so lets use that. In order to tell git where to write its configuration to, we can set the <code>HOME</code> environment variable.</p> <p>So in the <code>jenkins-x-controllerbuild</code> deployment, set the HOME environment variable to <code>/home/jenkins</code>.</p> <pre><code>- name: HOME\n  value: /home/jenkins\n</code></pre> <p>Add folder for <code>home/jenkins</code> via volume and volumeMount.</p> <pre><code>    volumeMounts:\n    - mountPath: /home/jenkins\n      name: jenkinshome\n</code></pre> <pre><code>  volumes:\n  - name: jenkinshome\n    emptyDir: {}\n</code></pre>"},{"location":"jenkinsx/lighthouse-bitbucket/#errata","title":"Errata","text":""},{"location":"jenkinsx/lighthouse-bitbucket/#import-quickstarts-source-repositories-always-https","title":"Import &amp; Quickstarts Source Repositories Always HTTPS","text":"<p>When you add applications to Jenkins X, either via the <code>jx import</code> or <code>jx create quickstart</code> processes, a <code>SourceRepository</code> CRD gets created.</p> <p>This resource will contain the the value <code>spec.httpCloneURL</code>. This is used in the Tekton pipelines for cloning the repository. This <code>httpCloneURL</code> is always set to <code>https://</code>, even if the repository <code>url</code> is <code>http://</code>.</p> <p>To retrieve the existing source repositories, you can do the following:</p> <pre><code>kubectl get sourcerepository\n</code></pre> <p>You can edit a specific source repository via:</p> <pre><code>kubectl edit sourcerepository jx-jx-go\n</code></pre> <p>And if required, change the <code>https://</code> into a <code>http://</code>.</p>"},{"location":"jenkinsx/lighthouse-bitbucket/#pullrequest-updates","title":"PullRequest Updates","text":"<p>Bitbucket Server does not send a specific webhook when there's an update to a branch participating in a PullRequest. It only sends a generic <code>Push</code> event, which does not give Jenkins X the information required to trigger a new build for the specific PullRequest.</p> <p>Atlassian has recently add this feature in Bitbucket Server <code>7.0.0</code>, confirmed by the March 5<sup>th</sup> update in this Jira ticket.</p> <p>As of March 2020, this is not yet supported by Jenkins X, nor is it expected at this point in time to find its way into earlier releases (such as 6.x) of Bitbucket server.</p>"},{"location":"jenkinsx/lighthouse-bitbucket/#references","title":"References","text":"<ol> <li> <p>https://jenkins-x.io/docs/reference/components/lighthouse/ \u21a9\u21a9</p> </li> <li> <p>https://jenkins-x.io/docs/reference/components/lighthouse/#bitbucket-server \u21a9</p> </li> <li> <p>https://jenkins-x.io/docs/getting-started/setup/boot/how-it-works/ \u21a9\u21a9</p> </li> <li> <p>https://jenkins-x.io/docs/getting-started/setup/boot/#bitbucket-server \u21a9</p> </li> <li> <p>https://jenkins-x.io/docs/reference/config/config/#config.jenkins.io/v1.ClusterConfig \u21a9</p> </li> <li> <p>https://jenkins-x.io/commands/jx_create_token_addon/ \u21a9</p> </li> </ol>"},{"location":"jenkinsx/maven/","title":"Jenkins X + Maven + Nexus","text":"<p>The goal of this article it to demonstrate how Jenkins X works with Maven and Sonatype Nexus.</p> <p>Unless you configure otherwise, Jenkins X comes with a Nexus instance pre-configure out-of-the-box.</p>"},{"location":"jenkinsx/maven/#create-jenkins-x-cluster","title":"Create Jenkins X Cluster","text":""},{"location":"jenkinsx/maven/#static","title":"Static","text":"<p>See:</p>"},{"location":"jenkinsx/maven/#example","title":"Example","text":"<p>Here's an example for creating a standard Jenkins X installation in Google Cloud with GKE. This example uses Google Cloud and it's CLI, gcloud.</p> <p>Where:</p> <ul> <li>CLUSTER_NAME: the name of your GKE cluster</li> <li>PROJECT: the project ID of your Google Project/account (<code>gcloud config list</code>)</li> <li>REGION: the region in Google Cloud where you want to run this cluster, if you don't know, use <code>us-east1</code></li> </ul> <pre><code>jx create cluster gke \\\n    --cluster-name ${CLUSTER_NAME} \\\n    --project-id ${PROJECT} \\\n    --region ${REGION} \\\n    --machine-type n1-standard-2 \\\n    --min-num-nodes 1 \\\n    --max-num-nodes 2 \\\n    --default-admin-password=admin \\\n    --default-environment-prefix jx-rocks \\\n    --git-provider-kind github \\\n    --skip-login \\\n    --batch-mode\n</code></pre>"},{"location":"jenkinsx/maven/#serverless","title":"Serverless","text":"<p>See: </p>"},{"location":"jenkinsx/maven/#nexus","title":"Nexus","text":""},{"location":"jenkinsx/maven/#open","title":"Open","text":"<p>To open Nexus' Web UI, you can use <code>jx open</code> to see it's URL.</p> <p>By default, the URL will be <code>nexus.jx.&lt;domain&gt;</code>, for example <code>http://nexus.jx.${LB_IP}.nip.io</code>.</p> <p>I recommend using a proper Domain name and use TLS via Let's Encrypt. Jenkins X has built in support for this, via the <code>jx upgrade ingress</code> command. This is food for another article though.</p>"},{"location":"jenkinsx/maven/#credentials","title":"Credentials","text":"<p>The username will be <code>admin</code>, the password depends on you. If you specified the <code>--default-admin-password</code>, it will be that.</p> <p>If you didn't specify the password, you can find it in a Kubernetes secret.</p> <pre><code>kubectl get secret nexus -o yaml\n</code></pre> <p>Which should look like this:</p> <pre><code>apiVersion: v1\ndata:\n  password: YWRtaW4=\nkind: Secret\n</code></pre> <p>To retrieve the password, we have to decode the value of <code>password</code> with Base64. On Mac or Linux, this should be as easy as the command below.</p> <pre><code>echo \"YWRtaW4=\" | base64 -D\n</code></pre>"},{"location":"jenkinsx/maven/#use","title":"Use","text":"<p>Log in as Administrator and you get two views. Either browse, which allows you to discover and inspect packages.</p> <p>Or, Administrate (the Gear Icon) which allows you to manage the repositories.</p> <p>For more information, read the Nexus 3 documentation.</p>"},{"location":"jenkinsx/maven/#use-nexus-with-maven-in-jenkins-x","title":"Use Nexus with Maven in Jenkins X","text":""},{"location":"jenkinsx/maven/#maven-library","title":"Maven Library","text":""},{"location":"jenkinsx/maven/#steps","title":"Steps","text":"<ul> <li>create new Jenkins X buildpack</li> <li>create new maven application</li> <li>import application into Jenkins X (with the Build Pack)<ul> <li>double check job in Jenkins</li> <li>double check webhook in GitHub</li> </ul> </li> <li>build the application in Jenkins</li> <li>verify package in Nexus</li> </ul>"},{"location":"jenkinsx/maven/#maven-application","title":"Maven Application","text":""},{"location":"jenkinsx/maven/#steps_1","title":"Steps","text":"<ul> <li>create new maven application</li> <li>add repository for local dev</li> <li>add dependency on library</li> <li>build locally</li> <li>import application into Jenkins X</li> <li>build application in Jenkins</li> </ul>"},{"location":"jenkinsx/maven/#how-the-magic-works","title":"How the magic works","text":"<ul> <li>build image<ul> <li>let's dig to see whats in it</li> </ul> </li> <li>kubernetes secret with settings.xml</li> <li>maven repo</li> <li>maven distribution management</li> <li>maven mirror</li> </ul>"},{"location":"jenkinsx/maven/#how-would-you-do-this-yourself","title":"How would you do this yourself","text":""},{"location":"jenkinsx/maven/#options","title":"Options","text":"<ul> <li>adjust Jenkins X's solution</li> <li>bridge Jenkins X's solution to your existing repo's</li> <li>create something yourself</li> </ul>"},{"location":"jenkinsx/maven/#adjust-jenkins-x-solution","title":"Adjust Jenkins X solution","text":"<ul> <li>?</li> </ul>"},{"location":"jenkinsx/maven/#bridge-to-existing","title":"Bridge to existing","text":""},{"location":"jenkinsx/maven/#only-to-external","title":"Only to external","text":""},{"location":"jenkinsx/maven/#library","title":"Library","text":"<ul> <li>buildpack https://github.com/jenkins-x-buildpacks/jenkins-x-classic/blob/master/packs/maven/pipeline.yaml</li> </ul>"},{"location":"jenkinsx/maven/#create-new-application","title":"Create new application","text":"<pre><code>mvn archetype:generate -DarchetypeGroupId=org.apache.maven.archetypes -DarchetypeArtifactId=maven-archetype-quickstart -DarchetypeVersion=1.4\n</code></pre>"},{"location":"jenkinsx/maven/#import-jx","title":"Import JX","text":"<pre><code>jx import --pack maven-lib -b\n</code></pre>"},{"location":"jenkinsx/maven/#do-we-need-it","title":"Do we need it?","text":"<p>Seems to work without it as well. Perhaps its inside the build image?</p>"},{"location":"jenkinsx/maven/#edit-secret","title":"Edit secret","text":"<ul> <li>secret: jenkins-maven-settings</li> <li>add labels:<ul> <li>jenkins.io/credentials-type: secretFile</li> </ul> </li> <li>https://jenkinsci.github.io/kubernetes-credentials-provider-plugin/examples/</li> <li>https://jenkinsci.github.io/kubernetes-credentials-provider-plugin/</li> </ul>"},{"location":"jenkinsx/maven/#pom-xml-config","title":"Pom xml config","text":"<p>In order to publish stuff, we need to make sure we have our distribution config setup.</p> <pre><code>&lt;profiles&gt;\n    &lt;profile&gt;\n        &lt;id&gt;jx-nexus&lt;/id&gt;\n        &lt;distributionManagement&gt;\n            &lt;repository&gt;\n                &lt;id&gt;nexus&lt;/id&gt;\n                &lt;name&gt;nexus&lt;/name&gt;\n                &lt;url&gt;${altReleaseDeploymentRepository}&lt;/url&gt;\n            &lt;/repository&gt;\n        &lt;/distributionManagement&gt;\n    &lt;/profile&gt;\n&lt;/profiles&gt;\n</code></pre>"},{"location":"jenkinsx/maven/#pipeline-example","title":"Pipeline example","text":"<pre><code>pipeline {\n    agent {\n        label \"jenkins-maven-java11\"\n    }\n    stages {\n        stage('Test') {\n            environment {\n                SETTINGS = credentials('another-test-file2')\n            }\n            steps {\n                sh \"echo ${SETTINGS}\"\n                sh 'cat ${SETTINGS}'\n                container('maven') {\n                    sh 'mvn clean javadoc:aggregate verify -C -e'\n                    sh \"mvn deploy --show-version --errors --activate-profiles jx-nexus --strict-checksums --settings ${SETTINGS}\"\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkinsx/maven/#config-example","title":"Config example","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n# this is the jenkins id.\n  name: \"another-test-file2\"\n  labels:\n# so we know what type it is.\n    \"jenkins.io/credentials-type\": \"secretFile\"\n  annotations:\n# description - can not be a label as spaces are not allowed\n    \"jenkins.io/credentials-description\" : \"secret file credential from Kubernetes\"\ntype: Opaque\nstringData:\n  filename: mySecret.txt\ndata:\n# base64 encoded bytes\n  data: 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\n</code></pre>"},{"location":"jenkinsx/maven/#create-app","title":"Create App","text":"<ul> <li>create new java application with maven or gradle</li> <li>add dependency</li> <li>add repo: https://nexus.jx.kearos.net/repository/maven-public/</li> </ul>"},{"location":"jenkinsx/maven/#know-issues","title":"Know Issues","text":"<ul> <li>Jenkins X doesn't have a <code>kubernetes</code> buildpack for Maven libraries, so I'm not sure how to import that directly<ul> <li>which is why, for now, we create a new build pack first</li> </ul> </li> <li>Jenkins X cannot import more than one application into static Jenkins within the same folder<ul> <li>requires GitHub issue + PR</li> </ul> </li> </ul>"},{"location":"jenkinsx/multi-cluster/","title":"Multi cluster","text":""},{"location":"jenkinsx/multi-cluster/#multi-cluster","title":"Multi Cluster","text":""},{"location":"jenkinsx/multi-cluster/#namespace-tls","title":"Namespace &amp; TLS","text":"<ul> <li>get DNS tokens from jx</li> <li>get cert/issuer from jx</li> <li><code>kubectl label namespace jx-staging certmanager.k8s.io/disable-validation=\"true\"</code></li> <li></li> </ul>"},{"location":"jenkinsx/rhos-311-minimal/","title":"Jenkins X on RedHat OpenShift 3.11","text":"<p>Why Jenkins X on RedHat OpenShift 3.11? Well, not everyone can use public cloud solutions.</p> <p>So, in order to help out those running OpenShift 3.11 and want to leverage Jenkins X, read along.</p> <p>Note</p> <p>This guide is written early March 2020, using <code>jx</code> version <code>2.0.1212</code> and OpenShift version <code>v3.11.170</code>.</p> <p>The OpenShift used is installed on GCP in a minimal fashion,  so some shortcuts are taken. For example, there's only one user, the Cluster Admin. This isn't likely in a production cluster, but it is a start.</p>"},{"location":"jenkinsx/rhos-311-minimal/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>jx binary</li> <li>kubectl is 1.16.x or less</li> <li>Helm v2</li> <li>running OpenShift 3.11 cluster<ul> <li>with cluster admin access - for withouth, take a look at this guide</li> </ul> </li> <li>GitHub account</li> </ul> <p>If you're like me, you're likely managing your packages via a package manager such as Homebrew or Chocolatey. This means you might run newer versions of Helm and kubectl and need to downgrade them. See below how!</p> <p>Caution</p> <p>If you run this in an on-premises solution or otherwise cannot contact GitHub, you have to use Lighthouse for managing the webhooks.</p> <p>As of March 2020, the support for Bitbucket Server is missing some features read here on what you can about that.  Meanwhile, we suggest you either use GitHub Enterprise or GitLab as alternatives with better support.</p>"},{"location":"jenkinsx/rhos-311-minimal/#temporarily-set-helm-v2","title":"Temporarily set Helm V2","text":"<p>Download Helm v2 release from Helms GitHub Releases page.</p> <p>Place the binary somewhere, for example <code>$HOME/Resource/helm2</code>. Then set your path with the location of Helm v2 first, before including the whole path to ensure Helm v2 is found first.</p> <pre><code>PATH=$HOME/Resources/helm2:$PATH\n</code></pre> <p>Ensure you're now running helm 2 by the command below:</p> <pre><code>helm version --client\n</code></pre> <p>It should show this:</p> <pre><code>Client: &amp;version.Version{SemVer:\"v2.16.1\", GitCommit:\"bbdfe5e7803a12bbdf97e94cd847859890cf4050\", GitTreeState:\"clean\"}\n</code></pre>"},{"location":"jenkinsx/rhos-311-minimal/#downgrade-kubctl","title":"Downgrade Kubctl","text":"<p>Downgrade kubectl (need lower than 1.17):</p> <pre><code>curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.16.7/bin/darwin/amd64/kubectl\nchmod +x ./kubectl\nsudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> <p>To confirm your kubectl version is as expected, run the command below:</p> <pre><code>kubectl version --client\n</code></pre> <p>The output should be as follows:</p> <pre><code>Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.7\", GitCommit:\"be3d344ed06bff7a4fc60656200a93c74f31f9a4\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T19:34:02Z\", GoVersion:\"go1.13.6\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\n</code></pre>"},{"location":"jenkinsx/rhos-311-minimal/#install-via-boot","title":"Install Via Boot","text":"<p>The current (as of March 2020) recommended way of installing Jenkins X, is via jx boot.</p>"},{"location":"jenkinsx/rhos-311-minimal/#boot-configuration","title":"Boot Configuration","text":"<ul> <li>provider: kubernetes: Normally, this is set to your cloud provider. in order to stay close to Kubernetes itself and thus OpenShift, we set this to <code>kubernetes</code></li> <li>registry: docker.io: If you're on a public cloud vender, <code>jx boot</code> creates a docker registry for you (GCR on GCP, ACR on AWS, and so on), in this example we leverage Docker Hub (<code>docker.io</code>). This should be indicative for any self-hosted registry as well!</li> <li>dockerRegistryOrg: caladreas:  when the docker registry owner - in my case, <code>caladreas</code>- is different from the git repository owner, you have to specify this via <code>dockerRegistryOrg</code></li> <li>secretStorage: local: Thre recommended approach is to use the HashiCorp Vault integration, but that isn't supported on OpenShift</li> <li>webhook: prow: This uses Prow for webhook management. In March 2020 the best option to use with GitHub. If you want to use Bitbucket read my guide on jx with lighthouse &amp; bitbucket.</li> </ul> jx-requirements.yaml <pre><code>autoUpdate:\n  enabled: false\nbootConfigURL: https://github.com/jenkins-x/jenkins-x-boot-config.git\ncluster:\n  clusterName: rhos11\n  environmentGitOwner: &lt;GitHub User&gt;\n  gitKind: github\n  gitName: github\n  gitServer: https://github.com\n  namespace: jx\n  provider: kubernetes\n  registry: docker.io\n  dockerRegistryOrg: caladreas\nenvironments:\n- ingress:\n    domain: openshift.kearos.net\n    externalDNS: false\n    ignoreLoadBalancer: true\n    namespaceSubDomain: -jx.\n  key: dev\n  repository: environment-rhos11-dev\n- ingress:\n    domain: \"staging.openshift.kearos.net\"\n    namespaceSubDomain: \"\"\n  key: staging\n  repository: env-rhos311-staging\n- ingress:\n    domain: \"openshift.kearos.net\"\n    namespaceSubDomain: \"\"\n  key: production\n  repository: env-rhos311-prod\ngitops: true\ningress:\n  domain: openshift.example.com\n  externalDNS: false\n  ignoreLoadBalancer: true\n  namespaceSubDomain: -jx.\nkaniko: true\nrepository: nexus\nsecretStorage: local\nversionStream:\n  ref: v1.0.361\n  url: https://github.com/jenkins-x/jenkins-x-versions.git\nwebhook: prow\n</code></pre>"},{"location":"jenkinsx/rhos-311-minimal/#jx-boot","title":"Jx Boot","text":"<p>Go to a directory where you want to clone the development environment repository.</p> <p>Create the initial configuration file, <code>jx-requirements.yml</code>, and run the initial <code>jx boot</code> iteration.</p> <pre><code>jx boot\n</code></pre> <p>It will ask you if you want to clone the <code>jenkins x boot config</code> repository:</p> <pre><code>? Do you want to clone the Jenkins X Boot Git repository? [? for help] (Y/n)\n</code></pre> <p>Say yes, and it will clone the configuration repository and start the jx boot pipeline. It will fail, because not all values are copied from your <code>jx-requirements.yml</code> into the new cloned repository.</p> <p>To resolve this, go into the new cloned repository and replace the values of <code>jx-requirements.yml</code> with your configuration. Once done, restart the installation.</p> <pre><code>jx boot\n</code></pre>"},{"location":"jenkinsx/rhos-311-minimal/#failed-to-install-certmanager","title":"Failed to install certmanager","text":"<p>Jenkins X will fail to install Certmanager, because it relies on newer API components from Kubernetes than are available in OpenShift 3.11. The <code>11</code> of 3.11 refers to Kubernetes <code>1.11</code>. Certmanager requires <code>1.12</code>+.</p> <p>To disable the installation of certmanager, we edit the <code>jenkins-x.yml</code>, which is the pipeline executed by <code>jx boot</code>.</p> <p>We have to remove the <code>step</code>, that tries to install certmanager; <code>install-cert-manager-crds</code>. The block of code we have to remove, is as follows:</p> <pre><code>            - args:\n              - apply\n              - --wait\n              - --validate=false\n              - -f\n              - https://raw.githubusercontent.com/jetstack/cert-manager/release-0.11/deploy/manifests/00-crds.yaml\n              command: kubectl\n              dir: /workspace/source\n              env:\n              - name: DEPLOY_NAMESPACE\n                value: cert-manager\n              name: install-cert-manager-crds\n</code></pre> <p>Once done, we can run <code>jx boot</code> again.</p>"},{"location":"jenkinsx/rhos-311-minimal/#pipeline-runner-faillure","title":"Pipeline Runner faillure","text":"<p>For me, the <code>pipeline runner</code> deployment failed, failing the <code>jx boot</code> process - when it validates if everything came up.</p> <pre><code>pipelinerunner-74897865f5-2k4vb                0/1     CrashLoopBackOff   55         4h\n</code></pre> <pre><code>error: unable to clone version dir: unable to create temp dir for version stream: mkdir /tmp/jx-version-repo-083799486: permission denied\n</code></pre> <p>A solution, is add a volume to the <code>pipelinerunner</code> deployment, which mounts an <code>emptyDir</code><sup>1</sup> to at <code>/tmp</code>.</p> <pre><code>    volumeMounts:\n    - mountPath: /tmp\n      name: cache-volume\n</code></pre> <pre><code>  volumes:\n  - name: cache-volume\n    emptyDir: {}\n</code></pre> <p>Once the pipelinerunner pod is running, rerun the <code>jx boot</code> installation.</p> <pre><code>jx boot\n</code></pre> <p>It should now succeed with <code>cluster ok</code>.</p>"},{"location":"jenkinsx/rhos-311-minimal/#create-quickstart","title":"Create Quickstart","text":"<p>To validate Jenkins X works as it should, the first step is to create a <code>quickstart</code><sup>2</sup><sup>3</sup>.</p> <p>For simplicity, lets stick to a Go (lang) project.</p> <pre><code>jx create quickstart --filter golang-http --project-name jx-go-rhos311 --batch-mode\n</code></pre> <p>This creates a new repository based on the quickstart for Go (lang)<sup>4</sup> and the build pack for Go (lang)<sup>5</sup>.</p> <p>I ran into two issues:</p> <ol> <li>Tekton is not mounting my Docker registry credentials, thus the Kaniko build fails with <code>401: not authenticated</code> </li> <li>the expose controller<sup>6</sup> is using Ingress resources by default, but doesn't want to create those on OpenShift<sup>7</sup></li> </ol> <p>Once the issues below are solved, the application is runing in the staging environment. You can view the applications in your cluster as follows:</p> <pre><code>jx get application\n</code></pre> <p>Which should look something like this:</p> <pre><code>APPLICATION STAGING PODS URL\njx-go       0.0.1   1/1  http://jx-go-jx-staging.staging.openshift.example.com\n</code></pre>"},{"location":"jenkinsx/rhos-311-minimal/#missing-docker-credentials","title":"Missing Docker Credentials","text":"<p>I used Docker hub as my Docker registry, but this applies to any other self-hosted Docker registry.</p> <p>We have to do the following:</p> <ol> <li>create a <code>docker-registry</code> secret in Kubernetes, with the credentials to our Docker registry (dockerhub or otherwise)</li> <li>mount this secret in a location Kaniko picks it up</li> </ol> <pre><code>kubectl create secret docker-registry kaniko-secret --docker-username=&lt;username&gt; --docker-password=&lt;password&gt;  --docker-email=&lt;email-address&gt;\n</code></pre> <p>Mount docker hub secret as json in classic Kaniko style.</p> <pre><code>pipelineConfig:\n  env:\n  -  name: DOCKER_CONFIG\n     value: /root/.docker/\n  pipelines:\n    overrides:\n    - pipeline: release\n      stage: build \n      name: container-build\n      volumes:\n        - name: kaniko-secret\n          secret:\n            defaultMode: 420\n            secretName: kaniko-secret\n            items:\n                - key: .dockerconfigjson\n                  path: config.json\n      containerOptions:\n        volumeMounts:\n          - mountPath: /root/.docker\n            name: kaniko-secret\n</code></pre> <p>This should be enough. But if Kaniko still runs into a <code>401 unathenticated</code> error, you have to change the ConfigMap for the builder PodTemplate. For example, if you use Go with the <code>go</code> build pack, your build container will use the <code>jenkins-x-pod-template-go</code> config map. This contains some environment variables related to Docker. If you still have issues, remove these environment variables.</p> <p>those pod template configmaps are to support traditional jenkins servers so we don't really need much from them anymore with tekton, though if we delete them things fail for now so need to keep them, but just try and remove all the DOCKER related stuff from the configmap</p> <pre><code>kubectl edit cm jenkins-x-pod-template-go\n</code></pre>"},{"location":"jenkinsx/rhos-311-minimal/#expose-controller-options","title":"Expose Controller Options","text":"<p>When the build succeeds, Jenkins X makes a PullRequest to your environment repository. By default, the first one is <code>Staging</code>, which will automatically promote<sup>10</sup> and run the application.</p> <p>This fails, because the default setting of the staging environment, is to expose the applications via the <code>Expose Controller</code> with an <code>Ingress</code> resource. Currently (March 2020), the Expose Controller assumes OpenShift cannot handle Ingress resources<sup>7</sup>.</p> <p>So there's two options here:</p> <ol> <li>configure the Expose Controller to use a <code>Route</code> to expose an application<sup>11</sup></li> <li>customize the Expose Controller to only issue a warning when using <code>exposer: Ingress</code> on OpenShift environment</li> </ol> <p>If you choose option one, change the value of <code>exposer</code> from <code>Ingress</code> to <code>Route</code>of the <code>env/values.yaml</code>.</p> <p>env/values.yaml</p> <pre><code>expose:\n  Annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded\n  Args:\n  - --v\n  - 4\n  config:\n    domain: staging.openshift.staging.example.com\n    exposer: Route\n    http: \"true\"\n    tlsacme: \"false\"\n    urltemplate: '{{.Service}}.{{.Domain}}'\n</code></pre> <p>If you choose option 2, fork the Expose Controller repository and change the line that stops it from creating Ingress resources<sup>7</sup>.</p> <p>As can be seen here: https://github.com/jenkins-x/exposecontroller/blob/master/exposestrategy/ingress.go#L48</p> <pre><code>    if t == openShift {\n        return nil, errors.New(\"ingress strategy is not supported on OpenShift, please use Route strategy\")\n    }\n</code></pre> <p>And the following steps:</p> <ol> <li>new local build</li> <li>create and push Docker image to a registry accessable in the cluster</li> <li>create a new helm package, in <code>charts</code> directory, execute <code>helm package exposecontroller</code></li> <li>upload Helm chart somewhere, for example, a GitHub repository</li> <li>update the <code>env/requirements.yaml</code> to use your helm chart instead of the Jenkins X one for the Expose Controller</li> </ol> <p>For example:</p> <p>env/requirements.yaml</p> <pre><code>dependencies:\n# - alias: expose\n#   name: exposecontroller\n#   repository: http://chartmuseum.jenkins-x.io\n#   version: 2.3.118\n- alias: expose\n  name: exposecontroller\n  version: 2.3.109\n  repository: https://raw.githubusercontent.com/joostvdg/helm-repo/master/\n</code></pre>"},{"location":"jenkinsx/rhos-311-minimal/#promote-to-production","title":"Promote To Production","text":"<p>To promote an application to the Production environment, we have to instruct Jenkins X to do it for us<sup>10</sup>.</p> <p>For example:</p> <pre><code>jx promote jx-go-rhos311-1 --version 0.0.34 --env production --batch-mode\n</code></pre> <p>Aside from the Expose Controller issues, there's nothing else to be done.</p> <p>Just be sure to make those changes in your production environment repository.</p>"},{"location":"jenkinsx/rhos-311-minimal/#preview-environments","title":"Preview Environments","text":"<p>The only thing required to generate a Preview Environment in Jenkins X<sup>12</sup>, is to create a PullRequest to the <code>master</code> branch from a other branch.</p> <p>Wether the preview environment succeeds, depends on two things.</p> <p>One, does the Jenkins X service account - <code>tekton-bot</code> - have enough permissions to create the namespace unique to the pull request - default naming scheme is <code>jx-&lt;user&gt;-&lt;app&gt;-pr-&lt;prNumber&gt;</code>.</p> <p>Two, because each Preview Environment has its one Expose Controller<sup>13</sup>, the Expose Controller needs to be configured to either use <code>Route</code> or accept creating <code>Ingress</code> when on OpenShift.</p> <p>If all is done, you can retrieve the current preview environments as follows:</p> <pre><code>jx get preview\n</code></pre> <p>Which should yield something like this:</p> <pre><code>PULL REQUEST                                           NAMESPACE        APPLICATION\nhttps://bitbucket.openshift.kearos.net/jx/jx-go/pull/1 jx-jx-jx-go-pr-1 http://jx-go.jx-jx-jx-go-pr-1.openshift.example.com\n</code></pre>"},{"location":"jenkinsx/rhos-311-minimal/#errata","title":"Errata","text":""},{"location":"jenkinsx/rhos-311-minimal/#registry-owner-mismatch","title":"Registry Owner Mismatch","text":"<p>It can happen that the docker registry owner is not the same for every application. If this is the case, the application will have to make a workaround after it is imported into Jenkins X (via <code>jx import</code> or <code>jx create quickstart</code>).</p> <p>In order to resolve the mismatch between the default Jenkins X installation Docker registry owner and the application's owner, we need to change two things in our Jenkins X pipeline (<code>jenkins-x.yml</code>)<sup>8</sup>.</p> <ol> <li>add an override for the Docker registry owner in the <code>jenkins-x.yml</code>, the pipeline of your application.</li> <li>add an override for the <code>container-build</code> step of the <code>build</code> stage, for both the <code>release</code> and <code>pullrequest</code> pipelines.</li> </ol> <p>Overriding the pipeline is done by specifying the stage to override under <code>pipelineConfig.overides</code><sup>8</sup><sup>9</sup>.</p> <p>When you set <code>dockerRegistryOwner</code>, it overrides the value generated elsewhere.</p> <pre><code>dockerRegistryOwner: caladreas\n</code></pre> <p>The only exception is where the image gets uploaded to via <code>Kaniko</code>. </p> <pre><code>- --destination=docker.io/caladreas/jx-go-rhos311-1:${inputs.params.version}\n</code></pre> <p>The end result will look like this.</p> <p>jenkins-x.yml</p> <pre><code>dockerRegistryOwner: caladreas\nbuildPack: go\npipelineConfig:\n    overrides:\n    - pipeline: release\n      stage: build \n      name: container-build\n      steps:\n        - name: container-build\n          dir: /workspace/source\n          image: gcr.io/kaniko-project/executor:9912ccbf8d22bbafbf971124600fbb0b13b9cbd6\n          command: /kaniko/executor\n          args:\n            - --cache=true\n            - --cache-dir=/workspace\n            - --context=/workspace/source\n            - --dockerfile=/workspace/source/Dockerfile\n            - --destination=docker.io/caladreas/jx-go-rhos311-1:${inputs.params.version}\n            - --cache-repo=docker.io/todo/cache\n            - --skip-tls-verify-registry=docker.io\n            - --verbosity=debug\n    - pipeline: pullrequest\n      stage: build \n      name: container-build\n      steps:\n        - name: container-build\n          dir: /workspace/source\n          image: gcr.io/kaniko-project/executor:9912ccbf8d22bbafbf971124600fbb0b13b9cbd6\n          command: /kaniko/executor\n          args:\n            - --cache=true\n            - --cache-dir=/workspace\n            - --context=/workspace/source\n            - --dockerfile=/workspace/source/Dockerfile\n            - --destination=docker.io/caladreas/jx-go-rhos311-1:${inputs.params.version}\n            - --cache-repo=docker.io/todo/cache\n            - --skip-tls-verify-registry=docker.io\n            - --verbosity=debug\n</code></pre>"},{"location":"jenkinsx/rhos-311-minimal/#references","title":"References","text":"<ol> <li> <p>https://kubernetes.io/docs/concepts/storage/volumes/#emptydir \u21a9</p> </li> <li> <p>https://jenkins-x.io/docs/getting-started/first-project/create-quickstart/ \u21a9</p> </li> <li> <p>https://github.com/jenkins-x-quickstarts \u21a9</p> </li> <li> <p>https://github.com/jenkins-x-quickstarts/golang-http \u21a9</p> </li> <li> <p>https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs \u21a9</p> </li> <li> <p>https://jenkins-x.io/docs/concepts/technology/#whats-is-exposecontroller \u21a9</p> </li> <li> <p>https://github.com/jenkins-x/exposecontroller/blob/master/exposestrategy/ingress.go#L48 \u21a9\u21a9\u21a9</p> </li> <li> <p>https://jenkins-x.io/docs/reference/pipeline-syntax-reference/ \u21a9\u21a9</p> </li> <li> <p>https://technologyconversations.com/2019/06/30/overriding-pipelines-stages-and-steps-and-implementing-loops-in-jenkins-x-pipelines/ \u21a9</p> </li> <li> <p>https://jenkins-x.io/docs/getting-started/promotion/ \u21a9\u21a9</p> </li> <li> <p>https://github.com/jenkins-x/exposecontroller#exposer-types \u21a9</p> </li> <li> <p>https://jenkins-x.io/docs/getting-started/build-test-preview/#generating-a-preview-environment \u21a9</p> </li> <li> <p>https://jenkins-x.io/docs/reference/preview/#charts \u21a9</p> </li> </ol>"},{"location":"jenkinsx/rhos-311-restricted/","title":"WIP Jenkins X on RedHat OpenShift 3.11 Restricted","text":"<p>Important</p> <p>This guide is still a Work In Progress (WIP). The guide is not - yet - complete in its ability to install Jenkins X without Cluster Administrative access. Do come back in the future!</p> <p>ps. at the bottom it should say when this guide was last updated</p> <p>Why Jenkins X on RedHat OpenShift 3.11? Well, not everyone can use public cloud solutions.</p> <p>So, in order to help out those running OpenShift 3.11 and want to leverage Jenkins X, read along. Unlike the other guide, Jenkins X on OpenShift (minimal), in th</p> <p>Note</p> <p>This guide is written early March 2020, using <code>jx</code> version <code>2.0.1212</code> and OpenShift version <code>v3.11.170</code>.</p> <p>The OpenShift used is installed on GCP,  with some shortcuts taken. </p>"},{"location":"jenkinsx/rhos-311-restricted/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>jx binary</li> <li>kubectl is 1.16.x or less</li> <li>Helm v2</li> <li>running OpenShift 3.11 cluster<ul> <li>with cluster admin access (will update how to avoid this)</li> </ul> </li> <li>GitHub or Bitbucket account - the installation doesn't progress far enough yet for this to matter</li> </ul> <p>If you're like me, you're likely managing your packages via a package manager such as Homebrew or Chocolatey. This means you might run newer versions of Helm and kubectl and need to downgrade them. See below how!</p> <p>Caution</p> <p>If you run this in an on-premises solution or otherwise cannot contact GitHub, you have to use Lighthouse for managing the webhooks.</p> <p>As of March 2020, the support for Bitbucket Server is missing some features read here on what you can about that.  Meanwhile, we suggest you either use GitHub Enterprise or GitLab as alternatives with better support.</p>"},{"location":"jenkinsx/rhos-311-restricted/#temporarily-set-helm-v2","title":"Temporarily set Helm V2","text":"<p>Download Helm v2 release from Helms GitHub Releases page.</p> <p>Place the binary somewhere, for example <code>$HOME/Resource/helm2</code>. Then set your path with the location of Helm v2 first, before including the whole path to ensure Helm v2 is found first.</p> <pre><code>PATH=$HOME/Resources/helm2:$PATH\n</code></pre> <p>Ensure you're now running helm 2 by the command below:</p> <pre><code>helm version --client\n</code></pre> <p>It should show this:</p> <pre><code>Client: &amp;version.Version{SemVer:\"v2.16.1\", GitCommit:\"bbdfe5e7803a12bbdf97e94cd847859890cf4050\", GitTreeState:\"clean\"}\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#downgrade-kubctl","title":"Downgrade Kubctl","text":"<p>Downgrade kubectl (need lower than 1.17):</p> <pre><code>curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.16.7/bin/darwin/amd64/kubectl\nchmod +x ./kubectl\nsudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> <p>To confirm your kubectl version is as expected, run the command below:</p> <pre><code>kubectl version --client\n</code></pre> <p>The output should be as follows:</p> <pre><code>Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.7\", GitCommit:\"be3d344ed06bff7a4fc60656200a93c74f31f9a4\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T19:34:02Z\", GoVersion:\"go1.13.6\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#install-via-boot","title":"Install Via Boot","text":"<p>The current (as of March 2020) recommended way of installing Jenkins X, is via jx boot.</p>"},{"location":"jenkinsx/rhos-311-restricted/#process","title":"Process","text":"<p>WIP</p>"},{"location":"jenkinsx/rhos-311-restricted/#config","title":"Config","text":"<p>WIP</p>"},{"location":"jenkinsx/rhos-311-restricted/#issues-encountered","title":"Issues Encountered","text":"<ul> <li>jx could not get YAML from GitHub for alpha plugins</li> <li>requires <code>kubectl</code> with access to the Kubernetes<ul> <li>if in OpenShift, first do a <code>oc login</code> </li> </ul> </li> <li>have to manually clone the Jenkins X repositories, no direct access to GitHub<ul> <li>jx version</li> <li>https://github.com/jenkins-x/jenkins-x-versions.git</li> <li><code>~/.jx/jenkins-x-versions</code></li> <li>jx boot config</li> <li>https://github.com/jenkins-x/jenkins-x-boot-config.git</li> </ul> </li> <li>storage option: <ul> <li>only local option is git server , but that seems to run into issues</li> <li>NFS would be the best option, but it doesn't exist</li> </ul> </li> <li>make sure kubectl has login config from OC<ul> <li>https://blog.christianposta.com/kubernetes/logging-into-a-kubernetes-cluster-with-kubectl/</li> </ul> </li> <li>RBAC issues<ul> <li>tries to access <code>kube-system</code> namespace</li> <li>install JX CRD's</li> <li>create service accounts, RBAC config (roles, role bindings, cluster roles, cluser role binding)</li> <li>create namespaces</li> <li>list all namespaces, this is a check to see if we are connected to a Kubernetes cluster, seems overly broad</li> <li>label a namespace</li> </ul> </li> </ul>"},{"location":"jenkinsx/rhos-311-restricted/#jx-binary-requests-yaml-from-github","title":"JX Binary requests YAML from GitHub","text":"<pre><code>-bash-4.4$ jx version\nWARNING: failed to discover alpha commands from github: failed to get YAML from https://raw.githubusercontent.com/jenkins-x-labs/jxl/master/alpha/plugins.yml: Get https://raw.githubusercontent.com/jenkins-x-labs/jxl/master/alpha/plugins.yml: dial tcp 151.101.112.133:443: connectex: No connection could be made because the target machine actively refused it.\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#jx-version","title":"JX Version","text":"<p>Warnings</p> <pre><code>WARNING: Failed to retrieve team settings: failed to create the jx-development-dev Dev namespace: Failed to label Namespace jx-development-dev namespaces \"jx-development-dev\" is forbidden: User \"tk9at\" cannot update namespaces in the namespace \"jx-development-dev\": no RBAC policy matched - falling back to default settings...\nWARNING: Failed to find helm installs: running helm list --all --namespace jx-development-dev: failed to run 'helm list --all --namespace jx-development-dev' command in directory '', output: 'Error: pods is forbidden: User \"tk9at\" cannot list pods in the namespace \"kube-system\": no RBAC policy matched'\nWARNING: Fail\n</code></pre> <pre><code>NAME               VERSION\njx                 2.0.1242\nKubernetes cluster v1.11.0+d4cacc0\ngit                2.24.0.windows.2\nOperating System   Windows 7 Enterprise unkown release build 7601\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#jx-boot-issues","title":"JX boot issues","text":"<pre><code>WARNING: failed to discover alpha commands from github: failed to get YAML from https://raw.githubusercontent.com/jenkins-x-labs/jxl/master/alpha/plugins.yml: Get https://raw.githubusercontent.com/jenkins-x-labs/jxl/master/alpha/plugins.yml: dial tcp 151.101.112.133:443: connectex: No connection could be made because the target machine actively refused it.\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#rbac-issues","title":"RBAC Issues","text":""},{"location":"jenkinsx/rhos-311-restricted/#rights-required","title":"Rights Required","text":"<ul> <li>list namespaces</li> <li>list pods</li> <li>list configmaps</li> <li>create namespaces</li> <li>get namespace (although if you can get only your own, that is ok!)</li> <li>label namespace</li> <li>create RBAC resources</li> </ul>"},{"location":"jenkinsx/rhos-311-restricted/#lazily-create-namespace-jx","title":"Lazily Create Namespace jx","text":"<p>Even if it exists, it tries to create the namespace <code>jx</code>.</p> <pre><code>error: failed to lazily create the namespace jx: Failed to create Namespace jx namespaces is forbidden: User \"six\" cannot create namespaces at the cluster scope: no RBAC policy matched\nerror: failed to interpret pipeline file jenkins-x.yml: failed to run '/bin/sh -c jx step verify preinstall --provider-values-dir=\"kubeProviders\"' command in directory '.', output: ''\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#jx-version_1","title":"jx version","text":"<pre><code>jx version\n</code></pre> <pre><code>error: namespaces \"jx\" is forbidden: User \"myuser\" cannot get namespaces in the namespace \"jx\": no RBAC policy matched\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#issue-with-jx-boot-startup","title":"Issue with jx boot startup","text":"<p>The <code>jx boot</code> process tries to verify our connect to a live Kubernetes environment.  It does so, by doing a <code>kubectl get namespaces</code> API call at pkg/cmd/boot/boot.go.</p> <p>Unfortunately, this is a very broad cluster wide permission, which is many OpenShift environments is not allowed.</p> <p>Current code:</p> <pre><code>#pkg/cmd/boot/boot.go\nfunc (o *BootOptions) verifyClusterConnection() error {\n    client, err := o.KubeClient()\n    if err == nil {\n        _, err = client.CoreV1().Namespaces().List(metav1.ListOptions{})\n    }\n    ...\n}\n</code></pre> <p>This is a potential solution:</p> <pre><code>#pkg/cmd/boot/boot.go\nfunc (o *BootOptions) verifyClusterConnection() error {\n    client, curNs, err := o.KubeClientAndNamespace()\n    if err == nil {\n        _, err = client.CoreV1().Pods(curNs).List(metav1.ListOptions{})\n    }\n    ...\n}\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#issue-with-swallowing-jx-requirement-errors","title":"Issue with swallowing jx requirement errors","text":"<p>If you make a mistake in the <code>jx-requirements.yml</code>, the parsing errors are swallowed. Instead, you get a generic error that makes no sense.</p> <pre><code>error: unable to load jx-requirements.yml (from .): jx-requirements.yml file not found\n</code></pre> <pre><code># config/install_requirements.go \nfunc LoadRequirementsConfig(dir string) (*RequirementsConfig, string, error) {\n    ....\n    # this line overrides any error previously found\n    return nil, \"\", errors.New(\"jx-requirements.yml file not found\")\n}\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#verify-storage","title":"Verify Storage","text":"<pre><code>Verifying Storage...\nerror: failed to ensure the bucket URL https://bitbucket.apps.ocp.kearos.net/scm/jx/build-logs.git is created: EnsureBucketIsCreated not implemented for LegacyBucketProvider\nerror: failed to interpret pipeline file jenkins-x.yml: failed to run '/bin/sh -c jx step verify preinstall --provider-values-dir=\"kubeProviders\"' command in directory '.', output: ''\n</code></pre> <p>Seems we cannot combine storage in Bitbucket Server together with logs as the start:</p> <pre><code>storage:\n  logs:\n    enabled: true\n    url: \"http://bitbucket.openshift.example.com/scm/jx/build-logs.git\"\n</code></pre> <p>Workaround:</p> <ul> <li>create with logs disabled</li> <li>then enable via command below</li> </ul> <pre><code>jx edit storage -c logs --git-url http://bitbucket.openshift.example.com/scm/jx/build-logs.git  --git-branch master\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#label-namespace","title":"Label Namespace","text":"<pre><code>error: failed to lazily create the namespace jx: Failed to label Namespace jx namespaces \"jx\" is forbidden: User \"myuser\" cannot update namespaces in the namespace \"jx\": no RBAC policy matched\nerror: failed to interpret pipeline file jenkins-x.yml: failed to run '/bin/sh -c jx step verify preinstall --provider-values-dir=\"kubeProviders\"' command in directory '.', output: ''\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#install-velero-even-if-disabled","title":"Install Velero, even if disabled","text":"<pre><code>STEP: install-velero command: /bin/sh -c jx step helm apply --boot --remote --no-vault --name velero in dir: /Users/joostvdg/Projects/Personal/Github/jenkins-examples/jx/openshift/311-2/jxboot-bs/jenkins-x-boot-config/systems/velero\n\nModified file /Users/joostvdg/Projects/Personal/Github/jenkins-examples/jx/openshift/311-2/jxboot-bs/jenkins-x-boot-config/systems/velero/Chart.yaml to set the chart to version 1\nerror: Failed to create Namespace velero namespaces is forbidden: User \"six\" cannot create namespaces at the cluster scope: no RBAC policy matched\nerror: failed to interpret pipeline file jenkins-x.yml: failed to run '/bin/sh -c jx step helm apply --boot --remote --no-vault --name velero' command in directory 'systems/velero', output: ''\n</code></pre> <p>Solution, disable steps <code>install-velero</code> and <code>install-velero-backups</code> in pipeline <code>jenkins-x.yml</code>.</p>"},{"location":"jenkinsx/rhos-311-restricted/#install-nginx-controller","title":"Install nginx controller","text":"<p>This is not required, as OpenShift already has the route controller. So disabling nginx installation is a solution, step <code>install-nginx-controller</code> in <code>jenkins-x.yml</code>.</p> <pre><code>STEP: install-nginx-controller command: /bin/sh -c jx step helm apply --boot --remote --no-vault --name jxing in dir: /Users/joostvdg/Projects/Personal/Github/jenkins-examples/jx/openshift/311-2/jxboot-bs/jenkins-x-boot-config/systems/jxing\n\nModified file /Users/joostvdg/Projects/Personal/Github/jenkins-examples/jx/openshift/311-2/jxboot-bs/jenkins-x-boot-config/systems/jxing/Chart.yaml to set the chart to version 1\nerror: Failed to create Namespace kube-system namespaces is forbidden: User \"six\" cannot create namespaces at the cluster scope: no RBAC policy matched\nerror: failed to interpret pipeline file jenkins-x.yml: failed to run '/bin/sh -c jx step helm apply --boot --remote --no-vault --name jxing' command in directory 'systems/jxing', output: ''\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#installing-cert-manager","title":"Installing cert manager","text":"<p>This is not required, so we can disable steps <code>install-cert-manager-crds</code> and <code>install-cert-manager</code> in <code>jenkins-x.ym</code>.  And even if we wanted to use cert manager, we would have to install a specific version for RHOS 3.11.<sup>14</sup></p>"},{"location":"jenkinsx/rhos-311-restricted/#installing-jx-crds","title":"Installing JX CRD's","text":"<pre><code>STEP: install-jx-crds command: /bin/sh -c jx upgrade crd in dir: /Users/joostvdg/Projects/Personal/Github/jenkins-examples/jx/openshift/311-2/jxboot-bs/jenkins-x-boot-config\n\nError creating commitstatuses.jenkins.io: customresourcedefinitions.apiextensions.k8s.io is forbidden: User \"six\" cannot create customresourcedefinitions.apiextensions.k8s.io at the cluster scope: no RBAC policy matched\n</code></pre> <p>Let and Admin install these and disable pipeline step?</p> <ul> <li>admin has to execute <code>jx upgrade crd</code></li> <li>remove step <code>install-jx-crds</code> from <code>jenkins-x.yml</code></li> </ul>"},{"location":"jenkinsx/rhos-311-restricted/#rbac-resources","title":"RBAC Resources","text":""},{"location":"jenkinsx/rhos-311-restricted/#role-for-namespaces","title":"Role for namespaces","text":"<pre><code>oc apply -f jx-install-role.yml -n jx\noc adm policy add-role-to-user jx-install six --role-namespace jx\n</code></pre> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: jx\n  name: jx-install\nrules:\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resources: [\"pods\", \"secrets\", \"configmaps\", \"pods/log\", \"services\", \"endpoints\"]\n  verbs: [\"get\", \"watch\", \"list\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"extensions\", \"apps\"]\n  resources: [\"deployments\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#namespaces","title":"Namespaces","text":"<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: jx\n  labels:\n    jenkins.io/created-by: \"jx-boot\"\n    env: \"dev\"\n    team: \"jx\"\n</code></pre> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: jx-staging\n  labels:\n    jenkins.io/created-by: \"jx-boot\"\n    env: \"staging\"\n    team: \"jx\"\n</code></pre> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: jx-production\n  labels:\n    jenkins.io/created-by: \"jx-boot\"\n    env: \"production\"\n    team: \"jx\"\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#jenkins-x-api-role","title":"Jenkins X API Role","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: jx\n  name: jx-api-install\nrules:\n- apiGroups: [\"jenkins.io\"] # \"\" indicates the core API group\n  resources: [\"*\"]\n  verbs: [\"*\"]\n</code></pre> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: myuser-jx-api-install\n  namespace: jx\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: jx-api-install\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: User\n  name: myuser\n  namespace: jx\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#service-accounts","title":"Service Accounts","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins-x-bucketrepo\n  namespace: jx\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins-x-controllerbuild\n  namespace: jx\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins-x-controllerrole\n  namespace: jx\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins-x-gcactivities\n  namespace: jx\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins-x-gcpods\n  namespace: jx\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins-x-gcpreviews\n  namespace: jx\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins-x-heapster\n  namespace: jx\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins\n  namespace: jx\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins-x-lighthouse\n  namespace: jx\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: tide\n  namespace: jx\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: tekton-pipelines\n  namespace: jx\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: tekton-bot\n  namespace: jx\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#jenkins-x-rbac-list","title":"Jenkins X RBAC List","text":""},{"location":"jenkinsx/rhos-311-restricted/#roles","title":"Roles","text":"<ul> <li>controllerbuild</li> <li>controllerrole</li> <li>gcactivities</li> <li>gcpods</li> <li>gcpreviews</li> <li>jenkins-x-heapster-pod-nanny</li> <li>committer<ul> <li>https://raw.githubusercontent.com/jenkins-x/jenkins-x-platform/master/jenkins-x-platform/templates/committer-role.yaml</li> <li>https://github.com/jenkins-x/jenkins-x-platform/blob/master/jenkins-x-platform/templates/committer-role.yaml</li> </ul> </li> <li>jx-view<ul> <li>https://github.com/jenkins-x/jenkins-x-platform/blob/master/jenkins-x-platform/templates/jx-view-role.yaml</li> </ul> </li> <li>owner <ul> <li>https://github.com/jenkins-x/jenkins-x-platform/blob/master/jenkins-x-platform/templates/owner-role.yaml</li> <li>https://raw.githubusercontent.com/jenkins-x/jenkins-x-platform/master/jenkins-x-platform/templates/owner-role.yaml</li> </ul> </li> <li>viewer <ul> <li>https://github.com/jenkins-x/jenkins-x-platform/blob/master/jenkins-x-platform/templates/viewer-role.yaml</li> <li>https://raw.githubusercontent.com/jenkins-x/jenkins-x-platform/master/jenkins-x-platform/templates/viewer-role.yaml</li> </ul> </li> <li>jenkins-x-lighthouse</li> <li>tide</li> <li>tekton-bot</li> </ul>"},{"location":"jenkinsx/rhos-311-restricted/#role-bindings","title":"Role Bindings","text":"<ul> <li>controllerbuild</li> <li>controllerrole</li> <li>gcactivities</li> <li>gcpods</li> <li>gcpreviews</li> <li>jenkins-x-heapster-pod-nanny</li> <li>jenkins-x-lighthouse</li> <li>tide</li> <li>tekton-bot</li> </ul>"},{"location":"jenkinsx/rhos-311-restricted/#ingress","title":"Ingress","text":"<ul> <li>hook</li> <li>chartmuseum</li> </ul>"},{"location":"jenkinsx/rhos-311-restricted/#cluster-role-bindings","title":"Cluster Role Bindings","text":"<ul> <li>controllerbuild-jx</li> <li>controllerrole-jx</li> <li>gcactivities-jx</li> <li>gcpreviews-jx</li> <li>jenkins-x-heapster</li> <li>jenkins-jx-role-binding</li> <li>tekton-pipelines-jx</li> </ul>"},{"location":"jenkinsx/rhos-311-restricted/#jenkins-x-resources","title":"Jenkins X Resources","text":"<ul> <li>Releases (jenkins.io/v1, releases)<ul> <li>controllerbuild-2.0.1243 (depends on jx version?!)</li> <li>controllerrole-2.0.1243</li> <li>gcactivities-2.0.1243</li> <li>gcpods-2.0.1243</li> <li>gcpreviews-2.0.1243</li> </ul> </li> <li>Schedulers (\"jenkins.io/v1, Resource=schedulers\")<ul> <li>default-scheduler </li> <li>env-scheduler </li> <li>pr-only </li> <li>release-only</li> </ul> </li> </ul>"},{"location":"jenkinsx/rhos-311-restricted/#list-of-resources-failed-to-be-created","title":"List Of Resources Failed To Be Created","text":"<ul> <li>Resource: \"/v1, Resource=serviceaccounts\", GroupVersionKind: \"/v1, Kind=ServiceAccount\"</li> <li> <p>Name: \"jenkins-x-bucketrepo\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=clusterrolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding\"</p> </li> <li> <p>Name: \"controllerbuild-jx\", Namespace: \"\"</p> </li> <li> <p>Resource: \"jenkins.io/v1, Resource=releases\", GroupVersionKind: \"jenkins.io/v1, Kind=Release\"</p> </li> <li> <p>Name: \"controllerbuild-2.0.1243\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=roles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=Role\"</p> </li> <li> <p>Name: \"controllerbuild\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=rolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=RoleBinding\"</p> </li> <li> <p>Name: \"controllerbuild\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"/v1, Resource=serviceaccounts\", GroupVersionKind: \"/v1, Kind=ServiceAccount\"</p> </li> <li> <p>Name: \"jenkins-x-controllerbuild\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=clusterrolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding\"</p> </li> <li> <p>Name: \"controllerrole-jx\", Namespace: \"\"</p> </li> <li> <p>Resource: \"jenkins.io/v1, Resource=releases\", GroupVersionKind: \"jenkins.io/v1, Kind=Release\"</p> </li> <li> <p>Name: \"controllerrole-2.0.1243\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=roles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=Role\"</p> </li> <li> <p>Name: \"controllerrole\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=rolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=RoleBinding\"</p> </li> <li> <p>Name: \"controllerrole\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"/v1, Resource=serviceaccounts\", GroupVersionKind: \"/v1, Kind=ServiceAccount\"</p> </li> <li> <p>Name: \"jenkins-x-controllerrole\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=clusterrolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding\"</p> </li> <li> <p>Name: \"gcactivities-jx\", Namespace: \"\"</p> </li> <li> <p>Resource: \"batch/v1beta1, Resource=cronjobs\", GroupVersionKind: \"batch/v1beta1, Kind=CronJob\"</p> </li> <li> <p>Name: \"jenkins-x-gcactivities\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"jenkins.io/v1, Resource=releases\", GroupVersionKind: \"jenkins.io/v1, Kind=Release\"</p> </li> <li> <p>Name: \"gcactivities-2.0.1243\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=roles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=Role\"</p> </li> <li> <p>Name: \"gcactivities\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=rolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=RoleBinding\"</p> </li> <li> <p>Name: \"gcactivities\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"/v1, Resource=serviceaccounts\", GroupVersionKind: \"/v1, Kind=ServiceAccount\"</p> </li> <li> <p>Name: \"jenkins-x-gcactivities\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"batch/v1beta1, Resource=cronjobs\", GroupVersionKind: \"batch/v1beta1, Kind=CronJob\"</p> </li> <li> <p>Name: \"jenkins-x-gcpods\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"jenkins.io/v1, Resource=releases\", GroupVersionKind: \"jenkins.io/v1, Kind=Release\"</p> </li> <li> <p>Name: \"gcpods-2.0.1243\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=roles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=Role\"</p> </li> <li> <p>Name: \"gcpods\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=rolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=RoleBinding\"</p> </li> <li> <p>Name: \"gcpods\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"/v1, Resource=serviceaccounts\", GroupVersionKind: \"/v1, Kind=ServiceAccount\"</p> </li> <li> <p>Name: \"jenkins-x-gcpods\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=clusterrolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding\"</p> </li> <li> <p>Name: \"gcpreviews-jx\", Namespace: \"\"</p> </li> <li> <p>Resource: \"batch/v1beta1, Resource=cronjobs\", GroupVersionKind: \"batch/v1beta1, Kind=CronJob\"</p> </li> <li> <p>Name: \"jenkins-x-gcpreviews\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"jenkins.io/v1, Resource=releases\", GroupVersionKind: \"jenkins.io/v1, Kind=Release\"</p> </li> <li> <p>Name: \"gcpreviews-2.0.1243\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=roles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=Role\"</p> </li> <li> <p>Name: \"gcpreviews\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=rolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=RoleBinding\"</p> </li> <li> <p>Name: \"gcpreviews\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"/v1, Resource=serviceaccounts\", GroupVersionKind: \"/v1, Kind=ServiceAccount\"</p> </li> <li> <p>Name: \"jenkins-x-gcpreviews\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1beta1, Resource=clusterrolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=ClusterRoleBinding\"</p> </li> <li> <p>Name: \"jenkins-x-heapster\", Namespace: \"\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1beta1, Resource=roles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=Role\"</p> </li> <li> <p>Name: \"jenkins-x-heapster-pod-nanny\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1beta1, Resource=rolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=RoleBinding\"</p> </li> <li> <p>Name: \"jenkins-x-heapster-pod-nanny\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"/v1, Resource=serviceaccounts\", GroupVersionKind: \"/v1, Kind=ServiceAccount\"</p> </li> <li> <p>Name: \"jenkins-x-heapster\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"/v1, Resource=persistentvolumeclaims\", GroupVersionKind: \"/v1, Kind=PersistentVolumeClaim\"</p> </li> <li> <p>Name: \"jenkins\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1beta1, Resource=clusterrolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=ClusterRoleBinding\"</p> </li> <li> <p>Name: \"jenkins-jx-role-binding\", Namespace: \"\"</p> </li> <li> <p>Resource: \"/v1, Resource=serviceaccounts\", GroupVersionKind: \"/v1, Kind=ServiceAccount\"</p> </li> <li> <p>Name: \"jenkins\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=roles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=Role\"</p> </li> <li> <p>Name: \"committer\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=roles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=Role\"</p> </li> <li> <p>Name: \"jx-view\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=roles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=Role\"</p> </li> <li> <p>Name: \"owner\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=roles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=Role\"</p> </li> <li> <p>Name: \"viewer\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"/v1, Resource=namespaces\", GroupVersionKind: \"/v1, Kind=Namespace\"</p> </li> <li> <p>Name: \"jx\", Namespace: \"\"</p> </li> <li> <p>Resource: \"extensions/v1beta1, Resource=ingresses\", GroupVersionKind: \"extensions/v1beta1, Kind=Ingress\"</p> </li> <li> <p>Name: \"chartmuseum\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"extensions/v1beta1, Resource=ingresses\", GroupVersionKind: \"extensions/v1beta1, Kind=Ingress\"</p> </li> <li> <p>Name: \"hook\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"jenkins.io/v1, Resource=schedulers\", GroupVersionKind: \"jenkins.io/v1, Kind=Scheduler\"</p> </li> <li> <p>Name: \"default-scheduler\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"jenkins.io/v1, Resource=environments\", GroupVersionKind: \"jenkins.io/v1, Kind=Environment\"</p> </li> <li> <p>Name: \"dev\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"jenkins.io/v1, Resource=sourcerepositories\", GroupVersionKind: \"jenkins.io/v1, Kind=SourceRepository\"</p> </li> <li> <p>Name: \"jx-env-dev\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"jenkins.io/v1, Resource=schedulers\", GroupVersionKind: \"jenkins.io/v1, Kind=Scheduler\"</p> </li> <li> <p>Name: \"env-scheduler\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"jenkins.io/v1, Resource=schedulers\", GroupVersionKind: \"jenkins.io/v1, Kind=Scheduler\"</p> </li> <li> <p>Name: \"pr-only\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"jenkins.io/v1, Resource=environments\", GroupVersionKind: \"jenkins.io/v1, Kind=Environment\"</p> </li> <li> <p>Name: \"production\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"jenkins.io/v1, Resource=sourcerepositories\", GroupVersionKind: \"jenkins.io/v1, Kind=SourceRepository\"</p> </li> <li> <p>Name: \"jx-env-prod\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"jenkins.io/v1, Resource=schedulers\", GroupVersionKind: \"jenkins.io/v1, Kind=Scheduler\"</p> </li> <li> <p>Name: \"release-only\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"jenkins.io/v1, Resource=environments\", GroupVersionKind: \"jenkins.io/v1, Kind=Environment\"</p> </li> <li> <p>Name: \"staging\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"jenkins.io/v1, Resource=sourcerepositories\", GroupVersionKind: \"jenkins.io/v1, Kind=SourceRepository\"</p> </li> <li> <p>Name: \"jx-env-staging\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1beta1, Resource=roles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=Role\"</p> </li> <li> <p>Name: \"jenkins-x-lighthouse\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1beta1, Resource=rolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=RoleBinding\"</p> </li> <li> <p>Name: \"jenkins-x-lighthouse\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"/v1, Resource=serviceaccounts\", GroupVersionKind: \"/v1, Kind=ServiceAccount\"</p> </li> <li> <p>Name: \"jenkins-x-lighthouse\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1beta1, Resource=rolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=RoleBinding\"</p> </li> <li> <p>Name: \"tide\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1beta1, Resource=roles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=Role\"</p> </li> <li> <p>Name: \"tide\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"/v1, Resource=serviceaccounts\", GroupVersionKind: \"/v1, Kind=ServiceAccount\"</p> </li> <li> <p>Name: \"tide\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"policy/v1beta1, Resource=podsecuritypolicies\", GroupVersionKind: \"policy/v1beta1, Kind=PodSecurityPolicy\"</p> </li> <li> <p>Name: \"tekton-pipelines\", Namespace: \"\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1beta1, Resource=clusterrolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=ClusterRoleBinding\"</p> </li> <li> <p>Name: \"tekton-pipelines-jx\", Namespace: \"\"</p> </li> <li> <p>Resource: \"/v1, Resource=serviceaccounts\", GroupVersionKind: \"/v1, Kind=ServiceAccount\"</p> </li> <li> <p>Name: \"tekton-pipelines\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"/v1, Resource=serviceaccounts\", GroupVersionKind: \"/v1, Kind=ServiceAccount\"</p> </li> <li> <p>Name: \"tekton-bot\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"apiextensions.k8s.io/v1beta1, Resource=customresourcedefinitions\", GroupVersionKind: \"apiextensions.k8s.io/v1beta1, Kind=CustomResourceDefinition\"</p> </li> <li> <p>Name: \"clustertasks.tekton.dev\", Namespace: \"\"</p> </li> <li> <p>Resource: \"apiextensions.k8s.io/v1beta1, Resource=customresourcedefinitions\", GroupVersionKind: \"apiextensions.k8s.io/v1beta1, Kind=CustomResourceDefinition\"</p> </li> <li> <p>Name: \"conditions.tekton.dev\", Namespace: \"\"</p> </li> <li> <p>Resource: \"apiextensions.k8s.io/v1beta1, Resource=customresourcedefinitions\", GroupVersionKind: \"apiextensions.k8s.io/v1beta1, Kind=CustomResourceDefinition\"</p> </li> <li> <p>Name: \"images.caching.internal.knative.dev\", Namespace: \"\"</p> </li> <li> <p>Resource: \"apiextensions.k8s.io/v1beta1, Resource=customresourcedefinitions\", GroupVersionKind: \"apiextensions.k8s.io/v1beta1, Kind=CustomResourceDefinition\"</p> </li> <li> <p>Name: \"pipelines.tekton.dev\", Namespace: \"\"</p> </li> <li> <p>Resource: \"apiextensions.k8s.io/v1beta1, Resource=customresourcedefinitions\", GroupVersionKind: \"apiextensions.k8s.io/v1beta1, Kind=CustomResourceDefinition\"</p> </li> <li> <p>Name: \"pipelineruns.tekton.dev\", Namespace: \"\"</p> </li> <li> <p>Resource: \"apiextensions.k8s.io/v1beta1, Resource=customresourcedefinitions\", GroupVersionKind: \"apiextensions.k8s.io/v1beta1, Kind=CustomResourceDefinition\"</p> </li> <li> <p>Name: \"pipelineresources.tekton.dev\", Namespace: \"\"</p> </li> <li> <p>Resource: \"apiextensions.k8s.io/v1beta1, Resource=customresourcedefinitions\", GroupVersionKind: \"apiextensions.k8s.io/v1beta1, Kind=CustomResourceDefinition\"</p> </li> <li> <p>Name: \"tasks.tekton.dev\", Namespace: \"\"</p> </li> <li> <p>Resource: \"apiextensions.k8s.io/v1beta1, Resource=customresourcedefinitions\", GroupVersionKind: \"apiextensions.k8s.io/v1beta1, Kind=CustomResourceDefinition\"</p> </li> <li> <p>Name: \"taskruns.tekton.dev\", Namespace: \"\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=clusterrolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding\"</p> </li> <li> <p>Name: \"tekton-bot-jx\", Namespace: \"\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=roles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=Role\"</p> </li> <li> <p>Name: \"tekton-bot\", Namespace: \"jx\"</p> </li> <li> <p>Resource: \"rbac.authorization.k8s.io/v1, Resource=rolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1, Kind=RoleBinding\"</p> </li> <li>Name: \"tekton-bot\", Namespace: \"jx\"</li> </ul>"},{"location":"jenkinsx/rhos-311-restricted/#jenkins-x-crd-list","title":"Jenkins X CRD List","text":"<pre><code>kubectl api-resources --api-group=jenkins.io\n</code></pre> <pre><code>NAME                      SHORTNAMES                                 APIGROUP     NAMESPACED   KIND\napps                      app                                        jenkins.io   true         App\nbuildpacks                bp                                         jenkins.io   true         BuildPack\ncommitstatuses            commitstatus                               jenkins.io   true         CommitStatus\nenvironmentrolebindings   envrolebindings,envrolebinding,envrb,erb   jenkins.io   true         EnvironmentRoleBinding\nenvironments              env                                        jenkins.io   true         Environment\nextensions                extension,ext                              jenkins.io   true         Extension\nfacts                     fact                                       jenkins.io   true         Fact\ngitservices               gits,gs                                    jenkins.io   true         GitService\npipelineactivities        activity,act,pa                            jenkins.io   true         PipelineActivity\npipelinestructures        structure,ps                               jenkins.io   true         PipelineStructure\nplugins                                                              jenkins.io   true         Plugin\nreleases                  rel                                        jenkins.io   true         Release\nschedulers                scheduler                                  jenkins.io   true         Scheduler\nsourcerepositories        sourcerepo,srcrepo,sr                      jenkins.io   true         SourceRepository\nsourcerepositorygroups    srg                                        jenkins.io   true         SourceRepositoryGroup\nteams                     tm                                         jenkins.io   true         Team\nusers                     usr                                        jenkins.io   true         User\nworkflows                 flow                                       jenkins.io   true         Workflow\n</code></pre> <pre><code>kubectl api-resources -o name\n</code></pre> <pre><code>apps.jenkins.io\nbuildpacks.jenkins.io\ncommitstatuses.jenkins.io\nenvironmentrolebindings.jenkins.io\nenvironments.jenkins.io\nextensions.jenkins.io\nfacts.jenkins.io\ngitservices.jenkins.io\npipelineactivities.jenkins.io\npipelinestructures.jenkins.io\nplugins.jenkins.io\nreleases.jenkins.io\nschedulers.jenkins.io\nsourcerepositories.jenkins.io\nsourcerepositorygroups.jenkins.io\nteams.jenkins.io\nusers.jenkins.io\nworkflows.jenkins.io\n</code></pre>"},{"location":"jenkinsx/rhos-311-restricted/#references","title":"References","text":"<ol> <li> <p>https://kubernetes.io/docs/concepts/storage/volumes/#emptydir \u21a9</p> </li> <li> <p>https://jenkins-x.io/docs/getting-started/first-project/create-quickstart/ \u21a9</p> </li> <li> <p>https://github.com/jenkins-x-quickstarts \u21a9</p> </li> <li> <p>https://github.com/jenkins-x-quickstarts/golang-http \u21a9</p> </li> <li> <p>https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs \u21a9</p> </li> <li> <p>https://jenkins-x.io/docs/concepts/technology/#whats-is-exposecontroller \u21a9</p> </li> <li> <p>https://github.com/jenkins-x/exposecontroller/blob/master/exposestrategy/ingress.go#L48 \u21a9</p> </li> <li> <p>https://jenkins-x.io/docs/reference/pipeline-syntax-reference/ \u21a9</p> </li> <li> <p>https://technologyconversations.com/2019/06/30/overriding-pipelines-stages-and-steps-and-implementing-loops-in-jenkins-x-pipelines/ \u21a9</p> </li> <li> <p>https://jenkins-x.io/docs/getting-started/promotion/ \u21a9</p> </li> <li> <p>https://github.com/jenkins-x/exposecontroller#exposer-types \u21a9</p> </li> <li> <p>https://jenkins-x.io/docs/getting-started/build-test-preview/#generating-a-preview-environment \u21a9</p> </li> <li> <p>https://jenkins-x.io/docs/reference/preview/#charts \u21a9</p> </li> <li> <p>https://cert-manager.io/docs/installation/openshift/ \u21a9</p> </li> </ol>"},{"location":"jenkinsx/serverless/","title":"Jenkins X Serverless","text":""},{"location":"jenkinsx/serverless/#what","title":"What","text":"<ul> <li>Tekton</li> <li>Jenkins X Serverless</li> <li>Jenkins X Pipelines</li> </ul>"},{"location":"jenkinsx/serverless/#commands","title":"Commands","text":""},{"location":"jenkinsx/serverless/#create-cluster","title":"Create Cluster","text":"<pre><code>jx create cluster gke \\\n    --cluster-name jx-rocks \\\n    --project-id $PROJECT \\\n    --region us-east1 \\\n    --machine-type n1-standard-2 \\\n    --min-num-nodes 1 \\\n    --max-num-nodes 2 \\\n    --default-admin-password=admin \\\n    --default-environment-prefix jx-rocks \\\n    --git-provider-kind github \\\n    --batch-mode\n</code></pre>"},{"location":"jenkinsx/serverless/#install-jx","title":"Install JX","text":"<p>Where Project, is Gcloud Project ID.</p> <p>Requires <code>docker-registry gcr.io</code>, else it doesn't work.</p> <pre><code>jx install \\\n    --provider $PROVIDER \\\n    --external-ip $LB_IP \\\n    --domain $DOMAIN \\\n    --default-admin-password=admin \\\n    --ingress-namespace $INGRESS_NS \\\n    --ingress-deployment $INGRESS_DEP \\\n    --default-environment-prefix tekton \\\n    --git-provider-kind github \\\n    --namespace ${INSTALL_NS} \\\n    --prow \\\n    --docker-registry gcr.io \\\n    --docker-registry-org $PROJECT \\\n    --tekton \\\n    --kaniko \\\n    -b\n</code></pre>"},{"location":"jenkinsx/serverless/#notes","title":"Notes","text":"<ul> <li><code>jx create cluster gke</code> cannot use kubernetes server version flag<ul> <li>it somehow sets an empty <code>--machineType</code> flag instead</li> </ul> </li> <li><code>jx install --tekton --prow</code> requires <code>--dockerRegistry</code> to be set</li> <li><code>jx install --tekton --prow</code> can be install multiple times in the same cluster<ul> <li>to differentiate, set different namespace (which becomes part of the domain)</li> <li>might need to update webhooks incase env's already existed</li> </ul> </li> <li>two jx serverless installs, and now <code>jx get build logs</code> doesn't work<ul> <li><code>error: no Tekton pipelines have been triggered which match the current filter</code></li> </ul> </li> </ul>"},{"location":"jenkinsx/workshop/","title":"Jenkins X Workshop","text":""},{"location":"jenkinsx/workshop/#create-cluster","title":"Create Cluster","text":"<pre><code>PROJECT=\nNAME=ws-feb\nZONE=europe-west4\nMACHINE=n1-standard-2\nMIN_NODES=3\nMAX_NODES=5\nPASS=admin\nPREFIX=ws\n</code></pre> <p>Warning</p> <p>You might want to do this: (not sure why) <pre><code>    echo \"nexus:\n    enabled: false\n    \" | tee myvalues.yaml\n</code></pre></p> <pre><code>jx create cluster gke -n $NAME -p $PROJECT -z $ZONE -m $MACHINE \\\n    --min-num-nodes $MIN_NODES --max-num-nodes $MAX_NODES \\\n    --default-admin-password=$PASS \\\n    --default-environment-prefix $NAME\n</code></pre>"},{"location":"jenkinsx/workshop/#alternatively","title":"Alternatively","text":"<p>Info</p> <p>Domain will get <code>.jx</code> as a prefix anyway.</p> <pre><code>JX_CLUSTER_NAME=joostvdg\nJX_DOMAIN=kearos.net\nJX_GIT_USER=joostvdg\nJX_ORG=joostvdg\nJX_K8S_REGION=europe-west4\nJX_NAME=jx-joostvdg\n</code></pre> <pre><code>JX_API_TOKEN=\nJX_ADMIN_PSS=\nJX_GCE_PROJECT=\n</code></pre> <pre><code>jx create cluster gke \\\n    -n ${JX_NAME} \\\n    --exposer='Ingress' \\\n    --preemptible=false \\\n    --cluster-name=\"${JX_CLUSTER_NAME}\" \\\n    --default-admin-password=\"${JX_ADMIN_PSS}\" \\\n    --domain=\"${JX_DOMAIN}\" \\\n    --machine-type='n1-standard-2' \\\n    --max-num-nodes=3 \\\n    --min-num-nodes=2 \\\n    --project-id=${JX_GCE_PROJECT} \\\n    --default-environment-prefix ${JX_NAME} \\\n    --zone=\"${JX_ZONE}\" \\\n    --http='false' \\\n    --tls-acme='true' \\\n    --skip-login\n</code></pre> <pre><code>    --prow \\\n    --no-tiller='true'\\\n        --vault='true' \\\n</code></pre>"},{"location":"jenkinsx/workshop/#issues","title":"Issues","text":"<ul> <li><code>-b</code> doesn't work with <code>--vault</code> as the config is empty</li> <li><code>--vault='true'</code> doesn't work with https (cert-manager) because <code>sync.go:64] Not syncing ingress jx/cm-acme-http-solver-stx47 as it does not contain necessary annotations</code> ** also, it seems its TLS config isn't correct for some reason</li> <li>does TLS with cert-manager actually work?</li> <li>now it doesn't actually install <code>cert-manager</code>? Whats up with that.</li> </ul>"},{"location":"jenkinsx/workshop/#install-certmanager","title":"Install Certmanager","text":""},{"location":"jenkinsx/workshop/#via-jx","title":"Via JX","text":"<p>Updates the entire ingress configuration, installs cert-mananger, certificates, replaces ingress definitions, updates webhooks, and allows you to set a different domain name.</p> <pre><code>jx upgrade ingress --cluster\n</code></pre>"},{"location":"jenkinsx/workshop/#manually","title":"Manually","text":"<p>This does not create certificates nor does it update the ingress defintions.</p> <pre><code>kubectl create namespace cert-manager\nkubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.6/deploy/manifests/00-crds.yaml\nhelm install --name cert-manager --namespace cert-manager stable/cert-manager\n</code></pre>"},{"location":"jenkinsx/workshop/#demo-app","title":"Demo App","text":""},{"location":"jenkinsx/workshop/#post-creation","title":"Post creation","text":"<pre><code>Creating GitHub webhook for joostvdg/cmg for url https://hook.jx.jx.kearos.net/hook\n\nWatch pipeline activity via:    jx get activity -f cmg -w\nBrowse the pipeline log via:    jx get build logs joostvdg/cmg/master\nOpen the Jenkins console via    jx console\nYou can list the pipelines via: jx get pipelines\nWhen the pipeline is complete:  jx get applications\n\nFor more help on available commands see: https://jenkins-x.io/developing/browsing/\n</code></pre>"},{"location":"jenkinsx/workshop/#promote","title":"Promote","text":"<pre><code>jx promote ${APP} --version $VERSION --env production -b\n</code></pre>"},{"location":"jenkinsx/workshop/#compliance","title":"Compliance","text":"<pre><code>jx compliance run\n</code></pre> <pre><code>jx compliance status\n</code></pre> <pre><code>jx compliance logs -f\n</code></pre> <pre><code>jx compliance delete\n</code></pre>"},{"location":"jenkinsx/workshop/#chartmuseum-auth-faillure","title":"Chartmuseum auth faillure","text":"<ul> <li>uses kubernetes secret</li> <li>relies on kubernetes-secret (Jenkins) plugin</li> <li>can have trouble with special charactes</li> <li>to fix, update the kubernetes secret (used by chartmuseum and the pipeline)</li> </ul>"},{"location":"jenkinsx/workshop/#workshop-responses","title":"Workshop responses","text":"<ul> <li>send to Alyssa &amp; Juni</li> <li>address for sending</li> </ul>"},{"location":"jenkinsx/workshop/#responses","title":"Responses","text":""},{"location":"jenkinsx/workshop/#extra-requirements","title":"Extra requirements","text":"<ul> <li>billing needs to be enabled, else you cannot create a cluster of that size</li> <li>we need to test more with windows ** without admin ** different python versions</li> </ul>"},{"location":"jenkinsx/java-native-prod/01-intro/","title":"Introduction","text":""},{"location":"jenkinsx/java-native-prod/01-intro/#stack","title":"Stack","text":"<ul> <li>Google Cloud Platform(GCP): while you can do evyrthing required with other providers, I've chosen GCP</li> <li>Kubernetes: in particular, GKE</li> <li>Jenkins X: CI/CD</li> <li>Helm: packaging our Kubernetes application<ul> <li>managed by Jenkins X (to a degree)</li> </ul> </li> <li>Google Cloud SQL(MySQL): our data storage</li> <li>HashiCorp Vault: secrets storage</li> <li>Quarkus: A Kubernetes Native Java stack tailored for OpenJDK HotSpot and GraalVM.<ul> <li>Spring Data JPA for ORM</li> <li>Spring Web for the REST API</li> </ul> </li> <li>Flyway: to manage our Database schema (introduced in Previews &amp; Integration Tests)</li> <li>Java 11</li> <li>GraalVM: compiler/runtime to create a native executable of our Java code</li> </ul>"},{"location":"jenkinsx/java-native-prod/01-intro/#what-we-will-do","title":"What We Will Do","text":"<p>The outline of the steps to take is below. Each has its own page, so if you feel you have.</p> <ul> <li>Create Google Cloud SQL (MySql flavor) as datasource (we're on GCP afterall)</li> <li>Create Quarkus application</li> <li>Import the application into Jenkins X</li> <li>Change the Build to Native Image (with GraalVM)</li> <li>Retrieve application secrets (such as Database username/password) from HashiCorp Vault</li> <li>Productionalize our Pipeline<ul> <li>Static Code Analysis with SonarQube/SonarCloud</li> <li>Dependency Vulnerability scan with Sonatype's OSS Index</li> <li>Integration Tests</li> </ul> </li> <li>Productionalize our Applications<ul> <li>Monitoring with Prometheus &amp; Grafana</li> <li>Tracing with OpenTracing &amp; Jaeger</li> <li>Manage our logs with Sentry.io</li> </ul> </li> <li>Promote the application to Jenkins X's Production environment</li> </ul>"},{"location":"jenkinsx/java-native-prod/01-intro/#pre-requisites","title":"Pre-requisites","text":"<p>The pre-requisites are a Kubernetes Cluster with Jenkins X installed, including Haschicorp Vault integration. The guide assumes you use GKE, we will create our MySQL database there, but should be reproducable on other Kubernetes services where Jenkins X supports Hashicorp Vault (currently GKE and AWS's EKS).</p> <p>If you want to focus on a stable production ready cluster, I can also recommend to use CloudBees' distribution of Jenkins X. Don't worry, this is also free with no caveats, but has a slower release candence to focus more on stability than the OSS mainline does.</p> <ul> <li>GKE Cluster: <ul> <li>GKE via Terraform</li> <li>GKE via Gcloud</li> <li>GKE via Jenkins X's Terraform module</li> </ul> </li> <li>EKS Cluster:<ul> <li>EKS via Jenkins X's Terraform module</li> <li>EKS via EKSCTL</li> <li>EKS</li> </ul> </li> <li>Jenkins X: <ul> <li>Jenkins X Getting Started on GKE Guide</li> <li>CloudBees Jenkins X Distribution</li> <li>Youtube video with installation and maintenance guidance</li> </ul> </li> </ul> <p>Important</p> <p>A little spoiler, but the Native Image build requires at least 6GB of memory but works best with about 8GB. This means your Kubernetes worker node that your build runs on, needs have at least about 10-12GB memory.</p> <p>If you're in GKE, as the guide assumes, the following machine types work:</p> <ul> <li><code>e2-highmem-2</code></li> <li><code>n2-highmem-2</code></li> <li><code>e2-standard-4</code></li> <li><code>n2-standard-4</code></li> </ul> <p>Keep in mind, you can use more than one Node Pool. You don't have to run all your nodes on these types, you need at least to be safe. Having autoscaling enabled for this Node Pool is recommended.</p>"},{"location":"jenkinsx/java-native-prod/01-intro/#why-quarkus","title":"Why Quarkus","text":"<p>Before we start, I'd like to make the case, why I chose to use Quarkus for this.</p> <p>Wanting to build a Native Image with Java 11 is part of the reason, we'll dive into that next.</p> <p>Quarkus has seen an tremendous amount of updates since its inception.  It is a really active framework, which does not require you to forget everything you've learned in other Java frameworks such as Spring and Spring Boot. I like to stay up-to-date with what happens in the Java community, so spending some time with Quarkus was on my todo list.</p> <p>It comes out of the same part from RedHat that is involved with OpenShift - RedHat's Kubernetes distribution. This ensures the framework is created with running Java on Kubernetes in mind.  Jenkins X starts from Kubernetes, so this makes it a natural fit.</p> <p>Next, the capabilities for making a Native Image and work done to ensure you - the developer - do not have to worry (too much) about how to get from a Spring application to a Native Image is staggering. This makes the Native Image experience pleasant and involve little to no debugging.</p>"},{"location":"jenkinsx/java-native-prod/01-intro/#resources","title":"Resources","text":"<ul> <li>https://quarkus.io/guides/writing-native-applications-tips</li> <li>https://quarkus.io/guides/building-native-image</li> <li>https://cloud.google.com/community/tutorials/run-spring-petclinic-on-app-engine-cloudsql</li> <li>https://github.com/GoogleCloudPlatform/community/tree/master/tutorials/run-spring-petclinic-on-app-engine-cloudsql/spring-petclinic/src/main/resources</li> <li>https://github.com/GoogleCloudPlatform/google-cloud-spanner-hibernate/blob/master/google-cloud-spanner-hibernate-samples/quarkus-jpa-sample</li> <li>https://medium.com/@hantsy/kickstart-your-first-quarkus-application-cde54f469973</li> <li>https://developers.redhat.com/blog/2020/04/10/migrating-a-spring-boot-microservices-application-to-quarkus/</li> <li>https://www.baeldung.com/rest-assured-header-cookie-parameter</li> <li>https://jenkins-x.io/docs/reference/pipeline-syntax-reference/#containerOptions</li> <li>https://openliberty.io/blog/2020/04/09/microprofile-3-3-open-liberty-20004.html#gra</li> <li>https://openliberty.io/docs/ref/general/#metrics-catalog.html</li> <li>https://grafana.com/grafana/dashboards/4701</li> <li>https://phauer.com/2017/dont-use-in-memory-databases-tests-h2/</li> <li>https://github.com/quarkusio/quarkus/tree/master/integration-tests</li> <li>https://hub.docker.com/r/postman/newman</li> <li>https://github.com/postmanlabs/newman</li> <li>https://learning.postman.com/docs/postman/launching-postman/introduction/</li> <li>https://jenkins-x.io/docs/guides/using-jx/pipelines/envvars/</li> <li>https://github.com/quarkusio/quarkus/tree/master/integration-tests/flyway/</li> <li>https://quarkus.io/guides/flyway</li> <li>https://rhuanrocha.net/2019/03/17/how-to-microprofile-opentracing-with-jaeger/</li> <li>https://medium.com/jaegertracing/microprofile-tracing-in-supersonic-subatomic-quarkus-43020f89a753</li> <li>https://github.com/opentracing-contrib/java-jdbc</li> <li>https://quarkus.io/guides/opentracing</li> <li>https://cloud.google.com/sql/docs/mysql/connect-kubernetes-engine?hl=en_US</li> <li>https://dbabulletin.com/index.php/2018/03/29/best-practices-using-flyway-for-database-migrations/</li> </ul>"},{"location":"jenkinsx/java-native-prod/02-cloud-sql/","title":"Google Cloud SQL","text":"<p>As with any public cloud, Google Cloud has classic Relation Database Management System offering. In GCP this is called CloudSQL, and supports several flavors such a MySQL and Postgres.</p> <p>We will explore the different ways you can create a CloudSQL database, so it should work for everyone.</p>"},{"location":"jenkinsx/java-native-prod/02-cloud-sql/#requirements","title":"Requirements","text":"<p>For this guide we need a MySQL Database.</p> <p>Personally, I don't like managing databases. I know enough to use them, and I while I know that I can get one in Kubernetes with ease, I still rather not manage it over time. With storage, upgrades, backups &amp; restore and so on. Which are definitely concerns you need to take care off, with a Database in Production.</p> <p>So, while it is not a hard requirement I strongly encourage you to use a managed database. And when in Google Cloud, the initial step is Cloud SQL.</p> <p>Staying true to the classics - at least for me - I want it to be like MySQL. No real reason, if you want Postgres or something else, go ahead. Most of this guide will be exactly the same.</p>"},{"location":"jenkinsx/java-native-prod/02-cloud-sql/#ui","title":"UI","text":"<p>If you do not want to automate the database creation, or you at least want to have a UI while exploring the options, Google offers a clean UI with a guide to get you started. Or even more handson, a Create a Managed MySQL Database with Cloud SQL Lab!</p> <p>The guide is very intuitive, so I'll leave you to it. If you prefer a CLI or Terraform, read on!</p>"},{"location":"jenkinsx/java-native-prod/02-cloud-sql/#gcloud-cli","title":"Gcloud CLI","text":"<p><code>gcloud</code> Is Google Cloud's CLI for interacting with Google Cloud API's.</p> <p>If you don't have it installed yet, read the installation guide, and after, the initialization guide for setting up access.</p> <p>Ensure you have a working <code>gcloud</code> with a default project configured. Use <code>gcloud config list</code>, to verify this is the case. From there, creating the MySQL database we need is straightforward.</p> <p>We're going to execute to steps:</p> <ol> <li>enable the API</li> <li>we create the Database Instace, read here for more info on this command</li> <li>we create the Database in the Instance, so please do remember the name! read here for more info on that command</li> </ol>"},{"location":"jenkinsx/java-native-prod/02-cloud-sql/#enable-cloud-sql-api","title":"Enable Cloud SQL API","text":"<pre><code>gcloud services enable sqladmin.googleapis.com\n</code></pre>"},{"location":"jenkinsx/java-native-prod/02-cloud-sql/#create-database-instance","title":"Create Database Instance","text":"<pre><code>gcloud sql instances create quarkus-fruits \\\n      --database-version=MYSQL_5_7 --tier=db-n1-standard-1 \\\n      --region=europe-west4 --root-password=password123\n</code></pre> <p>Warning</p> <p>Do change the root-password!</p>"},{"location":"jenkinsx/java-native-prod/02-cloud-sql/#create-database","title":"Create Database","text":"<pre><code>gcloud sql databases create fruits --instance quarkus-fruits\n</code></pre>"},{"location":"jenkinsx/java-native-prod/02-cloud-sql/#terraform","title":"Terraform","text":"<p>I'm a big fan of Configuration-as-Code, and Terraform is the absolute poster child of this concept. This is not a guide on Terraform, so we do dive further into the best practices. I do suggest you read about Terraform Backends.</p> <p>We won't use one in this guide - for brevity - but do read it and think about using it.</p> <p>In general, we use three files with Terraform:</p> <ol> <li>maint.tf: the main file with Module definitions and, if possible, the bulk of the configuration</li> <li>variables.tf: input variables, so we can more easily re-use or share our files</li> <li>outputs.tf: in the event you have output from the created resources, we have no outputs, so no file</li> </ol>"},{"location":"jenkinsx/java-native-prod/02-cloud-sql/#process","title":"Process","text":"<p>First we initialize our configuration. This ensures Terraform has the right modules available to talk to this particular cloud API.</p> <pre><code>terraform init\n</code></pre> <p>Then we let Terraform plan what needs to be done. Terraform is declarative, meaning, we state our end-result Terraform will make it happen. Terraform plan, lets Terraform tell us how it wants to do so.</p> <p>We let Terraform output the plan to disk, so we can apply the plan, rather than letting Terraform figure it out again.</p> <pre><code>terraform plan --out plan.out\n</code></pre> <p>If the plan is valid and looks correct, we can apply it. Now terraform will create our resources.</p> <pre><code>terraform apply \"plan.out\"\n</code></pre>"},{"location":"jenkinsx/java-native-prod/02-cloud-sql/#terraform-files","title":"Terraform Files","text":"<p>main.tf<pre><code>terraform {\n        required_version = \"~&gt; 0.12\"\n}\n\n# https://www.terraform.io/docs/providers/google/index.html\nprovider \"google\" {\n        version   = \"~&gt; 2.18.1\"\n        project   = var.project\n        region    = var.region\n        zone      = var.zone\n}\n\nresource \"google_sql_database_instance\" \"master\" {\n        name             = var.database_instance_name\n        database_version = \"MYSQL_5_7\"\n        region           = var.region\n\n        settings {\n                # Second-generation instance tiers are based on the machine\n                # type. See argument reference below.\n                tier = var.database_instance_tier\n        }\n}\n\nresource \"google_sql_database\" \"database\" {\n        name     = var.database_name\n        instance = google_sql_database_instance.master.name\n}\n</code></pre> </p> <p>variables.tf<pre><code>variable \"project\" { \n        description = \"GCP Project ID\"\n}\n\nvariable \"region\" {\n        default =\"europe-west4\"\n        description = \"GCP Region\"\n}\n\nvariable \"zone\" {\n        default = \"europe-west4-a\"\n        description = \"GCP Zone, should be within the Region\"\n}\n\nvariable \"database_instance_name\" {\n        description = \"The name of the database instance\"\n        default     = \"quarkus-fruits\"\n}\nvariable \"database_instance_tier\" {\n        default = \"db-n1-standard-1\"\n}\n\nvariable \"database_name\" {\n        description = \"The name of the database\"\n        default     = \"fruits\"\n}\n</code></pre> </p>"},{"location":"jenkinsx/java-native-prod/02-cloud-sql/#how-to-connect-to-the-database","title":"How To Connect To The Database","text":"<p>To access a Cloud SQL instance from an application running in Google Kubernetes Engine, you can use either the Cloud SQL Proxy (with public or private IP), or connect directly using a private IP address. The Cloud SQL Proxy is the recommended way to connect to Cloud SQL, even when using private IP. This is because the proxy provides strong encryption and authentication using IAM, which can help keep your database secure. MySQL Connect From Kubernetes Guide</p> <p>Our appliction runs in Kubernetes, so the above guide is perfect for us. It is recommened to use the <code>Cloud SQL Proxy</code> and I tend to listen to such advise. So in this guide, we're going to use the proxy.</p> <p>We won't configure the proxy until later in the guide, but lets make sure we have the pre-requisites in place.</p> <p>We need:</p> <ol> <li>a Google Cloud Service Account with access to Cloud SQL</li> <li>a JSON key for this Service Account, which the proxy can use as credentials</li> </ol>"},{"location":"jenkinsx/java-native-prod/02-cloud-sql/#create-service-account","title":"Create Service Account","text":"<p>To create the Service Account, you can either the UI or the <code>gcloud</code> CLI.</p> <p>Step one, create the Service Account:</p> <pre><code>gcloud iam service-accounts create my-sa-123 \\\n    --description=\"sa-description\" \\\n    --display-name=\"sa-display-name\"\n</code></pre> <p>Step two, give it the required permissions for Cloud SQL.</p> <pre><code>gcloud projects add-iam-policy-binding my-project-123 \\\n  --member serviceAccount:my-sa-123@my-project-123.iam.gserviceaccount.com \\\n  --role roles/cloudsql.admin\n</code></pre> <p>Important</p> <p>Do make sure you change the values such as <code>my-sa-123</code> and <code>my-project-123</code> to your values.</p>"},{"location":"jenkinsx/java-native-prod/02-cloud-sql/#generate-json-key","title":"Generate JSON Key","text":"<p>To create the Service Account key you can either use the UI, or the <code>gcloud</code> CLI.</p> <pre><code>gcloud iam service-accounts keys create ~key.json \\\n  --iam-account &lt;YOUR-SA-NAME&gt;&gt;@project-id.iam.gserviceaccount.com\n</code></pre> <p>And make sure you save the key.json, we will use it later.</p>"},{"location":"jenkinsx/java-native-prod/03-quarkus/","title":"Create Quarkus Application","text":"<p>There are several ways you can create a Quarkus application.</p> <p>You can create one by going to code.quarkus.io, fill in your details and select your dependencies - this is an API call, so automatable. You can start with an maven archetype and add Quarkus details, or you can start from one of the Quarkus Quickstarts.</p> <p>In this guide, we start with a Quarkus Quickstart (Spring Data JPA to be exact) and modify this to suit our needs.</p>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#fork-clone-or-copy","title":"Fork, Clone, or Copy","text":"<p>We're going to start from Quarkus' <code>spring-data-jpa-quickstart</code>, I leave it up to you how you get the code in your own repository. You can fork it, clone it and copy it or whatever floats your boat.</p> <p>You can find the quickstart here, and for reference, the Quarkus Guide that comes with it, here.</p>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#github-cli","title":"GitHub CLI","text":"<p>GitHub has a nice CLI that can help us here. There was hub before, but now there's gh which is better for the most common use cases.</p> Clone Quarkus QuickstartsCreate New GitHub RepoCopy Quickstart to Repo <pre><code>git clone https://github.com/quarkusio/quarkus-quickstarts.git\n</code></pre> <pre><code>gh repo create joostvdg/quarkus-fruits  -d \"Quarkus Fruits Demo\" --public\ncd ./quarkus-fruits/\n</code></pre> <pre><code>cp -R ../quarkus-quickstarts/spring-data-jpa-quickstart/ .\n</code></pre>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#update-project-configuration","title":"Update Project Configuration","text":""},{"location":"jenkinsx/java-native-prod/03-quarkus/#change-compiler-source-to-java-11","title":"Change compiler source to Java 11","text":"<p>pom.xml</p> <p>Replace the 1.8 with <code>11</code>.   <pre><code>&lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;\n&lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;\n</code></pre></p> <pre><code>&lt;maven.compiler.source&gt;11&lt;/maven.compiler.source&gt;\n&lt;maven.compiler.target&gt;11&lt;/maven.compiler.target&gt;\n</code></pre>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#update-artifact-metadata","title":"Update Artifact Metadata","text":"<p>This is optional, but I prefer not having my application be known as <code>org.acme:spring-data-jpa-quickstart</code>. So we update the fields <code>groupId</code> and <code>artifactId</code> in the pom.xml.</p> <p>pom.xml</p> <pre><code>&lt;groupId&gt;com.github.joostvdg.demo.jx&lt;/groupId&gt;\n&lt;artifactId&gt;quarkus-fruits&lt;/artifactId&gt;\n</code></pre>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#rename-package","title":"Rename Package","text":"<p>In the same vein, I'm renaming the packages in <code>src/main/java</code> and <code>src/test/java</code> to reflect the artifact's new group and artifact id's.</p>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#update-dependencies","title":"Update Dependencies","text":"<p>We're going to modify the application, so lets dive into the <code>pom.xml</code> and make our changes.</p> <p>First, we will use MySQL as our RDBMS so we drop the <code>quarkus-jdbc-postgresql</code> dependency.</p> <p>Next, we add our other spring dependencies and the <code>quarkus-jdbc-mysql</code> for MySQL.</p> <p>pom.xml</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n  &lt;artifactId&gt;quarkus-spring-web&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n  &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n  &lt;artifactId&gt;quarkus-spring-di&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n  &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n  &lt;artifactId&gt;quarkus-jdbc-mysql&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#remove-docker-plugin-configuration","title":"Remove Docker plugin configuration","text":"<p>The Quarkus Quickstart comes with a maven build plugin configuration for the Docker plugin. It is used to leverage Docker to start a Postgresql database for testing.</p> <p>One, we won't be able to rely on Docker when building with Jenkins X - a general Kubernetes best practice - and we're not using Postgresql.</p> <p>So remove the plugin <code>docker-maven-plugin</code> from the <code>&lt;build&gt; &lt;plugins&gt;</code> section of the pom.xml.</p> <p>pom.xml</p> <pre><code>&lt;plugin&gt;\n    &lt;!-- Automatically start PostgreSQL for integration testing - requires Docker --&gt;\n    &lt;groupId&gt;io.fabric8&lt;/groupId&gt;\n    &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt;\n    &lt;version&gt;${docker-plugin.version}&lt;/version&gt;\n    ...\n&lt;/plugun&gt;\n</code></pre>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#transform-resource-to-controller","title":"Transform Resource to Controller","text":"<p>Now that we're using Spring Web, we are going to change our Resource - FruitResource - to a Spring Web Controller.</p> <p>Replace this annotation:</p> <pre><code>@Path(\"/fruits\")\n</code></pre> <p>With this.</p> <pre><code>@RestController\n@RequestMapping(value = \"/fruits\")\n</code></pre> <p>Replace all <code>@PathParams</code> with Spring's <code>@PathVariable</code>'s. Mind you, these require the name of the variable as a parameter.</p> <p>For example:</p> <pre><code>@POST\n@Path(\"/name/{name}/color/{color}\")\n@Produces(\"application/json\")\npublic Fruit create(@PathParam String name, @PathParam String color) {}\n</code></pre> <p>Becomes:</p> <pre><code>@PostMapping(\"/name/{name}/color/{color}\")\npublic Fruit create(@PathVariable(value = \"name\") String name, @PathVariable(value = \"color\") String color) {\n</code></pre> <p>Then, replace the Http method annotations for the methods:</p> <ul> <li><code>@GET</code> with  <code>@GetMapping</code> </li> <li><code>@DELETE</code> with <code>@DeleteMapping</code> </li> <li><code>@POST</code> with <code>@PostMapping</code> </li> <li><code>@PUT</code> with <code>@PutMapping</code></li> </ul> <p>Note</p> <p>Spring's annotation includes the path, so you can collapse the <code>@PATH</code> into the Http method annotation.</p> <p>For example:</p> <pre><code>@GET\n@Path(\"/color/{color}\")\n@Produces(\"application/json\")\n</code></pre> <p>Becomes:</p> <pre><code>@GetMapping(\"/color/{color}\")\n</code></pre> FruitResource.java <pre><code>@RestController\n@RequestMapping(value = \"/fruits\")\npublic class FruitResource {\n\n    private final FruitRepository fruitRepository;\n\n    public FruitResource(FruitRepository fruitRepository) {\n        this.fruitRepository = fruitRepository;\n    }\n\n    @GetMapping(\"/\")\n    public List&lt;Fruit&gt; findAll() {\n        ...\n    }\n\n    @DeleteMapping(\"{id}\")\n    public void delete(@PathVariable(value = \"id\") long id) {\n        ...\n    }\n\n    @PostMapping(\"/name/{name}/color/{color}\")\n    public Fruit create(@PathVariable(value = \"name\") String name, @PathVariable(value = \"color\") String color) {\n        ...\n    }\n\n    @PutMapping(\"/id/{id}/color/{color}\")\n    public Fruit changeColor(@PathVariable(value = \"id\") Long id, @PathVariable(value = \"color\") String color) {\n        ...\n    }\n\n    @GetMapping(\"/color/{color}\")\n    public List&lt;Fruit&gt; findByColor(@PathVariable(value = \"color\") String color) {\n        ...\n    }\n}\n</code></pre>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#update-application-properties","title":"Update Application Properties","text":"<p>Let's update the applications properties, an initial configuration for MySQL.</p> <p>For the username and password, we use environment variables wich we will address later - when we import the aplication into Jenkins X.</p> <p>The JDBC URL looks a bit weird, but this has to do with how Google Cloud SQL can be accessed via Kubernetes. This is enough for now, we'll come back for more, don't worry.</p> <pre><code>quarkus.datasource.db-kind=mysql\nquarkus.datasource.jdbc.url=jdbc:mysql://127.0.0.1:3306/fruits\nquarkus.datasource.jdbc.max-size=8\nquarkus.datasource.jdbc.min-size=2\n\nquarkus.datasource.username=${GOOGLE_SQL_USER}\nquarkus.datasource.password=${GOOGLE_SQL_PASS}\n</code></pre>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#unit-testing","title":"Unit Testing","text":"<p>In order to build our application, we now need a MySQL database as we have unit tests, testing our FruitResource - as we should! Locally, we can address this by running MySQL as a Docker container. Unfortunately, when building applications in Jenkins X, we don't have access to Docker - you could, but in Kubernetes this is a big no-no. So, for now, we'll spin up an H2 database in MySQL mode to avoid the issue, but we should probably come back to that later.</p> <p>This was in part inspired by @hantsy's post on creating your first Quarkus application on Medium, definitely worth a read in general.</p> <p>In order to use the H2 database for our unit tests, we have to make three changes:</p> <ol> <li>delete the <code>FruitResourceIT</code> test class, we will solve this later in a different way</li> <li>add the H2 test dependency</li> <li>create a <code>application.properties</code> file for tests, in <code>src/test/resources</code></li> <li>annotate our test class with <code>@QuarkusTestResource(H2DatabaseTestResource.class)</code>, so Quarkus spins up the H2 database</li> </ol>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#add-dependency","title":"Add Dependency","text":"<p>pom.xml</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n  &lt;artifactId&gt;quarkus-test-h2&lt;/artifactId&gt;\n  &lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#create-test-properties-file","title":"Create Test Properties file","text":"<p>Create a new directory under <code>src/test</code> called <code>resources</code>. In this directory, create a new file called <code>application.properties</code>, with the contents below.</p> <p>src/test/resources/application.properties</p> <pre><code>quarkus.datasource.url=jdbc:h2:tcp://localhost/mem:fruits;MODE=MYSQL;DB_CLOSE_DELAY=-1\nquarkus.datasource.driver=org.h2.Driver\nquarkus.hibernate-orm.database.generation = drop-and-create\nquarkus.hibernate-orm.log.sql=true\n</code></pre>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#update-fruitresourcetest-annotations","title":"Update FruitResourceTest annotations","text":"<p>To use the H2 database for our tests, we add the <code>@QuarkusTestResource</code> to our <code>FruitResourceTest</code> test class.</p> <p>src/test/java/../FruitResourceTest.java</p> <pre><code>@QuarkusTestResource(H2DatabaseTestResource.class)\n@QuarkusTest\nclass FruitResourceTest {\n    ...\n}\n</code></pre>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#replace-jsonb-with-jackson","title":"Replace jsonb with jackson","text":"<p>Spring depends on <code>Jackson</code> for marshalling JSON to and from Java Objects. It makes sense to make our application depend on the same libary to reduce potential conflicts.</p> <p>Remove the <code>quarkus-resteasy-jsonb</code> dependency:</p> <p>pom.xml</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-resteasy-jsonb&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>And add  <code>quarkus-resteasy-jackson</code>.</p> <p>pom.xml</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n  &lt;artifactId&gt;quarkus-resteasy-jackson&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#update-fruitresource-findall","title":"Update FruitResource findAll","text":"<p>If you have tested our application, you might have noticed our <code>findAll()</code> method no longer works. This is because the <code>RestEasy Jackson</code> library doesn't properly marshall the Fruit's iterator. To solve this, we make and return a List instead.</p> <p>FruitResource.java</p> <pre><code>public List&lt;Fruit&gt; findAll() {\n    var it = fruitRepository.findAll();\n    List&lt;Fruit&gt; fruits = new ArrayList&lt;Fruit&gt;();\n    it.forEach(fruits::add);\n    return fruits;\n}\n</code></pre>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#test-that-it-works","title":"Test That It Works","text":"<pre><code>./mvnw clean test\n</code></pre> <p>The build should be successful and return as follows:</p> <pre><code>[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.634 s - in com.github.joostvdg.demo.jx.quarkusfruits.FruitResourceTest\n2020-05-31 16:20:09,388 INFO  [io.quarkus] (main) Quarkus stopped in 0.057s\n[INFO] H2 database was shut down; server status: Not started\n[INFO]\n[INFO] Results:\n[INFO]\n[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0\n[INFO]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  16.491 s\n[INFO] Finished at: 2020-05-31T16:20:09+02:00\n[INFO] ------------------------------------------------------------------------\n</code></pre> <p>In case the test is not successful, and you're unsure how to resolve it, do not dispair!</p> <p>I have saved the end result of this chapter in a branch in my version of the repository.</p> <p>You can find the repository here, and the results of this chapter here: 03 Quarkus.</p>"},{"location":"jenkinsx/java-native-prod/03-quarkus/#next-steps","title":"Next Steps","text":"<p>Running <code>mvn clean test</code> should result in a succesful build, with two tests testing most of our application.</p> <p>This means we're ready to go to the next step, importing the application into Jenkins X!</p>"},{"location":"jenkinsx/java-native-prod/04-jx-import/","title":"Jenkins X Import","text":"<p>Awesome, by now we have a Kubernetes cluster with Jenkins X, HashiCorp Vault for secrets, a MySQL database, and a Quarkus Application ready to import.</p> <p>If you don't have a working version of the application coming out of Chapter <code>03 Quarkus</code>, You can find the source code in the branch 03 Quarkus of my repository.</p>"},{"location":"jenkinsx/java-native-prod/04-jx-import/#jenkins-x-basics","title":"Jenkins X Basics","text":"<p>Just in case you're relatively new to Jenkins X, let me give you a bit of an introduction.</p> <p>If you prefer to see it in action and watch a video instead of reading more text, watch one of the video's from Viktor Farcic.</p> <p>Jenkins X provides pipeline automation, built-in GitOps, and preview environments to help teams collaborate and accelerate their software delivery at any scale.</p> <p>Ok, that might be a bit too vague.</p> <p>Jenkins X is built on top of four pillars:</p> <ol> <li>Jenkins X Pipelines: Jenkins X is a full CI/CD engine, which has pipelines to build, test, and deploy your applications on and to Kubernetes. As is popular these days, Jenkins X Pipelines are written in YAML.</li> <li>Manage Environments By GitOps: GitOps is a way of working, where you apply everything from Configuration as Code to your enviromnents. The assumption is that we all use Git nowadays, so we store this code in Git. We then leverage the Jenkins X Pipelines - environments have their own pipelines - to manage the environment.</li> <li>Preview Environments: when you make a PR, wouldn't it be awesome if you would get a temporary deployment so you can test if the PR not only works, but works as we want it? That's what preview environments are. </li> <li>Build Packs: not be confused with other Build Packs, these are Jenkins X's build packs and they ensure that you don't have to write any of the above mentioned workflows yourself. You can off course, or extend them where needed. The idea is that you get a full CI/CD Developer Experience with batteries included, but easily replaced.</li> </ol>"},{"location":"jenkinsx/java-native-prod/04-jx-import/#code-start","title":"Code Start","text":"<p>If you do not have a working version after the previous chapter, you can find the complete working code in the branch 03-quarkus.</p>"},{"location":"jenkinsx/java-native-prod/04-jx-import/#import","title":"Import","text":"<p>Oke, so now we're kinda getting to the point want to start running our new Quarkus application on this shiny new Jenkins X thingy. While Jenkins X has quickstarts (see the list here) to get you up and running fast, we already have an application.</p> <p>So we're going to use Jenkins X's import feature. This feature uses the mentioned build packs to generate everything our application is currently missing. After that, it imports the application in Jenkins X, by creating the required resources in the cluster to ensure Jenkins X now watches your application. This means, that after this, every push to your github repository will trigger a build.</p> <p>One of the things generated during the <code>jx import</code> process, is your Jenkins X Pipeline. This will be derived from an existing pipeline, you can see the list here, so the file in your repository will only state <code>buildpack: &lt;name of the build pack&gt;</code>.</p>"},{"location":"jenkinsx/java-native-prod/04-jx-import/#run-the-import","title":"Run The Import","text":"<p>In our case, we want to build with Maven and Java 11, there is no Quarkus Java 11 yet, so we will run <code>jx import --pack maven-java11</code>.</p> <pre><code>jx import --pack maven-java11\n</code></pre> <p>The import process also generates - if absent - an appropriate Dockerfile, and our Helm charts. The Helm chart creation is mandatory, so the import only succeeds if there is no <code>charts/</code> folder present.</p>"},{"location":"jenkinsx/java-native-prod/04-jx-import/#acitvity-build-logs","title":"Acitvity &amp; Build Logs","text":"<p>Once your application is succesfully imported, you will get a message explaining several Jenkins X commands.</p> <p>Two of the most important ones, are <code>jx get build log</code> and <code>jx get activity</code>. The message should give you a copyable command, so I'll leave that up to you to copy from there - it includes a filter with the name, which I can only guess.</p> <p>The activity log gives you an always running overview of pipeline activity for a given filter. If you're used to Jenkins, it's aching to seeing the classic Pipeline Overview page in console form.</p> <p>The build log gives you the log of the build while it is happening. As soon as a build is started, you can run <code>jx get build log</code>, and you should see your build as the top item.</p> <p>Once the application's pipeline finishes, it should trigger a promotion to the staging environment. To see that pipeline, you can run <code>jx get build log</code> again, and select its pipeline run.</p> <pre><code>&gt; joostvdg/env.../master #2 promotion\n  joostvdg/env.../pr-23 #1 promotion-build\n</code></pre>"},{"location":"jenkinsx/java-native-prod/04-jx-import/#end-of-successful-build-log","title":"End Of Successful Build Log","text":"<pre><code>Promoting app quarkus-fruits version 1.0.5 to namespace jx-staging\nCreated Pull Request: https://github.com/joostvdg/env.../pull/22\nAdded label updatebot to Pull Request https://github.com/joostvdg/env.../pull/22\ngot git provider status pending from PR https://github.com/joostvdg/env.../pull/22\n</code></pre>"},{"location":"jenkinsx/java-native-prod/04-jx-import/#application-in-staging","title":"Application In Staging","text":"<p>To verify the application landed successfully in the staging environment, you can run  <code>jx get applications</code>.</p> <pre><code>jx get applications\n</code></pre> <p>Which should give you a result like this:</p> <pre><code>APPLICATION    STAGING PODS URL\nquarkus-fruits 1.0.1        https://quarkus-fruits-jx-staging.staging.Your.Domain.com\n</code></pre> <p>As you can see, the <code>PODS</code> section is empty. This is because the application cannot talk to its Database yet. No worries, we resolve this later.</p>"},{"location":"jenkinsx/java-native-prod/04-jx-import/#dockerfile","title":"Dockerfile","text":"<p>This the Dockerfile below, is created by the Quarkus team, designed for a runnable Jar application build with Quarkus. If you did not start from the quickstart, but generated a new Quarkus project, it is available in <code>src/main/docker/Dockerfile.jvm</code>.</p> <p>Either way, Jenkins X expects a <code>Dockerfile</code> at the root, so update that <code>Dockerfile</code> with the contents below.</p> <p>I'd recommend using it, as it is well tested and does everything we need in minimal fashion and according to Docker's best practices.</p> <p>Dockerfile</p> <pre><code>FROM registry.access.redhat.com/ubi8/ubi-minimal:8.1\n\nARG JAVA_PACKAGE=java-11-openjdk-headless\nARG RUN_JAVA_VERSION=1.3.5\n\nENV LANG='en_US.UTF-8' LANGUAGE='en_US:en'\n\n# Install java and the run-java script\n# Also set up permissions for user `1001`\nRUN microdnf install curl ca-certificates ${JAVA_PACKAGE} \\\n    &amp;&amp; microdnf update \\\n    &amp;&amp; microdnf clean all \\\n    &amp;&amp; mkdir /deployments \\\n    &amp;&amp; chown 1001 /deployments \\\n    &amp;&amp; chmod \"g+rwX\" /deployments \\\n    &amp;&amp; chown 1001:root /deployments \\\n    &amp;&amp; curl https://repo1.maven.org/maven2/io/fabric8/run-java-sh/${RUN_JAVA_VERSION}/run-java-sh-${RUN_JAVA_VERSION}-sh.sh -o /deployments/run-java.sh \\\n    &amp;&amp; chown 1001 /deployments/run-java.sh \\\n    &amp;&amp; chmod 540 /deployments/run-java.sh \\\n    &amp;&amp; echo \"securerandom.source=file:/dev/urandom\" &gt;&gt; /etc/alternatives/jre/lib/security/java.security\n\n# Configure the JAVA_OPTIONS, you can add -XshowSettings:vm to also display the heap size.\nENV JAVA_OPTIONS=\"-Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager\"\n\nCOPY target/lib/* /deployments/lib/\nCOPY target/*-runner.jar /deployments/app.jar\n\nEXPOSE 8080\nUSER 1001\n\nENTRYPOINT [ \"/deployments/run-java.sh\" ]\n</code></pre>"},{"location":"jenkinsx/java-native-prod/04-jx-import/#health-check","title":"Health Check","text":"<p>One of the most important aspects of your application running in Kubernetes, is giving Kubernetes enough information about the state of your application. Kubernetes does this via Liveness and Readiness probes.</p>"},{"location":"jenkinsx/java-native-prod/04-jx-import/#what-are-these-probes","title":"What Are These Probes","text":"<p>If you don't know much about these probes, or want to understand them better, read the Kubernetes docs. For the lazy, I've added quotes for this docs page.</p> <p>livenessProbe: Indicates whether the Container is running. If the liveness probe fails, the kubelet kills the Container, and the Container is subjected to its restart policy. If a Container does not provide a liveness probe, the default state is Success.</p> <p>readinessProbe: Indicates whether the Container is ready to service requests. If the readiness probe fails, the endpoints controller removes the Pod\u2019s IP address from the endpoints of all Services that match the Pod. The default state of readiness before the initial delay is Failure. If a Container does not provide a readiness probe, the default state is Success.</p> <p>We recommend servicing each probe with a distinct endpoint if you can. </p>"},{"location":"jenkinsx/java-native-prod/04-jx-import/#quarkus-health-checks","title":"Quarkus Health Checks","text":"<p>Luckily, Quarkus has a dependency that gives us exactly that: <code>quarkus-smallrye-health</code>. <code>quarkus-smallrye-health</code> is an implementation of MicroProfile's Health specification, and they have a guide on using and extending it.</p> <p>For those wanting to skip the line, for the minimal implementation we add one dependency to our <code>pom.xml</code>, namely, the mentioned <code>quarkus-smallrye-health</code>. This dependency automatically gives us three endpoints for health checks:</p> <ul> <li><code>/health/live</code>: The application is up and running.</li> <li><code>/health/ready</code>: The application is ready to serve requests.</li> <li><code>/health</code>: Accumulating all health check procedures in the application.</li> </ul>"},{"location":"jenkinsx/java-native-prod/04-jx-import/#add-dependency","title":"Add Dependency","text":"<pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n  &lt;artifactId&gt;quarkus-smallrye-health&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"jenkinsx/java-native-prod/04-jx-import/#update-our-kubernetes-configuration","title":"Update Our Kubernetes Configuration","text":"<p>We now have the ability to give Kubernetes the information it needs to understand the status of our application. We still have to ensure Kubernetes knows how to get this information.</p> <p>To do so, we have two choices. </p> <ol> <li>Update the <code>values.yaml</code> of our Chart, it has one entry for both checks, so not optimal, but minimal effort</li> <li>Update the <code>templates/deployment.yaml</code> of our Chart, where we have to set both endpoints</li> </ol>"},{"location":"jenkinsx/java-native-prod/04-jx-import/#update-values","title":"Update Values","text":"<p>By default, the Helm chart that is generated has one variable for both the liveness and readyness probes: <code>probePath</code>.</p> <p>The value is specified between <code>resources</code> and <code>livenessProbe</code>:</p> <p>We set the value of <code>probePath</code> to <code>/health</code>:</p> <p>charts/Name-Of-Your-App/values.yaml</p> <pre><code>resources:\n  limits:\n    cpu: 250m\n    memory: 64Mi\n  requests:\n    cpu: 250m\n    memory: 64Mi\nprobePath: /health\nlivenessProbe:\n  initialDelaySeconds: 60\n  periodSeconds: 10\n  successThreshold: 1\n  timeoutSeconds: 1\n</code></pre>"},{"location":"jenkinsx/java-native-prod/04-jx-import/#update-deployment","title":"Update Deployment","text":"<p>The second way of make the required change, is to update our Deployment definition. You can choose to set the values directly in the template, or better, change the variables used in the probe paths and set the values of these variables in the <code>values.yaml</code>.</p> <p>We will do the latter here:</p> <p>charts/Name-Of-Your-App/templates/deployment.yaml</p> <pre><code>livenessProbe:\n  httpGet:\n    path: {{ .Values.livenessProbePath }}\n    port: {{ .Values.service.internalPort }}\n  initialDelaySeconds: {{ .Values.livenessProbe.initialDelaySeconds }}\n  periodSeconds: {{ .Values.livenessProbe.periodSeconds }}\n  successThreshold: {{ .Values.livenessProbe.successThreshold }}\n  timeoutSeconds: {{ .Values.livenessProbe.timeoutSeconds }}\nreadinessProbe:\n  httpGet:\n    path: {{ .Values.readinessProbePath }}\n    port: {{ .Values.service.internalPort }}\n  periodSeconds: {{ .Values.readinessProbe.periodSeconds }}\n  successThreshold: {{ .Values.readinessProbe.successThreshold }}\n  timeoutSeconds: {{ .Values.readinessProbe.timeoutSeconds }}\n</code></pre> <p>charts/Name-Of-Your-App/values.yaml</p> <pre><code>resources:\n  limits:\n    cpu: 250m\n    memory: 64Mi\n  requests:\n    cpu: 250m\n    memory: 64Mi\nlivenessProbePath: /health/live\nreadinessProbePath: /health/ready\nlivenessProbe:\n  initialDelaySeconds: 60\n  periodSeconds: 10\n  successThreshold: 1\n  timeoutSeconds: 1\n</code></pre>"},{"location":"jenkinsx/java-native-prod/04-jx-import/#next-steps","title":"Next Steps","text":"<p>We are going to do a lot more things, but this concludes the initial steps to import our application in Jenkins X. Our application still can't run, because it doesn't have the information to connect to the database. This is our next stop.</p>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/","title":"Database Connection And Secrets","text":"<p>In most cases, connecting to a database from your application requires three pieces of information.</p> <ol> <li>Database URL</li> <li>Username</li> <li>Password</li> </ol> <p>Now, you can argue if the fourth would be the schema, or if that is included in #1. </p> <p>Eitherway, our application is currently missing this information. If you remember, we've set our URL to <code>jdbc:mysql://127.0.0.1:3306/fruit</code>, and our username and password to <code>${GOOGLE_SQL_USER}</code> and <code>${GOOGLE_SQL_PASS}</code> respectively.</p> <p>Before we configure these secrets, lets manage our database and it schema in a way that fits Jenkins X.</p>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#code-start","title":"Code Start","text":"<p>If you do not have a working version after the previous chapter, you can find the complete working code in the branch 04-jx-import-finish.</p>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#manage-database-via-flyway","title":"Manage Database Via Flyway","text":"<p>One part of CI/CD that is often forgotten, is managing your Database (schema) updates. There are several options available, such as Liquibase and Flyway.</p> <p>As I've used Flyway in the past with good results, we use Flyway for this guide, but I'm certain you can achieve the same results wichever route you take. You can also choose to ignore database schema management, and leave it to Hibernate or Spring Data to update the schema based on your classes.</p> <p>Important</p> <p>If you do use Flyway or Liquibase to manage the Database schema, you have to re-create the database.</p> <p>The moment to do this, is when the updated application lands in the Staging environment.</p>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#quarkus-integration","title":"Quarkus Integration","text":"<p>Quarkus has excellent support for Flyway, and the Quarkus Flyway guide explains all the details . To use Flyway with our Quarkus application, we take the following steps:</p> <ol> <li>add the <code>quarkus-flyway</code> dependency to the <code>pom.xml</code></li> <li>update the <code>application.properties</code> file in <code>src/main/resources</code></li> <li>update our Fruit entity, <code>src/main/java/.../Fruit.java</code></li> <li>create initial schema definition</li> <li>ensure find all call return a sorted list</li> </ol> <p>pom.xml</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-flyway&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>src/main/resources/application.properties</p> <p>The highlighted lines are the changes.</p> <pre><code>quarkus.datasource.db-kind=mysql\nquarkus.datasource.jdbc.url=jdbc:mysql://127.0.0.1:3306/fruits\nquarkus.datasource.jdbc.max-size=8\nquarkus.datasource.jdbc.min-size=2\n\nquarkus.datasource.username=${GOOGLE_SQL_USER}\nquarkus.datasource.password=${GOOGLE_SQL_PASS}\n\nquarkus.flyway.migrate-at-start=true\nquarkus.flyway.baseline-on-migrate=true\n\nquarkus.hibernate-orm.database.generation=none\nquarkus.log.level=INFO\nquarkus.log.category.\"org.hibernate\".level=INFO\n</code></pre> <p>To ensure we don't run into any troubles with MySQL locally or within Google Cloud, it is best to explicitly set the Table name to exclusively lower case.</p> <p>Secondly, instead of letting Hibernate take care of the id of our entities, we can rely on MySQL doing this for us. So we set the generation strategy to <code>IDENTITY</code>.</p> <p>src/main/java/.../Fruit.java</p> <pre><code>@Table(name = \"fruit\")\n@Entity\npublic class Fruit {\n\n@Id\n@GeneratedValue(strategy = GenerationType.IDENTITY)\nprivate Long id;\n</code></pre>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#schema-definitions","title":"Schema definitions","text":"<p>Flyway uses SQL files for managing the schema - and data - of your database. By default, it looks in the <code>src/main/resources/db/migration</code> folder.</p> <p>Flyway runs the files in order and uses the filename for determining the schema version and update title. We create two files:</p> <ol> <li><code>V1.0.0__initial.sql</code>: houses our Fruit table</li> <li><code>V1.0.1__test_data.sql</code>: has our initial data entries, which we can then also use for our Integration Tests!</li> </ol> <p>V1.0.0__initial.sql</p> <p>The comments were generated by CloudSQL's export, if you use CloudSQL I recommend leaving them intact. If you use something else, probably best to remove them.</p> <pre><code>/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;\n/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;\n/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;\n/*!40101 SET NAMES utf8 */;\n/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;\n/*!40103 SET TIME_ZONE='+00:00' */;\n/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;\n/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;\n/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;\n/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;\n\n--\n-- Current Database: `fruits`\n--\n--\n-- CREATE DATABASE /*!32312 IF NOT EXISTS*/ `fruits` /*!40100 DEFAULT CHARACTER SET utf8 */;\n--\n-- USE `fruits`;\n\n--\n-- Table structure for table `Fruit`\n--\n\nDROP TABLE IF EXISTS `fruit`;\n/*!40101 SET @saved_cs_client     = @@character_set_client */;\n/*!40101 SET character_set_client = utf8 */;\nCREATE TABLE `fruit` (\n  `id` bigint(20) NOT NULL AUTO_INCREMENT,\n  `color` varchar(255) DEFAULT NULL,\n  `name` varchar(255) DEFAULT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n/*!40101 SET character_set_client = @saved_cs_client */;\n\nCREATE UNIQUE INDEX `fruit_name_color` ON `fruit` (`name`, `color`);\n</code></pre> <p>V1.0.1__test_data.sql</p> <pre><code>INSERT INTO fruit(name, color) VALUES ('Cherry', 'Red');\nINSERT INTO fruit(name, color) VALUES ('Apple', 'Red');\nINSERT INTO fruit(name, color) VALUES ('Banana', 'Yellow');\nINSERT INTO fruit(name, color) VALUES ('Avocado', 'Green');\nINSERT INTO fruit(name, color) VALUES ('Strawberry', 'Red');\n</code></pre>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#unit-test-configuration","title":"Unit Test Configuration","text":"<p>To ensure our unit tests also keep working, we have to make some changes to our Tests' <code>application.properties</code> file.</p> <p>The mandatory lines are highlighted, the lat three are for debug logging - should be obvious. This gives you more information on what Flyway is doing, which is almost mandatory when you run into issues. These can be safely ignored, but I thought it best to show you how to set that type of logging, just in case.</p> <p>src/test/resources/application.properties</p> <pre><code>quarkus.datasource.url=jdbc:h2:tcp://localhost/mem:fruits;MODE=MySQL;DB_CLOSE_ON_EXIT=FALSE;DB_CLOSE_DELAY=-1;DATABASE_TO_UPPER=false;\nquarkus.datasource.driver=org.h2.Driver\n\nquarkus.hibernate-orm.log.sql=true\nquarkus.hibernate-orm.database.default-schema=FRUITS\nquarkus.flyway.migrate-at-start=true\nquarkus.flyway.schemas=FRUITS\n\nquarkus.log.console.level=DEBUG\nquarkus.log.category.\"org.flywaydb.core\".level=DEBUG\nquarkus.log.category.\"io.quarkus.flyway\".level=DEBUG\n</code></pre>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#cloudsql-proxy-container","title":"CloudSQL Proxy Container","text":"<p>But first, we have to do one more configuration for our CloudSQL Database. If you do not use the CloudSQL database, you can skip this step, but make sure to replace the JDBC URL appropriately.</p> <p>Google takes its security serious, and thus doesn't allow you to access its CloudSQL databases from everywhere. Our Kubernetes cluster cannot directly access CloudSQL, but it can use a Proxy, provided we connect with a Google Cloud Service Account - not to be confused with a Kubernetes Service Account.</p> <p>We deal with the secrets later, we first have to add this Proxy container configuration. To add the container, we update the <code>deployment.yaml</code> in our <code>charts/Name-Of-Your-Application/templates</code> folder. Containers is a list, so we add the <code>cloudsql-proxy</code> container as an additional list item.</p> <pre><code>- name: cloudsql-proxy\n  image: gcr.io/cloudsql-docker/gce-proxy:1.16\n  command: [\"/cloud_sql_proxy\",\n            \"-instances={{.Values.secrets.sql_connection}}=tcp:3306\",\n            \"-credential_file=/secrets/cloudsql/credentials.json\"]\n</code></pre> <p>If you look carefully, you can see we already reference a secret: <code>{{.Values.secrets.sql_connection}}</code>. We come back to this later. For further clarification on how the containers section of our <code>deployment.yaml</code> should look like, expand the example below.</p> charts/Your-Application-Name/templates/deployment.yaml snippet <pre><code>spec:\n  containers:\n  - name: cloudsql-proxy\n    image: gcr.io/cloudsql-docker/gce-proxy:1.16\n    command: [\"/cloud_sql_proxy\",\n              \"-instances={{.Values.secrets.sql_connection}}=tcp:3306\",\n              \"-credential_file=/secrets/cloudsql/credentials.json\"]\n  - name: {{ .Chart.Name }}\n    envFrom:\n      - secretRef:\n          name: {{ template \"fullname\" . }}-sql-secret\n      - secretRef:\n          name: {{ template \"fullname\" . }}-sentry-dsn\n    image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n    imagePullPolicy: {{ .Values.image.pullPolicy }}\n</code></pre>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#jenkins-x-and-secrets","title":"Jenkins X and Secrets","text":"<p>If you have followed the pre-requisites, you have a Jenkins X installation with Hashicorp Vault. Where Jenkins X is configured to manage its secrets in Vault.</p> <p>This allows Jenkins X to retrieve secrets from Vault for you, and inject them where you need them. We will explore the different ways you can do so, in order to give our application enough information to connect to the MySQL database.</p> <p>Jenkins X can deal with secrets in several ways, and we will use most of them going forward.</p> <ol> <li>configure a Kubernetes secret in the Chart templates, not really Jenkins X, but remember you have this option as well</li> <li>inject a secret as environment variable into the container from Vault, this is a global secret, in the sense that the Vault URI is always the same, for every environment</li> <li>inject a secret via a <code>values.yaml</code> variable, you can - and should - change this depending on the environment, this is a environment secret as our Jenkins X environment will do the replacement from Vault URI to value</li> <li>use a container in our pipeline that has bot the Jenkins X binary (<code>jx</code>) and the Vault CLI, through which we can then interact with Vault's API</li> </ol> <p>Caution</p> <p>When injecting variables directly as environment variable, they will show up in Kubernetes manifests.</p> <p>This might leak the secrets further than you intent. You can use this a shortcut to centralized configuration management - forgoing something such as Consul or Spring Cloud Config Server.</p> <p>When using the option to read the secret into a <code>values.yaml</code> variable, you can use this variable in a template. This means you can create a Kubernetes Secret manifest in your templates folder, and have Jenkins X populate its value from Vault.</p>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#create-secrets","title":"Create Secrets","text":"<p>Lets configure the secrets in our application. we start by setting up variables and placeholders in the <code>values.yaml</code> of our Chart. After which we configure the secret in our Helm templates and enter them in Vault.</p> <p>Important</p> <p>To access Vault, you can use the Vault CLI, see how to install here, or via its UI.</p> <p>Either way, to get the Vault configuration of your Jenkins X cluster, use the get vault-config command:</p> <pre><code>jx get vault-config\n</code></pre> <p>It prints out the URL and the connection token.</p> <p>Every secret we create, is assumed to be in the <code>/secrets</code> KV vault. Jenkins X makes the same assumption, and omits this in the Vault secret URI.</p>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#configure-valuesyaml","title":"Configure Values.yaml","text":"<p>Depending on the secret, we either want it to be the same everywhere or unique per environment.  That partly depends on your, do you have a different database for every environment that isn't Production? </p> <p>In my case, the user of my database is always the same, so I enject it as a environment variable. We do this, by adding <code>key: value</code> pairs to the <code>env:</code> property.</p> <p>The other information pieces are both more sensitive and environment specific. So we create a new property called <code>secrets</code>, and fill in empty (placeholder) values:</p> <ul> <li>sql_password: the password for our database</li> <li>sql_connection: the connection information the CloudSQL Proxy container will use</li> <li>sqlsa: the Google Cloud Service Account (JSON) key for validating the database connection request</li> </ul> <p>values.yaml</p> <pre><code># Secrets that get loaded via the (jx) environment from Vault\nsecrets:\n  sql_password: \"\"\n  sql_connection: \"\"\n  sqlsa: \"\"\n\n# define environment variables here as a map of key: value\nenv:\n</code></pre>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#google-cloudsql-connection-url","title":"Google CloudSQL Connection URL","text":"<p>Access Vault and create a new Secret under <code>secrets/Name-Of-Your-Application</code>. In these examples, we use <code>quarkus-fruits</code> as our application name.</p> <p>Each secret in Vault is a set of Key/Value pairs.</p> <p>For the CloudSQL Connection URL, we use <code>INSTANCE_CONNECTION_NAME</code>, as this is how CloudSQL names this information.</p>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#google-cloudsql-service-account-key","title":"Google CloudSQL Service Account Key","text":"<p>If everything is allright, you have created this service account earlier. If not, please revisit How To Connect To The Database paragraph from the CloudSQL page.</p> <p>Due to the secret being a JSON file, it is best to Base64 encode the contents before adding it as a secret value in Vault.</p> <p>In Linux of MacOS, this should be sufficient:</p> <pre><code>cat credentials.json | base64\n</code></pre> <p>As key, we use <code>SA</code>.</p>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#create-secret-manifest","title":"Create Secret Manifest","text":"<p>This is very sensitive information, so we follow the best practice of moutning the secret. To do so, we have to create it as a secret in Kubernetes.  We create a new Kubernetes Secert manifest in the <code>charts/Your-Application-Name/templates</code> folder with the name <code>sql-sa-secret.yaml</code> so that Jenkins X will create this for us.</p> <p>templates/sql-sa-secret.yaml</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: {{ template \"fullname\" . }}-sql-sa\ntype: Opaque\ndata:\n  credentials.json: {{ .Values.secrets.sql_sa }}\n</code></pre>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#update-deployment","title":"Update Deployment","text":"<p>We have to make two changes. We have to create a volume and a volumeMount to ensure the <code>cloudsql-proxy</code> has access to the Service Account Key.</p> <p>templates/deployment.yaml</p> <pre><code>- name: cloudsql-proxy\n  image: gcr.io/cloudsql-docker/gce-proxy:1.16\n  command: [\"/cloud_sql_proxy\",\n            \"-instances={{.Values.secrets.sql_connection}}=tcp:3306\",\n            \"-credential_file=/secrets/cloudsql/credentials.json\"]\n  volumeMounts:\n    - name: cloudsql-instance-credentials\n      mountPath: /secrets/cloudsql\n      readOnly: true\n</code></pre> <pre><code>{{ toYaml .Values.resources | indent 12 }}\n      terminationGracePeriodSeconds: {{ .Values.terminationGracePeriodSeconds }}\n{{- end }}\n      volumes:\n        - name: cloudsql-instance-credentials\n          secret:\n            secretName: {{ template \"fullname\" . }}-sql-sa\n</code></pre> <p>Caution</p> <p>Kubernetes' YAML manifest files can be difficult to get right, especially the indenting.</p> <p>If you have <code>Helm</code> installed locally, you can use its linting feature to see if everything is allright.</p> <pre><code>helm lint charts/quarkus-fruits\n</code></pre>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#google-cloudsql-password","title":"Google CloudSQL Password","text":"<p>Add the CloudSQL password to your secret in Vault (in my case, <code>secrets/quarkus-fruits</code>) and use the Key <code>GOOGLE_SQL_PASS</code>.</p>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#create-secret-manifest_1","title":"Create Secret Manifest","text":"<p>We again make a Kubernetes Secret Manifest, this time by the name of <code>sql-secret.yaml</code>. Please not, that as we did not Base64 encode our password, we have to do this in our manifest. But no worries, Helm templating can do this for us, via <code>| b64enc</code>.</p> <p>templates/sql-secret.yaml</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: {{ template \"fullname\" . }}-sql-secret\ndata:\n  GOOGLE_SQL_PASS: {{ .Values.secrets.sql_password | b64enc }}\n</code></pre>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#update-deployment_1","title":"Update Deployment","text":"<p>Instead of mounting the secret, we inject it as a environment variable. Unlike the the username, which was added as a environment variable directly, we let Kubernetes take care of the injection via <code>envFrom</code>. This ensures the secret does not show up in the manifest (when you do a <code>kubectl get pod -o yaml</code> for example), and saves us the hassle of reading the property from a file.</p> <p>When using <code>envFrom</code>, there are several options of where it comes from. In our case, it comes from a secret, so we use <code>-secretRef</code> with the name of our <code>sql-secret</code> secret manifest.</p> <p>Important</p> <p>The <code>deployment.yaml</code> is set up in such a way, that it expect us to declare <code>envFrom</code> in the <code>values.yaml</code>. Unfortunately, in our case, that doesn't work because we don't know the name of the secret up front. It is derived from the name of our Helm chart's installation, and it is best to keep this templated.</p> <p>To solve this, we have to remove the line <code>{{ toYaml .Values.envFrom | indent 10 }}</code></p> <p>templates/deployment.yaml</p> <pre><code>      - name: {{ .Chart.Name }}\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n        imagePullPolicy: {{ .Values.image.pullPolicy }}\n        env:\n{{- range $pkey, $pval := .Values.env }}\n        - name: {{ $pkey }}\n          value: {{ quote $pval }}\n{{- end }}\n        envFrom:\n        - secretRef:\n            name: {{ template \"fullname\" . }}-sql-secret\n{{ toYaml .Values.envFrom | indent 10 }} ## LINE TO REMOVE\n</code></pre>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#google-cloudsql-username","title":"Google CloudSQL Username","text":"<p>Add the CloudSQL username to your secret in Vault (in my case, <code>secrets/quarkus-fruits</code>) and use the Key <code>GOOGLE_SQL_USER</code>.</p> <p>Info</p> <p>If you haven't created a separate user - which is probably best - the user would be <code>root</code>.</p> <p>If you create a new user, ensure the password set in <code>GOOGLE_SQL_PASS</code> matches the user in <code>GOOGLE_SQL_USER</code>.</p> <p>For the username, this is all we have to do, as we directly inject this variable as an environment variable.</p> <p>values.yaml</p> <pre><code>env:\n  GOOGLE_SQL_USER: vault:quarkus-fruits:GOOGLE_SQL_USER\n</code></pre>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#configure-jenkins-x-environments","title":"Configure Jenkins X Environments","text":"<p>For each Jenkins X environment that your application is going to land in, such as <code>jx-staging</code> and <code>jx-production</code>, we have to enable Vault support.</p> <p>We do this by making a change in the Environment's <code>jx-requirements.yml</code> file in the root of the repository of the environment. This file might not exist, if so, create it.</p> <p>If you're not sure where the repository of your environment is, you can retrieve this information via the <code>jx</code> CLI.</p> <pre><code>jx get environments\n</code></pre> <p>To enable support for Vault, we add <code>secretStorage: vault</code> to the file. The file will look like this:</p> <p>jx-requirements.yml</p> <pre><code>secretStorage: vault\n</code></pre>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#update-staging-environment","title":"Update Staging Environment","text":"<p>Once you create the secret and the <code>INSTANCE_CONNECTION_NAME</code> key with its secret value - which should be the instance connection name from your CloudSQL database - we can add this information to our Jenkins X environment(s), such as Staging. Again, here we assume the application's name is <code>quarkus-fruits</code>.</p> <p>env/values.yaml</p> <pre><code>quarkus-fruits:\n  secrets:\n    sql_connection: vault:quarkus-fruits:INSTANCE_CONNECTION_NAME\n    sql_sa: vault:quarkus-fruits:SA\n    sql_password: vault:quarkus-fruits:GOOGLE_SQL_PASS\n</code></pre> <p>For this secret, this is all we need to do. As our CloudSQL Proxy Container directly uses this value in the <code>deployment.yaml</code>.</p>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#summary-of-changes-made-to-application","title":"Summary of Changes Made To Application","text":"<ul> <li>created two new templates in the folder <code>charts/Name-Of-Your-Application/templates</code><ul> <li><code>sql-secret.yaml</code></li> <li><code>sql-sa-secret.yaml</code></li> </ul> </li> <li>updated the <code>charts/Name-Of-Your-Application/templates/deployment.yaml</code><ul> <li>add second container, for the CloudSQL Proxy</li> <li>add volume and volumeMount to CloudSQL Proxy container</li> <li>add environment injection from secret for password</li> </ul> </li> <li>updated <code>charts/Name-Of-Your-Application/values.yaml</code> with placeholders for the secrets</li> </ul> templates/deployment.yaml <p>Your deployment should now look like this:</p> <pre><code>{{- if .Values.knativeDeploy }}\n{{- else }}\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ template \"fullname\" . }}\n  labels:\n    draft: {{ default \"draft-app\" .Values.draft }}\n    chart: \"{{ .Chart.Name }}-{{ .Chart.Version | replace \"+\" \"_\" }}\"\nspec:\n  selector:\n    matchLabels:\n      app: {{ template \"fullname\" . }}\n{{- if .Values.hpa.enabled }}\n{{- else }}\n  replicas: {{ .Values.replicaCount }}\n{{- end }}\n  template:\n    metadata:\n      labels:\n        draft: {{ default \"draft-app\" .Values.draft }}\n        app: {{ template \"fullname\" . }}\n{{- if .Values.podAnnotations }}\n      annotations:\n{{ toYaml .Values.podAnnotations | indent 8 }}\n{{- end }}\n    spec:\n      containers:\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:1.16\n        command: [\"/cloud_sql_proxy\",\n                  \"-instances={{.Values.secrets.sql_connection}}=tcp:3306\",\n                  \"-credential_file=/secrets/cloudsql/credentials.json\"]\n        volumeMounts:\n          - name: cloudsql-instance-credentials\n            mountPath: /secrets/cloudsql\n            readOnly: true\n      - name: {{ .Chart.Name }}\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n        imagePullPolicy: {{ .Values.image.pullPolicy }}\n        env:\n{{- range $pkey, $pval := .Values.env }}\n        - name: {{ $pkey }}\n          value: {{ quote $pval }}\n{{- end }}\n        envFrom:\n        - secretRef:\n            name: {{ template \"fullname\" . }}-sql-secret\n        ports:\n        - containerPort: {{ .Values.service.internalPort }}\n        livenessProbe:\n          httpGet:\n            path: {{ .Values.livenessProbePath }}\n            port: {{ .Values.service.internalPort }}\n          initialDelaySeconds: {{ .Values.livenessProbe.initialDelaySeconds }}\n          periodSeconds: {{ .Values.livenessProbe.periodSeconds }}\n          successThreshold: {{ .Values.livenessProbe.successThreshold }}\n          timeoutSeconds: {{ .Values.livenessProbe.timeoutSeconds }}\n        readinessProbe:\n          httpGet:\n            path: {{ .Values.readinessProbePath }}\n            port: {{ .Values.service.internalPort }}\n          periodSeconds: {{ .Values.readinessProbe.periodSeconds }}\n          successThreshold: {{ .Values.readinessProbe.successThreshold }}\n          timeoutSeconds: {{ .Values.readinessProbe.timeoutSeconds }}\n        resources:\n{{ toYaml .Values.resources | indent 12 }}\n      terminationGracePeriodSeconds: {{ .Values.terminationGracePeriodSeconds }}\n{{- end }}\n      volumes:\n        - name: cloudsql-instance-credentials\n          secret:\n            secretName: {{ template \"fullname\" . }}-sql-sa\n</code></pre>"},{"location":"jenkinsx/java-native-prod/05-jx-secrets/#summary-of-changes-made-to-staging-environment","title":"Summary of Changes Made To Staging Environment","text":"<ul> <li>created a new file, called <code>jx-requirements.yml</code> at the root</li> <li>updated the <code>env/values.yaml</code> with values for our application</li> </ul> jx-requirements.yml <pre><code>secretStorage: vault\n</code></pre> env/values.yaml <pre><code>quarkus-fruits:\nsecrets:\n  sql_connection: vault:quarkus-fruits:INSTANCE_CONNECTION_NAME\n  sql_sa: vault:quarkus-fruits:SA\n  sql_password: vault:quarkus-fruits:GOOGLE_SQL_PASS\n</code></pre>"},{"location":"jenkinsx/java-native-prod/06-native-image/","title":"Build Native Image","text":"<p>We've now reached the point that our application is build and deployed with Jenkins X. If everything went allright, it now runs in the staging environment.</p> <p>It is time to run our application as a Native Image, rather than a Jar in a JVM.</p>"},{"location":"jenkinsx/java-native-prod/06-native-image/#what-is-java-native-image","title":"What is Java Native Image","text":"<p>GraalVM Native Image allows you to ahead-of-time compile Java code to a standalone executable, called a native image. This executable includes the application classes, classes from its dependencies, runtime library classes from JDK and statically linked native code from JDK. It does not run on the Java VM, but includes necessary components like memory management and thread scheduling from a different virtual machine, called \u201cSubstrate VM\u201d. Substrate VM is the name for the runtime components (like the deoptimizer, garbage collector, thread scheduling etc.). The resulting program has faster startup time and lower runtime memory overhead compared to a Java VM. - GraalVM reference manual</p> <p>In short, its makes your Java code into a runnable executable build for a specific environment. In our cse, we use RedHat's Universal Base Image, and as such, we know the application - or at least the Native Image distribution - always runs on this particular environment.</p>"},{"location":"jenkinsx/java-native-prod/06-native-image/#why","title":"Why","text":"<p>You might wonder, what is wrong with using a runnable Jar - such as Spring Boot, or Quarkus - or using a JVM? Nothing in and on itself. However, there are cases where having a long running process with a slow start-up time hurts you.</p> <p>In a Cloud Native world, including Kubernetes, this is far more likely than in traditional - read, VM's - environments. With the advent of creating many smaller services that may or may not be stateless, and should be capable of scaling horizontally from 0 to infinity, different characteristics are required.</p> <p>Some of these characterics:</p> <ul> <li>minimal resource use as we pay per usage (to a degree)</li> <li>fast startup time</li> <li>perform as expected on startup (JVM needs to warm up)</li> </ul> <p>A Native Image performs better on the above metrics than a classic Java application with a JVM. Next to that, when you have a fixed runtime, the benefit of Java's \"build once, run everywhere\" is not as useful. When you always run your application in the same container in similar Kubernetes environments, a Native Image is perfectly fine.</p> <p>On top of that, we distribute our application as a Helm Chart + Container Image. When we also ship a runtime environment, such as a JRE, our Container Image is larger in disk size and larger in runtime memory. We can run more Native Image applications in the same Kubernetes cluster than JVM based applications. Unless we share the JVM, but the whole point of using Containers was to avoid that.</p> <p>Wether a Native Image performs better for your application depends on your application and its usage. The Native Image is no silver bullet. So it is still on you to do load and performance tests to ensure you're not degrading your performance for no reason!</p>"},{"location":"jenkinsx/java-native-prod/06-native-image/#code-start","title":"Code Start","text":"<p>If you do not have a working version after the previous chapter, you can find the complete working code in the branch 05-db-and-secrets.</p>"},{"location":"jenkinsx/java-native-prod/06-native-image/#how","title":"How","text":"<p>One of the reasons for using Quarkus is the built-in support for building a native executable. </p>"},{"location":"jenkinsx/java-native-prod/06-native-image/#native-profile","title":"Native Profile","text":"<p>When you use Maven as builder for your Quarkus application, there is a profile called <code>native</code> pre-configured. By using the profile, <code>mvn package -Pnative</code>, the Quarkus (Maven) plugin uses GraalVM to generate the native image.</p> <p>You can do this by having GraalVM installed on your local machine, or use a Container Image.</p>"},{"location":"jenkinsx/java-native-prod/06-native-image/#native-image-build-container","title":"Native Image Build Container","text":"<p>For most people, I recommend using the build container provided by Quarkus. These are kept up-to-date and it reduces the number of things you need in your development environment.</p> <p>Quarkus provides a flag for building the native image via a container. And, depending on your needs to can explicitly set the container runtime to be used.</p> default container runtimedocker runtimepodman runtime <pre><code>./mvnw package -Pnative -Dquarkus.native.container-build=true\n</code></pre> <pre><code>./mvnw package -Pnative -Dquarkus.native.container-runtime=docker\n</code></pre> <pre><code>./mvnw package -Pnative -Dquarkus.native.container-runtime=podman\n</code></pre>"},{"location":"jenkinsx/java-native-prod/06-native-image/#build-native-image-container","title":"Build Native Image Container","text":"<p>By default, this file resides in <code>src/main/docker/Dockerfile.native</code>. As Jenkins X uses the <code>Dockerfile</code> from the root of the project, we update that <code>Dockerfile</code> to the contents below.</p> <p>I recommend using it, as it is well tested and does everything we need in minimal fashion and according to Docker's best practices.</p> <p>Dockerfile</p> <pre><code>FROM registry.access.redhat.com/ubi8/ubi-minimal:8.1\nWORKDIR /work/\nCOPY target/*-runner /work/application\n\n# set up permissions for user `1001`\nRUN chmod 775 /work /work/application \\\n    &amp;&amp; chown -R 1001 /work \\\n    &amp;&amp; chmod -R \"g+rwX\" /work \\\n    &amp;&amp; chown -R 1001:root /work\n\nEXPOSE 8080\nUSER 1001\n\nCMD [\"./application\", \"-Dquarkus.http.host=0.0.0.0\", \"-Xmx64m\"]\n</code></pre>"},{"location":"jenkinsx/java-native-prod/06-native-image/#update-jenkins-x-pipeline","title":"Update Jenkins X Pipeline","text":"<p>Now that we have a <code>Dockerfile</code> Jenkins X can use to build the Native Image end-result, we need to ensure the build steps are update - for both the release and pullrequest pipeline.</p> <p>As our current build pack does not contain GraalVM for building the Native Image, we will also change the image used for those steps. Images used for build steps are called <code>Jenkins X Builders</code>.</p>"},{"location":"jenkinsx/java-native-prod/06-native-image/#jenkins-x-builder","title":"Jenkins X Builder","text":"<p>At the time of writing, there is no Jenkins X Builder image for Java 11 with Maven and GraalVM. There is an open pullrequest to make this happen. Untill this is merged, read here how to create a Jenkins X Builder Image.</p> <p>Alternatively, you can use my image:</p> <ul> <li>Image URI: <code>caladreas/jx-builder-graalvm-maven-jdk11:v0.10.0</code></li> <li>Dockerhub</li> <li>GitHub source code</li> </ul>"},{"location":"jenkinsx/java-native-prod/06-native-image/#change-build-steps","title":"Change Build steps","text":"<p>At this point, our Jenkins X pipeline should look like this:</p> <p>jenkins-x.yml</p> <pre><code>buildPack:  maven-java11\n</code></pre> <p>We update the builds steps for pipelines (pullrequest and release). The release pipeline's build stage is called <code>mvn-deploy</code>, the equivalent for the pullrequest pipeline (don't ask me why they're different) is <code>mvn-install</code>.</p> <p>Three changes are required:</p> <ol> <li>the (Maven) command to include the <code>native</code> profile</li> <li>the image to use a Container Image that includes GraalVM</li> <li>container options to set the memory to required levels</li> </ol> <p>We can do each of the three changes by overriding the mentioned build steps.</p> <p>To override a step, we start with this initial syntax:</p> <pre><code>    pipelineConfig:\n      pipelines:\n        overrides:\n</code></pre> <p>After which we supply the list of steps we want to override. For each step, we state the <code>pipeline</code>, <code>stage</code> and the step <code>name</code>.</p> <pre><code>- pipeline: release\n  stage: build\n  name: mvn-deploy\n</code></pre> <p>We go over each change we make below, but for the full reference, I recommened reading the Jenkins X Pipeline Syntax page.</p>"},{"location":"jenkinsx/java-native-prod/06-native-image/#step-override","title":"Step Override","text":"<p>To override a a step, we start from the above mentioned override, add <code>steps:</code> and then include the list of steps to override. For each step, we can set the <code>name</code>, <code>command</code>, and <code>image</code> (the build container used). There are more options, but these are sufficient for now.</p> <pre><code>steps:\n  - name: mvn-deploy\n    command: mvn clean package -Pnative --show-version -DskipDocs\n    image: caladreas/jx-builder-graalvm-maven-jdk11:v0.9.0\n</code></pre>"},{"location":"jenkinsx/java-native-prod/06-native-image/#container-options","title":"Container Options","text":"<p>To change the configuration of the Container Image used for our step, such as environment varibiables or resources, we set <code>containerOptions</code>. Again, please refer to the Jenkins X Pipeline Syntax page for the options available and where you can leverage this configuration element.</p> <p>The options we need to set, are the resources for the container - we need at least 8GB - and the environment variable <code>_JAVA_OPTIONS</code>. The latter is required, because the default JAVA_OPTIONS is set to a very low number (<code>192mb</code>) which is insufficient for our build. Both Maven and GraalVM's Native Image build pick up this environment variable. Included are also some other JVM flags, which I've come across in several GitHub Issues as recommended.</p> <p>The required options are <code>-Xms8g -Xmx8g -XX:+UseSerialGC</code>, but I recommend using the other flags as well.</p> <pre><code>containerOptions:\n  env:\n    - name: _JAVA_OPTIONS\n      value: &gt;-\n        -XX:+UnlockExperimentalVMOptions -XX:+EnableJVMCI -XX:+UseJVMCICompiler\n        -Xms8g -Xmx8g -XX:+PrintCommandLineFlags -XX:+UseSerialGC\n  resources:\n    requests:\n      cpu: \"2\"\n      memory: 10Gi\n    limits:\n      cpu: \"2\"\n      memory: 10Gi\n</code></pre>"},{"location":"jenkinsx/java-native-prod/06-native-image/#release-pipeline","title":"Release Pipeline","text":"<p>jenkins-x.yml</p> <pre><code>- pipeline: release\n  stage: build\n  name: mvn-deploy\n  steps:\n    - name: mvn-deploy\n      command: mvn clean package -Pnative --show-version -DskipDocs\n      image: caladreas/jx-builder-graalvm-maven-jdk11:v0.9.0\n  containerOptions:\n    env:\n      - name: _JAVA_OPTIONS\n        value: &gt;-\n          -XX:+UnlockExperimentalVMOptions -XX:+EnableJVMCI -XX:+UseJVMCICompiler\n          -Xms8g -Xmx8g -XX:+PrintCommandLineFlags -XX:+UseSerialGC\n    resources:\n      requests:\n        cpu: \"2\"\n        memory: 10Gi\n      limits:\n        cpu: \"2\"\n        memory: 10Gi\n</code></pre>"},{"location":"jenkinsx/java-native-prod/06-native-image/#pullrequest-pipeline","title":"PullRequest Pipeline","text":"<p>jenkins-x.yml</p> <pre><code>- pipeline: pullRequest\n  stage: build\n  name: mvn-install\n  steps:\n    - name: mvn-deploy\n      command: mvn clean package -Pnative --show-version -DskipDocs\n      image: caladreas/jx-builder-graalvm-maven-jdk11:v0.9.0\n  containerOptions:\n    env:\n      - name: _JAVA_OPTIONS\n        value: &gt;-\n          -XX:+UnlockExperimentalVMOptions -XX:+EnableJVMCI -XX:+UseJVMCICompiler\n          -Xms8g -Xmx8g -XX:+PrintCommandLineFlags -XX:+UseSerialGC\n    resources:\n      requests:\n        cpu: \"2\"\n        memory: 10Gi\n      limits:\n        cpu: \"2\"\n        memory: 10Gi\n</code></pre>"},{"location":"jenkinsx/java-native-prod/06-native-image/#full-example","title":"Full Example","text":"jenkins-x.yml <pre><code>buildPack:  maven-java11\npipelineConfig:\n  pipelines:\n    overrides:\n      - pipeline: pullRequest\n        stage: build\n        name: mvn-install\n        steps:\n          - name: mvn-deploy\n            command: mvn clean package -Pnative --show-version -DskipDocs\n            image: caladreas/jx-builder-graalvm-maven-jdk11:v0.9.0\n        containerOptions:\n          env:\n            - name: _JAVA_OPTIONS\n              value: &gt;-\n                -XX:+UnlockExperimentalVMOptions -XX:+EnableJVMCI -XX:+UseJVMCICompiler\n                -Xms8g -Xmx8g -XX:+PrintCommandLineFlags -XX:+UseSerialGC\n          resources:\n            requests:\n              cpu: \"2\"\n              memory: 10Gi\n            limits:\n              cpu: \"2\"\n              memory: 10Gi\n      - pipeline: release\n        stage: build\n        name: mvn-deploy\n        steps:\n          - name: mvn-deploy\n            command: mvn clean package -Pnative --show-version -DskipDocs\n            image: caladreas/jx-builder-graalvm-maven-jdk11:v0.9.0\n        containerOptions:\n          env:\n            - name: _JAVA_OPTIONS\n              value: &gt;-\n                -XX:+UnlockExperimentalVMOptions -XX:+EnableJVMCI -XX:+UseJVMCICompiler\n                -Xms8g -Xmx8g -XX:+PrintCommandLineFlags -XX:+UseSerialGC\n          resources:\n            requests:\n              cpu: \"2\"\n              memory: 10Gi\n            limits:\n              cpu: \"2\"\n              memory: 10Gi\n</code></pre>"},{"location":"jenkinsx/java-native-prod/06-native-image/#update-container-resources","title":"Update Container Resources","text":"<p>If we look at our <code>charts/Name-of-your-Application/values.yaml</code> file, we can see it defines the CPU and Memory requests &amp; limits. These correspond to the expected bounds for our application.</p> <pre><code>resources:\n  limits:\n    cpu: 500m\n    memory: 512Mi\n  requests:\n    cpu: 400m\n    memory: 512Mi\n</code></pre> <p>The bounds that are in there, are set for a Java 11 application running on a JVM. Now that we changed our application to run as a Native Image, we can drastically reduce them.</p> <p>Please set them accordingly:</p> <p>values.yaml</p> <pre><code>resources:\n  limits:\n    cpu: 250m\n    memory: 64Mi\n  requests:\n    cpu: 250m\n    memory: 64Mi\n</code></pre>"},{"location":"jenkinsx/java-native-prod/06-native-image/#worker-node-capity","title":"Worker Node Capity","text":"<p>Important</p> <p>As stated in the pre-requisites, to have the builds work well, your Kubernetes worker nodes need at least 10GB of memory. If you do not have those at the moment, you can add an additional Node Pool with these machine types.</p> <p>If you're in GKE, as the guide assumes, the following machine types work:</p> <ul> <li><code>e2-highmem-2</code></li> <li><code>n2-highmem-2</code></li> <li><code>e2-standard-4</code></li> <li><code>n2-standard-4</code></li> </ul>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/","title":"Pipeline Improvements","text":"<p>Now that the application runs in staging as a Native Image, it is time to make further improvements to our pipeline.</p> <p>There are a dozen additional checks I'd like to add to the pipeline to increase confidence in my application. For now, we will dive into three extra steps:</p> <ol> <li>Static Code Analysis with SonarQube</li> <li>Dependency Vulnerability Scan with OSS Index</li> <li>Integration Test with PostMan (next page)</li> </ol>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#code-start","title":"Code Start","text":"<p>If you do not have a working version after the previous chapter, you can find the complete working code in the branch 06-native-image.</p>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#static-code-analysis-with-sonarqube","title":"Static Code Analysis with SonarQube","text":"<p>For me, SonarQube has been a tool I've almost always used to help me guard the quality of whatever I am writing. While it might not be your cup of tea, I do recommend you to follow along to understand how you can integrate such tools into a Jenkins X Pipeline.</p> <p>At the outset, we have two choice for doing SonarQube analysis. We can host SonarQube ourselves, via a Helm Chart for example, or we can choose to use SonarSource's (the company behind SonarQube) cloudservice sonarcloud.io.</p> <p>Which ever route you take, Self-Hosted or SonarCloud, both continue at Steps.</p>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#sonarcloud","title":"SonarCloud","text":"<ol> <li>sign up at sonarcloud.io</li> <li>register the application within your organization (within SonarCloud)</li> <li>create an API token</li> </ol>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#self-hosted","title":"Self-Hosted","text":"<ul> <li>ensure SonarQube is running with at least the Java plugin</li> <li>create an API token</li> </ul> <p>If you don't have SonarQube running yet, below we give you some hints for how to do so with Helm. Alternatively, you can add the SonarQube Helm Chart as a dependency to a Jenkins X Environment.</p>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#helm-install","title":"Helm Install","text":"<p>As the Helm Chart doesn't live in the default repository, we have to add the Oteemo Char Repository.</p> <p>After that, we run <code>helm repo update</code> and we can install the tool.</p> <pre><code>helm repo add oteemocharts https://oteemo.github.io/charts\nhelm repo update\n</code></pre> Helm v3Helm v2 <pre><code>helm install sonar oteemocharts/sonarqube -f values.yaml\n</code></pre> <pre><code>helm install --name sonar oteemocharts/sonarqube\n</code></pre>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#helm-values","title":"Helm Values","text":"<p>Here's an example Helm values for hosting SonarQube in Kubernetes via the Oteemo helm chart. This includes a set of basic plugins to start with, as the Helm installation by default comes without plugins - meaning, you can't run any analysis.</p> values.yaml <pre><code>ingress:\n  enabled: true\n  hosts:\n    - name: sonar.example.com\n      path: /\n  tls:\n  - hosts:\n    - sonar.sonar.example.com\n    secretName: tls-sonar-p\n  annotations: \n    kubernetes.io/ingress.class: nginx\n    kubernetes.io/tls-acme: \"true\"\n    nginx.ingress.kubernetes.io/proxy-body-size: \"8m\"\n    cert-manager.io/issuer: letsencrypt-prod\npersistence:\n  enabled: true\n  size: 25Gi\nplugins:\n  install:\n    - https://binaries.sonarsource.com/Distribution/sonar-java-plugin/sonar-java-plugin-6.3.0.21585.jar\n    - https://binaries.sonarsource.com/Distribution/sonar-javascript-plugin/sonar-javascript-plugin-6.2.1.12157.jar\n    - https://binaries.sonarsource.com/Distribution/sonar-go-plugin/sonar-go-plugin-1.6.0.719.jar\n    - https://github.com/Coveros/zap-sonar-plugin/releases/download/sonar-zap-plugin-1.2.0/sonar-zap-plugin-1.2.0.jar\n    - https://binaries.sonarsource.com/Distribution/sonar-typescript-plugin/sonar-typescript-plugin-2.1.0.4359.jar\n</code></pre>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#steps","title":"Steps","text":"<ul> <li>create Kubernetes secrets</li> <li>configure secrets in the Jenkins X Pipeline</li> <li>add build step to do Sonar Analysis</li> </ul>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#create-secrets","title":"Create Secrets","text":"<p>At the time of writing - May 2020 - Jenkins X doesn't support reading secrets from Vault into the Pipeline. To have secrets in the pipeline we create a Kubernetes secret.</p> <pre><code>kubectl create secret generic quarkus-fruits-sonar -n jx \\\n  --from-literal=SONAR_API_TOKEN='mytoken' \\\n  --from-literal=SONAR_HOST_URL='myurl'\n</code></pre> <p>If you don't like this, there are ways of having HashiCorp Vault integrated into Kubernetes so that injects values from Vault into your Kubernetes secrets. This is out of scope of the guide.</p>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#configure-secrets-in-pipeline","title":"Configure Secrets In Pipeline","text":"<p>We are now going to use the Kubernetes secret we created earlier. As the Sonar CLI automatically picks up some environments variables - such as the <code>SONAR_API_TOKEN</code> in our secret - we inject the secret as environment variables.</p> <p>We do this via the <code>envFrom</code> construction.</p> <pre><code>envFrom:\n  - secretRef:\n      name: quarkus-fruits-sonar\n</code></pre> <p>To create room for additional secrets, we add this on top as part of the root <code>containerOptions</code>. The root being <code>pipelineConfig</code>. Our <code>jenkins-x.yml</code>'s first few lines should now look like this.</p> <p>jenkins-x.yml</p> <p>Which means the <code>jenkins-x.yml</code> will look like this.</p> <pre><code>buildPack:  maven-java11\npipelineConfig:\n  containerOptions:\n    envFrom:\n      - secretRef:\n          name: quarkus-fruits-sonar\n</code></pre>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#create-sonar-analysis-build-step","title":"Create Sonar Analysis Build Step","text":"<p>To run the SonarQube analysis, we add another step to the pipeline, in the <code>jenkins-x.yml</code> file. There are various ways to do it, in this case I'm using the <code>overrides</code> mechanic to add a new step after <code>mvn-deploy</code>. We do this, by setting the <code>type</code> of the override to <code>after</code>. </p> <p>Note</p> <p>The default <code>type</code> of the pipeline override is <code>override</code>.  This is set implicitly, which is why we did not set this when overriding steps.</p> <p>We can add any Kubernetes container configuration to our stage's container, via Jenkins X's <code>containerOptions</code> key.</p> <p>jenkins-x.yml</p> <pre><code>pipelineConfig:\n  pipelines:\n    overrides:\n      - name: mvn-deploy\n        pipeline: release\n        stage: build\n        step:\n          name: sonar\n          command: mvn\n          args:\n            - compile\n            - org.sonarsource.scanner.maven:sonar-maven-plugin:3.6.0.1398:sonar\n            - -Dsonar.host.url=$SONAR_HOST_URL\n            - -e\n            - --show-version\n            - -DskipTests=true\n        type: after\n</code></pre> <p>For more syntax details, see the Jenkins X Pipeline page.</p> <p>Info</p> <p>When you use sonarcloud.io you have to add additional parameters.</p> <p>You have to add <code>sonar.login</code>, <code>sonar.organization</code>, and <code>sonar.projectKey</code> to the maven arguments. It is up to you to take an approach you prefer. Add them to the Kubernetes secret <code>quarkus-fruits-sonar</code> we created earlier, or hard code them into the pipeline.</p> <pre><code>args:\n  - compile\n  - org.sonarsource.scanner.maven:sonar-maven-plugin:3.6.0.1398:sonar\n  - -Dsonar.host.url=$SONAR_HOST_URL\n  - -Dsonar.projectKey=$SONAR_PROJECT_KEY\n  - -Dsonar.organization=$SONAR_ORG\n  - -Dsonar.login=$SONAR_API_TOKEN\n  - -e\n  - --show-version\n  - -DskipTests=true\n</code></pre>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#dependency-vulnerability-scan-with-oss-index","title":"Dependency Vulnerability Scan with OSS Index","text":"<p>While SonarQube - and many other tools - can help us identify issues in our code, we should also validate the code we import via Maven dependencies.</p> <p>Luckily, there are a lot of options now, such as Snyk, or Sonatype's OSS Index, GitHub has it even embedded in their repositories now.</p> <p>I've always been a fan of Sonatype, I'm molded by the Jenkins+Sonar+Nexus triumvirate, so in this guide we include Sonatype's OSS Index scanning. But feel free to find a solution you prefer.</p> <p>This is a similar process as with SonarQube:</p> <ul> <li>register application with OSS Index</li> <li>create a API token</li> <li>create Kubernetes secret with API token</li> <li>add secret to our pipeline</li> <li>add build step</li> </ul>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#create-secrets_1","title":"Create Secrets","text":"<p>At the time of writing - May 2020 - Jenkins X doesn't support reading secrets from Vault into the Pipeline. To have secrets in the pipeline we create a Kubernetes secret.</p> <p>The OSS Index scanner automatically uses the environment variable XXX, so we use that as our secret Key.</p> <pre><code>kubectl create secret generic quarkus-fruits-ossindex -n jx \\\n  --from-literal=OSS_INDEX_TOKEN='mytoken'\n</code></pre>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#configure-secrets-in-pipeline_1","title":"Configure Secrets In Pipeline","text":"<p>Again, we inject the secret as environment variable via the <code>envFrom</code> construction:</p> <pre><code>envFrom:\n  - secretRef:\n      name: quarkus-fruits-ossindex\n</code></pre> <p>jenkins-x.yml</p> <p>Which means the <code>jenkins-x.yml</code> will look like this.</p> <pre><code>buildPack:  maven-java11\npipelineConfig:\n  containerOptions:\n    envFrom:\n      - secretRef:\n          name: quarkus-fruits-sonar\n      - secretRef:\n          name: quarkus-fruits-ossindex\n</code></pre>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#add-pipeline-step","title":"Add Pipeline Step","text":"<p>In this case, we're going to run this build step after our <code>sonar</code> analysis. The name we match on, is <code>sonar</code> and type to <code>after</code>.</p> <p>The build step order in the <code>jenkins-x.yml</code> does not matter. However, I recommend to put the snippet below right after the sonar build step from the previous paragraph.</p> <p>jenkins-x.yml</p> <pre><code>  - name: sonar\n    stage: build\n    step:\n      name: sonatype-ossindex\n      command: mvn \n      args: \n        - org.sonatype.ossindex.maven:ossindex-maven-plugin:audit \n        - -f\n        - pom.xml\n        - -Dossindex.scope=compile\n        - -Dossindex.reportFile=ossindex.json\n        - -Dossindex.cvssScoreThreshold=4.0\n        - -Dossindex.fail=false\n    type: after\n</code></pre>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#fail-on-analysis-error","title":"Fail On Analysis Error","text":"<p>At the time of writing (June 2020), there is a vulnerability related to <code>org.apache.thrift:libthrift</code>, which is part of <code>quarkus-smallrey-opentracing</code>. While we're not using this library yet, we will add it later in this guide. A version of <code>libthrift</code> is not vulnerable is - at this time - not compatible with our other libraries.</p> <p>So, we can:</p> <ol> <li>not use open tracing -&gt; non negiotiable</li> <li>implement open tracing with another library -&gt; risk, might not be Native Image compatible</li> <li>ignore this particular vulnerability -&gt; risk, high chance we forget this</li> <li>not fail the build -&gt; risk, only acceptable if people are actively pursuing a clean build (meaning, no warnings)</li> </ol> <p>In the example I've gone for option #4, but I recommend you make your own choice.</p>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#container-image-scanning","title":"Container Image Scanning","text":"<p>There are a lot of other potential improvements to our pipeline, such as security scanning, load testing, and container image scanning. I feel that adding such a step would focus to much on the tool and not a lot on improving our Jenkins X Pipeline. So this is an exercise left to the reader.</p> <p>There might be a specific guide to validating Container Images in Jenkins X in the future, but this will be a standalone guide. It warrants more attention than I want to give it in this guide.</p>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#code-snapshots","title":"Code Snapshots","text":"<p>There's a branch for the status of the code after:</p> <ul> <li>adding Sonar analysis, in the branch 07-sonar.</li> <li>adding OSS Index analysis, in the branch 07-ossindex</li> </ul>"},{"location":"jenkinsx/java-native-prod/07-pipeline-improvements/#next-steps","title":"Next Steps","text":"<p>The next step is to add Integration Tests via Preview Environments, so we increase the confidence in our application's stability prior to promoting it to (semi-)permanent environments - such as Staging and Production.</p>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/","title":"Previews &amp; Integration Tests","text":"<p>As mentioned in Pipeline Improvements (previous page), here we will dive into - amongst other things - running integration tests with PostMan.</p> <p>The first part of the title is Previews. The name comes from the Jenkins X feature called Preview Environments. We will explore how we can leverage Preview Environments for running Integration Tests.</p> <p>Jenkins X allows users to test and validate changes to code in a specialized fourth tier called a preview environment, which is a temporary tier where quick testing, feedback and limited availability demos for a wider user base can be done before changes are merged to master for production deployment. This gives developers the ability to receive faster feedback for their changes. - CloudBees Jenkins X Distribution Guide</p> <p>The plan for this part of the guide, is to run a PostMan test suite every time we create or update a PullRequest (PR) on its related Preview Environment.</p> <p>Important</p> <p>While committing frequently is important, please hold of until the end please.</p> <p>The idea is to create a PullRequest, rather than a commit to <code>master</code>.</p>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#code-snapshots","title":"Code Snapshots","text":"<p>There's a branch for the status of the code after:</p> <ul> <li>adding Sonar analysis, in the branch 07-sonar.</li> <li>adding OSS Index analysis, in the branch 07-ossindex</li> </ul>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#postman-test-suite","title":"PostMan Test Suite","text":"<p>Postman is a good and commonly used [rest] API testing tool. It has a CLI alternative, which also ships as a Docker image, called Newman.</p> <p>We can use this to test our running Preview application.</p> <p>First, we create a collection of tests with Postman, which we can then export as a json file.</p>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#postman-test-suite-json","title":"PostMan Test Suite JSON","text":"postman-suite-01.json <p>We specify to variables in this script, <code>baseUrl</code>, which we set in the Jenkins X Pipeline Step, and <code>MANDARIN_ID</code>. <code>MANDARIN_ID</code> is set in one of the tests, when we add a new entry to the database.</p> <p>This way, we can ensure we also cleanup the database, so that an update to the PR has the original data set to work with.</p> <pre><code>{\n  \"variables\": [],\n  \"info\": {\n    \"name\": \"postman-suite-01\",\n    \"_postman_id\": \"2c62b599-f952-d49c-3b36-5fb1b7a77472\",\n    \"description\": \"\",\n    \"schema\": \"https://schema.getpostman.com/json/collection/v2.0.0/collection.json\"\n  },\n  \"item\": [\n    {\n      \"name\": \"find-all-fruits\",\n      \"event\": [\n        {\n          \"listen\": \"test\",\n          \"script\": {\n            \"type\": \"text/javascript\",\n            \"exec\": [\n              \"tests[\\\"Successful GET request\\\"] = responseCode.code === 200;\",\n              \"\",\n              \"tests[\\\"Response time is less than 400ms\\\"] = responseTime &lt; 400;\",\n              \"\",\n              \"var jsonData = JSON.parse(responseBody);\",\n              \"tests[\\\"JSON Data Test-1\\\"] = jsonData[0].name === \\\"Cherry\\\";\",\n              \"tests[\\\"JSON Data Test-2\\\"] = jsonData[1].name === \\\"Apple\\\";\",\n              \"tests[\\\"JSON Data Test-3\\\"] = jsonData[2].name === \\\"Banana\\\";\",\n              \"tests[\\\"JSON Data Test-4\\\"] = jsonData[3].color === \\\"Green\\\";\",\n              \"tests[\\\"JSON Data Test-5\\\"] = jsonData[4].color === \\\"Red\\\";\",\n              \"\"\n            ]\n          }\n        }\n      ],\n      \"request\": {\n        \"url\": \"{{baseUrl}}/fruits\",\n        \"method\": \"GET\",\n        \"header\": [],\n        \"body\": {},\n        \"description\": \"Test fina all fruits\"\n      },\n      \"response\": []\n    },\n    {\n      \"name\": \"post-new-fruit\",\n      \"event\": [\n        {\n          \"listen\": \"test\",\n          \"script\": {\n            \"type\": \"text/javascript\",\n            \"exec\": [\n              \"tests[\\\"Successful GET request\\\"] = responseCode.code === 200;\",\n              \"\",\n              \"tests[\\\"Response time is less than 400ms\\\"] = responseTime &lt; 400;\",\n              \"var jsonData = JSON.parse(responseBody);\",\n              \"postman.setGlobalVariable(\\\"MANDARIN_ID\\\", jsonData.id);\"\n            ]\n          }\n        }\n      ],\n      \"request\": {\n        \"url\": \"{{baseUrl}}/fruits/name/Mandarin/color/Orange\",\n        \"method\": \"POST\",\n        \"header\": [],\n        \"body\": {},\n        \"description\": \"\"\n      },\n      \"response\": []\n    },\n    {\n      \"name\": \"post-new-fruit-again\",\n      \"event\": [\n        {\n          \"listen\": \"test\",\n          \"script\": {\n            \"type\": \"text/javascript\",\n            \"exec\": [\n              \"tests[\\\"Successful GET request\\\"] = responseCode.code === 500;\",\n              \"\",\n              \"tests[\\\"Response time is less than 400ms\\\"] = responseTime &lt; 400;\",\n              \"\",\n              \"\"\n            ]\n          }\n        }\n      ],\n      \"request\": {\n        \"url\": \"{{baseUrl}}/fruits/name/Mandarin/color/Orange\",\n        \"method\": \"POST\",\n        \"header\": [],\n        \"body\": {},\n        \"description\": \"\"\n      },\n      \"response\": []\n    },\n    {\n      \"name\": \"{{baseUrl}}/fruits/{{MANDARIN_ID}}\",\n      \"event\": [\n        {\n          \"listen\": \"test\",\n          \"script\": {\n            \"type\": \"text/javascript\",\n            \"exec\": [\n              \"tests[\\\"Successful GET request\\\"] = responseCode.code === 204;\",\n              \"\",\n              \"tests[\\\"Response time is less than 400ms\\\"] = responseTime &lt; 400;\"\n            ]\n          }\n        }\n      ],\n      \"request\": {\n        \"url\": \"{{baseUrl}}/fruits/{{MANDARIN_ID}}\",\n        \"method\": \"DELETE\",\n        \"header\": [],\n        \"body\": {},\n        \"description\": \"\"\n      },\n      \"response\": []\n    }\n  ]\n}\n</code></pre>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#preview-environments","title":"Preview Environments","text":"<p>When a preview environment is up and running Jenkins X will submit a comment to a Pull Request with a link to a temporary build of the project so that development members or invited end users can demo the preview.  Using preview environments any pull request can have a preview version built and deployed, including libraries that feed into a downstream deployable application. This means development team members can perform code reviews, run unit or cross-functional behavior-driven development (BDD) tests, and grow consensus as to when a new feature can be deployed to production. - CloudBees Jenkins X Distribution Guide</p> <p>For more information on Preview Environments, you can read the Jenkins X Guide on Preview Environments, or the Guide on the Promotion mechanism within Jenkins X.</p> <ul> <li>create preview environment</li> <li>update preview pipeline</li> <li>add mysql database as preview dependency - so we test in a throwaway database</li> <li>update application.properties file</li> <li>tweak Helm Chart configuration</li> </ul>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#create-preview-environment","title":"Create Preview Environment","text":"<p>I included this paragraph for completeness. The only thing you have to do to create a Preview Environment, is to let Jenkins X create one for you. You do this by creating a Pull Request, on an application that is managed by Jenkins X. </p> <p>If you're not sure how to create a Pull Request, GitHub has a nice guide on this.</p>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#update-preview-pipeline","title":"Update Preview Pipeline","text":"<p>We can then call this JSON file an a new Jenkins X Pipeline step. We want to run that step against a running application, so we run it after preview-promote step, which will finish with confirming the preview is live.</p> <p>Which will something like this:</p> <p>jenkins-x.yml</p> <pre><code>- name: jx-preview\n  stage: promote\n  pipeline: pullRequest\n  step:\n    name: postman-tests\n    dir: /workspace/source\n    image: postman/newman\n    command: newman\n    args:\n      - run\n      - postman-suite-01.json\n      - --global-var\n      - \"baseUrl=http://REPO_NAME.jx-REPO_OWNER-REPO_NAME-pr-${PULL_NUMBER}.DEV_DOMAIN\"\n      - --verbose\n  type: after\n</code></pre> <p>Important</p> <p>Make sure you replace the URL with the actual URL of your application.</p> <p>The baseURL highlighted in the above example, <code>http://REPO_NAME.jx-REPO_OWNER-REPO_NAME-pr-${PULL_NUMBER}.DEV_DOMAIN</code>, depends on your domain, application name and repository owner.</p> <p>The value <code>${PULL_NUMBER}</code> is managed by Jenkins X, leave that in. The values <code>REPO_NAME</code>, <code>REPO_OWNER</code>, and <code>DEV_DOMAIN</code> depend on you. If you've forked, or otherwise reused my <code>quarkus-fruits</code> application, the <code>REPO_NAME</code> will be <code>quarkus-fruits</code>.</p> <p>Adjust the configuration accordingly!</p> <p>As each PR will have a unique URL based on the PR number, we set the global variable - from Newman perspective - <code>baseUrl</code> to <code>$PULL_NUMBER</code>. Which is a Pipeline environment variable provided by the Jenkins X Pipeline.</p>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#ensure-sorted-list-is-returned","title":"Ensure Sorted List Is Returned","text":"<p>The test I've written with PostMan is a bit silly.  It evaluates each element of the returned list, expecting a fixed order.</p> <p>As our code returns a List, we can sort it via a comparator. With Java's Lambda support, this becomes a quite readable single line.</p> <p>FruitResource.java</p> <pre><code>public List&lt;Fruit&gt; findAll() {\n    var it = fruitRepository.findAll();\n    List&lt;Fruit&gt; fruits = new ArrayList&lt;Fruit&gt;();\n    it.forEach(fruits::add);\n    fruits.sort(Comparator.comparing(Fruit::getId));\n    return fruits;\n}\n</code></pre>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#mysql-database-as-preview-dependency","title":"MySQL Database as Preview Dependency","text":"<p>You might have wondered until now, why there are two Helm Charts in the <code>charts/</code> folder. One of the Charts, which we haven't used until now, is called <code>preview</code>. </p> <p>Guess what, it is used to generate the preview environment installation.  It has a <code>requirements.yaml</code> file with its dependencies. As you can see, it always includes your main application via the <code>file://../</code> directive. This must be the last entry.</p> <p>We use a different database for our tests, and it should be a throw-away database. The easiest way to do so, is to add <code>mysql</code> as a dependency to our Preview Chart.</p> <p>This way, every preview environment has its own database, so our tests do not pollute other PR's or our permanent databases.</p> <p>charts/preview/requirements.yaml</p> <pre><code>- name: mysql\n  version: 1.6.3\n  repository:  https://kubernetes-charts.storage.googleapis.com\n\n  # !! \"alias: preview\" must be last entry in dependencies array !!\n  # !! Place custom dependencies above !!\n- alias: preview\n  name: quarkus-fruits\n  repository: file://../quarkus-fruits\n</code></pre>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#update-properties","title":"Update Properties","text":"<p>We make sure <code>quarkus.datasource.jdbc.url</code> is now a variable, so we can set a different value in the Preview Environment.</p> <p>src/main/resources/application.properties</p> <p>The highlighted lines are the changes. </p> <pre><code>quarkus.datasource.db-kind=mysql\nquarkus.datasource.jdbc.url=jdbc:mysql://127.0.0.1:3306/fruits\nquarkus.datasource.jdbc.max-size=8\nquarkus.datasource.jdbc.min-size=2\n\nquarkus.datasource.username=${GOOGLE_SQL_USER}\nquarkus.datasource.password=${GOOGLE_SQL_PASS}\nquarkus.datasource.jdbc.url=${GOOGLE_SQL_CONN}\n\nquarkus.flyway.migrate-at-start=true\nquarkus.flyway.baseline-on-migrate=true\n\nquarkus.hibernate-orm.database.generation=none\nquarkus.log.level=INFO\nquarkus.log.category.\"org.hibernate\".level=INFO\n</code></pre>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#update-helm-chart","title":"Update Helm Chart","text":"<p>We have to make a few related changes.</p> <ol> <li>Deployment: so we only include the CloudSQL proxy container if we will talk to a CloudSQL Database</li> <li>Chart Values: to set defaults for the CloudSQL proxy container configuration</li> <li>Preview Chart Values: to configure the MySQL dependency</li> </ol>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#deployment","title":"Deployment","text":"<p>A common way to enbale of disable segments in your Helm Charts, is by adding a <code>x.enabled</code> property, where <code>x</code> is the feature to be toggled.</p> <p>We do the same, and as we want to toggle the CloudSQL Proxy container, we add the Go templating equavalent if a <code>if x then ..</code>. Which is <code>{{ if eq }} ... {{ end }}</code>, as you can see below.</p> <p>The toggle now ensures we add the CloudSQL Proxy container if <code>cloudsql.enabled</code> is <code>true</code>.</p> <p>templates/deployment.yaml</p> <pre><code>{{ if eq .Values.cloudsql.enabled \"true\" }}\n- name: cloudsql-proxy\n  image: gcr.io/cloudsql-docker/gce-proxy:1.16\n  command: [\"/cloud_sql_proxy\",\n            \"-instances={{.Values.secrets.sql_connection}}=tcp:3306\",\n            \"-credential_file=/secrets/cloudsql/credentials.json\"]\n  volumeMounts:\n    - name: cloudsql-instance-credentials\n      mountPath: /secrets/cloudsql\n      readOnly: true\n{{ end }}\n</code></pre>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#chart-values","title":"Chart Values","text":"<p>In <code>charts/Name-of-Your-Application/values.yaml</code> we set default values for the CloudSQL configuration. Namely the <code>GOOGLE_SQL_CONN</code> to connect to the CloudSQL proxy container, and <code>cloudsql.enabled=true</code> for the toggle we created in the previous paragraph.</p> <p>values.yaml</p> <pre><code>cloudsql:\n  enabled: \"true\"\n\n# define environment variables here as a map of key: value\nenv:\n  GOOGLE_SQL_USER: vault:quarkus-petclinic:GOOGLE_SQL_USER\n  GOOGLE_SQL_CONN: jdbc:mysql://127.0.0.1:3306/fruits\n</code></pre>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#preview-chart-values","title":"Preview Chart Values","text":"<p>We add some basic configuration for our MySQL dependency. Such as the passwords, storage, and database name.</p> <p>Via the <code>preview</code> property, we configures our application's Helm Chart. We ensure our Helm Chart is configured so our application will connect to the Preview Environment's MySQL database, and not to run a CloudSQL proxy container.</p> <p>charts/preview/values.yaml</p> <pre><code>mysql: \n  mysqlUser: fruitsadmin\n  mysqlPassword: JFjec3c7MgFH6cZyKaVNaC2F\n  mysqlRootPassword: 4dDDPE5nj3dVPxDYsPgCzu9B\n  mysqlDatabase: fruits\n  persistence:\n    enabled: true\n    size: 50Gi\n\npreview:\n  cloudsql:\n    enabled: \"false\"\n  secrets:\n    sql_password: \"4dDDPE5nj3dVPxDYsPgCzu9B\"\n  env:\n    GOOGLE_SQL_USER: root\n    GOOGLE_SQL_CONN: jdbc:mysql://preview-mysql:3306/fruits\n</code></pre>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#creating-the-pullrequest","title":"Creating The PullRequest","text":""},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#verify-all-is-good","title":"Verify All Is Good","text":"<p>Before committing, ensure all the changes are correct.</p> Helm Lint PreviewHelm Lint ChartValidate JX PipelineMaven Test <pre><code>helm lint charts/preview\n</code></pre> <pre><code>helm lint charts/quarkus-fruits\n</code></pre> <pre><code>jx step syntax effective\n</code></pre> <pre><code>mvn test\n</code></pre>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#create-the-git-branch-commit","title":"Create The Git Branch &amp; Commit","text":"<p>The files that have changed are the following (confirm with <code>git status</code>):</p> <pre><code>\u2570\u2500\u276f git status\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory)\n\n    modified:   charts/preview/requirements.yaml\n    modified:   charts/preview/values.yaml\n    modified:   charts/quarkus-fruits/values.yaml\n    modified:   charts/quarkus-fruits/templates/deployment.yaml\n    modified:   jenkins-x.yml\n    modified:   src/main/java/com/github/joostvdg/demo/jx/quarkusfruits/FruitResource.java\n    modified:   src/main/resources/application.properties\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n\n    postman-suite-01.json\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n</code></pre> <p>Create a new git branch by typing the following:</p> <pre><code>git checkout -b pr-test\n</code></pre> <p>Add all the modified files and the untrackeded <code>postman-suite-01.json</code>, commit and verify the PR build.</p> <pre><code>git add .\ngit commit -m \"creating PR tests\"\n</code></pre> <p>Push the changes:</p> <pre><code>git push --set-upstream origin pr-test\n</code></pre>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#create-the-pullrequest","title":"Create The PullRequest","text":"<p>You don't need to follow the link GitHub gave you when you pushed, you can also create the PR via Jenkins X's CLI.</p> <pre><code>jx create pullrequest \\\n    --title \"Adding PR Tests\" \\\n    --body \"This is the text that describes the PR\" \\\n    --batch-mode\n</code></pre>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#watch-activity","title":"Watch Activity","text":"<p>To see the new PR build going, you can watch the activity log or the build log.</p> <pre><code>jx get activity -f quarkus-fruits -w\n</code></pre> <pre><code>jx get build log quarkus-fruits\n</code></pre> <p>Note</p> <p>Remember, the PostMan tests will only fire after the Preview Environment is created.</p> <p>Be patient and wait for the build to succeed, the preview environment to be created, and our application to be up and running in this environment. Only then will our amazing test run.</p>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#activity-result","title":"Activity Result","text":"<pre><code>  from build pack                                                7m49s    7m42s Succeeded\n    Credential Initializer P6j4d                                 7m49s       0s Succeeded\n    Working Dir Initializer Wk5gt                                7m49s       0s Succeeded\n    Place Tools                                                  7m49s       2s Succeeded\n    Git Source Joostvdg Quarkus Fruits Pr 2 Pr 28rkj 2njtr       7m47s       8s Succeeded https://github.com/joostvdg/quarkus-fruits.git\n    Git Merge                                                    7m39s       2s Succeeded\n    Build Set Version                                            7m37s      11s Succeeded\n    Build Mvn Deploy                                             7m26s    5m44s Succeeded\n    Build Skaffold Version                                       1m42s       0s Succeeded\n    Build Container Build                                        1m42s      29s Succeeded\n    Postbuild Post Build                                         1m13s       1s Succeeded\n    Promote Make Preview                                         1m12s      30s Succeeded\n    Promote Jx Preview                                             42s      33s Succeeded\n    Promote Postman Tests                                           9s       2s Succeeded\n  Preview                                                          13s           https://github.com/joostvdg/quarkus-fruits/pull/2\n    Preview Application                                            13s           http://quarkus-fruits.jx-joostvdg-quarkus-fruits-pr-2\n</code></pre>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#log-result","title":"Log Result","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         \u2502          executed \u2502            failed \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              iterations \u2502                 1 \u2502                 0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                requests \u2502                 4 \u2502                 0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502            test-scripts \u2502                 4 \u2502                 0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502      prerequest-scripts \u2502                 0 \u2502                 0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              assertions \u2502                13 \u2502                 0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 total run duration: 313ms                                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 total data received: 3.13KB (approx)                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 average response time: 30ms [min: 10ms, max: 55ms, s.d.: 20ms]  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 average DNS lookup time: 9ms [min: 9ms, max: 9ms, s.d.: 0\u00b5s]    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 average first byte time: 22ms [min: 6ms, max: 42ms, s.d.: 15ms] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"jenkinsx/java-native-prod/08-preview-int-test/#next-steps","title":"Next Steps","text":"<p>Now that we have more tests and validations in our application, we focus our attention on how the application is running. Things like logging, metrics, and tracing.</p>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/","title":"Production Improvements","text":"<p>Now that we have more tests and validations in our application, we focus our attention on how the application is running. Things like logging, metrics, and tracing.</p> <p>There are again, more than a dozen things we can do to keep tabs on our application in Production. Instead, we will dive into three common things that are easy to do in Kubernetes and supported by Quarkus.</p> <ol> <li>Logging, with Sentry</li> <li>Monitoring Metrics with Prometheus &amp; Grafana</li> <li>Tracing with OpenTracing &amp; Jaeger</li> </ol>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#code-start","title":"Code Start","text":"<p>If you do not have a working version after the previous chapter, you can find the complete working code in the branch 08-previews.</p>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#logging","title":"Logging","text":"<p>There are a many ways you can deal with Logging. You can make use of the ELK Stack (ElasticSearch, Logstash and Kibana), Grafana's Loki, or Public Cloud services such as Google's Operations (formerly Stackdriver) or AWS's CloudWatch.</p> <p>While they're great for a whole class or problems, depending on your needs, there are free SaaS solutions that can cover what you need. Two that I've used are Papertrail and Sentry. Personally, I'm quite happy with my hobby applications using Sentry - in the Free tier. Add the fact that it is natively supported by Quarkus, and I think I can get away with selecting Sentry for this exercise.</p> <p>If you don't agree, or want to use an alternative solution, skip ahead to the monitoring paragraph.</p> <p>Info</p> <p>Sentry also has a self-hosting option, so you do not have to use the SaaS if you don't want to.</p> <p>I recommend to use the SaaS, but if you can't or prefer to selve host, you can use the official docker container or the Helm Chart.</p>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#steps","title":"Steps","text":"<ul> <li>Create an account on sentry.io and create a new <code>project</code>. <ul> <li>or, host your own, and then create a new <code>project</code></li> </ul> </li> <li>add sentry dependency to <code>pom.xml</code></li> <li>retrieve Sentry DSN for you application</li> <li>add Sentry DSN into Vault</li> <li>add Kubernetes secret to templates mounting the vault secret as Kubernetes secret</li> <li>bind Kubernetes secret to Deployment</li> <li>configure Sentry logging</li> </ul>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#create-sentry-project-retrieve-sdn","title":"Create Sentry Project &amp; Retrieve SDN","text":"<p>First, create a new Project.</p> <p>Then, to retrieve the DSN:</p> <ul> <li><code>Settings</code> -&gt; <code>Projects</code> -&gt; <code>&lt;Select Your Project&gt;</code> -&gt; <code>Client Keys(DSN)</code></li> </ul>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#quarkus-sentry-dependency","title":"Quarkus Sentry Dependency","text":"<p>pom.xml</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-logging-sentry&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#add-secret-to-vault","title":"Add secret to Vault","text":"<p>We have to add the secret to Vault.</p> <p>If you've forgotten how to do this, you can ask Jenkins X for the URL and the Token to login.</p> <pre><code>jx get vault-config\n</code></pre> <p>After that, nagivate to <code>secrets/quarkus-fruits</code>, and create a new version of the secret. Add a new Key/Value pair with <code>SENTRY_DSN</code> as the Key, the DSN value as the Value.</p>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#add-secret-placeholder-to-valuesyaml","title":"Add secret placeholder to values.yaml","text":"<p>As Sentry can automatically detect which environment the application is running in, you don't need to have different authentication per environment. So we're free to use a global secret - one secret for all environment.</p> <p>charts/Name-Of-Your-Application/value.yaml</p> <pre><code>secrets:\n  sql_password: \"\"\n  sql_connection: \"\"\n  sqlsa: \"\"\n  sentry_dsn: vault:quarkus-fruits:SENTRY_DSN\n</code></pre>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#sentry-dsn-secret","title":"Sentry DSN Secret","text":"<p>We create a Kubernetes secret in our Chart's template folder. This way we can mount the secret as an environment variable </p> <p>The Sentry SDK automatically picks up the environment variable <code>SENTRY_DSN</code>, so we don't have to configure anything 7else to get it to work. The SDK is OK with not having the configuration as well, so for local tests or our builds we're fine without it.</p> <p>charts/Your-Application-Name/templates/sentry-dsn-secret.yaml</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: {{ template \"fullname\" . }}-sentry-dsn\ntype: Opaque\ndata:\n  SENTRY_DSN: {{ .Values.secrets.sentry_dsn | b64enc }}\n</code></pre>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#update-deployment-manifest","title":"Update Deployment manifest","text":"<p>Now that we have the Kubernetes secret, have to add the Sentry secret to our application's Container in the Deployment manifest.</p> <pre><code>envFrom:\n  - secretRef:\n      name: {{ template \"fullname\" . }}-sentry-dsn\n</code></pre> <p>Our Deployment now looks like this (at least, the section related to our container):</p> <p>charts/Name-Of-Your-Application/templates/deployment.yaml</p> <pre><code>      - name: {{ .Chart.Name }}\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n        imagePullPolicy: {{ .Values.image.pullPolicy }}\n        env:\n{{- range $pkey, $pval := .Values.env }}\n        - name: {{ $pkey }}\n          value: {{ quote $pval }}\n{{- end }}\n        envFrom:\n        - secretRef:\n            name: {{ template \"fullname\" . }}-sql-secret\n        - secretRef:\n            name: {{ template \"fullname\" . }}-sentry-dsn\n</code></pre>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#configure-logging","title":"Configure Logging","text":"<p>The last bit to get Sentry &amp; Quarkus work their magic, is to configure the logging in our applications's properties file.</p> <p>src/main/resources/application.properties</p> <pre><code>quarkus.log.sentry=true\nquarkus.log.sentry.dsn=${SENTRY_DSN}\nquarkus.log.sentry.level=WARN\nquarkus.log.sentry.in-app-packages=com.github.joostvdg\n</code></pre> <p>Important</p> <p>I've configured my personal package root, <code>com.github.joostvdg</code>, replace this with the package name of your application!</p>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#monitoring","title":"Monitoring","text":"<p>As we're running Jenkins X, we run in Kubernetes.  The most commonly used monitoring solution with Kubernetes is Prometheus and Grafana.</p> <p>Quarkus has out-of-the-box support for exposing prometheus metrics, via the <code>smallrye-metrics</code> library.</p> <p>To create a Grafana dashboard for our application, we need to take the following steps:</p> <ul> <li>ensure we have Prometheus and Grafana installed in our cluster</li> <li>add dependency on Quarkus' <code>smallrye-metrics</code> library</li> <li>add (Kubernetes) annotations to our Helm Chart's Deployment manifest</li> <li>add (Java) annotations to our code, specifying the metrics</li> </ul> <p>For more information on adding metrics to your Quarkus application, read the Quarkus Metrics guide.</p>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#install-promtheus-grafana","title":"Install Promtheus &amp; Grafana","text":"<p>The easiest way to install Promtheus and Grafana is via a Helm Chart.</p> <p>While they are commonly used together, they have their own Helm Charts.</p>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#prometheus","title":"Prometheus","text":"<p>Prometheus is part of Helm's stable charts, and you can find it here.</p> <p>The Prometheus Helm Chart is weldefined, so for testing purposes, there's nothing you need to configure.  I recommend you go through the options if this will be a (near)permanent installation.</p> Helm v3Helm v2 <pre><code>helm install prometheus stable/prometheus  --namespace ${NAMESPACE}\n</code></pre> <pre><code>helm install --name prometheus stable/prometheus  --namespace ${NAMESPACE}\n</code></pre>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#grafana","title":"Grafana","text":"<p>Grafana's Helm Chart is also part Helm's stable chart list.</p>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#values","title":"Values","text":"<p>Grafana does have some values we have to set. I recommend setting the persistence to enabled and the persistence type to <code>statefulset</code>. This ensures that dashboards you install or create survice restarts, quite useful.</p> <p>One thing you have to configure, is the <code>ingress</code>, else you cannot access your Grafana installation. Below is an example configuration, you have to replace the hosts' configuration with yours. And if you do not have TLS enabled, remove the TLS segment.</p> <p>I also included how you can configure dashboards out of the box. One of doing so that is, this allows you to add public dashboards by their ID. Visit the Grafana public Dashboards page to explore what the community has to offer.</p> <p>grafana-values.yaml</p> <pre><code>persistence:\n  enabled: true\n  type: statefulset\ningress:\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    kubernetes.io/tls-acme: \"true\"\n  enabled: true\n  hosts:\n  - grafana.example.com\n  tls:\n  - hosts:\n    - grafana.example.com\n    secretName: tls-grafana-p\ndashboardProviders:\n  dashboardproviders.yaml:\n    apiVersion: 1\n    providers:\n    - disableDeletion: true\n      editable: true\n      folder: default\n      name: Default\n      options:\n        path: /var/lib/grafana/dashboards/default\n      orgId: 1\n      type: file\ndashboards:\n  default:\n    Capacity:\n      datasource: Prometheus\n      gnetId: 5228\n      revision: 6\n</code></pre>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#install","title":"Install","text":"Helm v3Helm v2 <pre><code>helm install grafana stable/grafana -f grafana-values.yaml  --namespace ${NAMESPACE}\n</code></pre> <pre><code>helm install --name grafana stable/grafana -f grafana-values.yaml  --namespace ${NAMESPACE}\n</code></pre>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#prometheus-grafana-in-jenkins-x-environment","title":"Prometheus &amp; Grafana in Jenkins X Environment","text":"<p>You can also opt for installing Prometheus and Grafana as a dependency in a Jenkins X environment. There are two ways of creating custom - e.g. not Dev, Staging or Production - environment in Jenkins X. If you want the environment to be part of your <code>jx boot</code> installation, I've written a how to guide on that.</p> <p>If you want a shortcut, there's the jx create environment command. This creates a proper Jenkins X environment, but it isn't managed via <code>jx boot</code>. </p> <p>Either way, the end result is pretty similar. You end up with a Git repository for this environment. In here, you can add Helm Chart dependencies in <code>env/requirements.yaml</code>, and specify their values in <code>env/values.yaml</code>.</p> <p>As this is written, May 2020, Jenkins X still relies on Helm v2. You can read about how Helm v2 works with dependencies here.</p>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#add-quarkus-smallrye-metrics-dependency","title":"Add quarkus-smallrye-metrics Dependency","text":"<p>To make our application expose metrics, default or custom, in a way that Prometheus can scrape, we add the <code>quarkus-smallrye-metrics</code> dependency.</p> <p>pom.xml</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n  &lt;artifactId&gt;quarkus-smallrye-metrics&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#update-deployment-manifest_1","title":"Update Deployment manifest","text":"<p>By default, Prometheus is a bit shy and doesn't scrape your metrics. If you want Prometheus to do so, you have to tell it, it's ok. We do so by setting an annotation in the <code>templates/deployment.yaml</code>. </p> <p>We add two annotations, we add <code>prometheus.io/port: \"8080\"</code> to tell Prometheus on which port to talk to our application. And <code>prometheus.io/scrape: \"true\"</code>, to say that it is oke to scrape our application.</p> <p>templates/deployment.yaml</p> <pre><code>  template:\n    metadata:\n      labels:\n        draft: {{ default \"draft-app\" .Values.draft }}\n        app: {{ template \"fullname\" . }}\n      annotations:\n        prometheus.io/port: \"8080\"\n        prometheus.io/scrape: \"true\"\n{{- if .Values.podAnnotations }}\n{{ toYaml .Values.podAnnotations | indent 8 }} #Only for pods\n{{- end }}\n    spec:\n</code></pre> <p>Important</p> <p>Be sure to move the line <code>{{- if .Values.podAnnotations }}</code> so that <code>annotations:</code> is always set.</p>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#add-metrics-annotations-to-our-code","title":"Add Metrics Annotations to our Code","text":"<p>When you add the metrics dependency, some metrics are exposed by default. These might not say much about your application, so it is advisable to investigate what information you want to get and how to configure that.</p> <p>For example, on the <code>findAll()</code> method, for the <code>/fruits</code> endpoint, we can add a counter - how many times is this endpoint called - and a timer - various percentile buckets on the duration of the call:</p> <pre><code>@Counted(name = \"fruit_get_all_count\", description = \"How many times all Fruits have been retrieved.\")\n@Timed(name = \"fruit_get_all_timer\", description = \"A measure of how long it takes to retrieve all Fruits.\", unit = MetricUnits.MILLISECONDS)\n</code></pre> <p>Look at FruitResource.java for all the metrics I've added as examples.</p> <p>For information on these, I recommend taking another look at the Quarkus Metrics guide.</p>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#configure-grafana","title":"Configure Grafana","text":"<p>If you used this guide's examples for installing Prometheus and Grafana, you have to configure at least a datasource.</p> <p>We do this by logging into Grafana, and opening the Data Sources screen: left hand side, <code>Configuration</code> -&gt; <code>Data Sources</code>.</p> <p>Click <code>Add data source</code>, and select <code>Prometheus</code> as the type of data source. The only field you have to change, is <code>URL</code>. Set the value below, and hit <code>Save &amp; Test</code>.</p> <pre><code>http://prometheus-server:80\n</code></pre> <p>Now we can go to the <code>Dashboards</code> screen and create a dashboard. I leave it up to you to create one to your liking, visit grafana.com/tutorials to learn more.</p> <p>If you want to be lazy, use the dashboard JSON below. Hover over the <code>Dashboards</code> menu, and select <code>Manage</code>.</p> <p>Click on <code>Import</code>, and paste the <code>JSON</code> in the <code>Import via panel json</code> field and hit <code>Load</code>.</p> <p>Visit grafana.com/grafana/dashboards for more pre-designed dashboards.</p> <p>Info</p> <p>If you've used the Helm install above to install Grafana, you can retrieve its password with the command below.</p> <pre><code>kubectl get secret --namespace ${NAMESPACE} grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n</code></pre>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#empty-dashboard","title":"Empty Dashboard","text":"<p>If you have an empty dashboard, it could be because your application has a different name.</p> <p>If this is the case, go to the <code>dashboard settings</code> - top right, gear icon - -&gt; <code>Variables</code> (left menu) -&gt; <code>app</code>, and hit <code>Update</code> at the bottom. </p> <p>In the section <code>Preview of values</code>, the name of your application should now be visible.</p>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#dashboard-json","title":"Dashboard JSON","text":"<p>This is an example Grafana dashboard for the this application. You likely have to change the queries to reflect the name of your applcation.</p> grafana-quarkus-app-dashboard-example.json <pre><code>{\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": \"-- Grafana --\",\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations &amp; Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n  \"editable\": true,\n  \"gnetId\": null,\n  \"graphTooltip\": 0,\n  \"id\": 3,\n  \"iteration\": 1591881137283,\n  \"links\": [],\n  \"panels\": [\n    {\n      \"cacheTimeout\": null,\n      \"datasource\": null,\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": {},\n          \"mappings\": [],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\n                \"color\": \"green\",\n                \"value\": null\n              },\n              {\n                \"color\": \"red\",\n                \"value\": 80\n              }\n            ]\n          }\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 3,\n        \"w\": 7,\n        \"x\": 0,\n        \"y\": 0\n      },\n      \"id\": 4,\n      \"links\": [],\n      \"options\": {\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"orientation\": \"auto\",\n        \"reduceOptions\": {\n          \"calcs\": [\n            \"mean\"\n          ],\n          \"values\": false\n        }\n      },\n      \"pluginVersion\": \"7.0.1\",\n      \"targets\": [\n        {\n          \"expr\": \"sum(base_thread_count{app=\\\"$app\\\"}) by (app)\",\n          \"interval\": \"\",\n          \"legendFormat\": \"{{app}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Threads\",\n      \"type\": \"stat\"\n    },\n    {\n      \"cacheTimeout\": null,\n      \"colorBackground\": true,\n      \"colorPostfix\": false,\n      \"colorPrefix\": false,\n      \"colorValue\": false,\n      \"colors\": [\n        \"#d44a3a\",\n        \"rgba(237, 129, 40, 0.89)\",\n        \"#299c46\"\n      ],\n      \"datasource\": null,\n      \"decimals\": 1,\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": {}\n        },\n        \"overrides\": []\n      },\n      \"format\": \"s\",\n      \"gauge\": {\n        \"maxValue\": 100,\n        \"minValue\": 0,\n        \"show\": false,\n        \"thresholdLabels\": false,\n        \"thresholdMarkers\": true\n      },\n      \"gridPos\": {\n        \"h\": 3,\n        \"w\": 10,\n        \"x\": 7,\n        \"y\": 0\n      },\n      \"id\": 6,\n      \"interval\": null,\n      \"links\": [],\n      \"mappingType\": 1,\n      \"mappingTypes\": [\n        {\n          \"name\": \"value to text\",\n          \"value\": 1\n        },\n        {\n          \"name\": \"range to text\",\n          \"value\": 2\n        }\n      ],\n      \"maxDataPoints\": 100,\n      \"nullPointMode\": \"connected\",\n      \"nullText\": null,\n      \"postfix\": \"\",\n      \"postfixFontSize\": \"50%\",\n      \"prefix\": \"\",\n      \"prefixFontSize\": \"50%\",\n      \"rangeMaps\": [\n        {\n          \"from\": \"null\",\n          \"text\": \"N/A\",\n          \"to\": \"null\"\n        }\n      ],\n      \"sparkline\": {\n        \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n        \"full\": false,\n        \"lineColor\": \"rgb(31, 120, 193)\",\n        \"show\": false,\n        \"ymax\": null,\n        \"ymin\": null\n      },\n      \"tableColumn\": \"jx-quarkus-fruits \",\n      \"targets\": [\n        {\n          \"expr\": \"sum(base_jvm_uptime_seconds{app=\\\"$app\\\"}) by (app)\",\n          \"interval\": \"\",\n          \"legendFormat\": \"{{app}} \",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": \"0,3600\",\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Uptime\",\n      \"type\": \"singlestat\",\n      \"valueFontSize\": \"150%\",\n      \"valueMaps\": [\n        {\n          \"op\": \"=\",\n          \"text\": \"N/A\",\n          \"value\": \"null\"\n        }\n      ],\n      \"valueName\": \"current\"\n    },\n    {\n      \"cacheTimeout\": null,\n      \"colorBackground\": true,\n      \"colorValue\": false,\n      \"colors\": [\n        \"#299c46\",\n        \"rgba(237, 129, 40, 0.89)\",\n        \"#d44a3a\"\n      ],\n      \"datasource\": null,\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": {}\n        },\n        \"overrides\": []\n      },\n      \"format\": \"none\",\n      \"gauge\": {\n        \"maxValue\": 100,\n        \"minValue\": 0,\n        \"show\": false,\n        \"thresholdLabels\": false,\n        \"thresholdMarkers\": true\n      },\n      \"gridPos\": {\n        \"h\": 3,\n        \"w\": 6,\n        \"x\": 17,\n        \"y\": 0\n      },\n      \"id\": 10,\n      \"interval\": null,\n      \"links\": [],\n      \"mappingType\": 1,\n      \"mappingTypes\": [\n        {\n          \"name\": \"value to text\",\n          \"value\": 1\n        },\n        {\n          \"name\": \"range to text\",\n          \"value\": 2\n        }\n      ],\n      \"maxDataPoints\": 100,\n      \"nullPointMode\": \"connected\",\n      \"nullText\": null,\n      \"pluginVersion\": \"6.7.3\",\n      \"postfix\": \"\",\n      \"postfixFontSize\": \"50%\",\n      \"prefix\": \"\",\n      \"prefixFontSize\": \"150%\",\n      \"rangeMaps\": [\n        {\n          \"from\": \"null\",\n          \"text\": \"N/A\",\n          \"to\": \"null\"\n        }\n      ],\n      \"sparkline\": {\n        \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n        \"full\": false,\n        \"lineColor\": \"rgb(31, 120, 193)\",\n        \"show\": false,\n        \"ymax\": null,\n        \"ymin\": null\n      },\n      \"tableColumn\": \"{app=\\\"jx-quarkus-fruits\\\"}\",\n      \"targets\": [\n        {\n          \"expr\": \"sum(application_com_github_joostvdg_demo_jx_quarkusfruits_FruitResource_fruit_get_all_count_total{ app=\\\"$app\\\"}) by (app)\",\n          \"interval\": \"\",\n          \"legendFormat\": \"\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": \"\",\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Fruits GetAll Counter\",\n      \"type\": \"singlestat\",\n      \"valueFontSize\": \"150%\",\n      \"valueMaps\": [\n        {\n          \"op\": \"=\",\n          \"text\": \"N/A\",\n          \"value\": \"null\"\n        }\n      ],\n      \"valueName\": \"current\"\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": null,\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": {}\n        },\n        \"overrides\": []\n      },\n      \"fill\": 1,\n      \"fillGradient\": 0,\n      \"gridPos\": {\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 3\n      },\n      \"hiddenSeries\": false,\n      \"id\": 8,\n      \"legend\": {\n        \"avg\": false,\n        \"current\": false,\n        \"max\": false,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": false\n      },\n      \"lines\": true,\n      \"linewidth\": 1,\n      \"nullPointMode\": \"null\",\n      \"options\": {\n        \"dataLinks\": []\n      },\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"sum(rate(base_gc_time_total_seconds{app=\\\"$app\\\"}[5m])) by (app)\",\n          \"interval\": \"\",\n          \"legendFormat\": \"{{app}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"GC Time Spent Rate\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"s\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"s\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": null,\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": {}\n        },\n        \"overrides\": []\n      },\n      \"fill\": 2,\n      \"fillGradient\": 6,\n      \"gridPos\": {\n        \"h\": 7,\n        \"w\": 11,\n        \"x\": 12,\n        \"y\": 3\n      },\n      \"hiddenSeries\": false,\n      \"id\": 2,\n      \"legend\": {\n        \"alignAsTable\": true,\n        \"avg\": false,\n        \"current\": false,\n        \"max\": false,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": false\n      },\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"nullPointMode\": \"null\",\n      \"options\": {\n        \"dataLinks\": []\n      },\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"sum(base_memory_usedHeap_bytes{ app=~\\\"$app\\\"}) by (app)\",\n          \"interval\": \"\",\n          \"legendFormat\": \"{{app}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"Heap Size\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"decbytes\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"decbytes\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": null,\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": {}\n        },\n        \"overrides\": []\n      },\n      \"fill\": 2,\n      \"fillGradient\": 6,\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 10\n      },\n      \"hiddenSeries\": false,\n      \"id\": 16,\n      \"legend\": {\n        \"avg\": false,\n        \"current\": false,\n        \"max\": false,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": false\n      },\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"nullPointMode\": \"null\",\n      \"options\": {\n        \"dataLinks\": []\n      },\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        {\n          \"expr\": \"sum(rate(container_cpu_system_seconds_total{container_name=\\\"$container\\\"}[5m])) by (container_name)\",\n          \"interval\": \"\",\n          \"legendFormat\": \"{{container_name}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"Container CPU Over Time\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": null,\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": {}\n        },\n        \"overrides\": []\n      },\n      \"fill\": 2,\n      \"fillGradient\": 5,\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 11,\n        \"x\": 12,\n        \"y\": 10\n      },\n      \"hiddenSeries\": false,\n      \"id\": 14,\n      \"legend\": {\n        \"avg\": false,\n        \"current\": false,\n        \"max\": false,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": false\n      },\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"nullPointMode\": \"null\",\n      \"options\": {\n        \"dataLinks\": []\n      },\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": true,\n      \"targets\": [\n        {\n          \"expr\": \"sum(container_memory_usage_bytes{container_name=~\\\"$container\\\"}) by (container_name)\",\n          \"interval\": \"\",\n          \"legendFormat\": \"{{container_name}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"Container Memory\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"decbytes\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"decbytes\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    },\n    {\n      \"aliasColors\": {},\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": null,\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": {}\n        },\n        \"overrides\": []\n      },\n      \"fill\": 3,\n      \"fillGradient\": 6,\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 23,\n        \"x\": 0,\n        \"y\": 18\n      },\n      \"hiddenSeries\": false,\n      \"id\": 12,\n      \"legend\": {\n        \"avg\": false,\n        \"current\": false,\n        \"max\": false,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": false\n      },\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"nullPointMode\": \"null\",\n      \"options\": {\n        \"dataLinks\": []\n      },\n      \"percentage\": false,\n      \"pointradius\": 2,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": true,\n      \"targets\": [\n        {\n          \"expr\": \"sum(rate(application_com_github_joostvdg_demo_jx_quarkusfruits_FruitResource_fruit_get_all_count_total{app=\\\"$app\\\"}[5m])) by (app)\",\n          \"interval\": \"\",\n          \"legendFormat\": \"{{app}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeRegions\": [],\n      \"timeShift\": null,\n      \"title\": \"Fruits GetAll Call Rate\",\n      \"tooltip\": {\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      },\n      \"type\": \"graph\",\n      \"xaxis\": {\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      },\n      \"yaxes\": [\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        },\n        {\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        }\n      ],\n      \"yaxis\": {\n        \"align\": false,\n        \"alignLevel\": null\n      }\n    }\n  ],\n  \"refresh\": \"5m\",\n  \"schemaVersion\": 25,\n  \"style\": \"dark\",\n  \"tags\": [],\n  \"templating\": {\n    \"list\": [\n      {\n        \"allValue\": null,\n        \"current\": {\n          \"selected\": false,\n          \"text\": \"jx-quarkus-fruits\",\n          \"value\": \"jx-quarkus-fruits\"\n        },\n        \"datasource\": \"Prometheus\",\n        \"definition\": \"label_values(base_memory_usedHeap_bytes, app)\",\n        \"hide\": 0,\n        \"includeAll\": false,\n        \"label\": \"App\",\n        \"multi\": false,\n        \"name\": \"app\",\n        \"options\": [\n          {\n            \"selected\": true,\n            \"text\": \"jx-quarkus-fruits\",\n            \"value\": \"jx-quarkus-fruits\"\n          }\n        ],\n        \"query\": \"label_values(base_memory_usedHeap_bytes, app)\",\n        \"refresh\": 0,\n        \"regex\": \"\",\n        \"skipUrlSync\": false,\n        \"sort\": 0,\n        \"tagValuesQuery\": \"\",\n        \"tags\": [],\n        \"tagsQuery\": \"\",\n        \"type\": \"query\",\n        \"useTags\": false\n      },\n      {\n        \"allValue\": null,\n        \"current\": {\n          \"selected\": false,\n          \"text\": \"POD\",\n          \"value\": \"POD\"\n        },\n        \"datasource\": \"Prometheus\",\n        \"definition\": \"label_values(container_memory_usage_bytes, container_name)\\n\",\n        \"hide\": 0,\n        \"includeAll\": false,\n        \"label\": \"Container\",\n        \"multi\": false,\n        \"name\": \"container\",\n        \"options\": [],\n        \"query\": \"label_values(container_memory_usage_bytes, container_name)\\n\",\n        \"refresh\": 1,\n        \"regex\": \"\",\n        \"skipUrlSync\": false,\n        \"sort\": 0,\n        \"tagValuesQuery\": \"\",\n        \"tags\": [],\n        \"tagsQuery\": \"\",\n        \"type\": \"query\",\n        \"useTags\": false\n      }\n    ]\n  },\n  \"time\": {\n    \"from\": \"now-5m\",\n    \"to\": \"now\"\n  },\n  \"timepicker\": {\n    \"refresh_intervals\": [\n      \"10s\",\n      \"30s\",\n      \"1m\",\n      \"5m\",\n      \"15m\",\n      \"30m\",\n      \"1h\",\n      \"2h\",\n      \"1d\"\n    ]\n  },\n  \"timezone\": \"\",\n  \"title\": \"JX Quarkus Demo\",\n  \"uid\": \"6ne5tiRGk\",\n  \"version\": 3\n}\n</code></pre>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#tracing","title":"Tracing","text":"<p>While both Monitoring and Logging are important, they're not the full story in understand your application's behaviour. Your log might reveal there's an issue, and the metrics can show which calls are slowing down. They can't tell you what part of the code is slowing down!</p> <p>To get further information, you need to implement tracing. Well, isn't it just great that Quarkus has you covered here as well? Quarkus omplements OpenTracing via SmallRye OpenTracing. </p> <p>To display the traces, we'll use Jaeger. If you're interrested, Quarkus has a guide on the implementation, or read on and get to work right away.</p>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#steps_1","title":"Steps","text":"<ul> <li>install Jaeger</li> <li>add dependency</li> <li>change logger</li> <li>update the container environment variables</li> <li> <p>update the application's properties</p> </li> <li> <p>https://github.com/opentracing-contrib/java-jdbc</p> </li> </ul>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#install-jaeger","title":"Install Jaeger","text":"<p>Jaeger has an official Helm Chart, which has very sensible defaults. For production use, I do recommend investigation the options available, especially related to its data storage!</p> <pre><code>helm repo add jaegertracing https://jaegertracing.github.io/helm-charts\nhelm repo update\n</code></pre> <pre><code>helm install jaeger jaegertracing/jaeger \\\n  --namespace ${NAMESPACE} \\\n  -f jaeger-values.yaml\n</code></pre> <p>jaeger-values.yaml</p> <pre><code>query:\n  ingress:\n    enabled: true\n    hosts:\n      - chart-example.local\n</code></pre>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#set-logger","title":"Set Logger","text":"<p>To automatically have our traces logged in Jaeger, it seems - I can be wrong - that we have to use the JBoss Logger that Quarkus ships with. As it is there by default, we don't have to add any new dependency.</p> <p>FruitResource.java</p> <pre><code>import org.jboss.logging.Logger;\n\n@RestController\n@RequestMapping(value = \"/fruits\")\npublic class FruitResource {\n\n    private static final Logger LOG = Logger.getLogger(FruitResource.class);\n\n    @GetMapping(\"/\")\n    @Counted(name = \"fruit_get_all_count\", description = \"How many times all Fruits have been retrieved.\")\n    @Timed(name = \"fruit_get_all_timer\", description = \"A measure of how long it takes to retrieve all Fruits.\", unit = MetricUnits.MILLISECONDS)\n    public List&lt;Fruit&gt; findAll() {\n        var it = fruitRepository.findAll();\n        List&lt;Fruit&gt; fruits = new ArrayList&lt;Fruit&gt;();\n        it.forEach(fruits::add);\n        fruits.sort(Comparator.comparing(Fruit::getId));\n        LOG.infof(\"Found {} fruits\", fruits.size());\n        return fruits;\n    }\n}\n</code></pre>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#add-tracing-dependencies","title":"Add Tracing Dependencies","text":"<p>Speaking of dependencies, we do have to add not one, but two for tracing itself. The first one being <code>quarkus-smallrye-opentracing</code> for the tracing basics. </p> <p>If we also add <code>io.opentracing.contrib:opentracing-jdbc</code>, our trace spans will include our JDBC calls as well, how neat!</p> <p>pom.xml</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n  &lt;artifactId&gt;quarkus-smallrye-opentracing&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n  &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;\n  &lt;artifactId&gt;opentracing-jdbc&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#update-application-properties","title":"Update Application Properties","text":"<p>As you're probably used to by now, to ensure Quarkus does the right thing, we have to update the Application Properties. Most of these values are copied from the Quarkus tracing guide, including the log format. </p> <p>I've made two of the values a variable controllable via environment varibles.</p> <ol> <li><code>quarkus.jaeger.sampler-param</code>: this ensure the percentage of calls that is traced, ranging from 0 (0%) to 1 (100%).</li> <li><code>quarkus.jaeger.endpoint</code>: which is something that can vary per environment</li> </ol> <p>src/main/resources/application.properties</p> <pre><code>quarkus.jaeger.service-name=quarkus-fruits\nquarkus.jaeger.sampler-type=const\nquarkus.jaeger.sampler-param=${JAEGER_SAMPLER_RATE}\nquarkus.log.console.format=%d{HH:mm:ss} %-5p traceId=%X{traceId}, spanId=%X{spanId}, sampled=%X{sampled} [%c{2.}] (%t) %s%e%n\nquarkus.datasource.jdbc.driver=io.opentracing.contrib.jdbc.TracingDriver\nquarkus.jaeger.endpoint=${JAEGER_COLLECTOR_ENDPOINT}\n</code></pre>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#update-container-env","title":"Update Container Env","text":"<p>As I've set two variables in the Application Properties file that come from environment variables, we also have to set a default value in our Chart's <code>values.yml</code>.</p> <ul> <li>JAEGER_COLLECTOR_ENDPOINT: The endpoint where we send our jaeger metrics too</li> <li>JAEGER_SAMPLER_RATE: Sample all requests. Set sampler-param to somewhere between 0 and 1, e.g. 0.50, if you do not wish to sample all requests.</li> </ul> <p>Another change is the <code>GOOGLE_SQL_CONN</code> variable. In order for the tracing to work for the JDBC calls, we have to add <code>tracing</code> into the JDBC URL.</p> <p>charts/Name-Of-Your-Application/values.yaml</p> <pre><code>env:\n  GOOGLE_SQL_USER: vault:quarkus-fruits:GOOGLE_SQL_USER\n  GOOGLE_SQL_CONN: jdbc:tracing:mysql://127.0.0.1:3306/fruits\n  JAEGER_COLLECTOR_ENDPOINT: http://jaeger-collector.jaeger:14268/api/traces\n  JAEGER_SAMPLER_RATE: 1\n</code></pre> <p>Caution</p> <p>I've installed Jaeger via the Helm chart in the Namespace <code>jaeger</code>. So the Service name is <code>jaeger-collector.jaeger</code>, change this to reflect your installation.</p> <p>To be sure, you can always verify the service name via <code>kubectl</code>.</p> <pre><code>kubectl get svc -n $NAMESPACE\n</code></pre>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#see-it-in-action","title":"See It In Action","text":"<p>I you have the Jaeger Query UI running with an Ingress, you can access the interface.</p> <p>Make several calls to the application, and you should see it come up under the <code>Service</code> tab in the UI.</p> <p>Select the service and you see the traces, including the database calls made to the CloudSQL Proxy container!</p>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#code-snapshots","title":"Code Snapshots","text":"<p>There's a branch for the status of the code after:</p> <ul> <li>adding Sentry for logging, in the branch 09-sentry.</li> <li>adding Monitoring with Prometheus, in the branch 09-monitoring</li> <li>adding Jaeger with OpenTracing, in the branch 09-tracing</li> </ul>"},{"location":"jenkinsx/java-native-prod/09-production-improvements/#next-steps","title":"Next Steps","text":"<p>Now that we have (more) control over our application and the environment it runs in, we can promote the application to Production.</p>"},{"location":"jenkinsx/java-native-prod/10-promote-prod/","title":"Promote To Production","text":""},{"location":"jenkinsx/java-native-prod/10-promote-prod/#code-snapshots","title":"Code Snapshots","text":"<p>There's a branch for the status of the code after:</p> <ul> <li>adding Sentry for logging, in the branch 09-sentry.</li> <li>adding Monitoring with Prometheus, in the branch 09-monitoring</li> <li>adding Jaeger with OpenTracing, in the branch 09-tracing</li> </ul>"},{"location":"jenkinsx/java-native-prod/10-promote-prod/#configure-production-environment-repository","title":"Configure Production Environment Repository","text":"<p>For each Jenkins X environment that your application is going to land in, such as <code>jx-staging</code> and <code>jx-production</code>, we have to enable Vault support.</p> <p>We do this by making a change in the Environment's <code>jx-requirements.yml</code> file in the root of the repository of the environment. This file might not exist, if so, create it.</p> <p>If you're not sure where the repository of your environment is, you can retrieve this information via the <code>jx</code> CLI.</p> <pre><code>jx get environments\n</code></pre> <p>To enable support for Vault, we add <code>secretStorage: vault</code> to the file. The file will look like this:</p> <p>jx-requirements.yml</p> <pre><code>secretStorage: vault\n</code></pre>"},{"location":"jenkinsx/java-native-prod/10-promote-prod/#add-application-values","title":"Add Application Values","text":"<p>Same as we did in the chapter <code>Database Connection Ad Secrets</code>, we add the application's values in the <code>env/values.yaml</code> file of the environment.</p> <p>env/values.yaml</p> <pre><code>quarkus-fruits:\n  secrets:\n    sql_connection: vault:quarkus-fruits:INSTANCE_CONNECTION_NAME\n    sql_sa: vault:quarkus-fruits:SA\n    sql_password: vault:quarkus-fruits:GOOGLE_SQL_PASS\n</code></pre>"},{"location":"jenkinsx/java-native-prod/10-promote-prod/#promote-application","title":"Promote Application","text":"<p>First, retrieve the current version of your application.</p> <pre><code>jx get application\n</code></pre> <p>It should yield something like this:</p> <pre><code>APPLICATION    STAGING PODS URL\nquarkus-fruits 1.0.51  1/1  https://quarkus-fruits-jx-staging.staging.example.com\n</code></pre> <p>We can now promote the latest working version in staging, <code>1.0.49</code> in my casse, to Production!</p> <pre><code>VERSION=1.0.51\n</code></pre> <pre><code>jx promote --app quarkus-fruits --version ${VERSION} --env production -b\n</code></pre>"},{"location":"jenkinsx/java-native-prod/10-promote-prod/#completed","title":"Completed","text":"<p>Well, it is never finished now is it?</p> <p>But this is as much as I want to write down, for now.</p>"},{"location":"kubernetes/","title":"Kubernetes","text":"<p>Some resources on how to start with Kubernetes.</p>"},{"location":"kubernetes/#workshops","title":"Workshops","text":"<ul> <li>My Own</li> <li>Jerome Petazzoni</li> <li>Viktor Farcic</li> <li>Play With Kubernetes</li> <li>Katacoda</li> <li>VMWare</li> </ul>"},{"location":"kubernetes/#slide-decks","title":"Slide Decks","text":"<ul> <li>My Own</li> <li>Viktor Farcic</li> </ul>"},{"location":"kubernetes/#books","title":"Books","text":"<ul> <li>DevOps Toolkit 2.3 - Viktor Farcic</li> <li>DevOps Toolkit 2.4 - Viktor Farcic</li> <li>DevOps Toolkit 2.5 - Viktor Farcic</li> <li>Kubernetes Up And Running - Joe Beda, Brendan Burns, Kelsey Hightower</li> <li>The Kubernetes Book - Nigel Poulton</li> </ul>"},{"location":"kubernetes/#articles","title":"Articles","text":"<ul> <li>Article Looking At Kubernetes' Reconciliation</li> <li>In-depth Look At Pods</li> <li>https://medium.com/@vikram.fugro/container-networking-interface-aka-cni-bdfe23f865cf</li> <li>https://itnext.io/benchmark-results-of-kubernetes-network-plugins-cni-over-10gbit-s-network-36475925a560?</li> </ul>"},{"location":"kubernetes/#linux-basics","title":"Linux basics","text":""},{"location":"kubernetes/#namespaces-cgroups","title":"Namespaces &amp; CGroups","text":"<ul> <li>https://jvns.ca/blog/2016/10/10/what-even-is-a-container/</li> <li>https://www.youtube.com/watch?v=sK5i-N34im8</li> <li>https://www.ianlewis.org/en/what-are-kubernetes-pods-anyway</li> </ul>"},{"location":"kubernetes/#networking","title":"Networking","text":"<ul> <li>https://itnext.io/kubernetes-networking-behind-the-scenes-39a1ab1792bb</li> <li>https://github.com/nleiva/kubernetes-networking-links</li> <li>IP Tables</li> <li>CIDR Explanation video</li> <li>Packets &amp; Frames introduction</li> <li>Traefik on AWS</li> </ul>"},{"location":"kubernetes/#metrics","title":"Metrics","text":"<ul> <li>https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-66936addedae</li> </ul>"},{"location":"kubernetes/#secrets","title":"Secrets","text":"<ul> <li>Hashicorp Vault - Kelsey Hightower</li> <li>https://medium.com/qubit-engineering/kubernetes-up-integrated-secrets-configuration-5a15b9f5a6c6</li> </ul>"},{"location":"kubernetes/#security","title":"Security","text":"<ul> <li>RBAC</li> <li>11 ways not to get hacked on Kubernetes</li> <li>https://www.youtube.com/channel/UCiqnRXPAAk6iv2m47odUFzw</li> </ul>"},{"location":"kubernetes/#tools-to-use","title":"Tools to use","text":"<ul> <li>Microscanner</li> <li>Dex - OpenID Connect solution</li> <li>Sonubuoy</li> <li>Helm<ul> <li>Learn LUA book, lua's required for Helm 3.0</li> </ul> </li> <li>ChartMuseum</li> <li>Skaffold</li> <li>KSynch</li> <li>Traefik</li> <li>Istio</li> <li>Falco</li> <li>Prometheus</li> <li>Grafana</li> <li>Jenkins</li> <li>Kaniko</li> <li>Prow</li> <li>Tarmak</li> <li>Kube-Lego</li> <li>Knative</li> <li>Rook</li> <li>Stern - aggregate log rendering tool</li> <li>Linkerd 2</li> </ul>"},{"location":"kubernetes/cka-exam-prep/","title":"CKA Exam Prep","text":"<ul> <li>https://github.com/dgkanatsios/CKAD-exercises</li> <li>https://github.com/kelseyhightower/kubernetes-the-hard-way</li> <li>https://github.com/walidshaari/Kubernetes-Certified-Administrator</li> <li>https://github.com/kubernetes/community/blob/master/contributors/devel/e2e-tests.md</li> <li>https://www.cncf.io/certification/cka/</li> <li>https://oscon2018.container.training</li> <li>https://github.com/ahmetb/kubernetes-network-policy-recipes</li> <li>https://github.com/ramitsurana/awesome-kubernetes</li> <li>https://sysdig.com/blog/kubernetes-security-guide/</li> <li>https://severalnines.com/blog/installing-kubernetes-cluster-minions-centos7-manage-pods-services</li> <li>https://docs.google.com/presentation/d/1Gp-2blk5WExI_QR59EUZdwfO2BWLJqa626mK2ej-huo/edit#slide=id.g27a78b354c_0_0</li> <li>https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html</li> </ul>"},{"location":"kubernetes/cka-exam-prep/#some-basic-commands","title":"Some basic commands","text":"<pre><code>kubectl -n kube-public get secrets\n</code></pre>"},{"location":"kubernetes/cka-exam-prep/#test-network-policy","title":"Test network policy","text":"<p>For some common recipes, look at Ahmet's recipe repository.</p> <p>Warning</p> <p>Make sure you have CNI enabled and you have a network plugin that enforces the policies.</p> <p>Note</p> <p>You can check current existing policies like this: <code>kubectl get netpol --all-namespaces</code></p>"},{"location":"kubernetes/cka-exam-prep/#example-ingress-policy","title":"Example Ingress Policy","text":"<pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: dui-network-policy\n  namespace: dui\nspec:\n  podSelector:\n    matchLabels:\n      app: dui\n      distribution: server\n  ingress: []\n</code></pre>"},{"location":"kubernetes/cka-exam-prep/#run-test-pod","title":"Run test pod","text":"<p>Apply above network policy, and then test in the same <code>dui</code> namespace, and in the <code>default</code> namespace.</p> <p>Note</p> <p>Use <code>alpine:3.6</code> because telnet was dropped starting 3.7.</p> <pre><code>kubectl -n dui get pods -l app=dui -o wide\nkubectl run --rm -i -t --image=alpine:3.6 -n dui test -- sh\ntelnet 10.32.0.7 8888\n</code></pre> <p>This should now fail - timeout - due the packages being dropped.</p>"},{"location":"kubernetes/cka-exam-prep/#egress","title":"Egress","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: dui-network-policy-egress\n  namespace: dui\nspec:\n  podSelector:\n    matchLabels:\n      app: dui\n  policyTypes:\n  - Egress\n  egress:\n  - ports:\n    - port: 7777\n      protocol: TCP\n  - to:\n    - podSelector:\n        matchLabels:\n            app: dui\n</code></pre> <p>Warning</p> <p>This should in theory, block our test pod from reading this. As it doesn't have the label <code>app=dui</code>. But it seems it is working just fine.</p>"},{"location":"kubernetes/cka-exam-prep/#allow-dns","title":"Allow DNS","text":"<p>If it should also be able to do DNS calls, we have to enable port 53.</p> <pre><code>  - ports:\n    - port: 53\n      protocol: UDP\n    - port: 53\n      protocol: TCP\n    - port: 7777\n      protocol: TCP\n  - to:\n    - namespaceSelector: {}\n</code></pre>"},{"location":"kubernetes/cka-exam-prep/#create-a-test-pod-with-curl","title":"Create a test pod with curl","text":"<pre><code>kubectl run --rm -i -t --image=alpine:3.6 -n dui test -- sh\napk --no-cache add curl\ncurl 10.32.0.11:7777/servers\n</code></pre>"},{"location":"kubernetes/cka-exam-prep/#run-minikube-cluster","title":"Run minikube cluster","text":"<pre><code>######################\n# Create The Cluster #\n######################\n\n# Make sure that your minikube version is v0.25 or higher\n\n# WARNING!!!\n# Some users experienced problems starting the cluster with minikuber v0.26 and v0.27.\n# A few of the reported issues are https://github.com/kubernetes/minikube/issues/2707 and https://github.com/kubernetes/minikube/issues/2703\n# If you are experiencing problems creating a cluster, please consider downgrading to minikube v0.25.\n\nminikube start \\\n    --vm-driver virtualbox \\\n    --cpus 4 \\\n    --memory 12228 \\\n    --network-plugin=cni \\\n    --extra-config=kubelet.network-plugin=cni\n\n###############################\n# Install Ingress and Storage #\n###############################\n\nminikube addons enable ingress\n\nminikube addons enable storage-provisioner\n\nminikube addons enable default-storageclass\n\n##################\n# Install Tiller #\n##################\n\nkubectl create \\\n    -f https://raw.githubusercontent.com/vfarcic/k8s-specs/master/helm/tiller-rbac.yml \\\n    --record --save-config\n\nhelm init --service-account tiller\n\nkubectl -n kube-system \\\n    rollout status deploy tiller-deploy\n\n##################\n# Get Cluster IP #\n##################\n\nexport LB_IP=$(minikube ip)\n\n#######################\n# Install ChartMuseum #\n#######################\n\nCM_ADDR=\"cm.$LB_IP.nip.io\"\n\necho $CM_ADDR\n\nCM_ADDR_ESC=$(echo $CM_ADDR \\\n    | sed -e \"s@\\.@\\\\\\.@g\")\n\necho $CM_ADDR_ESC\n\nhelm install stable/chartmuseum \\\n    --namespace charts \\\n    --name cm \\\n    --values helm/chartmuseum-values.yml \\\n    --set ingress.hosts.\"$CM_ADDR_ESC\"={\"/\"} \\\n    --set env.secret.BASIC_AUTH_USER=admin \\\n    --set env.secret.BASIC_AUTH_PASS=admin\n\nkubectl -n charts \\\n    rollout status deploy \\\n    cm-chartmuseum\n\n# http \"http://$CM_ADDR/health\" # It should return `{\"healthy\":true}\n\n######################\n# Install Weave Net ##\n######################\n\nkubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\"\nkubectl -n kube-system rollout status daemonset weave-net\n</code></pre>"},{"location":"kubernetes/cka-exam-prep/#weave-net","title":"Weave Net","text":""},{"location":"kubernetes/cka-exam-prep/#on-minikube","title":"On minikube","text":"<p>To run Weave Net on minikube, after upgrading minikube, you need to overwrite the default CNI config shipped with minikube: mkdir -p ~/.minikube/files/etc/cni/net.d/ &amp;&amp; touch ~/.minikube/files/etc/cni.net.d/k8s.conf and then to start minikube with CNI enabled: minikube start --network-plugin=cni --extra-config=kubelet.network-plugin=cni. Afterwards, you can install Weave Net.</p> <pre><code>kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\"\n</code></pre>"},{"location":"kubernetes/cka-exam-prep/#install-stern","title":"Install stern","text":"<ul> <li>Stern - aggregate log rendering tool</li> </ul>"},{"location":"kubernetes/cka-exam-prep/#via-brew","title":"via brew","text":"<pre><code>brew install stern\n</code></pre>"},{"location":"kubernetes/cka-exam-prep/#binary-release","title":"Binary release","text":"<pre><code>sudo curl -L -o /usr/local/bin/stern \\\n   https://github.com/wercker/stern/releases/download/1.6.0/stern_linux_amd64\nsudo chmod +x /usr/local/bin/stern\n</code></pre>"},{"location":"kubernetes/cka-exam-prep/#sysdig","title":"Sysdig","text":""},{"location":"kubernetes/cka-exam-prep/#install-sysdig","title":"Install Sysdig","text":""},{"location":"kubernetes/cka-exam-prep/#run-sysdig-for-kubernetes","title":"Run Sysdig for Kubernetes","text":"<ul> <li>collect API server address</li> <li>collect client cert + key</li> <li>https://www.digitalocean.com/community/tutorials/how-to-monitor-your-ubuntu-16-04-system-with-sysdig</li> </ul> <pre><code>certificate-authority: /home/joostvdg/.minikube/ca.crt\nserver: https://192.168.99.100:8443\nclient-certificate: /home/joostvdg/.minikube/client.crt\nclient-key: /home/joostvdg/.minikube/client.key\n</code></pre> <pre><code>sysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key\n\nsysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key syslog.severity.str=info\n</code></pre>"},{"location":"kubernetes/cka-exam-prep/#csysdig","title":"CSysdig","text":"<pre><code>sudo csysdig -k https://192.168.99.100:8443 -K /home/joostvdg/.minikube/client.crt:/home/joostvdg/.minikube/client.key\n</code></pre>"},{"location":"kubernetes/cka-exam-prep/#from-udemy-course","title":"From Udemy Course","text":""},{"location":"kubernetes/cka-exam/","title":"Certified Kubernetes Administrator Exam","text":""},{"location":"kubernetes/dev-platform/","title":"Kubernetes As Developer Platform","text":""},{"location":"kubernetes/dev-platform/#resources","title":"Resources","text":"<ul> <li>https://medium.com/@jpcontad/a-year-of-running-kubernetes-as-a-product-7eed1204eecd</li> </ul>"},{"location":"kubernetes/observability/","title":"Kubernetes Observability","text":""},{"location":"kubernetes/observability/#monitoring","title":"Monitoring","text":""},{"location":"kubernetes/observability/#metrics-server","title":"Metrics Server","text":"<ul> <li>Helm chart: https://github.com/helm/charts/tree/master/stable/metrics-server</li> <li>Home: https://github.com/kubernetes-incubator/metrics-server</li> </ul> <pre><code>helm install stable/metrics-server \\\n    --name metrics-server \\\n    --version 2.0.3 \\\n    --namespace metrics\n\nkubectl -n metrics \\\n    rollout status \\\n    deployment metrics-server\n</code></pre>"},{"location":"kubernetes/observability/#prometheus-alert-manager","title":"Prometheus &amp; Alert Manager","text":""},{"location":"kubernetes/observability/#prometheus-helm-values","title":"Prometheus Helm Values","text":"<pre><code>server:\n  ingress:\n    enabled: true\n    annotations:\n      ingress.kubernetes.io/ssl-redirect: \"false\"\n      nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n  resources:\n    limits:\n      cpu: 100m\n      memory: 1000Mi\n    requests:\n      cpu: 10m\n      memory: 500Mi\nalertmanager:\n  ingress:\n    enabled: true\n    annotations:\n      ingress.kubernetes.io/ssl-redirect: \"false\"\n      nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n  resources:\n    limits:\n      cpu: 10m\n      memory: 20Mi\n    requests:\n      cpu: 5m\n      memory: 10Mi\nkubeStateMetrics:\n  resources:\n    limits:\n      cpu: 10m\n      memory: 50Mi\n    requests:\n      cpu: 5m\n      memory: 25Mi\nnodeExporter:\n  resources:\n    limits:\n      cpu: 10m\n      memory: 20Mi\n    requests:\n      cpu: 5m\n      memory: 10Mi\npushgateway:\n  resources:\n    limits:\n      cpu: 10m\n      memory: 20Mi\n    requests:\n      cpu: 5m\n      memory: 10Mi\n</code></pre>"},{"location":"kubernetes/observability/#grafana","title":"Grafana","text":""},{"location":"kubernetes/observability/#application-metrics","title":"Application Metrics","text":""},{"location":"kubernetes/observability/#resources","title":"Resources","text":"<ul> <li>https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-b190cc97f0f6</li> <li>https://brancz.com/2018/01/05/prometheus-vs-heapster-vs-kubernetes-metrics-apis/</li> <li>https://rancher.com/blog/2018/2018-06-26-measuring-metrics-that-matter-in-kubernetes-clusters/</li> </ul>"},{"location":"kubernetes/tools/","title":"Kubernetes Tools","text":""},{"location":"kubernetes/tools/#helm","title":"Helm","text":"<p>We use Helm as a package manager to more easily install other tools on Kubernetes.</p> <p>There's several repositories with a large number of mature charts - the name of the Helm packages.</p> <p>One being Helm/Stable another being Helm Hub.</p>"},{"location":"kubernetes/tools/#install","title":"Install","text":"<p>```bash tab=\"MacOS/Homebrew\" brew install kubernetes-helm <pre><code>```bash tab=\"Windows/Chocolatey\"\nhoco install kubernetes-helm\n</code></pre></p> <p>```bash tab=\"Ubuntu/Snap\" sudo snap install helm --classic <pre><code>```bash tab=\"Sccop\"\nscoop install helm\n</code></pre></p> <p>```bash tab=\"GoFish\" gofish install helm <pre><code>### Usage\n\n```bash\nhelm install stable/jenkins\n</code></pre></p>"},{"location":"kubernetes/tools/#kubecontext","title":"Kubecontext","text":"<p>Kubectx is a utility to manage and switch between Kubernetes (<code>kubectl</code>) contexts and namespaces (via <code>kubens</code>, see below).</p>"},{"location":"kubernetes/tools/#install_1","title":"Install","text":"<p>```bash tab=\"MacOS/Homebrew\" brew install kubectx <pre><code>```bash tab=\"Ubuntu\"\nsudo apt install kubectx\n</code></pre></p>"},{"location":"kubernetes/tools/#usage","title":"Usage","text":""},{"location":"kubernetes/tools/#kubectx","title":"Kubectx","text":"<pre><code>kubectx minikube\nSwitched to context \"minikube\".\n\n$ kubectx -\nSwitched to context \"oregon\".\n\n$ kubectx -\nSwitched to context \"minikube\".\n\n$ kubectx dublin=gke_ahmetb_europe-west1-b_dublin\nContext \"dublin\" set.\nAliased \"gke_ahmetb_europe-west1-b_dublin\" as \"dublin\".\n</code></pre>"},{"location":"kubernetes/tools/#kubens","title":"Kubens","text":"<p>Kubens (part of Kubectx) helps you manage your current Kubernetes namespace.</p> <pre><code>$ kubens kube-system\nContext \"test\" set.\nActive namespace is \"kube-system\".\n\n$ kubens -\nContext \"test\" set.\nActive namespace is \"default\".\n</code></pre>"},{"location":"kubernetes/tools/#kuard","title":"Kuard","text":"<p>Kuard is a small demo application to show your cluster works. Also exposes some info you might want to see.</p> <pre><code>kubectl run --restart=Never --image=gcr.io/kuar-demo/kuard-amd64:blue kuard\nkubectl port-forward kuard 8080:8080\n</code></pre> <p>Open your browser to http://localhost:8080.</p>"},{"location":"kubernetes/tools/#stern","title":"Stern","text":"<p>Stern allows you to <code>tail</code> multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging.</p> <pre><code>brew install stern\n</code></pre>"},{"location":"kubernetes/tools/#usage_1","title":"Usage","text":"<p>Imagine a build in Jenkins using more than one container in the Pod. You want to tail the logs of all containers... you can with stern.</p> <pre><code>stern maven-\n</code></pre>"},{"location":"kubernetes/tools/#kube-capacity","title":"Kube Capacity","text":"<p>Kube Capacity is a simple CLI that provides an overview of the resource requests, limits, and utilization in a Kubernetes cluster.</p> <pre><code>brew tap robscott/tap\nbrew install robscott/tap/kube-capacity\n</code></pre> <pre><code>kube-capacity\n</code></pre> <pre><code>NODE              CPU REQUESTS    CPU LIMITS    MEMORY REQUESTS    MEMORY LIMITS\n*                 560m (28%)      130m (7%)     572Mi (9%)         770Mi (13%)\nexample-node-1    220m (22%)      10m (1%)      192Mi (6%)         360Mi (12%)\nexample-node-2    340m (34%)      120m (12%)    380Mi (13%)        410Mi (14%)\n</code></pre> <pre><code>kube-capacity --pods\n</code></pre> <pre><code>NODE              NAMESPACE     POD                   CPU REQUESTS    CPU LIMITS    MEMORY REQUESTS    MEMORY LIMITS\n*                 *             *                     560m (28%)      780m (38%)    572Mi (9%)         770Mi (13%)\n\nexample-node-1    *             *                     220m (22%)      320m (32%)    192Mi (6%)         360Mi (12%)\nexample-node-1    kube-system   metrics-server-lwc6z  100m (10%)      200m (20%)    100Mi (3%)         200Mi (7%)\nexample-node-1    kube-system   coredns-7b5bcb98f8    120m (12%)      120m (12%)    92Mi (3%)          160Mi (5%)\n\nexample-node-2    *             *                     340m (34%)      460m (46%)    380Mi (13%)        410Mi (14%)\nexample-node-2    kube-system   kube-proxy-3ki7       200m (20%)      280m (28%)    210Mi (7%)         210Mi (7%)\nexample-node-2    tiller        tiller-deploy         140m (14%)      180m (18%)    170Mi (5%)         200Mi (7%)\n</code></pre> <pre><code>kube-capacity --util\n</code></pre> <pre><code>NODE              CPU REQUESTS    CPU LIMITS    CPU UTIL    MEMORY REQUESTS    MEMORY LIMITS   MEMORY UTIL\n*                 560m (28%)      130m (7%)     40m (2%)    572Mi (9%)         770Mi (13%)     470Mi (8%)\nexample-node-1    220m (22%)      10m (1%)      10m (1%)    192Mi (6%)         360Mi (12%)     210Mi (7%)\nexample-node-2    340m (34%)      120m (12%)    30m (3%)    380Mi (13%)        410Mi (14%)     260Mi (9%)\n</code></pre> <pre><code>kube-capacity --pods --util\n</code></pre>"},{"location":"kubernetes/tools/#velero","title":"Velero","text":"<p>Velero </p>"},{"location":"kubernetes/tools/#rbac-lookup","title":"RBAC Lookup","text":"<p>RBAC Lookup </p>"},{"location":"kubernetes/tools/#install_2","title":"Install","text":"<p>``` bash tab=\"bash\" brew install reactiveops/tap/rbac-lookup <pre><code>``` bash tab=\"Krew\"\nkubectl krew install rbac-lookup\n</code></pre></p>"},{"location":"kubernetes/tools/#lookup-user","title":"Lookup user","text":"<pre><code>rbac-lookup jvandergriendt -owide\n</code></pre>"},{"location":"kubernetes/tools/#lookup-gke-user","title":"Lookup GKE user","text":"<pre><code>rbac-lookup jvandergriendt  --gke\n</code></pre>"},{"location":"kubernetes/tools/#k9s","title":"K9S","text":"<p>K9S is a tool that gives you a console UI on your kubernetes cluster/namespace.</p>"},{"location":"kubernetes/tools/#install_3","title":"Install","text":"<pre><code>brew tap derailed/k9s &amp;&amp; brew install k9s\n</code></pre>"},{"location":"kubernetes/tools/#use","title":"Use","text":"<p>By default is looks at a single namespace, and allows you to view elements of the pods running.</p> <pre><code>k9s -n cje\n</code></pre>"},{"location":"kubernetes/tools/#k9s_1","title":"K9S","text":"<p>K9S is a tool that gives you a console UI on your kubernetes cluster/namespace.</p>"},{"location":"kubernetes/tools/#install_4","title":"Install","text":"<pre><code>brew tap derailed/k9s &amp;&amp; brew install k9s\n</code></pre>"},{"location":"kubernetes/tools/#use_1","title":"Use","text":"<p>By default is looks at a single namespace, and allows you to view elements of the pods running.</p> <pre><code>k9s -n cje\n</code></pre>"},{"location":"kubernetes/tools/#dive","title":"Dive","text":"<p>A tool for exploring a docker image, layer contents, and discovering ways to shrink your Docker image size.</p> <p>Dive is a tool for analyzing Docker images.</p>"},{"location":"kubernetes/tools/#install_5","title":"Install","text":"<p>```bash tab=\"Debian based\" wget https://github.com/wagoodman/dive/releases/download/v0.7.1/dive_0.7.1_linux_amd64.deb sudo apt install ./dive_0.7.1_linux_amd64.deb <pre><code>```bash tab=\"RHEL based\"\ncurl -OL https://github.com/wagoodman/dive/releases/download/v0.7.1/dive_0.7.1_linux_amd64.rpm\nrpm -i dive_0.7.1_linux_amd64.rpm\n</code></pre></p> <p>```bash tab=\"Homebrew\" brew tap wagoodman/dive brew install dive <pre><code>```bash tab=\"Windows\"\ngo get github.com/wagoodman/dive\n</code></pre></p>"},{"location":"kubernetes/tools/#use_2","title":"Use","text":"<p>```bash tab=\"Existing image\" dive  <pre><code>```bash tab=\"To be build image\"\ndive build -t &lt;some-tag&gt; .\n</code></pre> <p><code>bash tab=\"For CI builds\" CI=true dive &lt;your-image&gt;</code></p>"},{"location":"kubernetes/tools/#kiali","title":"Kiali","text":"<p>https://www.kiali.io/</p>"},{"location":"kubernetes/tools/#telepresence","title":"Telepresence","text":"<p>https://www.telepresence.io/</p>"},{"location":"kubernetes/cicd/cdp-jenkins-helm/","title":"CD Pipeline with Jenkins &amp; Helm","text":""},{"location":"kubernetes/cicd/cdp-jenkins-helm/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.9.x+ cluster</li> <li>Valid domain names</li> <li>Jenkins 2.x+ with pipeline plugins below</li> <li>Helm/Tiller</li> </ul>"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#tools","title":"Tools","text":"<ul> <li>Jenkins 2.x</li> </ul>"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#install-helm","title":"Install Helm","text":"<p>For more information, checkout the github page.</p> <p>Helm's current version (as of October 2018) - version 2 - consists of two parts. One is a local client - Helm - which you should install on your own machine, see here for how.</p> <p>The other is a server component part - Tiller - that should be installed in your Kubernetes cluster.</p>"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#install-tiller","title":"Install Tiller","text":"<pre><code>kubectl create serviceaccount tiller --namespace kube-system\n</code></pre> <ul> <li>create rbac config: rbac-config.yaml</li> </ul> <pre><code>apiVersion: v1\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: tiller-role-binding\nroleRef:\n  kind: ClusterRole\n  name: cluster-admin\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\n  name: tiller\n  namespace: kube-system\n</code></pre> <pre><code>kubectl apply -f rbac-config.yaml\nhelm init --service-account tiller\n</code></pre>"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#install-nginx-helm-chart","title":"install nginx helm chart","text":"<pre><code>helm install stable/nginx-ingress\n</code></pre>"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#jenkins-plugins","title":"Jenkins Plugins","text":"<ul> <li>Warnings Plugin: https://github.com/jenkinsci/warnings-plugin/blob/master/doc/Documentation.md</li> <li>Anchore: https://jenkins.io/blog/2018/06/20/anchore-image-scanning/</li> </ul>"},{"location":"kubernetes/cicd/cdp-jenkins-helm/#anchore","title":"Anchore","text":"<ul> <li>https://github.com/anchore/anchore-engine</li> <li>https://github.com/helm/charts/tree/master/stable/anchore-engine</li> <li>https://wiki.jenkins.io/display/JENKINS/Anchore+Container+Image+Scanner+Plugin</li> </ul>"},{"location":"kubernetes/distributions/","title":"Kubernetes","text":""},{"location":"kubernetes/distributions/#what-is-kubernetes","title":"What is kubernetes","text":""},{"location":"kubernetes/distributions/#kubernetes-objects","title":"Kubernetes Objects","text":""},{"location":"kubernetes/distributions/#kubernetes-tutorials","title":"Kubernetes tutorials","text":""},{"location":"kubernetes/distributions/#kubernetes-guides","title":"Kubernetes Guides","text":""},{"location":"kubernetes/distributions/#linux-basics","title":"Linux basics","text":""},{"location":"kubernetes/distributions/#namespaces-cgroups","title":"Namespaces &amp; CGroups","text":"<ul> <li>https://jvns.ca/blog/2016/10/10/what-even-is-a-container/</li> <li>https://www.youtube.com/watch?v=sK5i-N34im8</li> <li>https://www.ianlewis.org/en/what-are-kubernetes-pods-anyway</li> </ul>"},{"location":"kubernetes/distributions/#networking","title":"Networking","text":"<ul> <li>https://itnext.io/kubernetes-networking-behind-the-scenes-39a1ab1792bb</li> <li>https://github.com/nleiva/kubernetes-networking-links</li> <li>IP Tables</li> <li>CIDR Explanation video</li> <li>Packets &amp; Frames introduction</li> </ul>"},{"location":"kubernetes/distributions/#ingress","title":"Ingress","text":"<ul> <li>Traefik on AWS</li> </ul>"},{"location":"kubernetes/distributions/#metrics","title":"Metrics","text":"<ul> <li>https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-66936addedae</li> </ul>"},{"location":"kubernetes/distributions/#secrets","title":"Secrets","text":"<ul> <li>Hashicorp Vault - Kelsey Hightower</li> <li>https://medium.com/qubit-engineering/kubernetes-up-integrated-secrets-configuration-5a15b9f5a6c6</li> </ul>"},{"location":"kubernetes/distributions/#security","title":"Security","text":"<ul> <li>RBAC</li> <li>11 ways not to get hacked on Kubernetes</li> <li>https://www.youtube.com/channel/UCiqnRXPAAk6iv2m47odUFzw</li> </ul>"},{"location":"kubernetes/distributions/#tools-to-use","title":"Tools to use","text":"<ul> <li>Microscanner</li> <li>Dex - OpenID Connect solution</li> <li>Sonubuoy</li> <li>Helm<ul> <li>Learn LUA book, lua's required for Helm 3.0</li> </ul> </li> <li>ChartMuseum</li> <li>Skaffold</li> <li>KSynch</li> <li>Traefik</li> <li>Istio</li> <li>Falco</li> <li>Prometheus</li> <li>Grafana</li> <li>Jenkins</li> <li>Kaniko</li> <li>Prow</li> <li>Tarmak</li> <li>Kube-Lego</li> <li>Knative</li> <li>Rook</li> <li>Stern - aggregate log rendering tool</li> <li>Linkerd 2</li> </ul>"},{"location":"kubernetes/distributions/aks-cli/","title":"Azure CLI","text":""},{"location":"kubernetes/distributions/aks-cli/#configure-az-cli","title":"Configure AZ CLI","text":"<pre><code>az login\n</code></pre> <pre><code>az account show --query \"{subscriptionId:id, tenantId:tenantId}\"\n</code></pre> <pre><code>export SUBSCRIPTION_ID=\n</code></pre> <pre><code>SUBSCRIPTION_ID=...\naz account set --subscription=\"${SUBSCRIPTION_ID}\"\n</code></pre> <pre><code>az ad sp create-for-rbac --role=\"Owner\" --scopes=\"/subscriptions/${SUBSCRIPTION_ID}\"\n</code></pre>"},{"location":"kubernetes/distributions/aks-cli/#should-be-owner","title":"Should be owner","text":"<p>Should be owner, else it cannot create a LoadBalancer via the <code>nginx-ingress</code>.</p> <ul> <li>See: https://weidongzhou.wordpress.com/2018/06/27/could-not-get-external-ip-for-load-balancer-on-azure-aks/</li> <li>And: https://github.com/Azure/AKS/issues/427</li> </ul>"},{"location":"kubernetes/distributions/aks-cli/#configure-kubecontext","title":"Configure Kubecontext","text":"<pre><code>az aks get-credentials --resource-group cbcore --name cbcore\n</code></pre>"},{"location":"kubernetes/distributions/aks-cli/#configure-cluster-autoscaler","title":"Configure Cluster Autoscaler","text":"<pre><code>az extension add --name aks-preview\n</code></pre> <pre><code>az feature register --name VMSSPreview --namespace Microsoft.ContainerService\naz feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService/VMSSPreview')].{Name:name,State:properties.state}\"\naz provider register --namespace Microsoft.ContainerService\naz feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService')].{Name:name,State:properties.state}\"\n</code></pre>"},{"location":"kubernetes/distributions/aks-cli/#configure-multi-node-pool","title":"Configure multi-node pool","text":"<pre><code>az feature register --name MultiAgentpoolPreview --namespace Microsoft.ContainerService\naz feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService/MultiAgentpoolPreview')].{Name:name,State:properties.state}\"\n</code></pre> <pre><code>az provider register --namespace Microsoft.ContainerService\n</code></pre>"},{"location":"kubernetes/distributions/aks-cli/#create-aks-cluster-via-cli","title":"Create AKS cluster via CLI","text":""},{"location":"kubernetes/distributions/aks-cli/#resources","title":"Resources","text":"<ul> <li>https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md</li> <li>https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler</li> <li>https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/</li> </ul>"},{"location":"kubernetes/distributions/aks-cli/#get-available-versions","title":"Get available versions","text":"<pre><code>az aks get-versions --location westeurope\n</code></pre>"},{"location":"kubernetes/distributions/aks-cli/#create-initial-cluster","title":"Create initial cluster","text":""},{"location":"kubernetes/distributions/aks-cli/#prepare-variables","title":"Prepare variables","text":"<pre><code>RESOURCE_GROUP_NAME=\nCLUSTER_NAME=\nLOCATION=eastus\nNODE_POOL_MASTERS=masters\nNODE_POOL_BUILDS=builds\nVM_SIZE_MASTERS_NP=Standard_DS2_v2\nVM_SIZE_BUILDS_NP=?\n</code></pre>"},{"location":"kubernetes/distributions/aks-cli/#create-resource-group","title":"Create Resource Group","text":"<pre><code>az group create --name ${RESOURCE_GROUP_NAME} --location ${LOCATION}\n</code></pre>"},{"location":"kubernetes/distributions/aks-cli/#create-aks-cluster","title":"Create AKS Cluster","text":"<pre><code>az aks create \\\n    --resource-group ${RESOURCE_GROUP_NAME} \\\n    --name ${CLUSTER_NAME} \\\n    --enable-vmss \\\n    --node-count 1 \\\n    --nodepool-name ${NODE_POOL_MASTERS} \\\n    --node-vm-size ${VM_SIZE_MASTERS_NP} \\\n    --enable-cluster-autoscaler \\\n    --enable-vmss \\\n    --generate-ssh-keys\n</code></pre>"},{"location":"kubernetes/distributions/aks-cli/#podsecuritypolicy","title":"PodSecurityPolicy","text":"<pre><code>--enable-cluster-autoscaler    : Enable cluster autoscaler, default value is false.\n    If specified, please make sure the kubernetes version is larger than 1.10.6.\n</code></pre>"},{"location":"kubernetes/distributions/aks-cli/#networking","title":"Networking","text":"<pre><code>--network-plugin               : The Kubernetes network plugin to use.\n    Specify \"azure\" for advanced networking configurations. Defaults to \"kubenet\".\n--network-policy               : (PREVIEW) The Kubernetes network policy to use.\n    Using together with \"azure\" network plugin.\n    Specify \"azure\" for Azure network policy manager and \"calico\" for calico network policy\n    controller.\n    Defaults to \"\" (network policy disabled).\n</code></pre>"},{"location":"kubernetes/distributions/aks-cli/#retrieve-credentials","title":"Retrieve credentials","text":"<pre><code>az aks get-credentials --resource-group ${RESOURCE_GROUP_NAME} --name ${CLUSTER_NAME}\n</code></pre>"},{"location":"kubernetes/distributions/aks-cli/#add-second-node-pool","title":"Add second node pool","text":"<pre><code>az aks nodepool add \\\n    --resource-group ${RESOURCE_GROUP_NAME} \\\n    --cluster-name ${CLUSTER_NAME} \\\n    --name ${NODE_POOL_BUILDS} \\\n    --node-count 3\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/","title":"AKS Terraform","text":""},{"location":"kubernetes/distributions/aks-terraform/#resources","title":"Resources","text":"<ul> <li>https://docs.microsoft.com/en-us/azure/terraform/terraform-create-k8s-cluster-with-tf-and-aks</li> <li>https://www.terraform.io/docs/providers/azurerm/r/kubernetes_cluster.html</li> </ul>"},{"location":"kubernetes/distributions/aks-terraform/#pre-requisites","title":"Pre-Requisites","text":""},{"location":"kubernetes/distributions/aks-terraform/#create-service-principle","title":"Create Service Principle","text":"<p>It comes from this guide.</p> <pre><code>az account show --query \"{subscriptionId:id, tenantId:tenantId}\"\n</code></pre> <pre><code>az ad sp create-for-rbac --role=\"Owner\" --scopes=\"/subscriptions/${SUBSCRIPTION_ID}\"\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/#retrieve-current-kubernetes-versions","title":"Retrieve current Kubernetes Versions","text":"<pre><code>az aks get-versions --location westeurope --output table\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/#terraform-config","title":"Terraform Config","text":"<pre><code>ARM_SUBSCRIPTION_ID=\nARM_CLIENT_ID=\nARM_CLIENT_SECRET=\nARM_TENANT_ID=\nARM_ENVIRONMENT=\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/#create-storage-account-for-tf-state","title":"Create storage account for TF State","text":"<pre><code>LOCATION=westeurope\nRESOURCE_GROUP_NAME=joostvdg-cb-ext-storage\nSTORAGE_ACCOUNT_NAME=joostvdgcbtfstate\nCONTAINER_NAME=tfstate\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/#list-locations","title":"List locations","text":"<pre><code>az account list-locations \\\n    --query \"[].{Region:name}\" \\\n    --out table\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/#create-resource-group","title":"Create resource group","text":"<pre><code>az group create \\\n    --name ${RESOURCE_GROUP_NAME} \\\n    --location ${LOCATION}\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/#create-storage-account","title":"Create storage account","text":"<pre><code>az storage account create \\\n    --name ${STORAGE_ACCOUNT_NAME} \\\n    --resource-group ${RESOURCE_GROUP_NAME} \\\n    --location ${LOCATION} \\\n    --sku Standard_ZRS \\\n    --kind StorageV2\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/#retrieve-storage-account-login","title":"Retrieve storage account login","text":"<p>Apparently, no CLI commands available?</p> <p>Use the Azure Blog on AKS via Terraform for how via the UI.</p> <pre><code>STORAGE_ACCOUNT_KEY=\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/#create-tf-storage","title":"Create TF Storage","text":"<pre><code>az storage container create -n ${CONTAINER_NAME} --account-name ${STORAGE_ACCOUNT_NAME} --account-key ${STORAGE_ACCOUNT_KEY}\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/#init-terraform-backend","title":"Init Terraform backend","text":"<pre><code>terraform init -backend-config=\"storage_account_name=${STORAGE_ACCOUNT_NAME}\" \\\n -backend-config=\"container_name=${CONTAINER_NAME}\" \\\n -backend-config=\"access_key=${STORAGE_ACCOUNT_KEY}\" \\\n -backend-config=\"key=codelab.microsoft.tfstate\"\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/#expose-temp-variables","title":"Expose temp variables","text":"<p>These are from your Service Principle we created earlier. Where <code>client_id</code> = appId, and <code>client_secret</code> the password.</p> <pre><code>export TF_VAR_client_id=&lt;your-client-id&gt;\nexport TF_VAR_client_secret=&lt;your-client-secret&gt;\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/#rollout","title":"Rollout","text":""},{"location":"kubernetes/distributions/aks-terraform/#set-variables","title":"Set variables","text":"<pre><code>source ../export-variables.sh\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/#validate","title":"Validate","text":"<pre><code>terraform validate\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/#plan","title":"Plan","text":"<pre><code>terraform plan -out out.plan\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/#apply-the-plan","title":"Apply the plan","text":"<pre><code>terraform apply out.plan\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/#get-kubectl-config","title":"Get kubectl config","text":"<pre><code>AKS_RESOURCE_GROUP=joostvdg-cbcore\nAKS_CLUSTER_NAME=acctestaks1\n</code></pre> <pre><code>az aks get-credentials --resource-group ${AKS_RESOURCE_GROUP} --name ${AKS_CLUSTER_NAME}\n</code></pre>"},{"location":"kubernetes/distributions/aks-terraform/#enable-preview-features","title":"Enable Preview Features","text":"<p>Currently having cluster autoscalers requires enabling of a Preview Feature in Azure.</p> <p>The same holds true for enabling multiple node pools, which I think is a best practice for using Kubernetes.</p> <ul> <li>Enable Multi Node Pool</li> <li>Enable Cluster Autoscaler - via VMScaleSets</li> </ul>"},{"location":"kubernetes/distributions/aks-terraform/#terraform-code","title":"Terraform Code","text":"<p>Important</p> <p>When using Terraform for AKS and you want to use Multiple Node Pools and/or the Cluster Autoscaler, you need to use the minimum of <code>1.32.0</code> of the <code>azurerm</code> provider.</p> main.tf <pre><code>provider \"azurerm\" {\n    # whilst the `version` attribute is optional, we recommend pinning to a given version of the Provider\n    version = \"~&gt; 1.32.0\"\n}\n\nterraform {\n    backend \"azurerm\" {}\n}\n</code></pre> k8s.tf <pre><code>resource \"azurerm_kubernetes_cluster\" \"k8s\" {\n    name                = \"acctestaks1\"\n    location            = \"${azurerm_resource_group.k8s.location}\"\n    resource_group_name = \"${azurerm_resource_group.k8s.name}\"\n    dns_prefix          = \"jvdg\"\n    kubernetes_version  = \"${var.kubernetes_version}\"\n\n    agent_pool_profile {\n        name            = \"default\"\n        vm_size         = \"Standard_D2s_v3\"\n        os_type         = \"Linux\"\n        os_disk_size_gb = 30\n        enable_auto_scaling = true\n        count = 2\n        min_count = 2\n        max_count = 3\n        type = \"VirtualMachineScaleSets\"\n        node_taints = [\"mytaint=true:NoSchedule\"]\n    }\n\n    agent_pool_profile {\n        name            = \"pool1\"\n        vm_size         = \"Standard_D2s_v3\"\n        os_type         = \"Linux\"\n        os_disk_size_gb = 30\n        enable_auto_scaling = true\n        min_count = 1\n        max_count = 3\n        type = \"VirtualMachineScaleSets\"\n    }\n\n    agent_pool_profile {\n        name            = \"pool2\"\n        vm_size         = \"Standard_D4s_v3\"\n        os_type         = \"Linux\"\n        os_disk_size_gb = 30\n        enable_auto_scaling = true\n        min_count = 1\n        max_count = 3\n        type = \"VirtualMachineScaleSets\"\n    }\n\n    role_based_access_control {\n        enabled = true\n    }\n\n    service_principal {\n        client_id     = \"${var.client_id}\"\n        client_secret = \"${var.client_secret}\"\n    }\n\n    tags = {\n        Environment = \"Development\"\n        CreatedBy = \"Joostvdg\"\n    }\n}\n\noutput \"client_certificate\" {\n    value = \"${azurerm_kubernetes_cluster.k8s.kube_config.0.client_certificate}\"\n}\n\noutput \"kube_config\" {\n    value = \"${azurerm_kubernetes_cluster.k8s.kube_config_raw}\"\n}\n</code></pre> variables.tf <pre><code>variable \"client_id\" {}\nvariable \"client_secret\" {}\n\nvariable \"kubernetes_version\" {\n    default = \"1.14.6\"\n}\n\nvariable \"agent_count\" {\n    default = 3\n}\n\nvariable \"ssh_public_key\" {\n    default = \"~/.ssh/id_rsa.pub\"\n}\n\nvariable \"dns_prefix\" {\n    default = \"jvdg\"\n}\n\nvariable cluster_name {\n    default = \"cbcore\"\n}\n\nvariable resource_group_name {\n    default = \"joostvdg-cbcore\"\n}\n\nvariable container_registry_name {\n    default = \"joostvdgacr\"\n}\n\nvariable location {\n    default = \"westeurope\"\n}\n</code></pre> acr.tf <pre><code>resource \"azurerm_resource_group\" \"ecr\" {\n    name     = \"${var.resource_group_name}-acr\"\n    location = \"${var.location}\"\n}\n\nresource \"azurerm_container_registry\" \"acr\" {\n    name                     = \"${var.container_registry_name}\"\n    resource_group_name      = \"${azurerm_resource_group.ecr.name}\"\n    location                 = \"${azurerm_resource_group.k8s.location}\"\n    sku                      = \"Premium\"\n    admin_enabled            = false\n}\n</code></pre> resource-group.tf <pre><code>resource \"azurerm_resource_group\" \"k8s\" {\n    name     = \"${var.resource_group_name}\"\n    location = \"${var.location}\"\n}\n</code></pre>"},{"location":"kubernetes/distributions/aks/","title":"Azure Kubernetes Service","text":""},{"location":"kubernetes/distributions/aks/#resources","title":"Resources","text":"<ul> <li>https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md</li> <li>https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/</li> <li>https://www.cloudbees.com/blog/securing-jenkins-role-based-access-control-and-azure-active-directory</li> <li>https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/aks-install/#</li> <li>https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/kubernetes-helm-install/#_additional_information_creating_a_tls_certificate</li> </ul>"},{"location":"kubernetes/distributions/aks/#configure-az-cli","title":"Configure AZ CLI","text":"<pre><code>az login\n</code></pre> <pre><code>az account show --query \"{subscriptionId:id, tenantId:tenantId}\"\n</code></pre> <pre><code>export SUBSCRIPTION_ID=\n</code></pre> <pre><code>SUBSCRIPTION_ID=...\naz account set --subscription=\"${SUBSCRIPTION_ID}\"\n</code></pre> <pre><code>az ad sp create-for-rbac --role=\"Owner\" --scopes=\"/subscriptions/${SUBSCRIPTION_ID}\"\n</code></pre>"},{"location":"kubernetes/distributions/aks/#should-be-owner","title":"Should be owner","text":"<p>Should be owner, else it cannot create a LoadBalancer via the <code>nginx-ingress</code>.</p> <p>See: https://weidongzhou.wordpress.com/2018/06/27/could-not-get-external-ip-for-load-balancer-on-azure-aks/ And: https://github.com/Azure/AKS/issues/427</p>"},{"location":"kubernetes/distributions/aks/#terraform-config","title":"Terraform Config","text":"<pre><code>ARM_SUBSCRIPTION_ID\nARM_CLIENT_ID\nARM_CLIENT_SECRET\nARM_TENANT_ID\nARM_ENVIRONMENT\n</code></pre>"},{"location":"kubernetes/distributions/aks/#create-storage-account-for-tf-state","title":"Create storage account for TF State","text":"<pre><code>LOCATION=westeurope\nRESOURCE_GROUP_NAME=joostvdg-cb-ext-storage\nSTORAGE_ACCOUNT_NAME=joostvdgcbtfstate\nCONTAINER_NAME=tfstate\n</code></pre>"},{"location":"kubernetes/distributions/aks/#list-locations","title":"List locations","text":"<pre><code>az account list-locations \\\n    --query \"[].{Region:name}\" \\\n    --out table\n</code></pre>"},{"location":"kubernetes/distributions/aks/#create-resource-group","title":"Create resource group","text":"<pre><code>az group create \\\n    --name ${RESOURCE_GROUP_NAME} \\\n    --location ${LOCATION}\n</code></pre>"},{"location":"kubernetes/distributions/aks/#create-storage-account","title":"Create storage account","text":"<pre><code>az storage account create \\\n    --name ${STORAGE_ACCOUNT_NAME} \\\n    --resource-group ${RESOURCE_GROUP_NAME} \\\n    --location ${LOCATION} \\\n    --sku Standard_ZRS \\\n    --kind StorageV2\n</code></pre>"},{"location":"kubernetes/distributions/aks/#retrieve-storage-account-login","title":"Retrieve storage account login","text":"<p>Apparently, no CLI commands available?</p> <p>Use the Azure Blog on AKS via Terraform for how via the UI.</p> <pre><code>STORAGE_ACCOUNT_KEY=\n</code></pre>"},{"location":"kubernetes/distributions/aks/#create-tf-storage","title":"Create TF Storage","text":"<pre><code>az storage container create -n ${CONTAINER_NAME} --account-name ${STORAGE_ACCOUNT_NAME} --account-key ${STORAGE_ACCOUNT_KEY}\n</code></pre>"},{"location":"kubernetes/distributions/aks/#init-terraform-backend","title":"Init Terraform backend","text":"<pre><code>terraform init -backend-config=\"storage_account_name=${STORAGE_ACCOUNT_NAME}\" \\\n -backend-config=\"container_name=${CONTAINER_NAME}\" \\\n -backend-config=\"access_key=${STORAGE_ACCOUNT_KEY}\" \\\n -backend-config=\"key=codelab.microsoft.tfstate\"\n</code></pre>"},{"location":"kubernetes/distributions/aks/#expose-temp-variables","title":"Expose temp variables","text":"<p>These are from your Service Principle we created earlier. Where <code>client_id</code> = appId, and <code>client_secret</code> the password.</p> <pre><code>export TF_VAR_client_id=&lt;your-client-id&gt;\nexport TF_VAR_client_secret=&lt;your-client-secret&gt;\n</code></pre>"},{"location":"kubernetes/distributions/aks/#rollout","title":"Rollout","text":""},{"location":"kubernetes/distributions/aks/#set-variables","title":"Set variables","text":"<pre><code>source ../export-variables.sh\n</code></pre>"},{"location":"kubernetes/distributions/aks/#validate","title":"Validate","text":"<pre><code>terraform validate\n</code></pre>"},{"location":"kubernetes/distributions/aks/#plan","title":"Plan","text":"<pre><code>terraform plan -out out.plan\n</code></pre>"},{"location":"kubernetes/distributions/aks/#apply-the-plan","title":"Apply the plan","text":"<pre><code>terraform apply out.plan\n</code></pre>"},{"location":"kubernetes/distributions/aks/#configure-kubecontext","title":"Configure Kubecontext","text":"<pre><code>az aks get-credentials --resource-group cbcore --name cbcore\n</code></pre>"},{"location":"kubernetes/distributions/aks/#configure-cluster-autoscaler","title":"Configure Cluster Autoscaler","text":"<pre><code>az extension add --name aks-preview\n</code></pre> <pre><code>az feature register --name VMSSPreview --namespace Microsoft.ContainerService\naz feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService/VMSSPreview')].{Name:name,State:properties.state}\"\naz provider register --namespace Microsoft.ContainerService\naz feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService')].{Name:name,State:properties.state}\"\n</code></pre>"},{"location":"kubernetes/distributions/aks/#configure-multi-node-pool","title":"Configure multi-node pool","text":"<pre><code>az feature register --name MultiAgentpoolPreview --namespace Microsoft.ContainerService\naz feature list -o table --query \"[?contains(name, 'Microsoft.ContainerService/MultiAgentpoolPreview')].{Name:name,State:properties.state}\"\n</code></pre> <pre><code>az provider register --namespace Microsoft.ContainerService\n</code></pre>"},{"location":"kubernetes/distributions/aks/#create-aks-cluster-via-cli","title":"Create AKS cluster via CLI","text":""},{"location":"kubernetes/distributions/aks/#resources_1","title":"Resources","text":"<ul> <li>https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/aks/use-multiple-node-pools.md</li> <li>https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler</li> <li>https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/</li> </ul>"},{"location":"kubernetes/distributions/aks/#get-available-versions","title":"Get available versions","text":"<pre><code>az aks get-versions --location westeurope\n</code></pre>"},{"location":"kubernetes/distributions/aks/#create-initial-cluster","title":"Create initial cluster","text":"<pre><code># Create a resource group in East US\naz group create --name myResourceGroup --location eastus\n\n# Create a basic single-node AKS cluster\naz aks create \\\n    --resource-group myResourceGroup \\\n    --name myAKSCluster \\\n    --enable-vmss \\\n    --node-count 1 \\\n    --nodepool-name masters \\\n    --node-vm-size Standard_DS2_v2 \\\n    --enable-cluster-autoscaler \\\n    --enable-vmss \\\n    --generate-ssh-keys\n</code></pre>"},{"location":"kubernetes/distributions/aks/#podsecuritypolicy","title":"PodSecurityPolicy","text":"<pre><code>--enable-cluster-autoscaler    : Enable cluster autoscaler, default value is false.\n    If specified, please make sure the kubernetes version is larger than 1.10.6.\n</code></pre>"},{"location":"kubernetes/distributions/aks/#networking","title":"Networking","text":"<pre><code>--network-plugin               : The Kubernetes network plugin to use.\n    Specify \"azure\" for advanced networking configurations. Defaults to \"kubenet\".\n--network-policy               : (PREVIEW) The Kubernetes network policy to use.\n    Using together with \"azure\" network plugin.\n    Specify \"azure\" for Azure network policy manager and \"calico\" for calico network policy\n    controller.\n    Defaults to \"\" (network policy disabled).\n</code></pre>"},{"location":"kubernetes/distributions/aks/#retrieve-credentials","title":"Retrieve credentials","text":"<pre><code>az aks get-credentials --resource-group myResourceGroup --name myAKSCluster\n</code></pre>"},{"location":"kubernetes/distributions/aks/#add-second-node-pool","title":"Add second node pool","text":"<pre><code>az aks nodepool add \\\n    --resource-group myResourceGroup \\\n    --cluster-name myAKSCluster \\\n    --name mynodepool \\\n    --node-count 3\n</code></pre>"},{"location":"kubernetes/distributions/eks-eksctl/","title":"AWS EKS via eksctl","text":""},{"location":"kubernetes/distributions/eks-eksctl/#eks-access-configuration","title":"EKS Access Configuration","text":"<p>Some reference configuration, this is assuming you need temporary access tokens based on a <code>assume role</code> while having a MFA device configured. I seemed to have to create a new token every X minutes. If you don't run into this, ignore the configuration below and go straight to creating the cluster.</p>"},{"location":"kubernetes/distributions/eks-eksctl/#eks-keys-config","title":"EKS Keys Config","text":"<pre><code>[cloudbees-eks]\naws_access_key_id = ASI...\naws_secret_access_key = NMAX...\naws_session_token =  FQoGZXIvYXdzEJr//////////wE..................... // one long ass token\n</code></pre>"},{"location":"kubernetes/distributions/eks-eksctl/#generate-temporary-access-tokens-with-mfa","title":"Generate Temporary Access Tokens With MFA","text":"<pre><code>keys=($(aws sts assume-role --profile default --role-arn arn:aws:iam::&lt;ROLE_ARN&gt;:role/&lt;ROLE_NAME&gt; \\\n  --role-session-name MyEKSCTLSession \\\n  --serial-number arn:aws:iam::&lt;MFA_ARN&gt;:mfa/&lt;USER&gt; \\\n  --token-code &lt;REPLACE_THIS_WITH_MFA_TOKEN&gt; \\\n  --query 'Credentials.[AccessKeyId,SecretAccessKey,SessionToken]' --output text))\n</code></pre>"},{"location":"kubernetes/distributions/eks-eksctl/#cluster-create","title":"Cluster Create","text":"<pre><code>EKS_CLUSTER_NAME=mycluster\nAWS_PROFILE=cloudbees-eks\nAWS_REGION=us-east-1\nAWS_SSH_KEY_LOCATION=\"~/.ssh/id_rsa.pub\"\nEKS_NUM_NODES=4\n</code></pre> <pre><code>eksctl create cluster \\\n    --asg-access \\\n    --auto-kubeconfig \\\n    --full-ecr-access \\\n    --name ${EKS_CLUSTER_NAME} \\\n    --profile ${AWS_PROFILE} \\\n    --region ${AWS_REGION} \\\n    --set-kubeconfig-context \\\n    --ssh-public-key ${AWS_SSH_KEY_LOCATION} \\\n    --nodes=${EKS_NUM_NODES} \\\n    --verbose 4\n</code></pre> <pre><code>alias eks=\"kubectl --kubeconfig=~/.kube/eksctl/clusters/mycluster\"\n</code></pre>"},{"location":"kubernetes/distributions/eks-eksctl/#encrypted-network-with-weavenet","title":"Encrypted Network With Weavenet","text":"<p>If you want your network to be encrypted, you can use Weavenet.</p> <p>Warning</p> <p>The price of the encrypted network is high. So you're probably better off with a Network Policy.</p> <pre><code>WEAVENET_PASS=vjStsrzC4q7xDnb1wZkYacnk\nIPALLOC_RANGE=10.10.0.0/24\n</code></pre> <pre><code>echo \"${WEAVENET_PASS}\" &gt; weave-passwd\neks create secret -n kube-system generic weave-passwd --from-file=weave-passwd\neks apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')&amp;password-secret=weave-passwd&amp;env.IPALLOC_RANGE=${IPALLOC_RANGE}\"\n</code></pre>"},{"location":"kubernetes/distributions/eks-eksctl/#helm-tiller","title":"Helm &amp; Tiller","text":"<pre><code>alias helmks=\"helm --kubeconfig=~/.kube/eksctl/clusters/mycluster\"\n</code></pre> <pre><code>eks create serviceaccount --namespace kube-system tiller\neks create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller\nhelmks init --service-account tiller --upgrade\n</code></pre>"},{"location":"kubernetes/distributions/eks-eksctl/#nginx","title":"Nginx","text":"<p>Nginx Ingress Docs, How to install on AWS</p> <p>CloudBees AWS Docs</p> <pre><code>eks apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml\neks apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/aws/service-l4.yaml\neks apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/aws/patch-configmap-l4.yaml\neks patch service ingress-nginx -p '{\"spec\":{\"externalTrafficPolicy\":\"Local\"}}' -n ingress-nginx\n</code></pre>"},{"location":"kubernetes/distributions/eks-eksctl/#certmanager","title":"Certmanager","text":"<pre><code>helmks install --name cert-manager --namespace default stable/cert-manager\n\necho \"apiVersion: certmanager.k8s.io/v1alpha1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prd\nspec:\n  acme:\n    # The ACME server URL\n    server: https://acme-v02.api.letsencrypt.org/directory\n    # Email address used for ACME registration\n    email: yourname@example.com\n    # Name of a secret used to store the ACME account private key\n    privateKeySecretRef:\n      name: letsencrypt-prd\n    # Enable the HTTP-01 challenge provider\n    http01: {}\" &gt; cluster-issuer.yml\n\neks apply -f cluster-issuer.yml\n</code></pre>"},{"location":"kubernetes/distributions/eks-eksctl/#storage-class","title":"Storage class","text":"<p>Create a <code>gp2</code> storage class and set it as default.</p> <pre><code>echo \"kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: gp2\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n  type: gp2\n  encrypted: \\\"true\\\"\" &gt; gp2-storage.yaml\n\neks patch storageclass gp2 -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\neks patch storageclass default -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}'\n</code></pre>"},{"location":"kubernetes/distributions/eks-eksctl/#confirm","title":"Confirm","text":"<p>Confirm the storage class is create and set as default.</p> <pre><code>kubectl get sc\n</code></pre> <p>Expected result.</p> <pre><code>NAME            PROVISIONER             AGE\ngp2 (default)   kubernetes.io/aws-ebs   59\n</code></pre>"},{"location":"kubernetes/distributions/gke-terraform/","title":"GKE Terraform","text":""},{"location":"kubernetes/distributions/gke-terraform/#resources","title":"Resources","text":"<ul> <li>Terraform Google Cloud Container Cluster (GKE) Resource</li> <li>Un-official GKE Terraform Module</li> <li>Jetstack GKE Terraform Module</li> </ul>"},{"location":"kubernetes/distributions/gke-terraform/#pre-requisites","title":"Pre-Requisites","text":""},{"location":"kubernetes/distributions/gke-terraform/#terraform-configuration","title":"Terraform Configuration","text":"<p>The idea behind the Terraform configuration is as follows:</p> <ul> <li>Use Configuration-as-Code to create the GKE Cluster</li> <li>Have separate Node Pools for workload isolation / specialization</li> <li>Each Node Pool has a Cluster Autoscaler to make the cluster size dynamic</li> </ul>"},{"location":"kubernetes/distributions/gke-terraform/#variables","title":"Variables","text":"<pre><code>variable \"project\" { }\n\nvariable \"name\" {\n  description = \"The name of the cluster (required)\"\n  default     = \"my-awesome-jx-cluster\"\n}\n\nvariable \"description\" {\n  description = \"The description of the cluster\"\n  default     = \"Jenkins X Environment for ...\"\n}\n\nvariable \"location\" {\n  description = \"The location to host the cluster\"\n  default     = \"europe-west4\"\n}\n\nvariable \"cluster_master_version\" {\n  description = \"The minimum kubernetes version for the master nodes\"\n  default     = \"1.14.7-gke.10\"\n}\n</code></pre>"},{"location":"kubernetes/distributions/gke-terraform/#main","title":"Main","text":"<pre><code>terraform {\n  required_version = \"~&gt; 0.12\"\n}\n\n# https://www.terraform.io/docs/providers/google/index.html\nprovider \"google\" {\n  version   = \"~&gt; 2.18.1\"\n  project   = \"${var.project}\"\n  region    = \"europe-west4\"\n  zone      = \"europe-west4-b\"\n}\n</code></pre>"},{"location":"kubernetes/distributions/gke-terraform/#cluster","title":"Cluster","text":"<pre><code>resource \"google_container_cluster\" \"primary\" {\n  name        = \"${var.name}\"\n  location    = \"${var.location}\"\n\n  # We can't create a cluster with no node pool defined, but we want to only use\n  # separately managed node pools. So we create the smallest possible default\n  # node pool and immediately delete it.\n  remove_default_node_pool  = true\n  initial_node_count        = 1\n  min_master_version        = \"${var.cluster_master_version}\"\n  resource_labels           = {\n    environment = \"development\"\n    created-by  = \"terraform\"\n    owner       = \"joostvdg\"\n  }\n\n  # Configuration options for the NetworkPolicy feature.\n  network_policy {\n    # Whether network policy is enabled on the cluster. Defaults to false.\n    # In GKE this also enables the ip masquerade agent\n    # https://cloud.google.com/kubernetes-engine/docs/how-to/ip-masquerade-agent\n    enabled = true\n\n    # The selected network policy provider. Defaults to PROVIDER_UNSPECIFIED.\n    provider = \"CALICO\"\n  }\n}\n</code></pre>"},{"location":"kubernetes/distributions/gke-terraform/#node-pools","title":"Node Pools","text":"<pre><code>resource \"google_container_node_pool\" \"nodepool1\" {\n  name       = \"pool1\"\n  location   =  \"${var.location}\"\n  cluster    =  \"${google_container_cluster.primary.name}\"\n  node_count = 1\n\n  management {\n    auto_repair  = true\n    auto_upgrade = true\n  }\n\n  autoscaling {\n    min_node_count = 1\n    max_node_count = 4\n  }\n\n  node_config {\n    machine_type = \"n1-standard-2\"\n    oauth_scopes = [\n      \"https://www.googleapis.com/auth/compute\",\n      \"https://www.googleapis.com/auth/devstorage.read_only\",\n      \"https://www.googleapis.com/auth/logging.write\",\n      \"https://www.googleapis.com/auth/monitoring\",\n      \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\",\n      \"https://www.googleapis.com/auth/cloud-platform\"\n    ]\n  }\n}\n\nresource \"google_container_node_pool\" \"nodepool2\" {\n  name       = \"pool2\"\n  location   = \"europe-west4\"\n  cluster    = \"${google_container_cluster.primary.name}\"\n  node_count = 1\n\n  autoscaling {\n    min_node_count = 1\n    max_node_count = 3\n  }\n\n  node_config {\n    machine_type = \"n2-standard-2\"\n    oauth_scopes = [\n      \"https://www.googleapis.com/auth/compute\",\n      \"https://www.googleapis.com/auth/devstorage.read_only\",\n      \"https://www.googleapis.com/auth/logging.write\",\n      \"https://www.googleapis.com/auth/monitoring\",\n      \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\",\n      \"https://www.googleapis.com/auth/cloud-platform\"\n    ]\n  }\n}\n</code></pre>"},{"location":"kubernetes/distributions/install-gke/","title":"GKE with Helm","text":""},{"location":"kubernetes/distributions/install-gke/#env-variables","title":"Env Variables","text":"<pre><code>CLUSTER_NAME=MyGKECluster\nREGION=europe-west4\nNODE_LOCATIONS=${REGION}-a,${REGION}-b\nZONE=europe-west4-a\nK8S_VERSION=1.11.5-gke.4\nPROJECT_ID=\n</code></pre>"},{"location":"kubernetes/distributions/install-gke/#get-kubernetes-versions","title":"Get Kubernetes versions","text":"<pre><code>gcloud container get-server-config --region $REGION\n</code></pre>"},{"location":"kubernetes/distributions/install-gke/#create-cluster","title":"Create Cluster","text":"<pre><code>gcloud container clusters create ${CLUSTER_NAME} \\\n    --region ${REGION} --node-locations ${NODE_LOCATIONS} \\\n    --cluster-version ${K8S_VERSION} \\\n    --num-nodes 2 --machine-type n1-standard-2 \\\n    --addons=HorizontalPodAutoscaling \\\n    --min-nodes 2 --max-nodes 3 \\\n    --enable-autoupgrade \\\n    --enable-autoscaling \\\n    --enable-network-policy \\\n    --labels=purpose=practice\n</code></pre>"},{"location":"kubernetes/distributions/install-gke/#post-install","title":"Post Install","text":"<pre><code>kubectl create clusterrolebinding cluster-admin-binding \\\n    --clusterrole cluster-admin \\\n    --user $(gcloud config get-value account)\n</code></pre>"},{"location":"kubernetes/distributions/install-gke/#delete-cluster","title":"Delete Cluster","text":"<pre><code>gcloud container clusters delete $CLUSTER_NAME --region $REGION\n</code></pre>"},{"location":"kubernetes/distributions/install-gke/#configure-kubeconfig","title":"Configure kubeconfig","text":"<pre><code>gcloud container clusters get-credentials ${CLUSTER_NAME} --region ${REGION}\n</code></pre>"},{"location":"kubernetes/distributions/install-gke/#install-cluster-tools","title":"Install Cluster Tools","text":""},{"location":"kubernetes/distributions/install-gke/#helm","title":"Helm","text":"<p>We use Helm as a package manager to more easily install other tools on Kubernetes.</p> <p>There's several repositories with a large number of mature charts - the name of the Helm packages.</p> <p>One being Helm/Stable another being Helm Hub.</p>"},{"location":"kubernetes/distributions/install-gke/#create-service-account","title":"Create service account","text":"<pre><code>kubectl create serviceaccount --namespace kube-system tiller\n</code></pre> <p>Warning</p> <p>Tiller is deemed not safe for production, at least not in its default configuration. Either enable its TLS configuration and take other measures (such as namespace limitation) or use alternative solutions. Such as Kustomize, Pulumi, Jenkins X or raw Yaml.</p>"},{"location":"kubernetes/distributions/install-gke/#create-cluster-role-binding","title":"Create cluster role binding","text":"<pre><code>kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller\n</code></pre>"},{"location":"kubernetes/distributions/install-gke/#helm-init","title":"helm init","text":"<pre><code>helm init --service-account tiller\n</code></pre>"},{"location":"kubernetes/distributions/install-gke/#test-version","title":"Test version","text":"<pre><code>helm version\n</code></pre> <p>Warning</p> <p>Currently, nginx ingress controller has an issue with Helm 2.14. So if you 2.14, either downgrade to 2.13.1 or install the Ingress Controller via an alternative solution (such as Kustomize).</p>"},{"location":"kubernetes/distributions/install-gke/#ingress-controller","title":"Ingress Controller","text":"<pre><code>helm install --namespace ingress-nginx --name nginx-ingress stable/nginx-ingress \\\n    --set controller.service.externalTrafficPolicy=Local \\\n    --set controller.replicaCount=3 \\\n    --set rbac.create=true\n</code></pre>"},{"location":"kubernetes/distributions/install-gke/#get-loadbalancer-ip","title":"Get LoadBalancer IP","text":"<pre><code>export LB_IP=$(kubectl get svc -n ingress-nginx nginx-ingress-controller -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\necho $LB_IP\n</code></pre> <p>Warning</p> <p>Now is the time to configure your DNS to use whatever <code>LB_IP</code>'s value is.</p>"},{"location":"kubernetes/distributions/install-gke/#cert-manager","title":"Cert Manager","text":"<p>Cert Manager is the recommended approach for managing TLS certificates in Kubernetes. If you do not want to manage certificates yourself, please use this.</p> <p>The certificates it uses are real and valid certificates, provided by Let's Encrypt.</p>"},{"location":"kubernetes/distributions/install-gke/#install-crds","title":"Install CRD's","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8/deploy/manifests/00-crds.yaml\n</code></pre>"},{"location":"kubernetes/distributions/install-gke/#create-namespace","title":"Create Namespace","text":"<pre><code>kubectl create namespace cert-manager\n</code></pre>"},{"location":"kubernetes/distributions/install-gke/#label-namespace","title":"Label namespace","text":"<pre><code>kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true\n</code></pre>"},{"location":"kubernetes/distributions/install-gke/#add-helm-repo","title":"Add Helm Repo","text":"<pre><code>helm repo add jetstack https://charts.jetstack.io\nhelm repo update\n</code></pre>"},{"location":"kubernetes/distributions/install-gke/#install","title":"Install","text":"<pre><code>helm install \\\n    --name cert-manager \\\n    --namespace cert-manager \\\n    --version v0.8.0 \\\n    jetstack/cert-manager\n</code></pre>"},{"location":"kubernetes/khw-gce/","title":"Kubernetes the Hard Way - GCE","text":"<p>This assumes OSX and GCE.</p>"},{"location":"kubernetes/khw-gce/#goal","title":"Goal","text":"<p>The goal is to setup up HA Kubernetes cluster on GCE from it's most basic parts. That means we will install and configure the basic components ourselves, such as the API server and Kubelets.</p>"},{"location":"kubernetes/khw-gce/#setup","title":"Setup","text":"<p>As to limit the scope to doing the setup of the Kubernetes cluster ourselves, we will make it static. That means we will create and configure the network and compute resources to be fit for 3 Control Plane VM's and 3 worker VM's. We will not be able to recover a failing node or accomidate additional resources.</p>"},{"location":"kubernetes/khw-gce/#resources-in-gce","title":"Resources in GCE","text":"<ul> <li>Public IP address, as front-end for the three API servers</li> <li>3 VM's for the Control Plance</li> <li>3 VM's as workers</li> <li>VPC</li> <li>Network Routes: from POD CIDR blocks to the host VM (for workers)</li> <li>Firewall configuration: allow health checks, dns, internal communication and connection to API server</li> </ul>"},{"location":"kubernetes/khw-gce/#kubernetes-resources","title":"Kubernetes Resources","text":""},{"location":"kubernetes/khw-gce/#control-plane","title":"Control Plane","text":"<ul> <li>etcd: stores cluster state</li> <li>kube-api server: entry point for interacting with the cluster by exposing the api</li> <li>kube-scheduler: makes sure pods get scheduled</li> <li>kube-controller-manager: aggregate of required controllers</li> <li>Node Controller: &gt; Responsible for noticing and responding when nodes go down.</li> <li>Replication Controller:  &gt; Responsible for maintaining the correct number of pods for every replication controller object in the system.</li> <li>Endpoints Controller: &gt; Populates the Endpoints object (that is, joins Services &amp; Pods).</li> <li>Service Account &amp; Token Controller: &gt; Create default accounts and API access tokens for new namespaces.</li> </ul>"},{"location":"kubernetes/khw-gce/#worker-nodes","title":"Worker nodes","text":"<ul> <li>kubelet: &gt; An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.</li> <li>kube-proxy: &gt; kube-proxy enables the Kubernetes service abstraction by maintaining network rules on the host and performing connection forwarding</li> <li>A container runtime: this can be <code>Docker</code>, <code>rkt</code> or as in our case <code>containerd</code></li> </ul>"},{"location":"kubernetes/khw-gce/#network","title":"Network","text":"<ul> <li>https://blog.csnet.me/k8s-thw/part1/</li> <li>https://github.com/kelseyhightower/kubernetes-the-hard-way</li> </ul> <p>We will be using the network components - with Weave-Net and CoreDNS - as described in the csnet blog. But we will use the CIDR blocks as stated in the Kelsey Hightower's Kubernetes the Hard Way (<code>KHW</code>).</p>"},{"location":"kubernetes/khw-gce/#kelseys-khw","title":"Kelsey's KHW","text":"Range Use 10.240.0.10/24 LAN (GCE VMS) 10.200.0.0/16 k8s Pod network 10.32.0.0/24 k8s Service network 10.32.0.1 k8s API server 10.32.0.10 k8s dns <ul> <li>API Server: https://127.0.0.1:6443</li> <li>service-cluster-ip-range=10.32.0.0/24</li> <li>cluster-cidr=10.200.0.0/1</li> </ul>"},{"location":"kubernetes/khw-gce/#csnets","title":"CSNETs","text":"Range Use 10.32.2.0/24 LAN (csnet.me) 10.16.0.0/16 k8s Pod network 10.10.0.0/22 k8s Service network 10.10.0.1 k8s API server 10.10.0.10 k8s dns <ul> <li>API Server: https://10.32.2.97:6443</li> <li>service-cluster-ip-range=10.10.0.0/22</li> <li>cluster-cidr=10.16.0.0/16</li> </ul>"},{"location":"kubernetes/khw-gce/#install-tools","title":"Install tools","text":"<p>On the machine doing the installation, we will need some tools installed. We will use the following tools:</p> <ul> <li>kubectl: for communicating with the API server</li> <li>cfssl: for creating the certificates and sign them</li> <li>helm: for installing additional tools later</li> <li>stern: for viewing logs of multiple pods at once (for example, all kube-dns pods)</li> <li>terraform: for managing our resources in GCE</li> </ul> <pre><code>brew install kubernetes-cli\nbrew install cfssl\nbrew install kubernetes-helm\nbrew install stern\nbrew install terraform\n</code></pre>"},{"location":"kubernetes/khw-gce/#check-versions","title":"Check versions","text":"<pre><code>kubectl version -c -o yaml\ncfssl version\nhelm version -c --short\nstern --version\nterraform version\n</code></pre>"},{"location":"kubernetes/khw-gce/#terraform-remote-storage","title":"Terraform remote storage","text":"<p>The help with problems of local storage and potential loss of data when local OS problems occur,  we will use an S3 bucket as Terraform state storage.</p> <ul> <li>create s3 bucket</li> <li>configure Terraform to use this as remote state storage</li> <li>see how to this here</li> <li>read more about this, in Terraform's docs</li> </ul> <pre><code>export AWS_ACCESS_KEY_ID=\"anaccesskey\"\nexport AWS_SECRET_ACCESS_KEY=\"asecretkey\"\nexport AWS_DEFAULT_REGION=\"eu-central-1\"\n</code></pre> <pre><code>terraform {\n  backend \"s3\" {\n    bucket  = \"euros-terraform-state\"\n    key     = \"terraform.tfstate\"\n    region  = \"eu-central-1\"\n    encrypt = \"true\"\n  }\n}\n</code></pre>"},{"location":"kubernetes/khw-gce/#gke-service-account","title":"GKE Service Account","text":"<p>Create a new GKE service account, and export it's json credentials file for use with Terraform.</p> <p>See GKE Tutorial page for how you can do this.</p>"},{"location":"kubernetes/khw-gce/certificates/","title":"Certificates","text":"<p>Note</p> <p>Before we can continue here, we need to have our nodes up and running with their external ip addresses and our fixed public ip address. This is because some certificates require these external ip addresses! <pre><code>gcloud compute instances list\ngcloud compute addresses list --filter=\"name=('kubernetes-the-hard-way')\"\n</code></pre></p> <p>We need to create a whole lot of certificates, listed below, with the help of cfssl. A tool from CDN provider CloudFlare.</p>"},{"location":"kubernetes/khw-gce/certificates/#required-certificates","title":"Required certificates","text":"<ul> <li>CA (or Certificate Authority): will be the root certificate of our trust chain<ul> <li>result: <code>ca.pem</code> &amp; <code>ca-key.pem</code></li> </ul> </li> <li>Admin: the admin of our cluster (you!)<ul> <li>result: <code>admin-key.pem</code> &amp; <code>admin.pem</code></li> </ul> </li> <li>Kubelet: the certificates of the kubelet processes on the worker nodes<ul> <li>result:  <pre><code>worker-0.pem \nworker-1-key.pem \nworker-1.pem \nworker-2-key.pem \nworker-2.pem\n</code></pre></li> </ul> </li> <li>Controller Manager<ul> <li>result: <code>kube-controller-manager-key.pem</code> &amp; <code>kube-controller-manager.pem</code></li> </ul> </li> <li>Scheduler<ul> <li>result: <code>kube-scheduler-key.pem</code> &amp; <code>kube-scheduler.pem</code></li> </ul> </li> <li>API Server<ul> <li>result <code>kubernetes-key.pem</code> &amp; <code>kubernetes.pem</code></li> </ul> </li> <li>Service Account: ???<ul> <li>result: <code>service-account-key.pem</code> &amp; <code>service-account.pem</code></li> </ul> </li> </ul>"},{"location":"kubernetes/khw-gce/certificates/#certificate-example","title":"Certificate example","text":"<p>Because we will use the <code>cfssl</code> tool from CloudFlare, we will define our certificate signing request (CSR's) in json.</p> <pre><code>{\n  \"CN\": \"service-accounts\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"NL\",\n      \"L\": \"Utrecht\",\n      \"O\": \"Kubernetes\",\n      \"OU\": \"Kubernetes The Hard Way\",\n      \"ST\": \"Utrecht\"\n    }\n  ]\n}\n</code></pre>"},{"location":"kubernetes/khw-gce/certificates/#install-scripts","title":"Install scripts","text":"<p>Make sure you're in <code>k8s-the-hard-way/scripts</code></p> <pre><code>./certs.sh\n</code></pre>"},{"location":"kubernetes/khw-gce/controller/","title":"Controller Config","text":"<p>We have to configure the following:</p> <ul> <li>move certificates to the correct location</li> <li>move encryption configuration to <code>/var/lib/kubernetes</code></li> <li>download and install binaries</li> <li>kubectl</li> <li>kube-apiserver</li> <li>kube-scheduler</li> <li>kube-controller-manager</li> <li>configure API server</li> <li>systemd service</li> <li>configure Controller Manager</li> <li>systemd service</li> <li>configure Scheduler</li> <li>systemd service</li> <li>kubernetes configuration yaml <code>kind: KubeSchedulerConfiguration</code></li> <li>create nginx reverse proxy to enable GCE's health checks to reach each API Server instance</li> <li>configure RBAC configuration in the API server</li> <li>via <code>ClusterRole</code> and <code>ClusterRoleBinding</code></li> </ul>"},{"location":"kubernetes/khw-gce/controller/#install","title":"Install","text":"<p>We have an installer script, <code>controller-local.sh</code>, which should be executed on each controller VM.</p> <p>To do so, use the <code>controller.sh</code> script to upload this file to the VM's.</p> <pre><code>./controller.sh\n</code></pre>"},{"location":"kubernetes/khw-gce/debug/","title":"Debug","text":""},{"location":"kubernetes/khw-gce/debug/#kubernetes-components-not-healthy","title":"Kubernetes components not healthy","text":""},{"location":"kubernetes/khw-gce/debug/#check-for-healthy-status","title":"Check for healthy status","text":"<p>On a control plane node, check <code>etcd</code>.</p> <pre><code>sudo ETCDCTL_API=3 etcdctl member list \\\n    --endpoints=https://127.0.0.1:2379 \\\n    --cacert=/etc/etcd/ca.pem \\\n    --cert=/etc/etcd/kubernetes.pem \\\n    --key=/etc/etcd/kubernetes-key.pem\n</code></pre> <pre><code>3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379\nf98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379\nffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379\n</code></pre> <p>On a control plan node, check control plane components.</p> <pre><code>kubectl get componentstatuses --kubeconfig admin.kubeconfig\n</code></pre> <p>Should look like this:</p> <pre><code>NAME                 STATUS    MESSAGE              ERROR\ncontroller-manager   Healthy   ok\nscheduler            Healthy   ok\netcd-2               Healthy   {\"health\": \"true\"}\netcd-0               Healthy   {\"health\": \"true\"}\netcd-1               Healthy   {\"health\": \"true\"}\n</code></pre> <p>On a control plane node, check API server status (via nginx reverse proxy).</p> <pre><code>curl -H \"Host: kubernetes.default.svc.cluster.local\" -i http://127.0.0.1/healthz\n</code></pre> <pre><code>HTTP/1.1 200 OK\nServer: nginx/1.14.0 (Ubuntu)\nDate: Mon, 14 May 2018 13:45:39 GMT\nContent-Type: text/plain; charset=utf-8\nContent-Length: 2\nConnection: keep-alive\n\nok\n</code></pre> <p>On an external system, you can check if the API server is working and reachable via routing.</p> <pre><code>curl --cacert ca.pem https://${KUBERNETES_PUBLIC_ADDRESS}:6443/version\n</code></pre> <p>Assuming that GCE is used.</p> <pre><code>KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute addresses describe kubernetes-the-hard-way \\\n    --region $(gcloud config get-value compute/region) \\\n    --format 'value(address)')  \n</code></pre>"},{"location":"kubernetes/khw-gce/debug/#check-for-errors","title":"Check for errors","text":"<pre><code>journalctl\n</code></pre> <p>Or for specific components.</p> <pre><code>journalctl -u kube-scheduler\n</code></pre>"},{"location":"kubernetes/khw-gce/debug/#weave-net-pods-blocked","title":"Weave-Net pods Blocked","text":"<p>Sometimes when installing weave-net as the CNI plugin, the pods are blocked.</p> <pre><code>NAME              READY     STATUS    RESTARTS   AGE\nweave-net-fwvsr   0/2       Blocked   0          3m\nweave-net-v9z9n   0/2       Blocked   0          3m\nweave-net-zfghq   0/2       Blocked   0          3m\n</code></pre> <p>Usually this means something went wrong with the CNI configuration. Ideally, Weave-Net will generate this when installed, but sometimes this doesn't happen.</p> <p>This is easily found when checking the <code>journalctl</code> on the worker nodes (<code>journalctl -u kubelet</code>).</p> <p>There are three things to be done before installing weave-net again.</p>"},{"location":"kubernetes/khw-gce/debug/#ensure-ip4-forwarding-is-enabled","title":"Ensure ip4 forwarding is enabled","text":"<pre><code>sysctl net.ipv4.ip_forward=1\nsysctl -p /etc/sysctl.conf\n</code></pre> <p>See Kubernetes Docs for GCE routing  or Michael Champagne's blog on KHW.</p>"},{"location":"kubernetes/khw-gce/debug/#ensure-all-weave-net-resources-are-gone","title":"Ensure all weave-net resources are gone","text":"<p>I've noticed that when this problem occurs, deleting the weave-net resources with <code>kubectl delete -f &lt;weaveNet resource&gt;</code> leaves the pods. The pods are terminated (they never started) but are not removed.</p> <p>To remove them, use the line below, as explained on stackoverflow.</p> <pre><code>kubectl delete pod NAME --grace-period=0 --force\n</code></pre>"},{"location":"kubernetes/khw-gce/debug/#restart-kubelet","title":"Restart Kubelet","text":"<p>I'm not sure if this is 100% required, but I've had better luck with restarting the kubelet before reinstalling weave-net.</p> <p>So, login to each worker node, <code>gcloud compute ssh worker-?</code> and issue the following commands.</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart kubelet\n</code></pre>"},{"location":"kubernetes/khw-gce/debug/#dns-on-gce-not-working","title":"DNS on GCE not working","text":"<p>It seemed something has changed in GCE after Kelsey Hightower's Kubernetes The Hardway was written/updated.</p> <p>This means that if you follow through the documentation, you will run into this:</p> <pre><code>kubectl exec -ti $POD_NAME -- nslookup kubernetes\n;; connection timed out; no servers could be reached\n\ncommand terminated with exit code 1\n</code></pre> <p>The cure seems to be to add additional <code>resolve.conf</code> file configuration to the kubelet's systemd service definition.</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/systemd/system/kubelet.service\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=containerd.service\nRequires=containerd.service\n\n[Service]\nExecStart=/usr/local/bin/kubelet \\\n  --config=/var/lib/kubelet/kubelet-config.yaml \\\n  --container-runtime=remote \\\n  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\n  --resolv-conf=/run/systemd/resolve/resolv.conf \\\n  --image-pull-progress-deadline=2m \\\n  --kubeconfig=/var/lib/kubelet/kubeconfig \\\n  --network-plugin=cni \\\n  --register-node=true \\\n  --v=2\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre> <p>In addition, one should also use at least busybox 1.28 to do the dns check.</p> <p>For more information, read this issue.</p>"},{"location":"kubernetes/khw-gce/encryption/","title":"Encryption","text":"<p>Kubernetes stores a variety of data including cluster state, application configurations, and secrets. Kubernetes supports the ability to encrypt cluster data at rest.</p> <p>In order to use this ability to encrypt data at rest, each member of the control plane has to know the encryption key.</p> <p>So we will have to create one.</p>"},{"location":"kubernetes/khw-gce/encryption/#encryption-configuration","title":"Encryption configuration","text":"<p>We have to create a encryption key first. For the sake of embedding it into a yaml file, we will have to encode it to <code>base64</code>.</p> <pre><code>ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kubernetes/khw-gce/encryption/#install-scripts","title":"Install scripts","text":"<p>Make sure you're in <code>k8s-the-hard-way/scripts</code></p> <pre><code>./encryption.sh\n</code></pre>"},{"location":"kubernetes/khw-gce/etcd/","title":"ETCD","text":"<p>Kubernetes components are stateless and store cluster state in etcd. In this lab you will bootstrap a three node etcd cluster and configure it for high availability and secure remote access.</p> <p>The bare minimum is to have a single <code>etcd</code> instance running. But for production purposes it is best to run etcd in HA mode. This means we need to have three instances running that know eachother.</p> <p>Again, this is not a production ready setup, as the static nature prevents automatic recovery if a node fails.</p>"},{"location":"kubernetes/khw-gce/etcd/#steps-to-take","title":"Steps to take","text":"<ul> <li>download &amp; install etcd binary</li> <li>prepare required certificates</li> <li>create <code>systemd</code> service definition</li> <li>reload <code>systemd</code> configuration, enable &amp; start the service</li> </ul>"},{"location":"kubernetes/khw-gce/etcd/#install-script","title":"Install script","text":"<p>Make sure that the local install script is on every server, you can use the <code>etcd.sh</code> script for this.</p> <p>Then, make sure you're connect to all three controller VM's at the same time, for example via tmux or iterm. For iterm:</p> <ul> <li>use <code>ctrl</code> + <code>shift</code> + <code>d</code> to open three horizontal windows</li> <li>use <code>ctrl</code> + <code>shift</code> + <code>i</code> to write output to all three windows at once</li> <li>login to each controller <code>gcloud compute ssh controller-?</code></li> <li><code>./etcd-local.sh</code></li> </ul>"},{"location":"kubernetes/khw-gce/etcd/#verification","title":"Verification","text":"<pre><code>sudo ETCDCTL_API=3 etcdctl member list \\\n    --endpoints=https://127.0.0.1:2379 \\\n    --cacert=/etc/etcd/ca.pem \\\n    --cert=/etc/etcd/kubernetes.pem \\\n    --key=/etc/etcd/kubernetes-key.pem\n</code></pre>"},{"location":"kubernetes/khw-gce/etcd/#expected-output","title":"Expected Output","text":"<pre><code>3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379\nf98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379\nffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379\n</code></pre>"},{"location":"kubernetes/khw-gce/kubeconfigs/","title":"Kubeconfigs","text":"<p>Now that we have certificates we have to make sure we have configurations that the Kubernetes parts can actually use - certificates themselves are not enough.</p> <p>This is where we will use kubernetes configuration files, or <code>kubeconfigs</code>.</p> <p>We will have to create the following <code>kubeconfigs</code>:</p> <ul> <li>controller manager</li> <li>kubelet</li> <li>kube-proxy</li> <li>kube-scheduler</li> <li>admin user</li> </ul>"},{"location":"kubernetes/khw-gce/kubeconfigs/#create-test-kubeconfig-file","title":"Create &amp; Test kubeconfig file","text":"<p>Here's an example script:</p> <pre><code>kubectl config set-cluster kubernetes-the-hard-way \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://127.0.0.1:6443 \\\n    --kubeconfig=kube-controller-manager.kubeconfig\n\nkubectl config set-credentials system:kube-controller-manager \\\n    --client-certificate=kube-controller-manager.pem \\\n    --client-key=kube-controller-manager-key.pem \\\n    --embed-certs=true \\\n    --kubeconfig=kube-controller-manager.kubeconfig\n\nkubectl config set-context default \\\n    --cluster=kubernetes-the-hard-way \\\n    --user=system:kube-controller-manager \\\n    --kubeconfig=kube-controller-manager.kubeconfig\n\nkubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig\n</code></pre> <p>The steps we execute in order are the following:</p> <ul> <li>create a kubeconfig entry for our <code>kubernetes-the-hard-way</code> cluster and export this into a <code>.kubeconfig</code> file</li> <li>add credentials to this config file, in the form of our kubernetes component's certificate</li> <li>set the default config of this config file to namespace <code>default</code> and user to the component we're configuring</li> <li>test the configuration file by using it</li> </ul>"},{"location":"kubernetes/khw-gce/kubeconfigs/#install-scripts","title":"Install scripts","text":"<p>Make sure you're in <code>k8s-the-hard-way/scripts</code></p> <pre><code>./kube-configs.sh\n</code></pre>"},{"location":"kubernetes/khw-gce/network/","title":"Networking","text":"<p>First, configure external access so we can run <code>kubectl</code> commands from our own machine.</p> <p>Confirm the you can now call the following:</p> <pre><code>kubectl get nodes -o wide\n</code></pre>"},{"location":"kubernetes/khw-gce/network/#configure-weavenet","title":"Configure WeaveNet","text":"<pre><code>kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')&amp;env.IPALLOC_RANGE=10.200.0.0/16\"\n</code></pre>"},{"location":"kubernetes/khw-gce/network/#confirm-weavenet-works","title":"Confirm WeaveNet works","text":"<pre><code>kubectl get pod --namespace=kube-system -l name=weave-net\n</code></pre> <p>It should look like this:</p> <pre><code>NAME              READY     STATUS    RESTARTS   AGE\nweave-net-fwvsr   2/2       Running   1          4h\nweave-net-v9z9n   2/2       Running   1          4h\nweave-net-zfghq   2/2       Running   1          4h\n</code></pre>"},{"location":"kubernetes/khw-gce/network/#configure-coredns","title":"Configure CoreDNS","text":"<p>Before installing <code>CoreDNS</code>, please confirm networking is in order.</p> <pre><code>kubectl get nodes -o wide\n</code></pre> <p>Warning</p> <p>If nodes are not <code>Ready</code>, something is wrong and needs to be fixed before you continue.</p> <pre><code>kubectl apply -f ../configs/core-dns-config.yaml\n</code></pre>"},{"location":"kubernetes/khw-gce/network/#confirm-coredns-pods","title":"Confirm CoreDNS pods","text":"<pre><code>kubectl get pod --all-namespaces -l k8s-app=coredns -o wide\n</code></pre>"},{"location":"kubernetes/khw-gce/network/#confirm-dns-works","title":"Confirm DNS works","text":"<pre><code>kubectl run busybox --image=busybox:1.28 --command -- sleep 3600\n</code></pre> <pre><code>POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath=\"{.items[0].metadata.name}\")\n</code></pre> <pre><code>kubectl exec -ti $POD_NAME -- nslookup kubernetes\n</code></pre> <p>Note</p> <p>It should look like this: <pre><code>Server:    10.10.0.10\nAddress 1: 10.10.0.10 kube-dns.kube-system.svc.cluster.local\n\nName:      kubernetes\nAddress 1: 10.10.0.1 kubernetes.default.svc.cluster.local\n</code></pre></p>"},{"location":"kubernetes/khw-gce/remote-access/","title":"Remote Access","text":"<pre><code>KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute addresses describe kubernetes-the-hard-way \\\n    --region $(gcloud config get-value compute/region) \\\n    --format 'value(address)')\necho \"KUBERNETES_PUBLIC_ADDRESS=${KUBERNETES_PUBLIC_ADDRESS}\"\n\nkubectl config set-cluster kubernetes-the-hard-way \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443\n\nkubectl config set-credentials admin \\\n    --client-certificate=admin.pem \\\n    --client-key=admin-key.pem\n\nkubectl config set-context kubernetes-the-hard-way \\\n    --cluster=kubernetes-the-hard-way \\\n    --user=admin\n\nkubectl config use-context kubernetes-the-hard-way\n</code></pre>"},{"location":"kubernetes/khw-gce/remote-access/#confirm","title":"Confirm","text":"<pre><code>kubectl get nodes -o wide\n</code></pre>"},{"location":"kubernetes/khw-gce/terraform-compute/","title":"Compute resources","text":""},{"location":"kubernetes/khw-gce/terraform-compute/#create-network","title":"Create network","text":""},{"location":"kubernetes/khw-gce/terraform-compute/#vpc-with-firewall-rules","title":"VPC with Firewall rules","text":"<pre><code>provider \"google\" {\n  credentials = \"${file(\"${var.credentials_file_path}\")}\"\n  project     = \"${var.project_name}\"\n  region      = \"${var.region}\"\n}\n\nresource \"google_compute_network\" \"khw\" {\n  name                    = \"kubernetes-the-hard-way\"\n  auto_create_subnetworks = \"false\"\n}\n\nresource \"google_compute_subnetwork\" \"khw-kubernetes\" {\n  name          = \"kubernetes\"\n  ip_cidr_range = \"10.240.0.0/24\"\n  region        = \"${var.region}\"\n  network       = \"${google_compute_network.khw.self_link}\"\n}\n\nresource \"google_compute_firewall\" \"khw-allow-internal\" {\n  name    = \"kubernetes-the-hard-way-allow-internal\"\n  network = \"${google_compute_network.khw.name}\"\n\n  source_ranges = [\"10.240.0.0/24\", \"10.200.0.0/16\"]\n\n  allow {\n    protocol = \"tcp\"\n  }\n\n  allow {\n    protocol = \"udp\"\n  }\n\n  allow {\n    protocol = \"icmp\"\n  }\n}\n\nresource \"google_compute_firewall\" \"khw-allow-external\" {\n  name    = \"kubernetes-the-hard-way-allow-external\"\n  network = \"${google_compute_network.khw.name}\"\n\n  allow {\n    protocol = \"icmp\"\n  }\n\n  allow {\n    protocol = \"tcp\"\n    ports    = [\"22\", \"6443\"]\n  }\n\n  source_ranges = [\"0.0.0.0/0\"]\n}\n\nresource \"google_compute_firewall\" \"khw-allow-dns\" {\n  name    = \"kubernetes-the-hard-way-allow-dns\"\n  network = \"${google_compute_network.khw.name}\"\n\n  source_ranges = [\"0.0.0.0\"]\n\n  allow {\n    protocol = \"tcp\"\n    ports    = [\"53\", \"443\"]\n  }\n\n  allow {\n    protocol = \"udp\"\n    ports    = [\"53\"]\n  }\n}\n\nresource \"google_compute_firewall\" \"khw-allow-health-check\" {\n  name    = \"kubernetes-the-hard-way-allow-health-check\"\n  network = \"${google_compute_network.khw.name}\"\n\n  allow {\n    protocol = \"tcp\"\n  }\n\n  source_ranges = [\"209.85.152.0/22\", \"209.85.204.0/22\", \"35.191.0.0/16\"]\n}\n</code></pre>"},{"location":"kubernetes/khw-gce/terraform-compute/#confirm-network","title":"Confirm network","text":"<pre><code>gcloud compute firewall-rules list --filter=\"network:kubernetes-the-hard-way\"\n</code></pre> <p>Should look like:</p> <pre><code>NAME                                    NETWORK                  DIRECTION  PRIORITY  ALLOW                 DENY\nkubernetes-the-hard-way-allow-external  kubernetes-the-hard-way  INGRESS    1000      icmp,tcp:22,tcp:6443\nkubernetes-the-hard-way-allow-internal  kubernetes-the-hard-way  INGRESS    1000      icmp,udp,tcp\n</code></pre>"},{"location":"kubernetes/khw-gce/terraform-compute/#public-ip","title":"Public IP","text":"<pre><code>resource \"google_compute_address\" \"khw-lb-public-ip\" {\n    name = \"kubernetes-the-hard-way\"\n}\n</code></pre> <p>Confirm:</p> <pre><code>gcloud compute addresses list --filter=\"name=('kubernetes-the-hard-way')\"\n</code></pre> <p>Output:</p> <pre><code>NAME                     REGION        ADDRESS         STATUS\nkubernetes-the-hard-way  europe-west4  35.204.134.219  RESERVED\n</code></pre>"},{"location":"kubernetes/khw-gce/terraform-compute/#vm-definitions-with-terraform-modules","title":"VM Definitions with Terraform modules","text":"<p>We're going to need to create 6 VM's. 3 Controller nodes and 3 worker nodes.</p> <p>Within each of the two categories, all the three VM's will be the same. So it would be a waste to define them more than once. This can be achieved via Terraform's Module system(read more here.</p>"},{"location":"kubernetes/khw-gce/terraform-compute/#define-a-module","title":"Define a module","text":"<p>For the sake of naming convention, we'll put all of our <code>modules</code> in a modules subfolder. We'll start with the controller module, but you can do the same for the worker.</p> <pre><code>mkdir -p modules/controller\n</code></pre> <pre><code>ls -lath\ndrwxr-xr-x  27 joostvdg  staff   864B Aug 26 12:50 .\ndrwxr-xr-x  20 joostvdg  staff   640B Aug 22 14:47 ..\ndrwxr-xr-x   4 joostvdg  staff   128B Aug  7 22:43 modules\n</code></pre> <pre><code>ls -lath modules\ndrwxr-xr-x  27 joostvdg  staff   864B Aug 26 12:50 ..\ndrwxr-xr-x   4 joostvdg  staff   128B Aug  7 22:43 .\ndrwxr-xr-x   4 joostvdg  staff   128B Aug  7 22:03 controller\n</code></pre> <p>Inside <code>modules/controller</code> we create two files, <code>main.tf</code> and <code>variables.tf</code>. We have to create an additional variables file, as the module cannot use the main folder's variables.</p> <p>Then, in our main folder we'll create a tf file for using these modules, called <code>nodes.tf</code>. As stated above, we pass along any variable from our main <code>variables.tf</code> to the module.</p> <pre><code>module \"controller\" {\n  source       = \"modules/controller\"\n  machine_type = \"${var.machine_type_controllers}\"\n  num          = \"${var.num_controllers}\"\n  zone         = \"${var.region_default_zone}\"\n  subnet       = \"${var.subnet_name}\"\n}\n\nmodule \"worker\" {\n  source       = \"modules/worker\"\n  machine_type = \"${var.machine_type_workers}\"\n  num          = \"${var.num_workers}\"\n  zone         = \"${var.region_default_zone}\"\n  network      = \"${google_compute_network.khw.name}\"\n  subnet       = \"${var.subnet_name}\"\n}\n</code></pre>"},{"location":"kubernetes/khw-gce/terraform-compute/#controller-config","title":"Controller config","text":"<pre><code>data \"google_compute_image\" \"khw-ubuntu\" {\n  family  = \"ubuntu-1804-lts\"\n  project = \"ubuntu-os-cloud\"\n}\n\nresource \"google_compute_instance\" \"khw-controller\" {\n  count          = \"${var.num}\"\n  name           = \"controller-${count.index}\"\n  machine_type   = \"${var.machine_type}\"\n  zone           = \"${var.zone}\"\n  can_ip_forward = \"true\"\n\n  tags = [\"kubernetes-the-hard-way\", \"controller\"]\n\n  boot_disk {\n    initialize_params {\n      image = \"${data.google_compute_image.khw-ubuntu.self_link}\"\n      size  = 200                                                 // in GB\n    }\n  }\n\n  network_interface {\n    subnetwork = \"${var.subnet}\"\n    address    = \"10.240.0.1${count.index}\"\n\n    access_config {\n      // Ephemeral External IP\n    }\n  }\n\n  # compute-rw,storage-ro,service-management,service-control,logging-write,monitoring\n  service_account {\n    scopes = [\"compute-rw\",\n      \"storage-ro\",\n      \"service-management\",\n      \"service-control\",\n      \"logging-write\",\n      \"monitoring\",\n    ]\n  }\n}\n</code></pre>"},{"location":"kubernetes/khw-gce/terraform-compute/#variables","title":"Variables","text":"<pre><code>variable \"num\" {\n  description = \"The number of controller VMs\"\n}\n\nvariable \"machine_type\" {\n  description = \"The type of VM for controllers\"\n}\n\nvariable \"zone\" {\n  description = \"The zone to create the controllers in\"\n}\n\nvariable \"subnet\" {\n  description = \"The subnet to create the nic in\"\n}\n</code></pre>"},{"location":"kubernetes/khw-gce/terraform-compute/#worker-config","title":"Worker config","text":"<p>Extra config for the worker are the routes, to aid the pods going out of the node.</p> <pre><code>data \"google_compute_image\" \"khw-ubuntu\" {\n  family  = \"ubuntu-1804-lts\"\n  project = \"ubuntu-os-cloud\"\n}\n\nresource \"google_compute_instance\" \"khw-worker\" {\n  count          = \"${var.num}\"\n  name           = \"worker-${count.index}\"\n  machine_type   = \"${var.machine_type}\"\n  zone           = \"${var.zone}\"\n  can_ip_forward = \"true\"\n\n  tags = [\"kubernetes-the-hard-way\", \"worker\"]\n\n  metadata {\n    pod-cidr = \"10.200.${count.index}.0/24\"\n  }\n\n  boot_disk {\n    initialize_params {\n      image = \"${data.google_compute_image.khw-ubuntu.self_link}\"\n      size  = 200                                                 // in GB\n    }\n  }\n\n  network_interface {\n    subnetwork = \"${var.subnet}\"\n    address    = \"10.240.0.2${count.index}\"\n\n    access_config {\n      // Ephemeral External IP\n    }\n  }\n\n  service_account {\n    scopes = [\"compute-rw\",\n      \"storage-ro\",\n      \"service-management\",\n      \"service-control\",\n      \"logging-write\",\n      \"monitoring\",\n    ]\n  }\n}\n\nresource \"google_compute_route\" \"khw-worker-route\" {\n  count       = \"${var.num}\"\n  name        = \"kubernetes-route-10-200-${count.index}-0-24\"\n  network     = \"${var.network}\"\n  next_hop_ip = \"10.240.0.2${count.index}\"\n  dest_range  = \"10.200.${count.index}.0/24\"\n}\n</code></pre>"},{"location":"kubernetes/khw-gce/terraform-compute/#variables_1","title":"Variables","text":"<pre><code>variable \"num\" {\n  description = \"The number of controller VMs\"\n}\n\nvariable \"machine_type\" {\n  description = \"The type of VM for controllers\"\n}\n\nvariable \"zone\" {\n  description = \"The zone to create the controllers in\"\n}\n\nvariable \"network\" {\n  description = \"The network to use for routes\"\n}\n\nvariable \"subnet\" {\n  description = \"The subnet to create the nic in\"\n}\n</code></pre>"},{"location":"kubernetes/khw-gce/terraform-compute/#health-check","title":"Health check","text":"<p>Because we will have three controllers, we have to make sure that GKE forwards Kubernetes API requests to each of them via our public IP address.</p> <p>We do this via a http health check, wich involves a forwarding rule and a target pool. Target pool being the group of controller VM's for which the forwarding rule is active.</p> <pre><code>resource \"google_compute_target_pool\" \"khw-hc-target-pool\" {\n  name = \"instance-pool\"\n\n  # TODO: fixed set for now, maybe we can make this dynamic some day\n  instances = [\n    \"${var.region_default_zone}/controller-0\",\n    \"${var.region_default_zone}/controller-1\",\n    \"${var.region_default_zone}/controller-2\",\n  ]\n\n  health_checks = [\n    \"${google_compute_http_health_check.khw-health-check.name}\",\n  ]\n}\n\nresource \"google_compute_http_health_check\" \"khw-health-check\" {\n  name         = \"kubernetes\"\n  request_path = \"/healthz\"\n  description  = \"The health check for Kubernetes API server\"\n  host         = \"${var.kubernetes-cluster-dns}\"\n}\n\nresource \"google_compute_forwarding_rule\" \"khw-hc-forward\" {\n  name       = \"kubernetes-forwarding-rule\"\n  target     = \"${google_compute_target_pool.khw-hc-target-pool.self_link}\"\n  region     = \"${var.region}\"\n  port_range = \"6443\"\n  ip_address = \"${google_compute_address.khw-lb-public-ip.self_link}\"\n}\n</code></pre>"},{"location":"kubernetes/khw-gce/terraform-compute/#apply-terraform-state","title":"Apply Terraform state","text":"<p>In the end, our configuration should consist out of several <code>.tf</code> files and look something like this.</p> <pre><code>ls -lath\ndrwxr-xr-x  27 joostvdg  staff   864B Aug 26 12:50 .\ndrwxr-xr-x  20 joostvdg  staff   640B Aug 22 14:47 ..\ndrwxr-xr-x   4 joostvdg  staff   128B Aug  7 22:43 modules\n-rw-r--r--   1 joostvdg  staff   1.5K Aug 26 12:50 variables.tf\n-rw-r--r--   1 joostvdg  staff   1.3K Aug 17 16:03 firewall.tf\n-rw-r--r--   1 joostvdg  staff   4.4K Aug 17 12:06 worker-config.md\n-rw-r--r--   1 joostvdg  staff   1.6K Aug 17 09:35 healthcheck.tf\n-rw-r--r--   1 joostvdg  staff   517B Aug 16 17:09 nodes.tf\n-rw-r--r--   1 joostvdg  staff    92B Aug 16 13:52 publicip.tf\n-rw-r--r--   1 joostvdg  staff   365B Aug  7 22:07 vpc.tf\n-rw-r--r--   1 joostvdg  staff   189B Aug  7 16:51 base.tf\ndrwxr-xr-x   5 joostvdg  staff   160B Aug  7 21:52 .terraform\n-rw-r--r--   1 joostvdg  staff     0B Aug  7 18:28 terraform.tfstate\n</code></pre> <p>We're now going to <code>plan</code> and then <code>apply</code> our Terraform configuration to create the resources in GCE.</p> <pre><code>terraform plan\n</code></pre> <pre><code>terraform apply\n</code></pre>"},{"location":"kubernetes/khw-gce/worker/","title":"Worker Installation","text":""},{"location":"kubernetes/khw-gce/worker/#install-base-components","title":"Install base components","text":""},{"location":"kubernetes/khw-gce/worker/#download","title":"Download","text":"<pre><code>wget -q --show-progress --https-only --timestamping \\\n    https://github.com/kubernetes-incubator/cri-tools/releases/download/v1.0.0-beta.0/crictl-v1.0.0-beta.0-linux-amd64.tar.gz \\\n    https://storage.googleapis.com/kubernetes-the-hard-way/runsc \\\n    https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \\\n    https://github.com/containernetworking/plugins/releases/download/v0.7.1/cni-plugins-amd64-v0.7.1.tgz \\\n    https://github.com/containerd/containerd/releases/download/v1.1.2/containerd-1.1.2.linux-amd64.tar.gz \\\n    https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl \\\n    https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-proxy \\\n    https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubelet\n</code></pre>"},{"location":"kubernetes/khw-gce/worker/#prepare-landing-folders","title":"Prepare landing folders","text":"<pre><code>sudo mkdir -p \\\n    /etc/cni/net.d \\\n    /opt/cni/bin \\\n    /var/lib/kubelet \\\n    /var/lib/kube-proxy \\\n    /var/lib/kubernetes \\\n    /var/run/kubernetes \\\n    /etc/containerd/\n</code></pre>"},{"location":"kubernetes/khw-gce/worker/#unpack-to-folders","title":"Unpack to folders","text":"<pre><code>chmod +x kubectl kube-proxy kubelet runc.amd64 runsc\n    sudo mv runc.amd64 runc\n    sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/\n    sudo tar -xvf crictl-v1.0.0-beta.0-linux-amd64.tar.gz -C /usr/local/bin/\n    sudo tar -xvf cni-plugins-amd64-v0.7.1.tgz -C /opt/cni/bin/\n    sudo tar -xvf containerd-1.1.2.linux-amd64.tar.gz -C /\n</code></pre>"},{"location":"kubernetes/khw-gce/worker/#list-variables","title":"List variables","text":"<pre><code>POD_CIDR=$(curl -s -H \"Metadata-Flavor: Google\" \\\n    http://metadata.google.internal/computeMetadata/v1/instance/attributes/pod-cidr)\n\necho HOSTNAME=$HOSTNAME\necho POD_CIDR=$POD_CIDR\n</code></pre>"},{"location":"kubernetes/khw-gce/worker/#configure-containerd","title":"Configure ContainerD","text":""},{"location":"kubernetes/khw-gce/worker/#runtime-configuration-file","title":"Runtime configuration file","text":"<pre><code>cat &lt;&lt; EOF | sudo tee /etc/containerd/config.toml\n[plugins]\n  [plugins.cri.containerd]\n    snapshotter = \"overlayfs\"\n    [plugins.cri.containerd.default_runtime]\n      runtime_type = \"io.containerd.runtime.v1.linux\"\n      runtime_engine = \"/usr/local/bin/runc\"\n      runtime_root = \"\"\n    [plugins.cri.containerd.untrusted_workload_runtime]\n      runtime_type = \"io.containerd.runtime.v1.linux\"\n      runtime_engine = \"/usr/local/bin/runsc\"\n      runtime_root = \"/run/containerd/runsc\"\nEOF\n</code></pre>"},{"location":"kubernetes/khw-gce/worker/#systemd-service-configuration-file","title":"SystemD service configuration file","text":"<pre><code>cat &lt;&lt;EOF | sudo tee /etc/systemd/system/containerd.service\n[Unit]\nDescription=containerd container runtime\nDocumentation=https://containerd.io\nAfter=network.target\n\n[Service]\nExecStartPre=/sbin/modprobe overlay\nExecStart=/bin/containerd\nRestart=always\nRestartSec=5\nDelegate=yes\nKillMode=process\nOOMScoreAdjust=-999\nLimitNOFILE=1048576\nLimitNPROC=infinity\nLimitCORE=infinity\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre>"},{"location":"kubernetes/khw-gce/worker/#configure-cni","title":"Configure CNI","text":"<p>Warning</p> <p>We do not need to configure cni as we will setup Weave and it will do the necessary setup automagically.</p>"},{"location":"kubernetes/khw-gce/worker/#configure-kubelet","title":"Configure Kubelet","text":""},{"location":"kubernetes/khw-gce/worker/#move-certificates-to-correct-places","title":"Move certificates to correct places","text":"<pre><code>sudo mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/\nsudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig\nsudo mv ca.pem /var/lib/kubernetes/\n</code></pre>"},{"location":"kubernetes/khw-gce/worker/#create-k8s-yaml-configuration","title":"Create k8s yaml configuration","text":"<pre><code>cat &lt;&lt;EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml\nkind: KubeletConfiguration\napiVersion: kubelet.config.k8s.io/v1beta1\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    enabled: true\n  x509:\n    clientCAFile: \"/var/lib/kubernetes/ca.pem\"\nauthorization:\n  mode: Webhook\nclusterDomain: \"cluster.local\"\nclusterDNS:\n  - \"10.32.0.10\"\npodCIDR: \"${POD_CIDR}\"\nruntimeRequestTimeout: \"15m\"\ntlsCertFile: \"/var/lib/kubelet/${HOSTNAME}.pem\"\ntlsPrivateKeyFile: \"/var/lib/kubelet/${HOSTNAME}-key.pem\"\nEOF\n</code></pre>"},{"location":"kubernetes/khw-gce/worker/#systemd-service-configuration-file_1","title":"SystemD service configuration file","text":"<p>Warning</p> <p>One thing I see missing from your kubelet configuration is  <code>--non-masquerade-cidr flag.</code> Kubelet needs to be run with this option for traffic to outside clusterIP range. Refer here - kubenet</p> <pre><code>Kubelet should also be run with the `--non-masquerade-cidr=&lt;clusterCidr&gt;` argument to ensure traffic to IPs outside this range will use IP masquerade.\n</code></pre> <p>Not sure, if this is the cause, but looks like this is a requirement and is missing from the Kubelet config.</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/systemd/system/kubelet.service\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=containerd.service\nRequires=containerd.service\n\n[Service]\nExecStart=/usr/local/bin/kubelet \\\\\n  --allow-privileged=true \\\\\n  --config=/var/lib/kubelet/kubelet-config.yaml \\\\\n  --container-runtime=remote \\\\\n  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\\n  --image-pull-progress-deadline=2m \\\\\n  --kubeconfig=/var/lib/kubelet/kubeconfig \\\\\n  --network-plugin=cni \\\\\n  --register-node=true \\\\\n  --v=2\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre>"},{"location":"kubernetes/khw-gce/worker/#kube-proxy","title":"Kube-Proxy","text":""},{"location":"kubernetes/khw-gce/worker/#move-kubeconfig","title":"Move kubeconfig","text":"<pre><code>sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig\n</code></pre>"},{"location":"kubernetes/khw-gce/worker/#create-k8s-yaml-config","title":"Create k8s yaml config","text":"<pre><code>cat &lt;&lt;EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml\nkind: KubeProxyConfiguration\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nclientConnection:\n    kubeconfig: \"/var/lib/kube-proxy/kubeconfig\"\nmode: \"iptables\"\nclusterCIDR: \"10.200.0.0/16\"\nEOF\n</code></pre>"},{"location":"kubernetes/khw-gce/worker/#create-systemd-service","title":"Create SystemD service","text":"<pre><code>cat &lt;&lt;EOF | sudo tee /etc/systemd/system/kube-proxy.service\n[Unit]\nDescription=Kubernetes Kube Proxy\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nExecStart=/usr/local/bin/kube-proxy \\\\\n    --config=/var/lib/kube-proxy/kube-proxy-config.yaml \\\\\n    --v=2\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre>"},{"location":"kubernetes/khw-gce/worker/#configure-and-start-the-systemd-services","title":"Configure and start the SystemD services","text":"<pre><code>sudo systemctl daemon-reload\nsudo systemctl enable containerd kubelet kube-proxy\nsudo systemctl start containerd kubelet kube-proxy\n</code></pre>"},{"location":"kubernetes/khw-gce/worker/#validate","title":"Validate","text":"<p>Note</p> <p>Run this from a machine outside the cluster, with access to the admin kubeconfig.</p> <pre><code>gcloud compute ssh controller-0 --command \"kubectl get nodes --kubeconfig admin.kubeconfig\"\n</code></pre> <p>Note</p> <p>As we didn't configure networking yet, the nodes should be shown as <code>NotReady</code> status.</p>"},{"location":"linux/","title":"Linux","text":""},{"location":"linux/iptables/","title":"iptables","text":""},{"location":"linux/networking/","title":"Networking","text":""},{"location":"linux/systemd/","title":"SystemD","text":"<p>Increasingly, Linux distributions are adopting or planning to adopt the <code>systemd</code> init system.  This powerful suite of software can manage many aspects of your server,   from services to mounted devices and system states. <sup>1</sup></p>"},{"location":"linux/systemd/#concepts","title":"Concepts","text":""},{"location":"linux/systemd/#unit","title":"Unit","text":"<p>In <code>systemd</code>, a <code>unit</code> refers to any resource that the system knows how to operate on and manage.   This is the primary object that the <code>systemd</code> tools know how to deal with.    These resources are defined using configuration files called unit files. <sup>1</sup></p>"},{"location":"linux/systemd/#path","title":"Path","text":"<p>A path unit defines a filesystem <code>path</code> that <code>systmed</code> can monitor for changes.   Another unit must exist that will be be activated when certain activity is detected at the path location.   Path activity is determined through <code>inotify events</code>.</p> <p>My idea, you can use this for those services that should trigger on file uploads or backup dumps. Although I wonder if the Unit's main service knows which path was triggered? If it does, than it's easy, else you still need a \"file walker\".</p>"},{"location":"linux/systemd/#example","title":"Example","text":"<pre><code>[Unit]\nDescription=Timezone Helper Service\nAfter=network.target\nStartLimitIntervalSec=0\n\n[Service]\nType=simple\nRestart=always\nRestartSec=3\nUser=joostvdg\nExecStart=/usr/bin/timezone_helper_service\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"linux/systemd/#resources","title":"Resources","text":"<ul> <li>https://www.linuxjournal.com/content/linux-filesystem-events-inotify</li> </ul>"},{"location":"linux/systemd/#references","title":"References","text":"<ol> <li> <p>Introduction to systemd from Digital Ocean \u21a9\u21a9</p> </li> </ol>"},{"location":"openshift/rhos311-gcp-medium/","title":"RedHat OpenShift 3.11 on GCP - Medium","text":"<p>Initially, I wrote a guide to get RedHat OpenShift 3.11 running on Google Cloud (GCP) with minimal effort. It is based on the GCP guide from RedHat<sup>1</sup>, but limiting the configuration options to the bare minimum.</p> <p>So, why another guide? Well, I wanted to go a few steps further. Creating a separated network to make the cluster more secure. I also want to ensure the Domain Name, IP, and load balancing are automated to improve the OpenShift installation.</p> <p>I will guide you in creating a secured network, with an automated DNS configuration,  and automated TLS for our endpoints.</p> <p>Note</p> <p>This guide is written early March 2020, using <code>jx</code> version <code>2.0.1212</code> and OpenShift version <code>v3.11.170</code>.</p>"},{"location":"openshift/rhos311-gcp-medium/#pre-requisites","title":"Pre-requisites","text":"<p>What do we need to get started?</p> <ul> <li>active GCP account with billing enabled</li> <li>GCP project</li> <li>gcloud tool installed</li> <li>terraform <code>0.12</code>+  installed</li> <li>active RedHat account<ul> <li>which hasn't used its 60 day trial license of OpenShift yet</li> </ul> </li> <li>domain name registration which can forward subdomains to name servers (class <code>CNAME</code>)</li> </ul> <p>Important</p> <p>This doc contains a prefix for the VM's created in GCP. This prefix is set to <code>&lt;prefix&gt;</code>. Next to that, the user used, is set to <code>my_user</code>.</p> <p>You probably want to change that, so please take care with copy-pasting!</p>"},{"location":"openshift/rhos311-gcp-medium/#process","title":"Process","text":"<ul> <li>create Terraform configuration for GCP VM's</li> <li>create the VM's in GCP with RedHat Enterprise Linux v7</li> <li>configure dns</li> <li>install OpenShift pre-requisites on each VM</li> <li>create OpenShift Ansible configuration</li> <li>install OpenShift via Ansible</li> <li>create initial users</li> </ul>"},{"location":"openshift/rhos311-gcp-medium/#gcp-terraform","title":"GCP Terraform","text":""},{"location":"openshift/rhos311-gcp-medium/#what-do-we-need","title":"What Do We Need","text":"<p>Having gone through the process of installing RHOS 3.11 once, I ran into an issue. The documentation makes it seems you only need <code>master</code> nodes, <code>compute</code> nodes and VM's for <code>etcd</code> (can be the same as <code>Master</code>). However, you also need at least one <code>infra</code> node.</p> <p>You can opt for a HA cluster, with three <code>master</code> nodes, or a single <code>master</code> node for a test cluster. I'm going with the latter. The <code>master</code> node will house the Kubernetes Control Plane, the <code>infra</code> node will house the OpenShift infra. As we won't have cluster autoscaling - a bit fancy for a manual test cluster - we have to make sure the machines are beefy to take the entire workload.</p> <p>Another thing we need for OpenShift, is having DNS that works between the nodes. For example, you should be able to say <code>node1</code> and end up at the correct machine. Due to GCP networking, this internal DNS works out-of-the-box for any machine with our network/project.</p> <p>Important</p> <p>Our machines need to have unique names!</p> <p>So I ended up with the following:</p> <ul> <li>1x  <code>master</code> node -&gt; <code>n1-standard-8</code>: 8 cores, 30gb mem</li> <li>1x <code>infra</code> node -&gt;  <code>n1-standard-8</code>: 8 cores, 30gb mem</li> <li>4x <code>compute</code> node -&gt; <code>n1-standard-4</code>: 4 cores, 15gb mem (each)</li> </ul>"},{"location":"openshift/rhos311-gcp-medium/#vm-image","title":"VM Image","text":"<p>Of course, if you want to run RedHat OpenShift Enterprise (RHOS), your VM's need to run a RedHat Enterprise Linux distribution(RHEL).</p> <p>In order to figure out which vm images are currently available, you can issue the following command via <code>gcloud</code>.</p> <pre><code>gcloud compute images list --project rhel-cloud\n</code></pre> <p>Which should give a response like this:</p> <pre><code>NAME                                                  PROJECT            FAMILY                            DEPRECATED  STATUS\nrhel-6-v20200205                                      rhel-cloud         rhel-6                                        READY\nrhel-7-v20200205                                      rhel-cloud         rhel-7                                        READY\nrhel-8-v20200205                                      rhel-cloud         rhel-8                                        READY\n</code></pre> <p>For the VM image in our Terraform configuration, we will use the <code>NAME</code> of the image. For RHOS 3.11, RedHat strongly recommends using RHEL 7, so we use <code>rhel-7-v20200205</code>.</p>"},{"location":"openshift/rhos311-gcp-medium/#terraform-configuration","title":"Terraform Configuration","text":"<p>Google Cloud networking can get a bit complicated. In order to make the configuration easier to digest, I've created the Terraform configuration in the form of Terraform Modules<sup>2</sup>.</p> <p>There are four modules:</p> <ul> <li>Instance: VM image definition</li> <li>IP: static IP's and CloudDNS configuration</li> <li>LB: load balancers, port forwards and health checks </li> <li>VPC: <code>vpc</code>, subnet, and firewalls</li> </ul> <p>There are two main benefits to using the modules;</p> <ul> <li>the modules means we can reuse the terraform code - especially nice for the instances</li> <li>it is easier to port the outcome of one resource - for example an IP address - as input to another</li> </ul> <p>For this case, each module - except <code>IP</code> - will have three files:</p> <ol> <li>main.tf: contains the resource definitions of this module</li> <li>variables.tf: contains the input for the module</li> <li>outputs.tf: export values from created resources, to be used by other modules/resources</li> </ol> <p>In the main folder we have:</p> <ul> <li>main.tf: creating the resources via the modules</li> <li>variables.tf: input variables</li> <li>storage.tf: Google Bucket<sup>3</sup> as backing storage for the OpenShift (Docker) Registry<sup>4</sup></li> </ul>"},{"location":"openshift/rhos311-gcp-medium/#model","title":"Model","text":""},{"location":"openshift/rhos311-gcp-medium/#terraform-module-ip","title":"Terraform Module IP","text":"main.tf <pre><code>resource \"google_compute_global_address\" \"master\" {\n    name = \"master\"\n}\n\nresource \"google_compute_address\" \"nodes\" {\n    name = \"nodes\"\n}\n\nresource \"google_dns_record_set\" \"ocp\" {\n    name = \"api.${google_dns_managed_zone.my_ocp_domain.dns_name}\"\n    type = \"A\"\n    ttl  = 300\n    managed_zone = google_dns_managed_zone.my_ocp_domain.name\n    rrdatas = [google_compute_global_address.master.address]\n}\n\nresource \"google_dns_record_set\" \"apps\" {\n    name = \"*.apps.${google_dns_managed_zone.my_ocp_domain.dns_name}\"\n    type = \"A\"\n    ttl  = 300\n    managed_zone = google_dns_managed_zone.my_ocp_domain.name\n    rrdatas = [google_compute_address.nodes.address]\n}\n\nresource \"google_dns_managed_zone\" \"my_ocp_domain\" {\n    name     = \"my-ocp-domain\"\n    dns_name = \"my.ocp.domain.\"\n}\n</code></pre> outputs.tf <pre><code>output \"master_ip\" {\n    value = google_compute_global_address.master.address\n}\n\noutput \"nodes_ip\" {\n    value = google_compute_address.nodes.address\n}\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#terraform-module-vpc","title":"Terraform Module VPC","text":"main.tf <pre><code>resource \"google_compute_network\" \"vpc_network\" {\n    name                                 =  var.vpc_name\n    auto_create_subnetworks = \"false\"\n    routing_mode                   = \"GLOBAL\"\n}\n\nresource \"google_compute_subnetwork\" \"vpc_subnet_int\" {\n    name                         =  var.network_name_int\n    ip_cidr_range            = var.network_name_int_range\n    network                     = google_compute_network.vpc_network.self_link\n    region                        = var.region\n    private_ip_google_access = true\n} \n\nresource \"google_compute_firewall\" \"fw_access\" {\n    name    = \"my-rhos311-fw-ext\"\n    network = google_compute_network.vpc_network.name\n\n    allow {\n        protocol = \"icmp\"\n    }\n\n    allow {\n        protocol = \"tcp\"\n        ports    = [\"22\", \"80\", \"443\"]\n    }\n    source_ranges = [\"0.0.0.0/0\"]\n}\n\nresource \"google_compute_firewall\" \"fw_gcp_health_checks\" {\n    name    = \"my-rhos311-fw-gcp-health-checks\"\n    network = google_compute_network.vpc_network.name\n\n    allow {\n        protocol = \"tcp\"\n        ports    = [\"0-65535\"]\n    }\n\n    source_ranges = [\"35.191.0.0/16\",\"130.211.0.0/22\"]\n}\n\nresource \"google_compute_firewall\" \"allow-internal\" {\n    name    = \"my-rhos311-fw-int\"\n    network = google_compute_network.vpc_network.name\n\n    allow {\n        protocol = \"icmp\"\n    }\n\n    allow {\n        protocol = \"tcp\"\n        ports    = [\"0-65535\"]\n    }\n\n    allow {\n        protocol = \"udp\"\n        ports    = [\"0-65535\"]\n    }\n    source_ranges = [\n        var.network_name_int_range\n    ]\n}\n</code></pre> variables.tf <pre><code>variable \"region\" {}\nvariable \"vpc_name\" {}\nvariable \"network_name_int\" {}\n\nvariable \"network_name_int_range\" {\n    default = \"10.26.1.0/24\"\n}\n</code></pre> outputs.tf <pre><code>output \"vpc_link\" {\n    value = \"${google_compute_network.vpc_network.self_link}\"\n}\n\noutput \"vpc_subnet_int\" {\n    value =   \"${google_compute_subnetwork.vpc_subnet_int.self_link}\"\n}\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#terraform-module-lb","title":"Terraform Module LB","text":"main.tf <pre><code>resource \"google_compute_health_check\" \"master\" {\n    name         = \"https\"\n    healthy_threshold = 3\n    unhealthy_threshold = 3\n    timeout_sec        = 10\n    check_interval_sec = 10\n    https_health_check {\n        request_path = \"/healthz\"\n        port = \"443\"\n    }\n}\n\nresource \"google_compute_http_health_check\" \"nodes\" {\n    name         = \"nodes\"\n    request_path = \"/healthz\"\n    port = 1936\n    healthy_threshold = 3\n    unhealthy_threshold = 3\n    timeout_sec        = 10\n    check_interval_sec = 10\n}\n\nresource \"google_compute_backend_service\" \"master\" {\n    name          = \"backend-service-master\"\n    health_checks = [ \"${google_compute_health_check.master.self_link}\" ]\n    # load_balancing_scheme  = \"EXTERNAL\"\n    protocol = \"TCP\"\n    session_affinity =  \"CLIENT_IP\"\n    port_name   = \"ocp-api\"\n    backend {\n        group = var.instance_group_master\n    }\n}\n\nresource \"google_compute_target_tcp_proxy\" \"master_tcp_proxy\" {\n    name            = \"master-tcp-proxy\"\n    backend_service = google_compute_backend_service.master.self_link\n}\n\nresource \"google_compute_global_forwarding_rule\" \"master_forwarding\" {\n    name                  = \"master-forwarding\"\n    ip_address         = var.master_ip\n    target                = google_compute_target_tcp_proxy.master_tcp_proxy.self_link\n    port_range            = 443\n}\n\nresource \"google_compute_target_pool\" \"nodes\" {\n    name = \"nodes\"\n\n    instances = var.instance_group_nodes\n\n    health_checks = [\n        google_compute_http_health_check.nodes.name\n    ]\n}\n\nresource \"google_compute_forwarding_rule\" \"network-load-balancer-http\" {\n    name                  = \"network-load-balancer-http\"\n    ip_address         = var.nodes_ip\n    target                  =   google_compute_target_pool.nodes.self_link\n    port_range            = \"80\"\n    ip_protocol           = \"TCP\"\n}\n\nresource \"google_compute_forwarding_rule\" \"network-load-balancer-https\" {\n    name                  = \"network-load-balancer-https\"\n    ip_address         = var.nodes_ip\n    target                =     google_compute_target_pool.nodes.self_link\n    port_range            = \"443\"\n    ip_protocol           = \"TCP\"\n}\n</code></pre> variables.tf <pre><code>variable \"region\" {}\nvariable \"instance_group_master\" {}\nvariable \"instance_group_nodes\" {}\nvariable \"master_ip\" {}\nvariable \"nodes_ip\" {}\n</code></pre> outputs.tf <pre><code>output \"health_check_master\" {\n    value =  \"${google_compute_health_check.master.self_link}\"\n}\n\noutput \"health_check_nodes\" {\n    value =  \"${google_compute_http_health_check.nodes.self_link}\"\n}\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#terraform-module-instance","title":"Terraform Module Instance","text":"main.tf <pre><code>resource \"google_compute_instance\" \"instance\" {\n    name         = \"${var.prefix}-${var.instance_name}\"\n    machine_type = var.machine_type\n    zone         = var.zone\n    allow_stopping_for_update = true\n\n    boot_disk {\n        initialize_params {\n            image = var.vm_image\n            size = var.disk_size\n            type = \"pd-ssd\"\n        }\n    }\n\n    network_interface {\n        network = var.network\n        subnetwork = var.vpc_subnet_int\n        access_config  {\n            # gives the node an external IP address\n            # not sure if this is still required?\n        }\n    }\n\n    metadata = {\n        ssh-keys = var.ssh_key\n    }\n    service_account {\n        scopes = [\n            \"https://www.googleapis.com/auth/compute\",\n            \"https://www.googleapis.com/auth/devstorage.read_only\",\n            \"https://www.googleapis.com/auth/logging.write\",\n            \"https://www.googleapis.com/auth/monitoring\",\n            \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\",\n            \"https://www.googleapis.com/auth/cloud-platform\"\n        ]\n    }\n}\n</code></pre> variables.tf <pre><code>variable \"machine_type\" {\n    default = \"n1-standard-4\"\n}\n\nvariable \"vm_image\" {\n    default =\"rhel-7-v20200205\"\n}\n\nvariable \"disk_size\" {\n    default = 100\n}\n\nvariable \"ssh_key\" {}\nvariable \"prefix\" {}\nvariable \"instance_name\" { }\nvariable \"zone\" {}\nvariable \"network\" {}\nvariable \"vpc_subnet_ext\" {}\nvariable \"vpc_subnet_int\" {}\n</code></pre> outputs.tf <pre><code>output \"link\" {\n    value = \"${google_compute_instance.instance.self_link}\"\n}\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#terraform-main","title":"Terraform Main","text":"main.tf <pre><code>terraform {\n    required_version = \"~&gt; 0.12\"\n}\n\n# https://www.terraform.io/docs/providers/google/index.html\nprovider \"google\" {\n    version   = \"~&gt; 2.18.1\"\n    project   = var.project\n    region    =  var.region\n    zone      = \"europe-west4-b\"\n}\n\nmodule \"vpc\" {\n    source = \"./modules/vpc\"\n    region = var.region\n    vpc_name = \"vpc-my-rhos311\"\n    network_name_int = \"network-my-rhos311-int\"\n    network_name_ext = \"network-my-rhos311-ext\"\n}\n\nmodule \"master1\" {\n    source = \"./modules/instance\"\n    machine_type = \"n1-standard-8\"\n    instance_name = \"master1\"\n    prefix = var.instance_prefix\n    network = module.vpc.vpc_link\n    vpc_subnet_int = module.vpc.vpc_subnet_int\n    vpc_subnet_ext = module.vpc.vpc_subnet_ext\n    zone = var.zone\n    ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\"\n}\n\nmodule \"infra1\" {\n    source = \"./modules/instance\"\n    machine_type = \"n1-standard-8\"\n    instance_name = \"infra1\"\n    prefix = var.instance_prefix\n    network = module.vpc.vpc_link\n    vpc_subnet_int = module.vpc.vpc_subnet_int\n    vpc_subnet_ext = module.vpc.vpc_subnet_ext\n    zone = var.zone\n    ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\"\n}\n\nmodule \"node1\" {\n    source = \"./modules/instance\"\n    instance_name = \"node1\"\n    prefix = var.instance_prefix\n    network = module.vpc.vpc_link\n    vpc_subnet_int = module.vpc.vpc_subnet_int\n    vpc_subnet_ext = module.vpc.vpc_subnet_ext\n    zone = var.zone\n    ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\"\n}\n\nmodule \"node2\" {\n    source = \"./modules/instance\"\n    instance_name = \"node2\"\n    prefix = var.instance_prefix\n    network = module.vpc.vpc_link\n    vpc_subnet_int = module.vpc.vpc_subnet_int\n    vpc_subnet_ext = module.vpc.vpc_subnet_ext\n    zone = var.zone\n    ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\"\n}\n\nmodule \"node3\" {\n    source = \"./modules/instance\"\n    instance_name = \"node3\"\n    prefix = var.instance_prefix\n    network = module.vpc.vpc_link\n    vpc_subnet_int = module.vpc.vpc_subnet_int\n    vpc_subnet_ext = module.vpc.vpc_subnet_ext\n    zone = var.zone\n    ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\"\n}\n\nmodule \"node4\" {\n    source = \"./modules/instance\"\n    instance_name = \"node4\"\n    prefix = var.instance_prefix\n    network = module.vpc.vpc_link\n    vpc_subnet_int = module.vpc.vpc_subnet_int\n    vpc_subnet_ext = module.vpc.vpc_subnet_ext\n    zone = var.zone\n    ssh_key = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\"\n}\n\nresource \"google_compute_instance_group\" \"master\" {\n    name = \"my-rhos311-master\"\n    zone =  var.master_zone\n    instances = [\n        module.master1.link,\n    ]\n\n    named_port {\n        name = \"ocp-api\"\n        port = \"443\"\n    }\n}\n\nresource \"google_compute_instance_group\" \"nodes\" {\n    name = \"my-rhos311-nodes\"\n    zone =  var.master_zone\n    instances = [\n        module.infra1.link,\n        module.node1.link,\n        module.node2.link,\n        module.node3.link,\n        module.node4.link\n    ]\n\n    named_port {\n        name = \"ocp-api\"\n        port = \"1936\"\n    }\n}\nmodule \"ips\" {\n    source = \"./modules/ip\"\n}\n\nmodule \"lb\" {\n    source = \"./modules/lb\"\n    region = var.region\n    instance_group_master = \"${google_compute_instance_group.master.self_link}\"\n    instance_group_nodes = [\n    \"${var.zone}/${var.instance_prefix}-infra1\",\n    \"${var.zone}/${var.instance_prefix}-node1\",\n    \"${var.zone}/${var.instance_prefix}-node2\",\n    \"${var.zone}/${var.instance_prefix}-node3\",\n    \"${var.zone}/${var.instance_prefix}-node4\"\n    ]\n    master_ip = module.ips.master_ip\n    nodes_ip =  module.ips.nodes_ip\n}\n</code></pre> variables.tf <pre><code>variable \"project\" { }\n\nvariable \"region\" {\n    default =\"europe-west4\"\n}\n\nvariable \"name\" {\n    description = \"The name of the cluster (required)\"\n    default     = \"jx-openshift-311\"\n}\n\nvariable \"compute_machine_type\" {\n    default = \"n1-standard-4\"\n}\n\nvariable \"master_machine_type\" {\n    default = \"n1-standard-8\"\n}\n\nvariable \"instance_prefix\" {\n    default =\"my-rhos311\"\n}\n\nvariable \"vm_image\" {\ndefault =\"rhel-7-v20200205\"\n}\n\nvariable \"master_zone\" {\n    description = \"Zone in which the Master Node will be created\"\n    default = \"europe-west4-a\"\n}\n\nvariable \"zone\" {\n    default = \"europe-west4-a\"\n}\n</code></pre> storage.tf <pre><code>resource \"google_storage_bucket\" \"ocp-registry\" {\n    name          = \"my-rhos311-ocp-registry\"\n    location      = \"EU\"\n}\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#create-vms-with-terraform","title":"Create VMs with Terraform","text":"<pre><code>terraform init\n</code></pre> <pre><code>terraform validate\n</code></pre> <pre><code>terraform plan -out plan.out\n</code></pre> <pre><code>terraform apply \"plan.out\"\n</code></pre> <p>We should end up with six nodes:</p> <ol> <li><code>&lt;prefix&gt;-master1</code></li> <li><code>&lt;prefix&gt;-infra1</code></li> <li><code>&lt;prefix&gt;-node1</code></li> <li><code>&lt;prefix&gt;-node2</code></li> <li><code>&lt;prefix&gt;-node3</code></li> <li><code>&lt;prefix&gt;-node4</code></li> </ol>"},{"location":"openshift/rhos311-gcp-medium/#verify-vms","title":"Verify VMs","text":"<p>Before we can install the OpenShift pre-requisites, we verify if the VMs are ready to use. To verify the VMs, we will do the following:</p> <ol> <li>confirm we can <code>ssh</code> into each VM</li> <li>confirm we can use <code>sudo</code> in each VM</li> <li>confirm the infra node can call each VM by name (<code>&lt;prefix&gt;-infra1</code>, <code>&lt;prefix&gt;-master1</code>, <code>&lt;prefix&gt;-node1</code>, <code>&lt;prefix&gt;-node2</code>, <code>&lt;prefix&gt;-node3</code>, <code>&lt;prefix&gt;-node4</code>)</li> <li>confirm the infra node can <code>ssh</code> into all VMs (including itself!)</li> </ol>"},{"location":"openshift/rhos311-gcp-medium/#ssh-into-vms","title":"SSH into VMs","text":"<p>There are several ways to ssh into the VMs. You can do so via <code>ssh</code> installed on your machine, you can do so via the GCP console. I will use another option, using the <code>gcloud</code> CLI, using the ssh key I've configured in Terrafom (<code>ssh-keys = \"my_user:${file(\"~/.ssh/id_rsa.pub\")}\"</code>).</p> <p>Why am I using this form? Well, it makes it easier to reason about which machine I ssh into, as I can use the VM name.</p> <pre><code># your google project id\nPROJECT_ID=\n</code></pre> <pre><code># the google zone the vm is in, for example: europe-west-4a\nVM_ZONE=\n</code></pre> <pre><code>gcloud beta compute --project $PROJECT_ID ssh --zone $VM_ZONE \"&lt;prefix&gt;-infra1\"\n</code></pre> <p>Confirm you can ssh into each VM by changing the zone/name accordingly.</p>"},{"location":"openshift/rhos311-gcp-medium/#confirm-sudo","title":"Confirm Sudo","text":"<p>Our ssh user isn't root - as it should be - so we need to use sudo for some tasks.</p> <p>Confirm sudo works;</p> <pre><code>sudo cat /etc/locale.conf\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#confirm-local-dns","title":"Confirm Local DNS","text":"<p>The OpenShift installation process and later OpenShift itself, relies on local dns. This means, it assumes if there's a node called <code>&lt;prefix&gt;-master1</code>, it can do <code>ssh &lt;prefix&gt;-master1</code> and it works.</p> <p>In GCP, DNS works within a Project by default. So assuming all the VMs  are within the same project this works out-of-the-box. But, to avoid any surprises later, confirm it.</p> <pre><code>[my_user@master ~]$ ping &lt;prefix&gt;-master1\nPING master.c.MY_PROJECT_ID.internal (10.164.0.49) 56(84) bytes of data.\n64 bytes from &lt;prefix&gt;-master1.c.MY_PROJECT_ID.internal (10.164.0.49): icmp_seq=1 ttl=64 time=0.041 ms\n64 bytes from &lt;prefix&gt;-master1.c.MY_PROJECT_ID.internal (10.164.0.49): icmp_seq=2 ttl=64 time=0.094 ms\n</code></pre> <p>Note</p> <p>As you might expect, <code>MY_PROJECT_ID</code> will be the Google project Id where your VMs are. I've hidden that as a safety precaution, confirm it looks correct!</p>"},{"location":"openshift/rhos311-gcp-medium/#infra-node-can-ssh-into-others","title":"Infra Node can SSH into others","text":"<p>For the OpenShift installation, our installation VM has to be able to ssh into every other VM<sup>3</sup>. This doesn't work out of the box.</p> <p>Warning</p> <p>I used my own keys here directly, because this is a temporary project only used by me. If your usecase is different, and you're not sure how to proceed, consult a security professional!</p> <p>We have to create the <code>ssh</code> public key on every node for our ssh user (in my case, <code>my_user</code>) and the private also for our installation host (for example, <code>&lt;prefix&gt;-infra1</code>).</p> <p>This might not be a security best practice, but I did this by copying over my <code>~/.ssh/id_rsa</code> and <code>~/.ssh/id_rsa.pub</code> to each node's user home (<code>/home/my_user/.ssh/</code>).</p> <p>Important</p> <p>Once you've done this, ssh into the <code>infra</code> node, and confirm it can ssh to every other node.</p> <ul> <li><code>ssh my_user@&lt;prefix&gt;-node1</code></li> <li><code>ssh my_user@&lt;prefix&gt;-node2</code></li> <li><code>ssh my_user@&lt;prefix&gt;-node3</code></li> <li><code>ssh my_user@&lt;prefix&gt;-node4</code></li> <li><code>ssh my_user@&lt;prefix&gt;-master1</code></li> <li><code>ssh my_user@&lt;prefix&gt;-infra1</code> -&gt; YES, you have to ssh into yourself!</li> </ul> <p>This is important, because through this step, you can accept the prompt so the installation process can run unattended!</p> <p>Make sure to set the correct permissions to the <code>id_rsa</code> file via <code>sudo chmod 0400 ~/.ssh/id_rsa</code>!</p>"},{"location":"openshift/rhos311-gcp-medium/#fix-locale","title":"Fix Locale","text":"<p>I kept running into a <code>locale</code> warning, about using one that didn't exist on the VM.</p> <p>If you want to get rid of this, you can change the <code>/etc/locale.conf</code> file.</p> <pre><code>sudo vim /etc/locale.conf\n</code></pre> <p>Make sure it looks like this.</p> <pre><code>LANG=\"en_US.UTF-8\"\nLC_CTYPE=\"en_US.UTF-8\"\nLC_ALL=en_US.UTF-8\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#openshift-installation-pre-requisites","title":"OpenShift Installation Pre-requisites","text":"<p>Before we can install OpenShift, we have to bring our nodes into a certain state.</p> <p>We will do the following:</p> <ul> <li>register our VMs to RedHat</li> <li>register our VMs as part of our OpenShift Enterprise license</li> <li>configure <code>yum</code> for the installation process</li> <li>install and configure <code>docker</code> for the installation process</li> <li>login to the RedHat docker registry</li> </ul>"},{"location":"openshift/rhos311-gcp-medium/#register-vms","title":"Register VMs","text":"<p>Please note, these steps have to be done on every VM!</p> <p>If you use something like <code>iterm2</code><sup>5</sup>, you can save yourself some time by having four parallel sessions for each VM. You do this by creating a split window (<code>control</code> + <code>command</code>  + <code>D</code>), and once logged in, create a shared <code>cursor</code> via <code>command</code> + <code>shift</code>+ <code>i</code>.</p> <p>We start by installing the subscription manager.</p> <pre><code>sudo yum install subscription-manager -y\n</code></pre> <p>We then register our instance with our RedHat account.</p> <pre><code>sudo subscription-manager register --username=&lt;user_name&gt; --password=&lt;password&gt;\n</code></pre> <pre><code>sudo subscription-manager refresh\n</code></pre> <p>Find the OpenShift subscription and you should get a single option. Use the id as the <code>--pool</code> in the next command.</p> <pre><code>sudo subscription-manager list --available --matches '*OpenShift*'\n</code></pre> <pre><code>sudo subscription-manager attach --pool=?\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#configure-yum-repos","title":"Configure Yum Repos","text":"<p>There's commands to disable each individual repository, but I found it easier to disable all, and then add those we need after.</p> <pre><code>sudo subscription-manager repos --disable=\"*\"\nsudo yum repolist\n</code></pre> <pre><code>sudo yum-config-manager --disable \\*\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#install-default-packages","title":"Install Default Packages","text":"<p>As we've disable all of our <code>yum</code> repositories, we first add the once we need.</p> <pre><code>sudo subscription-manager repos \\\n    --enable=\"rhel-7-server-rpms\" \\\n    --enable=\"rhel-7-server-extras-rpms\" \\\n    --enable=\"rhel-7-server-ose-3.11-rpms\" \\\n    --enable=\"rhel-7-server-ansible-2.8-rpms\"\n</code></pre> <p>Once we have a set of usable <code>yum</code> repositories, we can then install all the packages we need.</p> <pre><code>sudo yum install wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct openshift-ansible atomic python-docker-py docker device-mapper-libs device-mapper-event-libs -y\n</code></pre> <p>Note</p> <p>There have been some bugs in the past, related to docker versions. If, for some reason, you have to downgrade to a known working version of docker, this is a way of doing that.</p> <pre><code>sudo yum downgrade docker-rhel-push-plugin-1.13.1-75.git8633870.el7_5  docker-client-1.13.1-75.git8633870.el7_5 docker-common-1.13.1-75.git8633870.el7_5 docker-1.13.1-75.git8633870.el7_5\n</code></pre> <p>Once we have all the packages installed, make sure they're updated and then we reboot our machines.</p> <pre><code>sudo yum update -y\nsudo reboot\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#install-docker","title":"Install Docker","text":"<p>I sneaked the docker packages into the previous installation command already, so we only have to enable/configure docker at this point.</p> <p>If you want to configure more details, such as where docker stores its volumes/data, please take a look at RedHat's installation guide<sup>6</sup>.</p> <pre><code>sudo systemctl start docker.service\nsudo systemctl enable docker.service\n</code></pre> <p>To confirm docker works:</p> <pre><code>sudo systemctl status docker.service\n</code></pre> <p>Make sure that on each node, your default user can use docker.</p> <pre><code>sudo setfacl --modify user:my_user:rw /var/run/docker.sock\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#setup-registry-authentication","title":"Setup Registry Authentication","text":"<p>The images for OpenShift come from RedHat's own docker registry. We have to login, before we can use those images<sup>7</sup>.</p> <p>So use your RedHat account credentials.</p> <pre><code>docker login https://registry.redhat.io -u &lt;USER&gt; -p &lt;PASS&gt;\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#create-ansible-inventory-file","title":"Create Ansible Inventory File","text":"<p>OpenShift comes with two ways of installing, via docker or via Ansible. The fun part, the docker container will use Ansible to install anyway.</p> <p>So no matter which way you will install OpenShift, you need to create a InventoryFile.</p> <p>RedHat has a couple of example files<sup>8</sup>, but these aren't complate - you need <code>infra</code> nodes as well!</p>"},{"location":"openshift/rhos311-gcp-medium/#important-configuration-items","title":"Important Configuration Items","text":"<p>Bellow follow some variables I recommend configuring, for information, consult the RedHat documentation^9.</p> <ul> <li>OSEv3:children:  the types of nodes to be configured</li> <li>OSEv3:vars: variables for the installation process<ul> <li>ansible_become: set the <code>True</code> if Ansible can not run as root</li> <li>ansible_ssh_user: if Ansible cannot run as root, as which user should it ssh into the other nodes</li> <li>oreg_url: template for the docker images used by OpenShift, this should be <code>registry.access.redhat.com/openshift3/ose-${component}:${version}</code>, it will be used by components such as ETCD, Kubelet and so on</li> <li>oreg_auth_user: your RedHat account username</li> <li>oreg_auth_password: your RedHat account password</li> <li>openshift_cloudprovider_kind: the kind of cloud provider where RHOS is installed on, in the case of GCP its <code>gce</code> (don't ask me)</li> <li>openshift_gcp_project: is required to allow OpenShift the ability to create local disks in GCP for PersistentVolumes, should be your Google Project ID</li> <li>openshift_gcp_prefix: prefix used for all the GCP resources, makes it easier to correlate, and reduces the chance of naming conflicts</li> <li>openshift_gcp_network_name: the name of the <code>vpc</code> created with Terraform</li> <li>openshift_gcp_multizone: set this to <code>False</code>, unless you have more than one master, in more than one gcp zone</li> <li>openshift_master_api_port=443:  by default OpenShift uses <code>8443</code>, but this is not supported by GCP loadbalancers</li> </ul> </li> <li>openshift_master_console_port=443: by default OpenShift uses <code>8443</code>, but this is not supported by GCP loadbalancers<ul> <li>os_firewall_use_firewalld: use <code>firewalld</code> instead of iptables, seems to work better and is recommended by the RHOS 3.11 install guide (as of 2018+ I believe</li> <li>openshift_master_cluster_public_hostname: </li> </ul> </li> <li>Node definitions (<code>etcd</code>, <code>masters</code>, <code>nodes</code>): instructs Ansible which machine should be configured and with what</li> </ul> <p>Info</p> <p>If you want to use the internal OpenShift (Docker) Registry, you have to configure it here<sup>9</sup>. You can add these variables under <code>OSEv3:vars</code>.</p> <ul> <li>openshift_hosted_registry_replicas: how many replicas, docs recommend <code>1</code></li> <li>openshift_hosted_registry_storage_kind: type of storage, if using the GCP Bucket, set this to <code>object</code></li> <li>openshift_hosted_registry_storage_provider: if using the GCP Bucket, set this to <code>gcs</code></li> <li>openshift_hosted_registry_storage_gcs_bucket: the full name of the GCP Bucket</li> <li>openshift_gcp_network_name: the network the GCP Bucket is part of, irrelevant if the Bucket is <code>Global</code><sup>10</sup></li> </ul>"},{"location":"openshift/rhos311-gcp-medium/#inventory-file","title":"Inventory File","text":"<pre><code># Create an OSEv3 group that contains the masters, nodes, and etcd groups\n[OSEv3:children]\nmasters\nnodes\netcd\n\n# Set variables common for all OSEv3 hosts\n[OSEv3:vars]\n# SSH user, this user should allow ssh based auth without requiring a password\nansible_ssh_user=my_user\n\n# If ansible_ssh_user is not root, ansible_become must be set to true\nansible_become=true\n\nopenshift_deployment_type=openshift-enterprise\noreg_url=registry.access.redhat.com/openshift3/ose-${component}:${version}\noreg_auth_user=\"YOUR_RED_HAT_USERNAME\"\noreg_auth_password=\"YOUR_RED_HAT_PASSWORD\"\nopenshift_cloudprovider_kind=gce\nopenshift_gcp_project=YOUR_GOOGLE_PROJECT_ID\nopenshift_gcp_prefix=YOUR_PREFIX\n# If deploying single zone cluster set to \"False\"\nopenshift_gcp_multizone=\"False\"\nopenshift_hosted_registry_replicas=1\nopenshift_hosted_registry_storage_kind=object\nopenshift_hosted_registry_storage_provider=gcs\nopenshift_hosted_registry_storage_gcs_bucket=&lt;prefix&gt;-ocp-registry\nopenshift_gcp_network_name=vpc-&lt;prefix&gt;\n\n#openshift_master_cluster_hostname=api.ocp.kearos.net\nopenshift_master_cluster_public_hostname=api.ocp.example.com\nopenshift_master_default_subdomain=apps.ocp.example.com\n\nopenshift_master_api_port=443\nopenshift_master_console_port=443\nos_firewall_use_firewalld=True\n\n# https://docs.openshift.com/container-platform/3.11/install_config/configuring_authentication.html#identity-providers_parameters\nopenshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]\nopenshift_master_htpasswd_users={'administrator': 'password'}\n\n# host group for masters\n[masters]\n&lt;prefix&gt;-master1.c.ps-dev-201405.internal\n\n# host group for etcd\n[etcd]\n&lt;prefix&gt;-master1.c.ps-dev-201405.internal\n\n# host group for nodes, includes region info\n[nodes]\n&lt;prefix&gt;-master1.c.ps-dev-201405.internal openshift_node_group_name='node-config-master'\n&lt;prefix&gt;-node1.c.ps-dev-201405.internal openshift_node_group_name='node-config-compute'\n&lt;prefix&gt;-node2.c.ps-dev-201405.internal openshift_node_group_name='node-config-compute'\n&lt;prefix&gt;-node3.c.ps-dev-201405.internal openshift_node_group_name='node-config-compute'\n&lt;prefix&gt;-node4.c.ps-dev-201405.internal openshift_node_group_name='node-config-compute'\n&lt;prefix&gt;-infra1.c.ps-dev-201405.internal openshift_node_group_name='node-config-infra'\n</code></pre> <p>Important</p> <p>Make sure to replace the <code>YOUR_...</code> placeholder values with your actual values. </p> <ul> <li><code>oreg_auth_user</code> (YOUR_RED_HAT_USERNAME)</li> <li><code>oreg_auth_password</code> (YOUR_RED_HAT_PASSWORD)</li> <li><code>YOUR_GOOGLE_PROJECT_ID</code></li> </ul>"},{"location":"openshift/rhos311-gcp-medium/#install-rhos-311-with-ansible","title":"Install RHOS 3.11 with Ansible","text":"<p>There are two ways to install RHOS 3.11. Via Ansible directly<sup>8</sup>, or via Ansible in a container^9. As our nodes are configured according to what the Ansible installation requires, there's no need to rely on the container.</p> <p>Additionally, if you want to use the container way, you have to make sure the container can use the same DNS configuration as the nodes can themselves. I've not done this, so this would be on you!</p>"},{"location":"openshift/rhos311-gcp-medium/#final-preparations","title":"Final Preparations","text":"<p>Ansible creates a fact file. It does so at a location a non-root user doesn't have access to.</p> <p>So it is best to create this file upfront - on every node - and chown it to the user that will do the ssh/Ansible install.</p> <pre><code>sudo mkdir -p /etc/ansible/facts.d\nsudo chown -R my_user /etc/ansible/facts.d\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#install-openshift","title":"Install OpenShift","text":"<p>We install OpenShift via two scripts, <code>playbooks/prerequisites.yml</code> and <code>playbooks/deploy_cluster.yml</code>. When we install <code>openshift-ansible atomic</code> via yum, we also get the Ansible playbooks for OpenShift.</p> <p>Either go into the directory of those files, or use the entire path;</p> <pre><code>cd /usr/share/ansible/openshift-ansible\n</code></pre> <p>Execute OpenShift Pre-requisites script:</p> <pre><code>ansible-playbook -i /home/my_user/inventoryFile /usr/share/ansible/openshift-ansible/playbooks/prerequisites.yml\n</code></pre> <p>If all is successful, it will end with all actions in green and <code>finished successfully</code> (or similar) . Once this is the case, execute OpenShift Installation:</p> <pre><code>ansible-playbook -i /home/my_user/inventoryFile  /usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml\n</code></pre> <p>Now you should be able to run <code>oc get nodes</code> on the installation node.</p>"},{"location":"openshift/rhos311-gcp-medium/#create-users","title":"Create Users","text":"<p>In the <code>inventoryFile</code>, I've configured the OpenShift user management as follows:</p> <pre><code>openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]\nopenshift_master_htpasswd_users={'administrator': 'password'}\n</code></pre> <p>Caution</p> <p>You probably do NOT want to do this for a production environment. RedHat has a guide on this<sup>10</sup>, I suggest you consult it for an overview of the options.</p> <p>This configuration means that you have to create the users on the <code>&lt;prefix&gt;-master1</code> node, via <code>htpasswd</code>.</p>"},{"location":"openshift/rhos311-gcp-medium/#create-user-via-htpasswd","title":"Create user via htpasswd","text":"<pre><code>sudo htpasswd /etc/origin/master/htpasswd &lt;user&gt;\n</code></pre> <p>Mind you, this user does not have any rights yet, nor a token with which they can login to the Kubernetes API!</p>"},{"location":"openshift/rhos311-gcp-medium/#give-him-admin-rights","title":"Give him admin rights","text":"<pre><code>oc create clusterrolebinding registry-controller --clusterrole=cluster-admin --user=&lt;user&gt;\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#get-config","title":"Get Config","text":"<p>First, login with <code>oc login</code>, this will prompt you for a username and then password. Fill in the values for the user you've created above with <code>htpasswd</code>.</p> <p>Only once you've logged in via <code>oc login</code>, do you get a token. Use the password for the console, but the token for kubeconfig/docker registry.</p> <p>To view the token, first confirm you're logged in with the right user:</p> <pre><code>oc whoami\n</code></pre> <p>Then retrieve the token as follows:</p> <pre><code>oc whoami -t\n</code></pre> <p>To retrieve the config file for Kubernetes's API, you can issue the following command.</p> <pre><code>oc config view\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#login-to-openshift-registry","title":"Login to OpenShift Registry","text":"<p>To retrieve the Registry URL:</p> <pre><code>kubectl get route --namespace default\n</code></pre> <p>Which should give you something like this:</p> <pre><code>NAME               HOST/PORT                                      PATH   SERVICES           PORT    TERMINATION   WILDCARD\ndocker-registry    docker-registry-default.apps.ocp.example.com           docker-registry    &lt;all&gt;   passthrough   None\nregistry-console   registry-console-default.apps.ocp..example.com          registry-console   &lt;all&gt;   passthrough   None\n</code></pre> <p>Unless you have configured a proper SSL certificate, docker won't let you login. See the next chapter for more information on that.</p> <p>If you do have a SSL certificate, this is how you can login:</p> <pre><code>docker login -u &lt;user&gt; -p &lt;token&gt; registry-console-default.apps.ocp..example.com\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#ssl-via-certmanager","title":"SSL Via Certmanager","text":"<p>There are many ways to get a valid SSL certificate for your domain.</p> <p>Here I guide you through the following:</p> <ul> <li>install old certmanager - the latest requires Kubernetes 1.12+</li> <li>Confgure DNS Verification to let Certmanager use CloudDNS<sup>12</sup></li> <li>generate cert for <code>*.apps.ocp.kearos.net</code></li> <li>add cert to the OpenShift Router and Docker Registry resources</li> </ul>"},{"location":"openshift/rhos311-gcp-medium/#install-compatible-certmanager","title":"Install Compatible Certmanager","text":"<p>Luckily, Certmanager has documentation on how to install itself on OpenShift 3.11<sup>11</sup>.</p> <p>Below are the steps, yes it is that simple.</p> <pre><code>oc create namespace cert-manager\n</code></pre> <pre><code>oc apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.14.0/cert-manager-legacy.yaml\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#confgure-dns-verification","title":"Confgure DNS Verification","text":"<p>First we need to setup a Google cloud service account<sup>13</sup>, and retrieve the <code>json</code> key.</p> <p>Save this file as <code>credentials.json</code>, and then create a kubernetes <code>secret</code>.</p> <pre><code>kubectl create secret generic external-dns-gcp-sa --from-file=credentials.json\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#certificate-issuer","title":"Certificate Issuer","text":"<pre><code>apiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    email: &lt;INSERT_YOUR_EMAIL&gt;\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    server: https://acme-v02.api.letsencrypt.org/directory\n    solvers:\n    - dns01:\n        clouddns:\n          project:  &lt;INSERT_YOUR_GOOGLE_PROJECT_ID&gt;\n          serviceAccountSecretRef:\n            key: credentials.json\n            name: external-dns-gcp-sa\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#wildcard-certificate","title":"Wildcard Certificate","text":"<pre><code>apiVersion: cert-manager.io/v1alpha2\nkind: Certificate\nmetadata:\n  name: acme-crt\nspec:\n  secretName: acme-crt-secret\n  dnsNames:\n  - apps.ocp.example.com\n  - \"*.apps.ocp.example.com\"\n  issuerRef:\n    name: letsencrypt-prod\n    # We can reference ClusterIssuers by changing the kind here.\n    # The default value is Issuer (i.e. a locally namespaced Issuer)\n    kind: Issuer\n    group: cert-manager.io\n</code></pre> <p>Caution</p> <p>Don't forget to change the <code>dnsNames</code>!</p>"},{"location":"openshift/rhos311-gcp-medium/#configure-certificate-in-router","title":"Configure Certificate In Router","text":"<p>HA Proxy expect a crt file with the key and cert, while the Certmanager secret's <code>tls.crt</code> file only contains the certificate - how unexpected.</p> <p>So, what we have to do is the following: * get <code>tls.crt</code> raw data (the actual certificate) * get <code>tls.key</code> raw data * add the data from <code>tls.key</code> to a file <code>tls.key</code> * add the data from both <code>tls.key</code> and <code>tls.crt</code> to a file <code>tls.crt</code> * create a new secret * edit the Router to use our new secret * kill the router pod to force it to load our new certificate</p> <pre><code>kubectl get secret acme-crt-secret -o yaml\n</code></pre> <pre><code>echo \"&lt;base64 secret value&gt; | base64 -D\"\n</code></pre> <p>Or, if you have <code>jq</code><sup>14</sup> installed, you can do something like this.</p> <pre><code>kubectl get secret  -n default  acme-crt-secret  -o json | jq -r '.data | map_values(@base64d)'\n</code></pre> <p>Add them both the PEM certificate and the key to the file <code>tls.crt</code>, and the key to the file <code>tls.key</code>.</p> <p>Create a new secret</p> <pre><code>kubectl create secret tls letsencrypt-copy --cert=tls.crt --key=tls.key\n</code></pre> <p>And then edit the Route, via the following command:</p> <p><code>kubectl edit -n default  replicationcontrollers router-1</code></p> <p>Change the <code>secret.secretName</code> value of the volume named <code>server-certificate</code> from <code>router-certs</code> to <code>letsencrypt-copy</code>.</p> <p>It should then look like this:</p> <pre><code>      volumes:\n      - name: metrics-server-certificate\n        secret:\n          defaultMode: 420\n          secretName: router-metrics-tls\n      - name: server-certificate\n        secret:\n          defaultMode: 420\n          secretName: letsencrypt-copy\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#configure-certificate-in-docker-registry","title":"Configure Certificate in Docker Registry","text":"<p>Edit docker registry as well:</p> <pre><code>kubectl edit replicationcontrollers docker-registry-1\n</code></pre> <p>Make sure to overide the naming convention of <code>registry.crt</code> and <code>registry.key</code> to the sensible <code>tls.key</code> and <code>tls.crt</code>.</p> <pre><code>        - name: REGISTRY_HTTP_TLS_KEY\n          value: /etc/secrets/tls.key\n        - name: OPENSHIFT_DEFAULT_REGISTRY\n          value: docker-registry.default.svc:5000\n        - name: REGISTRY_CONFIGURATION_PATH\n          value: /etc/registry/config.yml\n        - name: REGISTRY_OPENSHIFT_SERVER_ADDR\n          value: docker-registry.default.svc:5000\n        - name: REGISTRY_HTTP_TLS_CERTIFICATE\n          value: /etc/secrets/tls.crt\n</code></pre>"},{"location":"openshift/rhos311-gcp-medium/#references","title":"References","text":"<ol> <li> <p>https://access.redhat.com/documentation/en-us/reference_architectures/2018/html/deploying_and_managing_openshift_3.9_on_google_cloud_platform/ \u21a9</p> </li> <li> <p>https://www.terraform.io/docs/modules/index.html \u21a9</p> </li> <li> <p>https://cloud.google.com/storage/docs/key-terms#buckets \u21a9\u21a9</p> </li> <li> <p>https://docs.openshift.com/container-platform/3.11/install_config/registry/index.html \u21a9</p> </li> <li> <p>https://iterm2.com/ \u21a9</p> </li> <li> <p>https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-install-host-preparation#configuring-docker-storage \u21a9</p> </li> <li> <p>https://access.redhat.com/RegistryAuthentication \u21a9</p> </li> <li> <p>https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html-single/installing_clusters/index#install-config-example-inventories \u21a9\u21a9</p> </li> <li> <p>https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-configuring-inventory-file#advanced-install-registry-storage \u21a9</p> </li> <li> <p>https://docs.openshift.com/container-platform/3.11/install_config/configuring_authentication.html#identity-providers_parameters \u21a9\u21a9</p> </li> <li> <p>https://cert-manager.io/docs/installation/openshift/ \u21a9</p> </li> <li> <p>https://cloud.google.com/dns/ \u21a9</p> </li> <li> <p>https://access.redhat.com/documentation/en-us/reference_architectures/2018/html-single/deploying_and_managing_openshift_3.9_on_google_cloud_platform/index#google_cloud_platform_service_account \u21a9</p> </li> <li> <p>https://stedolan.github.io/jq/ \u21a9</p> </li> </ol>"},{"location":"openshift/rhos311-gcp-minimal/","title":"RedHat OpenShift 3.11 on GCP - Minimal","text":"<p>Why am I writing this guide? Well, to document my own steps taken. Also, I think the guide from Google <sup>1</sup> is to abstract to be useful, and the guide from RedHat<sup>2</sup> contains such number of options and digressions its hard to focus on what you require.</p> <p>Note</p> <p>This guide is written early March 2020, using <code>jx</code> version <code>2.0.1212</code> and OpenShift version <code>v3.11.170</code>.</p>"},{"location":"openshift/rhos311-gcp-minimal/#pre-requisites","title":"Pre-requisites","text":"<p>What do we need to get started?</p> <ul> <li>active GCP account with billing enabled</li> <li>GCP project</li> <li>gcloud tool installed</li> <li>terraform <code>0.12</code>+  installed</li> <li>active RedHat account<ul> <li>which hasn't used its 60 day trial license of OpenShift yet</li> </ul> </li> </ul>"},{"location":"openshift/rhos311-gcp-minimal/#process","title":"Process","text":"<ul> <li>create Terraform configuration for GCP VM's</li> <li>create the VM's in GCP with RedHat Enterprise Linux v7</li> <li>install OpenShift pre-requisites on each VM</li> <li>create OpenShift Ansible configuration</li> <li>install OpenShift via Ansible</li> </ul>"},{"location":"openshift/rhos311-gcp-minimal/#gcp-terraform","title":"GCP Terraform","text":""},{"location":"openshift/rhos311-gcp-minimal/#what-do-we-need","title":"What Do We Need","text":"<p>Having gone through the process of installing RHOS 3.11 once, I ran into an issue. The documentation makes it seems you only need <code>master</code> nodes, <code>compute</code> nodes and VM's for <code>etcd</code> (can be the same as <code>Master</code>). However, you also need at least one <code>infra</code> node.</p> <p>You can opt for a HA cluster, with three <code>master</code> nodes, or a single <code>master</code> node for a test cluster. I'm going with the latter. The <code>master</code> node will house the Kubernetes Control Plane, the <code>infra</code> node will house the OpenShift infra. As we won't have cluster autoscaling - a bit fancy for a manual test cluster - we have to make sure the machines are beefy to take the entire workload.</p> <p>Another thing we need for OpenShift, is having DNS that works between the nodes. For example, you should be able to say <code>node1</code> and end up at the correct machine. Due to GCP networking, this internal DNS works out-of-the-box for any machine with our network/project. </p> <p>!!!! important     Our machines need to have unique names!</p> <p>So I ended up with the following:</p> <ul> <li>1x  <code>master</code> node -&gt; <code>n1-standard-8</code>: 8 cores, 30gb mem</li> <li>1x <code>infra</code> node -&gt;  <code>n1-standard-8</code>: 8 cores, 30gb mem</li> <li>2x <code>compute</code> node -&gt; <code>n1-standard-4</code>: 4 cores, 15gb mem (each)</li> </ul> <p>Caution</p> <p>For a first iteration, I've ignored creating a separate VPC and networking configuration. This to avoid learning too many things at once. You probably do want that for a more secure cluster. Read the medium effort guide in case you want to.</p>"},{"location":"openshift/rhos311-gcp-minimal/#vm-image","title":"VM Image","text":"<p>Of course, if you want to run RedHat OpenShift Enterprise (RHOS), your VM's need to run a RedHat Enterprise Linux distribution(RHEL).</p> <p>In order to figure out which vm images are currently available, you can issue the following command via <code>gcloud</code>.</p> <pre><code>gcloud compute images list --project rhel-cloud\n</code></pre> <p>Which should give a response like this:</p> <pre><code>NAME                                                  PROJECT            FAMILY                            DEPRECATED  STATUS\nrhel-6-v20200205                                      rhel-cloud         rhel-6                                        READY\nrhel-7-v20200205                                      rhel-cloud         rhel-7                                        READY\nrhel-8-v20200205                                      rhel-cloud         rhel-8                                        READY\n</code></pre> <p>For the VM image in our Terraform configuration, we will use the <code>NAME</code> of the image. For RHOS 3.11, RedHat strongly recommends using RHEL 7, so we use <code>rhel-7-v20200205</code>.</p>"},{"location":"openshift/rhos311-gcp-minimal/#terraform-configuration","title":"Terraform Configuration","text":"<p>We have the following files:</p> <ul> <li>main.tf: contains the main configuration for the provider, in this case <code>google</code></li> <li>variables.tf: the variables and their defaults</li> <li>master-node.tf: the <code>master</code> node configuration</li> <li>infra-node.tf: the <code>infra</code> node configuration</li> <li>compute-nodes.tf: the two <code>compute</code> node configurations</li> </ul> <p>Important</p> <p>We need to ssh into the VMs. To make this easy, I'm using a local <code>ssh</code> key and make sure it is configured on the VMs. See <code>ssh-keys = \"joostvdg:${file(\"~/.ssh/id_rsa.pub\")}\"</code> in the <code>metadata</code> block.</p> <p>The first part of the value  <code>joostvdg</code> is my desired username. Change this if you want.</p> main.tf <pre><code>terraform {\n    required_version = \"~&gt; 0.12\"\n}\n\nprovider \"google\" {\n    version   = \"~&gt; 2.18.1\"\n    project   = var.project\n    region    = var.region\n    zone      = var.main_zone\n}\n</code></pre> variables.tf <pre><code>variable \"project\" { }\n\nvariable \"name\" {\n    default     = \"jx-openshift-311\"\n}\n\nvariable \"compute_machine_type\" {\n    default = \"n1-standard-4\"\n}\n\nvariable \"master_machine_type\" {\n    default = \"n1-standard-8\"\n}\n\nvariable \"vm_image\" {\n    default =\"rhel-7-v20200205\"\n}\n\nvariable \"master_zone\" {\n    default = \"europe-west4-a\"\n}\n</code></pre> master-node.tf <pre><code>resource \"google_compute_instance\" \"master\" {\n    name         = \"master\"\n    machine_type = var.master_machine_type\n    zone         = var.master_zone\n    allow_stopping_for_update = true\n\n    boot_disk {\n        initialize_params {\n            image = var.vm_image\n            size = 100\n        }\n    }\n\n    // Local SSD disk\n    scratch_disk {\n        interface = \"SCSI\"\n    }\n\n    network_interface {\n        network = \"default\"\n        # network_ip = google_compute_address.masterip.address\n        access_config {\n            # external address\n        }\n    }\n\n    metadata = {\n        ssh-keys = \"joostvdg:${file(\"~/.ssh/id_rsa.pub\")}\"\n    }\n    service_account {\n        scopes = [\n            \"https://www.googleapis.com/auth/compute\",\n            \"https://www.googleapis.com/auth/devstorage.read_only\",\n            \"https://www.googleapis.com/auth/logging.write\",\n            \"https://www.googleapis.com/auth/monitoring\",\n            \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\",\n            \"https://www.googleapis.com/auth/cloud-platform\"\n        ]\n    }\n}\n</code></pre> infra-node.tf <pre><code>resource \"google_compute_instance\" \"infra1\" {\n    name         = \"infra1\"\n    machine_type = var.master_machine_type\n    zone         = var.master_zone\n    allow_stopping_for_update = true\n\n    boot_disk {\n        initialize_params {\n            image = var.vm_image\n            size = 100\n        }\n    }\n\n    // Local SSD disk\n    scratch_disk {\n        interface = \"SCSI\"\n    }\n\n    network_interface {\n        network = \"default\"\n        # network_ip = google_compute_address.node2ip.address\n        access_config {\n            # external address\n        }\n    }\n    metadata = {\n        ssh-keys = \"joostvdg:${file(\"~/.ssh/id_rsa.pub\")}\"\n    }\n\n    service_account {\n        scopes = [\n            \"https://www.googleapis.com/auth/compute\",\n            \"https://www.googleapis.com/auth/devstorage.read_only\",\n            \"https://www.googleapis.com/auth/logging.write\",\n            \"https://www.googleapis.com/auth/monitoring\",\n            \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\",\n            \"https://www.googleapis.com/auth/cloud-platform\"\n        ]\n    }\n}\n</code></pre> compute-node.tf <pre><code>resource \"google_compute_instance\" \"node1\" {\n    name         = \"node1\"\n    machine_type = var.compute_machine_type\n    zone         = var.master_zone\n    allow_stopping_for_update = true\n\n    boot_disk {\n        initialize_params {\n            image = var.vm_image\n            size = 100\n        }\n    }\n\n    // Local SSD disk\n    scratch_disk {\n        interface = \"SCSI\"\n    }\n\n    network_interface {\n        network = \"default\"\n        # network_ip = google_compute_address.node1ip.address\n        access_config {\n            # external address\n        }\n    }\n    metadata = {\n        ssh-keys = \"joostvdg:${file(\"~/.ssh/id_rsa.pub\")}\"\n    }\n\n    service_account {\n        scopes = [\n            \"https://www.googleapis.com/auth/compute\",\n            \"https://www.googleapis.com/auth/devstorage.read_only\",\n            \"https://www.googleapis.com/auth/logging.write\",\n            \"https://www.googleapis.com/auth/monitoring\",\n            \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\",\n            \"https://www.googleapis.com/auth/cloud-platform\"\n        ]\n    }\n}\n\nresource \"google_compute_instance\" \"node2\" {\n    name         = \"node2\"\n    machine_type = \"n1-standard-4\"\n    zone         = var.master_zone\n    allow_stopping_for_update = true\n\n    boot_disk {\n        initialize_params {\n            image = var.vm_image\n            size = 100\n        }\n    }\n\n    // Local SSD disk\n    scratch_disk {\n        interface = \"SCSI\"\n    }\n\n    network_interface {\n        network = \"default\"\n        # network_ip = google_compute_address.node2ip.address\n        access_config {\n            # external address\n        }\n    }\n    metadata = {\n        ssh-keys = \"joostvdg:${file(\"~/.ssh/id_rsa.pub\")}\"\n    }\n\n    service_account {\n        scopes = [\n            \"https://www.googleapis.com/auth/compute\",\n            \"https://www.googleapis.com/auth/devstorage.read_only\",\n            \"https://www.googleapis.com/auth/logging.write\",\n            \"https://www.googleapis.com/auth/monitoring\",\n            \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\",\n            \"https://www.googleapis.com/auth/cloud-platform\"\n        ]\n    }\n}\n</code></pre> <p>We should end up with four nodes:</p> <ol> <li><code>master</code></li> <li><code>infra</code></li> <li><code>node1</code></li> <li><code>node2</code></li> </ol>"},{"location":"openshift/rhos311-gcp-minimal/#create-vms-with-terraform","title":"Create VMs with Terraform","text":"<pre><code>terraform init\n</code></pre> <pre><code>terraform validate\n</code></pre> <pre><code>terraform plan -out plan.out\n</code></pre> <pre><code>terraform apply \"plan.out\"\n</code></pre>"},{"location":"openshift/rhos311-gcp-minimal/#verify-vms","title":"Verify VMs","text":"<p>Before we can install the OpenShift pre-requisites, we verify if the VMs are ready to use. To verify the VMs, we will do the following:</p> <ol> <li>confirm we can <code>ssh</code> into each VM</li> <li>confirm we can use <code>sudo</code> in each VM</li> <li>confirm the infra node can call each VM by name (<code>infra</code>, <code>master</code>, <code>node1</code>, <code>node2</code>)</li> <li>confirm the infra node can <code>ssh</code> into all VMs (including itself!)</li> </ol>"},{"location":"openshift/rhos311-gcp-minimal/#ssh-into-vms","title":"SSH into VMs","text":"<p>There are several ways to ssh into the VMs. You can do so via <code>ssh</code> installed on your machine, you can do so via the GCP console. I will use another option, using the <code>gcloud</code> CLI, using the ssh key I've configured in Terrafom (<code>ssh-keys = \"joostvdg:${file(\"~/.ssh/id_rsa.pub\")}\"</code>).</p> <p>Why am I using this form? Well, it makes it easier to reason about which machine I ssh into, as I can use the VM name.</p> <pre><code># your google project id\nPROJECT_ID=\n</code></pre> <pre><code># the google zone the vm is in, for example: europe-west-4a\nVM_ZONE=\n</code></pre> <pre><code>gcloud beta compute --project $PROJECT_ID ssh --zone $VM_ZONE \"node1\"\n</code></pre> <p>Confirm you can ssh into each VM by changing the zone/name accordingly.</p>"},{"location":"openshift/rhos311-gcp-minimal/#confirm-sudo","title":"Confirm Sudo","text":"<p>Our ssh user isn't root - as it should be - so we need to use sudo for some tasks.</p> <p>Confirm sudo works;</p> <pre><code>sudo cat /etc/locale.conf\n</code></pre>"},{"location":"openshift/rhos311-gcp-minimal/#confirm-local-dns","title":"Confirm Local DNS","text":"<p>The OpenShift installation process and later OpenShift itself, relies on local dns. This means, it assumes if there's a node called <code>master</code>, it can do <code>ssh master</code> and it works.</p> <p>In GCP, DNS works within a Project by default. So assuming all the VMs  are within the same project this works out-of-the-box. But, to avoid any surprises later, confirm it.</p> <pre><code>[joostvdg@master ~]$ ping master\nPING master.c.MY_PROJECT_ID.internal (10.164.0.49) 56(84) bytes of data.\n64 bytes from master.c.MY_PROJECT_ID.internal (10.164.0.49): icmp_seq=1 ttl=64 time=0.041 ms\n64 bytes from master.c.MY_PROJECT_ID.internal (10.164.0.49): icmp_seq=2 ttl=64 time=0.094 ms\n</code></pre> <pre><code>[joostvdg@master ~]$ ping node1\nPING node1.c.MY_PROJECT_ID.internal (10.164.0.50) 56(84) bytes of data.\n64 bytes from node1.c.MY_PROJECT_ID.internal (10.164.0.50): icmp_seq=1 ttl=64 time=1.13 ms\n64 bytes from node1.c.MY_PROJECT_ID.internal (10.164.0.50): icmp_seq=2 ttl=64 time=0.343 ms\n</code></pre> <p>Note</p> <p>As you might expect, <code>MY_PROJECT_ID</code> will be the Google project Id where your VMs are. I've hidden that as a safety precaution, confirm it looks correct!</p>"},{"location":"openshift/rhos311-gcp-minimal/#infra-node-can-ssh-into-others","title":"Infra Node can SSH into others","text":"<p>For the OpenShift installation, our installation VM has to be able to ssh into every other VM<sup>3</sup>. This doesn't work out of the box.</p> <p>Warning</p> <p>I used my own keys here directly, because this is a temporary project only used by me. If your usecase is different, and you're not sure how to proceed, consult a security professional!</p> <p>We have to create the <code>ssh</code> public key on every node for our ssh user (in my case, <code>joostvdg</code>) and the private also for our installation host (for example, <code>infra</code>).</p> <p>This might not be a security best practice, but I did this by copying over my <code>~/.ssh/id_rsa</code> and <code>~/.ssh/id_rsa.pub</code> to each node's user home (<code>/home/joostvdg/.ssh/</code>).</p> <p>Important</p> <p>Once you've done this, ssh into the <code>infra</code> node, and confirm it can ssh to every other node.</p> <ul> <li><code>ssh joostvdg@node1</code></li> <li><code>ssh joostvdg@node2</code></li> <li><code>ssh joostvdg@master</code></li> <li><code>ssh joostvdg@infra</code> -&gt; YES, you have to ssh into yourself!</li> </ul> <p>This is important, because through this step, you can accept the prompt so the installation process can run unattended!</p> <p>Make sure to set the correct permissions to the <code>id_rsa</code> file via <code>sudo chmod 0400 ~/.ssh/id_rsa</code>!</p>"},{"location":"openshift/rhos311-gcp-minimal/#fix-locale","title":"Fix Locale","text":"<p>I kept running into a <code>locale</code> warning, about using one that didn't exist on the VM.</p> <p>If you want to get rid of this, you can change the <code>/etc/locale.conf</code> file.</p> <pre><code>sudo vim /etc/locale.conf\n</code></pre> <p>Make sure it looks like this.</p> <pre><code>LANG=\"en_US.UTF-8\"\nLC_CTYPE=\"en_US.UTF-8\"\nLC_ALL=en_US.UTF-8\n</code></pre>"},{"location":"openshift/rhos311-gcp-minimal/#openshift-installation-pre-requisites","title":"OpenShift Installation Pre-requisites","text":"<p>Before we can install OpenShift, we have to bring our nodes into a certain state.</p> <p>We will do the following:</p> <ul> <li>register our VMs to RedHat</li> <li>register our VMs as part of our OpenShift Enterprise license</li> <li>configure <code>yum</code> for the installation process</li> <li>install and configure <code>docker</code> for the installation process</li> <li>login to the RedHat docker registry</li> </ul>"},{"location":"openshift/rhos311-gcp-minimal/#register-vms","title":"Register VMs","text":"<p>Please note, these steps have to be done on every VM!</p> <p>If you use something like <code>iterm2</code>, you can save yourself some time by having four parallel sessions for each VM. You do this by creating a split window (<code>control</code> + <code>command</code>  + <code>D</code>), and once logged in, create a shared <code>cursor</code> via <code>command</code> + <code>shift</code>+ <code>i</code>.</p> <p>We start by installing the subscription manager.</p> <pre><code>sudo yum install subscription-manager -y\n</code></pre> <p>We then register our instance with our RedHat account.</p> <pre><code>sudo subscription-manager register --username=&lt;user_name&gt; --password=&lt;password&gt;\n</code></pre> <pre><code>sudo subscription-manager refresh\n</code></pre> <p>Find the OpenShift subscription and you should get a single option. Use the id as the <code>--pool</code> in the next command.</p> <pre><code>sudo subscription-manager list --available --matches '*OpenShift*'\n</code></pre> <pre><code>sudo subscription-manager attach --pool=?\n</code></pre>"},{"location":"openshift/rhos311-gcp-minimal/#configure-yum-repos","title":"Configure Yum Repos","text":"<p>There's commands to disable each individual repository, but I found it easier to disable all, and then add those we need after.</p> <pre><code>sudo subscription-manager repos --disable=\"*\"\nsudo yum repolist\n</code></pre> <pre><code>sudo yum-config-manager --disable \\*\n</code></pre>"},{"location":"openshift/rhos311-gcp-minimal/#install-default-packages","title":"Install Default Packages","text":"<p>As we've disable all of our <code>yum</code> repositories, we first add the once we need.</p> <pre><code>sudo subscription-manager repos \\\n    --enable=\"rhel-7-server-rpms\" \\\n    --enable=\"rhel-7-server-extras-rpms\" \\\n    --enable=\"rhel-7-server-ose-3.11-rpms\" \\\n    --enable=\"rhel-7-server-ansible-2.8-rpms\"\n</code></pre> <p>Once we have a set of usable <code>yum</code> repositories, we can then install all the packages we need.</p> <pre><code>sudo yum install wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct openshift-ansible atomic python-docker-py docker device-mapper-libs device-mapper-event-libs -y\n</code></pre> <p>Note</p> <p>There have been some bugs in the past, related to docker versions. If, for some reason, you have to downgrade to a known working version of docker, this is a way of doing that.</p> <pre><code>sudo yum downgrade docker-rhel-push-plugin-1.13.1-75.git8633870.el7_5  docker-client-1.13.1-75.git8633870.el7_5 docker-common-1.13.1-75.git8633870.el7_5 docker-1.13.1-75.git8633870.el7_5\n</code></pre> <p>Once we have all the packages installed, make sure they're updated and then we reboot our machines.</p> <pre><code>sudo yum update -y\nsudo reboot\n</code></pre>"},{"location":"openshift/rhos311-gcp-minimal/#install-docker","title":"Install Docker","text":"<p>I sneaked the docker packages into the previous installation command already, so we only have to enable/configure docker at this point.</p> <p>If you want to configure more details, such as where docker stores its volumes/data, please take a look at RedHat's installation guide<sup>4</sup>.</p> <pre><code>sudo systemctl start docker.service\nsudo systemctl enable docker.service\n</code></pre> <p>To confirm docker works:</p> <pre><code>sudo systemctl status docker.service\n</code></pre> <p>Make sure that on each node, your default user can use docker.</p> <pre><code>sudo setfacl --modify user:joostvdg:rw /var/run/docker.sock\n</code></pre>"},{"location":"openshift/rhos311-gcp-minimal/#setup-registry-authentication","title":"Setup Registry Authentication","text":"<p>The images for OpenShift come from RedHat's own docker registry. We have to login, before we can use those images<sup>5</sup>.</p> <p>So use your RedHat account credentials.</p> <pre><code>docker login https://registry.redhat.io -u &lt;USER&gt; -p &lt;PASS&gt;\n</code></pre>"},{"location":"openshift/rhos311-gcp-minimal/#create-ansible-inventory-file","title":"Create Ansible Inventory File","text":"<p>OpenShift comes with two ways of installing, via docker or via Ansible. The fun part, the docker container will use Ansible to install anyway.</p> <p>So no matter which way you will install OpenShift, you need to create a InventoryFile.</p> <p>RedHat has a couple of example files<sup>6</sup>, but these aren't complate - you need <code>infra</code> nodes as well!</p>"},{"location":"openshift/rhos311-gcp-minimal/#important-configuration-items","title":"Important Configuration Items","text":"<p>Bellow follow some variables I recommend configuring, for information, consult the RedHat documentation^7.</p> <ul> <li>OSEv3:children:  the types of nodes to be configured</li> <li>OSEv3:vars: variables for the installation process<ul> <li>ansible_become: set the <code>True</code> if Ansible can not run as root</li> <li>ansible_ssh_user: if Ansible cannot run as root, as which user should it ssh into the other nodes</li> <li>oreg_url: template for the docker images used by OpenShift, this should be <code>registry.access.redhat.com/openshift3/ose-${component}:${version}</code>, it will be used by components such as ETCD, Kubelet and so on</li> <li>oreg_auth_user: your RedHat account username</li> <li>oreg_auth_password: your RedHat account password</li> <li>openshift_cloudprovider_kind: the kind of cloud provider where RHOS is installed on, in the case of GCP its <code>gce</code> (don't ask me)</li> <li>openshift_gcp_project: is required to allow OpenShift the ability to create local disks in GCP for PersistentVolumes, should be your Google Project ID</li> <li>os_firewall_use_firewalld: use <code>firewalld</code> instead of iptables, seems to work better and is recommended by the RHOS 3.11 install guide (as of 2018+ I believe)</li> </ul> </li> <li>Node definitions (<code>etcd</code>, <code>masters</code>, <code>nodes</code>): instructs Ansible which machine should be configured and with what</li> </ul> <p>Important</p> <p>If you use an external LoadBalancer, also set <code>openshift_master_cluster_public_hostname</code>.</p> <p>This variable overrides the public host name for the cluster, which defaults to the host name of the master. If you use an external load balancer, specify the address of the external load balancer. </p>"},{"location":"openshift/rhos311-gcp-minimal/#example-inventory-file","title":"Example Inventory File","text":"<pre><code># Create an OSEv3 group that contains the masters, nodes, and etcd groups\n[OSEv3:children]\nmasters\nnodes\netcd\n\n# Set variables common for all OSEv3 hosts\n[OSEv3:vars]\n# SSH user, this user should allow ssh based auth without requiring a password\nansible_ssh_user=joostvdg\n# If ansible_ssh_user is not root, ansible_become must be set to true\nansible_become=true\n\nopenshift_deployment_type=openshift-enterprise\n# This is supposed to be a template, do not change!\noreg_url=registry.access.redhat.com/openshift3/ose-${component}:${version}\noreg_auth_user=\"YOUR_RED_HAT_USERNAME\"\noreg_auth_password=\"YOUR_RED_HAT_PASSWORD\"\nopenshift_cloudprovider_kind=gce\nopenshift_gcp_project=\"YOUR_GOOGLE_PROJECT_ID\"\nopenshift_gcp_prefix=joostvdgrhos\n# If deploying single zone cluster set to \"False\"\nopenshift_gcp_multizone=\"False\"\n\nopenshift_master_api_port=443\nopenshift_master_console_port=443\nos_firewall_use_firewalld=True\n# Enable if you want to use httpd for managing additional users\n# openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]\n# openshift_master_htpasswd_users={'administrator': 'password'}\n\n# host group for masters\n[masters]\nmaster.c.&lt;YOUR_GOOGLE_PROJECT_ID&gt;.internal\n\n# host group for etcd\n[etcd]\nmaster.c.&lt;YOUR_GOOGLE_PROJECT_ID&gt;.internal\n\n# host group for nodes, includes region info\n[nodes]\nmaster.c.&lt;YOUR_GOOGLE_PROJECT_ID&gt;.internal openshift_node_group_name='node-config-master'\nnode1.c.&lt;YOUR_GOOGLE_PROJECT_ID&gt;.internal openshift_node_group_name='node-config-compute'\nnode2.c.&lt;YOUR_GOOGLE_PROJECT_ID&gt;.internal openshift_node_group_name='node-config-compute'\ninfra1.c.&lt;YOUR_GOOGLE_PROJECT_ID&gt;.internal openshift_node_group_name='node-config-infra'\n</code></pre> <p>Important</p> <p>Make sure to replace the <code>YOUR_...</code> placeholder values with your actual values. </p> <ul> <li><code>oreg_auth_user</code> (YOUR_RED_HAT_USERNAME)</li> <li><code>oreg_auth_password</code> (YOUR_RED_HAT_PASSWORD)</li> <li><code>YOUR_GOOGLE_PROJECT_ID</code></li> </ul>"},{"location":"openshift/rhos311-gcp-minimal/#install-rhos-311-with-ansible","title":"Install RHOS 3.11 with Ansible","text":"<p>There are two ways to install RHOS 3.11. Via Ansible directly<sup>7</sup>, or via Ansible in a container<sup>8</sup>. As our nodes are configured according to what the Ansible installation requires, there's no need to rely on the container.</p> <p>Additionally, if you want to use the container way, you have to make sure the container can use the same DNS configuration as the nodes can themselves. I've not done this, so this would be on you!</p>"},{"location":"openshift/rhos311-gcp-minimal/#final-preparations","title":"Final Preparations","text":"<p>Ansible creates a fact file. It does so at a location a non-root user doesn't have access to.</p> <p>So it is best to create this file upfront - on every node - and chown it to the user that will do the ssh/Ansible install.</p> <pre><code>sudo mkdir -p /etc/ansible/facts.d\nsudo chown -R joostvdg /etc/ansible/facts.d\n</code></pre>"},{"location":"openshift/rhos311-gcp-minimal/#install-openshift","title":"Install OpenShift","text":"<p>We install OpenShift via two scripts, <code>playbooks/prerequisites.yml</code> and <code>playbooks/deploy_cluster.yml</code>. When we install <code>openshift-ansible atomic</code> via yum, we also get the Ansible playbooks for OpenShift.</p> <p>Either go into the directory of those files, or use the entire path;</p> <pre><code>cd /usr/share/ansible/openshift-ansible\n</code></pre> <p>Execute OpenShift Pre-requisites script:</p> <pre><code>ansible-playbook -i /home/joostvdg/inventoryFile /usr/share/ansible/openshift-ansible/playbooks/prerequisites.yml\n</code></pre> <p>If all is successful, it will end with all actions in green and <code>finished successfully</code> (or similar) . Once this is the case, execute OpenShift Installation:</p> <pre><code>ansible-playbook -i /home/joostvdg/inventoryFile  /usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml\n</code></pre> <p>Now you should be able to run <code>oc get nodes</code> on the installation node.</p>"},{"location":"openshift/rhos311-gcp-minimal/#read-more","title":"Read More","text":"<ul> <li>https://access.redhat.com/documentation/en-us/reference_architectures/2018/html-single/deploying_and_managing_openshift_3.9_on_google_cloud_platform/index#google_cloud_platform_networking</li> <li>https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html-single/installing_clusters/index#what-s-next</li> <li>https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-install-prerequisites#prereq-network-access</li> <li>http://crunchtools.com/hackers-guide-to-installing-openshift-container-platform-3-11/</li> <li>https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html-single/installing_clusters/index#install-config-example-inventories</li> <li>https://docs.openshift.com/container-platform/3.11/admin_guide/manage_users.html</li> <li>https://itnext.io/explore-different-methods-to-build-and-push-image-to-private-registry-with-tekton-pipelines-5cad9dec1ddc</li> </ul>"},{"location":"openshift/rhos311-gcp-minimal/#references","title":"References","text":"<ol> <li> <p>https://cloud.google.com/solutions/partners/openshift-on-gcp \u21a9</p> </li> <li> <p>https://access.redhat.com/documentation/en-us/reference_architectures/2018/html/deploying_and_managing_openshift_3.9_on_google_cloud_platform/ \u21a9</p> </li> <li> <p>https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-install-host-preparation#ensuring-host-access \u21a9</p> </li> <li> <p>https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-config-install-host-preparation#configuring-docker-storage \u21a9</p> </li> <li> <p>https://access.redhat.com/RegistryAuthentication \u21a9</p> </li> <li> <p>https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html-single/installing_clusters/index#install-config-example-inventories \u21a9</p> </li> <li> <p>https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-running-installation-playbooks#running-the-advanced-installation-rpm \u21a9</p> </li> <li> <p>https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/installing_clusters/install-running-installation-playbooks#running-the-advanced-installation-containerized \u21a9</p> </li> </ol>"},{"location":"other/mkdocs/","title":"MKDocs","text":"<p>This website is build using the following:</p> <ul> <li>MKDocs a python tool for building static websites from MarkDown files</li> <li>MK Material expansion/theme of MK Docs that makes it a responsive website with Google's Material theme</li> </ul>"},{"location":"other/mkdocs/#add-information-to-the-docs","title":"Add information to the docs","text":"<p>MKDocs can be a bit daunting to use, especially when extended with <code>MKDocs Material</code> and PyMdown Extensions.</p> <p>There are two parts to the site: 1) the markdown files, they're in <code>docs/</code> and 2) the site listing (mkdocs.yml) and automation scripts, these can be found in <code>docs-scripts/</code>.</p>"},{"location":"other/mkdocs/#extends-current-page","title":"Extends current page","text":"<p>To extend a current page, simply write the MarkDown as you're used to.</p> <p>For the specific extensions offered by PyMX and Material, checkout the following pages:</p> <ul> <li>MKDocs Material Getting Started Guide</li> <li>MKDocs Extensions</li> <li>PyMdown Extensions Usage Guide</li> </ul>"},{"location":"other/mkdocs/#add-a-new-page","title":"Add a new page","text":"<p>In the <code>docs-scripts/mkdocs.yml</code> you will find the site structure under the yml item of <code>pages</code>.</p> <pre><code>pages:\n- Home: index.md\n- Other Root Page: some-page.md\n- Root with children:\n  - ChildOne: root2/child1.md\n  - ChildTwo: root2/child2.md\n</code></pre>"},{"location":"other/mkdocs/#things-to-know","title":"Things to know","text":"<ul> <li>All .md files that are listed in the <code>pages</code> will be translated to an HTML file and dubbed {OriginalFileName}.html</li> <li>Naming a file index.md will allow you to refer to it by path without the file name<ul> <li>we can refer to root2 simply by <code>site/root2</code> and can omit the index. <pre><code>- Root: index.md\n- Root2: root2/index.html\n</code></pre></li> </ul> </li> </ul>"},{"location":"other/mkdocs/#configuration-of-this-website","title":"Configuration Of This Website","text":"<pre><code># Theme\n# Configuration\ntheme:\n  feature:\n    tabs: true\n  name: 'material'\n  language: 'en'\n  logo:\n    icon: 'public'\n  palette:\n    primary: 'orange'\n    accent: 'red'\n  font:\n    text: 'Roboto'\n    code: 'Roboto Mono'\n\nplugins:\n  - search\n  - minify:\n      minify_html: true\n\nextra:\n  social:\n    - type: 'github'\n      link: 'https://github.com/joostvdg'\n    - type: 'twitter'\n      link: 'https://twitter.com/joost_vdg'\n    - type: 'linkedin'\n      link: 'https://linkedin.com/in/joostvdg'\n\n# Extensions\nmarkdown_extensions:\n  - admonition\n  - codehilite:\n      linenums: true\n      guess_lang: true\n  - footnotes\n  - meta\n  - toc:\n      permalink: true\n  - pymdownx.arithmatex\n  - pymdownx.betterem:\n      smart_enable: all\n  - pymdownx.caret\n  - pymdownx.details\n  - pymdownx.critic\n  - pymdownx.inlinehilite\n  - pymdownx.magiclink\n  - pymdownx.mark\n  - pymdownx.smartsymbols\n  - pymdownx.superfences\n  - pymdownx.tasklist:\n      custom_checkbox: true\n  - pymdownx.tilde\n</code></pre>"},{"location":"other/mkdocs/#build-the-site-locally","title":"Build the site locally","text":"<p>As it is a Python tool, you can easily build it with Python (2.7 is recommended).</p> <p>The requirements are captured in a pip install scripts: <code>docs-scripts/install.sh</code> where the dependencies are in Pip's requirements.txt.</p> <p>Once that is done, you can do the following:</p> <pre><code>mkdocs build --clean\n</code></pre> <p>Which will generate the site into <code>docs-scripts/site</code> where you can simply open the index.html with a browser - it is a static site.</p> <p>For docker, you can use the <code>*.sh</code> scripts, or simply <code>run.sh</code> to kick of the entire build.</p>"},{"location":"other/mkdocs/#dependencies","title":"Dependencies","text":"<p>You can use pip to manage the dependencies required for building the site.</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"other/mkdocs/#requirementstxt","title":"Requirements.txt","text":"<pre><code>mkdocs&gt;=1.0.4\nmkdocs-bootswatch&gt;=0.4.0\npython-jenkins&gt;=0.4.10\nmkdocs-material&gt;=4.4.0\nmkdocs-minify-plugin&gt;=0.1.0\npygments&gt;=2.4.2\npymdown-extensions&gt;=6.0.0\nMarkdown&gt;=3.0.1\n</code></pre>"},{"location":"other/mkdocs/#host-it-with-docker","title":"Host It With Docker","text":""},{"location":"other/mkdocs/#dockerfile","title":"Dockerfile","text":"<pre><code>FROM nginx:mainline\n\nLABEL authors=\"Joost van der Griendt &lt;joostvdg@gmail.com&gt;\"\nLABEL version=\"1.0.0\"\nLABEL description=\"Mr J's knowledge base\"\n\nRUN apt-get update &amp;&amp; apt-get install --no-install-recommends -y curl=7.* &amp;&amp; rm -rf /var/lib/apt/lists/*\nHEALTHCHECK CMD curl --fail http://localhost:80/docs/ || exit 1\nCOPY site/ /usr/share/nginx/html/docs\nRUN ls -lath /usr/share/nginx/html/docs\n</code></pre>"},{"location":"other/mkdocs/#build","title":"Build","text":"<pre><code>#!/usr/bin/env bash\nTAGNAME=\"joostvdg-github-io-image\"\n\necho \"# Building new image with tag: $TAGNAME\"\ndocker build --tag=$TAGNAME .\n</code></pre>"},{"location":"other/mkdocs/#run","title":"Run","text":"<pre><code>#!/usr/bin/env bash\nIMAGE=\"joostvdg-github-io-image\"\nNAME=\"joostvdg-github-io-instance\"\n\nRUNNING=`docker ps | grep -c $NAME`\nif [ $RUNNING -gt 0 ]\nthen\n   echo \"Stopping $NAME\"\n   docker stop $NAME\nfi\n\nEXISTING=`docker ps -a | grep -c $NAME`\nif [ $EXISTING -gt 0 ]\nthen\n   echo \"Removing $NAME\"\n  docker rm $NAME\nfi\n\necho \"Create new instance $NAME based on $IMAGE\"\ndocker run --name $NAME -d -p 8088:80 $IMAGE\n\necho \"Tail the logs of the new instance\"\ndocker logs $NAME\n\n# IP=$(docker inspect --format '{{.NetworkSettings.Networks.bridge.IPAddress}}' $NAME)\n# echo \"IP address of the container: $IP\"\necho \"http://127.0.0.1.nip.io:8088/docs/\"\n</code></pre>"},{"location":"other/mkdocs/#jenkins-build","title":"Jenkins build","text":""},{"location":"other/mkdocs/#declarative-format","title":"Declarative format","text":"<pre><code>pipeline {\n    agent none\n    options {\n        timeout(time: 10, unit: 'MINUTES')\n        timestamps()\n        buildDiscarder(logRotator(numToKeepStr: '5'))\n    }\n    stages {\n        stage('Prepare'){\n            agent { label 'docker' }\n            steps {\n                deleteDir()\n            }\n        }\n        stage('Checkout'){\n            agent { label 'docker' }\n            steps {\n                checkout scm\n                script {\n                    env.GIT_COMMIT_HASH = sh returnStdout: true, script: 'git rev-parse --verify HEAD'\n                }\n            }\n        }\n        stage('Build Docs') {\n            agent {\n                docker {\n                    image \"caladreas/mkdocs-docker-build-container\"\n                    label \"docker\"\n                }\n            }\n            steps {\n                sh 'cd docs-scripts &amp;&amp; mkdocs build'\n            }\n        }\n        stage('Prepare Docker Image'){\n            agent { label 'docker' }\n            environment {\n                DOCKER_CRED = credentials('ldap')\n            }\n            steps {\n                parallel (\n                        TestDockerfile: {\n                            script {\n                                def lintResult = sh returnStdout: true, script: 'cd docs-scripts &amp;&amp; docker run --rm -i lukasmartinelli/hadolint &lt; Dockerfile'\n                                if (lintResult.trim() == '') {\n                                    println 'Lint finished with no errors'\n                                } else {\n                                    println 'Error found in Lint'\n                                    println \"${lintResult}\"\n                                    currentBuild.result = 'UNSTABLE'\n                                }\n                            }\n                        }, // end test dockerfile\n                        BuildImage: {\n                            sh 'chmod +x docs-scripts/build.sh'\n                            sh 'cd docs-scripts &amp;&amp; ./build.sh'\n                        },\n                        login: {\n                            sh \"docker login -u ${DOCKER_CRED_USR} -p ${DOCKER_CRED_PSW} registry\"\n                        }\n                )\n            }\n            post {\n                success {\n                    sh 'chmod +x push.sh'\n                    sh './push.sh'\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"other/shell/","title":"Shell","text":""},{"location":"other/shell/#end-result","title":"End Result","text":""},{"location":"other/shell/#osx","title":"OSX","text":""},{"location":"other/shell/#iterm2-solarized-ozsh-font-awesome","title":"Iterm2 + Solarized + OZSH + Font Awesome","text":"<ul> <li>https://gist.github.com/kevin-smets/8568070</li> </ul>"},{"location":"other/shell/#using-powerline-font-with-vs-code-terminal","title":"Using Powerline Font with VS Code Terminal","text":"<ul> <li>https://medium.com/@hippojs.guo/vs-code-fix-fonts-in-terminal-761cc821ef41</li> </ul>"},{"location":"other/shell/#my-config-9k","title":"My Config - 9K","text":"<pre><code>ZSH_THEME=\"powerlevel9k/powerlevel9k\"\n#ZSH_THEME=\"powerlevel10k/powerlevel10k\"\nPOWERLEVEL9K_MODE=\"awesome-patched\"\nP9KGT_BACKGROUND='dark'\nP9KGT_COLORS='light'\nPOWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(\n        dir\n        vcs\n)\nPOWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(\n        status\n        command_execution_time\n        kubecontext\n        battery\n        ram\n        time\n)\n\nPOWERLEVEL9K_STATUS_ICON_BEFORE_CONTENT=true\nPOWERLEVEL9K_PROMPT_ON_NEWLINE=true\nPOWERLEVEL9K_RPROMPT_ON_NEWLINE=true\nPOWERLEVEL9K_PROMPT_ADD_NEWLINE=true\n</code></pre>"},{"location":"other/vim/","title":"VIM","text":""},{"location":"other/vim/#end-result","title":"End Result","text":""},{"location":"other/vim/#install-vundle","title":"Install Vundle","text":"<pre><code>git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim\n</code></pre>"},{"location":"other/vim/#install-plugins","title":"Install plugins","text":"<pre><code>vim ~/.vimrc\n</code></pre> <pre><code>filetype off\nfiletype plugin indent on\nsyntax on\n\nset rtp+=~/.vim/bundle/Vundle.vim\ncall vundle#begin()\n\nPlugin 'gmarik/Vundle.vim'\nPlugin 'reedes/vim-thematic'\nPlugin 'airblade/vim-gitgutter'\nPlugin 'vim-airline/vim-airline'\nPlugin 'vim-airline/vim-airline-themes'\nPlugin 'itchyny/lightline.vim'\nPlugin 'nathanaelkane/vim-indent-guides'\nPlugin 'scrooloose/nerdtree'\nPlugin 'editorconfig/editorconfig-vim'\nPlugin 'mhinz/vim-signify'\n\ncall vundle#end()\n\nfiletype plugin indent on\n</code></pre> <p>Open VIM, and install the plugins:</p> <pre><code>:installPlugins\n</code></pre>"},{"location":"productivity/","title":"Developer Productivity","text":""},{"location":"productivity/#commoditization","title":"Commoditization","text":"<p>\"The big change has been in the hardware/software cost ratio. The buyer of a $2 million machine in 1960 felt that he could afford $250,000 more ofr a customized payroll program, one that slipped easily and nondisruptively into the computer-hostile social environment. Buyers of %50,000 office machines today cannot conceivably afford customized payroll programs; so they adapt their paryoll procedures to the packages available.\" - <sup>2</sup> F. Brooks - No Silver Bullet</p>"},{"location":"productivity/#where-should-productivity-be-sought","title":"Where should productivity be sought","text":"<p>If you're looking to increase productivity, it would be best to answer some fundamental questions first.</p> <ul> <li>What should we be productive in?</li> <li>What is productivity?</li> <li>How do you measure productivity?</li> </ul> <p>The first step is to determine, what you should be productive in. If you're building software for example, it is in finding out what to build.</p> <p>\"The hardest single part of building a software system is deciding precisely what to build.\" - F. Brooks <sup>2</sup></p> <p>That is actually already one step to far, as you would need a reason to build a software system. So the first step for any individual or organization (start up, or otherwise) is to find out what people want that you can offer.</p> <p>\"The fundamental activity of a startup is to turn ideas into products, measure how customers respond, and then learn whether to pivot or persevere. All successful startup processes should be geared to accelerate that feedback loop.\" - The Lean Startup <sup>3</sup></p>"},{"location":"productivity/#how-do-you-measure-productivity","title":"How do you measure productivity","text":""},{"location":"productivity/#grow-vs-build","title":"Grow v.s. Build","text":""},{"location":"productivity/#the-balancing-act-between-centralized-and-decentralized","title":"The Balancing act between centralized and decentralized","text":""},{"location":"productivity/#on-multitasking","title":"On Multitasking","text":"<ul> <li>Deep Work</li> <li>Attention Residue &gt; Why is it so hard to do my work</li> </ul>"},{"location":"productivity/#learning-from-leantoyota","title":"Learning from Lean/Toyota","text":""},{"location":"productivity/#open-space-floor-plans","title":"Open Space Floor Plans","text":"<p>http://rstb.royalsocietypublishing.org/content/373/1753/20170239</p>"},{"location":"productivity/#conways-law","title":"Conway's Law","text":"<p>\"organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations.\" - M. Conway <sup>1</sup></p>"},{"location":"productivity/#undifferentiated-heavy-lifting","title":"Undifferentiated Heavy Lifting","text":"<p>\"Work that needs to get done, but having it done doesn't bring our customers any direct benefit.\" - Dave Hahn </p>"},{"location":"productivity/#agile","title":"Agile","text":"<ul> <li>https://www.infoq.com/articles/agile-agile-blah-blah/</li> <li>https://www.infoq.com/articles/death-agile-beyond</li> </ul>"},{"location":"productivity/#hermetic-builds","title":"Hermetic Builds","text":"<p>Build system. All code in Google's repository builds with a customized version of the Bazel build system,5 requiring that builds be hermetic; that is, all inputs must be explicitly declared and stored in source control so the builds are easily distributed and parallelized. - Lessons From Building Static Code Analysis Tools At Google</p>"},{"location":"productivity/#how-to-get-developers-to-adopt-something","title":"How To Get Developers To Adopt Something","text":""},{"location":"productivity/#clear-the-bucket-before-fixing-the-leak","title":"Clear The Bucket Before Fixing The Leak","text":"<p>Concurrent with FindBugs experimentation, the C++ workflow at Google was improving with the addition of new checks to the Clang compiler. The Clang team implemented new compiler checks, along with suggested fixes, then used ClangMR38 to run the updated compiler in a distributed way over the entire Google codebase, refine checks, and programmatically fix all existing instances of a problem in the codebase. Once the codebase was cleansed of an issue, the Clang team enabled the new diagnostic as a compiler error (not a warning, which the Clang team found Google developers ignored) to break the build, a report difficult to disregard. The Clang team was very successful improving the codebase through this strategy. - Lessons From Building Static Code Analysis Tools At Google</p>"},{"location":"productivity/#make-it-part-of-their-daily-work","title":"Make It Part Of Their Daily Work","text":"<p>Most developers will not go out of their way to use static analysis tools. Following in the footsteps of many commercial tools, Google's initial implementation of FindBugs relied on engineers choosing to visit a central dashboard to see the issues found in their projects, though few of them actually made such a visit. Finding bugs in checked-in code (that may already be deployed and running without user-visible problems) is too late. To ensure that most or all engineers see static-analysis warnings, analysis tools must be integrated into the workflow and enabled by default for everyone. Instead of providing bug dashboards, projects like Error Prone extend the compiler with additional checks, and surface analysis results in code review. - Lessons From Building Static Code Analysis Tools At Google</p>"},{"location":"productivity/#make-it-usefull-for-them","title":"Make It Usefull For Them","text":"<p>Developer happiness is key. In our experience and in the literature, many attempts to integrate static analysis into a software-development organization fail. At Google, there is typically no mandate from management that engineers use static analysis tools. Engineers working on static analysis must demonstrate impact through hard data. For a static analysis project to succeed, developers must feel they benefit from and enjoy using it. - Lessons From Building Static Code Analysis Tools At Google</p>"},{"location":"productivity/#further-reading","title":"Further reading","text":""},{"location":"productivity/#others","title":"Others","text":"<ul> <li>https://www.youtube.com/watch?v=UTKIT6STSVM</li> <li>https://en.wikipedia.org/wiki/Complex_adaptive_system</li> <li>https://jobs.netflix.com/culture</li> <li>http://blackswanfarming.com/cost-of-delay/</li> <li>https://www.rundeck.com/blog/tickets_make_operations_unnecessarily_miserable</li> <li>https://www.digitalocean.com/community/tutorials/what-is-immutable-infrastructure</li> <li>https://hbr.org/2015/12/what-the-research-tells-us-about-team-creativity-and-innovation</li> <li>https://www.thoughtworks.com/insights/blog/continuous-improvement-safe-environment</li> <li>https://qualitysafety.bmj.com/content/13/suppl_2/ii22</li> <li>https://www.plutora.com/wp-content/uploads/dlm_uploads/2018/03/StateOfDevOpsTools_v14.pdf</li> <li>https://medium.com/@ATavgen/never-fail-twice-608147cb49b</li> <li>https://blogs.dropbox.com/dropbox/2018/07/study-high-performing-teams/?_tk=social&amp;oqa=183tl01liov&amp;linkId=100000003064606</li> <li>http://psycnet.apa.org/record/1979-28632-001</li> <li>https://pdfs.semanticscholar.org/a85d/432f44e43d61753bb8a121c246127b562a39.pdf</li> <li>https://medium.com/@dr_eprice/laziness-does-not-exist-3af27e312d01</li> <li>https://en.wikipedia.org/wiki/Mindset#Fixed_and_growth</li> <li>http://www.reinventingorganizationswiki.com/Teal_Organizations</li> <li>https://www.mckinsey.com/business-functions/organization/our-insights/the-irrational-side-of-change-management</li> <li>https://www.barrypopik.com/index.php/new_york_city/entry/how_do_you_eat_an_elephant</li> <li>https://kadavy.net/blog/posts/mind-management-intro/</li> <li>https://en.wikipedia.org/wiki/Planning_fallacy</li> <li>https://stories.lemonade.com/lemonade-proves-trust-pays-off-big-time-fdcf587af5a1</li> <li>https://www.venturi-group.com/developer-to-cto/</li> <li>https://dzone.com/articles/an-introduction-to-devops-principles</li> <li>https://www.thoughtworks.com/insights/blog/evolving-thoughtworks-internal-it-solve-broader-cross-cutting-problems</li> <li>https://www.thoughtworks.com/insights/blog/platform-tech-strategy-three-layers</li> <li>https://www.thoughtworks.com/insights/blog/why-it-departments-must-reinvent-themselves-part-1</li> <li>https://en.wikipedia.org/wiki/Peter_principle</li> <li>https://hackernoon.com/why-all-engineers-must-understand-management-the-view-from-both-ladders-cc749ae14905</li> <li>https://medium.freecodecamp.org/cognitive-bias-and-why-performance-management-is-so-hard-8852a1b874cd</li> <li>https://en.wikipedia.org/wiki/Horn_effect</li> <li>https://en.wikipedia.org/wiki/Halo_effect</li> <li>http://serendipstudio.org/bb/neuro/neuro02/web2/hhochman.html</li> <li>https://betterhumans.coach.me/how-to-be-a-better-manager-by-understanding-the-difference-between-market-norms-and-social-norms-3082d97d440f</li> <li>https://skillsmatter.com/skillscasts/10466-deep-dive-on-kubernetes-networking</li> <li>https://purplegriffon.com/blog/is-itil-agile-enough</li> <li>https://launchdarkly.com/blog/progressive-delivery-a-history-condensed/</li> <li>http://www.collaborativefund.com/blog/real-world-vs-book-knowledge/</li> <li>https://blog.codeship.com/using-jx-create-gitops-managed-jenkins-x-installation-cloudbees-codeship-terraform-google-kubernetes-engine/</li> <li>https://martinfowler.com/articles/serverless.html</li> <li>https://www.quora.com/Some-people-including-the-creator-of-C-claim-that-there-is-a-huge-decline-of-quality-among-software-developers-What-seems-to-be-the-main-cause</li> <li>https://medium.com/netflix-techblog/full-cycle-developers-at-netflix-a08c31f83249</li> <li>https://queue.acm.org/detail.cfm?id=3182626</li> <li>http://www.safetydifferently.com/why-do-things-go-right/</li> <li>https://www.quora.com/What-is-better-to-become-an-specialist-or-a-generalist-in-software-development-Does-the-full-stack-term-still-makes-sense</li> <li>https://cengizhan.com/3-pillars-of-observability-8e6cb5434206</li> <li>https://www.thoughtworks.com/perspectives/edition1-agile-article</li> <li>https://blog.mexia.com.au/a-pace-layered-integration-architecture</li> </ul>"},{"location":"productivity/#presentations","title":"Presentations","text":"<ul> <li>https://speakerdeck.com/tylertreat/the-future-of-ops</li> <li>https://www.slideshare.net/rheinwein/the-container-shame-spiral</li> </ul>"},{"location":"productivity/#articles","title":"Articles","text":"<ul> <li>https://dzone.com/articles/a-praise-for-self-service-in-it-value-streams</li> <li>http://blog.christianposta.com/microservices/application-safety-and-correctness-cannot-be-offloaded-to-istio-or-any-service-mesh/</li> <li>https://www.gatesnotes.com/Books/Capitalism-Without-Capital?WT.mc_id=08_16_2018_06_CapitalismWithoutCapital_BG-LI_&amp;WT.tsrc=BGLI&amp;linkId=55623312</li> <li>https://uxdesign.cc/stop-delivering-software-with-agile-it-doesn-t-work-edccea3ab5d3</li> <li>Concept of Shared Services and beyond</li> <li>Introduction to Observability by Weave Net</li> <li>Article on the state of Systems Languages</li> <li>Article on SILO's</li> <li>Blog on Twitter's Engineering Efficiency</li> <li>Why Companies should have a Heroku platform for their developers</li> <li>Multitasking is bad for your health</li> <li>Microsoft research on Developer's perception of productivity</li> <li>Developer Productivity Struggles</li> <li>You cannot measure productivity</li> <li>The Productivity Paradox</li> <li>There is no Productivity Paradox: it lags behind investments</li> <li>Economist: solving the paradox</li> <li>The Myth Of Developer Productivity</li> <li>Effectiveness vs. Efficiency</li> <li>Lean Manufactoring</li> <li>Theory of Constraints</li> <li>Thoughtworks: demystifying Conway's Law</li> <li>John Allspaw: a mature role for automation</li> <li>Research from DORA</li> <li>Mik Kersten - Cambrian Explosion of DevOps Tools</li> <li>Mik Kersten - End of Manufacturing Line Analogy</li> <li>Mik Kersten - Mining the ground thruth of Enterprise Toolchains</li> <li>Framework for putting Mental Models to practice</li> <li>Lessons from Building Static Analysis Tools at Google</li> <li>GitHub - Shifting Security Left</li> </ul>"},{"location":"productivity/#books","title":"Books","text":"<ul> <li>The Goal</li> <li>The Phoenix Project</li> <li>Continuous Delivery</li> <li>The Lean Startup</li> <li>The Lean Enterprise</li> <li>DevOps Handbook</li> <li>Thinking Fast and Slow</li> <li>Sapiens</li> <li>Project to Product</li> <li>Debugging Teams</li> <li>The Trusted Advisor</li> <li>Crossing the Chasm</li> </ul>"},{"location":"productivity/#papers","title":"Papers","text":"<ul> <li>https://www.researchgate.net/publication/200085969_Using_task_context_to_improve_programmer_productivity/link/540dce6b0cf2df04e75676d0/download</li> <li>https://storage.googleapis.com/pub-tools-public-publication-data/pdf/3d102e42ad79a345ebd6464047ac9a6cd10670f4.pdf</li> </ul>"},{"location":"productivity/#on-writing","title":"On Writing","text":"<ul> <li>https://www.proofreadingservices.com/pages/very</li> <li></li> </ul>"},{"location":"productivity/#references","title":"References","text":"<ol> <li> <p>Conway's law in wikipedia \u21a9</p> </li> <li> <p>No Silver Bullet - F. Brooks \u21a9\u21a9</p> </li> <li> <p>The Lean Startup Principles \u21a9</p> </li> </ol>"},{"location":"productivity/incidents/","title":"Incidents","text":"<ul> <li>incidents</li> <li>five why's</li> <li>blameless postmortems<ul> <li>identify causes (not culprits)</li> <li>assume good will</li> <li>take your time</li> </ul> </li> <li>dangers of automation</li> <li>observability</li> <li>human bias</li> <li>human factors</li> <li>percententize work<ul> <li>how much percent of work should be what?m</li> <li>figure out how to track, make it easy/automated</li> <li>identify real vs. desired, figure out how to get (closer) to desired</li> </ul> </li> <li>\"Just Culture\" (as in, justice, it is just)</li> <li>bad apple theory = debunked<ul> <li>bad apple theory = remove the small percentage of bad apples and the problem goes away</li> </ul> </li> </ul>"},{"location":"productivity/incidents/#notes","title":"Notes","text":"<ul> <li>most incidents happen near updates/upgrades/deployments<ul> <li>make deployments a non-event</li> <li>small increments, high frequency, automated, tested</li> </ul> </li> <li>it is not systems, it not humans, but humans within systems<ul> <li>human will make mistakes</li> <li>processes are (almost) always part of the problem</li> <li>automation needs to include sanity checks (ranges of sane values)</li> </ul> </li> </ul>"},{"location":"productivity/incidents/#references","title":"References","text":"<p>Below is a significant collection of references to resources that tackle the different parts of incident management. They can explain it better than I ever can, so use them to better your own understanding just as I have.</p>"},{"location":"productivity/incidents/#books","title":"Books","text":"<ul> <li>Foundations of Safety Science</li> <li>Code Complete</li> </ul>"},{"location":"productivity/incidents/#talks","title":"Talks","text":"<ul> <li>Ironies Of Automation</li> <li>Google SRE: Postmortems and Retrospectives</li> <li>John Allspaw: Blameless Post Mortems</li> </ul>"},{"location":"productivity/incidents/#papers","title":"Papers","text":"<ul> <li>Patient Safety and the \"Just Culture\" - Marx D</li> <li>How do systems manage their adaptive capacity to successfully handle disruptions - M Branlat &amp; D Woods</li> <li>Ironies Of Automation - Lisanne Bainbridge</li> <li>Managing The Development Of Large Software Systems - Winston Royce</li> </ul>"},{"location":"productivity/incidents/#articles","title":"Articles","text":"<ul> <li>Weathering The Unexpected - Kripa Krishnan</li> <li>John Allspaw: a mature role for automation</li> <li>John Allspaw: Resillience Engineering: Part I</li> <li>John Allspaw: getting the messy details is critical</li> <li>John Allspaw: Ask Me Anything</li> <li>John Allspaw: Blameless PostMortems And A Just Culture</li> <li>Etsy's Postmortem Proces</li> <li>Etsy's Winning Secret: Don't Play The Blame Game</li> <li>Blameless Portmortems at Etsy</li> <li>Google SRE - Postmortem Culture: Learning from Failure</li> <li>HoneyComb.io - Incident Review</li> <li>GitHub Outage Incident Analysis</li> <li>Google Postmortem</li> <li>AWS Postmortem (S3 outage)</li> <li>GitHub Page Listing Public Post Mortems</li> <li>Charity Majors: I Test In Prod</li> <li>The Network Is Reliable: an informal survey of real-world communications failures</li> <li>Charity Majors: Shipping Software Should Not Be Scary</li> <li>Circle CI: A brief history of devops part I: waterfall</li> <li>Circle CI: A brief history of devops part II: agile development</li> <li>Circle CI: A brief history of devops part III: automated testing and continuous integration</li> <li>Circle CI: A brief history of devops part IV: continuous delivery and deployment</li> </ul>"},{"location":"productivity/intellij/","title":"Intelli J","text":""},{"location":"productivity/intellij/#beneficial-os-changes","title":"Beneficial OS changes","text":""},{"location":"productivity/intellij/#linux","title":"Linux","text":"<ul> <li>Increase inotify watches\\</li> </ul>"},{"location":"productivity/paradigms/","title":"Paradigms","text":""},{"location":"productivity/paradigms/#product-centered","title":"Product centered","text":""},{"location":"productivity/paradigms/#resources","title":"Resources","text":""},{"location":"productivity/remote/","title":"Working Remote","text":""},{"location":"productivity/remote/#resources","title":"Resources","text":"<ul> <li>https://open.nytimes.com/how-to-grow-as-an-engineer-working-remotely-3baff8211f3e</li> </ul>"},{"location":"productivity/studies/","title":"Developer Productivity Studies","text":""},{"location":"productivity/team-development/","title":"Team Development","text":""},{"location":"productivity/team-development/#resources","title":"Resources","text":"<ul> <li>Article about Tuckman's theory of group development</li> <li>Wiki of Tuckman's theory of group development</li> <li>How google thinks about Team Effectiveness</li> <li>Google Re:Work tutorial on Team Effectiveness</li> <li>https://charity.wtf/2018/12/02/software-sprawl-the-golden-path-and-scaling-teams-with-agency/</li> </ul>"},{"location":"productivity/tools/","title":"Developer Productivity Tools","text":""},{"location":"swe/API/","title":"API's","text":"<ul> <li>https://docs.microsoft.com/en-us/azure/architecture/best-practices/api-design</li> </ul>"},{"location":"swe/ddd/","title":"Domain Driven Design","text":""},{"location":"swe/distributed/","title":"Distributed Computing","text":""},{"location":"swe/distributed/#distributed-computing-fundamentals","title":"Distributed Computing fundamentals","text":""},{"location":"swe/distributed/#time-and-event-ordering","title":"Time and Event ordering","text":"<p>See: Lamport timestamp</p>"},{"location":"swe/distributed/#distributed-applications","title":"Distributed Applications","text":""},{"location":"swe/distributed/#topics-to-take-into-account","title":"Topics to take into account","text":"<ul> <li>logging <ul> <li>structured</li> <li>pulled into central log service</li> <li>Java: SLF4J + LogBack?</li> <li>Go: logrus</li> </ul> </li> <li>tracing <ul> <li>sampling based</li> </ul> </li> <li>metrics<ul> <li>prometheus</li> <li>including alert definitions</li> </ul> </li> <li>network connection stability<ul> <li>services discovery</li> <li>loadbalancing</li> <li>circuit brakers</li> <li>backpressure</li> <li>shallow queues</li> <li>connection pools</li> <li>dynamic/randomized backoff procedures</li> </ul> </li> <li>network connection performance<ul> <li>3-step handshake</li> <li>binary over http</li> <li>standard protocols</li> <li>thin wrapper for UI: GraphQL</li> <li>thick wrapper for UI: JSON over HTTP (restful)</li> <li>Service to Service: gRPC / twirp</li> </ul> </li> </ul>"},{"location":"swe/distributed/#designing-distributed-systems-brandon-burns","title":"Designing Distributed Systems - Brandon Burns","text":""},{"location":"swe/distributed/#sidecar-pattern","title":"Sidecar pattern","text":"<pre><code>docker run -d &lt;my-app-image&gt;\n</code></pre> <p>After you run that image, you will receive the identifier for that specific container. It will look something like: cccf82b85000... If you don\u2019t have it, you can always look it up using the docker ps command, which will show all currently running containers. Assuming you have stashed that value in an environment variable named APP_ID, you can then run the topz container in the same PID namespace using:</p> <pre><code>docker run --pid=container:${APP_ID} \\ -p 8080:8080 brendanburns/topz:db0fa58 /server --address=0.0.0.0:8080\n</code></pre>"},{"location":"swe/distributed/#resources","title":"Resources","text":"<ul> <li>Coursera course</li> <li>Article on synchronization in a distributed system</li> <li>http://label-schema.org/</li> <li>https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying</li> <li>https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236</li> <li>https://eng.lyft.com/announcing-envoy-c-l7-proxy-and-communication-bus-92520b6c8191</li> <li>https://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc</li> <li>https://cse.buffalo.edu/~demirbas/publications/cloudConsensus.pdf</li> <li>http://www.read.seas.harvard.edu/~kohler/class/08w-dsi/chandra07paxos.pdf</li> <li>https://medium.com/source-code/understanding-the-memcached-source-code-slab-i-9199de613762</li> </ul>"},{"location":"swe/http-caching/","title":"HTTP Caching","text":"<ul> <li>https://medium.freecodecamp.org/http-caching-in-depth-part-1-a853c6af99db</li> </ul>"},{"location":"swe/important-concepts/","title":"Software Engineering Concepts","text":""},{"location":"swe/important-concepts/#resource-management","title":"Resource management","text":"<p>When there are finite resources in your system, manage them explicitly.</p> <p>Be that memory, CPU, amount of connections to a database or incoming http connections.</p>"},{"location":"swe/important-concepts/#back-pressure","title":"Back Pressure","text":"<p>Back pressure</p> <p>When one component is struggling to keep-up, the system as a whole needs to respond in a sensible way. </p> <p>It is unacceptable for the component under stress to fail catastrophically or to drop messages in an uncontrolled fashion. </p> <p>Since it can\u2019t cope and it can\u2019t fail it should communicate the fact that it is under stress to upstream components  and so get them to reduce the load. This back-pressure is an important feedback mechanism that allows systems to  gracefully respond to load rather than collapse under it. The back-pressure may cascade all the way up to the user,  at which point responsiveness may degrade, but this mechanism will ensure that the system is resilient under load,  and will provide information that may allow the system itself to apply other resources to help distribute the load, see Elasticity. <sup>1</sup></p> <p>Further reading:</p> <ul> <li>DZone article</li> <li>Spotify Engineering</li> </ul>"},{"location":"swe/important-concepts/#memoization","title":"Memoization","text":"<p>Memoization</p> <p>In computing, memoization or memoisation is an optimization technique used primarily to speed up computer programs  by storing the results of expensive function calls and returning the cached result when the same inputs occur again. </p> <p>Memoization has also been used in other contexts (and for purposes other than speed gains),  such as in simple mutually recursive descent parsing.</p>"},{"location":"swe/important-concepts/#important-theories","title":"Important Theories","text":"<ul> <li>Theory of constraints</li> <li>Law of demeter</li> <li>Conway's law</li> <li>Little's law</li> <li>Commoditization</li> <li>Amdahl's Law</li> </ul>"},{"location":"swe/important-concepts/#web-technologies","title":"Web Technologies","text":""},{"location":"swe/important-concepts/#http-caching","title":"HTTP Caching","text":"<ul> <li>https://medium.freecodecamp.org/http-caching-in-depth-part-1-a853c6af99db</li> </ul> <ol> <li> <p>Reactive Manifesto \u21a9</p> </li> <li> <p>Wikipedia article on Memoization \u21a9</p> </li> </ol>"},{"location":"swe/microservices/","title":"Microservices","text":""},{"location":"swe/naming/","title":"On Naming","text":"<p>There are only two hard things in Computer Science: cache invalidation and naming things. -- Phil Karlton</p> <p>There are 2 hard problems in computer science: cache invalidation, naming things, and off-by-1 errors. - ?</p> <p>Naming things is hard, but very important. Naming things correctly conveys meaning, helps us understand and reduces cognitive load.</p>"},{"location":"swe/naming/#talks","title":"Talks","text":"<ul> <li>Peter Hilton: How To Name Things: The Hardest Problem In Programming</li> <li>Jimmy Bogard: Domain Driven Design: The Good Parts</li> </ul>"},{"location":"swe/naming/#slides","title":"Slides","text":"<ul> <li>Peter Hilto: Naming Guidelines For Professional Programmers</li> </ul>"},{"location":"swe/naming/#books","title":"Books","text":"<ul> <li>Domain Driven Design - Eric Evans</li> <li>Clean Code Collection - Robert C Martin</li> </ul>"},{"location":"swe/naming/#other","title":"Other","text":"<ul> <li>Martin Fowler - Two Hard Things</li> <li>Martin Fowler - Ubiquitous Language</li> <li>Industry Specific Dictionaries</li> <li>Clean Coders</li> <li>Peter Hilton - Why Naming Things Is Hard</li> </ul>"},{"location":"swe/observability/","title":"Observability","text":""},{"location":"swe/observability/#resources","title":"Resources","text":"<ul> <li>https://www.vividcortex.com/blog/monitoring-isnt-observability</li> <li>https://medium.com/@copyconstruct/monitoring-and-observability-8417d1952e1c</li> <li>https://codeascraft.com/2011/02/15/measure-anything-measure-everything/</li> </ul>"},{"location":"swe/others/","title":"Software Engineering Concepts","text":""},{"location":"swe/others/#resource-management","title":"Resource management","text":"<p>When there are finite resources in your system, manage them explicitly.</p> <p>Be that memory, CPU, amount of connections to a database or incoming http connections.</p>"},{"location":"swe/others/#back-pressure","title":"Back Pressure","text":"<p>Back pressure</p> <p>When one component is struggling to keep-up, the system as a whole needs to respond in a sensible way. </p> <p>It is unacceptable for the component under stress to fail catastrophically or to drop messages in an uncontrolled fashion. </p> <p>Since it can\u2019t cope and it can\u2019t fail it should communicate the fact that it is under stress to upstream components  and so get them to reduce the load. This back-pressure is an important feedback mechanism that allows systems to  gracefully respond to load rather than collapse under it. The back-pressure may cascade all the way up to the user,  at which point responsiveness may degrade, but this mechanism will ensure that the system is resilient under load,  and will provide information that may allow the system itself to apply other resources to help distribute the load, see Elasticity. <sup>1</sup></p> <p>Further reading:</p> <ul> <li>DZone article</li> <li>Spotify Engineering</li> </ul>"},{"location":"swe/others/#memoization","title":"Memoization","text":"<p>Memoization</p> <p>In computing, memoization or memoisation is an optimization technique used primarily to speed up computer programs  by storing the results of expensive function calls and returning the cached result when the same inputs occur again. </p> <p>Memoization has also been used in other contexts (and for purposes other than speed gains),  such as in simple mutually recursive descent parsing.</p>"},{"location":"swe/others/#important-theories","title":"Important Theories","text":"<ul> <li>Theory of constraints</li> <li>Law of demeter</li> <li>Conway's law</li> <li>Little's law</li> <li>Commoditization</li> <li>Amdahl's Law</li> </ul> <ol> <li> <p>Reactive Manifesto \u21a9</p> </li> <li> <p>Wikipedia article on Memoization \u21a9</p> </li> </ol>"},{"location":"tanzu/","title":"What Is VMware Tanzu","text":"<p>VMware is known, amongst other things, as something related to Virtual Machines.</p> <p>And while VM technology is still essential and helps manage data centers around the globe, it is not at the forefront of software development innovation.</p> <p>In recent years the industry shifted to (more) public cloud and container orchestration as the de facto standard \"operating system.\"</p> <p>In this light, VMware invests a lot in cloud technologies that can work in the public cloud, VMware-managed data centers, and any Kubernetes distribution.</p> <p>These technologies are the VMware Tanzu<sup>1</sup> brand. This also means that VMware Tanzu is not a single thing but a range of products and (OSS) technologies.</p>"},{"location":"tanzu/#tanzu-products","title":"Tanzu Products","text":"<p>The Tanzu family is a rich set of products ranging from licensed self-hosted applications, and cloud-based subscription services, to collections of VMware products<sup>2</sup> with OpenSource Software.</p> <p>Below is a limited list to give you an idea. For a complete overview, visit the VMware Tanzu products.</p> Name Type Abbreviation Description Tanzu Application Platform Suite TAP Suite tools (incl. OSS) for building, testing, and deploying applications in Kubernetes (CI/CD, GitOps, DevSecOps) Tanzu for Kubernetes Operations Suite TKO Suite of licensed (VMware) products for managing Kubernetes clusters (Tanzu Mission Control, Tanzu Observability, and Tanzu Service Mesh) Tanzu Kubernetes Grid - Stand alone Self-host TKGm Kubernetes distribution that runs \"anywhere.\" TKG Manages the underlying infra with Cluster API. Tanzu Kubernetes Grid - vCenter embedded Self-host TKGs Kubernetes distribution managed directly by vCenter. Also known as <code>vSphere with Tanzu</code> (and under the code name <code>Project Pacific</code>) Tanzu Mission Control SaaS TMC Online management platform for managing a fleet of Kubernetes clusters, but not limited to TKG. (package repositories, policies, and namespace). Tanzu Service Mesh SaaS + Self-host TSM Built on top of Istio Service Mesh. Adds a management plane that lets you combine multiple clusters into a single Service Mesh. Tanzu Observability SaaS TO Originally known as Wavefront. Provides a suite of tools to monitor and trace applications and infrastructure (Kubernetes, vCenter, VMs, and so on) Tanzu Build Service Self-host TBS Automated container creation, management, and governance. Build on top of Cloud Native Build packs."},{"location":"tanzu/#references","title":"References","text":"<ol> <li> <p>VMware Tanzu landing page \u21a9</p> </li> <li> <p>VMware Tanzu products overview page \u21a9</p> </li> </ol>"},{"location":"tanzu/custom-ca/","title":"Setup Custom CA","text":"","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/custom-ca/#setup-certificate-authority","title":"Setup Certificate Authority","text":"<p>In Addition, we need to have the Certificate Authority.</p> <p>If you don't have one, or want to learn how to create one yourself, follow along. We use the tools from CloudFlare, cfssl, for this.</p> <p>The documentation is pretty heavy and hard to follow at times, so we take inspiration from this Medium blog post to stick to the basics.</p> <p>The steps are as follows:</p> <ul> <li>create cfssl profile</li> <li>create CA config</li> <li>generate CA certificate and key</li> <li>create server certificate JSON config</li> <li>generate derived certificate and key</li> </ul> <p>The step generate derived certificate and key is done later, when we generate the Harbor certificate (our self-hosted registry of choice).</p>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/custom-ca/#cfssl-profile","title":"CFSSL Profile","text":"<p>Save the following as <code>cfssl.json</code>:</p> cfssl.json<pre><code>{\n  \"signing\": {\n    \"default\": {\n      \"expiry\": \"8760h\"\n    },\n    \"profiles\": {\n      \"intermediate_ca\": {\n        \"usages\": [\n            \"signing\",\n            \"digital signature\",\n            \"key encipherment\",\n            \"cert sign\",\n            \"crl sign\",\n            \"server auth\",\n            \"client auth\"\n        ],\n        \"expiry\": \"8760h\",\n        \"ca_constraint\": {\n            \"is_ca\": true,\n            \"max_path_len\": 0,\n            \"max_path_len_zero\": true\n        }\n      },\n      \"peer\": {\n        \"usages\": [\n            \"signing\",\n            \"digital signature\",\n            \"key encipherment\",\n            \"client auth\",\n            \"server auth\"\n        ],\n        \"expiry\": \"8760h\"\n      },\n      \"server\": {\n        \"usages\": [\n          \"signing\",\n          \"digital signing\",\n          \"key encipherment\",\n          \"server auth\"\n        ],\n        \"expiry\": \"8760h\"\n      },\n      \"client\": {\n        \"usages\": [\n          \"signing\",\n          \"digital signature\",\n          \"key encipherment\",\n          \"client auth\"\n        ],\n        \"expiry\": \"8760h\"\n      }\n    }\n  }\n}\n</code></pre>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/custom-ca/#create-ca","title":"Create CA","text":"<p>Create a JSON config file for your CA: <code>ca.json</code>.</p> <p>This file contains the values of your CA.</p> ca.json<pre><code>{\n  \"CN\": \"Kearos Tanzu Root CA\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"NL\",\n      \"L\": \"Utrecht\",\n      \"O\": \"Kearos\",\n      \"OU\": \"Kearos Tanzu\",\n      \"ST\": \"Utrecht\"\n    }\n  ]\n}\n</code></pre> <p>Field names meaning</p> <p>If you are wondering what those names, such as <code>C</code>, <code>L</code>, mean, here's a table:</p> Abbreviation Description CN CommonName OU OrganizationalUnit O Organization L Locality S StateOrProvinceName C CountryName or CountryCode <p>And then generate the <code>ca.pem</code> and <code>ca-key.pem</code> files:</p> <pre><code>cfssl gencert -initca ca.json | cfssljson -bare ca\n</code></pre>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/custom-ca/#create-server-certificate-config-file","title":"Create Server Certificate Config file","text":"<p>This is very similar to the <code>ca.json</code> file, you can copy most of it.</p> <p>You can include the <code>CN</code> and <code>hostnames</code> fields, but if you want to generate more than one certificate (for multiple hosts), it is better to leave them blank. In the command with which you generate the certificate, you can then supply those with environment variables to make them more dynamic, and make it easier to update them later.</p> <p>Create the following file: <code>base-service-cert.json</code></p> base-service-cert.json<pre><code>{\n    \"key\": {\n      \"algo\": \"rsa\",\n      \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"NL\",\n            \"L\": \"Utrecht\",\n            \"O\": \"Kearos Tanzu\",\n            \"OU\": \"Kearos Tanzu Hosts\",\n            \"ST\": \"Utrecht\"\n        }\n    ]\n}\n</code></pre>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/dependencies/","title":"Dependencies","text":"","tags":["ldap","openldap","dependencies","TAP","TANZU"]},{"location":"tanzu/dependencies/#ldap-dependency","title":"LDAP Dependency","text":"<p>There are many ways to run an LDAP server.</p> <p>I generally use OpenLDAP, via the following:</p> <ul> <li>osixia/openldap container image, well constructed and decently maintained</li> <li>jp-gouin/helm-openldap helm chart, uses the above image and has an HA version</li> </ul>","tags":["ldap","openldap","dependencies","TAP","TANZU"]},{"location":"tanzu/dependencies/#image-relocation","title":"Image Relocation","text":"<p>In case you need to relocate the image to another registry, you can use a Docker client or imgpkg</p> <pre><code>docker pull osixia/openldap:1.5.0 --platform linux/amd64\ndocker tag osixia/openldap:1.5.0 harbor.10.220.2.199.sslip.io/tap/openldap:1.5.0\ndocker push harbor.10.220.2.199.sslip.io/tap/openldap:1.5.0\n</code></pre> <pre><code>imgpkg copy \\\n  -i osixia/openldap:1.5.0 \\\n  --to-repo=harbor.10.220.2.199.sslip.io/tap/openldap\n</code></pre>","tags":["ldap","openldap","dependencies","TAP","TANZU"]},{"location":"tanzu/dependencies/#helm-install","title":"Helm Install","text":"<pre><code>helm repo add helm-openldap https://jp-gouin.github.io/helm-openldap/\n</code></pre> <pre><code>helm repo update\n</code></pre> <pre><code>helm upgrade --install ldap helm-openldap/openldap \\\n  --namespace ldap --create-namespace \\\n  --values ldap-values.yaml\\\n  --version 2.0.4\n</code></pre>","tags":["ldap","openldap","dependencies","TAP","TANZU"]},{"location":"tanzu/dependencies/#helm-values-file","title":"Helm Values File","text":"<p>Hint</p> <p>Dont't forget to replace the <code>image.repository</code> value if you relocated the image!</p> ldap-values.yaml<pre><code>image:\n  repository: osixia/openldap\n  tag: 1.5.0\n\nphpldapadmin:\n  enabled: false\n\nltb-passwd:\n  enabled: false\n\ncustomLdifFiles:\n\n  main.ldif: |\n  ...\n\nadminPassword: C5z6DUTNSMDoiWCHI2GIuSPIzCJt5Zo0\nconfigPassword: C5z6DUTNSMDoiWCHI2GIuSPIzCJt5Zo0\n</code></pre> <p>Warning</p> <p>A full example of the helm values is found github.com/joostvdg/tanzu-example.</p>","tags":["ldap","openldap","dependencies","TAP","TANZU"]},{"location":"tanzu/dependencies/#main-ldif","title":"Main LDIF","text":"<p>The following file is not recommend for production, but should be fine for testing and PoC environments.</p> main.ldif <p>This is an example contents of the <code>customLdifFiles.main.ldif</code> property of the values file above.</p> main.ldif<pre><code># define people and groups as category\ndn: ou=People, dc=example,dc=org\nobjectclass: top\nobjectclass: organizationalunit\nou: People\n\ndn: ou=Groups, dc=example,dc=org\nobjectclass: top\nobjectclass: organizationalunit\nou: Groups\n\n# add Administrator group and add me and admin as members\ndn: cn=BlueAdmins, ou=Groups,dc=example,dc=org\nobjectclass: top\nobjectclass: groupOfNames\ncn: BlueAdmins\nou: Groups\nmember: uid=blueadmin,ou=People, dc=example,dc=org\n\ndn: cn=Blue, ou=Groups,dc=example,dc=org\nobjectclass: top\nobjectclass: groupOfNames\ncn: Blue\nou: Groups\nmember: uid=blueadmin,ou=People, dc=example,dc=org\nmember: uid=bluedev,ou=People, dc=example,dc=org\n\ndn: uid=blueadmin, ou=People, dc=example,dc=org\nuid: blueadmin\ncn: blueadmin\nsn: Admin\ngivenname: Blue\nobjectclass: top\nobjectclass: person\nobjectclass: organizationalPerson\nobjectclass: inetOrgPerson\nou: People\nmail: blueadmin@example.org\nuserpassword: C5z6DUTNSMDoiWCHI2GIuSPIzCJt5Zo0\n\ndn: uid=bluedev, ou=People, dc=example,dc=org\nuid: bluedev\ncn: bluedev\nsn: Dev\ngivenname: Blue\nobjectclass: top\nobjectclass: person\nobjectclass: organizationalPerson\nobjectclass: inetOrgPerson\nou: People\nmail: bluedev@example.org\nuserpassword: C5z6DUTNSMDoiWCHI2GIuSPIzCJt5Zo0\n</code></pre>","tags":["ldap","openldap","dependencies","TAP","TANZU"]},{"location":"tanzu/dependencies/#verify-ldap-with-ldapsearch","title":"Verify LDAP with ldapsearch","text":"<p>In case you're not sure about the credentials used, you can retrieve them from the namespace you installed LDAP in.</p> <p>In this case, I've installed LDAP in the <code>ldap</code> namespace, and named the helm release <code>ldap</code>, which then gets <code>-openldap</code> concatenated to it.</p> <pre><code>LDAP_ADMIN_PASS=$(kubectl get secret --namespace ldap ldap-openldap -o jsonpath=\"{.data.LDAP_ADMIN_PASSWORD}\" | base64 --decode)\nLDAP_CONFIG_PASS=$(kubectl get secret --namespace ldap ldap-openldap -o jsonpath=\"{.data.LDAP_CONFIG_PASSWORD}\" | base64 --decode)\n</code></pre> <p>You can open a shell to any of the Pods, there are three by default.</p> <pre><code>kubectl get pod -n ldap\n</code></pre> <pre><code>NAME              READY   STATUS    RESTARTS   AGE\nldap-openldap-0   1/1     Running   0          19h\nldap-openldap-1   1/1     Running   0          19h\nldap-openldap-2   1/1     Running   0          19h\n</code></pre> <p>Either execute a single command at a time:</p> <pre><code>kubectl exec -n ldap --stdin=true --tty=true ldap-openldap-0 \\\n    -- ldapsearch -x -H ldap://ldap-openldap.ldap.svc.cluster.local:389 \\\n    -b dc=example,dc=org -D \"cn=admin,dc=example,dc=org\" -w $LDAP_ADMIN_PASS\n</code></pre> <p>Or you can enter the shell session, and run multiple commands.</p> <pre><code>kubectl exec -n ldap --stdin=true --tty=true ldap-openldap-0 -- bash\n</code></pre> <p>Set the password to the <code>LDAP_ADMIN_PASSWORD</code> that you collected via the shell commands above.</p> <pre><code>LDAP_ADMIN_PASS=\n</code></pre>","tags":["ldap","openldap","dependencies","TAP","TANZU"]},{"location":"tanzu/dependencies/#ldap-searches","title":"LDAP Searches","text":"<p>Note</p> <p>This assumes your LDAP is installed in the namespace <code>ldap</code> with Helm release name <code>ldap</code>. The service fronting the LDAP pods is then accessible at this dns: <code>ldap-openldap.ldap.svc.cluster.local</code>.</p> <p>If you changed these values, change the commands appropriately.</p> <p>Finds all entries.</p> <pre><code>ldapsearch -x \\\n    -H ldap://ldap-openldap.ldap.svc.cluster.local:389 \\\n    -b dc=example,dc=org \\\n    -D \"cn=admin,dc=example,dc=org\" \\\n    -w $LDAP_ADMIN_PASS\n</code></pre> <p>Finds users.</p> <pre><code>ldapsearch -x \\\n    -H ldap://ldap-openldap.ldap.svc.cluster.local:389 \\\n    -b dc=example,dc=org -D \"cn=admin,dc=example,dc=org\" \\\n    -s sub \"(objectclass=inetOrgPerson)\"\\\n    -w $LDAP_ADMIN_PASS\n</code></pre> <p>Finds groups.</p> <pre><code>ldapsearch -x \\\n    -H ldap://ldap-openldap.ldap.svc.cluster.local:389\\\n    -b dc=example,dc=org -D \"cn=admin,dc=example,dc=org\" \\\n    -s sub \"(&amp;(objectClass=groupOfNames)(ou=Groups))\"\\\n    -w $LDAP_ADMIN_PASS\n</code></pre> <p>Finds groups this user belongs to.</p> <pre><code>ldapsearch -x \\\n    -H ldap://ldap-openldap.ldap.svc.cluster.local:389 \\\n    -b dc=example,dc=org -D \"cn=admin,dc=example,dc=org\" \\\n    -s sub \"(&amp; (objectclass=groupOfNames)(member=uid=bluedev, ou=People, dc=example,dc=org) )\" \\\n    -w $LDAP_ADMIN_PASS\n</code></pre>","tags":["ldap","openldap","dependencies","TAP","TANZU"]},{"location":"tanzu/grype-airgapped/","title":"Use Grype In Restricted Environments","text":"<p>Warning</p> <p>This guide is aimed at TAP version <code>1.3.4</code>. While some things might be applicable to other versions, it is best to verify.</p> <p>One of the key components of the Scanning and Testing Supply Chain, is Grype.</p> <p>It cross references packages from SBOM files with CVE databases.</p> <p>In restricted environments, Grype cannot retrieve these databases. So we bring it to Grype instead.</p> <p>What we want to achieve:</p> <ul> <li>Grype has access to an up-to-date CVE database</li> <li>TAP's Scanning &amp; Testing pipeline uses Grype</li> </ul> <p>To achieve that, we do the following:</p> <ul> <li>retrieve the database listing file, containing the index of vulnerability database files</li> <li>strip the listing to the latest files relevant to our version of Grype (version 5)</li> <li>relocate the database file(s) from our listing to an in-cluster storage solution</li> <li>update the listing to point to the relocated database file(s)</li> <li>upload the listing to the same in-cluster storage solution</li> <li>configure TAP so its Grype tasks, use the relocated listing and database files</li> </ul> <p>Pre-Requisites</p> <p>As for the in-cluster storage solution, VMware supports and recommends MinIO.</p> <p>MinIO supports everything we need, so we use it for this guide.</p> <p>For installing MinIO, you can follow the MinIO with custom CA guide.</p> <p>To store the files we need, create a Bucket in MinIO named <code>grype</code>.</p>","tags":["TKG","TAP","TANZU","DevSecOps","Grype"]},{"location":"tanzu/grype-airgapped/#relocate-grype-database","title":"Relocate Grype Database","text":"","tags":["TKG","TAP","TANZU","DevSecOps","Grype"]},{"location":"tanzu/grype-airgapped/#verify-minio-connection","title":"Verify MinIO Connection","text":"<p>First, make sure you have the MinIO Client installed.</p> <p>Next, ensure your connection works by setting an alias.</p> <pre><code>export MINIO_HOSTNAME=\"minio-console.view.h2o-2-4864.h2o.vmware.com\"\nmc alias set minio_h20 https://$MINIO_HOSTNAME administrator 'VMware123!'\n</code></pre>","tags":["TKG","TAP","TANZU","DevSecOps","Grype"]},{"location":"tanzu/grype-airgapped/#download-database-listing","title":"Download Database Listing","text":"<p>The database listing of Grype is hosted by Anchor, the company behind the project.</p> <p>First, ensure your current directory does not have an existing file.</p> <pre><code>rm listing.json\n</code></pre> <p>Next, download the listing with your favorite CLI.</p> HTTPieCurl <pre><code>http --download https://toolbox-data.anchore.io/grype/databases/listing.json\n</code></pre> <pre><code>curl -O https://toolbox-data.anchore.io/grype/databases/listing.json\n</code></pre>","tags":["TKG","TAP","TANZU","DevSecOps","Grype"]},{"location":"tanzu/grype-airgapped/#limit-listing","title":"Limit Listing","text":"<p>The listing file contains all the versions of the vulnerability database of the last couple of years. Some of those are in different formats, to support older versions of Grype.</p> <p>The current format is <code>v5</code>. And, assuming you use the latest file, we can create a new <code>listing.json</code> file to limit it to the latest entry of <code>v5</code>.</p> <pre><code>cp listing.json listing_original.json\necho '{\"available\": {\"5\": [' &gt; listing_tmp.json\ncat listing_original.json | jq '.available.\"5\"[0]' &gt;&gt; listing_tmp.json\necho ']}}' &gt;&gt; listing_tmp.json\ncat listing_tmp.json | jq &gt; listing.json\n</code></pre>","tags":["TKG","TAP","TANZU","DevSecOps","Grype"]},{"location":"tanzu/grype-airgapped/#relocate-database-file","title":"Relocate Database File","text":"<p>Oke, so we now have the listing limited to the only database file we need.</p> <p>We need to relocate the Database file(s) in the listing to MinIO.</p> <p>The first step is to download them. The script below generates a script with the download instructions per database file.</p> HTTPieCurlWget <pre><code>cat listing.json |jq -r '.available[] | values[].url' \\\n  | awk '{print \"http --download \" $1}' &gt; grype_down.sh\n</code></pre> <pre><code>cat listing.json |jq -r '.available[] | values[].url' \\\n  | awk '{print \"curl -O \" $1}' &gt; grype_down.sh\n</code></pre> <pre><code>cat listing.json |jq -r '.available[] | values[].url' \\\n  | awk '{print \"wget \" $1}' &gt; grype_down.sh\n</code></pre> <p>Make the script executable and run it, to download the database file.</p> <pre><code>chmod +x ./grype_down.sh\n./grype_down.sh\n</code></pre> <p>We then upload the file to MinIO.</p> <pre><code>mc cp *.tar.gz minio_h20/grype/databases/\n</code></pre>","tags":["TKG","TAP","TANZU","DevSecOps","Grype"]},{"location":"tanzu/grype-airgapped/#update-listing-addresses","title":"Update Listing Addresses","text":"<p>Now that the database file is in MinIO, we update the Listing file with the new address.</p> <p>We're using trusted old <code>sed</code> to replace the original URL with our MinIO one.</p> <pre><code>cp listing.json listing_copy.json\nsed -i -e \\\n  \"s/https:\\/\\/toolbox-data.anchore.io\\/grype/https:\\/\\/$MINIO_HOSTNAME\\/grype/g\" \\\n  listing.json\n</code></pre> <p>Then we upload the updated listing file to MinIO as well.</p> <pre><code>mc cp listing.json minio_h20/grype/databases/\n</code></pre>","tags":["TKG","TAP","TANZU","DevSecOps","Grype"]},{"location":"tanzu/grype-airgapped/#verify-storage","title":"Verify Storage","text":"<p>You can use the MinIO Client to verify all the files are where they need to be.</p> <pre><code>mc ls minio_h20/grype/databases/\n</code></pre> <p>Which in my case looks like this:</p> <pre><code>[2023-02-24 12:30:11 CET]   362B STANDARD listing.json\n[2023-02-24 12:30:08 CET] 115MiB STANDARD vulnerability-db_v5_2023-02-22T08:14:22Z_cdcf8d5090cea7f88618.tar.gz\n[2023-02-24 12:30:06 CET] 115MiB STANDARD vulnerability-db_v5_2023-02-24T08:14:14Z_c949c91133733755c359.tar.gz\n</code></pre> <p>Make sure to verify the download address. We need this for the Grype configuration in the TAP install values.</p> <p>For example, like this:</p> <pre><code>http --verify=false \\\n  https://$MINIO_HOSTNAME/grype/databases/listing.json\n</code></pre>","tags":["TKG","TAP","TANZU","DevSecOps","Grype"]},{"location":"tanzu/grype-airgapped/#update-tap-install","title":"Update TAP Install","text":"<p>It's great we have the database and listing file internally now.</p> <p>Unfortunately, unless the supply chain uses this, Grype still fails.</p> <p>Pre-requisites</p> <p>Using Grype with TAP van be done manually, or via the pre-defined Scanning &amp; Testing supply chain.</p> <p>The rest of the guide assumes you are have the following installed via TAP: * OOTB Scanning &amp; Testing supply chain * Metadata Store</p>","tags":["TKG","TAP","TANZU","DevSecOps","Grype"]},{"location":"tanzu/grype-airgapped/#changes-required","title":"Changes Required","text":"<p>The steps related to Grype, are the Source Scan and the Image Scan.</p> <p>Both of these are configured via a Kubernetes CR, ScanTemplate.</p> <p>There are a couple of things we have to change in those Scan Templates, to get our supply chain to work.</p> <ul> <li>Trust the certificate Metadata Store</li> <li>Tell Grype to use our Listing file in MinIO</li> <li>Force Grype to update its database</li> </ul> <p>We have two ways of doing this:</p> <ol> <li>we can create a new supply chain, with alternatieve ScanTemplate's</li> <li>we use a YTT Overlay to patch the existing ScanTemplate's</li> </ol> <p>Customizing the Supply Chain and creating new Scan Templates requires more work. So let us stick to the Overlay solution.</p> <p>We first explain each component we add to the Overlay before showing the end result.</p> <p>If you are impatient, you can go directly to the overlay YAML</p>","tags":["TKG","TAP","TANZU","DevSecOps","Grype"]},{"location":"tanzu/grype-airgapped/#overlay-for-scan-templates","title":"Overlay For Scan Templates","text":"<p>To ensure Grype trusts the Metadata Store's certificate, we do the following:</p> <ul> <li>create a ConfigMap with the CA certificate in the Developer namespace</li> <li>add the ConfigMap to volumes and a volumeMount of the Scan Template task definition</li> </ul> <p>The first one is straight forward:</p> <pre><code>kubectl create configmap ca-cert --from-file=ca.crt \\\n  --namespace ${TAP_DEVELOPER_NAMESPACE}\n</code></pre> <p>For each ScanTemplate we want to change, we have to add the volume, volumeMount, and set the <code>GRYPE_DB_CA_CERT</code> environment variable.</p> <pre><code>    volumeMounts:\n      #@overlay/append\n      - name: ca-cert\n        mountPath: /etc/ssl/certs/custom-ca.crt\n        subPath: \"ca.crt\"\nvolumes:\n#@overlay/append\n- name: ca-cert\n  configMap:\n    name: ca-cert \n</code></pre> <pre><code>- name: GRYPE_DB_CA_CERT\n  value: /etc/ssl/certs/custom-ca.crt\n</code></pre> <p>Next, we configure Grype to use our in-cluster hosted database listing.</p> <p>We do this by setting several environment variables.</p> <p>Set environment vars for the scanning:</p> <ul> <li>GRYPE_CHECK_FOR_APP_UPDATE: Grype runs as part of a fixed container, updating won't work</li> <li>GRYPE_DB_AUTO_UPDATE: the automatic database update doesn't seem to work, so disable it and do it manually</li> <li>GRYPE_DB_UPDATE_URL: this is where we specify the location of our <code>listing.json</code> (e.g., <code>https://minio.view.h2o-2-4864.h2o.vmware.com/grype/databases/listing.json</code>)</li> <li>GRYPE_DB_MAX_ALLOWED_BUILT_AGE: specify how old the database is allowed to be, assuming you refresh the database every X period, specify X*2 to be safe (e.g., <code>240h</code>)</li> <li>GRYPE_DB_VALIDATE_AGE: or you can disable the database age check all together</li> </ul> <p>This will look as follows:</p> <pre><code>- name: GRYPE_CHECK_FOR_APP_UPDATE\n  value: \"false\"\n- name: GRYPE_DB_AUTO_UPDATE\n  value: \"false\"\n- name: GRYPE_DB_UPDATE_URL\n  value: https://minio.view.h2o-2-4864.h2o.vmware.com/grype/databases/listing.json\n- name: GRYPE_DB_MAX_ALLOWED_BUILT_AGE #! see note on best practices\n  value: \"1200h\"\n- name: GRYPE_DB_VALIDATE_AGE\n  value: \"false\"\n</code></pre> <p>Next, we override the container commands, to force Grype to update its database.</p> <p>We add the additional command <code>grype db update -vv</code>, which forces Grype to update its database with the current settings. The <code>-vv</code> ensures verbose logging, which is especially useful when setting this up the first time.</p> <p>It will look like this for the Image Scan, i.e. the ScanTemplate named <code>private-image-scan-template</code>.</p> <pre><code>#@overlay/match by=overlay.subset({\"name\": \"scan-plugin\"}), expects=\"1+\"\n- name: scan-plugin\n  #@overlay/replace\n  args:\n    - -c\n    - |\n      grype db update -vv\n      ./image/scan-image.sh /workspace /workspace/scan.xml true\n</code></pre> <p>Verify Commands On Upgrades</p> <p>We are also overriding the command used by the ScanTemplate.</p> <p>When upgrading TAP, ensure these commands have not changed!</p> <p>For the Source Scan, we do something similar. It has a different command, so when it comes to the Overlay itself, you'll see more than one <code>#@overlay/match</code>.</p> <p>For the Source Scan, we override two commands. One for the <code>scan-plugin</code>, and one for the <code>metadatastore-plugin-config</code>.</p> <p>For the <code>scan-plugin</code>, we add the same <code>grype db update -vv</code>.</p> <pre><code>#@overlay/match by=overlay.subset({\"name\": \"scan-plugin\"}), expects=\"1+\"\n- name: scan-plugin\n  #@overlay/replace\n  args:\n    - -c\n    - |\n      grype db update -vv\n      ./source/scan-source.sh /workspace/source/scan.xml /workspace/source/out.yaml /workspace/source/repo blob\n</code></pre> <p>For the <code>metadata-store-plugin-config</code>, we add <code>/insight health</code> after the <code>/insight config</code> command, to verify the connection is working.</p> <pre><code>#@overlay/match by=overlay.subset({\"name\": \"metadata-store-plugin-config\"}), expects=\"1+\"\n- name: metadata-store-plugin-config\n  #@overlay/replace\n  args:\n    - -c\n    - |\n      set -euo pipefail\n      /insight config set-target $METADATA_STORE_URL --ca-cert /metadata-store/ca.crt --access-token $METADATA_STORE_ACCESS_TOKEN\n      /insight health\n</code></pre> <p>Insight command</p> <p>The <code>/insight</code> command in the above Overlay snippet, is the Tanzu CLI Insight plug-in.</p> <p>Before showing you the full overlay YAML, let's look at the overlay match statements.</p> <p>For the Image Scan, we do the following:</p> <pre><code>#@overlay/match by=overlay.subset({\"kind\":\"ScanTemplate\",\"metadata\":{\"namespace\":\"default\", \"name\": \"private-image-scan-template\"}}),expects=\"1+\"\n</code></pre> <p>This matches a <code>ScanTemplate</code> CR, in the developer namespace (in my case, <code>default</code>), named <code>private-image-scan-template</code>.</p> <p>We then select the <code>scan-plugin</code> initContainer, with the following spec:</p> <pre><code>spec:\n  template:\n    initContainers:\n      #@overlay/match by=overlay.subset({\"name\": \"scan-plugin\"}), expects=\"1+\"\n      - name: scan-plugin\n</code></pre> <p>For the Soure Scan, we do something similar enough, it does not need to be repeated.</p>","tags":["TKG","TAP","TANZU","DevSecOps","Grype"]},{"location":"tanzu/grype-airgapped/#overlay-yaml","title":"Overlay YAML","text":"Overlay YAML grype-airgap-overlay.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: grype-airgap-overlay\n  namespace: tap-install #! namespace where tap is installed\nstringData:\n  patch.yaml: |\n    #@ load(\"@ytt:overlay\", \"overlay\")\n\n    #@overlay/match by=overlay.subset({\"kind\":\"ScanTemplate\",\"metadata\":{\"namespace\":\"default\", \"name\": \"private-image-scan-template\"}}),expects=\"1+\"\n    #! developer namespace you are using\n    ---\n    spec:\n      template:\n        initContainers:\n          #@overlay/match by=overlay.subset({\"name\": \"scan-plugin\"}), expects=\"1+\"\n          - name: scan-plugin\n            #@overlay/match missing_ok=True\n            env:\n              - name: GRYPE_CHECK_FOR_APP_UPDATE\n                value: \"false\"\n              - name: GRYPE_DB_AUTO_UPDATE\n                value: \"false\"\n              - name: GRYPE_DB_UPDATE_URL\n                value: https://minio.view.h2o-2-4864.h2o.vmware.com/grype/databases/listing.json\n              - name: GRYPE_DB_MAX_ALLOWED_BUILT_AGE #! see note on best practices\n                value: \"1200h\"\n              - name: GRYPE_DB_VALIDATE_AGE\n                value: \"false\"\n              - name: GRYPE_DB_CA_CERT\n                value: /etc/ssl/certs/custom-ca.crt\n            #@overlay/replace\n            args:\n              - -c\n              - |\n                grype db update -vv\n                ./image/scan-image.sh /workspace /workspace/scan.xml true\n            volumeMounts:\n              #@overlay/append\n              - name: ca-cert\n                mountPath: /etc/ssl/certs/custom-ca.crt\n                subPath: \"ca.crt\" #! key pointing to ca certificate\n        volumes:\n        #@overlay/append\n        - name: ca-cert\n          configMap:\n            name: ca-cert #! name of the configmap created\n\n    #@overlay/match by=overlay.subset({\"kind\":\"ScanTemplate\",\"metadata\":{\"namespace\":\"default\", \"name\": \"blob-source-scan-template\"}}),expects=\"1+\"\n    #! developer namespace you are using\n    ---\n    spec:\n      template:\n        initContainers:\n          #@overlay/match by=overlay.subset({\"name\": \"scan-plugin\"}), expects=\"1+\"\n          - name: scan-plugin\n            #@overlay/match missing_ok=True\n            env:\n              - name: GRYPE_CHECK_FOR_APP_UPDATE\n                value: \"false\"\n              - name: GRYPE_DB_AUTO_UPDATE\n                value: \"false\"\n              - name: GRYPE_DB_UPDATE_URL\n                value: https://minio.view.h2o-2-4864.h2o.vmware.com/grype/databases/listing.json\n              - name: GRYPE_DB_MAX_ALLOWED_BUILT_AGE #! see note on best practices\n                value: \"1200h\"\n              - name: GRYPE_DB_VALIDATE_AGE\n                value: \"false\"\n              - name: GRYPE_DB_CA_CERT\n                value: /etc/ssl/certs/custom-ca.crt\n            #@overlay/replace\n            args:\n              - -c\n              - |\n                grype db update -vv\n                ./source/scan-source.sh /workspace/source/scan.xml /workspace/source/out.yaml /workspace/source/repo blob\n            volumeMounts:\n              #@overlay/append\n              - name: ca-cert\n                mountPath: /etc/ssl/certs/custom-ca.crt\n                subPath: \"ca.crt\" #! key pointing to ca certificate\n\n          #@overlay/match by=overlay.subset({\"name\": \"metadata-store-plugin-config\"}), expects=\"1+\"\n          - name: metadata-store-plugin-config\n            #@overlay/replace\n            args:\n              - -c\n              - |\n                set -euo pipefail\n                /insight config set-target $METADATA_STORE_URL --ca-cert /metadata-store/ca.crt --access-token $METADATA_STORE_ACCESS_TOKEN\n                /insight health\n\n          #@overlay/match by=overlay.subset({\"name\": \"metadata-store-plugin\"}), expects=\"1+\"\n          - name: metadata-store-plugin\n            #@overlay/match missing_ok=True\n            env:\n              - name: METADATA_STORE_URL\n                value: https://metadata-store.view.h2o-2-4864.h2o.vmware.com/\n              - name: METADATA_STORE_ACCESS_TOKEN\n                valueFrom:\n                  secretKeyRef:\n                    key: auth_token\n                    name: store-auth-token\n            volumeMounts:\n              #@overlay/append\n              - mountPath: /metadata-store\n                name: metadata-store-ca-cert\n                readOnly: true\n        volumes:\n        #@overlay/append\n        - name: ca-cert\n          configMap:\n            name: ca-cert #! name of the configmap created\n</code></pre> <pre><code>kubectl apply -f grype-airgap-overlay.yaml\n</code></pre>","tags":["TKG","TAP","TANZU","DevSecOps","Grype"]},{"location":"tanzu/grype-airgapped/#debug-scanning-steps","title":"Debug Scanning Steps","text":"<p>If you want to debug the Grype commands, you have three ways of doing so.</p> <ol> <li>you can create a custom supply chain with custom ScanTemplate CRs</li> <li>you can add debug statements to the Overlay from the previous chapter</li> <li>you can create a temporary Pod, with the same container and a <code>sleep</code> command</li> </ol> <p>Below is an example of the debug Pod.</p> Tanzu Insight CLI Debug Container <p>Dont't forget to replace the <code>METADATA_STORE_URL</code> and <code>image</code> values to your situation.</p> tanzu-insight-cli-debug.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: tanzu-insight-cli\n  name: tanzu-insight-cli\nspec:\n  containers:\n  - image: harbor.h2o-2-4864.h2o.vmware.com/tap/tap-packages@sha256:be7283548e81621899bd69b43b5b2cdf367eb82b111876690cea7cdca51bb9a2\n    name: tanzu-insight-cli\n    command: ['bash', '-c', 'echo \"Hello, Kubernetes!\" &amp;&amp; sleep 3600']\n    env:\n      - name: METADATA_STORE_URL\n        value: http://metadata-store.view.h2o-2-4864.h2o.vmware.com/\n      - name: METADATA_STORE_ACCESS_TOKEN\n        valueFrom:\n          secretKeyRef:\n            key: auth_token\n            name: store-auth-token\n    volumeMounts:\n    - mountPath: /workspace\n      name: workspace\n      readOnly: false\n    - mountPath: /.config\n      name: insight-config\n      readOnly: false\n    - mountPath: /metadata-store\n      name: metadata-store-ca-cert\n      readOnly: true\n  volumes:\n  - emptyDir: {}\n    name: workspace\n  - emptyDir: {}\n    name: insight-config\n  - emptyDir: {}\n    name: cache\n  - name: metadata-store-ca-cert\n    secret:\n      secretName: store-ca-cert\n  - configMap:\n      name: ca-cert\n    name: ca-cert\n</code></pre> <p>Create the Pod as usual:</p> <pre><code>kubectl apply -f tanzu-insight-cli-debug.yaml\n</code></pre> <p>And enter the Pod via <code>kubectl exec</code>.</p> <pre><code>kubectl exec tanzu-insight-cli -ti -- /bin/bash\n</code></pre> <p>Once inside the Pod, you can use the same commands we overide in the Overlay. So you can debug the commands from there, and tweak your configuration before going through supply chain runs.</p>","tags":["TKG","TAP","TANZU","DevSecOps","Grype"]},{"location":"tanzu/harbor-ca/","title":"Harbor custom CA","text":"<p>As a pre-requisite, make sure you have setup a Certificate Authirity with CFSSL.</p> <p>If you not already done so, follow Set up custom Certificate Authority.</p>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/harbor-ca/#install-shared-services-cluster-pre-requisites","title":"Install Shared Services Cluster Pre-requisites","text":"<p>We need to setup permissions, install an Ingress Controller, and install our self-hosted Container Registry.</p>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/harbor-ca/#podsecuritypolicies-tgks","title":"PodSecurityPolicies - TGKs","text":"<p>By default, TKGs has restrictions in place that will prevent us from installing all the pre-requisites. So we have to give our (admin) account enough permissions by applying the following PodSecurityPolicy and bind it to our user group.</p> <pre><code>kubectl create role psp:privileged \\\n    --verb=use \\\n    --resource=podsecuritypolicy \\\n    --resource-name=vmware-system-privileged\n\nkubectl create clusterrolebinding default-tkg-admin-privileged-binding \\\n    --clusterrole=psp:vmware-system-privileged \\\n    --group=system:authenticated\n</code></pre>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/harbor-ca/#kapp-controller-package-repository","title":"Kapp Controller &amp; Package Repository","text":"<p>We need to install several services into our cluster, such as an Ingress Controller. We will do so via the Tanzu Packages, and for that we need to install the Kapp Controller</p> <p>The Kapp Controller comes with its on requirements on permissions, so we begin with a PodSecurityPolicy.</p> <pre><code>kubectl apply -f tanzu-system-kapp-ctrl-restricted.yaml\n</code></pre>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/harbor-ca/#kapp-controller-pod-security-policy","title":"Kapp Controller Pod Security Policy","text":"tanzu-system-kapp-ctrl-restricted.yaml<pre><code>apiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: tanzu-system-kapp-ctrl-restricted\nspec:\n  privileged: false\n  allowPrivilegeEscalation: false\n  requiredDropCapabilities:\n    - ALL\n  volumes:\n    - configMap\n    - emptyDir\n    - projected\n    - secret\n    - downwardAPI\n    - persistentVolumeClaim\n  hostNetwork: false\n  hostIPC: false\n  hostPID: false\n  runAsUser:\n    rule: MustRunAsNonRoot\n  seLinux:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: MustRunAs\n    ranges:\n      - min: 1\n        max: 65535\n  fsGroup:\n    rule: MustRunAs\n    ranges:\n      - min: 1\n        max: 65535\n  readOnlyRootFilesystem: false\n</code></pre>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/harbor-ca/#install-kapp-controller","title":"Install Kapp Controller","text":"<p>We have to add a Kapp Controller Configuration, so the Kapp Controller accepts our Custom CA.</p> <pre><code>export CA_CERT=$(cat ssl/ca.pem)\nexport KAPP_CONTROLLER_NAMESPACE=\"tkg-system\"\n</code></pre> <pre><code>ytt -f ytt/kapp-controller.ytt.yml \\\n  -v namespace=\"$KAPP_CONTROLLER_NAMESPACE\" \\\n  -v caCert=\"${CA_CERT}\" \\\n  &gt; \"kapp-controller.yml\"\n</code></pre> <pre><code>kubectl apply -f kapp-controller.yml\n</code></pre> <pre><code>kubectl get pods -n ${KAPP_CONTROLLER_NAMESPACE} | grep kapp-controller\n</code></pre>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/harbor-ca/#package-repository","title":"Package Repository","text":"<p>At the time of writing, November 2022, the latest supported TKG version is 1.6. So we use the 1.6 package repository.</p> <pre><code>export PKG_REPO_NAME=tanzu-standard\nexport PKG_REPO_URL=projects.registry.vmware.com/tkg/packages/standard/repo:v1.6.0\nexport PKG_REPO_NAMESPACE=tanzu-package-repo-global\n</code></pre> <p>Info</p> <p>The namespace <code>tanzu-package-repo-global</code> is a special namespace.</p> <p>If you install the Kapp Controller as we did, that namespace is considered the packaging global namespace. This mean that any package made available there, via a Package Repository, can have an instance installed in any namespace.</p> <p>Otherwise, you can only create a package instance in the namespace the package is installed in.</p> <pre><code>tanzu package repository add ${PKG_REPO_NAME} \\\n    --url ${PKG_REPO_URL} \\\n    --namespace ${PKG_REPO_NAMESPACE}\n</code></pre> <p>This should yield the following:</p> <pre><code> Adding package repository 'tanzu-standard'\n Validating provided settings for the package repository\n Creating package repository resource\n Waiting for 'PackageRepository' reconciliation for 'tanzu-standard'\n 'PackageRepository' resource install status: Reconciling\n 'PackageRepository' resource install status: ReconcileSucceeded\n 'PackageRepository' resource successfully reconciled\nAdded package repository 'tanzu-standard' in namespace 'tanzu-package-repo-global'\n</code></pre> <p>Verify the package repository is healthy.</p> <pre><code>tanzu package repository get ${PKG_REPO_NAME} --namespace ${PKG_REPO_NAMESPACE}\n</code></pre> <p>We can now view all the available packages.</p> <pre><code>tanzu package available list\n</code></pre> <pre><code>  NAME                                          DISPLAY-NAME               SHORT-DESCRIPTION                                                                 LATEST-VERSION         \n  cert-manager.tanzu.vmware.com                 cert-manager               Certificate management                                                            1.7.2+vmware.1-tkg.1   \n  contour.tanzu.vmware.com                      contour                    An ingress controller                                                             1.20.2+vmware.1-tkg.1  \n  external-dns.tanzu.vmware.com                 external-dns               This package provides DNS synchronization functionality.                          0.11.0+vmware.1-tkg.2  \n  fluent-bit.tanzu.vmware.com                   fluent-bit                 Fluent Bit is a fast Log Processor and Forwarder                                  1.8.15+vmware.1-tkg.1  \n  fluxcd-helm-controller.tanzu.vmware.com       Flux Helm Controller       Helm controller is one of the components in FluxCD GitOps toolkit.                0.21.0+vmware.1-tkg.1  \n  fluxcd-kustomize-controller.tanzu.vmware.com  Flux Kustomize Controller  Kustomize controller is one of the components in Fluxcd GitOps toolkit.           0.24.4+vmware.1-tkg.1  \n  fluxcd-source-controller.tanzu.vmware.com     Flux Source Controller     The source-controller is a Kubernetes operator, specialised in artifacts          0.24.4+vmware.1-tkg.4  \n                                                                           acquisition from external sources such as Git, Helm repositories and S3 buckets.                         \n  grafana.tanzu.vmware.com                      grafana                    Visualization and analytics software                                              7.5.16+vmware.1-tkg.1  \n  harbor.tanzu.vmware.com                       harbor                     OCI Registry                                                                      2.5.3+vmware.1-tkg.1   \n  multus-cni.tanzu.vmware.com                   multus-cni                 This package provides the ability for enabling attaching multiple network         3.8.0+vmware.1-tkg.1   \n                                                                           interfaces to pods in Kubernetes                                                                         \n  prometheus.tanzu.vmware.com                   prometheus                 A time series database for your metrics                                           2.36.2+vmware.1-tkg.1  \n  whereabouts.tanzu.vmware.com                  whereabouts                A CNI IPAM plugin that assigns IP addresses cluster-wide                          0.5.1+vmware.2-tkg.1   \n</code></pre>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/harbor-ca/#certmanager-contour-packages","title":"Certmanager &amp; Contour Packages","text":"<p>We need an Ingress Controller. The Ingress Controller of choice for VMware is Contour.</p> <p>While not strictly necessary, VMware always recommends installing Certmanager before Contour.</p> <p>We don't specify anything specific for these two packakages, sticking to the VMware suggested values.</p> <pre><code>cd scripts &amp;&amp; sh 10-cluster-add-contour-package.sh tap-s1\n</code></pre> 10-cluster-add-contour-package.sh 10-cluster-add-contour-package.sh<pre><code>#!/bin/bash\nCLUSTER_NAME=$1\nINFRA=vsphere\nPACKAGES_NAMESPACE=\"tanzu-packages\"\n\necho \"Creating namespace ${PACKAGES_NAMESPACE} for holding package installations\"\nkubectl create namespace ${PACKAGES_NAMESPACE}\n\n./install-package-with-latest-version.sh $CLUSTER_NAME cert-manager\nkubectl get po,svc -n cert-manager\n\n./install-package-with-latest-version.sh $CLUSTER_NAME contour \"${INFRA}-values/contour.yaml\"\nkubectl get po,svc,ing --namespace tanzu-system-ingress\n</code></pre> install-package-with-latest-version.sh install-package-with-latest-version.sh<pre><code>#!/bin/bash\nCLUSTER_NAME=$1\nPACKAGE_NAME=$2\nVALUES_FILE=$3\nPACKAGES_NAMESPACE=\"tanzu-packages\"\n\necho \"Retrieving latest version of ${PACKAGE_NAME}.tanzu.vmware.com package\"\nPACKAGE_VERSION=$(tanzu package \\\n    available list ${PACKAGE_NAME}.tanzu.vmware.com -A \\\n    --output json | jq --raw-output 'sort_by(.version)|reverse|.[0].version')\n\necho \"Installing package ${PACKAGE_NAME} version ${PACKAGE_VERSION} into namespace ${PACKAGES_NAMESPACE}\"\nINSTALL_COMMAND=\"tanzu package install ${PACKAGE_NAME}  \\\n    --package-name ${PACKAGE_NAME}.tanzu.vmware.com \\\n    --namespace ${PACKAGES_NAMESPACE} \\\n    --version ${PACKAGE_VERSION} \"\n\nif [ -n \"$3\" ]; then\n    echo \"Found a values file, appending to command\"\n    INSTALL_COMMAND=\"${INSTALL_COMMAND} --values-file ${VALUES_FILE}\"\nfi\neval $INSTALL_COMMAND\n</code></pre>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/harbor-ca/#install-harbor-and-prepare-tap-images","title":"Install Harbor and prepare TAP images","text":"<p>We use Harbor as our Container Registry of choice.</p> <p>Once installed, we relocate the OCI images and bundles related to TAP and Tanzu Build Service (TBS) to Harbor.</p>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/harbor-ca/#prepare-harbor-certificate","title":"Prepare Harbor Certificate","text":"<p>The first thing we will do, is generate a certificate for Harbor. For this, we need to know all the names it should be known as, for both internal and external traffic.</p> <p>In this scenario, I'm using sslip.io to fake a full FQN domain name. It will resolve <code>&lt;anything&gt;.&lt;valid IP address&gt;.sslip.io</code> to the valid IP addres.</p> <pre><code>export LB_IP=$(kubectl get svc -n tanzu-system-ingress envoy -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\nexport DOMAIN=\"${LB_IP}.sslip.io\"\nexport HARBOR_HOSTNAME=\"harbor.${DOMAIN}\"\nexport NOTARY_HOSTNAME=\"notary.${HARBOR_HOSTNAME}\"\necho \"HARBOR_HOSTNAME=${HARBOR_HOSTNAME}\"\necho \"NOTARY_HOSTNAME=${NOTARY_HOSTNAME}\"\n</code></pre> <p>We use the <code>cfssl</code> utility to generate the Harbor certificate, signing it with the CA we created earlier.</p> <pre><code>cfssl gencert -ca ssl/ca.pem -ca-key ssl/ca-key.pem \\\n  -config ssl/cfssl.json \\\n  -profile=server \\\n  -cn=\"${HARBOR_HOSTNAME}\" \\\n  -hostname=\"${HARBOR_HOSTNAME},${NOTARY_HOSTNAME},harbor.harbor.svc.cluster.local,localhost\" \\\n   ssl/base-service-cert.json   | cfssljson -bare harbor\n</code></pre> <p>I like to separate my files, so I move them to the <code>ssl</code> folder, but feel free to skip this. Just remember that if you do, change the location of the files in the environment variables below.</p> <pre><code>mv harbor.csr  ssl/harbor.csr\nmv harbor-key.pem ssl/harbor-key.pem\nmv harbor.pem ssl/harbor.pem\n</code></pre>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/harbor-ca/#configure-harbor-values","title":"Configure Harbor Values","text":"<p>We need to set the storage class for several volumes.</p> <pre><code>kubectl get storageclass\n</code></pre> <p>In my case, the storage class is what is defined below. I also load up the certificate values into environment variables.</p> <pre><code>export STORAGE_CLASS=\"vc01cl01-t0compute\"\nexport CLUSTER_NAME=tap-s1\nexport HARBOR_NAMESPACE=tanzu-system-registry\nexport HARBOR_ADMIN_PASS=''\nexport TLS_CERT=$(cat ssl/harbor.pem)\nexport TLS_KEY=$(cat ssl/harbor-key.pem)\nexport CA_CERT=$(cat ssl/ca.pem)\n</code></pre> <p>This way we can leverage ytt to use our values template to generate the values file we will use.</p> <pre><code>ytt -f ytt/harbor.ytt.yml \\\n  -v namespace=\"$HARBOR_NAMESPACE\" \\\n  -v adminPassword=\"$HARBOR_ADMIN_PASS\" \\\n  -v hostname=\"$HARBOR_HOSTNAME\" \\\n  -v storaceClass=\"$STORAGE_CLASS\" \\\n  -v tlsCert=\"${TLS_CERT}\" \\\n  -v tlsKey=\"${TLS_KEY}\" \\\n  -v caCert=\"${CA_CERT}\" \\\n  &gt; \"vsphere-values/${CLUSTER_NAME}-harbor.yml\"\n</code></pre> Harbor Values Template <p>There are a couple of more secret values in here. They need to be defined, but won't be used in this guide.</p> <p>Feel free to change them.</p> ytt/harbor.ytt.yml<pre><code>#@ load(\"@ytt:data\", \"data\")\n---\nnamespace: #@ data.values.namespace\nhostname: #@ data.values.hostname\nport:\n  https: 443\nlogLevel: info\ntlsCertificate:\n  tls.crt: #@ data.values.tlsCert\n  tls.key: #@ data.values.tlsKey\n  ca.crt: #@ data.values.caCert\nenableContourHttpProxy: true\nharborAdminPassword: #@ data.values.adminPassword\nsecretKey: j0Kn0UlfSGzMTBx6\ndatabase:\n  password: 4Oj0848rTIvzJiMc\ncore:\n  replicas: 1\n  secret: vFib2c87qg1FFZqI\n  xsrfKey: sGn5nIgBQKdwx89tZLO5pTJAqbCwVRU8\njobservice:\n  replicas: 1\n  secret: vFib2c87qg1FFZqI\nregistry:\n  replicas: 1\n  secret: vFib2c87qg1FFZqI\nnotary:\n  enabled: true\ntrivy:\n  enabled: true\n  replicas: 1\n  gitHubToken: \"\"\n  skipUpdate: true\npersistence:\n  persistentVolumeClaim:\n    registry:\n      storageClass: #@ data.values.storaceClass\n      accessMode: ReadWriteOnce\n      size: 100Gi\n    jobservice:\n      storageClass: #@ data.values.storaceClass\n      accessMode: ReadWriteOnce\n      size: 1Gi\n    database:\n      storageClass: #@ data.values.storaceClass\n      accessMode: ReadWriteOnce\n      size: 1Gi\n    redis:\n      storageClass: #@ data.values.storaceClass\n      accessMode: ReadWriteOnce\n      size: 1Gi\n    trivy:\n      storageClass: #@ data.values.storaceClass\n      accessMode: ReadWriteOnce\n      size: 5Gi\n  imageChartStorage:\n    disableredirect: false\n    type: filesystem\n    filesystem:\n      rootdirectory: /storage\npspNames: vmware-system-privileged\nmetrics:\n  enabled: false\n  core:\n    path: /metrics\n    port: 8001\n  registry:\n    path: /metrics\n    port: 8001\n  jobservice:\n    path: /metrics\n    port: 8001\n  exporter:\n    path: /metrics\n    port: 8001\nnetwork:\n  ipFamilies: [\"IPv4\", \"IPv6\"]\n</code></pre>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/harbor-ca/#install-harbor","title":"Install Harbor","text":"<p>Now that we have the values file, we can install Harbor.</p> <pre><code>sh 20-cluster-add-harbor-package.sh tap-s1\n</code></pre> 20-cluster-add-harbor-package.sh <p>The scripts can be found here.</p> 20-cluster-add-harbor-package.sh<pre><code>#!/bin/bash\nCLUSTER_NAME=$1\nINFRA=vsphere\nHARBOR_VALUES_CLUSTER=\"${INFRA}-values/${CLUSTER_NAME}-harbor.yml\"\nPACKAGES_NAMESPACE=\"tanzu-packages\"\n\n./install-package-with-latest-version.sh $CLUSTER_NAME harbor \"${HARBOR_VALUES_CLUSTER}\"\nkubectl --namespace tanzu-system-registry get po,svc\n</code></pre>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/harbor-ca/#test-image-from-harbor","title":"Test Image From Harbor","text":"<pre><code>docker tag joostvdgtanzu/go-demo-kbld:kbld-rand-1674551140548590000-1629935139128 ${HARBOR_HOSTNAME}/tap-apps/go-demo:0.1.0\ndocker push ${HARBOR_HOSTNAME}/tap-apps/go-demo:0.1.0\n</code></pre> <pre><code>kubectl run go-demo --image ${HARBOR_HOSTNAME}/tap-apps/go-demo:0.1.0\n</code></pre> <pre><code>kubectl port-forward go-demo 8080:8080\n</code></pre> <pre><code>http :8080/\n</code></pre> <pre><code>curl http://localhost:8080\n</code></pre>","tags":["TKG","Vsphere","Harbor","TANZU"]},{"location":"tanzu/jenkins-tap/","title":"Jenkins and Tanzu Application Platform","text":""},{"location":"tanzu/jenkins-tap/#ways-to-combine-tap-with-jenkins","title":"Ways to combine TAP with Jenkins","text":"<ul> <li>Jenkins + Image To URL:  Use Jenkins as a CI solution, ending with a Container Image. Jenkins then initiates the handover by triggering an Image To Source supply chain.</li> <li>Jenkins + Maven Artifact To URL: Use Jenkins as a CI solution; instead of the handover point being a Container Image, it is a Maven Artifact. TAP can watch (or is there an Event system?) Maven Artifacts and trigger a Cartographer supply chain (which includes generating the Container Image)</li> </ul>"},{"location":"tanzu/jenkins-tap/#jenkins-and-image-to-url","title":"Jenkins and Image To URL","text":"<ul> <li>TAP setup</li> <li>Jenkins setup</li> <li>Cartographer SupplyChain</li> <li>Application Pipeline</li> </ul>"},{"location":"tanzu/jenkins-tap/#tap-setup","title":"TAP setup","text":"<ul> <li>Harbor with custom Certificate Authority (CA)</li> <li>TAP with iterate or run profile</li> <li>Register Application (Workload)</li> </ul> <p>...</p>"},{"location":"tanzu/jenkins-tap/#jenkins-setup","title":"Jenkins setup","text":"<ul> <li>Jenkins in the same cluster</li> <li>Use JCasC</li> <li>Use Shared Libraries</li> <li>Use Declarative Pipeline</li> </ul>"},{"location":"tanzu/jenkins-tap/#cartographer-supplychain","title":"Cartographer SupplyChain","text":"<ul> <li>Use Out Of The Box (OOTB) supply chains<ul> <li>especially \"Image To URL\"</li> </ul> </li> <li>Verify the trigger keyword (annotation for workload)</li> </ul>"},{"location":"tanzu/jenkins-tap/#application-pipeline","title":"Application Pipeline","text":"<ul> <li>Trigger update by updating the workload CR</li> <li>Git event triggers start </li> <li>Build and test application</li> <li>Build and test Container Image with the application</li> <li>Push and Label Container Image</li> <li>Show Diagram</li> </ul>"},{"location":"tanzu/jenkins-tap/#jenkins-and-maven-artifact-to-url","title":"Jenkins and Maven Artifact To URL","text":"<p>TODO</p>"},{"location":"tanzu/jenkins-tap/#references","title":"References","text":""},{"location":"tanzu/minio-ca/","title":"Minio custom CA","text":"<p>As a pre-requisite, make sure you have setup a Certificate Authirity with CFSSL.</p> <p>If you not already done so, follow Set up custom Certificate Authority.</p>","tags":["TKG","Vsphere","minio","TANZU"]},{"location":"tanzu/minio-ca/#relocate-images","title":"Relocate Images","text":"<p>In case you are in an restricted environment, or you want to avoid DockerHub rate limits, relocate the images used in the Helm chart.</p> <p>The tags of the images depend on the version of the Helm chart. We will use version <code>12.1.8</code>, which comes with the version of the <code>bitnami/minio</code> and <code>bitnami/minio-client</code> images.</p> <p>First, set the hHostname of your Registry:</p> <pre><code>export REGISTRY_HOSTNAME=\n</code></pre> <p>Warning</p> <p>Make sure you are authenticated with your registry.</p> <p>For example, you can use <code>docker login</code> or Docker replacement equivalent commands:</p> <pre><code>docker login $REGISTRY_HOSTNAME\n</code></pre> <p>Then you can relocate the <code>bitnami/minio</code> image:</p> <pre><code>docker pull bitnami/minio:2023.2.22-debian-11-r0\ndocker tag bitnami/minio:2023.2.22-debian-11-r0 ${REGISTRY_HOSTNAME}/bitnami/minio:2023.2.22-debian-11-r0\ndocker push ${REGISTRY_HOSTNAME}/bitnami/minio:2023.2.22-debian-11-r0\n</code></pre> <p>And the <code>bitnami/minio-client</code> image:</p> <pre><code>docker pull bitnami/minio-client:2023.2.16-debian-11-r1\ndocker tag bitnami/minio-client:2023.2.16-debian-11-r1 ${REGISTRY_HOSTNAME}/bitnami/minio-client:2023.2.16-debian-11-r1\ndocker push ${REGISTRY_HOSTNAME}/bitnami/minio-client:2023.2.16-debian-11-r1\n</code></pre>","tags":["TKG","Vsphere","minio","TANZU"]},{"location":"tanzu/minio-ca/#create-certificate","title":"Create Certificate","text":"<p>Before we create the certificate, we need to be sure of the domain names to use for the MinIO API and the GUI.</p> <pre><code>export MINIO_HOSTNAME=\nexport MINIO_CONSOLE_HOSTNAME=\n</code></pre> <p>To ensure you can use MinIO within the cluster without going through the Ingress Controller, we also add internal hostnames.</p> <pre><code>cfssl gencert -ca ca.pem -ca-key ca-key.pem \\\n  -config cfssl.json \\\n  -profile=server \\\n  -cn=\"${MINIO_HOSTNAME}\" \\\n  -hostname=\"${MINIO_HOSTNAME},${MINIO_CONSOLE_HOSTNAME},*.minio-headless.minio.svc.cluster.local,minio.minio.svc.cluster.local,localhost\" \\\n   base-service-cert.json   | cfssljson -bare minio-server\n</code></pre> <p>We need to add the certificate, its key, and the CA certificate to Kubernetes secret. We copy the files to reduce the complexity of the commands that follow.</p> <pre><code>mkdir minio-certs\ncp minio-server-key.pem minio-certs/tls.key\ncp minio-server.pem minio-certs/tls.crt\ncp ca.crt minio-certs/ca.crt\ncd minio-certs/\n</code></pre>","tags":["TKG","Vsphere","minio","TANZU"]},{"location":"tanzu/minio-ca/#create-secrets","title":"Create Secrets","text":"<p>First, ensure the <code>minio</code> Namespace exists.</p> <pre><code>kubectl create namespace minio\n</code></pre> <p>Then you can create the secret for letting MinIO terminate TLS.</p> <p>This is recommended, so you secure connections in your cluster as well. That is also why we added the internal hostnames to the Certificate.</p> <pre><code>kubectl create secret generic tls-ssl-minio-unmanaged \\\n  --from-file=tls.crt \\\n  --from-file=tls.key \\\n  --from-file=ca.crt \\\n  --namespace minio\n</code></pre> <p>We then create a secret we use for the HTTPProxy resources, or Ingress CR depending on your Ingress Controller.</p> <pre><code>kubectl create secret tls tls-ssl-minio-for-proxy \\\n  --cert=tls.crt \\\n  --key=tls.key \\\n  --namespace minio\n</code></pre> <p>Then we create a secret with only the <code>ca.crt</code> for Contour, so it can verify the connection with TLS.</p> <pre><code>kubectl create secret generic client-root-ca \\\n  --from-file=ca.crt \\\n  --namespace minio\n</code></pre> <p>Next up is setting up the credentials for MinIO:</p> <pre><code>MINIO_USER=\nMINIO_PASS=\n</code></pre> <pre><code>kubectl create secret generic minio-credentials \\\n  --from-literal=root-user=\"${MINIO_USER}\" \\\n  --from-literal=root-password=\"${MINIO_PASS}\" \\\n  --namespace minio\n</code></pre>","tags":["TKG","Vsphere","minio","TANZU"]},{"location":"tanzu/minio-ca/#helm-chart-install","title":"Helm Chart Install","text":"<p>We use the Bitnami MinIO Helm chart.</p>","tags":["TKG","Vsphere","minio","TANZU"]},{"location":"tanzu/minio-ca/#create-values-file","title":"Create Values File","text":"<p>Create <code>minio-values.yaml</code>.</p> <p>The goal of using something like MinIO is to provide reliable storage. So the assumption is that you need replica's and replication.</p> <p>For this, we set <code>mode: distributed</code> and <code>statefulset.replicaCount: 4</code>. If you are using it as a test in a non-production environment, you can set this lower values.</p> <p>Consult the helm chart docs for more details</p> <p>Helm Values</p> minio-values.yaml<pre><code>global:\n  imageRegistry: REPLACE_WITH_IMAGE_REGISTRY\n  storageClass: REPLACE_WITH_STORAGE_CLASS\nauth:\n  existingSecret: minio-credentials\nmode: distributed\nstatefulset:\n  replicaCount: 4\nservice:\n  annotations:\n    projectcontour.io/upstream-protocol.tls: \"9000,9001\"\ntls:\n  enabled: true\n  existingSecret: tls-ssl-minio-unmanaged\n</code></pre>","tags":["TKG","Vsphere","minio","TANZU"]},{"location":"tanzu/minio-ca/#helm-chart-install_1","title":"Helm Chart Install","text":"<p>And then you can install the Helm chart.</p> <p>Helm Install</p> <pre><code>helm upgrade --install \\\n  --namespace minio \\\n  --values minio-values.yaml \\\n  --version 12.1.8 \\\n  minio \\\n  bitnami/minio\n</code></pre>","tags":["TKG","Vsphere","minio","TANZU"]},{"location":"tanzu/minio-ca/#minio-httpproxies","title":"MinIO HTTPProxies","text":"<p>TODO: create YTT template and generate both files</p> <p>We create two HTTPProxy resources, one for the GUI (console) and one for the backend (API).</p> <p>Warning</p> <p>If you do not use Contour as Ingress Controller, create the equiavalent Ingress CRs.</p> <p>YTT Template for HTTPPRoxy</p> minio-httpproxy.ytt.yml<pre><code>#@ load(\"@ytt:data\", \"data\")\n---\napiVersion: projectcontour.io/v1\nkind: HTTPProxy\nmetadata:\n  name: #@ data.values.name\n  namespace: minio\nspec:\n  virtualhost:\n    fqdn: #@ data.values.fqdn\n    tls:\n      secretName: tls-ssl-minio-for-proxy\n  routes:\n    - services:\n        - name: minio\n          port: #@ data.values.port\n          validation:\n            caSecret: client-root-ca\n            subjectName: #@ data.values.fqdn\n</code></pre> <ul> <li>create <code>minio-console-httpproxy.yaml</code></li> </ul> <p>Web Console HTTPProxy</p> <pre><code>ytt -f minio-httpproxy.ytt.yml \\\n  -v fqdn=\"${MINIO_CONSOLE_HOSTNAME}\" \\\n  -v name=\"minio-console\" \\\n  -v port=\"9001\" \\\n  &gt; minio-console-httpproxy.yaml\n</code></pre> <pre><code>kubectl apply -f minio-console-httpproxy.yaml\n</code></pre> <ul> <li>create <code>minio-api-httpproxy.yaml</code>:</li> </ul> <p>API HTTPProxy</p> <pre><code>ytt -f minio-httpproxy.ytt.yml \\\n  -v fqdn=\"${MINIO_HOSTNAME}\" \\\n  -v name=\"minio-api\" \\\n  -v port=\"9000\" \\\n  &gt; minio-api-httpproxy.yaml\n</code></pre> <pre><code>kubectl apply -f minio-api-httpproxy.yaml\n</code></pre>","tags":["TKG","Vsphere","minio","TANZU"]},{"location":"tanzu/tap-on-tkgm-on-aws/","title":"TAP on TKGm on AWS","text":"<p>Coming soon.</p>"},{"location":"tanzu/tap-tekton/","title":"Tekton for TAP","text":""},{"location":"tanzu/tap-tekton/#tekton-dashboard","title":"Tekton Dashboard","text":""},{"location":"tanzu/tap-tekton/#tasks-catalog","title":"Tasks &amp; Catalog","text":""},{"location":"tanzu/tap-tekton/#pipeline-for-testing","title":"Pipeline for Testing","text":""},{"location":"tanzu/tkg-grafana/","title":"TKG Grafana with LDAP","text":"<p>In this guide we're going to install the Tanzu Kubernetes Grid (TGK) Grafana package, using LDAP for user authN and authZ.</p> <p>We will do the following steps:</p> <ul> <li>Install the TKG package repository</li> <li>Install the Prometheus package (dependency)</li> <li>Install the Grafana package</li> <li>Create a ConfigMap for the LDAP configuration</li> <li>Apply an overlay to the Grafana package install to use the configmap</li> </ul> <p>We assume you've installed LDAP as described in Tanzu Dependencies - LDAP</p>","tags":["grafana","LDAP","packages","tkg","TANZU"]},{"location":"tanzu/tkg-grafana/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>TKG cluster with cluster essentials</li> <li>Tanzu CLI</li> </ul>","tags":["grafana","LDAP","packages","tkg","TANZU"]},{"location":"tanzu/tkg-grafana/#install-tkg-package-repository","title":"Install TKG Package Repository","text":"<p>This is according to the Tanzu documentation.</p> <p>First, ensure the repository is not yet installed.</p> <pre><code>tanzu package repository list -A\n</code></pre> <p>Then, set the name and source.</p> <pre><code>export PKG_REPO_NAME=tanzu-standard\nexport PKG_REPO_URL=projects.registry.vmware.com/tkg/packages/standard/repo:v1.6.0\nexport PKG_REPO_NAMESPACE=tanzu-package-repo-global\n</code></pre> <p>We then install the package repository.</p> <pre><code>tanzu package repository add ${PKG_REPO_NAME} \\\n    --url ${PKG_REPO_URL} \\\n    --namespace ${PKG_REPO_NAMESPACE}\n</code></pre> <p>Which should yield something like this:</p> <pre><code> Adding package repository 'tanzu-standard'\n Validating provided settings for the package repository\n Creating package repository resource\n Waiting for 'PackageRepository' reconciliation for 'tanzu-standard'\n 'PackageRepository' resource install status: Reconciling\n 'PackageRepository' resource install status: ReconcileSucceeded\n 'PackageRepository' resource successfully reconciled\nAdded package repository 'tanzu-standard' in namespace 'tanzu-package-repo-global'\n</code></pre> <p>Verify it is installed correcly:</p> <pre><code>tanzu package repository get ${PKG_REPO_NAME} --namespace ${PKG_REPO_NAMESPACE}\n</code></pre> <p>Which should yield something like this:</p> <pre><code>NAME            REPOSITORY                                               TAG     STATUS               DETAILS  \ntanzu-standard  projects.registry.vmware.com/tkg/packages/standard/repo  v1.6.0  Reconcile succeeded   \n</code></pre> <p>Verify the packages are now available:</p> <pre><code>tanzu package available list\n</code></pre> <p>Which should list the following set of packages.</p> <pre><code>NAME                                          DISPLAY-NAME               SHORT-DESCRIPTION                                                                 LATEST-VERSION         \ncert-manager.tanzu.vmware.com                 cert-manager               Certificate management                                                            1.7.2+vmware.1-tkg.1   \ncontour.tanzu.vmware.com                      contour                    An ingress controller                                                             1.20.2+vmware.1-tkg.1  \nexternal-dns.tanzu.vmware.com                 external-dns               This package provides DNS synchronization functionality.                          0.11.0+vmware.1-tkg.2  \nfluent-bit.tanzu.vmware.com                   fluent-bit                 Fluent Bit is a fast Log Processor and Forwarder                                  1.8.15+vmware.1-tkg.1  \nfluxcd-helm-controller.tanzu.vmware.com       Flux Helm Controller       Helm controller is one of the components in FluxCD GitOps toolkit.                0.21.0+vmware.1-tkg.1  \nfluxcd-kustomize-controller.tanzu.vmware.com  Flux Kustomize Controller  Kustomize controller is one of the components in Fluxcd GitOps toolkit.           0.24.4+vmware.1-tkg.1  \nfluxcd-source-controller.tanzu.vmware.com     Flux Source Controller     The source-controller is a Kubernetes operator, specialised in artifacts          0.24.4+vmware.1-tkg.4  \n                                                                        acquisition from external sources such as Git, Helm repositories and S3 buckets.                         \ngrafana.tanzu.vmware.com                      grafana                    Visualization and analytics software                                              7.5.16+vmware.1-tkg.1  \nharbor.tanzu.vmware.com                       harbor                     OCI Registry                                                                      2.5.3+vmware.1-tkg.1   \nmultus-cni.tanzu.vmware.com                   multus-cni                 This package provides the ability for enabling attaching multiple network         3.8.0+vmware.1-tkg.1   \n                                                                        interfaces to pods in Kubernetes                                                                         \nprometheus.tanzu.vmware.com                   prometheus                 A time series database for your metrics                                           2.36.2+vmware.1-tkg.1  \nwhereabouts.tanzu.vmware.com                  whereabouts                A CNI IPAM plugin that assigns IP addresses cluster-wide                          0.5.1+vmware.2-tkg.1   \n</code></pre>","tags":["grafana","LDAP","packages","tkg","TANZU"]},{"location":"tanzu/tkg-grafana/#grafana-package-install","title":"Grafana Package Install","text":"<p>Before we can install the Grafana package, we need to install the Prometheus package.</p> <p>This package doesn't need any specific configuration for our needs.</p>","tags":["grafana","LDAP","packages","tkg","TANZU"]},{"location":"tanzu/tkg-grafana/#install-prometheus","title":"Install Prometheus","text":"<pre><code>PACKAGES_NAMESPACE=tanzu-packages\nPACKAGE_NAME=prometheus.tanzu.vmware.com\nPACKAGE_VERSION=2.36.2+vmware.1-tkg.1\n</code></pre> <pre><code>tanzu package install ${PACKAGE_NAME} \\\n    --package-name ${PACKAGE_NAME} \\\n    --namespace ${PACKAGES_NAMESPACE} \\\n    --version ${PACKAGE_VERSION} \\\n    --create-namespace\n</code></pre>","tags":["grafana","LDAP","packages","tkg","TANZU"]},{"location":"tanzu/tkg-grafana/#install-grafana","title":"Install Grafana","text":"<pre><code>PACKAGES_NAMESPACE=tanzu-packages\nPACKAGE_NAME=grafana.tanzu.vmware.com\nPACKAGE_VERSION=7.5.16+vmware.1-tkg.1\n</code></pre> <p>To view all the available values you can configure, you can run the following command:</p> <pre><code>tanzu package available get ${TAP_PACKAGE_NAME}/${TAP_PACKAGE_VERSION} --namespace tap-install --values-schema   \n</code></pre> <p>Once you have constructed your Grafana values file, you can install it:</p> <pre><code>tanzu package installed update --install ${PACKAGE_NAME} \\\n    --package-name ${PACKAGE_NAME} \\\n    --namespace ${PACKAGES_NAMESPACE} \\\n    --version ${PACKAGE_VERSION} \\\n    --values-file \"grafana-data-values.yaml\"\n</code></pre>","tags":["grafana","LDAP","packages","tkg","TANZU"]},{"location":"tanzu/tkg-grafana/#grafana-values","title":"Grafana Values","text":"<p>Below is an example Grafana values file, inspired by VMware docs.</p> grafana-data-values.yaml<pre><code>namespace: tanzu-system-grafana\n\ngrafana:\n  deployment:\n    replicas: 1\n    containers:\n      resources: {}\n    podAnnotations: {}\n    podLabels: {}\n    k8sSidecar:\n      containers:\n        resources: {}\n  service:\n    type: LoadBalancer\n    port: 80\n    targetPort: 3000\n    labels: {}\n    annotations: {}\n  config:\n    grafana_ini: |\n      [analytics]\n      check_for_updates = false\n      [grafana_net]\n      url = https://grafana.com\n      [log]\n      mode = console\n      [paths]\n      data = /var/lib/grafana/data\n      logs = /var/log/grafana\n      plugins = /var/lib/grafana/plugins\n      provisioning = /etc/grafana/provisioning\n      [auth.ldap]\n      # Set to `true` to enable LDAP integration (default: `false`)\n      enabled = true\n\n      # Path to the LDAP specific configuration file (default: `/etc/grafana/ldap.toml`)\n      config_file = /etc/grafana/ldap.toml\n\n      # Allow sign-up should be `true` (default) to allow Grafana to create users on successful LDAP authentication.\n      # If set to `false` only already existing Grafana users will be able to login.\n      allow_sign_up = true\n    datasource_yaml: |-\n      apiVersion: 1\n      datasources:\n        - name: Prometheus\n          type: prometheus\n          url: prometheus-server.tanzu-system-monitoring.svc.cluster.local\n          access: proxy\n          isDefault: true\n    dashboardProvider_yaml: |-\n      apiVersion: 1\n      providers:\n        - name: 'sidecarDashboardProvider'\n          orgId: 1\n          folder: ''\n          folderUid: ''\n          type: file\n          disableDeletion: false\n          updateIntervalSeconds: 10\n          allowUiUpdates: false\n          options:\n            path: /tmp/dashboards\n            foldersFromFilesStructure: true\n  pvc:\n    annotations: {}\n    storageClassName: null\n    accessMode: ReadWriteOnce\n    storage: \"2Gi\"\n  secret:\n    type: \"Opaque\"\n    admin_user: \"YWRtaW4=\"\n    admin_password: \"YWRtaW4=\"\n\ningress:\n  enabled: true\n  virtual_host_fqdn: \"grafana.10.220.2.199.sslip.io\"\n  prefix: \"/\"\n  servicePort: 80\n</code></pre>","tags":["grafana","LDAP","packages","tkg","TANZU"]},{"location":"tanzu/tkg-grafana/#ldap-configmap","title":"LDAP Configmap","text":"<p>We need to give Grafana enough information to use our LDAP server.</p> <p>We do that by creating a ConfigMap, which we will apply to the installation via an YTT Overlay (next paragraph).</p> <p>We configure the following:</p> <ul> <li>Host values, such as <code>host</code>, <code>port</code>, and use of ssl (no)</li> <li>Bind user, via the <code>bind_dn</code> and the admin's password</li> <li>User search, via <code>search_filter</code> and <code>search_base_dns</code><ul> <li>search_base_dns: where in the hierarchy are users located</li> <li>search_filter: how can we recognize users, in our case, we use the objectClass with the name inetOrgPerson</li> </ul> </li> <li>Group search, via <code>group_search_filter</code>, <code>group_search_filter_user_attribute</code>, <code>group_search_base_dns</code><ul> <li>group_search_base_dns: the list of <code>dn</code>'s that where Groups are to be found</li> <li>group_search_filter_user_attribute: which property of the users do we use in the <code>group_search_filter</code></li> <li>group_search_filter: the filter for finding which Groups a User belongs to</li> </ul> </li> <li>Server attribute mapping, translating the server attributes to attributes Grafana uses<ul> <li>withouth these, such as <code>username</code>, Grafana cannot identify the user properly and gives conflicts</li> </ul> </li> <li>Group RBAC mapping, via one or more <code>[[servers.group_mappings]]</code> entries<ul> <li>here you can map a <code>group_dn</code> entry from LDAP to the Roles in Grafana</li> </ul> </li> </ul> <p>Danger</p> <p>The LDAP dn's and filters cannot contains spaces. If they contain spaces, the TOML file in the ConfigMap will introduce line breaks, corrupting the configuration file!</p> grafana-ldap-configmap.yml<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: ldap-config\n  namespace: tanzu-system-grafana\ndata:\n  ldap.toml: |-\n    [[servers]]\n    host = \"ldap-openldap.ldap.svc.cluster.local\"\n    port = 389\n    use_ssl = false\n    start_tls = false\n    ssl_skip_verify = true\n\n    bind_dn = \"cn=admin,dc=example,dc=org\"\n    bind_password = \"C5z6DUTNSMDoiWCHI2GIuSPIzCJt5Zo0\"\n\n    search_filter = \"(&amp;(objectClass=inetOrgPerson)((cn=%s)))\"\n    search_base_dns = [\"ou=People,dc=example,dc=org\"]\n\n    group_search_filter = \"(&amp;(objectClass=groupOfNames)(member=uid=%s,ou=People,dc=example,dc=org))\"\n    group_search_filter_user_attribute = \"uid\"\n    group_search_base_dns = [\"ou=Groups,dc=example,dc=org\"]\n\n    # Specify names of the ldap attributes your ldap uses\n    [servers.attributes]\n    email =  \"mail\"\n    name = \"givenName\"\n    surname = \"sn\"\n    username = \"uid\"\n\n\n    [[servers.group_mappings]]\n    group_dn = \"cn=Administrators,dc=example,dc=org\"\n    org_role = \"Admin\"\n    grafana_admin = true\n\n\n    [[servers.group_mappings]]\n    group_dn = \"cn=BlueAdmins,ou=Groups,dc=example,dc=org\"\n    org_role = \"Admin\" \n    grafana_admin = true\n\n    [[servers.group_mappings]]\n    group_dn = \"cn=GreenAdmins,ou=Groups,dc=example,dc=org\"\n    org_role = \"Editor\"\n\n    [[servers.group_mappings]]\n    group_dn = \"*\"\n    org_role = \"Viewer\"\n</code></pre> <pre><code>kubectl apply -f grafana-ldap-configmap.yml\n</code></pre>","tags":["grafana","LDAP","packages","tkg","TANZU"]},{"location":"tanzu/tkg-grafana/#ldapad-config","title":"LDAP/AD Config","text":"Group configuration when using LDAP/AD <pre><code>[[servers]]\nhost = \u201c34.220.248.176\u201d\nport = 389\nuse_ssl = false\nstart_tls = false\nssl_skip_verify = false\nbind_dn = \u201cgrafana@fullauto.local\u201d\nbind_password = \u2018adasdads@\u2019\nsearch_filter = \u201c(sAMAccountName=%s)\u201d\nsearch_base_dns = [\u201cdc=fullauto,dc=local\u201d]\n[servers.attributes]\nname = \u201cgivenName\u201d\nsurname = \u201csn\u201d\nusername = \u201csAMAccountName\u201d\nmember_of = \u201cmemberOf\u201d\nemail =  \u201cmail\u201d\n[[servers.group_mappings]]\ngroup_dn = \u201cCN=grafana-admin,CN=Users,DC=fullauto,DC=LOCAL\u201d\norg_role = \u201cAdmin\u201d\n[[servers.group_mappings]]\ngroup_dn = \u201cCN=grafana-editor,CN=Users,DC=fullauto,DC=LOCAL\u201d\norg_role = \u201cEditor\u201d\n[[servers.group_mappings]]\ngroup_dn = \u201cCN=grafana-viewer,CN=Users,DC=fullauto,DC=LOCAL\u201d\norg_role = \u201cViewer\u201d\n[[servers.group_mappings]]\ngroup_dn = \u201c*\u201d\norg_role = \u201cViewer\u201d\n</code></pre>","tags":["grafana","LDAP","packages","tkg","TANZU"]},{"location":"tanzu/tkg-grafana/#grafana-ldap-overlay","title":"Grafana LDAP Overlay","text":"<p>Because the Grafana package does not support the LDAP values, we patch the Grafana installation using a YTT Overlay.</p> <p>You can read up more about Overlays on the YTT website, and see another example for Harbor in the VMware docs.</p> grafana-ldap-overlay.yaml<pre><code>#@ load(\"@ytt:overlay\", \"overlay\")\n\n#@overlay/match by=overlay.and_op(overlay.subset({\"kind\": \"Deployment\"}), overlay.subset({\"metadata\": {\"name\": \"grafana\"}}))\n---\nspec:\n  template:\n    spec:\n      containers:\n        #@overlay/match by=\"name\"\n        - name: grafana\n          volumeMounts:\n            #@overlay/match by=overlay.index(0)\n            #@overlay/insert before=True\n            - mountPath: /etc/grafana/ldap.toml\n              name: ldapconfig\n              subPath: ldap.toml\n      volumes:\n        #@overlay/match by=overlay.index(0)\n        #@overlay/insert before=True\n        - configMap:\n            defaultMode: 420\n            name: ldap-config\n          name: ldapconfig\n</code></pre> <p>The Overlay is \"packaged\" as a Kubernetes secret. Make sure this secret is in the namespace the Grafana Package is installed (not Grafana itself).</p> <pre><code>kubectl create secret generic grafana-ldap-overlay \\\n  --from-file=grafana-ldap-overlay.yaml \\\n  -n tanzu-packages\n</code></pre> <p>Then we annotate the Grafana Package Install (the Carvel CR), telling it to apply our Overlay to the installation.</p> <pre><code>kubectl annotate packageinstalls grafana.tanzu.vmware.com \\\n  ext.packaging.carvel.dev/ytt-paths-from-secret-name.1=grafana-ldap-overlay \\\n  -n tanzu-packages\n</code></pre>","tags":["grafana","LDAP","packages","tkg","TANZU"]},{"location":"tanzu/supplychain/gitops-flow/","title":"Customize TAP GitOps Supply Chain","text":"<ul> <li>Change folder structure for GitOps repository</li> </ul>","tags":["TAP","Tekton","Cartographer","CI/CD","Carvel","Tanzu"]},{"location":"tanzu/supplychain/overview/","title":"Customize TAP Supply Chains","text":"<ul> <li>Tekton Pipelines with Tasks</li> <li>Tekton Tasks + Workspace + overwriting the OOTB Supply Chain</li> <li>Change folder structure for GitOps repository</li> <li>Test Containers + DinD</li> <li>use Docker in Docker alternative from ITQ guy</li> </ul>","tags":["TAP","Tekton","Cartographer","CI/CD","Carvel","Tanzu"]},{"location":"tanzu/tap/fluxcd-ssh/","title":"Git access with SSH","text":"<p>We recommend having access to your git server via SSH.</p> <p>You can take a look at TAP's official docs or continue with my guide below.</p> <p>Using Gitea as example</p> <p>This guide takes Gitea as example, although it should work for any of the major Git servers (GitHub, GitHub Enterprise, GitLab, Bitbucket).</p> <p>To install Gitea, look here.</p>","tags":["TKG","TAP","TANZU","SSH"]},{"location":"tanzu/tap/fluxcd-ssh/#collect-known-hosts","title":"Collect Known hosts","text":"<p>Run <code>netshoot</code> or some other container with <code>ssh</code> tools inside the cluster.</p> <pre><code>kubectl run tmp-shell --rm -i --tty --image ${HARBOR_HOSTNAME}/test/netshoot:v0.9 --namespace default -- /bin/bash\n</code></pre> <pre><code>ssh-keyscan gitea-ssh.gitea.svc.cluster.local &gt; gitea-known-hosts.txt\n</code></pre> <p>The contents of this file go into your <code>GIT_SSH_KNOWN_HOSTS</code> variable.</p> <pre><code>cat gitea-known-hosts.txt\n</code></pre>","tags":["TKG","TAP","TANZU","SSH"]},{"location":"tanzu/tap/fluxcd-ssh/#generate-ssh-secret","title":"Generate SSH Secret","text":"<p>Assuming you are using Gitea, I'll also assume the <code>ssh key</code> for Gitea is located here: <code>~/.ssh/gitea_id_rsa</code>.</p> <pre><code>export GIT_SSH_SECRET_KEY=\"tap-build-ssh\"\nexport GIT_SERVER=\"${GITEA_HOSTNAME}\"\nexport GIT_SSH_PUSH_KEY=$(cat ~/.ssh/gitea_id_rsa)\nexport GIT_SSH_PULL_KEY=$(cat ~/.ssh/gitea_id_rsa)\nexport GIT_SSH_PULL_ID=$(cat ~/.ssh/gitea_id_rsa.pub)\nexport GIT_SSH_KNOWN_HOSTS=$(cat gitea-known-hosts.txt)\n</code></pre> <pre><code>ytt -f ytt/tap-build-ssh-key-secret.ytt.yml \\\n  -v secretName=\"$GIT_SSH_SECRET_KEY\" \\\n  -v server=\"$GITEA_HOSTNAME\" \\\n  -v sshPushKey=\"$GIT_SSH_PUSH_KEY\" \\\n  -v sshPullKey=\"$GIT_SSH_PULL_KEY\" \\\n  -v sshPullId=\"$GIT_SSH_PULL_ID\" \\\n  -v knownHosts=\"$GIT_SSH_KNOWN_HOSTS\" \\\n  &gt; \"tap-build-ssh-key-secret.yaml\"\n</code></pre> <p>NOTE: which namespaces? Says <code>ServiceAccount configured for the workload</code>, but what about FluxCD?</p> <pre><code>kubectl apply -f tap-build-ssh-key-secret.yaml \\\n  --namespace ${TAP_DEVELOPER_NAMESPACE}\n</code></pre> <ul> <li>Add the secret to the secrets of the SA doing the workloads</li> <li>for example, <code>default</code> in Namespace <code>default</code></li> </ul> <p>You can inspect the SA:</p> <pre><code>kubectl get sa -n default default -o yaml\n</code></pre> <p>Which will then look as follows:</p> <pre><code>apiVersion: v1\nimagePullSecrets:\n- name: registry-credentials\n- name: tap-registry\nkind: ServiceAccount\nmetadata:\n  creationTimestamp: \"2023-01-30T10:09:54Z\"\n  name: default\n  namespace: default\n  resourceVersion: \"12849738\"\n  uid: 2b97fa15-c681-447b-9700-85cafb6b561e\nsecrets:\n- name: registry-credentials\n- name: default-token-rxzbh\n- name: tap-build-ssh\n</code></pre>","tags":["TKG","TAP","TANZU","SSH"]},{"location":"tanzu/tap/gitea/","title":"Setup Gitea","text":"<p>For testing purposes, or in case there is no Git server yet, you can use Gitea.</p> <p>Gitea is a community managed lightweight code hosting solution written in Go. It is published under the MIT license.</p> <p>This is especially useful for testing things like SSH access with TAP in restricted environments.</p>","tags":["Gitea","Git","TAP"]},{"location":"tanzu/tap/gitea/#relocate-images","title":"Relocate Images","text":"","tags":["Gitea","Git","TAP"]},{"location":"tanzu/tap/gitea/#helm-decomposer-to-list-images","title":"Helm decomposer to list images","text":"<p>Helm Decomposer is a small project that helps you list images used in a particular helm chart.</p> <p>The Helm chart from Bitnami was a bit confusing, so I opted to use the official one instead.</p> <p>Helm Decomposer requires the Helm chart binary, which we can get by running <code>helm pull &lt;chartRepo&gt;/&lt;chartName&gt;</code>.</p> <pre><code>helm pull gitea-charts/gitea --version 7.0.2\n</code></pre> <p>We then run Helm Decomposer on the file.</p> <pre><code>./helm-decomposer -chart gitea-7.0.2.tgz -i -o\n</code></pre> <p>Which gives us a complete list of the images used.</p> <pre><code>\u2192 docker.io/bitnami/memcached:1.6.9-debian-10-r114\n\u2192 docker.io/bitnami/postgresql:11.11.0-debian-10-r62\n\u2192 busybox\n\u2192 gitea/gitea:1.18.3\n</code></pre>","tags":["Gitea","Git","TAP"]},{"location":"tanzu/tap/gitea/#copy-images","title":"Copy Images","text":"<p>Assuming you want to relocate the images directly from Dockerhub to a Harbor instance, you can run the commands below.</p> <p>Warning</p> <p>These commands expect that you have a project in Harbor named <code>bitnami</code>.</p> <pre><code>docker pull docker.io/bitnami/memcached:1.6.9-debian-10-r114  --platform linux/amd64\ndocker tag docker.io/bitnami/memcached:1.6.9-debian-10-r114 $HARBOR_HOSTNAME/bitnami/memcached:1.6.9-debian-10-r114\ndocker push $HARBOR_HOSTNAME/bitnami/memcached:1.6.9-debian-10-r114\n</code></pre> <pre><code>docker pull docker.io/bitnami/postgresql:11.11.0-debian-10-r62  --platform linux/amd64\ndocker tag docker.io/bitnami/postgresql:11.11.0-debian-10-r62 $HARBOR_HOSTNAME/bitnami/postgresql:11.11.0-debian-10-r62\ndocker push $HARBOR_HOSTNAME/bitnami/postgresql:11.11.0-debian-10-r62\n</code></pre> <pre><code>docker pull docker.io/gitea/gitea:1.18.3  --platform linux/amd64\ndocker tag docker.io/gitea/gitea:1.18.3 $HARBOR_HOSTNAME/gitea/gitea:1.18.3\ndocker push $HARBOR_HOSTNAME/gitea/gitea:1.18.3\n</code></pre> <pre><code>docker pull docker.io/library/busybox:1.36.0 --platform linux/amd64\ndocker tag docker.io/library/busybox:1.36.0 $HARBOR_HOSTNAME/library/busybox:1.36.0\ndocker push $HARBOR_HOSTNAME/library/busybox:1.36.0\ndocker tag docker.io/library/busybox:1.36.0 $HARBOR_HOSTNAME/library/busybox:latest\ndocker push $HARBOR_HOSTNAME/library/busybox:latest\n</code></pre>","tags":["Gitea","Git","TAP"]},{"location":"tanzu/tap/gitea/#certificate","title":"Certificate","text":"<p>It is recommended to always use a certificate with a Git server.</p> <p>If you haven't setup an CA yet, follow this guide first.</p> <p>Set the env variables that make sense for you.</p> <pre><code>export DOMAIN=h2o-2-4864.h2o.vmware.com\nexport BUILD_DOMAIN=\"build.${DOMAIN}\"\nexport GITEA_HOSTNAME=\"gitea.${BUILD_DOMAIN}\"\n</code></pre> <p>Then run the <code>cfssl</code> command to generate your certificate.</p> <pre><code>cfssl gencert -ca ssl/ca.pem -ca-key ssl/ca-key.pem \\\n  -config ssl/cfssl.json \\\n  -profile=server \\\n  -cn=\"${GITEA_HOSTNAME}\" \\\n  -hostname=\"${GITEA_HOSTNAME},gitea.gitea.svc.cluster.local,localhost\" \\\n   ssl/base-service-cert.json   | cfssljson -bare gitea\n</code></pre> <pre><code>mv gitea-key.pem ssl/\nmv gitea.pem ssl/\n</code></pre>","tags":["Gitea","Git","TAP"]},{"location":"tanzu/tap/gitea/#setup-secrets","title":"Setup Secrets","text":"","tags":["Gitea","Git","TAP"]},{"location":"tanzu/tap/gitea/#tls-secret","title":"TLS Secret","text":"<p>First ensure the namespace exists.</p> <pre><code>kubectl create namespace gitea | true\n</code></pre> <p>Then create the TLS secret with your certificate.</p> <pre><code>kubectl create secret tls gitea-tls \\\n  --cert=ssl/gitea.pem \\\n  --key=ssl/gitea-key.pem \\\n  --namespace gitea\n</code></pre>","tags":["Gitea","Git","TAP"]},{"location":"tanzu/tap/gitea/#admin-secret","title":"Admin secret","text":"<p>First, ensure the namespace exists.</p> <pre><code>kubectl create namespace gitea | true\n</code></pre> <p>Then set the env variables so they make sense for you.</p> <pre><code>GITEA_ADMIN_USERNAME=\"gitea\"\nGITEA_ADMIN_PASSWORD='gitea'\nGITEA_ADMIN_EMAIL=\"gitea@local.domain\"\nGITEA_ADMIN_SECRET=\"gitea-admin\"\n</code></pre> <p>And then create the admin secret for Gitea to use.</p> <pre><code>kubectl create secret generic ${GITEA_ADMIN_SECRET} \\\n  --namespace gitea \\\n  --from-literal=password=${GITEA_PASSWORD} \\\n  --from-literal=username=${GITEA_ADMIN_PASSWORD} \\\n  --from-literal=email=${GITEA_ADMIN_EMAIL} \n</code></pre>","tags":["Gitea","Git","TAP"]},{"location":"tanzu/tap/gitea/#install-gitea-helm-chart","title":"Install Gitea Helm Chart","text":"<p>First add the repository to your Helm client.</p> <pre><code>helm repo add gitea-charts https://dl.gitea.io/charts/\nhelm repo update\n</code></pre> <p>Assuming you still have the env variables setup, run the <code>ytt</code> command to generat the Helm values.</p> <pre><code>ytt -f ytt/gitea.ytt.yml \\\n  -v tlsSecret=\"gitea-tls\" \\\n  -v passwordSecret=\"$GITEA_ADMIN_SECRET\" \\\n  -v hostname=\"$GITEA_HOSTNAME\" \\\n  &gt; \"gitea-values.yml\"\n</code></pre> <p>Info</p> <p>I always recommend using <code>helm upgrade --install</code> over <code>helm install</code>.</p> <p>As this means you will always use the same command for installing and updating that install.</p> <p>Else you run the risk of the install and update command diverging.</p> <p>And then run the <code>helm upgrade --install</code> command.</p> <pre><code>helm upgrade --install \\\n  --values gitea-values.yml \\\n  --namespace gitea \\\n  gitea \\\n  gitea-charts/gitea\n</code></pre> Gitea YTT Template <pre><code>#@ load(\"@ytt:data\", \"data\")\n---\nglobal:\n  imageRegistry: harbor.h2o-2-4864.h2o.vmware.com\n\ngitea:\n  admin:\n    existingSecret: #@ data.values.passwordSecret\n\ningress:\n  enabled: true\n  className: contour\n  hosts:\n    - host: #@ data.values.hostname\n      paths:\n        - path: /\n          pathType: Prefix\n  tls:\n  - secretName: #@ data.values.tlsSecret\n    hosts:\n      - #@ data.values.hostname\n</code></pre>","tags":["Gitea","Git","TAP"]},{"location":"tanzu/tap/gitea/#references","title":"References","text":"<ul> <li>Gitea Official Helm Chart</li> <li>Bitnami Gitea Helm chart</li> <li>Blog post on using SSH keys with Gitea</li> </ul>","tags":["Gitea","Git","TAP"]},{"location":"tanzu/tap-gitops/","title":"TAP GitOps","text":"<p>We can install the Tanzu Application Platform, or TAP, in one of two supported ways.</p> <ol> <li>Scripted<sup>2</sup></li> <li>GitOps<sup>1</sup></li> </ol> <p>In the Scripted way, we run CLI and Shell commands to configure the prerequisites, configure the installation values, and install the TAP package. This is the original and currently only GA method.</p> <p>The GitOps manner leverages the increasingly more common way of having a Kubernetes Controller synchronizing Kubernetes Manifests from a Git Repository.</p> <p>TAP uses the Git synchronization feature of the KAPP Controller to manage the TAP package installation. Continuously synchronizing a bootstrapped Git repository containing all the TAP installation values to update the installation.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/#goals-of-this-guide","title":"Goals of this Guide","text":"<p>The goals of this are the following:</p> <ol> <li>Provide a more prescriptive guide for installing TAP with GitOps than the official documentation provides</li> <li>Provide concrete examples of the repositories involved (for TAP installation and the application management after)</li> <li>Provide an example of how to create a complete environment setup, leveraging GitOps for resources outside of the TAP Install scope</li> </ol> <p>In other words, we want to ensure we can install TAP via its GitOps install. And explore how to do so in the broader context of managing an environment spanning multiple clusters with some dedicated to running applications.</p> <p>The difference between this and the official documentation is that this guide is prescriptive. It makes choices and chooses a singular path.</p> <p>More paths and options exist and are equally valid.</p> <p>One such choice is where to store secrets that need to end up in the Kubernetes clusters used by TAP components and applications.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/#choose-secrets-management","title":"Choose Secrets Management","text":"<p>TAP currently supports two ways of managing the secrets the installation requires.</p> <ol> <li>TAP with External Secrets Operator<sup>3</sup></li> <li>TAP with SOPS<sup>4</sup></li> </ol> <p>The External Secrets Operator<sup>5</sup> is a Kubernetes operator that synchronizes secrets from external secret management systems into Kubernetes.</p> <p>Warning</p> <p>TAP 1.5 only supports AWS's KMS when using the ESO.</p> <p>Secrets OPerationS<sup>6</sup> is a tool for managing your secrets within your Git repository.</p> <p>Both options are solid solutions used by many.</p> <p>For this guide, we'll work with SOPS.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/#environment-to-build","title":"Environment To Build","text":"<p>Through this guide, we build an environment consisting of five clusters:</p> <ol> <li>Shared Services: this cluster houses services that all the other clusters need, such as the image registry (Harbor), Git server (GitLab), and more.</li> <li>Build: where we install TAP with a Build profile to build and test our software.</li> <li>View: where we install TAP with a View profile to explore the workloads in the other clusters.</li> <li>Run-01: where we install TAP with a Run profile. We consider this our Staging environment.</li> <li>Run-02: where we install TAP with a Run profile. We consider this our Production environment.</li> </ol> <p></p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/#tools-and-clis","title":"Tools and CLIs","text":"<ul> <li>Tanzu CLI (for TAP)<sup>7</sup></li> <li>yq<sup>8</sup></li> <li>jq<sup>9</sup></li> <li>flux CLI<sup>10</sup> CLI for FluxCD</li> <li>knative CLI<sup>11</sup></li> <li>gnupg<sup>12</sup></li> <li>age<sup>13</sup></li> <li>sops<sup>14</sup></li> </ul>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/#references","title":"References","text":"<ol> <li> <p>TAP 1.5 - GitOps Install introduction \u21a9</p> </li> <li> <p>TAP 1.5 - Online Install (scripted) \u21a9</p> </li> <li> <p>TAP 1.5 - GitOps with External Secrets Operator \u21a9</p> </li> <li> <p>TAP 1.5 - GitOps with SOPS \u21a9</p> </li> <li> <p>External Secrets Operator docs \u21a9</p> </li> <li> <p>SOPS GitHub and Docs \u21a9</p> </li> <li> <p>Tanzu CLI \u21a9</p> </li> <li> <p>yq - lightweight and portable command-line YAML, JSON and XML processor \u21a9</p> </li> <li> <p>jq - jq is like sed for JSON data \u21a9</p> </li> <li> <p>Flux CLI \u21a9</p> </li> <li> <p>Knative CLI \u21a9</p> </li> <li> <p>gnupg \u21a9</p> </li> <li> <p>age - simple, modern and secure file encryption tool \u21a9</p> </li> <li> <p>SOPS - Secret OPerationS \u21a9</p> </li> </ol>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/next-steps/","title":"Next Steps","text":"<p>If you've followed along and installed TAP with GitOps in the Build, Run, and View cluster, you might wonder what is next?</p> <p>We have prepared some next steps of things that often see customers do:</p> <ul> <li>Run Cluster deploying applications with ArgoCD</li> <li>Finalize the GitOps Flow for the Applications</li> <li>Configure GitLab Integrations in TAP GUI</li> <li>TAP GUI TechDocs hosted with MinIO</li> <li>More Supply Chain Customizations Are Coming</li> </ul>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/run-with-argocd/","title":"TAP Run with ArgoCD","text":"","tags":["TKG","TAP","GitOps","Carvel","Tanzu","ArgoCD"]},{"location":"tanzu/tap-gitops/services/","title":"Shared Services","text":"<p>The goal of this cluster is to provide all the services the TAP clusters need.</p> <p>This includes but is not limited to:</p> <ul> <li>Image Registry -&gt; Harbor<sup>11</sup></li> <li>Binary Artifact Repository -&gt; Sonatype Nexus<sup>12</sup></li> <li>Git Server -&gt; GitLab<sup>22</sup></li> <li>Authentication -&gt; OpenLDAP<sup>13</sup></li> <li>Single Sign On -&gt; Keycloak<sup>14</sup></li> <li>Monitoring -&gt; Prometheus<sup>15</sup> (Thanos<sup>16</sup>) + Grafana<sup>17</sup></li> <li>SAST (Static Code Analysis) -&gt; SonarQube<sup>18</sup></li> <li>Certificate Management -&gt; Hashicorp Vault<sup>19</sup></li> <li>Object Storage -&gt; MinIO<sup>20</sup></li> </ul> <p>The services themselves don't matter that much persee. We focus initially on the function of the server and how we get the services installed.</p> <p>Where applicable, we will look at a service's configuration in detail.</p> <p>Note On Infrastructure<p>While most of the installation values we use can be applied to any Kubernetes cluster, there are some infrastructure specific.</p> <p>As this environment is installed on Tanzu Kubernetes Grid 2.1 with a management cluster (e.g., TGKm), there are places where the values are specific to this infrastructure.</p> </p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#gitops-tool-of-choice","title":"GitOps Tool Of Choice","text":"<p>As the goal is to dive into using TAP's GitOps capabilities, it makes a lot of sense to install the services that way as well.</p> <p>There are several tools available that can do the job. For the sake of this guide, I've select FluxCD.</p> <p>For two reasons:</p> <ol> <li>Several VMware Tanzu products include FluxCD, such as TMC and TAP, so I want more practice.</li> <li>FluxCD is straightforward for when you need to create \"layers\" of dependant resurces, which is excellent for bootstrapping core services on Kubernetes.</li> </ol> <p>A bonus reason, ArgoCD seems to work very well as a complementary technology for installing Applications. We can use it for ensure TAP's Deliverable's are synchronized to the Run clusters. This way, we can leverage two of the most popular GitOps tools and leave it to the reader to decide any preference.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#fluxcd-bootstrapping","title":"FluxCD Bootstrapping","text":"<p>To avoid a chicken and egg situation with the Git server we use GitHub to bootstrap FluxCD<sup>21</sup>.</p> <p>Set the environment variables for your GitHub user and the PAT token (recommended).</p> <pre><code>export GITHUB_TOKEN=&lt;your-token&gt;\nexport GITHUB_USER=&lt;your-username&gt;\nexport GITHUB_REPO=tap-gitops\n</code></pre> <p>Verify all is in order:</p> <pre><code>flux check --pre\n</code></pre> <p>If alls is good, we can run the flux bootstrap:</p> <pre><code>flux bootstrap github \\\n  --owner=${GITHUB_USER} \\\n  --repository=${GITHUB_REPO} \\\n  --branch=main \\\n  --path=./platforms/clusters/services \\\n  --personal\n</code></pre> <p>Repository Folder Structure<p>As you can see, I use several layers for the folder structure for FluxCD.</p> <p>At the first layer, I differentiate between the platform configuration and other types of configuration. You can think about infrastructure, docs, or possibly application configuration.</p> <p>Inside platforms, I use the customary <code>clusters</code> folder, in which the GitOps folders for each (Kubernetes) cluster are located.</p> </p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#sops-for-secrets","title":"SOPS For Secrets","text":"<p>In the introduction, we decided to use SOPS for managing secrets that need to be synchronized into the Kubernetes clusters.</p> <p>FluxCD supports SOPS as well, so there's no need to choose differently <sup>2</sup>.</p> <p>The actual encryption is done via an encryption library. Here we have the choice for GPG, or Age <sup>3</sup>.</p> <p>I've chosen to use Age, primarily because I had not used it before<sup>4</sup>.</p> <p>Encrypting with Vault</p> <p>If you have a HashiCorp Vault instance running outside of the cluster, it is likely a better solution.</p> <p>Same goes for using the public cloud managed solutions if your cluster is running there.</p> <p>As this environment is build on-prem, and the Vault instance is going to be installed in this cluster, we cannot use those.</p> <p>Create the Age key:</p> <pre><code>age-keygen -o age.agekey\n</code></pre> <p>Ensure the <code>flux-system</code> namespace exists:</p> <pre><code>kubectl create namespace flux-system\n</code></pre> <p>And then create the decryption secret for FluxCD:</p> <pre><code>cat age.agekey |\nkubectl create secret generic sops-age \\\n--namespace=flux-system \\\n--from-file=age.agekey=/dev/stdin\n</code></pre> <p>Use the following to quickly export the Age key for use in SOPS commands:</p> <pre><code>export SOPS_AGE_KEY=$(cat age.agekey  | grep \"# public key: \" | sed 's/# public key: //')\n</code></pre> <p>Such as encrypting a file, containing a Kubernetes Secret manifest, in place:</p> <pre><code>sops --age=${SOPS_AGE_KEY} \\\n  --encrypt \\  \n  --encrypted-regex '^(data|stringData)$' \\\n  --in-place basic-auth.yaml\n</code></pre> <p>To create such a secret file, we can use the following command:</p> <pre><code>kubectl create secret generic basic-auth \\\n  --from-literal=username='MyUser' \\\n  --from-literal=password='MySecretPassword' \\\n  --namespace targetNamespace \\\n  --dry-run=client \\\n  -oyaml &gt; basic-auth.yaml\n</code></pre> <p>Configure Decryption</p> <p>Last but not least, any (FluxCD) Kustomization that needs to decrypt these secrets, needs to the decryption configuration<sup>5</sup>.</p> <p>This configuration references the secret containing the decryption key, and the type of encryption used.</p> <p>In our case, with the above created secret (<code>sops-age</code>) and the Age encryption, it should be the following:</p> Kustomization Example<pre><code>apiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nspec:\n  decryption:\n    provider: sops\n    secretRef:\n      name: sops-age\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#source-layers","title":"Source Layers","text":"<p>One of the complication with applying resources to a Kubernetes cluster, is the ordering.</p> <p>Tool A might depend on Tool B to exist. Tool B might need a secret, that secret requires a namespace, and so we can there are different layers to be applied in order.</p> <p>Not only do we need this ordering at the start, we need to be able to add \u00a0</p> <p>I generaly use the following set of Kustomizations and their ordering:</p> <ol> <li>Namespaces</li> <li>Secrets</li> <li>Cluster Essentials<ul> <li>this is a reference to the Tanzu Cluster Essentials, namely, KAPP Controller and SecretGen Controller</li> <li>as I'm using TKG 2.x, this is already installed by default</li> </ul> </li> <li>Deployment Pre-requisites<ul> <li>this contains pre-requisites for Helm installs and KAPP package installs (e.g., two seperate <code>Kustomization</code>'s)</li> <li>for example, Helm Repository resources</li> </ul> </li> <li>Deployments<ul> <li>this contains a <code>Kustomization</code> for Helm charts and KAPP packages</li> </ul> </li> <li>Post-Deployments<ul> <li>this contains resources that can only be installed (e.g., depend on CRD's) after an installation is finished</li> </ul> </li> </ol> <p>Especially the Post-Deployments is a tricky category.</p> <p>Unfortunately, there are some Kubernetes Controllers that install their CRD's lazily. It is a good solution in general, but it does mean we're better of seperating the installation of these CR's into their own category.</p> <p>I usually separate the KAPP universe from the Helm universe, but for the ordering and end result this does not matter.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#installing-services","title":"Installing Services","text":"<p>Let's go over some of these layers, how I've configured them and what options you have.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#namespaces-and-secrets","title":"Namespaces and Secrets","text":"<p>So first, we need to inform FluxCD what to synchronize. So we define Kustomizations for these two \"core\" layer resources.</p> ./platforms/clusters/services/kustomizations/core-layer-kustomizations.yaml <pre><code>apiVersion: kustomize.toolkit.fluxcd.io/v1beta2\nkind: Kustomization\nmetadata:\n  name: namespaces\n  namespace: flux-system\nspec:\n  interval: 3m0s\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n  path: ./platforms/clusters/services/flux-sources/namespaces\n  prune: true\n---\napiVersion: kustomize.toolkit.fluxcd.io/v1beta2\nkind: Kustomization\nmetadata:\n  name: secrets\n  namespace: flux-system\nspec:\n  dependsOn:\n    - name: namespaces\n  interval: 3m0s\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n  path: ./platforms/clusters/services/flux-sources/secrets\n  prune: true\n  decryption:\n    provider: sops\n    secretRef:\n      name: sops-age\n</code></pre> <p>Then we can create namespace definitions in the <code>flux-sources/namespaces</code> folder. Running:</p> <pre><code>tree flux-sources\n</code></pre> <p>Gives you an idea which names we want to create and configure:</p> <pre><code>flux-sources\n\u251c\u2500\u2500 namespaces\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 auth.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cert-manager.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gitea.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gitlab.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 jenkins.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 minio.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 monitoring.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 nexus.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 psql.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 sonar.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tanzu-packages.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tanzu-system-ingress.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tanzu-system-registry.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tap-install.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 trivy.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 vault.yaml\n\u2514\u2500\u2500 secrets\n    \u251c\u2500\u2500 basic-auth.yaml\n    \u251c\u2500\u2500 gitea-credentials.yaml\n    \u251c\u2500\u2500 keycloak-credentials.yaml\n    \u251c\u2500\u2500 ldap-credentials.yaml\n    \u251c\u2500\u2500 minio-credentials.yaml\n    \u2514\u2500\u2500 sonar-credentials.yaml\n</code></pre> <p>For example:</p> namespaces/auth.yaml<pre><code>kind: Namespace\napiVersion: v1\nmetadata:\n  name: auth\n</code></pre> <p>This ensures we can create the secret for Keycloak in here:</p> Keycloak Postgres Secret example secrets/keycloak-credentials.yaml<pre><code>apiVersion: v1\ndata:\n    password: ENC[AES256_...g==,iv:KJ9maeBJ+5aFC2+....+8=,tag:.../phmQ==,type:str]\n    postgres-password: ENC[AES256_GCM,data:...+xQ==,iv:EJWCeSuRynCC/...+7b+hA=,tag:...==,type:str]\n    postgres-user: ENC[AES256_GCM,data:Jf0B+.../yEyWRHeZbtA=,iv:../ZGID4xHLs140SQhUQCwQZ1g=,tag:..==,type:str]\n    username: ENC[AES256_GCM,data:...=,iv:...=,tag:..==,type:str]\nkind: Secret\nmetadata:\n    creationTimestamp: null\n    name: keycloak-credentials\n    namespace: auth\nsops:\n    kms: []\n    gcp_kms: []\n    azure_kv: []\n    hc_vault: []\n    age: []\n    lastmodified: \"2023-06-16T07:03:16Z\"\n    mac: ENC[AES256_GCM,data:..../....+taPwJY=,iv:zjB3mMdVkplMAKVGBYv0T0zg4tbBz6fhXtydP6Xowok=,tag:..==,type:str]\n    pgp:\n        - created_at: \"2023-06-16T07:03:09Z\"\n          enc: |\n            -----BEGIN PGP MESSAGE-----\n\n            hQIMA97+BPKHpJmVARAAxjycg/w8e2FlABl7givc25B8RArpY5m/lLKIgOg5fVOq\n            ...\n            ZS0JXt78e0Y8VwndrNRgqGVsrtk8K4RlLkFhJZPGteljmh+lPodMrGf3477guGdL\n            zkS+rtJIlA==\n            =BugG\n            -----END PGP MESSAGE-----\n          fp: FEFCF5923A0CD7DD810696B19B7D92BE442BD4EC\n    encrypted_regex: ^(data|stringData)$\n    version: 3.7.3\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#helm","title":"Helm","text":"<p>For Helm Charts, we have two steps.</p> <p>First, we need to ensure we have access to Helm Chart by feeding FluxCD the Helm Repository, then we can install the Helm Chart with its install values.</p> <p>We point a <code>Kustomization</code> to the sub-folder <code>flux-sources/helm-prereqs</code>, so all files in there get synchronized. This means you can choose to put each <code>HelmRepository</code> in its own file, or one file with all repositories or any combination.</p> <p>I separated them into their own files, so it is easier for people to create a PR with adding a repository by adding a new file.</p> <pre><code>flux-sources\n\u251c\u2500\u2500 helm-prereqs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 aquasecurity-repo.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bitnami-repo.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gitlab.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hashicorp-repo.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 jenkins-repo.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 openldap-repo.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 sonarqube-repo.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 sonatype.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tanzu-repo.yaml\n</code></pre> <p>Where each file like something like this:</p> flux-sources/helm-prereqs/tanzu-repo.yaml<pre><code>apiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: HelmRepository\nmetadata:\n  name: tanzu\n  namespace: default\nspec:\n  interval: 5m\n  url: https://vmware-tanzu.github.io/helm-charts\n</code></pre> <p>Then in the <code>flux-sources/helm</code> sub-folder we store all the Helm Chart installations.</p> <p>For each I intent of having everything related to that installation in the same file.</p> <p>As long as I can guarantee I can install it at the same time, else it needs to move to the helm-post <code>Kustomization</code>, to avoid locking the reconciliation loop.</p> <p>For example, for a service that needs to be exposed to the outside world, we might add a <code>Certificate</code> and a <code>HTTPProxy</code>:</p> Helm Install Example flux-sources/helm/nexus.yaml<pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: nexus\n  namespace: nexus\nspec:\n  interval: 5m\n  timeout: 10m0s\n  chart:\n    spec:\n      chart: nexus-repository-manager\n      version: \"55.0.0\"\n      sourceRef:\n        kind: HelmRepository\n        name: sonatype\n        namespace: default\n      interval: 5m\n  values:\n    image:\n      repository: harbor.services.my-domain.com/dh-proxy/sonatype/nexus3\n    nexus:\n      resources:\n        requests:\n          cpu: 4\n          memory: 8Gi\n        limits:\n          cpu: 4\n          memory: 8Gi\n    ingress:\n      enabled: false\n---\napiVersion: projectcontour.io/v1\nkind: HTTPProxy\nmetadata:\n  name: nexus\n  namespace: nexus\nspec:\n  ingressClassName: contour\n  virtualhost:\n    fqdn: nexus.services.my-domain\n    tls:\n      secretName: nexus-tls\n  routes:\n  - services:\n    - name: nexus-nexus-repository-manager\n      port: 8081\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: gitea-ssh\n  namespace: nexus\nspec:\n  secretName: nexus-tls\n  issuerRef:\n    name: vault-issuer\n    kind: \"ClusterIssuer\"\n  commonName: nexus.services.my-domain\n  dnsNames:\n  - nexus.services.my-domain\n---\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#carvel-apps","title":"Carvel Apps","text":"<p>I treat Carvel Apps, or K-Apps, as a separate category.</p> <p>The structure to handle them is very similar to the Helm applications though. We use a pre-reqs, main, and post folders to handle them.</p> <p>The recommended approach for the Carvel application installations, is to have a specific ServiceAccount for them. So you start with an RBAC configuration file, and then add the sources of the packages: <code>PackageRepository</code> resources.</p> <p>The RBAC file is quite long, so I'll just refer to it. You can find it here <sup>1</sup>.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#package-repository","title":"Package Repository","text":"<p>As I intend to install tools such as Harbor from the Tanzu kubernetes Grid repository, I add the TKG repository:</p> platforms/clusters/services/flux-sources/kapp-prereqs/tkg-v2-1-1.yaml<pre><code>apiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageRepository\nmetadata:\n  annotations:\n    packaging.carvel.dev/downgradable: \"\" # because it sorts on the hash...\n  name: standard\n  namespace: tkg-system\nspec:\n  fetch:\n    imgpkgBundle:\n      image: projects.registry.vmware.com/tkg/packages/standard/repo:v2.1.1\n</code></pre> <p>Then we can install our desired packages through those.</p> <p>Unlike the FluxCD CRs related to Helm, FluxCD synchronizes these to the cluster and let's KAPP Controller handle it from there.</p> <p>For KAPP Controller, there are only two relevant namespaces for packages.</p> <ol> <li>the namespace a <code>Package</code> is made available via a <code>PackageRepository</code></li> <li>the global package namespace</li> </ol> <p>KAPP Controller defines a namespace as its global namespace. Any <code>Package</code> made available here through a <code>PackageRepository</code> can then be installed in any namespace.</p> <p>Otherwise, a <code>Package</code> can only be installed in the namespace of the <code>PackageRepository</code>.</p> <p>The flag <code>packaging-global-namespace</code> determines the namespace deemed global, which is set to <code>tkg-system</code> for TKG 2.x based clusters.</p> <p>Which is why the <code>PackageRepository</code> above is installed in that namespace, so we can install the packages anywhere we want.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#example-package","title":"Example Package","text":"<p>To show how to install a Carvel Package, I use the Contour package as an example.</p> <p>It requires the Cert-manager package to exist, but KAPP Controller will keep reconciling it, so as long as you eventually install the Cert-manager package, it will succeed.</p> <p>The config file consists of several YAML files:</p> <ul> <li>The <code>Role</code> and <code>RoleBinding</code> so KAPP Controller is allowed to install Contour in the correct namespace (e.g., <code>tanzu-system-ingress</code>)</li> <li>A <code>Secret</code>, containing the installation Values for the Carvel Package, similar to Helm install values</li> <li>The <code>PackageInstall</code>, which instructs KAPP Controller how to install the package</li> </ul> Contour Package Example platforms/clusters/services/flux-sources/kapp/contour.yaml<pre><code>---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: kapp-controller-role\n  namespace: tanzu-system-ingress\nrules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\", \"services\", \"secrets\", \"pods\", \"serviceaccounts\"]\n  verbs: [\"*\"]\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\", \"replicasets\", \"daemonsets\"]\n  verbs: [\"*\"]\n- apiGroups: [\"cert-manager.io\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: kapp-controller-role-binding\n  namespace: tanzu-system-ingress\nsubjects:\n- kind: ServiceAccount\n  name: kapp-controller-sa\n  namespace: tanzu-packages\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: kapp-controller-role\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: contour-values\n  namespace: tanzu-packages\nstringData:\n  values.yml: |\n    infrastructure_provider: vsphere\n    namespace: tanzu-system-ingress\n    contour:\n      useProxyProtocol: false\n      replicas: 2\n      pspNames: \"vmware-system-restricted\"\n      logLevel: info\n    envoy:\n      service:\n        type: LoadBalancer\n        annotations: {}\n        nodePorts:\n          http: null\n          https: null\n        externalTrafficPolicy: Cluster\n        disableWait: false\n      hostPorts:\n        enable: true\n        http: 80\n        https: 443\n      hostNetwork: false\n      terminationGracePeriodSeconds: 300\n      logLevel: info\n      pspNames: null\n    certificates:\n      duration: 8760h\n      renewBefore: 360h\n\n---\napiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageInstall\nmetadata:\n  name: contour\n  namespace: tanzu-packages\nspec:\n  serviceAccountName: kapp-controller-sa\n  packageRef:\n    refName: contour.tanzu.vmware.com\n    versionSelection:\n      constraints: 1.22.3+vmware.1-tkg.1\n  values:\n  - secretRef:\n      name: contour-values\n      key: values.yml\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#harbor-and-dockerhub-proxy","title":"Harbor and Dockerhub Proxy","text":"<p>Docker and DockerHub have been amazing contributors to the growth of Containerization. And I believe both the technology and the service have contributed productivity gains.</p> <p>Unfortunately, DockerHub now has strict rate limits, which makes it cumbersome and a bit of a lotery for certain popular images.</p> <p>So I usually register my DockerHub account in my Harbor, and create a Proxy repository mapping to those DockerHub registry. Then, within the Helm Chart values, I override the image repositories to the proxy repository.</p> <p>For a more in-depth explanation on how to do this, read this guide on the Tanzu Developer portal<sup>10</sup>.</p> <p>As an example, this is the Nexus Helm install values shown earlier:</p> <pre><code>values:\n  image:\n    repository: harbor.services.my-domain.com/dh-proxy/sonatype/nexus3\n  nexus:\n    ...\n</code></pre> <p>Harbor will download the images from DockerHub with an account, reducing the rate limit problem, and cache them, reducing the problem further.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#hashicorp-vault-certificate-management","title":"HashiCorp Vault Certificate Management","text":"<p>For leveraging HashiCorp Vault to manage your certificates, I refer you to the official documentation <sup>23</sup>.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#references","title":"References","text":"<p>Each file looks something like this:</p> flux-sources/helm-prereqs/tanzu-repo.yaml<pre><code>apiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: HelmRepository\nmetadata:\n  name: tanzu\n  namespace: default\nspec:\n  interval: 5m\n  url: https://vmware-tanzu.github.io/helm-charts\n</code></pre> <p>Then, in the <code>flux-sources/helm</code> sub-folder, we store all the Helm Chart installations.</p> <p>I intend to have everything related to that installation in the same file.</p> <p>As long as I can guarantee I can install it simultaneously, it needs to move to the helm-post <code>Kustomization</code>, to avoid locking the reconciliation loop.</p> <p>For example, for a service that needs to be exposed to the outside world, we might add a <code>Certificate</code> and a <code>HTTPProxy</code>:</p> Helm Install Example flux-sources/helm/nexus.yaml<pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: nexus\n  namespace: nexus\nspec:\n  interval: 5m\n  timeout: 10m0s\n  chart:\n    spec:\n      chart: nexus-repository-manager\n      version: \"55.0.0\"\n      sourceRef:\n        kind: HelmRepository\n        name: sonatype\n        namespace: default\n      interval: 5m\n  values:\n    image:\n      repository: harbor.services.my-domain.com/dh-proxy/sonatype/nexus3\n    nexus:\n      resources:\n        requests:\n          cpu: 4\n          memory: 8Gi\n        limits:\n          cpu: 4\n          memory: 8Gi\n    ingress:\n      enabled: false\n---\napiVersion: projectcontour.io/v1\nkind: HTTPProxy\nmetadata:\n  name: nexus\n  namespace: nexus\nspec:\n  ingressClassName: contour\n  virtualhost:\n    fqdn: nexus.services.my-domain\n    tls:\n      secretName: nexus-tls\n  routes:\n  - services:\n    - name: nexus-nexus-repository-manager\n      port: 8081\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: gitea-ssh\n  namespace: nexus\nspec:\n  secretName: nexus-tls\n  issuerRef:\n    name: vault-issuer\n    kind: \"ClusterIssuer\"\n  commonName: nexus.services.my-domain\n  dnsNames:\n  - nexus.services.my-domain\n---\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#carvel-apps_1","title":"Carvel Apps","text":"<p>I treat Carvel Apps, or K-Apps, as a separate category.</p> <p>The structure to handle them is very similar to the Helm applications though. We use prereqs, primary, and post folders to handle them.</p> <p>The recommended approach for the Carvel application installations is having a specific ServiceAccount. So you start with an RBAC configuration file and then add the sources of the packages: <code>PackageRepository</code> resources.</p> <p>The RBAC file is long, so I'll refer to it. You can find it here <sup>1</sup>.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#package-repository_1","title":"Package Repository","text":"<p>As I intend to install tools such as Harbor from the Tanzu Kubernetes Grid repository, I add the TKG repository:</p> platforms/clusters/services/flux-sources/kapp-prereqs/tkg-v2-1-1.yaml<pre><code>apiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageRepository\nmetadata:\n  annotations:\n    packaging.carvel.dev/downgradable: \"\" # because it sorts on the hash...\n  name: standard\n  namespace: tkg-system\nspec:\n  fetch:\n    imgpkgBundle:\n      image: projects.registry.vmware.com/tkg/packages/standard/repo:v2.1.1\n</code></pre> <p>Then we can install our desired packages through those.</p> <p>Unlike the FluxCD CRs related to Helm, FluxCD synchronizes these to the cluster and lets the KAPP Controller handle it from there.</p> <p>For the KAPP Controller, there are only two relevant namespaces for packages.</p> <ol> <li>the namespace a <code>Package</code> is made available via a <code>PackageRepository</code></li> <li>the global package namespace</li> </ol> <p>KAPP Controller defines a namespace as its global namespace. Any <code>Package</code> made available here through a <code>PackageRepository</code> can then be installed in any namespace.</p> <p>Otherwise, a <code>Package</code> can only be installed in the namespace of the <code>PackageRepository</code>.</p> <p>The flag <code>packaging-global-namespace</code> determines the namespace deemed global, which is set to <code>tkg-system</code> for TKG 2.x based clusters.</p> <p>This is why the <code>PackageRepository</code> above is installed in that namespace, so we can install the packages anywhere we want.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#example-package_1","title":"Example Package","text":"<p>To show how to install a Carvel Package, I use the Contour package as an example.</p> <p>It requires the Cert-manager package to exist, but KAPP Controller will keep reconciling it, so as long as you eventually install the Cert-manager package, it will succeed.</p> <p>The config file consists of several YAML files:</p> <ul> <li>The <code>Role</code> and <code>RoleBinding</code> so the KAPP Controller is allowed to install Contour in the correct namespace (e.g., <code>tanzu-system-ingress</code>)</li> <li>A <code>Secret</code>, containing the installation Values for the Carvel Package, similar to Helm install values</li> <li>The <code>PackageInstall</code>, which instructs KAPP Controller how to install the package</li> </ul> Contour Package Example platforms/clusters/services/flux-sources/kapp/contour.yaml<pre><code>---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: kapp-controller-role\n  namespace: tanzu-system-ingress\nrules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\", \"services\", \"secrets\", \"pods\", \"serviceaccounts\"]\n  verbs: [\"*\"]\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\", \"replicasets\", \"daemonsets\"]\n  verbs: [\"*\"]\n- apiGroups: [\"cert-manager.io\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: kapp-controller-role-binding\n  namespace: tanzu-system-ingress\nsubjects:\n- kind: ServiceAccount\n  name: kapp-controller-sa\n  namespace: tanzu-packages\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: kapp-controller-role\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: contour-values\n  namespace: tanzu-packages\nstringData:\n  values.yml: |\n    infrastructure_provider: vsphere\n    namespace: tanzu-system-ingress\n    contour:\n      useProxyProtocol: false\n      replicas: 2\n      pspNames: \"vmware-system-restricted\"\n      logLevel: info\n    envoy:\n      service:\n        type: LoadBalancer\n        annotations: {}\n        nodePorts:\n          http: null\n          https: null\n        externalTrafficPolicy: Cluster\n        disableWait: false\n      hostPorts:\n        enable: true\n        http: 80\n        https: 443\n      hostNetwork: false\n      terminationGracePeriodSeconds: 300\n      logLevel: info\n      pspNames: null\n    certificates:\n      duration: 8760h\n      renewBefore: 360h\n\n---\napiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageInstall\nmetadata:\n  name: contour\n  namespace: tanzu-packages\nspec:\n  serviceAccountName: kapp-controller-sa\n  packageRef:\n    refName: contour.tanzu.vmware.com\n    versionSelection:\n      constraints: 1.22.3+vmware.1-tkg.1\n  values:\n  - secretRef:\n      name: contour-values\n      key: values.yml\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#harbor-and-dockerhub-proxy_1","title":"Harbor and Dockerhub Proxy","text":"<p>Docker and DockerHub have been outstanding contributors to the growth of Containerization. And both the technology and the service have contributed to productivity gains.</p> <p>Unfortunately, DockerHub now has strict rate limits, which makes it cumbersome and a bit of a lottery for specific popular images.</p> <p>I usually register my DockerHub account in my Harbor and create a Proxy repository mapping to that DockerHub registry. Then, I override the image repositories within the Helm Chart values to the proxy repository.</p> <p>For a more in-depth explanation of how to do this, read this guide on the Tanzu Developer portal<sup>10</sup>.</p> <p>As an example, this is the Nexus Helm install values shown earlier:</p> <pre><code>values:\n  image:\n    repository: harbor.services.my-domain.com/dh-proxy/sonatype/nexus3\n  nexus:\n    ...\n</code></pre> <p>Harbor will download the images from DockerHub with an account, reducing the rate limit problem and cache them, reducing the pain further.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#hashicorp-vault-certificate-management_1","title":"HashiCorp Vault Certificate Management","text":"<p>For leveraging HashiCorp Vault to manage your certificates, I refer you to the official documentation <sup>23</sup>.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/services/#references_1","title":"References","text":"<ol> <li> <p>TAP GitOps Example Repo - Services Cluster \u21a9\u21a9</p> </li> <li> <p>FluxCD - Encrypting using SOPS \u21a9</p> </li> <li> <p>FluxCD - SOPS using Age Key \u21a9</p> </li> <li> <p>Age - encryption tool \u21a9</p> </li> <li> <p>FluxCD - Kustomizations \u21a9</p> </li> <li> <p>Tanzu Cluster Essentials \u21a9</p> </li> <li> <p>Carvel - KAPP Controller \u21a9</p> </li> <li> <p>Carvel - SecretGen Controller \u21a9</p> </li> <li> <p>Tanzu Kubernetes Grid 2.1 - Package Repository \u21a9</p> </li> <li> <p>Tanzu Developer Portal - Using Harbor as DockerHub Proxy \u21a9\u21a9</p> </li> <li> <p>Harbor - Container/OCI Registry \u21a9</p> </li> <li> <p>Sonatype Nexus - Binary Artifact Repository \u21a9</p> </li> <li> <p>OpenLDAP \u21a9</p> </li> <li> <p>Keycloak - Open Source Identity and Access Management \u21a9</p> </li> <li> <p>Prometheus - Open Source timeseries database \u21a9</p> </li> <li> <p>Thanos - Open Source, HA Prometheus setup \u21a9</p> </li> <li> <p>Grafana - Open Source monitoring graphics \u21a9</p> </li> <li> <p>SonarQube - self-manage static code analysis tool \u21a9</p> </li> <li> <p>HashiCorp Vault - Secrets management tool \u21a9</p> </li> <li> <p>MinIO - Kubernetes native storage solution \u21a9</p> </li> <li> <p>FluxCD Bootstrap for GitHub \u21a9</p> </li> <li> <p>GitLab - Git Server (and CI/CD platform) \u21a9</p> </li> <li> <p>HashiCorp Vault - Using Vault to manage PKI infra in Kubernetes \u21a9\u21a9</p> </li> </ol>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/","title":"TAP Build","text":"<p>For large-scale deployments of TAP, we recommend separating the Build and Test phases of the supply chain into a separate Build cluster.</p> <p>TAP supports this via the build profile, only installing the components related to these activities <sup>1</sup>.</p> <p>The components installed, among others, are Cartographer, Tekton, Tanzu Build Service, and Grype.</p> <p>This chapter focuses on creating a GitOps install of the Build profile and configuring TAP to integrate with our tools of choice.</p> <p>We will take a look at the following:</p> <ol> <li>Install Tanzu Build Service and its dependencies via GitOps</li> <li>Configure Build Profile</li> <li>Add ServiceAccount for the View profile</li> <li>Manage Workloads</li> </ol> <p>Warning</p> <p>Before diving into the Build profile topics, the GitOps Prep page is considered a pre-requisite.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#install-tbs","title":"Install TBS","text":"<p>Unsurprisingly, with such a name, one of the significant components of the Build profile is the component that does the building: Tanzu Build Service (TBS).</p> <p>Unfortunately, TBS can be unwieldy. It requires a good chunk of storage for ContainerD (~100GB), downloads a lot of (container) images, and relies on DockerHub images.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#relocate-tbs-dependencies","title":"Relocate TBS Dependencies","text":"<p>For these reasons, we recommend to always relocate the TBS dependencies to a registry you control, preferably close (network wise) to your clusters<sup>4</sup>.</p> <p>The docs are very straightforward, so I won't repeat it here; please follow them and return <sup>4</sup>.</p> <p>TAP TBS, not TBS</p> <p>One thing that has bitten some customers is that they \"know\" TBS.</p> <p>So what they do is they download the TBS product and its dependencies.</p> <p>Unfortunately, what TAP relies on isn't the same packaging.</p> <p>Please make sure you relocate <code>tanzu-application-platform/full-tbs-deps-package-repo</code>!</p> <p>If you need help determining which version of the TBS dependencies you need, you can verify this with a TAP Package Repository.</p> <p>Please refer to the Retrieving Package Schemas section in the TAP GitOps Prep page if you are unsure how to check what versions are in a Package Repository quickly.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#tbs-and-tap-gitops-install","title":"TBS and TAP GitOps Install","text":"<p>For our installation, we want to install everything via GitOps.</p> <p>By default, TAP installs TBS in online mode. We run into a slight issue when we point our TAP install to our relocated packages instead.</p> <p>We also need the TAP TBS Full Dependency Package Repository and the ***Package** installed. This is a different Package Repository, so it is not included in the TAP install.</p> <p>This is where community members like vrabbi come in<sup>2</sup>. He has written on how to handle this, and has an example repository<sup>3</sup></p> <p>Info</p> <p>vrabbi has a lot of good blog posts related to TAP.</p> <p>I recommend visiting his blog whenever a new version of TAP is released; he usually does a breakdown of what changed.</p> <p>The solution comes down to the following:</p> <ol> <li>Add additional properties to our custom Schema file (<code>cluster-config/config/custom/00-custom-schema.yaml</code>)</li> <li>Add manifest for the Package Repository (Carvel K8S CR)</li> <li>Add manifest for the Package Installation (Carvel K8S CR)</li> </ol> <p>Let's get to it.</p> <p>In the existing custom schema file, add the following:</p> cluster-config/config/custom/00-custom-schema.yaml<pre><code>custom:\n  tbs_full_dependencies:\n    enabled: true\n    pkgr_version: \"1.10.10\" #! matches TAP 1.5.4 I believe\n    pkgr_repo_url: harbor.services.my-domain.com/buildservice/tbs-full-deps\n</code></pre> <p>Then, we create a <code>tbs-install</code> folder parallel to the <code>tap-install</code> folder in <code>cluster-config/config</code>. We create the files <code>00-pkgr.yaml</code> and <code>01-pkgi.yaml</code> for the Package Repository and Package Install, respectively.</p> <p>The folder structure should look like this now (limited to relevant files/folders):</p> <pre><code>build-01\n\u251c\u2500\u2500 cluster-config\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 custom\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 00-custom-schema.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 01-shared.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 02-flux-sync.yaml\n\u2502   \u2502   \u251c\u2500\u2500 tap-install\n\u2502   \u2502   \u2514\u2500\u2500 tbs-install\n\u2502   \u2502       \u251c\u2500\u2500 00-pkgr.yaml\n\u2502   \u2502       \u2514\u2500\u2500 01-pkgi.yaml\n\u2502   \u2514\u2500\u2500 values\n\u251c\u2500\u2500 flux\n\u251c\u2500\u2500 ns-provisioner\n\u2514\u2500\u2500 tanzu-sync\n</code></pre> <p>And for the content:</p> cluster-config/config/tbs-install/00-pkgr.yaml<pre><code>#@ load(\"@ytt:data\", \"data\")\n---\n#@ if data.values.custom.tbs_full_dependencies.enabled:\napiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageRepository\nmetadata:\n  name: tbs-full-deps-repository\n  namespace: tap-install\n  annotations:\n    kapp.k14s.io/change-group: pkgr\nspec:\n  fetch:\n    imgpkgBundle:\n      image: #@ \"{}:{}\".format(data.values.custom.tbs_full_dependencies.pkgr_repo_url,data.values.custom.tbs_full_dependencies.pkgr_version)\n#@ end\n</code></pre> <p>The Package Install:</p> cluster-config/config/tbs-install/01-pkgi.yaml<pre><code>#@ load(\"@ytt:data\", \"data\")\n---\n#@ if data.values.custom.tbs_full_dependencies.enabled:\napiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageInstall\nmetadata:\n  name: full-tbs-deps\n  namespace: tap-install\n  annotations:\n    kapp.k14s.io/change-group: tbs\n    kapp.k14s.io/change-rule.0: \"upsert after upserting pkgi\"\n    kapp.k14s.io/change-rule.1: \"delete before deleting pkgi\"\nspec:\n  serviceAccountName: tap-installer-sa\n  packageRef:\n    refName: full-tbs-deps.tanzu.vmware.com\n    versionSelection:\n      constraints: #@ data.values.custom.tbs_full_dependencies.pkgr_version\n#@ end\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#configure-build-profile","title":"Configure Build Profile","text":"<p>Now that we have taken care of our dependencies, we can look at the Build profile proper<sup>1</sup>.</p> <p>I'm preparing the profile to use the Out Of The Box Supply Chain Testing &amp; Scanning<sup>5</sup>.</p> <p>Let's look at what sections we need to provide:</p> <ol> <li>Profile generics, <code>profile</code>, <code>shared</code>, <code>ceip_policy_disclosed</code>, and <code>contour</code></li> <li>BuildService configuration</li> <li>Supply Chain configuration (<code>ootb_supply_chain_testing_scanning</code>)</li> <li><code>scanning.metadataStore</code> for legacy reasons</li> </ol> <p>We split the profile values into sensitive and non-sensitive.</p> <p>Let's start with the non-sensitive values.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#non-sensitive","title":"Non-Sensitive","text":"","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#shared","title":"Shared","text":"<p>I recommend configuring the TAP Install Registry as sensitive, so that leaves us with two properties for the <code>shared</code>: The <code>shared.ingress_domain</code> is the DNS \"wildcard\" for this profile/cluster, and, if required, a Custom CA cert via <code>shared.ca_cert_data</code>.</p> <pre><code>ingress_domain: build.my-domain.com\nca_cert_data: |-\n  ...\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#build-service","title":"Build Service","text":"<p>The build service, by default, leverages the registry from the <code>shared.image_registry</code>. As we've relocated the packages, that is a different registry.</p> <p>We also have to tell it not to install the dependencies and instead pull them from our defined TBS registry (<code>kp_default_repository</code>).</p> <p>We that as follows:</p> <pre><code>buildservice:\n  pull_from_kp_default_repo: true\n  exclude_dependencies: true\n  #! registry for TBS Dependencies\n  kp_default_repository: \"harbor.services.my-domain.com/buildservice/tbs-full-deps\"\n</code></pre> <p>It still does leverage the credentials used for the <code>shared.image_registry</code>. See the sensitive values section below if you need different credentials.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#ootb-supply-chain","title":"OOTB Supply Chain","text":"<p>First, we declare which supply chain we use on the top level (e.g., <code>tap_install.values.</code>). We do so by setting <code>supply_chain</code> to our desired Supply Chain, in my case, <code>testing_scanning</code>.</p> <p>As before, we want to do everything as GitOps.</p> <p>TAP supports two Workload flows<sup>7</sup>:</p> <ol> <li>Registry Ops: Build, Test, and end with a Cartographer <code>Deliverable</code> CR, which a Run profile can install</li> <li>GitOps: Build, Test, and end with a PullRequest(PR) (or Merge Request for GitLab) containing the Knative Service CR</li> </ol> <p>We choose the GitOps flow. This means we need to tell it several things, such as the server, repository, branch, and some information for the PR.</p> <p>Last but not least, we inform it of the repository for storing the container images made by TBS (or Kaniko).</p> <pre><code>supply_chain: testing_scanning\nootb_supply_chain_testing_scanning:\n  external_delivery: true\n  gitops:\n    server_address: https://gitlab.services.my-domain.com\n    repository_owner: root\n    repository_name: tap-apps\n    branch: main\n    commit_strategy: pull_request\n    pull_request:\n      server_kind: gitlab\n      commit_branch: \"\"\n      pull_request_title: ready for review\n      pull_request_body: generated by supply chain\n  registry:\n    server: harbor.services.my-domain.com\n    repository: tap-apps #! registry project for Workload images\n</code></pre> <p>Secret for the PullRequests</p> <p>By default, the PullRequest (or MergeRequest) is made by a Tekton Pipeline.</p> <p>Tekton requires a specific kind of Secret format as described in the Tanzu Docs<sup>7</sup> and Tekton Docs<sup>8</sup>.</p> <p>This secret needs to be assigned the <code>default</code> ServiceAccount in the namespace (this is the default, pun intended). As you might remember from our GitOps Prepare page, we create the appropriate secrets and use the Namespace Provisioner to add them to the ServiceAccount. This is why!</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#full-example","title":"Full Example","text":"Full Profile Example platforms/clusters/build-01/cluster-config/values/tap-non-sensitive-values.yaml<pre><code>---\ntap_install:\n  values:\n    profile: build\n    shared:\n      ingress_domain: build.my-domain.com\n      ca_cert_data: |- #! if you need a custom support a custom Certificate Authority (CA)\n        -----BEGIN CERTIFICATE-----\n        iUdqs7FZN2uKkLKekdTgW0QkTFEJTk5Yk9t/hOrjnHoWQfB+mLhO3vPhip\n        ...\n        vhs=\n        -----END CERTIFICATE-----\n    buildservice:\n      pull_from_kp_default_repo: true\n      exclude_dependencies: true\n      #! registry for TBS Dependencies\n      kp_default_repository: \"harbor.services.my-domain.com/buildservice/tbs-full-deps\"\n\n    supply_chain: testing_scanning\n    ootb_supply_chain_testing_scanning:\n      external_delivery: true\n      gitops:\n        server_address: https://gitlab.services.my-domain.com\n        repository_owner: root\n        repository_name: tap-apps\n        branch: main\n        commit_strategy: pull_request\n        pull_request:\n          server_kind: gitlab\n          commit_branch: \"\"\n          pull_request_title: ready for review\n          pull_request_body: generated by supply chain\n      registry:\n        server: harbor.services.my-domain.com\n        repository: tap-apps #! registry project for Workload images\n\n    scanning:\n      metadataStore:\n        url: \"\" #! a bug requires this setting for TAP 1.4 and 1.5 (not sure about 1.6)\n\n    ceip_policy_disclosed: true\n    contour:\n      envoy:\n        service:\n          type: LoadBalancer\n\n    #! this is from the GitOps Preparation page\n    namespace_provisioner:\n      controller: false\n      gitops_install:\n        ref: origin/main\n        subPath: platforms/clusters/build-01/ns-provisioner/install\n        url: git@github.com:joostvdg/tap-gitops.git\n        secretRef:\n          name: github-ssh\n          namespace: shared\n          create_export: false\n      additional_sources:\n      - git:\n          ref: origin/main\n          subPath: platforms/clusters/build-01/ns-provisioner/additional-sources\n          url: git@github.com:joostvdg/tap-gitops.git\n          # secretRef section is only needed if connecting to a Private Git repo\n          secretRef:\n            name: github-ssh-1\n            namespace: shared\n            create_export: false\n        path: _ytt_lib/testing-scanning-supplychain-setup\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#sensitive-values","title":"Sensitive Values","text":"<p>Below are the sensitive values.</p> <p>The <code>shared.image_registry</code> with the URL, username, and password.</p> <p>Mind you, the example below is before encryption with SOPS (or ESO); encrypt it before placing it at that location.</p> platforms/clusters/build-01/cluster-config/values/tap-sensitive-values.sops.yaml<pre><code>tap_install:\n    sensitive_values:\n        shared:\n            #! registry for the TAP installation packages\n            image_registry:\n                project_path: harbor.services.mydomain.com/tap/tap-packages\n                username: #! username\n                password: #! password or PAT\ncustom:\n  sensitive_values:\n    github:\n      ssh:\n        private_key: |\n          ...\n        known_hosts: |\n          ...\n</code></pre> <p>Same credentials for TAP and TBS</p> <p>Mind you, the example below is before encryption with SOPS (or ESO); encrypt it before placing it at that location.</p> platforms/clusters/build-01/cluster-config/values/tap-sensitive-values.sops.yaml<pre><code>tap_install:\n  sensitive_values:\n    buildservice:\n      kp_default_repository: #! registry, e.g.,  \"index.docker.io/joostvdgtanzu/build-service\"\n      kp_default_repository_username: #! username\n      kp_default_repository_password: #! password or PAT\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#add-view-serviceaccount","title":"Add View ServiceAccount","text":"<p>One of the essential features of TAP is its GUI.</p> <p>For the TAP GUI to show the Supply Chains, one of its core features, it needs access to the Kubernetes cluster hosting them.</p> <p>In this scenario, we separate the Build cluster from the cluster the TAP GUI runs in (View cluster). So, we must provide the TAP GUI with alternative means to view the Supply Chain resources.</p> <p>We do that by creating a ServiceAccount in each cluster we want the TAP GUI to have visibility<sup>9</sup>. Then, copy the token of that ServiceAccount into the View profile configuration.</p> <p>Add this ServiceAccount with the required permission and a Token to our additional Kubernetes resources.</p> <p>Let's place this in the folder <code>cluster-config/config/custom</code>, and call the file <code>03-tap-gui-service-account.yaml</code>.</p> TAP GUI Viewer Service Account platforms/clusters/build-01/cluster-config/config/custom/03-tap-gui-service-account.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: tap-gui\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  namespace: tap-gui\n  name: tap-gui-viewer\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: tap-gui-viewer\n  namespace: tap-gui\n  annotations:\n    kubernetes.io/service-account.name: tap-gui-viewer\ntype: kubernetes.io/service-account-token\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: tap-gui-read-k8s\nsubjects:\n- kind: ServiceAccount\n  namespace: tap-gui\n  name: tap-gui-viewer\nroleRef:\n  kind: ClusterRole\n  name: k8s-reader\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: k8s-reader\nrules:\n- apiGroups: ['']\n  resources: ['pods', 'pods/log', 'services', 'configmaps', 'limitranges']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['metrics.k8s.io']\n  resources: ['pods']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['apps']\n  resources: ['deployments', 'replicasets', 'statefulsets', 'daemonsets']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['autoscaling']\n  resources: ['horizontalpodautoscalers']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['networking.k8s.io']\n  resources: ['ingresses']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['networking.internal.knative.dev']\n  resources: ['serverlessservices']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: [ 'autoscaling.internal.knative.dev' ]\n  resources: [ 'podautoscalers' ]\n  verbs: [ 'get', 'watch', 'list' ]\n- apiGroups: ['serving.knative.dev']\n  resources:\n  - configurations\n  - revisions\n  - routes\n  - services\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['carto.run']\n  resources:\n  - clusterconfigtemplates\n  - clusterdeliveries\n  - clusterdeploymenttemplates\n  - clusterimagetemplates\n  - clusterruntemplates\n  - clustersourcetemplates\n  - clustersupplychains\n  - clustertemplates\n  - deliverables\n  - runnables\n  - workloads\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['source.toolkit.fluxcd.io']\n  resources:\n  - gitrepositories\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['source.apps.tanzu.vmware.com']\n  resources:\n  - imagerepositories\n  - mavenartifacts\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['conventions.apps.tanzu.vmware.com']\n  resources:\n  - podintents\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['kpack.io']\n  resources:\n  - images\n  - builds\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['scanning.apps.tanzu.vmware.com']\n  resources:\n  - sourcescans\n  - imagescans\n  - scanpolicies\n  - scantemplates\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['tekton.dev']\n  resources:\n  - taskruns\n  - pipelineruns\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['kappctrl.k14s.io']\n  resources:\n  - apps\n  verbs: ['get', 'watch', 'list']\n- apiGroups: [ 'batch' ]\n  resources: [ 'jobs', 'cronjobs' ]\n  verbs: [ 'get', 'watch', 'list' ]\n- apiGroups: ['conventions.carto.run']\n  resources:\n  - podintents\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['appliveview.apps.tanzu.vmware.com']\n  resources:\n  - resourceinspectiongrants\n  verbs: ['get', 'watch', 'list', 'create']\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#add-workload-specific-resources","title":"Add Workload Specific Resources","text":"<p>The main question to ask: how do you separate Workloads and their specific resources?</p> <p>In TAP, a likely answer is to separate Workloads in TAP's Developer Namespaces.</p> <p>Sure, you might have a Team with more than one similar kind of application (or at least a repository). Beyond that, it makes sense to separate them via Kubernetes Namespaces.</p> <p>So, what resources are we talking about?</p> <ol> <li>The <code>Workload</code> definition, as a starter</li> <li>The Tekton test Pipeline, see next section on why</li> <li>Any other possible resource, such as a Secret, ConfigMap, or what have you</li> </ol> <p>Unfortunately, where we are in the GitOps install, we hit a bit of a snag:</p> <ul> <li>We cannot add them to the Tanzu Sync folder(<code>cluster-config/config</code>), as the Namespace Provisioner manages the namespaces (i.e., they aren't guaranteed to exist yet, blocking reconciliation)</li> <li>We cannot add them to the Namespace Provisioner, as we need different ones per Namespace</li> </ul> <p>While technically, we could add them to the Namespace Provisioner's <code>namespaces.yaml</code> file, I believe it is the wrong place to do so.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#possible-solution","title":"Possible Solution","text":"<p>In the GitOps preparation page, we explored how we can add additional FluxCD controllers and resources. We should leverage those to move the responsibility to synchronize these resources outside TAP.</p> <p>This also gives us more flexibility in terms of which (Git) repositories to use:</p> <ol> <li>Create a Namespace called <code>apps</code></li> <li>Create a <code>GitRepository</code> in there, pointing to a Git repository containing the teams/applications and their resources per environment</li> <li>Add a <code>Kustomization</code> to instruct FluxCD which resources to synchronize (from that GitRepository)</li> <li>We can then freely add any other FluxCD resources we want to synchronize in that Git repository</li> </ol> <p>How do we do that?</p> <p>In my case, we create a Source, a <code>GitRepository</code>. This ensures FluxCD keeps an up-to-date clone of that repository in the cluster.</p> GitRepository <p><code>yaml title=\"platforms/clusters/build-01/cluster-config/config/flux-sources/apps-git-repository.yaml apiVersion: source.toolkit.fluxcd.io/v1beta2 kind: GitRepository metadata:   name: apps   namespace: apps   annotations:     kapp.k14s.io/change-rule: upsert after upserting GitRepository spec:   gitImplementation: go-git   ignore: |     !.git   interval: 1m0s   ref:     branch: main   secretRef:     name: apps-gitlab   url: https://gitlab.services.my-domain.com/root/tap-apps.git</code></p> <p>And then we can choose, do we want to manage Workload specific <code>Kustomization</code>'s directly in the same Git repository, or do we want some indirection.</p> <p>I create an abstraction layer by having my Workload specific <code>Kustomization</code> in a directory in the Git repository we synchronize.</p> <p>The <code>Kustomization</code> we include in the TAP install repository points to that directory instead.</p> Kustomization <pre><code>apiVersion: kustomize.toolkit.fluxcd.io/v1beta2\nkind: Kustomization\nmetadata:\n  name: apps\n  namespace: apps\nspec:\n  interval: 5m0s\n  path: ./tap/apps/build\n  prune: true\n  targetNamespace: apps\n  sourceRef:\n    kind: GitRepository\n    name: apps\n</code></pre> <p>TODO: describe folder structure/Git repository structure     * add diagram</p> <p>For example, my <code>tap-apps</code> Git repository looks as follows:</p> <pre><code>.\n\u251c\u2500\u2500 tap\n\u2502   \u2514\u2500\u2500 apps\n\u2502       \u251c\u2500\u2500 build\n\u2502       \u2502   \u2514\u2500\u2500 team-orange-kustomization.yaml\n\u2502       \u251c\u2500\u2500 run-01\n\u2502       \u2514\u2500\u2500 run-02\n\u2514\u2500\u2500 teams\n    \u2514\u2500\u2500 orange\n        \u251c\u2500\u2500 backstage\n        \u2502   \u251c\u2500\u2500 team.yaml\n        \u2502   \u2514\u2500\u2500 user-example.yaml\n        \u251c\u2500\u2500 build\n        \u2502   \u251c\u2500\u2500 namespace-setup.yaml\n        \u2502   \u251c\u2500\u2500 tap-demo-03\n        \u2502   \u2502   \u2514\u2500\u2500 workload.yaml\n        \u2502   \u251c\u2500\u2500 tap-demo-04\n        \u2502   \u2502   \u2514\u2500\u2500 workload.yaml\n        \u2502   \u2514\u2500\u2500 tekton\n        \u2502       \u2514\u2500\u2500 java-test-pipeline.yaml\n        \u251c\u2500\u2500 prod\n        \u2502   \u2514\u2500\u2500 tap-demo-04\n        \u2502       \u2514\u2500\u2500 delivery.yml\n        \u2514\u2500\u2500 staging\n            \u251c\u2500\u2500 tap-demo-03\n            \u2502   \u2514\u2500\u2500 tap-demo-03.yaml\n            \u2514\u2500\u2500 tap-demo-04\n                \u2514\u2500\u2500 delivery.yml\n</code></pre> <p>The <code>Kustomization</code> in the <code>tap-gitops</code> repository points to the <code>tap/apps/build</code> folder, synchronizing all applicable files to the cluster.</p> <p>As you can see, one of those files is <code>team-orange-kustomization.yaml</code>.</p> <p>This file points to <code>teams/orange/build/</code> and synchronizes all the <code>Workload</code> manifests we need. It also includes a file <code>namespace-setup.yaml</code> to create the namespace and any secret we need.</p> <p>A folder named <code>tekton</code> includes any team-specific Tekton resource, such as its Pipeline for testing.</p> <p>In my case, the production and staging resources for the <code>run-01</code> and <code>run-02</code> clusters are in the same repository. But that is mainly because this is a compacted example; you might separate those into distinct Git repositories instead.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#add-tekton-pipeline","title":"Add Tekton Pipeline","text":"<p>Speaking of Tekton pipelines, there is a reason we included the Pipeline for testing in this example.</p> <p>To the surprise of many, the Testing &amp; Scanning OOTB Supply Chain still needs to be completed. It misses the Test Pipeline, which is expected to be a Tekton Pipeline with the Label <code>apps.tanzu.vmware.com/pipeline: test</code>.</p> <p>The Cartographer Supply Chains are written to be reusable across applications and tech stacks. A Tekton Pipeline is not, which is one of the reasons Cartographer exists in the first place.</p> <p>So we recommend to:</p> <ol> <li>Leverage Tekton Tasks in the Tekton Pipelines</li> <li>Include the Tasks in the Namespace Provisioner so each Workload Namespace has all of them</li> <li>Separately synchronize the team or application-specific Tekton Pipeline for testing</li> </ol> <p>We've tackled steps two and three. The contents of the Pipeline is out of scope for this guide, see the TAP Supply Chain Customization section for more information on that.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#visualization","title":"Visualization","text":"<p>This might be a complex situation, hard to grasp with just words alone.</p> <p>Hopefully, we can make it easier to digest with a visualization:</p> <p></p> <p>Let's break down the interactions in the cluster with this setup.</p> <p>We start with two Git repositories: <code>tap-gitops</code> without TAP install configuration and <code>tap-apps</code>, with everything related to our Workloads.</p> <ol> <li>The FluxCD synchronization we set up in the GitOps preparation synchronizes the (FluxCD) <code>GitRepository</code> and Apps <code>Kustomization</code></li> <li>The <code>GitRepository</code> points to our <code>tap-apps</code> Git repository, making FluxCD do a Git clone</li> <li>The <code>Kustomization</code> in the <code>apps</code> Namespace lets FluxCD synchronize resources from a sub-path of a Git repository</li> <li>The <code>Kustomization</code> also points towards the <code>GitRepository,</code> so FluxCD knows which Git clone to use</li> <li>The <code>Kustomization</code> tells FluxCD to synchronize the <code>tap-apps/apps/build</code> folder and its subdirectories</li> <li>This includes, among other things, another <code>Kustomization</code>, for Team Orange</li> <li>This <code>Kustomization</code> is pointing to the same <code>GitRepository</code> (as I'm using a single Git repository)</li> <li>In this \"Team Orange\" <code>Kustomization</code>, we have the <code>Workload</code> (the Cartographer CR) and other resources, such as the Tekton Pipeline for testing</li> </ol>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#install","title":"Install","text":"<p>You are now ready to install the TAP Build profile.</p> <p>It has been a long journey, but each subsequent cluster and profile is similar. </p> <p>For the actual install commands, I refer to the docs <sup>10</sup>.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-build/#references","title":"References","text":"<ol> <li> <p>TAP Install 1.5 - Build profile \u21a9\u21a9</p> </li> <li> <p>VRABBI - Blogs on VMware/Tanzu products \u21a9</p> </li> <li> <p>VRABBI -  \u21a9</p> </li> <li> <p>TAP - Offline Install - Tanzu Build Service &amp; Dependencies \u21a9\u21a9</p> </li> <li> <p>TAP - OOTB Supply Chain - Testing &amp; Scanning \u21a9</p> </li> <li> <p>TAP - OOTB Supply Chain - Tekton Test Pipeline Example \u21a9</p> </li> <li> <p>TAP - OOTB Supply Chain - Registry Ops vs. GitOps \u21a9\u21a9</p> </li> <li> <p>Tekton - Git server secrets \u21a9</p> </li> <li> <p>TAP GUI - View resources on multiple clusters \u21a9</p> </li> <li> <p>TAP 1.5 Install - GitOps Install with SOPS deploy \u21a9</p> </li> </ol>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-gitops-prep/","title":"TAP GitOps Install Prep","text":"<p>According the the documentation, these are the key components of implementing GitOps<sup>2</sup>:</p> <p>Git as the single source of truth: The desired state is stored in a Git repository. To change the cluster state, you must change it in the Git repository instead of modifying it directly on the cluster.</p> <p>Declarative configuration: GitOps follows a declarative approach, where the desired state is defined in the declarative configuration files.</p> <p>Pull-based synchronization: GitOps follows a pull-based model. Kubernetes cluster periodically pulls the desired state from the Git repository. This approach ensures that the cluster is always in sync with the desired configuration.</p> <p>In this chapter we're going to go over all the things we need before we can install TAP via GitOps principles, using the TAP GitOps Reference Implementation <sup>2</sup>.</p> <p>TAP Package Relocation</p> <p>Wether you want to do an offline install, or reduce the reliance on external systems or any other reason, you probably want to relocate the TAP packages and the associated container images.</p> <p>In this guide we focus on the GitOps installation, so please refer to the documentation on the steps required<sup>1</sup>.</p> <p>Or go directly to the GitOps SOPS install guide's section on relocating images, to start with the relocation <sup>3</sup>.</p> <p>We'll take a look at the following:</p> <ul> <li>Retrieving the reference implementation scripts</li> <li>Initiate a profile install folder</li> <li>Setup SOPS</li> <li>Other generic steps to repeat for each Profile</li> </ul>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-gitops-prep/#retrieve-reference-implementation-scripts","title":"Retrieve Reference Implementation Scripts","text":"<p>The TAP resources on Tanzu Network contain the scripts for the TAP GitOps installation<sup>4</sup>.</p> <p>First, ensure you are logged in (as you also need to accept the EULA's). Then, select the appropriate release from the dropdown, for example <code>Release 1.5.6</code> as shown belown.</p> <p></p> <p>One of the items in the list, is the <code>Tanzu GitOps Reference Implementation</code>, download this archive and extract it to a useful folder<sup>5</sup>. You need these for configuring your installation folder for bootstrapping.</p> <p>Hint for downloading</p> <p>It isn't always clear, hovering over an item shows a little cloud icon.</p> <p>Clicking on this icon starts the download.</p> <p>If the download does not start, verify you are logged in!</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-gitops-prep/#initiate-a-profile-install-folder","title":"Initiate a profile install folder","text":"<p>The repository preperation script is <code>setup-repo.sh</code>.</p> <p>It takes two arguments, the cluster name and the tool for managing secrets.</p> <p>In my case, those are <code>build-01</code> and <code>sops</code> respectively:</p> <pre><code>./setup-repo.sh build-01 sops\n</code></pre> <p>As I want to structure my folders slightly differently from the default (e.g., adding the <code>platforms</code> folder), I move the end result into my desired Git repository.</p> <p>Folderwise, it now looks like this:</p> <pre><code>platforms\n\u2514\u2500\u2500 clusters\n    \u251c\u2500\u2500 build-01\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cluster-config\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 values\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 tanzu-sync\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 app\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 values\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 bootstrap\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 scripts\n    \u251c\u2500\u2500 other clusters ...\n</code></pre> <p>Where <code>other cluster ...</code> is where other cluster's folders will be placed.</p> <p>As you can see, we have two main folders, <code>cluster-config</code> and <code>tanzu-sync</code>.</p> <p>Each has a distinct role to play.</p> <ul> <li>cluster-config: contains the configuration of what goes into the cluster.<ul> <li>cluster-config/config: folder is synchronized into the cluster directly</li> <li>cluster-config/values: folder is used to configure the TAP installation. And contains similar content as the <code>tap-values.yaml</code> used with a traditional installation (i.e., <code>tanzu package available install ...</code>)</li> </ul> </li> <li>tanzu-sync: contains the configuration for managing the \"GitOps\" configuration.<ul> <li>tanzu-sync/scripts: the scripts to prepare the configuration</li> <li>tanzu-sync/app: is a Carvel App, with a <code>config</code> directory containing the templates, and a <code>values</code> folder used to configure these templates. This file gets generated by the scripts.</li> </ul> </li> </ul> <p>As we go along, we update the repository and get it ready for installing a TAP profile.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-gitops-prep/#setup-sops","title":"Setup SOPS","text":"<p>Some TAP installation values are secrets.</p> <p>If we're going to store them in a Git repository, we need to encrypt them first.</p> <p>For this, we use SOPS with Age, as discussed in the introduction. Let's setup the sensitive values<sup>6</sup>.</p> <p>We start with setting the environment variables, including the name of the cluster (the folder name).</p> <pre><code>export SOPS_AGE_RECIPIENTS=$(cat key.txt | grep \"# public key: \" | sed 's/# public key: //')\nexport TAP_CLUSTER=build-01\n</code></pre> <p>I suggest using a \"staging\" folder with these unencrypted files that you do not commit.</p> <pre><code>touch \"${TAP_CLUSTER}-tap-sensitive-values.yaml\"\n</code></pre> <p>Use a .gitignore</p> <p>To ensure you do not commit this staging folder, it is recommended to add it to a <code>.gitignore</code> file.</p> <p>The initial contents of the file are as follows:</p> tap-sensitive-values.sops.yaml<pre><code>---\ntap_install:\n  sensitive_values:\n</code></pre> <p>We then encrypt this using SOPS:</p> <pre><code>sops --encrypt --in-place \"${TAP_CLUSTER}-tap-sensitive-values.yaml\" \\\n   &gt; \"${TAP_CLUSTER}-tap-sensitive-values.sops.yaml\"\n</code></pre> <p>Followed by moving the file to the appropriate folder in the GitOps repository:</p> <pre><code>mv \"${TAP_CLUSTER}-tap-sensitive-values.sops.yaml\" \\\n  \"platforms/clusters/${TAP_CLUSTER}/cluster-config/values/tap-sensitive-values.sops.yaml\"\n</code></pre> <p>Update, Encrypt, Move everytime</p> <p>When you need to update or add a sensitive value, we need to do this set of steps again.</p> <p>Yes, unfortunately, we need to do the edit, encrypt, and move everytime we need to change a value.</p> <p>These files are unique per cluster, so we end up with a bunch of these files. Hence, we add the cluster name to the file name in the staging folder.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-gitops-prep/#generic-steps-for-each-profile","title":"Generic steps for each Profile","text":"<p>Beyond the sensitive values file that we encrypt, there are a few more steps we repeat for each cluster:</p> <ol> <li>A non-sensitive values file, for all the other TAP installation values</li> <li>Additional Kubernetes resources that we want in the cluster, that are tied to the TAP install</li> <li>Namespace Provisioner configuration<ul> <li>including the main sources</li> <li>and the additional source</li> </ul> </li> <li>Share Secrets (Optional)<ul> <li>extending the Additional Kubernetes resources</li> </ul> </li> <li>FluxCD resources (optional)</li> </ol> <p>Let's look at each.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-gitops-prep/#non-sensitive-values-file","title":"Non-Sensitive Values File","text":"<p>The folder <code>cluster-config/values/</code> is read in its entirety.</p> <p>So any file in there, with content that is valid for the TAP installation values schema is read and used.</p> <p>Ah yes, the schema files. There is one for all values, and one specifically for the sensitive values we addressed earlier.</p> <ul> <li><code>cluster-config/config/tap-install/.tanzu-managed/schema.yaml</code></li> <li><code>cluster-config/config/tap-install/.tanzu-managed/schema--tap-sensitive-values.yaml</code></li> </ul> <p>If you're not clear on YTT schema files, take a look at the documentation on how to write a schema<sup>7</sup>.</p> <p>All the installation files, schema files, and your values are given to YTT to process the resulting YAML files that get applied to the cluster (by KAPP Controller). If you're curious as to how this process goes, you can take a look at the YTT docs<sup>8</sup>, or the KAPP Controller docs<sup>9</sup>.</p> <p>For the values that go into the non-sensitive values file, you have two main sources of information.</p> <ol> <li>The TAP installation documentation, having an example for each profile<sup>10</sup></li> <li>You can request the values schema from the package, when made available via a package repository</li> </ol> <p>The second is a bit difficult to do if you are using the GitOps installation, as you are creating the profile before the package repository is installed.</p> <p>A possible  solution here, is to install the TAP package repository in temporary namespace and use it to query the packages.</p> <p>Retrieving Package Schemas</p> <p>Because this is not always clear to people how to do this, let's explore the steps.</p> <p>First, you need to have a Package Repository installed in your cluster.</p> <p>For this specific purpose, I recommend making a (Kubernetes) namespace dedicated for this purpose.</p> <pre><code>kubectl create namespace tap-package-repository-1-5-6\n</code></pre> <p>Then install the package repository for the TAP version you want to explore:</p> <pre><code>tanzu package repository add tanzu-tap-repository-1-5-6 \\\n  --url registry.tanzu.vmware.com/tanzu-application-platform/tap-packages:1.5.6 \\\n  --namespace tap-package-repository-1-5-6\n</code></pre> <p>As I'm not going to install the packages from there, I'm using the direct URL of the Tanzu Registry. If you cannot use that due environment restrictions, replace it with your internal URL (requires relocation).</p> <p>Verify the package repository is successfully reconciled:</p> <pre><code>tanzu package repository get tanzu-tap-repository-1-5-6 \\\n  --namespace tap-package-repository-1-5-6\n</code></pre> <p>If successful, the response looks like this:</p> <pre><code>NAME:          tanzu-tap-repository-1-5-6\nVERSION:       36742796\nREPOSITORY:    registry.tanzu.vmware.com/tanzu-application-platform/tap-packages\nTAG:           1.5.6\nSTATUS:        Reconcile succeeded\nREASON:\n</code></pre> <p>When can now list the packages made available in the namespace:</p> <pre><code>tanzu package available list -n tap-package-repository-1-5-6\n</code></pre> <p>Which should give you a list of all the packages like this (edited to fit this page):</p> <pre><code>NAME                              DISPLAY-NAME               SHORT-DESCRIPTION   LATEST-VERSION\naccelerator.apps.tanzu.vmware.com Application Accelerator..  Used to create...   1.5.3\napi-portal.tanzu.vmware.com       API portal                 A unified user...   1.3.9\n...\ntap.tanzu.vmware.com              Tanzu Application Platform Package to inst..   1.5.6\n</code></pre> <p>We can then use the combination of the package name and version to retrieve its schema.</p> <p>For example, for the main TAP install, we use <code>tap.tanzu.vmware.com</code> and <code>1.5.6</code>:</p> <pre><code>tanzu package available -n tap-package-repository-1-5-6\\\n  get tap.tanzu.vmware.com/1.5.6\\\n  --values-schema\n</code></pre> <p>Which returns to us a decent portion of all the values we can set (abbreviated to fit the page):</p> <pre><code>KEY                       DEFAULT    TYPE    DESCRIPTION\nnamespace_provisioner     &lt;nil&gt;      object  Namespace Provisioner configuration\nservices_toolkit          &lt;nil&gt;      object  Services Toolkit configuration\n</code></pre> <p>I say portion, because not all values of every package installed by TAP is returned to us.</p> <p>If you want to know each package's total set of possible values, you need to query them individually.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-gitops-prep/#additional-kubernetes-resources","title":"Additional Kubernetes Resources","text":"<p>There are likely other Kubernetes resources you want to apply to the cluster, that are related to the TAP installation but not covered by it.</p> <p>I recommend seperating these into three categories:</p> <ol> <li>Resources that are required for each TAP workload</li> <li>Resources that need to exist only once, and are strongly tied to TAP</li> <li>Resources that need to exist only once, and are loosely tied to TAP</li> </ol> <p>The first category are resources you want to cover with the Namespace Provisioner, addressed in the next section.</p> <p>The second and third, are more complicated.</p> <p>You might be wondering what the complications are. Let's take a look:</p> <ul> <li>TAP is synchronized with KAPP Controller, this is not a full-featured GitOps solution<ul> <li>the main problem is its limited options for ordering and ability to convey dependencies</li> </ul> </li> <li>TAP includes the base installation of FluxCD, but not its Kustomize or Helm controllers</li> <li>Some of the resources require secrets in the cluster</li> <li>Some of the resources use CRDs that are lazily installed during the TAP install</li> </ul> <p>Hopefully that paints a sufficient picture of the situation at hand.</p> <p>We have some choices to make on how to proceed.</p> <p>My goal is to have the TAP installation to be the initiate of everything. That does not mean it needs to handle everything by itself.</p> <p>To be more precise, I will include the Helm and Kustomize controllers for FluxCD.</p> <p>First, as a generic step, we add a custom (YTT) Schema file.</p> <p>We create it in a subfolder of <code>cluster-config/config</code>, so that it is included in the initial Tanzu Sync synchronization.</p> <p>Let's create it as <code>custom/00-custom-schema.yaml</code>.</p> <p>In this, we create the most basic schema file we can:</p> cluster-config/config/custom/00-custom-schema.yaml<pre><code>#@data/values-schema\n#@overlay/match-child-defaults missing_ok=True\n---\ncustom:\n</code></pre> <p>Before we make use of this schema, let's first examine the Namespace Provisioner.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-gitops-prep/#namespace-provisioner","title":"Namespace Provisioner","text":"<p>The Namespace Provisioner has two modes.</p> <ol> <li>Controller Mode</li> <li>GitOps Mode<sup>18</sup></li> </ol> <p>The Controller mode is the continuation of how TAP provisions namespaces (for TAP Workloads) so far. Meaning, it watches for a Label on the Namespace, and applies a set of resources to that Namespace.</p> <p>The GitOps mode gives us the option to point it to a Git repository for its configuration.</p> <p>As the goal of this guide is to do everything through a singular installation via GitOps, we'll take this route.</p> <p>To change from Controller mode (the default) to GitOps mode, we disable the controller<sup>11</sup>:</p> <pre><code>namespace_provisioner:\n  controller: false\n</code></pre> <p>The Namespace Provisioner splits its configuration into two:</p> <ol> <li>The main sources, which is how the provisioner creates the namespace</li> <li>Additional sources, which are, as the name implies, additional Kubernetes resources it applies to each namespace it creates</li> </ol> <p>TAP 1.5 secrets issue</p> <p>A headsup when using TAP 1.5 (and possibly later versions as well), there is a bug when using the additional sources.</p> <p>Assuming that you need to use a secret for the Git repository for both the sources and additional_sources, you need two secrets. One for each configuration item.</p> <p>Unfortunately, both resources result in a <code>SecretExport</code> and <code>SecretImport</code> resource with the name of secret in the corresponding namespaces. So when the secret has the same name, there is a conflict and the KAPP Controller stops the reconciliation.</p> <p>The solution is to ensure each entry has a unique secret.</p> <p>You can, for example, create a secret via the Kubernetes resources (see previous section) and create two secrets with the same input. And then let the Namespace Provisioner installation copy both to the correct namespace (it does so via the SecretGen Controller<sup>12</sup>).</p> <p>In terms of configuring the Namespace Provisioner, we get something like the following YAML file:</p> tap-non-sensitive-values.yaml<pre><code>namespace_provisioner:\n  controller: false\n  gitops_install:\n    ref: origin/main\n    subPath: platforms/clusters/build-01/ns-provisioner/install\n    url: git@github.com:joostvdg/tap-gitops-example.git\n    secretRef:\n      name: github-ssh\n      namespace: shared\n      create_export: false\n  additional_sources:\n  - git:\n      ref: origin/main\n      subPath: platforms/clusters/build-01/ns-provisioner/additional-sources\n      url: git@github.com:joostvdg/tap-gitops-example.git\n      secretRef:\n        name: github-ssh-1\n        namespace: shared\n        create_export: false\n    path: _ytt_lib/my-resources\n</code></pre> <p>The <code>gitops_install</code> refers to the main resources, and the <code>additional_sources</code> takes a list of Git configurations following the spec from the App CR of Carvel<sup>13</sup>. Which is the same as the <code>namespace_provisioner.gitops_install</code> property.</p> <p>In my case, I've put both of these sets of files in the same Git repository, in their respective subfolder. Which is parallel to the <code>cluster-config</code> of the relevants clusters.</p> <p>The secret reference is pointing to a secret in another namespace, and creates a <code>SecretExport</code> (in the mentioned namespace) and a <code>SecretImport</code> (in the Namespace Provisioner's namespace) CRs<sup>12</sup>. This is why the secrets cannot have the same name, else these resources conflict. We can disable the export with <code>create_export</code> in case you use a secret that is already exported.</p> <p>The Namespace Provisioner expects that the main folder contains the files <code>desired-namespaces.yaml</code> and <code>namespaces.yaml</code>. This gives us the following folder structure (limited to relevant folders/files):</p> <pre><code>platforms/clusters/build-01\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 cluster-config\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 values\n\u251c\u2500\u2500 ns-provisioner\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 install\n\u2502\u00a0\u00a0 \u2502   \u251c\u2500\u2500 desired-namespaces.yaml\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 namespaces.yaml\n|   \u2514\u2500\u2500 additional-sources\n\u2514\u2500\u2500 tanzu-sync\n</code></pre> <p>Let's take a look at the content of these two mandatory files.</p> <p>desired-namespaces.yaml</p> <p>This file contains a list of the namespaces we want the Namespace Provisioner to manage for us:</p> namespace-provisioner/main/desired-namespaces.yaml<pre><code>#@data/values\n---\nnamespaces:\n#! The only required parameter is the name of the namespace. All additional values provided here \n#! for a namespace will be available under data.values for templating additional sources\n- name: dev\n- name: qa\n- name: orange\n- name: apps\n- name: teal\n- name: green\n- name: cyan\n- name: demo1\n</code></pre> <p>The <code>namespaces</code> file contains more details.</p> <p>namespaces.yaml</p> <p>In this file, we can affect how the Namespace Provisioner creates the namespaces.</p> <p>The file is included in the YTT processing, and thus we have access to some variables, like the namespace name.</p> <p>In my example, I ensure we create the namespace, and through an overlay append additional secrets to the default <code>ServiceAccount</code>.</p> <p>For example, for the Build profile you might want to ensure the <code>ServiceAccount</code> has access to a Git credential for Tekton, and a encryption secret for Cosign (see Supply Chain Security Tools - Sign<sup>19</sup>).</p> namespace-provisioner/main/namespaces.yaml<pre><code>#@ load(\"@ytt:data\", \"data\")\n#@ load(\"@ytt:overlay\", \"overlay\")\n#! This for loop will loop over the namespace list in desired-namespaces.yaml and will create those namespaces.\n#! NOTE: if you have another tool like Tanzu Mission Control or some other process that is taking care of creating namespaces for you, \n#! and you don\u2019t want namespace provisioner to create the namespaces, you can delete this file from your GitOps install repository.\n#@ for ns in data.values.namespaces:\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: #@ ns.name\n---\n#@overlay/match by=overlay.subset({\"kind\":\"ServiceAccount\", \"metadata\":{ \"name\": \"default\"}}),expects=\"1+\"\n---\nsecrets:\n  #@overlay/append\n  - name: github-ssh\n#@ end\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-gitops-prep/#share-secrets","title":"Share Secrets","text":"<p>Combining the Namespace Provisioner configuration and the additional resources configuration (the custom Schema we added), we can address adding additional secrets.</p> <p>The way to do this, is as follows:</p> <ol> <li>Ensure that the custom Schema has a property for sensitive values</li> <li>Manage the secret input in the same sensitive values file that you encrypt with SOPS as you do for the TAP Install</li> <li>Create Secret manifests with the YTT templating in the <code>cluster-config/config</code> folder</li> </ol> <p>If the secret needs to exist in more than one namespace, we recommend creating a shared Namespace for this.</p> <p>Then create <code>SecretExport</code> and <code>SecretImport</code> as appropriate.</p> <p>Let us use the Namespace Provisioner as the example.</p> <p>In the custom schema file, we add the property <code>custom.sensitive_values</code> as follows:</p> cluster-config/config/custom/00-custom-schema.yaml<pre><code>#@data/values-schema\n#@overlay/match-child-defaults missing_ok=True\n---\ncustom:\n\n  #@schema/title \"Other Values (sensitive)\"\n  #@schema/desc \"Sensitive portion of the non TAP Install configuration values.\"\n  #@schema/type any=True\n  sensitive_values: {}\n</code></pre> <p>Then, in the senstive input file, before SOPS encryption, we set our values as follows:</p> sensitive-file-before-encryption.yaml<pre><code>custom:\n  sensitive_values:\n    github:\n      ssh:\n        private_key: |\n          -----BEGIN OPENSSH PRIVATE KEY-----\n          ...\n          -----END OPENSSH PRIVATE KEY-----\n        known_hosts: |\n          ...\n</code></pre> <p>As you can see, we add them add the top level, as we put our custom schema in parallel to the one for TAP Install. Encrypt the file as you've done before.</p> <pre><code>export SOPS_AGE_RECIPIENTS=$(cat key.txt | grep \"# public key: \" | sed 's/# public key: //')\nsops --encrypt tap-full-01-sensitive-values.yaml  &gt; tap-full-01-sensitive-values-sops.yaml\n</code></pre> <p>Now we create the Kubernetes Secret's to share in the custom folder, alongside the Schema.</p> <p>As I want to communicate what the file does, I'll name <code>01-shared.yaml</code>. This way it is also clear that is should be the first non-schema file to be used/handled, containing resources intended to be shared.</p> <p>The files in my <code>cluster-config</code> folder are now as follows:</p> <pre><code>cluster-config/\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 custom\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 00-custom-schema.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 01-shared.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tap-install\n\u2514\u2500\u2500 values\n    \u251c\u2500\u2500 non-sensitive-values.yaml\n    \u251c\u2500\u2500 tap-install-values.yaml\n    \u2514\u2500\u2500 tap-sensitive-values.sops.yaml\n</code></pre> <p>As the example below is quite large, it is collapsed. You can open it to view the file and its explanation.</p> Shared Config <p>My <code>01-shared.yaml</code> file is represented below.</p> <p>We first load the appropriate YTT libraries, and then define the Namespace.</p> <p>This way, we can guarantee the other resources can be created there.</p> <p>I then create the secrets as they should be for Tekton, so I do not have to worry about that later.</p> <p>You can choose to make the Git server URL a property or just define it as a hardcoded value like this.</p> <p>In addition, I want to share the secrets. They should be copied to the appropriate places.</p> <p>So each secret that needs to be shared, has an accompanying <code>SecretExport</code>.</p> <p>For the Namespace Provisioner GitOps and Additional Resources configuration, I create the <code>github-ssh</code> secret twice. This way each segment has its own key.</p> <p>In my case, the sensitive values are the same, but you can choose to use different values for different keys/purposes.</p> cluster-config/config/custom/01-shared.yaml<pre><code>#@ load(\"@ytt:data\", \"data\")\n#@ load(\"@ytt:base64\", \"base64\")\n#@ load(\"@ytt:yaml\", \"yaml\")\n\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: shared\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: gitlab-https\n  namespace: shared\n  annotations:\n    tekton.dev/git-0: http://gitlab.services.my-domain.com\ntype: kubernetes.io/basic-auth\nstringData:\n  caFile: #@ data.values.custom.sensitive_values.gitlab.ca_cert_data\n  password: #@ data.values.custom.sensitive_values.gitlab.pat\n  username: #@ data.values.custom.sensitive_values.gitlab.username\n---\napiVersion: secretgen.carvel.dev/v1alpha1\nkind: SecretExport\nmetadata:\n  name: gitlab-https\n  namespace: shared\nspec:\n  toNamespaces:\n  - '*'\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: github-ssh-1\n  namespace: shared\n  annotations:\n    tekton.dev/git-0: github.com\ntype: kubernetes.io/ssh-auth\ndata:\n  ssh-privatekey: #@ base64.encode(data.values.custom.sensitive_values.github.ssh.private_key)\n  ssh-knownhosts: #@ base64.encode(data.values.custom.sensitive_values.github.ssh.known_hosts)\n---\napiVersion: secretgen.carvel.dev/v1alpha1\nkind: SecretExport\nmetadata:\n  name: github-ssh-1\n  namespace: shared\nspec:\n  toNamespaces:\n  - '*'\n\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: github-ssh\n  namespace: shared\n  annotations:\n    tekton.dev/git-0: github.com\ntype: kubernetes.io/ssh-auth\ndata:\n  ssh-privatekey: #@ base64.encode(data.values.custom.sensitive_values.github.ssh.private_key)\n  ssh-knownhosts: #@ base64.encode(data.values.custom.sensitive_values.github.ssh.known_hosts)\n---\napiVersion: secretgen.carvel.dev/v1alpha1\nkind: SecretExport\nmetadata:\n  name: github-ssh\n  namespace: shared\nspec:\n  toNamespaces:\n  - '*'\n---\napiVersion: secretgen.carvel.dev/v1alpha1\nkind: SecretImport\nmetadata:\n  name: github-ssh\n  namespace: tap-install\nspec:\n  fromNamespace: shared\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-gitops-prep/#fluxcd-controllers","title":"FluxCD Controllers","text":"<p>Next, we want add the FluxCD Controllers for Kustomizations<sup>16</sup> and Helm<sup>14</sup> charts. This way we can leverage FluxCD to manage any other resources we want to install into the cluster that are tightly coupled to TAP.</p> <p>Think of a Tekton Pipeline, or a Cartographer <code>Workload</code> resource.</p> <p>In order to synchronize these Controllers and their respective resources, we use a Carvel <code>App</code>. Making use of the same Git synchronization as the Tanzu Sync does, we can even re-use its secret.</p> <p>You might be wondering why we cannot add these directly, like we did with the secrets. Unfortunately, for <code>Kustomization</code>'s to work, we need a (Flux) Source. For example, a <code>GitRepository</code>.</p> <p>The CRD of this CR is installed by the FluxCD Source Controller, which in turn is installed by TAP. This means we need to wait until the CRD exists before we can create the Source.</p> <p>Which means we need to guarantee the Tanzu Sync is not the one synchronizing these resources, as it will block installing TAP if it finds a resource that the Kubernetes cluster doesn't support (i.e., a CR, of which the CRD does not exist).</p> <p>First, we create the <code>App</code> CR, which can exist in the <code>cluster-config/config</code> tree. This way, the Tanzu Sync App creates this as a separate synchronizer. It does its reconcillation loop until the required CRDs exist and eventually reconcile successfully.</p> <p>As a start, we copy the Tanzu Sync <code>App</code>, located at <code>tanzu-sync/app/config/.tanzu-managed/sync.yaml</code>. Assuming we put the Flux configuration files in the same Git repository, you can re-use the Git configuration (located at <code>tanzu-sync/app/values/tanzu-sync.yaml</code>).</p> <p>The reduce the complexity, we remove the <code>valuesFrom</code> section from the <code>template</code>, and add the values directly.</p> <p>We keep the <code>paths</code> section, and keep the path as <code>config</code>. Meaning, when we add the FluxCD resources, such as the Controllers, <code>GitRepository</code> and so on, we put them in a sub-folder called <code>config</code> which is a convention of Carvel Apps.</p> platforms/clusters/full-tap-cluster/cluster-config/config/custom/02-flux-inital-content-sync.yaml<pre><code>apiVersion: kappctrl.k14s.io/v1alpha1\nkind: App\nmetadata:\n  name: flux-initial-content-sync\n  namespace: tanzu-sync\n  annotations:\n    kapp.k14s.io/change-group: tanzu-sync\n    kapp.k14s.io/change-rule.0: \"upsert after upserting tanzu-sync-secrets\"\n    kapp.k14s.io/change-rule.1: \"upsert after upserting install-registry-export\"\n    #! if registry credentials are deleted before sync-managed software is removed, uninstall can be slow or fail.\n    kapp.k14s.io/change-rule.2: \"delete before deleting tanzu-sync-secrets\"\n    kapp.k14s.io/change-rule.3: \"delete before deleting install-registry-export\"\nspec:\n  serviceAccountName: sync-sa\n  fetch:\n    - git:\n        url: git@github.com:joostvdg/tap-gitops.git\n        ref:  origin/main\n        secretRef:\n          name: sync-git\n        subPath: platforms/clusters/full-tap-cluster/flux\n  template:\n    - ytt:\n        paths: \n        - config\n  deploy:\n    - kapp: {}\n</code></pre> <p>Our folder now looks like this:</p> <pre><code>cluster-config/\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 custom\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 00-custom-schema.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 01-shared.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 02-flux-sync.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tap-install\n\u2514\u2500\u2500 values\n    \u251c\u2500\u2500 non-sensitive-values.yaml\n    \u251c\u2500\u2500 tap-install-values.yaml\n    \u2514\u2500\u2500 tap-sensitive-values.sops.yaml\n</code></pre> <p>And our synchronizations are looking like this:</p> <p></p> <ol> <li>We run the deploy script of the TAP GitOps Reference Implementation, which installs the <code>sync</code> App into the <code>tanzu-sync</code> Namespace</li> <li>Included in the content that is synchronized to the cluster via the <code>sync</code> App, is the TAP <code>PackageInstall</code>, and the Flux <code>App</code> we just created</li> <li>The TAP package installs all the other packages we configure with the TAP install values, generally guided by the <code>profile</code> we select</li> <li>One the packages that TAP installs, is the <code>namespace-provisioner</code>, and when opting for the GitOps configuration for this package, it installs another package, called <code>provisioner</code> in the <code>tap-namespace-provisioning</code> Namespace</li> <li>The last step in this chain, is the <code>provisioner</code> App creating the Namespaces you defined in its <code>desired-namespaces.yaml</code> config file</li> </ol> <p>I hope the picture and explanation clarify what is going on, and what App is synchronizing what.</p> <p>Let's create the configuration folders mentioned in the <code>App</code>, we create the following folders:</p> <ol> <li>The <code>flux</code> folder, as top level folder in parallel to the <code>cluster-config</code> and <code>tanzu-sync</code> folders</li> <li>The <code>config</code> folder as sub-folder, to house all the files that we synchronize directly to the cluster</li> <li>In the <code>config</code> folder, we create another sub-folder <code>controllers</code>, it is here we paste the release files of those Controllers<sup>15</sup><sup>17</sup>.</li> </ol> <p>For safety, we comment out the Namespace manifest in these files.</p> <p>Namespaces and KAPP Packages</p> <p>When KAPP Controller installs a Package, it add ownership labels on each resource created via that Package.</p> <p>If the Package creates a Namespace, it owns it, and other Packages are not allowed to create or update it. When a Package attempts to do so, the KAPP Controller blocks the action and returns an error.</p> <p>In the current scenario, we create the configuration for the Flux Controllers before Flux is installed via the TAP Install.</p> <p>In order for the TAP Package to own the <code>flux-system</code> Namespace, the Tanzu Sync Package most not create it.</p> <p>The best course of action here, is the manually create the Namespace prior to running the Tanzu Sync deploy:</p> <pre><code>kubectl create namespace flux-system\n</code></pre> <p>Our folder structure now looks like this:</p> <pre><code>.\n\u251c\u2500\u2500 cluster-config\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 values\n\u251c\u2500\u2500 flux\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 config\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 controllers\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 helm.yaml\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 kustomize.yaml\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 kustomizations\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 sources\n\u251c\u2500\u2500 ns-provisioner\n\u2514\u2500\u2500 tanzu-sync\n</code></pre> <p>For clarity, I also included the <code>kustomizations</code> and <code>sources</code> folders. It is there where we create the FluxCD content files later.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-gitops-prep/#tap-install-reconciliation","title":"TAP Install Reconciliation","text":"<p>When adding more configuration to the TAP Install, you run the risk the Tanzu Sync runs into a timeout.</p> <p>For example, the Namepace Provisioner installs additional packages for each Namespace it creates.</p> <p>Some of these packages rely on other packages to be completed before they can complete.</p> <p>This means the Namespace Provisioner waits on those packages, waiting on other packages.</p> <p>The TAP package installation waits on the Namespace Provisioner.</p> <p>And in turn, the Tanzu Sync app waits on the TAP package installation.</p> <p>Expected Time</p> <p>While we strive for having TAP install go smoothly and fast, due to the reconcilliation loops of each Package Install, wait times increase.</p> <p>For example, my installation from scratch  took about 21 minutes.</p> <p>This was installing a Full Profile with additional resources such as secrets, Flux Controllers, and several Namspaces via the Namespace Provisioner.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-gitops-prep/#references","title":"References","text":"<ol> <li> <p>TAP Install 1.5 - Offline installation \u21a9</p> </li> <li> <p>TAP Install 1.5 - GitOps Intro \u21a9\u21a9</p> </li> <li> <p>TAP Install 1.5 - GitOps SOPS Install - relocate images \u21a9</p> </li> <li> <p>Tanzu Network - Tanzu Application Platform resources \u21a9</p> </li> <li> <p>TAP Install 1.5 - GitOps use the Reference Implementation tarbal \u21a9</p> </li> <li> <p>TAP Install 1.5 - GitOps SOPS - preparing sensitive values \u21a9</p> </li> <li> <p>Carvel YTT - How to write a schema \u21a9</p> </li> <li> <p>Carvel YTT - How it works \u21a9</p> </li> <li> <p>Carvel Kapp Controller - Docs \u21a9</p> </li> <li> <p>TAP Install 1.5 - Multicluster profile information \u21a9</p> </li> <li> <p>Namespace Provisioner - Modes \u21a9</p> </li> <li> <p>SecretGen Controller - Secret Export and Import \u21a9\u21a9</p> </li> <li> <p>Carvel KAPP - App Spec \u21a9</p> </li> <li> <p>FluxCD - Helm Controller documentation \u21a9</p> </li> <li> <p>FluxCD - Helm Controller releases \u21a9</p> </li> <li> <p>FluxCD - Kustomize Controller documentation \u21a9</p> </li> <li> <p>FluxCD - Kustomize Controller releases \u21a9</p> </li> <li> <p>TAP Install - Namespace Provisioner GitOps \u21a9</p> </li> <li> <p>TAP - Supply Chain Security Tools - Sign \u21a9</p> </li> </ol>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-run/","title":"TAP Run","text":"<p>For large-scale deployments of TAP, we recommend separating the Build and Test phases of the supply chain from the Delivery or Run phase.</p> <p>TAP supports this via the Run profile, only installing the components related to these activities <sup>1</sup>.</p> <p>The components installed, among others, are Contour, KNative Serving, and Crossplane.</p> <p>This chapter focuses on creating a GitOps install of the Run profile and configuring TAP to integrate with our tools of choice.</p> <p>We will take a look at the following:</p> <ol> <li>Configure Run Profile</li> <li>Add ServiceAccount for the View profile</li> <li>Manage Workloads</li> </ol> <p>Warning</p> <p>Before we dive into the Run profile-specific topics, the GitOps Prep page is considered a pre-requisite.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-run/#configure-run-profile","title":"Configure Run profile","text":"<p>As with the general preparation and the Run profile, we'll have several config files to prepare.</p> <ol> <li>The non-sensitive values for the Profile install</li> <li>The sensitive values (to be encrypted with SOPS) for the Profile install</li> <li>The Namespace Provisioner</li> </ol>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-run/#non-sensitive-values","title":"Non-Sensitive Values","text":"<p>The Run profile values are similar to the ones for the Build profile.</p> <p>The main difference is that we do not configure the Tanzu Build Service, which isn't included in this profile.</p> <p>For the Supply Chain, we select Basic this time, as there is little to do in Profile beyond synchronizing resources. We aren't using those components if we use the GitOps flow for the applications.</p> <p>Why don't we exclude them, then? Excluding components is more complex than configuring them with the most basic configuration.</p> <p>Feel free to add packages to the <code>exclude_packages</code> list if you need to limit the resource usage.</p> <p>The only new section we have here is the <code>appliveview_connector</code>. It connects to the component running in the View cluster.</p> <p>Disabled TLS for App Live View</p> <p>In this example, we disable the SSL (or TLS).</p> <p>We don't need to supply a certificate, but you probably do not want this in production.</p> <p>In that case, you must either have a valid certificate (e.g., one from a trusted source) or add it in a secret the connector can use.</p> <p>The base example is as follows:</p> <pre><code>---\ntap_install:\n  values:\n    profile: run\n    shared:\n      ingress_domain: run-01.my-domain.com\n      ca_cert_data: |-\n        -----BEGIN CERTIFICATE-----\n        MIID7jCCAtagAwIBAgIURv5DzXSDklERFu4gL2sQBNeRg+owDQYJKoZIhvcNAQEL\n        ...\n        vhs=\n        -----END CERTIFICATE-----\n\n    supply_chain: basic\n    ootb_supply_chain_basic:\n      registry:\n        server: harbor.services.my-domain.com\n        repository: tap-apps\n\n    ceip_policy_disclosed: true\n    contour:\n      envoy:\n        service:\n          type: LoadBalancer\n\n    appliveview_connector:\n      backend:\n        sslDeactivated: true\n        ingressEnabled: true\n        host: appliveview.view.my-domain.com\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-run/#namespace-provisioner","title":"Namespace Provisioner","text":"<p>Next, we add an almost identical <code>namespace_provisioner</code> configuration as we had in the GitOps preparation page and the Build profile.</p> <p>The only difference is that the subfolder is now <code>run-01</code>, so we get the correct values for our Run cluster.</p> <pre><code>#! This snippet is from the GitOps Preparation page\nnamespace_provisioner:\n  controller: false\n  gitops_install:\n    ref: origin/main\n    subPath: platforms/clusters/run-01/ns-provisioner/install\n    url: git@github.com:joostvdg/tap-gitops.git\n    secretRef:\n      name: github-ssh\n      namespace: shared\n      create_export: false\n  additional_sources:\n  - git:\n      ref: origin/main\n      subPath: platforms/clusters/run-01/ns-provisioner/additional-sources\n      url: git@github.com:joostvdg/tap-gitops.git\n      # secretRef section is only needed if connecting to a Private Git repo\n      secretRef:\n        name: github-ssh-1\n        namespace: shared\n        create_export: false\n    path: _ytt_lib/testing-scanning-supplychain-setup\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-run/#full-non-sensitive-example","title":"Full Non-Sensitive Example","text":"<p>Then, we combine the base values with the Namespace Provisioner values to reach a complete example.</p> Non-Sensitive Values Example <pre><code>---\ntap_install:\n  values:\n    profile: run\n    shared:\n      ingress_domain: run-01.my-domain.com\n      ca_cert_data: |-\n        -----BEGIN CERTIFICATE-----\n        MIID7jCCAtagAwIBAgIURv5DzXSDklERFu4gL2sQBNeRg+owDQYJKoZIhvcNAQEL\n        ...\n        vhs=\n        -----END CERTIFICATE-----\n\n    supply_chain: basic\n    ootb_supply_chain_basic:\n      registry:\n        server: harbor.services.my-domain.com\n        repository: tap-apps\n\n    ceip_policy_disclosed: true\n    contour:\n      envoy:\n        service:\n          type: LoadBalancer\n\n    appliveview_connector:\n      backend:\n        sslDeactivated: true\n        ingressEnabled: true\n        host: appliveview.view.my-domain.com\n\n    namespace_provisioner:\n      controller: false\n      gitops_install:\n        ref: origin/main\n        subPath: platforms/clusters/run-01/ns-provisioner/install\n        url: git@github.com:joostvdg/tap-gitops.git\n        secretRef:\n          name: github-ssh\n          namespace: shared\n          create_export: false\n      additional_sources:\n      - git:\n          ref: origin/main\n          subPath: platforms/clusters/run-01/ns-provisioner/additional-sources\n          url: git@github.com:joostvdg/tap-gitops.git\n          # secretRef section is only needed if connecting to a Private Git repo\n          secretRef:\n            name: github-ssh-1\n            namespace: shared\n            create_export: false\n        path: _ytt_lib/testing-scanning-supplychain-setup\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-run/#sensitive-values","title":"Sensitive Values","text":"<p>Below are the sensitive values.</p> <p>The <code>shared.image_registry</code> with the URL, username, and password.</p> <p>Mind you, the example below is before encryption with SOPS (or ESO); encrypt the file before placing it at that location.</p> platforms/clusters/run-01/cluster-config/values/tap-sensitive-values.sops.yaml<pre><code>tap_install:\n    sensitive_values:\n        shared:\n            #! registry for the TAP installation packages\n            image_registry:\n                project_path: harbor.services.mydomain.com/tap/tap-packages\n                username: #! username\n                password: #! password or PAT\ncustom:\n  sensitive_values:\n    github:\n      ssh:\n        private_key: |\n          ...\n        known_hosts: |\n          ...\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-run/#add-serviceaccount-for-the-view-profile","title":"Add ServiceAccount for the View profile","text":"<p>One of the essential features of TAP is its GUI.</p> <p>For the TAP GUI to show the Supply Chains, one of its core features, it needs access to the Kubernetes cluster hosting them.</p> <p>In this scenario, we separate the Build cluster from the cluster the TAP GUI runs in (View cluster). So, we must provide the TAP GUI with alternative means to view the Supply Chain resources.</p> <p>We do that by creating a ServiceAccount in each cluster we want the TAP GUI to have visibility<sup>3</sup>. Then, copy the token of that ServiceAccount into the View profile configuration.</p> <p>Add this ServiceAccount with the required permission and a Token to our additional Kubernetes resources.</p> <p>Let's place this in the folder <code>cluster-config/config/custom</code>, and call the file <code>03-tap-gui-service-account.yaml</code>.</p> TAP GUI Viewer Service Account platforms/clusters/build-01/cluster-config/config/custom/03-tap-gui-service-account.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: tap-gui\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  namespace: tap-gui\n  name: tap-gui-viewer\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: tap-gui-viewer\n  namespace: tap-gui\n  annotations:\n    kubernetes.io/service-account.name: tap-gui-viewer\ntype: kubernetes.io/service-account-token\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: tap-gui-read-k8s\nsubjects:\n- kind: ServiceAccount\n  namespace: tap-gui\n  name: tap-gui-viewer\nroleRef:\n  kind: ClusterRole\n  name: k8s-reader\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: k8s-reader\nrules:\n- apiGroups: ['']\n  resources: ['pods', 'pods/log', 'services', 'configmaps', 'limitranges']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['metrics.k8s.io']\n  resources: ['pods']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['apps']\n  resources: ['deployments', 'replicasets', 'statefulsets', 'daemonsets']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['autoscaling']\n  resources: ['horizontalpodautoscalers']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['networking.k8s.io']\n  resources: ['ingresses']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['networking.internal.knative.dev']\n  resources: ['serverlessservices']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: [ 'autoscaling.internal.knative.dev' ]\n  resources: [ 'podautoscalers' ]\n  verbs: [ 'get', 'watch', 'list' ]\n- apiGroups: ['serving.knative.dev']\n  resources:\n  - configurations\n  - revisions\n  - routes\n  - services\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['carto.run']\n  resources:\n  - clusterconfigtemplates\n  - clusterdeliveries\n  - clusterdeploymenttemplates\n  - clusterimagetemplates\n  - clusterruntemplates\n  - clustersourcetemplates\n  - clustersupplychains\n  - clustertemplates\n  - deliverables\n  - runnables\n  - workloads\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['source.toolkit.fluxcd.io']\n  resources:\n  - gitrepositories\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['source.apps.tanzu.vmware.com']\n  resources:\n  - imagerepositories\n  - mavenartifacts\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['conventions.apps.tanzu.vmware.com']\n  resources:\n  - podintents\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['kpack.io']\n  resources:\n  - images\n  - builds\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['scanning.apps.tanzu.vmware.com']\n  resources:\n  - sourcescans\n  - imagescans\n  - scanpolicies\n  - scantemplates\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['tekton.dev']\n  resources:\n  - taskruns\n  - pipelineruns\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['kappctrl.k14s.io']\n  resources:\n  - apps\n  verbs: ['get', 'watch', 'list']\n- apiGroups: [ 'batch' ]\n  resources: [ 'jobs', 'cronjobs' ]\n  verbs: [ 'get', 'watch', 'list' ]\n- apiGroups: ['conventions.carto.run']\n  resources:\n  - podintents\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['appliveview.apps.tanzu.vmware.com']\n  resources:\n  - resourceinspectiongrants\n  verbs: ['get', 'watch', 'list', 'create']\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-run/#manage-workloads","title":"Manage Workloads","text":"<p>Like the section for the Build profile, we assume to leverage the FluxCD Kustomize Controller to synchronize these resources.</p> <p>One difference is that the GitOps workflow (for the applications) creates a PullRequest with the resources for us.</p> <p>So we do not have to create the resource files ourselves. We create the expected folder structure and then point a <code>Kustomization</code> towards the correct folder.</p> <p>For example, create the file <code>team-orange-kustomization.yaml</code> in the appropriate sub-folder (<code>tap/apps/run-01</code>) of the <code>tap-apps</code> Git repository.</p> tap/apps/run-01/team-orange-kustomization.yaml<pre><code>apiVersion: kustomize.toolkit.fluxcd.io/v1beta2\nkind: Kustomization\nmetadata:\n  name: orange\n  namespace: apps\nspec:\n  interval: 5m0s\n  path: ./teams/orange/staging\n  prune: true\n  targetNamespace: orange\n  sourceRef:\n    kind: GitRepository\n    name: apps\n</code></pre> <p>Use ArgoCD</p> <p>Working on these GitOps deployments of TAP with customers, I come across ArgoCD quite a bit.</p> <p>The synchronizations of the \"Deliverable\" resources are entirely separated from the others.</p> <p>This allows leveraging something like ArgoCD for managing your applications in Staging and Production instead of relying on FluxCD.</p> <p>If you are already using ArgoCD and wondering how to combine it with TAP, this is where TAP and ArgoCD fit together well<sup>4</sup>.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-run/#install","title":"Install","text":"<p>You are now ready to install the TAP Run profile.</p> <p>For the actual install commands, I refer to the docs <sup>1</sup>.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-run/#references","title":"References","text":"<ol> <li> <p>TAP 1.5 Install - GitOps Install with SOPS deploy \u21a9\u21a9</p> </li> <li> <p>TAP Install 1.5 - Run profile \u21a9</p> </li> <li> <p>TAP GUI - View resources on multiple clusters \u21a9</p> </li> <li> <p>ArgoCD - declarative, GitOps continuous delivery tool for Kubernetes \u21a9</p> </li> </ol>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-view/","title":"TAP View","text":"<p>For large-scale deployments of TAP, we recommend separating the Build and Test phases of the supply chain from the Delivery phase and then separating the cluster that views those workloads across the clusters.</p> <p>TAP supports this via the View profile, only installing the components related to these activities <sup>1</sup>.</p> <p>The components installed, among others, are TAP GUI and Metadata Store(stores scan results).</p> <p>This chapter focuses on creating a GitOps install of the View profile and configuring TAP to integrate with our tools of choice.</p> <p>We will take a look at the following:</p> <ol> <li>Collect ServiceAccount Tokens from other clusters</li> <li>Configure View Profile</li> <li>Look at next steps</li> </ol>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-view/#collect-serviceaccount-tokens-from-other-clusters","title":"Collect ServiceAccount Tokens from other clusters","text":"<p>For each cluster, we need to record the Kubernetes API server URL and the token of the TAP GUI ServiceAccount<sup>3</sup>.</p> <p>Connect to target cluster</p> <p>Ensure you are connected to the cluster you want to collect the information from.</p> <p>We collect the URL as follows:</p> <pre><code>CLUSTER_URL=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')\n</code></pre> <p>Assuming you followed the <code>Add ServiceAccount for the View profile</code> sections, we can collect the token this way:</p> <pre><code>CLUSTER_TOKEN=$(kubectl -n tap-gui get secret tap-gui-viewer -o=json \\\n| jq -r '.data[\"token\"]' \\\n| base64 --decode)\n</code></pre> <p>Print them out to be sure you collected them correctly:</p> <pre><code>echo CLUSTER_URL: $CLUSTER_URL\necho CLUSTER_TOKEN: $CLUSTER_TOKEN\n</code></pre> <p>Which should result in something like this:</p> <pre><code>CLUSTER_URL: https://10.220.10.38:6443\nCLUSTER_TOKEN: eyJhbGciOiJSUzI1NiIsImtpZCI6IjJK.....\n</code></pre> <p>Do this for every cluster the TAP GUI needs to read resources from. Collect the information and hold it ready for filling in the View profile values.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-view/#configure-view-profile","title":"Configure View profile","text":"<p>As usual, we have to create two configuration files:</p> <ol> <li>The non-sensitive values for the Profile install</li> <li>The sensitive values (to be encrypted with SOPS) for the Profile install</li> </ol> <p>No Namespace Provisioner</p> <p>A TAP View cluster (or Profile installation) does not run workloads.</p> <p>So, we do not configure the Namespace Provisioner for this cluster.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-view/#non-sensitive-values","title":"Non-Sensitive Values","text":"<p>Let us start with the non-sensitive values.</p> <p>The primary component to configure is the TAP GUI.</p> <p>The base example is as follows:</p> <pre><code>---\ntap_install:\n  values:\n    profile: view\n    ceip_policy_disclosed: true\n    shared:\n      ingress_domain: view.my-domain.com\n      ca_cert_data: |-\n        -----BEGIN CERTIFICATE-----\n        MIID7jCCAtagAwIBAgIURv5DzXSDklERFu4gL2sQBNeRg+owDQYJKoZIhvcNAQEL\n        ...\n        vhs=\n        -----END CERTIFICATE-----\n    contour:\n      envoy:\n        service:\n          type: LoadBalancer\n\n    tap_gui:\n      metadataStoreAutoconfiguration: true\n      service_type: ClusterIP\n      app_config:\n        auth:\n          allowGuestAccess: true \n        organization:\n          name: 'My Portal'\n</code></pre> <p>You might be curious about the authentication section:</p> <pre><code>tap_gui:\n  app_config:\n    auth:\n      allowGuestAccess: true \n</code></pre> <p>Unless you define some form of authentication, the developer portal (based on Backstage<sup>4</sup>) is locked.</p> <p>To limit our configuration, we enable guest access, which essentially turns off authentication.</p> <p>Always Use Authentication In Production</p> <p>Because the TAP GUI has some access to Kubernetes resources and one or more clusters, we recommend always using a proper authentication mechanism in Production.</p> <p>For more information, read the docs on how to set up an authentication provider or refer to the Backstage auth docs <sup>5</sup><sup>6</sup>.</p> <p>We configure the access to the other clusters in the Sensitive Values section.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-view/#sensitive-values","title":"Sensitive Values","text":"<p>Below are the sensitive values.</p> <p>The <code>shared.image_registry</code> with the URL, username, and password.</p> <p>Mind you, the example below is before encryption with SOPS (or ESO); encrypt the file before placing it at that location.</p> <p>Here, we configure the access to the Kubernetes clusters we collected earlier.</p> <p>We place the configuration under <code>tap_gui.app_config.kubernetes</code>, where we specify the type locator methods and the list of clusters.</p> <p>In the <code>clusterLocatorMethods</code> there's a <code>clusters</code> property; here, we can put the list of clusters. For each cluster we record how to access it and what to name it.</p> <p>In the <code>serviceAccountToken</code> field, we put the <code>$CLUSTER_TOKEN</code> we recorded earlier.</p> platforms/clusters/view-01/cluster-config/values/tap-sensitive-values.sops.yaml<pre><code>tap_install:\n  sensitive_values:\n    shared:\n      #! registry for the TAP installation packages\n      image_registry:\n        project_path: harbor.services.mydomain.com/tap/tap-packages\n        username: #! username\n        password: #! password or PAT\n    tap_gui:\n      app_config:\n        kubernetes:\n          serviceLocatorMethod:\n            type: 'multiTenant'\n          clusterLocatorMethods:\n            - type: 'config'\n              clusters:\n                - url: https://172.16.50.23:6443\n                  name: build-01\n                  authProvider: serviceAccount\n                  serviceAccountToken: eyJhbG...1u_O_A\n                  skipTLSVerify: true\n                  skipMetricsLookup: false\n</code></pre>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-view/#install","title":"Install","text":"<p>You are now ready to install the TAP View profile.</p> <p>For the actual install commands, I refer to the docs <sup>1</sup>.</p>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/tap-view/#references","title":"References","text":"<ol> <li> <p>TAP 1.5 Install - GitOps Install with SOPS deploy \u21a9\u21a9</p> </li> <li> <p>TAP Install 1.5 - View profile \u21a9</p> </li> <li> <p>TAP GUI - View resources on multiple clusters \u21a9</p> </li> <li> <p>Backstage - OSS Developer Portal, upstream of TAP GUI \u21a9</p> </li> <li> <p>TAP GUI - Configure Authentication \u21a9</p> </li> <li> <p>Backstage - Authentication Documentation \u21a9</p> </li> </ol>","tags":["TKG","TAP","GitOps","Carvel","Tanzu"]},{"location":"tanzu/tap-gitops/todo/","title":"TODO","text":"<ul> <li>Next steps<ul> <li>Applications with ArgoCD</li> <li>Supply Chain creation</li> <li>Supply Chain optimization</li> <li>Customize TAP GUI</li> <li>Configure Auth for TAP</li> </ul> </li> <li>write about Supply Chain extensions<ul> <li>Tekton Pipelines with Tasks</li> <li>Tekton Tasks + Workspace + overwriting the OOTB Supply Chain</li> <li>Change folder structure for GitOps repository</li> <li>Test Containers + DinD</li> <li>use Docker in Docker alternative from ITQ guy</li> </ul> </li> <li>TAP in EKS<ul> <li>Use KMS</li> <li>Use Workload Identity</li> <li>Use RDS for Hello World App</li> </ul> </li> </ul>"},{"location":"tanzu/tap-gitops/todo/#notes-tap-gitops-17","title":"Notes - TAP GitOps 1.7","text":"<ul> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.7/tap/install-gitops-sops.html</li> </ul>"},{"location":"tanzu/tap-gitops/todo/#tkr-126-admission-policies","title":"TKR 1.26 Admission Policies","text":"<ul> <li>see issue: https://vmware.slack.com/archives/C02D60T1ZDJ/p1697207282314919</li> <li>create cluster via TMC</li> <li>create mutation policies to add labels to all namespaces</li> </ul> <pre><code>type:\n  kind: Policy\n  version: v1alpha1\n  package: vmware.tanzu.manage.v1alpha1.clustergroup.policy\nfullName:\n  orgId: 26620245-46a1-4f87-8b0c-63f6b4c41198\n  clusterGroupName: joostvdg-h2o\n  name: enforce\nspec:\n  type: mutation-policy\n  recipe: label\n  recipeVersion: v1\n  input:\n    scope: \"*\"\n    targetKubernetesResources:\n      - apiGroups:\n          - \"\"\n        kinds:\n          - Namespace\n    label:\n      key: pod-security.kubernetes.io/enforce\n      value: privileged\n  namespaceSelector:\n    matchExpressions:\n      - key: kubernetes.io/metadata.name\n        operator: Exists\n</code></pre> <pre><code>type:\n  kind: Policy\n  version: v1alpha1\n  package: vmware.tanzu.manage.v1alpha1.clustergroup.policy\nfullName:\n  orgId: 26620245-46a1-4f87-8b0c-63f6b4c41198\n  clusterGroupName: joostvdg-h2o\n  name: enforce-version\nspec:\n  type: mutation-policy\n  recipe: label\n  recipeVersion: v1\n  input:\n    scope: \"*\"\n    targetKubernetesResources:\n      - apiGroups:\n          - \"\"\n        kinds:\n          - Namespace\n    label:\n      key: pod-security.kubernetes.io/enforce-version\n      value: latest\n  namespaceSelector:\n    matchExpressions:\n      - key: kubernetes.io/metadata.name\n        operator: Exists\n</code></pre> <pre><code>export INSTALL_REGISTRY_HOSTNAME=registry.tanzu.vmware.com\nexport INSTALL_REGISTRY_USERNAME=jvandergrien@vmware.com\nexport INSTALL_REGISTRY_PASSWORD='X6qRPlP@0056$&amp;qx%SCSIOFH'\nexport GIT_SSH_PRIVATE_KEY=$(cat $HOME/.ssh/id_rsa)\nexport GIT_KNOWN_HOSTS=$(ssh-keyscan github.com)\nexport SOPS_AGE_KEY=$(cat /Users/joostvdg/Projects/tap-gitops/key.txt)\nexport TAP_PKGR_REPO=registry.tanzu.vmware.com/tanzu-application-platform/tap-packages\n</code></pre>"},{"location":"tanzu/tap-gui/","title":"TAP GUI - Configuration Examples","text":"<p>...</p>","tags":["TAP","Tanzu","Backstage","Developer Portal"]},{"location":"tanzu/tap-gui/gitlab-integration/","title":"TAP GUI - GitLab Integration","text":"<p>Possible integrations with GitLab:</p> <ol> <li>Authentication</li> <li>Trust Catalog Items Source</li> <li>Catalog Items</li> </ol>","tags":["TAP","Tanzu","Backstage","Developer Portal"]},{"location":"tanzu/tap-gui/gitlab-integration/#authentication","title":"Authentication","text":"","tags":["TAP","Tanzu","Backstage","Developer Portal"]},{"location":"tanzu/tap-gui/gitlab-integration/#trust-catalog-items-source","title":"Trust Catalog Items Source","text":"","tags":["TAP","Tanzu","Backstage","Developer Portal"]},{"location":"tanzu/tap-gui/gitlab-integration/#catalog-items","title":"Catalog Items","text":"","tags":["TAP","Tanzu","Backstage","Developer Portal"]},{"location":"tanzu/tap-gui/gitlab-integration/#references","title":"References","text":"","tags":["TAP","Tanzu","Backstage","Developer Portal"]},{"location":"tanzu/tap-gui/techdocs-minio/","title":"TAP GUI - TechDocs with MinIO","text":"","tags":["TAP","Tanzu","Backstage","Developer Portal","MinIO","TechDocs"]},{"location":"tanzu/tkgs/tap13-overview/","title":"TAP on TKGs","text":"<p>Update January 2023</p> <p>This guide has been updated January 31<sup>st</sup>, to reflect TAP 1.3.4.</p> <p>Some improvements in the multi-cluster installation have been made. Enough to warrant an update to this guide.</p> <p>TKGs stands for Tanzu Kubernetes Grid vSphere, or vSphere with Tanzu.</p> <p>TAP stands for Tanzu Application Platform.</p> <p>This guide is about installing and using TAP on TGKs, with the following additional constraints:</p> <ul> <li>restricted internet access</li> <li>prepare Certificate Authority</li> <li>self-hosted Container Registry</li> <li>certificates signed with custom Certificate Authority (CA)</li> <li>separate Kubernetes cluster for TAP roles<ul> <li>Shared Services Cluster, <code>tap-s1</code> for the Container Registry and a TAP Build profile</li> <li>Workload Cluster, <code>tap-w1</code> for a TAP Run profile</li> </ul> </li> </ul> <p>The scripts and other configuration files can found in my Tanzu Example repository.</p> <p>Warning</p> <p>This guide is tested with TAP <code>1.3.x</code>, not everything applies for <code>1.4.0+</code>.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#steps","title":"Steps","text":"<ul> <li>Installation machine pre-requisites</li> <li>Shared services cluster pre-requisites</li> <li>Relocate Tanzu Application Platform (TAP) and Tanzu Build Service (TBS) images to local Harbor instance</li> <li>Install TAP Build profile</li> <li>Install TAP Run profile</li> <li>Install TAP View profile</li> <li>Use TAP GUI to create and register new Workloads</li> </ul>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#install-machine-pre-requisites","title":"Install Machine Pre-requisites","text":"<p>We have pre-requisites for the cluster, and we have pre-requisites for the machine which runs the commands. Here are the pre-requisites for all the commands:</p> <ul> <li>Kubernetes CLI Tools for vSphere</li> <li>Tanzu CLI v1.4 or later</li> <li>Tanzu CLI plugins for TKGs</li> <li>Tanzu CLI plugins for TAP</li> <li>kubectl</li> <li>yq</li> <li>jq</li> <li>http (or Curl)</li> <li>Carvel tools (mostly <code>ytt</code> and <code>imgpkg</code>)</li> </ul>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#tkgs-considerations","title":"TKGs Considerations","text":"<p>There are several requirements to your TKGs workload clusters.</p> <ol> <li> <p>Trust CA: In order to trust the certificate of Harbor for using images from there, your worker nodes need to trust it.</p> </li> <li> <p>Memory &amp; CPU TAP is resource intensive, reserve at least 12 CPU and 10GB of RAM for a full TAP install. About half for Run or Build profiles.</p> </li> <li> <p>Storage Tanzu Build Service will store its images on the nodes. These worker nodes need at leat 70GB of storage available.</p> </li> </ol> Cluster Definition <p>This is a Cluster definition for TKGs.</p> <p>You can apply this to a SuperVisor Cluster.</p> tap-s1.yml<pre><code>apiVersion: run.tanzu.vmware.com/v1alpha2\nkind: TanzuKubernetesCluster\nmetadata:\n  name: tap-s1\n  namespace: tap\nspec:\n  topology:\n    controlPlane:\n      replicas: 1\n      vmClass: best-effort-large\n      storageClass: vc01cl01-t0compute \n      tkr:\n        reference:\n          name: v1.22.9---vmware.1-tkg.1.cc71bc8\n    nodePools:\n    - name: worker-pool-1\n      replicas: 1\n      vmClass: best-effort-4xlarge\n      storageClass: vc01cl01-t0compute\n      volumes:\n        - name: containerd\n          mountPath: /var/lib/containerd\n          capacity:\n            storage: 90Gi \n      tkr:\n        reference:\n          name: v1.22.9---vmware.1-tkg.1.cc71bc8\n  settings:\n    storage:\n      defaultClass: vc01cl01-t0compute\n    network:\n      cni:\n        name: antrea    \n      trust: \n        additionalTrustedCAs: \n          - name: KearosCA\n            data: |\n              LS0tLS1CRUdJTiBDRVJUSUZJQ0...\n</code></pre> <p>Warning</p> <p>If you use Tanzu Mission Control to create the workload clusters, you cannot specify the <code>spec.settings.network.trust</code> section.</p> <p>Or if you do not want to configure the CA certificate for every cluster.</p> <p>For both scenarios, you can use a TkgServiceConfiguration resource on the supervisor cluster.</p> <pre><code>apiVersion: run.tanzu.vmware.com/v1alpha1\nkind: TkgServiceConfiguration\nmetadata:\n  name: tkg-service-configuration\nspec:\n  defaultCNI: antrea\n  trust:\n    additionalTrustedCAs:\n      - name: KearosCA\n        data: |\n            LS0tLS1CRUd...\n</code></pre> <p>Danger</p> <p>You have to set the correct PodSecurityProfiles, when dealing with TGKs.</p> <p>Below is a fast and terrible solution to ignore them. Use at your own risk.</p> <pre><code>kubectl create role psp:privileged \\\n    --verb=use \\\n    --resource=podsecuritypolicy \\\n    --resource-name=vmware-system-privileged\n\nkubectl create rolebinding default:psp:privileged \\\n    --role=psp:privileged \\\n    --serviceaccount=elastic-system:default\n\nkubectl create clusterrolebinding default-tkg-admin-privileged-binding --clusterrole=psp:vmware-system-privileged --group=system:authenticated\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#relocate-images-to-harbor","title":"Relocate Images To Harbor","text":"<p>Warning</p> <p>We expect you installed and configured a Harbor with a custom CA.</p> <p>I have described it in the Harbor with a Custom CA guide.</p> <p>We are executing the following steps:</p> <ul> <li>prepare local machine</li> <li>authenticate with Tanzu Network and local Harbor instance</li> <li>create new project in Harbor</li> <li>use Carvel's <code>imgpkg</code> to copy TAP images from Tanzu Network to Harbor</li> <li>use Carvel's <code>imgpkg</code> to copy TBS's from Tanzu Network to Harbor?</li> </ul>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#prepare-local-machine","title":"Prepare Local Machine","text":"<p>Important</p> <p>You can also authenticate <code>imgpkg</code> with both registries via Environment Variables.</p> <p>Or verify one of the other possible authentication methods.</p> <p>Below I'm showing how to leverage the Docker client.</p> <p>Set the credentials for Tanzu Network.</p> <pre><code>export TANZU_NETWORK_USER=\nexport TANZU_NETWORK_PASS=\n</code></pre> <p>And ensure your Docker client is authenticated.</p> <pre><code>docker login registry.tanzu.vmware.com --username ${TANZU_NETWORK_USER} --password ${TANZU_NETWORK_PASS}\n</code></pre> <p>Set the hostname and credentials for Harbor.</p> <pre><code>HARBOR_ADMIN_NAME=admin\nHARBOR_ADMIN_PASS=\nHARBOR_HOSTNAME=\n</code></pre> <p>As Harbor has a custom CA, we need Docker to trust it.</p> <p>Docker has an excellent guide on trusting registry certificates. Once you have completes the relevant steps for your OS, you can now authenticate with Harbor.</p> <pre><code>docker login ${HARBOR_HOSTNAME} --username ${HARBOR_ADMIN_NAME} --password ${TANZU_NETWORK_PASS}\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#create-harbor-projects","title":"Create Harbor Projects","text":"<p>To store images for TAP and for our applications, we need projects in Harbor to exist. We make them all public, to avoid having to create image pull secret, but feel free to do otherwise.</p> <p>Create the following Harbor Projects:</p> <ul> <li>tap</li> <li>tap-apps</li> <li>buildservice</li> </ul> <pre><code>http -a admin:${HARBOR_ADMIN_PASS} \\\n  POST \"https://${HARBOR_HOSTNAME}/api/v2.0/projects\" \\\n  project_name=\"tap\" public:=true --verify=false\n\nhttp -a admin:${HARBOR_ADMIN_PASS} \\\n  POST \"https://${HARBOR_HOSTNAME}/api/v2.0/projects\" \\\n  project_name=\"tap-apps\" public:=true --verify=false\n\nhttp -a admin:${HARBOR_ADMIN_PASS} \\\n  POST \"https://${HARBOR_HOSTNAME}/api/v2.0/projects\" \\\n  project_name=\"buildservice\" public:=true --verify=false\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#copy-tap-images","title":"Copy TAP Images","text":"<pre><code>export TAP_VERSION=\"1.3.4\"\n</code></pre> <pre><code>imgpkg copy --registry-verify-certs=false \\\n -b registry.tanzu.vmware.com/tanzu-application-platform/tap-packages:${TAP_VERSION} \\\n --to-repo ${HARBOR_HOSTNAME}/tap/tap-packages\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#copy-tbs-images-to-harbor","title":"Copy TBS Images To Harbor","text":"<ul> <li>https://docs.vmware.com/en/Tanzu-Build-Service/1.7/vmware-tanzu-build-service/GUID-installing.html</li> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.3/tap/GUID-tbs-offline-install-deps.html</li> </ul> <pre><code>tanzu package available list buildservice.tanzu.vmware.com --namespace tap-install\n</code></pre> <p>TAP <code>1.3.4</code> ships with TBS <code>1.7.4</code>:</p> <pre><code>TBS_VERSION=1.7.4\n</code></pre> <p>!!!! Warning     The TBS Full Dependency tar file is approximately 10GB of data.</p> <pre><code>Make sure you have the space before downloading it.\n</code></pre> <pre><code>imgpkg copy -b registry.tanzu.vmware.com/tanzu-application-platform/full-tbs-deps-package-repo:$TBS_VERSION \\\n  --to-tar=tbs-full-deps.tar\n</code></pre> <pre><code>imgpkg copy --tar tbs-full-deps-${TBS_VERSION}.tar \\\n  --to-repo=$HARBOR_HOSTNAME/buildservice/tbs-full-deps\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#satisfy-pre-requisites","title":"Satisfy Pre-requisites","text":"<p>A TAP installation has some pre-requisites, which are expected to exist in the cluster.</p> <ul> <li>Cluster Essentials: this means the Kapp and SecretGen controllers</li> <li>Namespace: the namespace to install TAP in, <code>tap-install</code> is the convention</li> <li>Registry Secrets: the secrets to the registry used for installing packages from, and the registry to push build images to</li> <li>TAP Package Repository: to install the TAP package, the TAP Package Repository needs to exist in the cluster</li> </ul> <p>When installing a Build, Iterate, or Full profile in an internetes restricted environment, we also need:</p> <ul> <li>Tanzu Build Service Dependencies: TBS assumes it can download all of its dependencies durring the install, if it can't, the installation fails</li> </ul> <p>The scripts below are called in order via the profile's install script. We cover them so you know what is in them, and why.</p> <p>The scripts themselves are available in GitHub.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#cluster-essentials","title":"Cluster Essentials","text":"<p>Not Required when using TMC</p> <p>When using Tanzu Mission Controll to create your clusters, TMC installs the Cluster Essentials for you.</p> <p>A basic script to install the Kapp and SecretGen controllers.</p> <p>For vSphere with Tanzu, TKGs, you need a specific version of the Kapp configuration.</p> <p>We assume you are using a Custom CA, so this script also creates a ConfigMap to configure Kapp to trust the CA certificate.</p> install-cluster-essentials.sh<pre><code>#!/usr/bin/env bash\nset -euo pipefail\nKAPP_CONTROLLER_NAMESPACE=${KAPP_CONTROLLER_NAMESPACE:-\"tkg-system\"}\nSECRET_GEN_VERSION=${SECRET_GEN_VERSION:-\"v0.9.1\"}\nPLATFORM=${PLATFORM:=\"tkgs\"}\nTKGS=\"tkgs\"\n\necho \"&gt; Installing Kapp Controller\"\nif [[ $PLATFORM eq $TGKS ]]\nthen\n  # This is a TKGs version\n  ytt -f ytt/kapp-controller.ytt.yml \\\n    -v namespace=\"$KAPP_CONTROLLER_NAMESPACE\" \\\n    -v caCert=\"${CA_CERT}\" \\\n    &gt; \"kapp-controller.yml\"\n  kubectl apply -f kapp-controller.yml\nelse\n  # Non TKGs\n  kubectl apply -f https://github.com/vmware-tanzu/carvel-kapp-controller/releases/latest/download/release.yml\n  echo \"Configure Custom Cert for Kapp Controller\"\n  ytt -f ytt/kapp-controller-config.ytt.yml \\\n    -v namespace=kapp-controller \\\n    -v caCert=\"${CA_CERT}\" \\\n    &gt; \"kapp-controller-config.yml\"\n  kubectl apply -f kapp-controller-config.yml --namespace kapp-controller\nfi\n\necho \"&gt; Installing SecretGen Controller with version ${SECRET_GEN_VERSION}\"\nkapp deploy -y -a sg -f https://github.com/vmware-tanzu/carvel-secretgen-controller/releases/download/\"$SECRET_GEN_VERSION\"/release.yml%\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#secrets-package-repo","title":"Secrets &amp; Package Repo","text":"<p>The convention is to install TAP and its packages in the namespace <code>tap-install</code>.</p> <p>With this script we create the namespace, the two registry secrets (install and build), and install the TAP Package Repository.</p> <p>We use the <code>tanzu secret registry</code> command to create the registry secrets. This uses the SecretGen controller we install via the Cluster Essentials.</p> <p>The reason for this, is that it allows you to create the secret once, and have it available in all the namespace that need it. When you need to update these secrets, you don't have to hunt for them, you use the same <code>tanzu secret registry</code> command.</p> <p>Danger</p> <p>Be aware the Build secret is also <code>--export-to-all-namespaces</code> in this scenario.</p> <p>For a POC or single team installation that is fine.</p> <p>When you want to fine grained permissions to allow each development team to only upload images to their respective repository, you have to change this!</p> install-tap-fundamentals.sh<pre><code>#!/usr/bin/env bash\nset -euo pipefail\n\nTAP_VERSION=${TAP_VERSION:-\"1.3.4\"}\nTAP_INSTALL_NAMESPACE=\"tap-install\"\nDOMAIN_NAME=${DOMAIN_NAME:-\"127.0.0.1.nip.io\"}\nINSTALL_REGISTRY_SECRET=\"tap-registry\"\nBUILD_REGISTRY_SECRET=\"registry-credentials\"\n\necho \"&gt; Creating tap-install namespace: $TAP_INSTALL_NAMESPACE\"\nkubectl create ns $TAP_INSTALL_NAMESPACE || true\n\nINSTALL_REGISTRY_HOSTNAME=${INSTALL_REGISTRY_HOSTNAME:-\"registry.tanzu.vmware.com\"}\nINSTALL_REGISTRY_USERNAME=${INSTALL_REGISTRY_USERNAME:-\"\"}\nINSTALL_REGISTRY_PASSWORD=${INSTALL_REGISTRY_PASSWORD:-\"\"}\nINSTALL_REGISTRY_REPO=${INSTALL_REGISTRY_REPO:-\"tap/tap-packages\"}\n\necho \"&gt; Creating ${INSTALL_REGISTRY_SECRET} secret\"\ntanzu secret registry add ${INSTALL_REGISTRY_SECRET} \\\n    --server    $INSTALL_REGISTRY_HOSTNAME \\\n    --username  $INSTALL_REGISTRY_USERNAME \\\n    --password  $INSTALL_REGISTRY_PASSWORD \\\n    --namespace ${TAP_INSTALL_NAMESPACE} \\\n    --export-to-all-namespaces \\\n    --yes\n\nBUILD_REGISTRY=${BUILD_REGISTRY:-\"dev.registry.tanzu.vmware.com\"}\nBUILD_REGISTRY_REPO=${BUILD_REGISTRY_REPO:-\"\"}\nBUILD_REGISTRY_USER=${BUILD_REGISTRY_USER:-\"\"}\nBUILD_REGISTRY_PASS=${BUILD_REGISTRY_PASS:-\"\"}\n\necho \"&gt; Creating ${BUILD_REGISTRY_SECRET} secret\"\ntanzu secret registry add ${BUILD_REGISTRY_SECRET} \\\n    --server    $BUILD_REGISTRY \\\n    --username  $BUILD_REGISTRY_USER \\\n    --password  $BUILD_REGISTRY_PASS \\\n    --namespace ${TAP_INSTALL_NAMESPACE} \\\n    --export-to-all-namespaces \\\n    --yes\n\nPACKAGE_REPOSITORY=\"$INSTALL_REGISTRY_HOSTNAME\"/$INSTALL_REGISTRY_REPO:\"$TAP_VERSION\"\necho \"&gt; Install TAP Package Repository: ${PACKAGE_REPOSITORY}\"\ntanzu package repository add tanzu-tap-repository --url \"$PACKAGE_REPOSITORY\" --namespace ${TAP_INSTALL_NAMESPACE} || true\nkubectl wait --for=condition=ReconcileSucceeded PackageRepository tanzu-tap-repository -n ${TAP_INSTALL_NAMESPACE} --timeout=15m\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#tbs-dependencies","title":"TBS Dependencies","text":"<p>Build, Iterate, Full Profiles only</p> <p>When installing other profiles, such as View and Run, you do not need Tanzu Build Service or its dependencies.</p> <p>When in an internetes restricted environment, we need to install the TBS dependencies by ourselves.</p> <p>This way we can leverage relocated images that come from a local source we can use, such as Harbor.</p> <p>The commands below assume you've already relocated the images to an accessible Harbor instance.</p> <pre><code>HARBOR_HOSTNAME=\nTBS_VERSION=\n</code></pre> <p>Info</p> <p>TAP <code>1.3.4</code> ships with Tanzu Build Service (TBS) version <code>1.7.4</code>.</p> <p>Install the package repository, pointing to the manifests in Harbor.</p> <pre><code>tanzu package repository add tbs-full-deps-repository \\\n  --url $HARBOR_HOSTNAME/buildservice/tbs-full-deps:$TBS_VERSION  \\\n  --namespace tap-install\n</code></pre> <p>Verify the packages are available.</p> <pre><code>tanzu package repository list --namespace tap-install\n</code></pre> <p>We can then install the all the dependencies.</p> <pre><code>tanzu package install full-tbs-deps \\\n  -p full-tbs-deps.tanzu.vmware.com \\\n  -v $TBS_VERSION \\\n  -n tap-install\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#install-tap-profiles","title":"Install TAP Profiles","text":"<p>The suggested order is as follows:</p> <ul> <li>View</li> <li>Build - Basic Supply Chain</li> <li>Build - Scanning &amp; Testing</li> <li>Run</li> </ul> <p>With the addition that if you install for the first time, you first in stall the Basic Supply Chain with the Build profile.</p> <p>Once you prove the Build profile works, you update it to the Scanning &amp; Testing supply chain.</p> <p>You can also wait with that update, until you've completed the complete View, Build, and Run setup.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#setup-developer-namespace","title":"Setup Developer Namespace","text":"<p>To make a namespace usable for TAP, we need the following:</p> <ul> <li>the namespace needs to exist</li> <li>we need the <code>registry-credentials</code> secret for reading/writing to and from the OCI registry</li> <li>rbac permissions for the namespace's default Service Account</li> </ul> <p>This script resides in the tap/1.3.4/scripts folder.</p> <pre><code>./tap-developer-namespace.sh\n</code></pre> Create Developer Namespace Script <pre><code>#!/usr/bin/env bash\nset -euo pipefail\n\nDEVELOPER_NAMESPACE=${DEVELOPER_NAMESPACE:-\"default\"}\nBUILD_REGISTRY=${BUILD_REGISTRY:-\"\"}\nBUILD_REGISTRY_USER=${BUILD_REGISTRY_USER:-\"\"}\nBUILD_REGISTRY_PASS=${BUILD_REGISTRY_PASS:-\"\"}\n\necho \"&gt; Creating Dev namspace $DEVELOPER_NAMESPACE\"\nkubectl create ns ${DEVELOPER_NAMESPACE} || true\n\necho \"&gt; Creating Dev namespace registry secret\"\ntanzu secret registry add registry-credentials  \\\n  --server    $BUILD_REGISTRY \\\n  --username  $BUILD_REGISTRY_USER \\\n  --password  $BUILD_REGISTRY_PASS \\\n  --namespace ${DEVELOPER_NAMESPACE} \\\n  --yes\n\necho \"&gt; Configuring RBAC for developer namespace\"\nkubectl apply -f dev-namespace-rbac.yml -n $DEVELOPER_NAMESPACE\n</code></pre> <p>You can also run this as follows, to setup a different namespace than <code>default</code>.</p> <pre><code>DEVELOPER_NAMESPACE=\"some-other-namespace\" ./tap-build-install-basic.sh\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#cross-cluster-test","title":"Cross-cluster Test","text":"<ul> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.3/tap/GUID-multicluster-getting-started.html#start-the-workload-on-the-build-profile-cluster-1</li> </ul>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#get-deliverable","title":"Get Deliverable","text":"<ol> <li>verify Deliverable content for your workload exists in the Build cluster     <pre><code>kubectl get configmap $APP_NAME --namespace ${DEVELOPER_NAMESPACE} -o go-template='{{.data.deliverable}}'\n</code></pre></li> <li>retrieve (from the Build cluster) and store it on disk     <pre><code>kubectl get configmap $APP_NAME -n ${DEVELOPER_NAMESPACE} -o go-template='{{.data.deliverable}}' &gt; deliverable-$APP_NAME.yaml\n</code></pre></li> <li>apply it to the Run cluster     <pre><code>kubectl apply -f deliverable-$APP_NAME.yaml --namespace ${DEVELOPER_NAMESPACE}\n</code></pre></li> <li>verify it works     <pre><code>kubectl get deliverables --namespace ${DEVELOPER_NAMESPACE}\n</code></pre></li> </ol>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#verify-application","title":"Verify Application","text":"<pre><code>kubectl get httpproxy -n ${DEVELOPER_NAMESPACE} -l contour.networking.knative.dev/parent=tap-hello-world -ojsonpath=\"{.items.*.spec.virtualhost}\"\n</code></pre> <pre><code>PROXY_URL=$(kubectl get httpproxy -n ${DEVELOPER_NAMESPACE} -l contour.networking.knative.dev/parent=tap-hello-world -ojsonpath=\"{.items.*.spec.virtualhost}\" | jq .fqdn | grep ssl | cut -d '\"' -f 2)\n\n```sh\nhttp $PROXY_URL\n</code></pre> <p>App Update</p> <p>The Bundle of the application will always have the same tag. The latest build will get this tag, and this means only the latest bundle will only ever have this tag.</p> <p>As a consequence of this, the Deliverable in the Run Cluster, is always correct. TAP will automatically detect the updated bundle (tag) and update the deployment.</p> <p>So the copying of the Deliverable, is a one time step.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13-overview/#links","title":"Links","text":"<ul> <li>TKGs - Cluster Config settings (Custom CA)</li> <li>TKGs - Workload Cluster Creation</li> <li>TKGs - Install Kapp Controller</li> <li>TKGs - Install Packages with TKG 1.6</li> <li>TAP 1.3 - Install Guide</li> <li>TAP 1.3 - Offline Installation for TBS</li> <li>TAP 1.3 - Run profile</li> <li>TAP 1.3 - Multicluster Overview</li> <li>TAP 1.3 - Install Sigstore Stack</li> <li>TBS - Airgapped Installation</li> <li>TBS - Offline Installation</li> <li>TBS Dependencies - Airgapped</li> <li>Known Issue TAP 1.3.0 Cosign - TUF Key Invalid</li> <li>Docker - Trust Registry Custom Certificate/CA</li> </ul>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/build-basic/","title":"TAP Build Profile - Basic Supply Chain","text":"<p>The basic profile uses the basic supply chain.</p> <p>This means it does not install Grype and the Metadata store, and other scanning related tools.</p> <p>Make sure you go through the Satisfy Pre-requisites section of the main guide first.</p> <p>Now that we have all the pre-requisites out of the way, we can install the actual profile.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/build-basic/#install-script","title":"Install Script","text":"<p>The install script encapsulates installed the Cluster Essentials, if required, and the TAP Fundamentals (secrets, namespace etc.) if required.</p> <p>It also creates a package values file via a YTT template.</p> tap-build-install-basic.sh<pre><code>#!/usr/bin/env bash\nset -euo pipefail\n\nTAP_VERSION=${TAP_VERSION:-\"1.3.4\"}\nTAP_INSTALL_NAMESPACE=\"tap-install\"\nSECRET_GEN_VERSION=${SECRET_GEN_VERSION:-\"v0.9.1\"}\nDOMAIN_NAME=${DOMAIN_NAME:-\"127.0.0.1.nip.io\"}\nDEVELOPER_NAMESPACE=${DEVELOPER_NAMESPACE:-\"default\"}\nINSTALL_TAP_FUNDAMENTALS=${INSTALL_TAP_FUNDAMENTALS:-\"true\"}\nINSTALL_CLUSTER_ESSENTIALS=${INSTALL_CLUSTER_ESSENTIALS:-\"false\"}\nBUILD_REGISTRY_SECRET=${BUILD_REGISTRY_SECRET:-\"registry-credentials\"}\n\nif [ \"$INSTALL_CLUSTER_ESSENTIALS\" = \"true\" ]; then\n  echo \"&gt; Installing Cluster Essentials (Kapp Controller, SecretGen Controller)\"\n  ./install-cluster-essentials.sh\nfi\n\nif [ \"$INSTALL_TAP_FUNDAMENTALS\" = \"true\" ]; then\n  echo \"&gt; Installing TAP Fundamentals (namespace, secrets)\"\n  ./install-tap-fundamentals.sh\nfi\n\nytt -f ytt/tap-build-profile-basic.ytt.yml \\\n  -v tbsRepo=\"$TBS_REPO\" \\\n  -v buildRegistry=\"$BUILD_REGISTRY\" \\\n  -v buildRegistrySecret=\"$BUILD_REGISTRY_SECRET\" \\\n  -v buildRepo=\"$BUILD_REGISTRY_REPO\" \\\n  -v domainName=\"$DOMAIN_NAME\" \\\n  -v caCert=\"${CA_CERT}\" \\\n  &gt; \"tap-build-basic-values.yml\"\n\n\ntanzu package installed update --install tap \\\n  -p tap.tanzu.vmware.com \\\n  -v $TAP_VERSION \\\n  --values-file tap-build-basic-values.yml \\\n  -n ${TAP_INSTALL_NAMESPACE}\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/build-basic/#ytt-template","title":"YTT Template","text":"<p>The YTT template makes it easy to generate different configurations over time and for different environments.</p> <p>Because we don't need the scanning tools when using the Basic supply chain, we exclude the related packages.     <pre><code>excluded_packages:\n  - scanning.apps.tanzu.vmware.com\n  - grype.scanning.apps.tanzu.vmware.com\n</code></pre></p> <p>This does assume that you're oke with TAP installing the Certmanager and Contour packages. If not, you should disable those as well.</p> <p>They are:      <pre><code>- cert-manager.tanzu.vmware.com\n- contour.tanzu.vmware.com\n</code></pre></p> tap-build-profile-basic.ytt.yml<pre><code>#@ load(\"@ytt:data\", \"data\")\n#@ dv = data.values\n#@ kpRegistry = \"{}/{}\".format(dv.buildRegistry, dv.tbsRepo)\n---\nprofile: build\nbuildservice:\n  pull_from_kp_default_repo: true\n  exclude_dependencies: true\n  kp_default_repository: #@ kpRegistry\n  kp_default_repository_secret:\n    name: #@ dv.buildRegistrySecret\n    namespace: tap-install\n\nsupply_chain: basic\nootb_supply_chain_basic:\n  registry:\n    server: #@ dv.buildRegistry\n    repository: #@ dv.buildRepo\n\nshared:\n  ingress_domain: #@ dv.domainName\n  ca_cert_data: #@ dv.caCert\n\nceip_policy_disclosed: true\n\ncontour:\n  envoy:\n    service:\n      type: LoadBalancer\n\nexcluded_packages:\n  - scanning.apps.tanzu.vmware.com\n  - grype.scanning.apps.tanzu.vmware.com\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/build-basic/#script-input","title":"Script Input","text":"<p>The install script it designed to be fed with environment variables.</p> <p>The script has some sane defaults, and where applicable we override them.</p> <p>Warning</p> <p>Don't forget to fill in the values for the registry secret passwords!</p> <pre><code>export INSTALL_TAP_FUNDAMENTALS=\"true\" # creates namespace and secrets\nexport INSTALL_REGISTRY_HOSTNAME=${HARBOR_HOSTNAME}\nexport INSTALL_REGISTRY_USERNAME=admin\nexport INSTALL_REGISTRY_PASSWORD=''\n\nexport BUILD_REGISTRY=${HARBOR_HOSTNAME}\nexport BUILD_REGISTRY_REPO=tap-apps\nexport BUILD_REGISTRY_USER=admin\nexport BUILD_REGISTRY_PASS=''\n\nexport TAP_VERSION=1.3.4\nexport TBS_REPO=buildservice/tbs-full-deps\n\nexport DOMAIN_NAME=\"\"\nexport DEVELOPER_NAMESPACE=\"default\"\nexport CA_CERT=$(cat ssl/ca.pem)\n\nexport INSTALL_CLUSTER_ESSENTIALS=\"false\"\n</code></pre> <p>Disable Cluster Essentials when using TMC</p> <p>Clusters created via TMC get the Cluster Essentials installed automatically.</p> <p>So you set <code>INSTALL_CLUSTER_ESSENTIALS</code> to false, to avoid installing them twice.</p> <p>You do now have to create the ConfigMap for the Kapp controller for trusting the registry's CA.</p> <pre><code>#@ load(\"@ytt:data\", \"data\")\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: kapp-controller-config\n  namespace: #@ data.values.namespace\nstringData:\n  caCerts: #@ data.values.caCert\n</code></pre> <pre><code>KAPP_CONTROLLER_NAMESPACE=kapp-controller\nCA_CERT=$(cat ssl/ca.pem)\n</code></pre> <pre><code>ytt -f ytt/kapp-controller-config.ytt.yml \\\n  -v namespace=kapp-controller \\\n  -v caCert=\"${CA_CERT}\" \\\n  &gt; \"kapp-controller-config.yml\" \n</code></pre> <pre><code>kubectl apply -f kapp-controller-config.yml --namespace $KAPP_CONTROLLER_NAMESPACE\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/build-basic/#values-file-output","title":"Values File Output","text":"<p>When running the install script, <code>tap-build-install-basic.sh</code>, it will generate the package value file.</p> <p>The file, <code>tap-build-basic-values.yml</code>, will contain the translated values from the environment variables.</p> <p>Below is an example from my own installation.</p> <p>Resulting Values File</p> tap-build-basic-values.yml<pre><code>profile: build\nbuildservice:\n  pull_from_kp_default_repo: true\n  exclude_dependencies: true\n  kp_default_repository: harbor.h2o-2-4864.h2o.vmware.com/buildservice/tbs-full-deps\n  kp_default_repository_secret:\n    name: registry-credentials\n    namespace: tap-install\nsupply_chain: basic\nootb_supply_chain_basic:\n  registry:\n    server: harbor.h2o-2-4864.h2o.vmware.com\n    repository: tap-apps\nshared:\n  ingress_domain: build.h2o-2-4864.h2o.vmware.com\n  ca_cert_data: |-\n    -----BEGIN CERTIFICATE-----\n    ...\n    vhs=\n    -----END CERTIFICATE-----\nceip_policy_disclosed: true\nexcluded_packages:\n- scanning.apps.tanzu.vmware.com\n- grype.scanning.apps.tanzu.vmware.com\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/build-basic/#run-install","title":"Run Install","text":"<pre><code>./tap-build-install-basic.sh\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/build-basic/#test-workload","title":"Test Workload","text":"<p>We first set the name of the developer namespace you have setup for TAP.</p> <pre><code>DEVELOPER_NAMESPACE=${DEVELOPER_NAMESPACE:-\"default\"}\n</code></pre> <p>Set up Developer Namespace</p> <p>If you have not setup the developer namespace yet, you can do so in this section of the main guide.</p> <p>We can then either use the CLI or the <code>Workload</code> CR to create our test workload.</p> Tanzu CLIKubernetes Manifest <pre><code>tanzu apps workload create smoke-app \\\n  --git-repo https://github.com/sample-accelerators/tanzu-java-web-app.git \\\n  --git-branch main \\\n  --type web \\\n  --label app.kubernetes.io/part-of=smoke-app \\\n  --annotation autoscaling.knative.dev/minScale=1 \\\n  --yes \\\n  -n \"$DEVELOPER_NAMESPACE\"\n</code></pre> <pre><code>echo \"apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\n  labels:\n    app.kubernetes.io/part-of: smoke-app\n    apps.tanzu.vmware.com/workload-type: web\n  name: smoke-app\n  namespace: ${DEVELOPER_NAMESPACE}\nspec:\n  params:\n  - name: annotations\n    value:\n      autoscaling.knative.dev/minScale: \\\"1\\\"\n  source:\n    git:\n      ref:\n        branch: main\n      url: https://github.com/sample-accelerators/tanzu-java-web-app.git\n\" &gt; workload.yml\n</code></pre> <pre><code>kubectl apply -f workload.yml\n</code></pre> <p>Use <code>kubectl wait</code> to wait for the app to be ready.</p> <pre><code>kubectl wait --for=condition=Ready Workload smoke-app --timeout=10m -n \"$DEVELOPER_NAMESPACE\"\n</code></pre> <p>To see the logs:</p> <pre><code>tanzu apps workload tail smoke-app\n</code></pre> <p>To get the status:</p> <pre><code>tanzu apps workload get smoke-app\n</code></pre> <p>And then we can delete our test workload if want to.</p> <pre><code>tanzu apps workload delete smoke-app -y -n \"$DEVELOPER_NAMESPACE\"\n</code></pre> <p>This script resides in the tap/scripts folder.</p> <p>It does all the steps outlined in this paragraph, including the wait and cleanup.</p> <pre><code>./tap-workload-demo.sh\n</code></pre> Test TAP Workload Script <pre><code>#!/usr/bin/env bash\nset -euo pipefail\nDEVELOPER_NAMESPACE=${DEVELOPER_NAMESPACE:-\"default\"}\n\ntanzu apps workload delete smoke-app -y -n \"$DEVELOPER_NAMESPACE\" || true\n\ntanzu apps workload create smoke-app -y \\\n  --git-repo https://github.com/sample-accelerators/tanzu-java-web-app.git \\\n  --git-branch main \\\n  --type web \\\n  -n \"$DEVELOPER_NAMESPACE\"\n\nkubectl wait --for=condition=Ready Workload smoke-app --timeout=10m -n \"$DEVELOPER_NAMESPACE\"\n\ntanzu apps workload delete smoke-app -y -n \"$DEVELOPER_NAMESPACE\"\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/","title":"TAP Build Profile - Scanning &amp; Testing Supply Chain","text":"<p>Make sure you go through the Satisfy Pre-requisites section of the main guide first.</p> <p>Now that we have all the pre-requisites out of the way, we can install the actual profile.</p> <p>SSH Access to Git Server</p> <p>We recommend having access to your git server via SSH.</p> <p>You can take a look at TAP's official docs or my guide using Gitea.</p> <p>To install Gitea, look here.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#metadata-store","title":"Metadata Store","text":"<p>As the name implies, the Testing &amp; Scanning contains components that scan.</p> <p>After the scans, it needs to put the result somewhere.</p> <p>That somewhere is the Metadata Store.</p> <p>The Metadata Store is part of the View profile, is it makes the scan results viewable.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#collect-metadata-store-secrets","title":"Collect Metadata Store Secrets","text":"<p>In order for the scanning tools to talk to the Metadata Store, they need a write token and its CA (to trust its certificate).</p> <p>We collect these with from the cluster the View profile is installed on.</p> <p>Run these commands on the view cluster (assuming the View profile is already installed).</p> <pre><code>AUTH_TOKEN_SECRET_NAME=$(kubectl get secret -n metadata-store -o name | grep metadata-store-app-auth-token-)\nexport METADATA_STORE_CA=$(kubectl get -n metadata-store ${AUTH_TOKEN_SECRET_NAME} -o yaml | yq '.data.\"ca.crt\"')\nexport METADATA_STORE_ACCESS_TOKEN=$(kubectl get secrets metadata-store-read-write-client -n metadata-store -o jsonpath=\"{.data.token}\" | base64 -d)\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#create-build-secrets","title":"Create Build Secrets","text":"<p>Now, go back to the cluster you aim to install (or update) the Build profile with the Testing &amp; Scanning supply chain.</p> <p>First, create the namespace to hold the secrets.</p> <pre><code>kubectl create namespace metadata-store-secrets\n</code></pre> <p>Create the certificate secret file.</p> <pre><code>cat &lt;&lt;EOF &gt; store_ca.yaml\n---\napiVersion: v1\nkind: Secret\ntype: Opaque\nmetadata:\n  name: store-ca-cert\n  namespace: metadata-store-secrets\ndata:\n  ca.crt: $METADATA_STORE_CA\nEOF\n</code></pre> <p>And apply it to the cluster.</p> <pre><code>kubectl apply -f store_ca.yaml\n</code></pre> <p>And now create the access Token secret.</p> <pre><code>kubectl create secret generic store-auth-token \\\n  --from-literal=auth_token=$METADATA_STORE_ACCESS_TOKEN -n metadata-store-secrets\n</code></pre> <p>Naming Conventions</p> <p>The convention used here, is that both secrets are created in the <code>metadata-store-secrets</code> namespace.</p> <ul> <li>The CA secret is called <code>store-ca-cert</code>, with the key <code>ca.crt</code></li> <li>The Token secret is called <code>store-auth-token</code>, and its key is <code>auth_token</code></li> </ul> <p>These values are used in the TAP package values. If you change them here, you need to change them there as well</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#install-script","title":"Install Script","text":"<p>The install script encapsulates installed the Cluster Essentials, if required, and the TAP Fundamentals (secrets, namespace etc.) if required.</p> <p>It also creates a package values file via a YTT template.</p> tap-build-install-scan-test.sh<pre><code>#!/usr/bin/env bash\nset -euo pipefail\n\nTAP_VERSION=${TAP_VERSION:-\"1.3.4\"}\nTAP_INSTALL_NAMESPACE=\"tap-install\"\nSECRET_GEN_VERSION=${SECRET_GEN_VERSION:-\"v0.9.1\"}\nDOMAIN_NAME=${DOMAIN_NAME:-\"127.0.0.1.nip.io\"}\nDEVELOPER_NAMESPACE=${DEVELOPER_NAMESPACE:-\"default\"}\nINSTALL_TAP_FUNDAMENTALS=${INSTALL_TAP_FUNDAMENTALS:-\"true\"}\nINSTALL_CLUSTER_ESSENTIALS=${INSTALL_CLUSTER_ESSENTIALS:-\"false\"}\nVIEW_DOMAIN_NAME=${VIEW_DOMAIN_NAME:-\"127.0.0.1.nip.io\"}\nGIT_SSH_SECRET_KEY=${GIT_SSH_SECRET_KEY:-\"tap-build-ssh\"}\nMETADATA_STORE_URL=\"metadata-store.${VIEW_DOMAIN_NAME}\"\nMETADATA_STORE_SECRETS_NAMESPACE=${METADATA_STORE_SECRETS_NAMESPACE:-\"metadata-store-secrets\"}\n\nif [ \"$INSTALL_CLUSTER_ESSENTIALS\" = \"true\" ]; then\n  echo \"&gt; Installing Cluster Essentials (Kapp Controller, SecretGen Controller)\"\n  ./install-cluster-essentials.sh\nfi\n\nif [ \"$INSTALL_TAP_FUNDAMENTALS\" = \"true\" ]; then\n  echo \"&gt; Installing TAP Fundamentals (namespace, secrets)\"\n  ./install-tap-fundamentals.sh\nfi\n\nkubectl create namespace ${METADATA_STORE_SECRETS_NAMESPACE} | true\n\nkubectl create secret generic store-ca-cert \\\n  --namespace $METADATA_STORE_SECRETS_NAMESPACE \\\n  --from-literal=ca.crt=${METADATA_STORE_CA}\n\nkubectl create secret generic store-auth-token \\\n  --namespace $METADATA_STORE_SECRETS_NAMESPACE \\\n  --from-literal=token=\"${METADATA_STORE_AUTH}\"\n\nytt -f ytt/tap-build-profile-scan-test.ytt.yml \\\n  -v tbsRepo=\"$TBS_REPO\" \\\n  -v buildRegistry=\"$BUILD_REGISTRY\" \\\n  -v buildRegistrySecret=\"$BUILD_REGISTRY_SECRET\" \\\n  -v buildRepo=\"$BUILD_REGISTRY_REPO\" \\\n  -v domainName=\"$DOMAIN_NAME\" \\\n  -v devNamespace=\"$DEVELOPER_NAMESPACE\" \\\n  -v metadatastoreUrl=\"${METADATA_STORE_URL}\" \\\n  -v sshSecret=\"${GIT_SSH_SECRET_KEY}\" \\\n  -v caCert=\"${CA_CERT}\" \\\n  &gt; \"tap-build-scan-test-values.yml\"\n\n\ntanzu package installed update --install tap \\\n  -p tap.tanzu.vmware.com \\\n  -v $TAP_VERSION \\\n  --values-file tap-build-scan-test-values.yml \\\n  -n ${TAP_INSTALL_NAMESPACE}\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#ytt-template","title":"YTT Template","text":"<p>The YTT template makes it easy to generate different configurations over time and for different environments.</p> <p>Grype in Internet Restricted environments</p> <p>The main scanning tool used, is Grype.</p> <p>If your environment has restricted internet access, Grype requires some additional steps to work.</p> <p>Follow the guide to Airgapped Grype, and then come back.</p> <p>If you do not need this, remove the <code>package_overlays</code> section from the YTT and this values file.</p> <p>Other notable elements, are <code>grype</code>, <code>supply_chain</code>, and <code>ootb_supply_chain_testing_scanning</code> configuration items.</p> <p>It is through these settings we configure the Testing &amp; Scanning supply chain.</p> tap-build-profile-scan-test.ytt.yml<pre><code>#@ load(\"@ytt:data\", \"data\")\n#@ dv = data.values\n#@ kpRegistry = \"{}/{}\".format(dv.buildRegistry, dv.tbsRepo)\n---\nprofile: build\nbuildservice:\n  pull_from_kp_default_repo: true\n  exclude_dependencies: true\n  kp_default_repository: #@ kpRegistry\n  kp_default_repository_secret:\n    name: #@ dv.buildRegistrySecret\n    namespace: tap-install\n\nsupply_chain: testing_scanning\nootb_supply_chain_testing_scanning:\n  registry:\n    server: #@ dv.buildRegistry\n    repository: #@ dv.buildRepo\n  gitops:\n    ssh_secret: #@ dv.sshSecret\n\nshared:\n  ingress_domain: #@ dv.domainName\n  ca_cert_data: #@ dv.caCert\n\nceip_policy_disclosed: true\n\ngrype:\n  namespace: #@ dv.devNamespace\n  targetImagePullSecret: #@ dv.buildRegistrySecret\n  metadataStore:\n    url: #@ dv.metadatastoreUrl\n    caSecret:\n        name: store-ca-cert\n        importFromNamespace: metadata-store-secrets\n    authSecret:\n        name: store-auth-token\n        importFromNamespace: metadata-store-secrets\nscanning:\n  metadataStore:\n    url: \"\" #! this config has changed, but this value is still required 'for historical reasons'\npackage_overlays:\n  - name: \"grype\"\n    secrets:\n      - name: \"grype-airgap-overlay\" #! see warning\n</code></pre> <p>Note</p> <p>The configuration element <code>scanning.metadataStore.url</code> is a left over from previous versions.</p> <p>Unfortunately, we have to set it to <code>\"\"</code>, to avoid problems.</p> <p>If it is set to a value, it will trigger an automation for configuring components which is no longer functional, causing the installation to fail.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#script-input","title":"Script Input","text":"<p>The install script it designed to be fed with environment variables.</p> <p>The script has some sane defaults, and where applicable we override them.</p> <p>Warning</p> <p>Don't forget to fill in the values for the registry secret passwords!</p> <pre><code>export INSTALL_TAP_FUNDAMENTALS=\"true\" # creates namespace and secrets\nexport INSTALL_REGISTRY_HOSTNAME=${HARBOR_HOSTNAME}\nexport INSTALL_REGISTRY_USERNAME=admin\nexport INSTALL_REGISTRY_PASSWORD=''\n\nexport BUILD_REGISTRY=${HARBOR_HOSTNAME}\nexport BUILD_REGISTRY_REPO=tap-apps\nexport BUILD_REGISTRY_USER=admin\nexport BUILD_REGISTRY_PASS=''\n\nexport TAP_VERSION=1.3.4\nexport TBS_REPO=buildservice/tbs-full-deps\n\nexport DOMAIN_NAME=\"\"\nexport DEVELOPER_NAMESPACE=\"default\"\nexport CA_CERT=$(cat ssl/ca.pem)\n\nexport INSTALL_CLUSTER_ESSENTIALS=\"false\"\n</code></pre> <p>Disable Cluster Essentials when using TMC</p> <p>Clusters created via TMC get the Cluster Essentials installed automatically.</p> <p>So you set <code>INSTALL_CLUSTER_ESSENTIALS</code> to false, to avoid installing them twice.</p> <p>You do now have to create the ConfigMap for the Kapp controller for trusting the registry's CA.</p> <pre><code>#@ load(\"@ytt:data\", \"data\")\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: kapp-controller-config\n  namespace: #@ data.values.namespace\nstringData:\n  caCerts: #@ data.values.caCert\n</code></pre> <pre><code>KAPP_CONTROLLER_NAMESPACE=kapp-controller\nCA_CERT=$(cat ssl/ca.pem)\n</code></pre> <pre><code>ytt -f ytt/kapp-controller-config.ytt.yml \\\n  -v namespace=kapp-controller \\\n  -v caCert=\"${CA_CERT}\" \\\n  &gt; \"kapp-controller-config.yml\" \n</code></pre> <pre><code>kubectl apply -f kapp-controller-config.yml --namespace $KAPP_CONTROLLER_NAMESPACE\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#values-file-output","title":"Values File Output","text":"<p>When running the install script, <code>tap-build-install-scan-test.sh</code>, it will generate the package value file.</p> <p>The file, <code>tap-build-scan-test-values.yml</code>, will contain the translated values from the environment variables.</p> <p>Below is an example from my own installation.</p> <p>Resulting Values File</p> tap-build-scan-test-values.yml<pre><code>profile: build\nbuildservice:\n  pull_from_kp_default_repo: true\n  exclude_dependencies: true\n  kp_default_repository: harbor.h2o-2-4864.h2o.vmware.com/buildservice/tbs-full-deps\n  kp_default_repository_secret:\n    name: registry-credentials\n    namespace: tap-install\nsupply_chain: testing_scanning\nootb_supply_chain_testing_scanning:\n  registry:\n    server: harbor.h2o-2-4864.h2o.vmware.com\n    repository: tap-apps\n  gitops:\n    ssh_secret: tap-build-ssh\nshared:\n  ingress_domain: build.h2o-2-4864.h2o.vmware.com\n  ca_cert_data: |-\n    -----BEGIN CERTIFICATE-----\n    ...\n    vhs=\n    -----END CERTIFICATE-----\nceip_policy_disclosed: true\ngrype:\n  namespace: default\n  targetImagePullSecret: registry-credentials\n  metadataStore:\n    url: \"https://metadata-store.view.h2o-2-4864.h2o.vmware.com/\"\n    caSecret:\n      name: store-ca-cert\n      importFromNamespace: metadata-store-secrets\n    authSecret:\n      name: store-auth-token\n      importFromNamespace: metadata-store-secrets\nscanning:\n  metadataStore:\n    url: \"\" \npackage_overlays:\n  - name: \"grype\"\n    secrets:\n      - name: \"grype-airgap-overlay\"\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#run-install","title":"Run Install","text":"<pre><code>./tap-build-install-basic.sh\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#testing-pipeline","title":"Testing Pipeline","text":"<p>In TAP <code>1.3.x</code>, the Testing &amp; Scanning Supply Chain does not contain a Testing Pipeline OOTB.</p> <p>It expects that you create your own Tekton Pipeline which can be found via the <code>apps.tanzu.vmware.com/pipeline: test</code> label.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#tekton-introduction","title":"Tekton Introduction","text":"<p>Tekton has a large number of CRs that let you create complex Continuous Delivery workflows.</p> <p>For the sake of brevity, and to stick close to TAP, we'll limit this to the five most relevant ones.</p> <ul> <li>Pipeline: defines one or more tasks to be run, the order and how the tasks relate, essentially a template</li> <li>PipelineRun: a single run of a Pipeline, supplying the workspaces and parameters, an instance of a Pipeline (template)</li> <li>Task: a collection of one or more steps, using container images to execute commands, essentially a template</li> <li>TaskRun: a single run of a Task, supplying the workspaces and parameters, an instance of a Task (template)</li> <li>Workspace: not a CR itself, but an important concept, workspaces are volumes mounted into the container to do and share their work</li> </ul> <pre><code>classDiagram\n  Pipeline --&gt; Task\n  PipelineRun --&gt; Pipeline\n  TaskRun --&gt; Task\n  class Pipeline{\n    +Task[] tasks\n    +Parameter[] params\n    +Workspace[] workspaces\n  }\n  class Task {\n    +Parameter[] params\n    +Workspace[] workspaces\n    +Step[] steps\n  }\n  class TaskRun {\n    +Task taskRef\n    +Workspace[] workspaces\n  }\n  class PipelineRun {\n    +Pipeline pipelineRef\n    +Workspace[] workspaces\n    +Parameter[] params\n  }</code></pre> <p>The reason we explain this, is because you will find these resources (bar the <code>workspace</code>) in your namespace.</p> <p>And in the event of failures or other problems, you need to take a lookt at their events.</p> <p>With regard to creating Task CRs, there is the Tekton Catalog with a user friendly GUI for exploring community Tasks.</p> <p>Pipeline and Task are Namespaced</p> <p>How the Tekton CRs work, can be confusing at first.</p> <p>Remember that both the Pipeline and Task CRs are namespaces. Meaning, they exist per namespace.</p> <p>So a Pipeline runs in a namespace (via its twin, PipelineRun) and can only use Tasks that live in the same Namspace!</p> <p>You can also create ***ClusterTask***s, which are as the name implied, cluster wide. </p> <p>We recommend sticking to Pipelines and Tasks until you get more familliar with Tekton.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#tekton-and-tap","title":"Tekton and TAP","text":"<p>You migth wonder, \"how does TAP trigger my Tekton Pipeline?\".</p> <p>The route is follows:</p> <pre><code>graph LR\n  A[ClusterSupplyChain] --&gt; B[ClusterSourceTemplate];\n  B --&gt; C[ClusterRunTemplate]\n  C --&gt; D[PipelineRun]\n  D --&gt; E[Pipeline]</code></pre> <p>Don't worry if that sounds very daunting.</p> <p>For a first iteration, you only need to create a <code>Pipeline</code> CR yourself. The rest is taken care of by TAP and the OOTB Testing &amp; Scanning Supply Chain.</p> <p>For anything other than a POC, I recommend diving in a little more.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#official-docs-example","title":"Official Docs Example","text":"<p>Eventhough TAP doesn't ship with a default Tekton Pipeline CR, there is one listed in the official docs.</p> <p>You can see the example below:</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\n  name: developer-defined-tekton-pipeline\n  labels:\n    apps.tanzu.vmware.com/pipeline: test     # (!) required\nspec:\n  params:\n    - name: source-url                       # (!) required\n    - name: source-revision                  # (!) required\n  tasks:\n    - name: test\n      params:\n        - name: source-url\n          value: $(params.source-url)\n        - name: source-revision\n          value: $(params.source-revision)\n      taskSpec:\n        params:\n          - name: source-url\n          - name: source-revision\n        steps:\n          - name: test\n            image: gradle\n            script: |-\n              cd `mktemp -d`\n\n              wget -qO- $(params.source-url) | tar xvz -m\n              ./mvnw test\n</code></pre> <p>Assuming that you use a Java application build with Maven, this will work.</p> <p>But I recommend learning how to create a proper Tekton Pipeline</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#create-tekton-pipeline","title":"Create Tekton Pipeline","text":"<p>As described before, a Tekton Pipeline consists of Tasks, Parameters, and Workspaces.</p> <p>The official docs example doesn't use a workspace, because it runs a single inline task.</p> <p>In anyother scenario, you will want to define a Workspace, and that means we'll override the default ClusterRunTemplate. We'll come to that later, for now, let's define a conceptual pipeline.</p> <p>Let's say I have a Java application with Maven as its build tool.</p> <p>For this application I want to run multiple tests and scans not included in TAP (e.g., SonarQube, Snyk).</p> <p>We will need the following:</p> <ul> <li>a Workspace we can share between tasks (so we only checkout the code once)</li> <li>a Task that checks out our code (we can leverage the example for the docs)</li> <li>a Task that runs our tests</li> <li>a Task that runs other steps</li> </ul> <p>The base Pipeline resource looks as like this:</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\n  name: fluxcd-maven-test\nspec:\n  workspaces: []\n  params: []\n  tasks: []\n</code></pre> <p>TAP by default expects a label on the Pipeline, and uses two paramaters. As those parameters make sense, let's add those pre-defined elements to the Pipeline.</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\n  name: fluxcd-maven-test\n  labels:\n    apps.tanzu.vmware.com/pipeline: test     # (!) required\nspec:\n  workspaces: []\n  params:\n    - name: source-url                       # (!) required\n    - name: source-revision                  # (!) required\n  tasks: []\n</code></pre> <p>We need two workspaces. One for Maven settings, and one for sharing the source copy between tasks.</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\n  name: fluxcd-maven-test\n  labels:\n    apps.tanzu.vmware.com/pipeline: test     # (!) required\nspec:\n  workspaces:\n    - name: shared-workspace\n    - name: maven-settings\n  params:\n    - name: source-url                       # (!) required\n    - name: source-revision                  # (!) required\n  tasks: []\n</code></pre> <p>With TAP, FluxCD is constantly polling and updating its storage of GitRepositories. So instead of having to do a Git checkout, we can instead opt to download the sources from FluxCD.</p> <p>This is not recommended for Production, but its oke to get started. And it saves us the trouble of having to setup a lot more resources and make a lot more changes to the OOTB Supply Chain.</p> <p>Let's call that task <code>fetch-repository</code>.</p> <p>For now, let's stick to a single Maven task which runs our tests.</p> <p>Let's call that task <code>maven</code>.</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\n  name: fluxcd-maven-test\n  labels:\n    apps.tanzu.vmware.com/pipeline: test     # (!) required\nspec:\n  workspaces:\n    - name: shared-workspace\n    - name: maven-settings\n  params:\n    - name: source-url                       # (!) required\n    - name: source-revision                  # (!) required\n  tasks:\n    - name: fetch-repository\n    - name: maven\n</code></pre> <p>Oke, but these tasks only have names so far. We can either define them inline, or reference an existing Task (recommend).</p> <p>We'll create the <code>fetch-repository</code> Task in the next section. But for now, let's assume we'll give it the <code>source-url</code> parameter as input, as it contains the URL from where we can download the sources from FluxCD.</p> <p>We'll also have to give it the <code>shared-workspace</code> Workspace, so it can copy the sources onto the shared workspace.</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\n  name: fluxcd-maven-test\n  labels:\n    apps.tanzu.vmware.com/pipeline: test     # (!) required\nspec:\n  workspaces:\n    - name: shared-workspace\n    - name: maven-settings\n  params:\n    - name: source-url                       # (!) required\n    - name: source-revision                  # (!) required\n  tasks:\n    - name: fetch-repository\n      taskRef:\n        name: fluxcd-repo-download\n      workspaces:\n        - name: output\n          workspace: shared-workspace\n      params:\n        - name: source-url  \n          value: $(params.source-url)\n    - name: maven\n</code></pre> <p>For the maven Task, we have provide the Workspace, Parameters, and tell Tekton to run it after <code>fetch-repository</code>. Else, we cannot guarantee the shared workspace contains the source code.</p> <p>We do this with the <code>runAfter</code> property.</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\n  name: fluxcd-maven-test\n  labels:\n    apps.tanzu.vmware.com/pipeline: test     # (!) required\nspec:\n  workspaces:\n    - name: shared-workspace\n    - name: maven-settings\n  params:\n    - name: source-url                       # (!) required\n    - name: source-revision                  # (!) required\n  tasks:\n    - name: fetch-repository\n      taskRef:\n        name: fluxcd-repo-download\n      workspaces:\n        - name: output\n          workspace: shared-workspace\n      params:\n        - name: source-url  \n          value: $(params.source-url)\n    - name: maven\n      taskRef:\n        name: maven\n      runAfter:\n        - fetch-repository\n      params:\n        - name: CONTEXT_DIR\n          value: tanzu-java-web-app\n        - name: GOALS\n          value:\n            - clean\n            - verify\n      workspaces:\n        - name: maven-settings\n          workspace: maven-settings\n        - name: output\n          workspace: shared-workspace\n</code></pre> <p>Verify the Pipeline exists with the correct label.</p> <pre><code>kubectl get pipeline -l apps.tanzu.vmware.com/pipeline=test\n</code></pre> <p>This should yield:</p> <pre><code>NAME                 AGE\nfluxcd-maven-test    7h8m\n</code></pre> <p>We now have a complete Pipeline, but we are referencing Tasks, via the <code>taskRef.name</code>, that don't exist.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#tasks","title":"Tasks","text":"<p>Let us define the tasks that we refered to in the Pipeline.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#fluxcd-repo-download","title":"fluxcd-repo-download","text":"<p>Normally, we would start a Pipeline with checking out the source with the relevant revision. In the case of TAP, that is done for us by FluxCD.</p> <p>Instead, we'll convert the inline Task, the <code>taskSpec</code>, from the official docs example into a propert Tekton Task. In our Pipeline, we refered to it as <code>fluxcd-repo-download</code>, so let us use that name.</p> task-fluxcd-repo-download.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\n  name: fluxcd-repo-download\nspec:\n  params:\n    - name: source-url\n      type: string\n      description: |\n        the source url to download the code from, \n        in the form of a FluxCD repository checkout .tar.gz\n  workspaces:\n    - name: output\n      description: The git repo will be cloned onto the volume backing this Workspace.\n  steps:\n    - name: download-source\n      image: public.ecr.aws/docker/library/gradle:jdk17-focal\n      script: |\n        #!/usr/bin/env sh\n        cd $(workspaces.output.path)\n        wget -qO- $(params.source-url) | tar xvz -m\n</code></pre> <p>Some things to note:</p> <ul> <li>we define a workspace and a parameter, the Pipeline (via a PipelineRun) will provide these</li> <li>we use an image from AWS's public repository, to avoid rate limits at DockerHub</li> <li>we use a script, where we first change directory into our shared Workspace</li> </ul> <p>Don't forget to apply this Task into TAP Developer Namespace.</p> <pre><code>kubectl apply -f task-fluxcd-repo-download.yaml \\\n  --namespace $TAP_DEVELOPER_NAMESPACE\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#maven","title":"maven","text":"<p>The second and last task, is the Maven task.</p> <p>Here we can leverage the work of others, by starting with the community task.</p> <p>In my case, something didn't align with the workspaces, so I renamed the Workspace in the Task to <code>output</code>, the same as in other common tasks (such as git-clone).</p> Maven Task task-maven.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\n  name: maven\n  labels:\n    app.kubernetes.io/version: \"0.2\"\n  annotations:\n    tekton.dev/pipelines.minVersion: \"0.12.1\"\n    tekton.dev/categories: Build Tools\n    tekton.dev/tags: build-tool\n    tekton.dev/platforms: \"linux/amd64,linux/s390x,linux/ppc64le\"\nspec:\n  description: &gt;-\n    This Task can be used to run a Maven build.\n\n  workspaces:\n    - name: output\n      description: The workspace consisting of maven project.\n    - name: maven-settings\n      description: &gt;-\n        The workspace consisting of the custom maven settings\n        provided by the user.\n  params:\n    - name: MAVEN_IMAGE\n      type: string\n      description: Maven base image\n      default: gcr.io/cloud-builders/mvn@sha256:57523fc43394d6d9d2414ee8d1c85ed7a13460cbb268c3cd16d28cfb3859e641 #tag: latest\n    - name: GOALS\n      description: maven goals to run\n      type: array\n      default:\n        - \"package\"\n    - name: MAVEN_MIRROR_URL\n      description: The Maven repository mirror url\n      type: string\n      default: \"\"\n    - name: SERVER_USER\n      description: The username for the server\n      type: string\n      default: \"\"\n    - name: SERVER_PASSWORD\n      description: The password for the server\n      type: string\n      default: \"\"\n    - name: PROXY_USER\n      description: The username for the proxy server\n      type: string\n      default: \"\"\n    - name: PROXY_PASSWORD\n      description: The password for the proxy server\n      type: string\n      default: \"\"\n    - name: PROXY_PORT\n      description: Port number for the proxy server\n      type: string\n      default: \"\"\n    - name: PROXY_HOST\n      description: Proxy server Host\n      type: string\n      default: \"\"\n    - name: PROXY_NON_PROXY_HOSTS\n      description: Non proxy server host\n      type: string\n      default: \"\"\n    - name: PROXY_PROTOCOL\n      description: Protocol for the proxy ie http or https\n      type: string\n      default: \"http\"\n    - name: CONTEXT_DIR\n      type: string\n      description: &gt;-\n        The context directory within the repository for sources on\n        which we want to execute maven goals.\n      default: \".\"\n  steps:\n    - name: mvn-settings\n      image: registry.access.redhat.com/ubi8/ubi-minimal:8.2\n      script: |\n        #!/usr/bin/env bash\n\n        [[ -f $(workspaces.maven-settings.path)/settings.xml ]] &amp;&amp; \\\n        echo 'using existing $(workspaces.maven-settings.path)/settings.xml' &amp;&amp; exit 0\n\n        cat &gt; $(workspaces.maven-settings.path)/settings.xml &lt;&lt;EOF\n        &lt;settings&gt;\n          &lt;servers&gt;\n            &lt;!-- The servers added here are generated from environment variables. Don't change. --&gt;\n            &lt;!-- ### SERVER's USER INFO from ENV ### --&gt;\n          &lt;/servers&gt;\n          &lt;mirrors&gt;\n            &lt;!-- The mirrors added here are generated from environment variables. Don't change. --&gt;\n            &lt;!-- ### mirrors from ENV ### --&gt;\n          &lt;/mirrors&gt;\n          &lt;proxies&gt;\n            &lt;!-- The proxies added here are generated from environment variables. Don't change. --&gt;\n            &lt;!-- ### HTTP proxy from ENV ### --&gt;\n          &lt;/proxies&gt;\n        &lt;/settings&gt;\n        EOF\n\n        xml=\"\"\n        if [ -n \"$(params.PROXY_HOST)\" -a -n \"$(params.PROXY_PORT)\" ]; then\n          xml=\"&lt;proxy&gt;\\\n            &lt;id&gt;genproxy&lt;/id&gt;\\\n            &lt;active&gt;true&lt;/active&gt;\\\n            &lt;protocol&gt;$(params.PROXY_PROTOCOL)&lt;/protocol&gt;\\\n            &lt;host&gt;$(params.PROXY_HOST)&lt;/host&gt;\\\n            &lt;port&gt;$(params.PROXY_PORT)&lt;/port&gt;\"\n          if [ -n \"$(params.PROXY_USER)\" -a -n \"$(params.PROXY_PASSWORD)\" ]; then\n            xml=\"$xml\\\n                &lt;username&gt;$(params.PROXY_USER)&lt;/username&gt;\\\n                &lt;password&gt;$(params.PROXY_PASSWORD)&lt;/password&gt;\"\n          fi\n          if [ -n \"$(params.PROXY_NON_PROXY_HOSTS)\" ]; then\n            xml=\"$xml\\\n                &lt;nonProxyHosts&gt;$(params.PROXY_NON_PROXY_HOSTS)&lt;/nonProxyHosts&gt;\"\n          fi\n          xml=\"$xml\\\n              &lt;/proxy&gt;\"\n          sed -i \"s|&lt;!-- ### HTTP proxy from ENV ### --&gt;|$xml|\" $(workspaces.maven-settings.path)/settings.xml\n        fi\n\n        if [ -n \"$(params.SERVER_USER)\" -a -n \"$(params.SERVER_PASSWORD)\" ]; then\n          xml=\"&lt;server&gt;\\\n            &lt;id&gt;serverid&lt;/id&gt;\"\n          xml=\"$xml\\\n                &lt;username&gt;$(params.SERVER_USER)&lt;/username&gt;\\\n                &lt;password&gt;$(params.SERVER_PASSWORD)&lt;/password&gt;\"\n          xml=\"$xml\\\n              &lt;/server&gt;\"\n          sed -i \"s|&lt;!-- ### SERVER's USER INFO from ENV ### --&gt;|$xml|\" $(workspaces.maven-settings.path)/settings.xml\n        fi\n\n        if [ -n \"$(params.MAVEN_MIRROR_URL)\" ]; then\n          xml=\"    &lt;mirror&gt;\\\n            &lt;id&gt;mirror.default&lt;/id&gt;\\\n            &lt;url&gt;$(params.MAVEN_MIRROR_URL)&lt;/url&gt;\\\n            &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;\\\n          &lt;/mirror&gt;\"\n          sed -i \"s|&lt;!-- ### mirrors from ENV ### --&gt;|$xml|\" $(workspaces.maven-settings.path)/settings.xml\n        fi\n\n    - name: mvn-goals\n      image: $(params.MAVEN_IMAGE)\n      workingDir: $(workspaces.output.path)/$(params.CONTEXT_DIR)\n      command: [\"/usr/bin/mvn\"]\n      args:\n        - -s\n        - $(workspaces.maven-settings.path)/settings.xml\n        - \"$(params.GOALS)\"\n</code></pre> <p>Don't forget to apply this Task into TAP Developer Namespace.</p> <pre><code>kubectl apply -f task-maven.yaml \\\n  --namespace $TAP_DEVELOPER_NAMESPACE\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#override-tap-templates","title":"Override TAP Templates","text":"<p>Unfortuantely, we're not there yet.</p> <p>The predefined setup from TAP, creates a PipelineRun that does not match our Pipeline.</p> <p>We have two options here, we can either update the existing ClusterRunTemplate <code>tekton-source-pipelinerun</code>, or create a new one.</p> <p>If we create a new one, we also have to update the relevant ClusterSupplyChain and ClusterSourceTemplate. The benefit, is that it gives a better understanding on how Cartographer's Supply Chains work. Because of that, I recommend the first option: creating a new ClusterRunTemplate.</p> <p>We can start by copying the existing ClusterRunTemplate; <code>tekton-source-pipelinerun</code>.</p> <pre><code>kubectl get ClusterRunTemplate tekton-source-pipelinerun \\\n   -o yaml &gt; tekton-source-pipelinerun.yaml\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#create-clusterruntemplate","title":"Create ClusterRunTemplate","text":"<p>We have to add the two Workspaces, <code>maven-settings</code> and <code>shared-workspace</code>.</p> <p>These are essentially container Volumes. The Maven settings Workspace can be an <code>emptyDir: {}</code>, which is the minimum required to satisfy the requirement.</p> <p>For the Shared Workspace, we do not want to have a volume automatically generated. So we'll use a <code>volumeClaimTemplate</code>:</p> <pre><code>workspaces:\n- name: maven-settings\n  emptyDir: {}\n- name: shared-workspace\n  volumeClaimTemplate:\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Mi\n      volumeMode: Filesystem\n</code></pre> <p>The cleaned up end-result looks like this.</p> tekton-source-pipelinerun-workspace.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterRunTemplate\nmetadata:\n  name: tekton-source-pipelinerun-workspace\nspec:\n  outputs:\n    revision: spec.params[?(@.name==\"source-revision\")].value\n    url: spec.params[?(@.name==\"source-url\")].value\n  template:\n    apiVersion: tekton.dev/v1beta1\n    kind: PipelineRun\n    metadata:\n      generateName: $(runnable.metadata.name)$-\n      labels: $(runnable.metadata.labels)$\n    spec:\n      params: $(runnable.spec.inputs.tekton-params)$\n      pipelineRef:\n        name: $(selected.metadata.name)$\n      podTemplate:\n        securityContext:\n          fsGroup: 65532\n      workspaces:\n      - name: maven-settings\n        emptyDir: {}\n      - name: shared-workspace\n        volumeClaimTemplate:\n          spec:\n            accessModes:\n            - ReadWriteOnce\n            resources:\n              requests:\n                storage: 500Mi\n            volumeMode: Filesystem\n</code></pre> <p>And apply it to your cluster.</p> <pre><code>kubectl apply -f tekton-source-pipelinerun-workspace\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#create-clustersourcetemplate","title":"Create ClusterSourceTemplate","text":"<p>Now we either create a new ClusterSourceTemplate or we update the existing one.</p> <p>There is not much different to do here, but in order to show where it is used in the Supply Chain, I recommend creating a new one.</p> <p>The difficulty is that the value we need to change is inside a flattened YTT template. There's a reference to our ClusterRunTemplate, the property being <code>runTemplateRef</code>.</p> <p>I found it easiest to get export the existing one to a file, do a find &amp; replace of the existing value, and apply the new file.</p> <p>Don't forget to cleanup ids, annotations, and other managed fields.</p> <pre><code>kubectl get ClusterSourceTemplate testing-pipeline \\\n  -o yaml &gt; testing-pipeline-workspace.yaml\n</code></pre> <p>Use your tool of choice, or refer back to <code>sed</code>.</p> <p>First, we'll rename the resource.</p> <pre><code>sed -i -e \"s/testing-pipeline/testing-pipeline-workspace/g\" testing-pipeline-workspace.yaml\n</code></pre> <p>And then we'll update the reference to our ClusterRunTemplate.</p> <pre><code>sed -i -e \"s/tekton-source-pipelinerun/tekton-source-pipelinerun-workspace/g\" testing-pipeline-workspace.yaml\n</code></pre> <p>And apply it to your cluster.</p> <pre><code>kubectl apply -f testing-pipeline-workspace.yaml\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#update-clustersupplychain","title":"Update ClusterSupplyChain","text":"<p>Last but not least, we update to ClusterSupplyChain, <code>source-test-scan-to-url</code>, to use the new ClusterSourceTemplate.</p> <p>Retrieve the file.</p> <pre><code>kubectl get ClusterSupplyChain source-test-scan-to-url \\\n  -o yaml &gt; source-test-scan-to-url.yaml\n</code></pre> <p>Don't forget to cleanup ids, annotations, and other managed fields.</p> <pre><code>sed -i -e \"s/testing-pipeline/testing-pipeline-workspace/g\" source-test-scan-to-url.yaml\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#scan-policies","title":"Scan Policies","text":"<p>We cannot yet run our Testing &amp; Scanning pipeline, we need a Scan Policy!</p> <p>The Scan Policy contains the rules by which to judge the outcome of the vulnerability scans.</p> <p>For more information about this policy, refer to the TAP docs.</p> scan-policy.yaml<pre><code>apiVersion: scanning.apps.tanzu.vmware.com/v1beta1\nkind: ScanPolicy\nmetadata:\n  name: scan-policy\n  labels:\n    'app.kubernetes.io/part-of': 'enable-in-gui'\nspec:\n  regoFile: |\n    package main\n\n    # Accepted Values: \"Critical\", \"High\", \"Medium\", \"Low\", \"Negligible\", \"UnknownSeverity\"\n    notAllowedSeverities := [\"Critical\", \"High\", \"UnknownSeverity\"]\n    ignoreCves := []\n\n    contains(array, elem) = true {\n      array[_] = elem\n    } else = false { true }\n\n    isSafe(match) {\n      severities := { e | e := match.ratings.rating.severity } | { e | e := match.ratings.rating[_].severity }\n      some i\n      fails := contains(notAllowedSeverities, severities[i])\n      not fails\n    }\n\n    isSafe(match) {\n      ignore := contains(ignoreCves, match.id)\n      ignore\n    }\n\n    deny[msg] {\n      comps := { e | e := input.bom.components.component } | { e | e := input.bom.components.component[_] }\n      some i\n      comp := comps[i]\n      vulns := { e | e := comp.vulnerabilities.vulnerability } | { e | e := comp.vulnerabilities.vulnerability[_] }\n      some j\n      vuln := vulns[j]\n      ratings := { e | e := vuln.ratings.rating.severity } | { e | e := vuln.ratings.rating[_].severity }\n      not isSafe(vuln)\n      msg = sprintf(\"CVE %s %s %s\", [comp.name, vuln.id, ratings])\n    }\n</code></pre> <p>Update The Policy To Reflect Reality</p> <p>It is better to fix the leak before attempting to clear the bucket.</p> <p>So you might want to setup lest strict rules to start with, so that people get time to resolve them.</p> <p>For example, in our test application (see next section) there are some vulnerabilities.</p> <p>As I don't care too much about those at this point in time, I will update the policy.</p> <p>First, I'll restrict the <code>notAllowedSeverities</code> to <code>Critical</code> only.</p> <p>And then I add the known vulnerabilities in that category. If new Criticals show up, it will fail, but for now, we can start the pipeline</p> <pre><code>notAllowedSeverities := [\"Critical\"]\nignoreCves := [\"CVE-2016-1000027\", \"CVE-2016-0949\",\"CVE-2017-11291\",\"CVE-2018-12805\",\"CVE-2018-4923\",\"CVE-2021-40719\",\"CVE-2018-25076\",\"GHSA-45hx-wfhj-473x\",\"GHSA-jvfv-hrrc-6q72\",\"CVE-2018-12804\",\"GHSA-36p3-wjmg-h94x\",\"GHSA-36p3-wjmg-h94x\",\"GHSA-6v73-fgf6-w5j7\"]\n</code></pre> <p>Read the Triaging and Remediating CVEs guide for more information.</p> <p>When you're satistfied, apply the policy to the cluster.</p> <pre><code>kubectl apply -f scan-policy.yaml\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#test-workload","title":"Test Workload","text":"<p>We first set the name of the developer namespace you have setup for TAP.</p> <p>When creating a test Workload for the Testing &amp; Scanning Suuply Chain, you need to set an additional label.</p> <pre><code>--label apps.tanzu.vmware.com/has-tests=true\n</code></pre> <p>This will trigger the Supply Chain and start your build. If you omit this label, the TAP will say it cannot find a matching Supply Chain.</p> <pre><code>DEVELOPER_NAMESPACE=${DEVELOPER_NAMESPACE:-\"default\"}\n</code></pre> <p>Set up Developer Namespace</p> <p>If you have not setup the developer namespace yet, you can do so in this section of the main guide.</p> <p>We can then either use the CLI or the <code>Workload</code> CR to create our test workload.</p> Tanzu CLIKubernetes Manifest <pre><code>tanzu apps workload create smoke-app \\\n  --git-repo https://github.com/sample-accelerators/tanzu-java-web-app.git \\\n  --git-branch main \\\n  --type web \\\n  --label app.kubernetes.io/part-of=smoke-app \\\n  --label apps.tanzu.vmware.com/has-tests=true \\\n  --annotation autoscaling.knative.dev/minScale=1 \\\n  --yes \\\n  -n \"$DEVELOPER_NAMESPACE\"\n</code></pre> <pre><code>echo \"apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\n  labels:\n    app.kubernetes.io/part-of: smoke-app\n    apps.tanzu.vmware.com/workload-type: web\n    apps.tanzu.vmware.com/has-tests=true\n  name: smoke-app\n  namespace: ${DEVELOPER_NAMESPACE}\nspec:\n  params:\n  - name: annotations\n    value:\n      autoscaling.knative.dev/minScale: \\\"1\\\"\n  source:\n    git:\n      ref:\n        branch: main\n      url: https://github.com/sample-accelerators/tanzu-java-web-app.git\n\" &gt; workload.yml\n</code></pre> <pre><code>kubectl apply -f workload.yml\n</code></pre> <p>Use <code>kubectl wait</code> to wait for the app to be ready.</p> <pre><code>kubectl wait --for=condition=Ready Workload smoke-app --timeout=10m -n \"$DEVELOPER_NAMESPACE\"\n</code></pre> <p>To see the logs:</p> <pre><code>tanzu apps workload tail smoke-app\n</code></pre> <p>To get the status:</p> <pre><code>tanzu apps workload get smoke-app\n</code></pre> <p>And then we can delete our test workload if want to.</p> <pre><code>tanzu apps workload delete smoke-app -y -n \"$DEVELOPER_NAMESPACE\"\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/build-scanning-testing/#gitea-ssh","title":"Gitea &amp; SSH","text":"<p>If you've setup Gitea and SSH, you can upload the example application to Gitea.</p> <pre><code>git clone https://github.com/vmware-tanzu/application-accelerator-samples &amp;&amp; cd application-accelerator-samples\ngit remote add gitea https://gitea.build.h2o-2-4864.h2o.vmware.com/gitea/application-accelerator-samples.git\ngit push -u gitea main\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU","Grype"]},{"location":"tanzu/tkgs/tap13/run/","title":"TAP Run Profile","text":"<p>Make sure you go through the Satisfy Pre-requisites section of the main guide first.</p> <p>Now that we have all the pre-requisites out of the way, we can install the actual profile.</p> <p>Now that we have all the pre-requisites out of the way, we can install the actual profile.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/run/#install-script","title":"Install Script","text":"<p>The install script encapsulates installed the Cluster Essentials, if required, and the TAP Fundaments (secrets, namespace etc.) if required.</p> <p>It also creates a package values file via a YTT template.</p> tap-run-install.sh<pre><code>#!/usr/bin/env bash\nset -euo pipefail\n\nTAP_VERSION=${TAP_VERSION:-\"1.3.4\"}\nTAP_INSTALL_NAMESPACE=\"tap-install\"\nDOMAIN_NAME=${DOMAIN_NAME:-\"127.0.0.1.nip.io\"}\nVIEW_DOMAIN_NAME=${VIEW_DOMAIN_NAME:-\"127.0.0.1.nip.io\"}\nINSTALL_TAP_FUNDAMENTALS=${INSTALL_TAP_FUNDAMENTALS:-\"true\"}\nINSTALL_CLUSTER_ESSENTIALS=${INSTALL_CLUSTER_ESSENTIALS:-\"false\"}\n\nif [ \"$INSTALL_CLUSTER_ESSENTIALS\" = \"true\" ]; then\n  echo \"&gt; Installing Cluster Essentials (Kapp Controller, SecretGen Controller)\"\n  ./install-cluster-essentials.sh\nfi\n\nif [ \"$INSTALL_TAP_FUNDAMENTALS\" = \"true\" ]; then\n  echo \"&gt; Installing TAP Fundamentals (namespace, secrets)\"\n  ./install-tap-fundamentals.sh\nfi\n\nytt -f ytt/tap-run-profile.ytt.yml \\\n  -v domainName=\"$DOMAIN_NAME\" \\\n  -v buildRegistry=\"$BUILD_REGISTRY\" \\\n  -v buildRepo=\"$BUILD_REGISTRY_REPO\" \\\n  -v viewDomainName=\"$VIEW_DOMAIN_NAME\" \\\n  -v caCert=\"${CA_CERT}\" \\\n  &gt; \"tap-run-values.yml\"\n\ntanzu package installed update --install tap \\\n  -p tap.tanzu.vmware.com \\\n  -v $TAP_VERSION \\\n  --values-file tap-run-values.yml \\\n  -n ${TAP_INSTALL_NAMESPACE}\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/run/#ytt-template","title":"YTT Template","text":"<p>The YTT template makes it easy to generate different configurations over time and for different environments.</p> tap-run-profile.ytt.yml<pre><code>#@ load(\"@ytt:data\", \"data\")\n#@ dv = data.values\n---\nprofile: run\nceip_policy_disclosed: true\n\nshared:\n  ingress_domain: #@ dv.domainName\n  ca_cert_data: #@ dv.caCert\n\nsupply_chain: basic\nootb_supply_chain_basic:\n  registry:\n    server: #@ dv.buildRegistry\n    repository: #@ dv.buildRepo\n\ncontour:\n  envoy:\n    service:\n      type: LoadBalancer\n\nappliveview_connector:\n  backend:\n    sslDisabled: true\n    ingressEnabled: true\n    host: #@ \"appliveview.\"+dv.viewDomainName\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/run/#script-input","title":"Script Input","text":"<p>The install script it designed to be fed with environment variables.</p> <p>The script has some sane defaults, and where applicable we override them.</p> <p>Warning</p> <p>Don't forget to fill in the values for the registry secret passwords!</p> <pre><code>export INSTALL_TAP_FUNDAMENTALS=\"true\" # creates namespace and secrets\nexport INSTALL_REGISTRY_HOSTNAME=${HARBOR_HOSTNAME}\nexport INSTALL_REGISTRY_USERNAME=admin\nexport INSTALL_REGISTRY_PASSWORD=''\n\nexport BUILD_REGISTRY=${HARBOR_HOSTNAME}\nexport BUILD_REGISTRY_REPO=tap-apps\nexport BUILD_REGISTRY_USER=admin\nexport BUILD_REGISTRY_PASS=''\n\nexport TAP_VERSION=1.3.4\nexport TBS_REPO=buildservice/tbs-full-deps\n\nexport DOMAIN_NAME=\"\"\nexport VIEW_DOMAIN_NAME=\"\"\nexport DEVELOPER_NAMESPACE=\"default\"\nexport CA_CERT=$(cat ssl/ca.pem)\nexport INSTALL_CLUSTER_ESSENTIALS=\"false\"\n</code></pre> <p>Disable Cluster Essentials when using TMC</p> <p>Clusters created via TMC get the Cluster Essentials installed automatically.</p> <p>So you set <code>INSTALL_CLUSTER_ESSENTIALS</code> to false, to avoid installing them twice.</p> <p>You do now have to create the ConfigMap for the Kapp controller for trusting the registry's CA.</p> <pre><code>#@ load(\"@ytt:data\", \"data\")\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: kapp-controller-config\n  namespace: #@ data.values.namespace\nstringData:\n  caCerts: #@ data.values.caCert\n</code></pre> <pre><code>KAPP_CONTROLLER_NAMESPACE=kapp-controller\nCA_CERT=$(cat ssl/ca.pem)\n</code></pre> <pre><code>ytt -f ytt/kapp-controller-config.ytt.yml \\\n  -v namespace=kapp-controller \\\n  -v caCert=\"${CA_CERT}\" \\\n  &gt; \"kapp-controller-config.yml\" \n</code></pre> <pre><code>kubectl apply -f kapp-controller-config.yml --namespace $KAPP_CONTROLLER_NAMESPACE\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/run/#values-file-output","title":"Values File Output","text":"<p>When running the install script, <code>tap-run-install.sh</code>, it will generate the package value file.</p> <p>The file, <code>tap-run-values.yml</code>, will contain the translated values from the environment variables.</p> <p>Below is an example from my own installation.</p> <p>Resulting Values File</p> tap-run-values.yml<pre><code>profile: run\nceip_policy_disclosed: true\nshared:\n  ingress_domain: run.h2o-2-4864.h2o.vmware.com\n  ca_cert_data: |-\n    -----BEGIN CERTIFICATE-----\n    ...\n    vhs=\n    -----END CERTIFICATE-----\nsupply_chain: basic\nootb_supply_chain_basic:\n  registry:\n    server: harbor.h2o-2-4864.h2o.vmware.com\n    repository: tap-apps\ncontour:\n  envoy:\n    service:\n      type: LoadBalancer\nappliveview_connector:\n  backend:\n    sslDisabled: true\n    ingressEnabled: true\n    host: appliveview.view.h2o-2-4864.h2o.vmware.com\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/run/#run-install","title":"Run Install","text":"<pre><code>./tap-run-install.sh\n</code></pre> <p>Next Steps</p> <p>Verify the TAP installation succeeded.</p> <pre><code>tanzu package installed list --namespace tap-install\n</code></pre> <p>Which should look like this:</p> <pre><code>NAME                      PACKAGE-NAME                                        PACKAGE-VERSION  STATUS\napi-auto-registration     apis.apps.tanzu.vmware.com                          0.1.2            Reconcile succeeded\nappliveview-connector     connector.appliveview.tanzu.vmware.com              1.3.1            Reconcile succeeded\nappsso                    sso.apps.tanzu.vmware.com                           2.0.0            Reconcile succeeded\ncartographer              cartographer.tanzu.vmware.com                       0.5.4            Reconcile succeeded\ncert-manager              cert-manager.tanzu.vmware.com                       1.7.2+tap.1      Reconcile succeeded\ncnrs                      cnrs.tanzu.vmware.com                               2.0.2            Reconcile succeeded\ncontour                   contour.tanzu.vmware.com                            1.22.0+tap.5     Reconcile succeeded\neventing                  eventing.tanzu.vmware.com                           2.0.2            Reconcile succeeded\nfluxcd-source-controller  fluxcd.source.controller.tanzu.vmware.com           0.27.0+tap.1     Reconcile succeeded\nimage-policy-webhook      image-policy-webhook.signing.apps.tanzu.vmware.com  1.1.10           Reconcile succeeded\nootb-delivery-basic       ootb-delivery-basic.tanzu.vmware.com                0.10.5           Reconcile succeeded\nootb-templates            ootb-templates.tanzu.vmware.com                     0.10.5           Reconcile succeeded\nservice-bindings          service-bindings.labs.vmware.com                    0.8.1            Reconcile succeeded\nservices-toolkit          services-toolkit.tanzu.vmware.com                   0.8.1            Reconcile succeeded\nsource-controller         controller.source.apps.tanzu.vmware.com             0.5.1            Reconcile succeeded\ntap                       tap.tanzu.vmware.com                                1.3.4            Reconcile succeeded\ntap-auth                  tap-auth.tanzu.vmware.com                           1.1.0            Reconcile succeeded\ntap-telemetry             tap-telemetry.tanzu.vmware.com                      0.3.2            Reconcile succeeded\n</code></pre> <p>Assuming you already have completed setting up the Build,  you can continue with the Cross-cluster verification.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/view/","title":"TAP View Profile","text":"<p>Make sure you go through the Satisfy Pre-requisites section of the main guide first.</p> <p>Now that we have all the pre-requisites out of the way, we can install the actual profile.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/view/#setup-read-permissions","title":"Setup Read Permissions","text":"<p>The View profile is designed to view the resources across all related TAP clusters.</p> <p>This means it needs read permission on each cluster.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/view/#create-service-accounts","title":"Create Service Accounts","text":"<ul> <li> <p>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.3/tap/GUID-tap-gui-cluster-view-setup.html</p> </li> <li> <p>create: <code>tap-gui-viewer-service-account-rbac.yaml</code></p> </li> </ul> Service Account RBAC tap-gui-viewer-service-account-rbac.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: tap-gui\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  namespace: tap-gui\n  name: tap-gui-viewer\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: tap-gui-read-k8s\nsubjects:\n- kind: ServiceAccount\n  namespace: tap-gui\n  name: tap-gui-viewer\nroleRef:\n  kind: ClusterRole\n  name: k8s-reader\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: k8s-reader\nrules:\n- apiGroups: ['']\n  resources: ['pods', 'pods/log', 'services', 'configmaps', 'limitranges']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['metrics.k8s.io']\n  resources: ['pods']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['apps']\n  resources: ['deployments', 'replicasets', 'statefulsets', 'daemonsets']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['autoscaling']\n  resources: ['horizontalpodautoscalers']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['networking.k8s.io']\n  resources: ['ingresses']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['networking.internal.knative.dev']\n  resources: ['serverlessservices']\n  verbs: ['get', 'watch', 'list']\n- apiGroups: [ 'autoscaling.internal.knative.dev' ]\n  resources: [ 'podautoscalers' ]\n  verbs: [ 'get', 'watch', 'list' ]\n- apiGroups: ['serving.knative.dev']\n  resources:\n  - configurations\n  - revisions\n  - routes\n  - services\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['carto.run']\n  resources:\n  - clusterconfigtemplates\n  - clusterdeliveries\n  - clusterdeploymenttemplates\n  - clusterimagetemplates\n  - clusterruntemplates\n  - clustersourcetemplates\n  - clustersupplychains\n  - clustertemplates\n  - deliverables\n  - runnables\n  - workloads\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['source.toolkit.fluxcd.io']\n  resources:\n  - gitrepositories\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['source.apps.tanzu.vmware.com']\n  resources:\n  - imagerepositories\n  - mavenartifacts\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['conventions.apps.tanzu.vmware.com']\n  resources:\n  - podintents\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['kpack.io']\n  resources:\n  - images\n  - builds\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['scanning.apps.tanzu.vmware.com']\n  resources:\n  - sourcescans\n  - imagescans\n  - scanpolicies\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['tekton.dev']\n  resources:\n  - taskruns\n  - pipelineruns\n  verbs: ['get', 'watch', 'list']\n- apiGroups: ['kappctrl.k14s.io']\n  resources:\n  - apps\n  verbs: ['get', 'watch', 'list']\n- apiGroups: [ 'batch' ]\n  resources: [ 'jobs', 'cronjobs' ]\n  verbs: [ 'get', 'watch', 'list' ]\n- apiGroups: ['conventions.carto.run']\n  resources:\n  - podintents\n  verbs: ['get', 'watch', 'list']\n</code></pre> <p>Apply this to each Build and Run cluster that you want this View profile cluster to have access to.</p> <pre><code>kubectl create -f tap-gui-viewer-service-account-rbac.yaml\n</code></pre> <p>And then retrieve the resulting URL and Token for each cluster.</p> <p>We need these for the next step.</p> <p>Info</p> <p>This guides assumes you are using a single Build and a single Run cluster.</p> <p>If you have a different setup, you have to update the scripts and YTT template accordingly.</p> <pre><code>CLUSTER_URL=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')\n\nCLUSTER_TOKEN=$(kubectl -n tap-gui get secret $(kubectl -n tap-gui get sa tap-gui-viewer -o=json \\\n| jq -r '.secrets[0].name') -o=json \\\n| jq -r '.data[\"token\"]' \\\n| base64 --decode)\n\necho CLUSTER_URL: $CLUSTER_URL\necho CLUSTER_TOKEN: $CLUSTER_TOKEN\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/view/#install-script","title":"Install Script","text":"<p>The install script encapsulates installed the Cluster Essentials, if required, and the TAP Fundamentals (secrets, namespace etc.) if required.</p> <p>It also creates a package values file via a YTT template.</p> tap-view-install.sh<pre><code>#!/usr/bin/env bash\nset -euo pipefail\n\nTAP_VERSION=${TAP_VERSION:-\"1.3.4\"}\nTAP_INSTALL_NAMESPACE=\"tap-install\"\nDOMAIN_NAME=${DOMAIN_NAME:-\"127.0.0.1.nip.io\"}\nINSTALL_TAP_FUNDAMENTALS=${INSTALL_TAP_FUNDAMENTALS:-\"true\"}\nINSTALL_CLUSTER_ESSENTIALS=${INSTALL_CLUSTER_ESSENTIALS:-\"false\"}\n\nif [ \"$INSTALL_CLUSTER_ESSENTIALS\" = \"true\" ]; then\n  echo \"&gt; Installing Cluster Essentials (Kapp Controller, SecretGen Controller)\"\n  ./install-cluster-essentials.sh\nfi\n\nif [ \"$INSTALL_TAP_FUNDAMENTALS\" = \"true\" ]; then\n  echo \"&gt; Installing TAP Fundamentals (namespace, secrets)\"\n  ./install-tap-fundamentals.sh\nfi\n\necho \"&gt; Generating tap-view-values.yml\"\nytt -f ytt/tap-view-profile.ytt.yml \\\n  -v caCert=\"${CA_CERT}\" \\\n  -v domainName=\"$DOMAIN_NAME\" \\\n  -v buildClusterUrl=\"${BUILD_CLUSTER_URL}\" \\\n  -v buildClusterName=\"${BUILD_CLUSTER_NAME}\" \\\n  -v buildClusterToken=\"${BUILD_CLUSTER_TOKEN}\" \\\n  -v buildClusterTls=\"${BUILD_CLUSTER_TLS}\" \\\n  -v runClusterUrl=\"${RUN_CLUSTER_URL}\" \\\n  -v runClusterName=\"${RUN_CLUSTER_NAME}\" \\\n  -v runClusterToken=\"${RUN_CLUSTER_TOKEN}\" \\\n  -v runClusteTls=\"${RUN_CLUSTER_TLS}\" \\\n  &gt; \"tap-view-values.yml\"\n\necho \"&gt; Installing TAP $TAP_VERSION in $TAP_INSTALL_NAMESPACE\"\ntanzu package installed update --install tap \\\n  -p tap.tanzu.vmware.com \\\n  -v $TAP_VERSION \\\n  --values-file tap-view-values.yml \\\n  -n ${TAP_INSTALL_NAMESPACE}\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/view/#ytt-template","title":"YTT Template","text":"<p>The YTT template makes it easy to generate different configurations over time and for different environments.</p> <p>The View profile has one main component, the TAP GUI, based on the OSS project Backstage.</p> <p>We have to configure this with various values, three noticable values:</p> <ul> <li><code>tap_gui.app_config.catalog.locations</code>: ensure there's always some applications registered in the TAP GUI</li> <li><code>tap_gui.app_config.kubernetes.clusterLocatorMethods</code>: this is how the View cluster can read resources from the other clusters</li> </ul> <p>Info</p> <p>For more information about the configuration, use the Tanzu CLI to retrieve its value schema.</p> <pre><code>tanzu package available  get tap-gui.tanzu.vmware.com/1.3.5 --values-schema -n tap-install\n</code></pre> tap-view-profile.ytt.yml<pre><code>#@ load(\"@ytt:data\", \"data\")\n#@ dv = data.values\n---\nprofile: view\nceip_policy_disclosed: true #! Installation fails if this is not set to true. Not a string.\n\nshared:\n  ingress_domain: #@ data.values.domainName\n  ca_cert_data: #@ data.values.caCert\n\ntap_gui:\n  service_type: ClusterIP\n  ingressEnabled: true\n  ingressDomain: #@ data.values.domainName\n  app_config:\n    auth:\n      allowGuestAccess: true\n    customize:\n      #! custom_logo: 'BASE-64-IMAGE'\n      custom_name: 'Portal McPortalFace'\n    organization:\n      name: 'Org McOrg Face'\n    app:\n      baseUrl: #@ \"http://tap-gui.\"+data.values.domainName\n    catalog:\n      locations:\n        - type: url\n          target: https://github.com/joostvdg/tap-catalog/blob/main/catalog-info.yaml\n        - type: url\n          target: https://github.com/joostvdg/tap-hello-world/blob/main/catalog/catalog-info.yaml\n    backend:\n      baseUrl: #@ \"http://tap-gui.\"+data.values.domainName\n      cors:\n        origin: #@ \"http://tap-gui.\"+data.values.domainName\n    kubernetes:\n      serviceLocatorMethod:\n        type: 'multiTenant'\n      clusterLocatorMethods:\n        - type: 'config'\n          clusters:\n            - url: #@ data.values.buildClusterUrl\n              name: #@ data.values.buildClusterName\n              authProvider: serviceAccount\n              serviceAccountToken: #@ data.values.buildClusterToken\n              skipTLSVerify: true\n              skipMetricsLookup: false\n            - url: #@ data.values.runClusterUrl\n              name: #@ data.values.runClusterName\n              authProvider: serviceAccount\n              serviceAccountToken: #@ data.values.runClusterToken\n              skipTLSVerify: true\n              skipMetricsLookup: false\n\nappliveview:\n  ingressEnabled: true\n  sslDisabled: true\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/view/#script-input","title":"Script Input","text":"<p>The install script it designed to be fed with environment variables.</p> <p>The script has some sane defaults, and where applicable we override them.</p> <p>Warning</p> <p>Don't forget to fill in the values for the registry secret passwords!</p> <pre><code>export INSTALL_TAP_FUNDAMENTALS=\"true\" # creates namespace and secrets\nexport INSTALL_REGISTRY_HOSTNAME=${HARBOR_HOSTNAME}\nexport INSTALL_REGISTRY_USERNAME=admin\nexport INSTALL_REGISTRY_PASSWORD=''\n\nexport BUILD_REGISTRY=${HARBOR_HOSTNAME}\nexport BUILD_REGISTRY_REPO=tap-apps\nexport BUILD_REGISTRY_USER=admin\nexport BUILD_REGISTRY_PASS=''\n\nexport TAP_VERSION=1.3.4\nexport DOMAIN_NAME=\"\"\nexport CA_CERT=$(cat ssl/ca.pem)\n\nexport BUILD_CLUSTER_URL=https://1.2.3.4:6443\nexport BUILD_CLUSTER_NAME=my-build-cluster\nexport BUILD_CLUSTER_TOKEN=''\n\nexport RUN_CLUSTER_URL=https://5.6.7.8:6443\nexport RUN_CLUSTER_NAME=my-run-cluster\nexport RUN_CLUSTER_TOKEN=\n\nexport INSTALL_CLUSTER_ESSENTIALS=\"false\"\n</code></pre> <p>Disable Cluster Essentials when using TMC</p> <p>Clusters created via TMC get the Cluster Essentials installed automatically.</p> <p>So you set <code>INSTALL_CLUSTER_ESSENTIALS</code> to false, to avoid installing them twice.</p> <p>You do now have to create the ConfigMap for the Kapp controller for trusting the registry's CA.</p> <pre><code>#@ load(\"@ytt:data\", \"data\")\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: kapp-controller-config\n  namespace: #@ data.values.namespace\nstringData:\n  caCerts: #@ data.values.caCert\n</code></pre> <pre><code>KAPP_CONTROLLER_NAMESPACE=kapp-controller\nCA_CERT=$(cat ssl/ca.pem)\n</code></pre> <pre><code>ytt -f ytt/kapp-controller-config.ytt.yml \\\n  -v namespace=kapp-controller \\\n  -v caCert=\"${CA_CERT}\" \\\n  &gt; \"kapp-controller-config.yml\" \n</code></pre> <pre><code>kubectl apply -f kapp-controller-config.yml --namespace $KAPP_CONTROLLER_NAMESPACE\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/view/#values-file-output","title":"Values File Output","text":"<p>When running the install script, <code>tap-view-install.sh</code>, it will generate the package value file.</p> <p>The file, <code>tap-view-values.yml</code>, will contain the translated values from the environment variables.</p> <p>Below is an example from my own installation.</p> <p>Resulting Values File</p> tap-run-values.yml<pre><code>profile: view\nceip_policy_disclosed: true\nshared:\n  ingress_domain: view.h2o-2-4864.h2o.vmware.com\n  ca_cert_data: |-\n    -----BEGIN CERTIFICATE-----\n    ...\n    vhs=\n    -----END CERTIFICATE-----\ntap_gui:\n  service_type: ClusterIP\n  ingressEnabled: true\n  ingressDomain: view.h2o-2-4864.h2o.vmware.com\n  app_config:\n    auth:\n      allowGuestAccess: true\n    customize:\n      custom_name: Portal McPortalFace\n    organization:\n      name: Org McOrg Face\n    app:\n      baseUrl: http://tap-gui.view.h2o-2-4864.h2o.vmware.com\n    catalog:\n      locations:\n      - type: url\n        target: https://github.com/joostvdg/tap-catalog/blob/main/catalog-info.yaml\n    backend:\n      baseUrl: http://tap-gui.view.h2o-2-4864.h2o.vmware.com\n      cors:\n        origin: http://tap-gui.view.h2o-2-4864.h2o.vmware.com\n    locations:\n    - type: url\n      target: https://github.com/joostvdg/tap-catalog/blob/main/catalog-info.yaml\n    - type: url\n      target: https://github.com/joostvdg/tap-hello-world/blob/main/catalog/catalog-info.yaml\n    kubernetes:\n      serviceLocatorMethod:\n        type: multiTenant\n      clusterLocatorMethods:\n      - type: config\n        clusters:\n        - url: https://10.11.1.1:6443\n          name: build-01\n          authProvider: serviceAccount\n          serviceAccountToken: eyJhb...\n          skipTLSVerify: true\n          skipMetricsLookup: false\n        - url: https://10.12.1.1:6443\n          name: run-01\n          authProvider: serviceAccount\n          serviceAccountToken: eyJhb...\n          skipTLSVerify: true\n          skipMetricsLookup: false\nappliveview:\n  ingressEnabled: true\n  sslDisabled: true\n</code></pre>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]},{"location":"tanzu/tkgs/tap13/view/#run-install","title":"Run Install","text":"<pre><code>./tap-run-install.sh\n</code></pre> <p>Next Steps</p> <p>Verify the TAP installation succeeded.</p> <pre><code>tanzu package installed list --namespace tap-install\n</code></pre> <p>Which should look like this:</p> <pre><code>NAME                      PACKAGE-NAME                               PACKAGE-VERSION  STATUS\naccelerator               accelerator.apps.tanzu.vmware.com          1.3.2            Reconcile succeeded\napi-portal                api-portal.tanzu.vmware.com                1.2.5            Reconcile succeeded\nappliveview               backend.appliveview.tanzu.vmware.com       1.3.1            Reconcile succeeded\ncert-manager              cert-manager.tanzu.vmware.com              1.7.2+tap.1      Reconcile succeeded\ncontour                   contour.tanzu.vmware.com                   1.22.0+tap.5     Reconcile succeeded\nfluxcd-source-controller  fluxcd.source.controller.tanzu.vmware.com  0.27.0+tap.1     Reconcile succeeded\nlearningcenter            learningcenter.tanzu.vmware.com            0.2.4            Reconcile succeeded\nlearningcenter-workshops  workshops.learningcenter.tanzu.vmware.com  0.2.3            Reconcile succeeded\nmetadata-store            metadata-store.apps.tanzu.vmware.com       1.3.4            Reconcile succeeded\nsource-controller         controller.source.apps.tanzu.vmware.com    0.5.1            Reconcile succeeded\ntap                       tap.tanzu.vmware.com                       1.3.4            Reconcile succeeded\ntap-gui                   tap-gui.tanzu.vmware.com                   1.3.5            Reconcile succeeded\ntap-telemetry             tap-telemetry.tanzu.vmware.com             0.3.2            Reconcile succeeded\n</code></pre> <p>Assuming you already have completed setting up the Build,  you can continue with the Cross-cluster verification.</p>","tags":["TKG","Vsphere","TAP","1.3.4","TANZU"]}]}